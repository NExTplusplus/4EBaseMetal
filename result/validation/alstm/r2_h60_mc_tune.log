
		{"drop_out": 0.4, "drop_out_mc": 0.05, "repeat_mc": 50, "hidden": 30, "embedding_size": 5, "batch": 512, "lag": 3}
LME_Co_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5112 5112 5112
1.7082474 -0.6288155 0.48142856 -0.33910522
Validation: 570 570 570
Testing: 774 774 774
pre-processing time: 0.00018739700317382812
the split date is 2010-07-01
train dropout: 0.4 test dropout: 0.05
net initializing with time: 0.31015610694885254
preparing training and testing date with time: 4.76837158203125e-07
current epoch: 1
train loss is 0.012857
average val loss: 0.008321, accuracy: 0.1105
average test loss: 0.017039, accuracy: 0.1706
case acc: 0.2348293
case acc: 0.047470354
case acc: 0.2180923
case acc: 0.18383786
case acc: 0.18447945
case acc: 0.1551287
top acc: 0.1897 ::: bot acc: 0.2942
top acc: 0.0802 ::: bot acc: 0.0251
top acc: 0.1575 ::: bot acc: 0.2734
top acc: 0.1365 ::: bot acc: 0.2249
top acc: 0.1510 ::: bot acc: 0.2162
top acc: 0.1181 ::: bot acc: 0.1885
current epoch: 2
train loss is 0.011409
average val loss: 0.010495, accuracy: 0.1075
average test loss: 0.004986, accuracy: 0.0855
case acc: 0.103071466
case acc: 0.1460586
case acc: 0.08949741
case acc: 0.06447927
case acc: 0.064914554
case acc: 0.044851888
top acc: 0.0588 ::: bot acc: 0.1626
top acc: 0.1947 ::: bot acc: 0.0906
top acc: 0.0328 ::: bot acc: 0.1431
top acc: 0.0210 ::: bot acc: 0.1039
top acc: 0.0313 ::: bot acc: 0.0964
top acc: 0.0147 ::: bot acc: 0.0751
current epoch: 3
train loss is 0.012650
average val loss: 0.022387, accuracy: 0.1782
average test loss: 0.005834, accuracy: 0.0741
case acc: 0.0406992
case acc: 0.23474935
case acc: 0.044438962
case acc: 0.040237766
case acc: 0.0350272
case acc: 0.0492757
top acc: 0.0450 ::: bot acc: 0.0593
top acc: 0.2832 ::: bot acc: 0.1790
top acc: 0.0769 ::: bot acc: 0.0410
top acc: 0.0821 ::: bot acc: 0.0121
top acc: 0.0658 ::: bot acc: 0.0110
top acc: 0.0865 ::: bot acc: 0.0178
current epoch: 4
train loss is 0.015466
average val loss: 0.013926, accuracy: 0.1291
average test loss: 0.003711, accuracy: 0.0632
case acc: 0.052524265
case acc: 0.1687025
case acc: 0.054515637
case acc: 0.040525436
case acc: 0.032729924
case acc: 0.030464616
top acc: 0.0108 ::: bot acc: 0.1111
top acc: 0.2168 ::: bot acc: 0.1132
top acc: 0.0250 ::: bot acc: 0.0953
top acc: 0.0259 ::: bot acc: 0.0658
top acc: 0.0181 ::: bot acc: 0.0550
top acc: 0.0266 ::: bot acc: 0.0480
current epoch: 5
train loss is 0.013609
average val loss: 0.006616, accuracy: 0.0909
average test loss: 0.009217, accuracy: 0.1252
case acc: 0.15901989
case acc: 0.054932557
case acc: 0.15116638
case acc: 0.13543302
case acc: 0.12549438
case acc: 0.12519789
top acc: 0.1148 ::: bot acc: 0.2188
top acc: 0.0947 ::: bot acc: 0.0162
top acc: 0.0894 ::: bot acc: 0.2080
top acc: 0.0857 ::: bot acc: 0.1785
top acc: 0.0908 ::: bot acc: 0.1579
top acc: 0.0856 ::: bot acc: 0.1605
current epoch: 6
train loss is 0.008310
average val loss: 0.007054, accuracy: 0.0868
average test loss: 0.005883, accuracy: 0.0984
case acc: 0.12216399
case acc: 0.072624885
case acc: 0.11959663
case acc: 0.102279164
case acc: 0.08230366
case acc: 0.091689445
top acc: 0.0781 ::: bot acc: 0.1821
top acc: 0.1189 ::: bot acc: 0.0210
top acc: 0.0574 ::: bot acc: 0.1762
top acc: 0.0525 ::: bot acc: 0.1452
top acc: 0.0477 ::: bot acc: 0.1146
top acc: 0.0526 ::: bot acc: 0.1265
current epoch: 7
train loss is 0.008020
average val loss: 0.008872, accuracy: 0.0970
average test loss: 0.003731, accuracy: 0.0741
case acc: 0.08411634
case acc: 0.09842513
case acc: 0.0882075
case acc: 0.06998687
case acc: 0.04487108
case acc: 0.059221957
top acc: 0.0396 ::: bot acc: 0.1441
top acc: 0.1469 ::: bot acc: 0.0420
top acc: 0.0308 ::: bot acc: 0.1430
top acc: 0.0237 ::: bot acc: 0.1109
top acc: 0.0174 ::: bot acc: 0.0734
top acc: 0.0230 ::: bot acc: 0.0929
current epoch: 8
train loss is 0.008488
average val loss: 0.007500, accuracy: 0.0875
average test loss: 0.004437, accuracy: 0.0830
case acc: 0.09891235
case acc: 0.07569464
case acc: 0.10424085
case acc: 0.08677048
case acc: 0.05517545
case acc: 0.077250436
top acc: 0.0549 ::: bot acc: 0.1584
top acc: 0.1224 ::: bot acc: 0.0231
top acc: 0.0428 ::: bot acc: 0.1609
top acc: 0.0374 ::: bot acc: 0.1294
top acc: 0.0229 ::: bot acc: 0.0862
top acc: 0.0387 ::: bot acc: 0.1121
current epoch: 9
train loss is 0.007906
average val loss: 0.006853, accuracy: 0.0836
average test loss: 0.004815, accuracy: 0.0867
case acc: 0.104996555
case acc: 0.06278524
case acc: 0.11296349
case acc: 0.094881415
case acc: 0.058272254
case acc: 0.086230606
top acc: 0.0608 ::: bot acc: 0.1649
top acc: 0.1065 ::: bot acc: 0.0158
top acc: 0.0509 ::: bot acc: 0.1701
top acc: 0.0451 ::: bot acc: 0.1381
top acc: 0.0250 ::: bot acc: 0.0897
top acc: 0.0473 ::: bot acc: 0.1212
current epoch: 10
train loss is 0.007464
average val loss: 0.007013, accuracy: 0.0841
average test loss: 0.004318, accuracy: 0.0811
case acc: 0.09664548
case acc: 0.062278304
case acc: 0.107647024
case acc: 0.08933998
case acc: 0.049727596
case acc: 0.0806692
top acc: 0.0523 ::: bot acc: 0.1566
top acc: 0.1064 ::: bot acc: 0.0155
top acc: 0.0458 ::: bot acc: 0.1647
top acc: 0.0401 ::: bot acc: 0.1324
top acc: 0.0200 ::: bot acc: 0.0795
top acc: 0.0419 ::: bot acc: 0.1156
current epoch: 11
train loss is 0.007316
average val loss: 0.007225, accuracy: 0.0855
average test loss: 0.003894, accuracy: 0.0760
case acc: 0.08879103
case acc: 0.06256889
case acc: 0.102177836
case acc: 0.08379401
case acc: 0.043093972
case acc: 0.0758313
top acc: 0.0446 ::: bot acc: 0.1489
top acc: 0.1062 ::: bot acc: 0.0155
top acc: 0.0407 ::: bot acc: 0.1589
top acc: 0.0345 ::: bot acc: 0.1269
top acc: 0.0172 ::: bot acc: 0.0711
top acc: 0.0377 ::: bot acc: 0.1106
current epoch: 12
train loss is 0.007301
average val loss: 0.007321, accuracy: 0.0861
average test loss: 0.003649, accuracy: 0.0731
case acc: 0.08394823
case acc: 0.06068879
case acc: 0.09945016
case acc: 0.08088808
case acc: 0.03978317
case acc: 0.073605634
top acc: 0.0399 ::: bot acc: 0.1435
top acc: 0.1042 ::: bot acc: 0.0146
top acc: 0.0387 ::: bot acc: 0.1557
top acc: 0.0317 ::: bot acc: 0.1236
top acc: 0.0170 ::: bot acc: 0.0661
top acc: 0.0356 ::: bot acc: 0.1082
current epoch: 13
train loss is 0.007245
average val loss: 0.007244, accuracy: 0.0855
average test loss: 0.003560, accuracy: 0.0719
case acc: 0.08189523
case acc: 0.05796498
case acc: 0.098693535
case acc: 0.080379084
case acc: 0.03866404
case acc: 0.07386418
top acc: 0.0378 ::: bot acc: 0.1419
top acc: 0.0999 ::: bot acc: 0.0151
top acc: 0.0380 ::: bot acc: 0.1553
top acc: 0.0310 ::: bot acc: 0.1234
top acc: 0.0169 ::: bot acc: 0.0642
top acc: 0.0358 ::: bot acc: 0.1085
current epoch: 14
train loss is 0.007108
average val loss: 0.007099, accuracy: 0.0843
average test loss: 0.003549, accuracy: 0.0718
case acc: 0.08159915
case acc: 0.054825887
case acc: 0.09925524
case acc: 0.0809779
case acc: 0.038765553
case acc: 0.075529754
top acc: 0.0377 ::: bot acc: 0.1413
top acc: 0.0942 ::: bot acc: 0.0169
top acc: 0.0383 ::: bot acc: 0.1557
top acc: 0.0318 ::: bot acc: 0.1241
top acc: 0.0175 ::: bot acc: 0.0643
top acc: 0.0377 ::: bot acc: 0.1101
current epoch: 15
train loss is 0.006962
average val loss: 0.007047, accuracy: 0.0837
average test loss: 0.003479, accuracy: 0.0710
case acc: 0.08009003
case acc: 0.05283002
case acc: 0.09851293
case acc: 0.08068061
case acc: 0.03815494
case acc: 0.07570497
top acc: 0.0363 ::: bot acc: 0.1398
top acc: 0.0902 ::: bot acc: 0.0189
top acc: 0.0378 ::: bot acc: 0.1549
top acc: 0.0314 ::: bot acc: 0.1237
top acc: 0.0172 ::: bot acc: 0.0635
top acc: 0.0379 ::: bot acc: 0.1103
current epoch: 16
train loss is 0.006905
average val loss: 0.007039, accuracy: 0.0835
average test loss: 0.003400, accuracy: 0.0700
case acc: 0.078056835
case acc: 0.051278323
case acc: 0.09727546
case acc: 0.079887345
case acc: 0.03774929
case acc: 0.0759123
top acc: 0.0339 ::: bot acc: 0.1378
top acc: 0.0870 ::: bot acc: 0.0208
top acc: 0.0366 ::: bot acc: 0.1538
top acc: 0.0309 ::: bot acc: 0.1228
top acc: 0.0174 ::: bot acc: 0.0630
top acc: 0.0378 ::: bot acc: 0.1107
current epoch: 17
train loss is 0.006806
average val loss: 0.007061, accuracy: 0.0835
average test loss: 0.003311, accuracy: 0.0689
case acc: 0.076132394
case acc: 0.050287865
case acc: 0.0959769
case acc: 0.07869624
case acc: 0.036918495
case acc: 0.07552198
top acc: 0.0320 ::: bot acc: 0.1363
top acc: 0.0845 ::: bot acc: 0.0229
top acc: 0.0358 ::: bot acc: 0.1523
top acc: 0.0295 ::: bot acc: 0.1218
top acc: 0.0169 ::: bot acc: 0.0619
top acc: 0.0377 ::: bot acc: 0.1101
current epoch: 18
train loss is 0.006797
average val loss: 0.007283, accuracy: 0.0851
average test loss: 0.003069, accuracy: 0.0659
case acc: 0.07118706
case acc: 0.050801784
case acc: 0.091495514
case acc: 0.07481778
case acc: 0.034763794
case acc: 0.0724292
top acc: 0.0273 ::: bot acc: 0.1310
top acc: 0.0856 ::: bot acc: 0.0223
top acc: 0.0323 ::: bot acc: 0.1474
top acc: 0.0264 ::: bot acc: 0.1174
top acc: 0.0179 ::: bot acc: 0.0580
top acc: 0.0349 ::: bot acc: 0.1068
current epoch: 19
train loss is 0.006795
average val loss: 0.007459, accuracy: 0.0863
average test loss: 0.002903, accuracy: 0.0638
case acc: 0.06765116
case acc: 0.050960436
case acc: 0.08810243
case acc: 0.07224645
case acc: 0.03338159
case acc: 0.07052249
top acc: 0.0237 ::: bot acc: 0.1275
top acc: 0.0858 ::: bot acc: 0.0220
top acc: 0.0302 ::: bot acc: 0.1434
top acc: 0.0247 ::: bot acc: 0.1145
top acc: 0.0185 ::: bot acc: 0.0558
top acc: 0.0333 ::: bot acc: 0.1049
current epoch: 20
train loss is 0.006774
average val loss: 0.007510, accuracy: 0.0866
average test loss: 0.002827, accuracy: 0.0629
case acc: 0.06598421
case acc: 0.050175164
case acc: 0.08646079
case acc: 0.07106027
case acc: 0.03311893
case acc: 0.07041733
top acc: 0.0223 ::: bot acc: 0.1255
top acc: 0.0842 ::: bot acc: 0.0230
top acc: 0.0289 ::: bot acc: 0.1416
top acc: 0.0240 ::: bot acc: 0.1131
top acc: 0.0186 ::: bot acc: 0.0554
top acc: 0.0331 ::: bot acc: 0.1047
current epoch: 21
train loss is 0.006721
average val loss: 0.007432, accuracy: 0.0857
average test loss: 0.002843, accuracy: 0.0632
case acc: 0.06607792
case acc: 0.0489906
case acc: 0.08657408
case acc: 0.07155019
case acc: 0.033902414
case acc: 0.07182172
top acc: 0.0224 ::: bot acc: 0.1258
top acc: 0.0810 ::: bot acc: 0.0259
top acc: 0.0289 ::: bot acc: 0.1417
top acc: 0.0244 ::: bot acc: 0.1135
top acc: 0.0182 ::: bot acc: 0.0568
top acc: 0.0344 ::: bot acc: 0.1061
current epoch: 22
train loss is 0.006666
average val loss: 0.007282, accuracy: 0.0842
average test loss: 0.002911, accuracy: 0.0641
case acc: 0.06765432
case acc: 0.047161303
case acc: 0.08742257
case acc: 0.07319622
case acc: 0.03511651
case acc: 0.07408144
top acc: 0.0239 ::: bot acc: 0.1272
top acc: 0.0767 ::: bot acc: 0.0294
top acc: 0.0295 ::: bot acc: 0.1424
top acc: 0.0252 ::: bot acc: 0.1158
top acc: 0.0178 ::: bot acc: 0.0589
top acc: 0.0366 ::: bot acc: 0.1086
current epoch: 23
train loss is 0.006603
average val loss: 0.007478, accuracy: 0.0857
average test loss: 0.002749, accuracy: 0.0619
case acc: 0.06407418
case acc: 0.04753292
case acc: 0.084339194
case acc: 0.07056876
case acc: 0.033350606
case acc: 0.07172974
top acc: 0.0204 ::: bot acc: 0.1238
top acc: 0.0774 ::: bot acc: 0.0285
top acc: 0.0273 ::: bot acc: 0.1392
top acc: 0.0237 ::: bot acc: 0.1125
top acc: 0.0182 ::: bot acc: 0.0560
top acc: 0.0344 ::: bot acc: 0.1060
current epoch: 24
train loss is 0.006592
average val loss: 0.007353, accuracy: 0.0845
average test loss: 0.002815, accuracy: 0.0629
case acc: 0.06549354
case acc: 0.04621907
case acc: 0.08512729
case acc: 0.07165842
case acc: 0.034716
case acc: 0.07394854
top acc: 0.0219 ::: bot acc: 0.1250
top acc: 0.0738 ::: bot acc: 0.0320
top acc: 0.0277 ::: bot acc: 0.1402
top acc: 0.0241 ::: bot acc: 0.1141
top acc: 0.0180 ::: bot acc: 0.0582
top acc: 0.0365 ::: bot acc: 0.1085
current epoch: 25
train loss is 0.006534
average val loss: 0.007477, accuracy: 0.0854
average test loss: 0.002716, accuracy: 0.0616
case acc: 0.06346756
case acc: 0.046525467
case acc: 0.08298979
case acc: 0.06987402
case acc: 0.034121342
case acc: 0.07261393
top acc: 0.0198 ::: bot acc: 0.1233
top acc: 0.0744 ::: bot acc: 0.0319
top acc: 0.0265 ::: bot acc: 0.1375
top acc: 0.0229 ::: bot acc: 0.1120
top acc: 0.0185 ::: bot acc: 0.0570
top acc: 0.0353 ::: bot acc: 0.1070
current epoch: 26
train loss is 0.006545
average val loss: 0.007652, accuracy: 0.0867
average test loss: 0.002598, accuracy: 0.0600
case acc: 0.060902044
case acc: 0.046960693
case acc: 0.0800927
case acc: 0.067956336
case acc: 0.032919936
case acc: 0.07110606
top acc: 0.0175 ::: bot acc: 0.1205
top acc: 0.0755 ::: bot acc: 0.0306
top acc: 0.0248 ::: bot acc: 0.1342
top acc: 0.0219 ::: bot acc: 0.1095
top acc: 0.0189 ::: bot acc: 0.0550
top acc: 0.0338 ::: bot acc: 0.1055
current epoch: 27
train loss is 0.006535
average val loss: 0.007733, accuracy: 0.0872
average test loss: 0.002542, accuracy: 0.0592
case acc: 0.05971666
case acc: 0.0466959
case acc: 0.07876976
case acc: 0.066900745
case acc: 0.032691963
case acc: 0.07069338
top acc: 0.0163 ::: bot acc: 0.1193
top acc: 0.0750 ::: bot acc: 0.0307
top acc: 0.0243 ::: bot acc: 0.1324
top acc: 0.0211 ::: bot acc: 0.1084
top acc: 0.0190 ::: bot acc: 0.0545
top acc: 0.0335 ::: bot acc: 0.1052
current epoch: 28
train loss is 0.006532
average val loss: 0.007740, accuracy: 0.0870
average test loss: 0.002541, accuracy: 0.0593
case acc: 0.05970575
case acc: 0.046403464
case acc: 0.07807962
case acc: 0.066873804
case acc: 0.03290122
case acc: 0.07155318
top acc: 0.0163 ::: bot acc: 0.1193
top acc: 0.0743 ::: bot acc: 0.0318
top acc: 0.0238 ::: bot acc: 0.1316
top acc: 0.0212 ::: bot acc: 0.1084
top acc: 0.0187 ::: bot acc: 0.0552
top acc: 0.0343 ::: bot acc: 0.1060
current epoch: 29
train loss is 0.006533
average val loss: 0.007996, accuracy: 0.0890
average test loss: 0.002386, accuracy: 0.0571
case acc: 0.056510586
case acc: 0.04711059
case acc: 0.0746047
case acc: 0.064075984
case acc: 0.031171732
case acc: 0.06891397
top acc: 0.0135 ::: bot acc: 0.1159
top acc: 0.0763 ::: bot acc: 0.0298
top acc: 0.0219 ::: bot acc: 0.1273
top acc: 0.0196 ::: bot acc: 0.1049
top acc: 0.0198 ::: bot acc: 0.0519
top acc: 0.0320 ::: bot acc: 0.1032
current epoch: 30
train loss is 0.006518
average val loss: 0.007922, accuracy: 0.0882
average test loss: 0.002431, accuracy: 0.0577
case acc: 0.0574753
case acc: 0.046320904
case acc: 0.075117745
case acc: 0.06506159
case acc: 0.03195668
case acc: 0.07049283
top acc: 0.0142 ::: bot acc: 0.1170
top acc: 0.0739 ::: bot acc: 0.0320
top acc: 0.0222 ::: bot acc: 0.1280
top acc: 0.0200 ::: bot acc: 0.1062
top acc: 0.0194 ::: bot acc: 0.0534
top acc: 0.0334 ::: bot acc: 0.1049
current epoch: 31
train loss is 0.006474
average val loss: 0.007812, accuracy: 0.0871
average test loss: 0.002480, accuracy: 0.0584
case acc: 0.05882326
case acc: 0.04543442
case acc: 0.07581267
case acc: 0.06603122
case acc: 0.032372862
case acc: 0.072052404
top acc: 0.0153 ::: bot acc: 0.1184
top acc: 0.0715 ::: bot acc: 0.0344
top acc: 0.0226 ::: bot acc: 0.1288
top acc: 0.0205 ::: bot acc: 0.1076
top acc: 0.0191 ::: bot acc: 0.0541
top acc: 0.0346 ::: bot acc: 0.1066
current epoch: 32
train loss is 0.006453
average val loss: 0.007823, accuracy: 0.0871
average test loss: 0.002469, accuracy: 0.0583
case acc: 0.058865942
case acc: 0.045106143
case acc: 0.07542396
case acc: 0.06601659
case acc: 0.032198116
case acc: 0.072154194
top acc: 0.0156 ::: bot acc: 0.1184
top acc: 0.0703 ::: bot acc: 0.0357
top acc: 0.0223 ::: bot acc: 0.1284
top acc: 0.0205 ::: bot acc: 0.1075
top acc: 0.0192 ::: bot acc: 0.0538
top acc: 0.0348 ::: bot acc: 0.1066
current epoch: 33
train loss is 0.006435
average val loss: 0.007791, accuracy: 0.0868
average test loss: 0.002484, accuracy: 0.0585
case acc: 0.059388872
case acc: 0.044512603
case acc: 0.075632006
case acc: 0.06633473
case acc: 0.032360807
case acc: 0.072740406
top acc: 0.0159 ::: bot acc: 0.1189
top acc: 0.0685 ::: bot acc: 0.0373
top acc: 0.0225 ::: bot acc: 0.1286
top acc: 0.0207 ::: bot acc: 0.1078
top acc: 0.0191 ::: bot acc: 0.0541
top acc: 0.0354 ::: bot acc: 0.1072
current epoch: 34
train loss is 0.006405
average val loss: 0.007697, accuracy: 0.0858
average test loss: 0.002535, accuracy: 0.0592
case acc: 0.060795452
case acc: 0.04389553
case acc: 0.07619024
case acc: 0.06722289
case acc: 0.03309559
case acc: 0.07406389
top acc: 0.0172 ::: bot acc: 0.1207
top acc: 0.0665 ::: bot acc: 0.0394
top acc: 0.0227 ::: bot acc: 0.1294
top acc: 0.0211 ::: bot acc: 0.1088
top acc: 0.0189 ::: bot acc: 0.0552
top acc: 0.0365 ::: bot acc: 0.1088
current epoch: 35
train loss is 0.006382
average val loss: 0.007700, accuracy: 0.0858
average test loss: 0.002532, accuracy: 0.0592
case acc: 0.061144356
case acc: 0.043715853
case acc: 0.075937666
case acc: 0.06716174
case acc: 0.033170886
case acc: 0.074208915
top acc: 0.0177 ::: bot acc: 0.1208
top acc: 0.0655 ::: bot acc: 0.0404
top acc: 0.0227 ::: bot acc: 0.1290
top acc: 0.0211 ::: bot acc: 0.1089
top acc: 0.0188 ::: bot acc: 0.0554
top acc: 0.0367 ::: bot acc: 0.1087
current epoch: 36
train loss is 0.006366
average val loss: 0.007755, accuracy: 0.0862
average test loss: 0.002502, accuracy: 0.0588
case acc: 0.060823612
case acc: 0.04364787
case acc: 0.07515773
case acc: 0.06653517
case acc: 0.032968555
case acc: 0.073717296
top acc: 0.0172 ::: bot acc: 0.1204
top acc: 0.0655 ::: bot acc: 0.0404
top acc: 0.0223 ::: bot acc: 0.1281
top acc: 0.0208 ::: bot acc: 0.1082
top acc: 0.0191 ::: bot acc: 0.0550
top acc: 0.0362 ::: bot acc: 0.1082
current epoch: 37
train loss is 0.006351
average val loss: 0.007764, accuracy: 0.0861
average test loss: 0.002503, accuracy: 0.0588
case acc: 0.06100553
case acc: 0.043598834
case acc: 0.07483663
case acc: 0.06663234
case acc: 0.032837525
case acc: 0.073967084
top acc: 0.0174 ::: bot acc: 0.1208
top acc: 0.0649 ::: bot acc: 0.0410
top acc: 0.0221 ::: bot acc: 0.1276
top acc: 0.0209 ::: bot acc: 0.1083
top acc: 0.0191 ::: bot acc: 0.0550
top acc: 0.0365 ::: bot acc: 0.1086
current epoch: 38
train loss is 0.006341
average val loss: 0.007750, accuracy: 0.0859
average test loss: 0.002508, accuracy: 0.0589
case acc: 0.06160711
case acc: 0.04340083
case acc: 0.074713096
case acc: 0.06671095
case acc: 0.032908745
case acc: 0.07422494
top acc: 0.0180 ::: bot acc: 0.1213
top acc: 0.0641 ::: bot acc: 0.0418
top acc: 0.0221 ::: bot acc: 0.1275
top acc: 0.0210 ::: bot acc: 0.1083
top acc: 0.0191 ::: bot acc: 0.0550
top acc: 0.0368 ::: bot acc: 0.1089
current epoch: 39
train loss is 0.006334
average val loss: 0.007774, accuracy: 0.0860
average test loss: 0.002495, accuracy: 0.0587
case acc: 0.0616311
case acc: 0.043158893
case acc: 0.07421656
case acc: 0.06637016
case acc: 0.03292477
case acc: 0.074061766
top acc: 0.0181 ::: bot acc: 0.1214
top acc: 0.0636 ::: bot acc: 0.0421
top acc: 0.0217 ::: bot acc: 0.1269
top acc: 0.0207 ::: bot acc: 0.1080
top acc: 0.0192 ::: bot acc: 0.0550
top acc: 0.0365 ::: bot acc: 0.1086
current epoch: 40
train loss is 0.006306
average val loss: 0.007753, accuracy: 0.0858
average test loss: 0.002514, accuracy: 0.0590
case acc: 0.06233563
case acc: 0.043011684
case acc: 0.0742678
case acc: 0.066849805
case acc: 0.033093482
case acc: 0.07458127
top acc: 0.0187 ::: bot acc: 0.1221
top acc: 0.0626 ::: bot acc: 0.0432
top acc: 0.0217 ::: bot acc: 0.1270
top acc: 0.0212 ::: bot acc: 0.1084
top acc: 0.0190 ::: bot acc: 0.0553
top acc: 0.0371 ::: bot acc: 0.1092
current epoch: 41
train loss is 0.006302
average val loss: 0.007715, accuracy: 0.0854
average test loss: 0.002544, accuracy: 0.0594
case acc: 0.063375846
case acc: 0.04267911
case acc: 0.07460419
case acc: 0.067342974
case acc: 0.03335963
case acc: 0.07528718
top acc: 0.0196 ::: bot acc: 0.1232
top acc: 0.0614 ::: bot acc: 0.0443
top acc: 0.0220 ::: bot acc: 0.1275
top acc: 0.0214 ::: bot acc: 0.1091
top acc: 0.0187 ::: bot acc: 0.0558
top acc: 0.0377 ::: bot acc: 0.1098
current epoch: 42
train loss is 0.006271
average val loss: 0.007700, accuracy: 0.0852
average test loss: 0.002555, accuracy: 0.0596
case acc: 0.06405908
case acc: 0.042486038
case acc: 0.07453254
case acc: 0.06740153
case acc: 0.03351253
case acc: 0.07545211
top acc: 0.0203 ::: bot acc: 0.1239
top acc: 0.0605 ::: bot acc: 0.0453
top acc: 0.0218 ::: bot acc: 0.1275
top acc: 0.0213 ::: bot acc: 0.1092
top acc: 0.0187 ::: bot acc: 0.0560
top acc: 0.0379 ::: bot acc: 0.1101
current epoch: 43
train loss is 0.006269
average val loss: 0.007636, accuracy: 0.0846
average test loss: 0.002598, accuracy: 0.0602
case acc: 0.06528177
case acc: 0.04220178
case acc: 0.0752955
case acc: 0.06817226
case acc: 0.03368544
case acc: 0.07636087
top acc: 0.0216 ::: bot acc: 0.1251
top acc: 0.0591 ::: bot acc: 0.0467
top acc: 0.0223 ::: bot acc: 0.1283
top acc: 0.0218 ::: bot acc: 0.1101
top acc: 0.0184 ::: bot acc: 0.0565
top acc: 0.0387 ::: bot acc: 0.1109
current epoch: 44
train loss is 0.006260
average val loss: 0.007565, accuracy: 0.0840
average test loss: 0.002654, accuracy: 0.0609
case acc: 0.0668605
case acc: 0.041912995
case acc: 0.07611277
case acc: 0.06916852
case acc: 0.0341708
case acc: 0.077286474
top acc: 0.0231 ::: bot acc: 0.1266
top acc: 0.0577 ::: bot acc: 0.0482
top acc: 0.0227 ::: bot acc: 0.1293
top acc: 0.0223 ::: bot acc: 0.1113
top acc: 0.0183 ::: bot acc: 0.0573
top acc: 0.0395 ::: bot acc: 0.1120
current epoch: 45
train loss is 0.006240
average val loss: 0.007478, accuracy: 0.0831
average test loss: 0.002727, accuracy: 0.0619
case acc: 0.068734385
case acc: 0.041611463
case acc: 0.07704381
case acc: 0.07045475
case acc: 0.034835696
case acc: 0.07877024
top acc: 0.0250 ::: bot acc: 0.1285
top acc: 0.0557 ::: bot acc: 0.0501
top acc: 0.0232 ::: bot acc: 0.1304
top acc: 0.0232 ::: bot acc: 0.1127
top acc: 0.0181 ::: bot acc: 0.0584
top acc: 0.0409 ::: bot acc: 0.1135
current epoch: 46
train loss is 0.006224
average val loss: 0.007495, accuracy: 0.0832
average test loss: 0.002731, accuracy: 0.0620
case acc: 0.06912424
case acc: 0.04149938
case acc: 0.07684328
case acc: 0.070484236
case acc: 0.03480815
case acc: 0.078947395
top acc: 0.0252 ::: bot acc: 0.1289
top acc: 0.0553 ::: bot acc: 0.0505
top acc: 0.0232 ::: bot acc: 0.1301
top acc: 0.0232 ::: bot acc: 0.1128
top acc: 0.0183 ::: bot acc: 0.0583
top acc: 0.0410 ::: bot acc: 0.1139
current epoch: 47
train loss is 0.006211
average val loss: 0.007452, accuracy: 0.0828
average test loss: 0.002771, accuracy: 0.0625
case acc: 0.07013854
case acc: 0.041422024
case acc: 0.07715097
case acc: 0.071052805
case acc: 0.035365198
case acc: 0.07978274
top acc: 0.0263 ::: bot acc: 0.1300
top acc: 0.0545 ::: bot acc: 0.0514
top acc: 0.0233 ::: bot acc: 0.1307
top acc: 0.0236 ::: bot acc: 0.1135
top acc: 0.0178 ::: bot acc: 0.0594
top acc: 0.0417 ::: bot acc: 0.1146
current epoch: 48
train loss is 0.006208
average val loss: 0.007461, accuracy: 0.0828
average test loss: 0.002776, accuracy: 0.0626
case acc: 0.07056976
case acc: 0.041412648
case acc: 0.076791726
case acc: 0.07121169
case acc: 0.035521563
case acc: 0.079933345
top acc: 0.0266 ::: bot acc: 0.1305
top acc: 0.0543 ::: bot acc: 0.0515
top acc: 0.0230 ::: bot acc: 0.1302
top acc: 0.0237 ::: bot acc: 0.1137
top acc: 0.0177 ::: bot acc: 0.0596
top acc: 0.0419 ::: bot acc: 0.1148
current epoch: 49
train loss is 0.006198
average val loss: 0.007599, accuracy: 0.0839
average test loss: 0.002686, accuracy: 0.0614
case acc: 0.06902798
case acc: 0.041650467
case acc: 0.075119495
case acc: 0.069667414
case acc: 0.034534544
case acc: 0.078457974
top acc: 0.0251 ::: bot acc: 0.1289
top acc: 0.0560 ::: bot acc: 0.0498
top acc: 0.0223 ::: bot acc: 0.1280
top acc: 0.0227 ::: bot acc: 0.1119
top acc: 0.0182 ::: bot acc: 0.0579
top acc: 0.0406 ::: bot acc: 0.1132
current epoch: 50
train loss is 0.006196
average val loss: 0.007588, accuracy: 0.0837
average test loss: 0.002705, accuracy: 0.0617
case acc: 0.069687024
case acc: 0.04163107
case acc: 0.07501511
case acc: 0.06994913
case acc: 0.034836445
case acc: 0.07891198
top acc: 0.0259 ::: bot acc: 0.1296
top acc: 0.0556 ::: bot acc: 0.0503
top acc: 0.0222 ::: bot acc: 0.1280
top acc: 0.0227 ::: bot acc: 0.1123
top acc: 0.0183 ::: bot acc: 0.0583
top acc: 0.0410 ::: bot acc: 0.1137
LME_Co_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5136 5136 5136
1.7082474 -0.6288155 0.48738334 -0.33910522
Validation: 576 576 576
Testing: 744 744 744
pre-processing time: 0.00018596649169921875
the split date is 2011-01-01
train dropout: 0.4 test dropout: 0.05
net initializing with time: 0.002201080322265625
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012616
average val loss: 0.016262, accuracy: 0.1652
average test loss: 0.020775, accuracy: 0.1736
case acc: 0.21172556
case acc: 0.043096438
case acc: 0.26271725
case acc: 0.20755571
case acc: 0.19801995
case acc: 0.11858079
top acc: 0.0973 ::: bot acc: 0.3210
top acc: 0.0561 ::: bot acc: 0.0598
top acc: 0.1837 ::: bot acc: 0.3473
top acc: 0.1112 ::: bot acc: 0.2835
top acc: 0.0716 ::: bot acc: 0.3287
top acc: 0.0281 ::: bot acc: 0.2186
current epoch: 2
train loss is 0.012346
average val loss: 0.007018, accuracy: 0.1042
average test loss: 0.009107, accuracy: 0.1147
case acc: 0.11741608
case acc: 0.09741093
case acc: 0.15381205
case acc: 0.11068509
case acc: 0.11664602
case acc: 0.09238372
top acc: 0.0356 ::: bot acc: 0.2105
top acc: 0.1518 ::: bot acc: 0.0390
top acc: 0.0746 ::: bot acc: 0.2378
top acc: 0.0223 ::: bot acc: 0.1830
top acc: 0.0300 ::: bot acc: 0.2275
top acc: 0.1117 ::: bot acc: 0.1265
current epoch: 3
train loss is 0.011812
average val loss: 0.008588, accuracy: 0.0871
average test loss: 0.007964, accuracy: 0.1041
case acc: 0.082311966
case acc: 0.20125778
case acc: 0.06584767
case acc: 0.067891866
case acc: 0.09976287
case acc: 0.10743188
top acc: 0.1319 ::: bot acc: 0.0935
top acc: 0.2563 ::: bot acc: 0.1419
top acc: 0.0412 ::: bot acc: 0.1227
top acc: 0.0972 ::: bot acc: 0.0762
top acc: 0.1400 ::: bot acc: 0.1176
top acc: 0.2101 ::: bot acc: 0.0298
current epoch: 4
train loss is 0.012907
average val loss: 0.008267, accuracy: 0.0867
average test loss: 0.007620, accuracy: 0.1024
case acc: 0.08284719
case acc: 0.19309066
case acc: 0.064914994
case acc: 0.0684491
case acc: 0.100142814
case acc: 0.10476658
top acc: 0.1382 ::: bot acc: 0.0864
top acc: 0.2474 ::: bot acc: 0.1348
top acc: 0.0444 ::: bot acc: 0.1194
top acc: 0.0962 ::: bot acc: 0.0778
top acc: 0.1453 ::: bot acc: 0.1119
top acc: 0.2041 ::: bot acc: 0.0347
current epoch: 5
train loss is 0.012132
average val loss: 0.005517, accuracy: 0.0874
average test loss: 0.007099, accuracy: 0.1023
case acc: 0.09649423
case acc: 0.094521396
case acc: 0.12374609
case acc: 0.10043725
case acc: 0.10646394
case acc: 0.09210272
top acc: 0.0551 ::: bot acc: 0.1697
top acc: 0.1487 ::: bot acc: 0.0368
top acc: 0.0441 ::: bot acc: 0.2083
top acc: 0.0217 ::: bot acc: 0.1684
top acc: 0.0664 ::: bot acc: 0.1905
top acc: 0.1121 ::: bot acc: 0.1250
current epoch: 6
train loss is 0.008382
average val loss: 0.005703, accuracy: 0.0932
average test loss: 0.007747, accuracy: 0.1056
case acc: 0.10261484
case acc: 0.071747854
case acc: 0.14317568
case acc: 0.113194175
case acc: 0.108289704
case acc: 0.094748385
top acc: 0.0458 ::: bot acc: 0.1835
top acc: 0.1202 ::: bot acc: 0.0255
top acc: 0.0637 ::: bot acc: 0.2275
top acc: 0.0226 ::: bot acc: 0.1868
top acc: 0.0594 ::: bot acc: 0.1976
top acc: 0.0930 ::: bot acc: 0.1440
current epoch: 7
train loss is 0.007115
average val loss: 0.005009, accuracy: 0.0833
average test loss: 0.006719, accuracy: 0.0995
case acc: 0.09386318
case acc: 0.07878911
case acc: 0.12724644
case acc: 0.10145258
case acc: 0.10297253
case acc: 0.09252124
top acc: 0.0636 ::: bot acc: 0.1618
top acc: 0.1293 ::: bot acc: 0.0276
top acc: 0.0476 ::: bot acc: 0.2119
top acc: 0.0215 ::: bot acc: 0.1696
top acc: 0.0852 ::: bot acc: 0.1728
top acc: 0.1099 ::: bot acc: 0.1278
current epoch: 8
train loss is 0.007063
average val loss: 0.004705, accuracy: 0.0761
average test loss: 0.005963, accuracy: 0.0943
case acc: 0.08746168
case acc: 0.08815524
case acc: 0.10873853
case acc: 0.09145855
case acc: 0.099518836
case acc: 0.09065986
top acc: 0.0868 ::: bot acc: 0.1385
top acc: 0.1407 ::: bot acc: 0.0327
top acc: 0.0306 ::: bot acc: 0.1925
top acc: 0.0280 ::: bot acc: 0.1515
top acc: 0.1092 ::: bot acc: 0.1483
top acc: 0.1263 ::: bot acc: 0.1111
current epoch: 9
train loss is 0.007028
average val loss: 0.004550, accuracy: 0.0749
average test loss: 0.005818, accuracy: 0.0931
case acc: 0.086570844
case acc: 0.08285186
case acc: 0.10754487
case acc: 0.09126757
case acc: 0.099279426
case acc: 0.0908433
top acc: 0.0912 ::: bot acc: 0.1342
top acc: 0.1345 ::: bot acc: 0.0298
top acc: 0.0294 ::: bot acc: 0.1912
top acc: 0.0281 ::: bot acc: 0.1514
top acc: 0.1145 ::: bot acc: 0.1434
top acc: 0.1249 ::: bot acc: 0.1128
current epoch: 10
train loss is 0.006815
average val loss: 0.004424, accuracy: 0.0764
average test loss: 0.006015, accuracy: 0.0940
case acc: 0.08800365
case acc: 0.068995506
case acc: 0.11809973
case acc: 0.0971653
case acc: 0.099676356
case acc: 0.0921915
top acc: 0.0833 ::: bot acc: 0.1416
top acc: 0.1159 ::: bot acc: 0.0248
top acc: 0.0385 ::: bot acc: 0.2024
top acc: 0.0234 ::: bot acc: 0.1627
top acc: 0.1074 ::: bot acc: 0.1502
top acc: 0.1127 ::: bot acc: 0.1247
current epoch: 11
train loss is 0.006407
average val loss: 0.004285, accuracy: 0.0752
average test loss: 0.005912, accuracy: 0.0931
case acc: 0.08740383
case acc: 0.06490153
case acc: 0.11748515
case acc: 0.09690163
case acc: 0.099359535
case acc: 0.09238114
top acc: 0.0873 ::: bot acc: 0.1381
top acc: 0.1099 ::: bot acc: 0.0246
top acc: 0.0383 ::: bot acc: 0.2017
top acc: 0.0237 ::: bot acc: 0.1620
top acc: 0.1111 ::: bot acc: 0.1466
top acc: 0.1121 ::: bot acc: 0.1257
current epoch: 12
train loss is 0.006226
average val loss: 0.004134, accuracy: 0.0729
average test loss: 0.005681, accuracy: 0.0913
case acc: 0.0857335
case acc: 0.06445532
case acc: 0.11224554
case acc: 0.094227545
case acc: 0.09912478
case acc: 0.091804154
top acc: 0.0946 ::: bot acc: 0.1302
top acc: 0.1095 ::: bot acc: 0.0245
top acc: 0.0333 ::: bot acc: 0.1964
top acc: 0.0256 ::: bot acc: 0.1571
top acc: 0.1181 ::: bot acc: 0.1394
top acc: 0.1156 ::: bot acc: 0.1216
current epoch: 13
train loss is 0.006171
average val loss: 0.004022, accuracy: 0.0718
average test loss: 0.005608, accuracy: 0.0905
case acc: 0.085155785
case acc: 0.060864724
case acc: 0.11167223
case acc: 0.094087735
case acc: 0.09916621
case acc: 0.09195784
top acc: 0.0976 ::: bot acc: 0.1274
top acc: 0.1038 ::: bot acc: 0.0244
top acc: 0.0331 ::: bot acc: 0.1955
top acc: 0.0257 ::: bot acc: 0.1566
top acc: 0.1193 ::: bot acc: 0.1385
top acc: 0.1142 ::: bot acc: 0.1231
current epoch: 14
train loss is 0.006030
average val loss: 0.003917, accuracy: 0.0711
average test loss: 0.005556, accuracy: 0.0899
case acc: 0.08493721
case acc: 0.05768246
case acc: 0.111030065
case acc: 0.094251715
case acc: 0.09920281
case acc: 0.092112005
top acc: 0.0999 ::: bot acc: 0.1254
top acc: 0.0985 ::: bot acc: 0.0253
top acc: 0.0326 ::: bot acc: 0.1949
top acc: 0.0258 ::: bot acc: 0.1570
top acc: 0.1197 ::: bot acc: 0.1382
top acc: 0.1123 ::: bot acc: 0.1248
current epoch: 15
train loss is 0.005944
average val loss: 0.003807, accuracy: 0.0693
average test loss: 0.005387, accuracy: 0.0885
case acc: 0.084195554
case acc: 0.057470266
case acc: 0.106230006
case acc: 0.091922775
case acc: 0.09919453
case acc: 0.09194901
top acc: 0.1060 ::: bot acc: 0.1193
top acc: 0.0984 ::: bot acc: 0.0255
top acc: 0.0286 ::: bot acc: 0.1897
top acc: 0.0277 ::: bot acc: 0.1524
top acc: 0.1233 ::: bot acc: 0.1346
top acc: 0.1151 ::: bot acc: 0.1224
current epoch: 16
train loss is 0.005934
average val loss: 0.003726, accuracy: 0.0677
average test loss: 0.005193, accuracy: 0.0869
case acc: 0.08301746
case acc: 0.058447585
case acc: 0.10009459
case acc: 0.08891533
case acc: 0.099205635
case acc: 0.09156296
top acc: 0.1135 ::: bot acc: 0.1111
top acc: 0.1005 ::: bot acc: 0.0246
top acc: 0.0243 ::: bot acc: 0.1827
top acc: 0.0310 ::: bot acc: 0.1462
top acc: 0.1283 ::: bot acc: 0.1294
top acc: 0.1194 ::: bot acc: 0.1183
current epoch: 17
train loss is 0.005937
average val loss: 0.003645, accuracy: 0.0669
average test loss: 0.005115, accuracy: 0.0861
case acc: 0.08249505
case acc: 0.05697526
case acc: 0.09803955
case acc: 0.08820733
case acc: 0.09923528
case acc: 0.09140478
top acc: 0.1165 ::: bot acc: 0.1082
top acc: 0.0977 ::: bot acc: 0.0254
top acc: 0.0230 ::: bot acc: 0.1803
top acc: 0.0319 ::: bot acc: 0.1450
top acc: 0.1291 ::: bot acc: 0.1284
top acc: 0.1192 ::: bot acc: 0.1181
current epoch: 18
train loss is 0.005860
average val loss: 0.003551, accuracy: 0.0663
average test loss: 0.005114, accuracy: 0.0859
case acc: 0.08262331
case acc: 0.05376736
case acc: 0.098973036
case acc: 0.089164235
case acc: 0.09910138
case acc: 0.09168089
top acc: 0.1160 ::: bot acc: 0.1086
top acc: 0.0917 ::: bot acc: 0.0276
top acc: 0.0235 ::: bot acc: 0.1815
top acc: 0.0312 ::: bot acc: 0.1466
top acc: 0.1265 ::: bot acc: 0.1308
top acc: 0.1159 ::: bot acc: 0.1214
current epoch: 19
train loss is 0.005769
average val loss: 0.003490, accuracy: 0.0654
average test loss: 0.005014, accuracy: 0.0850
case acc: 0.08220812
case acc: 0.05357169
case acc: 0.095816694
case acc: 0.08744361
case acc: 0.099231265
case acc: 0.09150189
top acc: 0.1203 ::: bot acc: 0.1046
top acc: 0.0915 ::: bot acc: 0.0275
top acc: 0.0218 ::: bot acc: 0.1776
top acc: 0.0331 ::: bot acc: 0.1430
top acc: 0.1290 ::: bot acc: 0.1284
top acc: 0.1179 ::: bot acc: 0.1194
current epoch: 20
train loss is 0.005733
average val loss: 0.003424, accuracy: 0.0648
average test loss: 0.004968, accuracy: 0.0844
case acc: 0.08226358
case acc: 0.052274067
case acc: 0.09453476
case acc: 0.08671361
case acc: 0.09921428
case acc: 0.091620535
top acc: 0.1222 ::: bot acc: 0.1031
top acc: 0.0891 ::: bot acc: 0.0285
top acc: 0.0209 ::: bot acc: 0.1761
top acc: 0.0339 ::: bot acc: 0.1414
top acc: 0.1295 ::: bot acc: 0.1282
top acc: 0.1180 ::: bot acc: 0.1194
current epoch: 21
train loss is 0.005687
average val loss: 0.003354, accuracy: 0.0642
average test loss: 0.004942, accuracy: 0.0841
case acc: 0.082159385
case acc: 0.05060004
case acc: 0.0941986
case acc: 0.08680979
case acc: 0.09921616
case acc: 0.09161352
top acc: 0.1232 ::: bot acc: 0.1020
top acc: 0.0855 ::: bot acc: 0.0301
top acc: 0.0207 ::: bot acc: 0.1756
top acc: 0.0343 ::: bot acc: 0.1415
top acc: 0.1284 ::: bot acc: 0.1292
top acc: 0.1166 ::: bot acc: 0.1204
current epoch: 22
train loss is 0.005633
average val loss: 0.003300, accuracy: 0.0636
average test loss: 0.004895, accuracy: 0.0836
case acc: 0.081994526
case acc: 0.049572207
case acc: 0.09296844
case acc: 0.086216286
case acc: 0.09910544
case acc: 0.09167582
top acc: 0.1245 ::: bot acc: 0.1005
top acc: 0.0835 ::: bot acc: 0.0313
top acc: 0.0204 ::: bot acc: 0.1737
top acc: 0.0349 ::: bot acc: 0.1403
top acc: 0.1287 ::: bot acc: 0.1288
top acc: 0.1170 ::: bot acc: 0.1203
current epoch: 23
train loss is 0.005596
average val loss: 0.003234, accuracy: 0.0632
average test loss: 0.004939, accuracy: 0.0838
case acc: 0.082126535
case acc: 0.04739553
case acc: 0.09486188
case acc: 0.08734692
case acc: 0.09921134
case acc: 0.091960154
top acc: 0.1226 ::: bot acc: 0.1023
top acc: 0.0778 ::: bot acc: 0.0359
top acc: 0.0210 ::: bot acc: 0.1764
top acc: 0.0331 ::: bot acc: 0.1429
top acc: 0.1257 ::: bot acc: 0.1322
top acc: 0.1135 ::: bot acc: 0.1237
current epoch: 24
train loss is 0.005541
average val loss: 0.003196, accuracy: 0.0628
average test loss: 0.004898, accuracy: 0.0834
case acc: 0.08196476
case acc: 0.046627246
case acc: 0.09390488
case acc: 0.08694131
case acc: 0.09908969
case acc: 0.09203118
top acc: 0.1238 ::: bot acc: 0.1012
top acc: 0.0762 ::: bot acc: 0.0373
top acc: 0.0207 ::: bot acc: 0.1750
top acc: 0.0342 ::: bot acc: 0.1420
top acc: 0.1256 ::: bot acc: 0.1317
top acc: 0.1131 ::: bot acc: 0.1241
current epoch: 25
train loss is 0.005554
average val loss: 0.003160, accuracy: 0.0622
average test loss: 0.004811, accuracy: 0.0827
case acc: 0.08197814
case acc: 0.04725064
case acc: 0.090819046
case acc: 0.08554679
case acc: 0.099085525
case acc: 0.09172981
top acc: 0.1278 ::: bot acc: 0.0971
top acc: 0.0777 ::: bot acc: 0.0360
top acc: 0.0197 ::: bot acc: 0.1710
top acc: 0.0371 ::: bot acc: 0.1384
top acc: 0.1284 ::: bot acc: 0.1291
top acc: 0.1155 ::: bot acc: 0.1217
current epoch: 26
train loss is 0.005572
average val loss: 0.003137, accuracy: 0.0619
average test loss: 0.004768, accuracy: 0.0823
case acc: 0.08194051
case acc: 0.047180288
case acc: 0.08914043
case acc: 0.085037306
case acc: 0.098989375
case acc: 0.091779724
top acc: 0.1294 ::: bot acc: 0.0955
top acc: 0.0773 ::: bot acc: 0.0364
top acc: 0.0191 ::: bot acc: 0.1687
top acc: 0.0382 ::: bot acc: 0.1371
top acc: 0.1287 ::: bot acc: 0.1285
top acc: 0.1158 ::: bot acc: 0.1214
current epoch: 27
train loss is 0.005546
average val loss: 0.003134, accuracy: 0.0618
average test loss: 0.004701, accuracy: 0.0818
case acc: 0.08205788
case acc: 0.04773416
case acc: 0.086244926
case acc: 0.083814844
case acc: 0.09937567
case acc: 0.091549724
top acc: 0.1328 ::: bot acc: 0.0918
top acc: 0.0788 ::: bot acc: 0.0352
top acc: 0.0181 ::: bot acc: 0.1649
top acc: 0.0411 ::: bot acc: 0.1339
top acc: 0.1314 ::: bot acc: 0.1263
top acc: 0.1174 ::: bot acc: 0.1197
current epoch: 28
train loss is 0.005560
average val loss: 0.003098, accuracy: 0.0614
average test loss: 0.004685, accuracy: 0.0816
case acc: 0.08235698
case acc: 0.047311123
case acc: 0.08539388
case acc: 0.08364696
case acc: 0.09936475
case acc: 0.09172887
top acc: 0.1338 ::: bot acc: 0.0913
top acc: 0.0777 ::: bot acc: 0.0362
top acc: 0.0178 ::: bot acc: 0.1640
top acc: 0.0419 ::: bot acc: 0.1332
top acc: 0.1318 ::: bot acc: 0.1259
top acc: 0.1176 ::: bot acc: 0.1199
current epoch: 29
train loss is 0.005546
average val loss: 0.003063, accuracy: 0.0611
average test loss: 0.004686, accuracy: 0.0815
case acc: 0.08211323
case acc: 0.046158284
case acc: 0.08636221
case acc: 0.08378673
case acc: 0.09905886
case acc: 0.09174297
top acc: 0.1322 ::: bot acc: 0.0927
top acc: 0.0743 ::: bot acc: 0.0394
top acc: 0.0181 ::: bot acc: 0.1651
top acc: 0.0405 ::: bot acc: 0.1342
top acc: 0.1302 ::: bot acc: 0.1271
top acc: 0.1156 ::: bot acc: 0.1215
current epoch: 30
train loss is 0.005442
average val loss: 0.003011, accuracy: 0.0609
average test loss: 0.004784, accuracy: 0.0823
case acc: 0.08212749
case acc: 0.044194106
case acc: 0.089934655
case acc: 0.08568205
case acc: 0.09916151
case acc: 0.09240262
top acc: 0.1277 ::: bot acc: 0.0979
top acc: 0.0675 ::: bot acc: 0.0459
top acc: 0.0192 ::: bot acc: 0.1698
top acc: 0.0365 ::: bot acc: 0.1388
top acc: 0.1259 ::: bot acc: 0.1320
top acc: 0.1104 ::: bot acc: 0.1269
current epoch: 31
train loss is 0.005375
average val loss: 0.002990, accuracy: 0.0610
average test loss: 0.004903, accuracy: 0.0831
case acc: 0.082134135
case acc: 0.042821262
case acc: 0.09385352
case acc: 0.08783096
case acc: 0.09906672
case acc: 0.092959195
top acc: 0.1226 ::: bot acc: 0.1026
top acc: 0.0609 ::: bot acc: 0.0525
top acc: 0.0209 ::: bot acc: 0.1749
top acc: 0.0325 ::: bot acc: 0.1440
top acc: 0.1208 ::: bot acc: 0.1367
top acc: 0.1045 ::: bot acc: 0.1325
current epoch: 32
train loss is 0.005354
average val loss: 0.002972, accuracy: 0.0604
average test loss: 0.004765, accuracy: 0.0820
case acc: 0.08195038
case acc: 0.043684814
case acc: 0.08938457
case acc: 0.08568527
case acc: 0.09905735
case acc: 0.092526875
top acc: 0.1277 ::: bot acc: 0.0974
top acc: 0.0652 ::: bot acc: 0.0483
top acc: 0.0190 ::: bot acc: 0.1691
top acc: 0.0369 ::: bot acc: 0.1387
top acc: 0.1255 ::: bot acc: 0.1320
top acc: 0.1090 ::: bot acc: 0.1283
current epoch: 33
train loss is 0.005408
average val loss: 0.002957, accuracy: 0.0601
average test loss: 0.004709, accuracy: 0.0816
case acc: 0.08195218
case acc: 0.043751094
case acc: 0.08762532
case acc: 0.08480817
case acc: 0.09901143
case acc: 0.09239051
top acc: 0.1297 ::: bot acc: 0.0953
top acc: 0.0663 ::: bot acc: 0.0469
top acc: 0.0184 ::: bot acc: 0.1669
top acc: 0.0391 ::: bot acc: 0.1362
top acc: 0.1269 ::: bot acc: 0.1305
top acc: 0.1108 ::: bot acc: 0.1266
current epoch: 34
train loss is 0.005402
average val loss: 0.002938, accuracy: 0.0599
average test loss: 0.004691, accuracy: 0.0814
case acc: 0.08204439
case acc: 0.04367986
case acc: 0.0869598
case acc: 0.08437303
case acc: 0.09916719
case acc: 0.09240415
top acc: 0.1303 ::: bot acc: 0.0947
top acc: 0.0659 ::: bot acc: 0.0474
top acc: 0.0182 ::: bot acc: 0.1660
top acc: 0.0398 ::: bot acc: 0.1353
top acc: 0.1275 ::: bot acc: 0.1302
top acc: 0.1112 ::: bot acc: 0.1263
current epoch: 35
train loss is 0.005366
average val loss: 0.002917, accuracy: 0.0597
average test loss: 0.004710, accuracy: 0.0816
case acc: 0.08213229
case acc: 0.043304652
case acc: 0.0876692
case acc: 0.084647454
case acc: 0.09926957
case acc: 0.09244236
top acc: 0.1291 ::: bot acc: 0.0962
top acc: 0.0640 ::: bot acc: 0.0494
top acc: 0.0185 ::: bot acc: 0.1669
top acc: 0.0393 ::: bot acc: 0.1359
top acc: 0.1270 ::: bot acc: 0.1309
top acc: 0.1102 ::: bot acc: 0.1273
current epoch: 36
train loss is 0.005332
average val loss: 0.002914, accuracy: 0.0595
average test loss: 0.004654, accuracy: 0.0811
case acc: 0.08223455
case acc: 0.043498352
case acc: 0.08584902
case acc: 0.083607465
case acc: 0.09920015
case acc: 0.09222468
top acc: 0.1315 ::: bot acc: 0.0939
top acc: 0.0651 ::: bot acc: 0.0482
top acc: 0.0179 ::: bot acc: 0.1645
top acc: 0.0418 ::: bot acc: 0.1332
top acc: 0.1297 ::: bot acc: 0.1279
top acc: 0.1125 ::: bot acc: 0.1250
current epoch: 37
train loss is 0.005322
average val loss: 0.002887, accuracy: 0.0592
average test loss: 0.004674, accuracy: 0.0813
case acc: 0.08210772
case acc: 0.04313859
case acc: 0.08697289
case acc: 0.08402187
case acc: 0.099125
case acc: 0.092254974
top acc: 0.1295 ::: bot acc: 0.0956
top acc: 0.0626 ::: bot acc: 0.0508
top acc: 0.0182 ::: bot acc: 0.1660
top acc: 0.0407 ::: bot acc: 0.1342
top acc: 0.1279 ::: bot acc: 0.1297
top acc: 0.1108 ::: bot acc: 0.1264
current epoch: 38
train loss is 0.005263
average val loss: 0.002866, accuracy: 0.0592
average test loss: 0.004763, accuracy: 0.0820
case acc: 0.082050905
case acc: 0.04248645
case acc: 0.09012764
case acc: 0.08531838
case acc: 0.099044494
case acc: 0.0927724
top acc: 0.1252 ::: bot acc: 0.1001
top acc: 0.0578 ::: bot acc: 0.0559
top acc: 0.0193 ::: bot acc: 0.1702
top acc: 0.0374 ::: bot acc: 0.1379
top acc: 0.1236 ::: bot acc: 0.1338
top acc: 0.1071 ::: bot acc: 0.1302
current epoch: 39
train loss is 0.005223
average val loss: 0.002853, accuracy: 0.0591
average test loss: 0.004800, accuracy: 0.0823
case acc: 0.08209058
case acc: 0.042206768
case acc: 0.09136453
case acc: 0.08580348
case acc: 0.099041164
case acc: 0.093005165
top acc: 0.1234 ::: bot acc: 0.1018
top acc: 0.0555 ::: bot acc: 0.0581
top acc: 0.0199 ::: bot acc: 0.1718
top acc: 0.0365 ::: bot acc: 0.1389
top acc: 0.1217 ::: bot acc: 0.1357
top acc: 0.1052 ::: bot acc: 0.1322
current epoch: 40
train loss is 0.005201
average val loss: 0.002843, accuracy: 0.0588
average test loss: 0.004707, accuracy: 0.0815
case acc: 0.081960365
case acc: 0.042680375
case acc: 0.088536575
case acc: 0.08442903
case acc: 0.099005595
case acc: 0.092661455
top acc: 0.1266 ::: bot acc: 0.0985
top acc: 0.0590 ::: bot acc: 0.0548
top acc: 0.0189 ::: bot acc: 0.1680
top acc: 0.0396 ::: bot acc: 0.1355
top acc: 0.1243 ::: bot acc: 0.1328
top acc: 0.1081 ::: bot acc: 0.1293
current epoch: 41
train loss is 0.005227
average val loss: 0.002843, accuracy: 0.0586
average test loss: 0.004633, accuracy: 0.0809
case acc: 0.08204746
case acc: 0.04292243
case acc: 0.0858611
case acc: 0.08325008
case acc: 0.099086896
case acc: 0.092399724
top acc: 0.1295 ::: bot acc: 0.0956
top acc: 0.0617 ::: bot acc: 0.0515
top acc: 0.0181 ::: bot acc: 0.1645
top acc: 0.0430 ::: bot acc: 0.1320
top acc: 0.1272 ::: bot acc: 0.1303
top acc: 0.1110 ::: bot acc: 0.1264
current epoch: 42
train loss is 0.005275
average val loss: 0.002825, accuracy: 0.0585
average test loss: 0.004660, accuracy: 0.0812
case acc: 0.081937306
case acc: 0.042666096
case acc: 0.08699285
case acc: 0.08361879
case acc: 0.09910173
case acc: 0.092593685
top acc: 0.1276 ::: bot acc: 0.0975
top acc: 0.0602 ::: bot acc: 0.0533
top acc: 0.0183 ::: bot acc: 0.1659
top acc: 0.0419 ::: bot acc: 0.1332
top acc: 0.1257 ::: bot acc: 0.1318
top acc: 0.1094 ::: bot acc: 0.1281
current epoch: 43
train loss is 0.005254
average val loss: 0.002817, accuracy: 0.0585
average test loss: 0.004726, accuracy: 0.0817
case acc: 0.081923336
case acc: 0.0422793
case acc: 0.08938736
case acc: 0.08451224
case acc: 0.09912364
case acc: 0.09290517
top acc: 0.1243 ::: bot acc: 0.1006
top acc: 0.0562 ::: bot acc: 0.0574
top acc: 0.0193 ::: bot acc: 0.1691
top acc: 0.0392 ::: bot acc: 0.1357
top acc: 0.1232 ::: bot acc: 0.1344
top acc: 0.1063 ::: bot acc: 0.1311
current epoch: 44
train loss is 0.005205
average val loss: 0.002809, accuracy: 0.0585
average test loss: 0.004758, accuracy: 0.0819
case acc: 0.082129605
case acc: 0.042073734
case acc: 0.09044362
case acc: 0.08484154
case acc: 0.099102125
case acc: 0.092970066
top acc: 0.1232 ::: bot acc: 0.1022
top acc: 0.0545 ::: bot acc: 0.0592
top acc: 0.0196 ::: bot acc: 0.1705
top acc: 0.0386 ::: bot acc: 0.1366
top acc: 0.1225 ::: bot acc: 0.1351
top acc: 0.1051 ::: bot acc: 0.1321
current epoch: 45
train loss is 0.005168
average val loss: 0.002805, accuracy: 0.0585
average test loss: 0.004785, accuracy: 0.0822
case acc: 0.082207136
case acc: 0.042017873
case acc: 0.09128296
case acc: 0.0851724
case acc: 0.09908424
case acc: 0.09320997
top acc: 0.1217 ::: bot acc: 0.1036
top acc: 0.0531 ::: bot acc: 0.0607
top acc: 0.0198 ::: bot acc: 0.1716
top acc: 0.0379 ::: bot acc: 0.1375
top acc: 0.1213 ::: bot acc: 0.1362
top acc: 0.1040 ::: bot acc: 0.1332
current epoch: 46
train loss is 0.005143
average val loss: 0.002802, accuracy: 0.0585
average test loss: 0.004781, accuracy: 0.0821
case acc: 0.082197905
case acc: 0.041926604
case acc: 0.09124586
case acc: 0.08510785
case acc: 0.09901585
case acc: 0.09320418
top acc: 0.1213 ::: bot acc: 0.1040
top acc: 0.0528 ::: bot acc: 0.0608
top acc: 0.0198 ::: bot acc: 0.1716
top acc: 0.0380 ::: bot acc: 0.1371
top acc: 0.1207 ::: bot acc: 0.1367
top acc: 0.1035 ::: bot acc: 0.1337
current epoch: 47
train loss is 0.005163
average val loss: 0.002794, accuracy: 0.0581
average test loss: 0.004664, accuracy: 0.0812
case acc: 0.08197983
case acc: 0.042338043
case acc: 0.08761312
case acc: 0.083342105
case acc: 0.09909939
case acc: 0.09270215
top acc: 0.1254 ::: bot acc: 0.1000
top acc: 0.0574 ::: bot acc: 0.0562
top acc: 0.0184 ::: bot acc: 0.1668
top acc: 0.0422 ::: bot acc: 0.1326
top acc: 0.1248 ::: bot acc: 0.1328
top acc: 0.1075 ::: bot acc: 0.1297
current epoch: 48
train loss is 0.005175
average val loss: 0.002791, accuracy: 0.0579
average test loss: 0.004620, accuracy: 0.0809
case acc: 0.08203257
case acc: 0.042662706
case acc: 0.08626825
case acc: 0.082602546
case acc: 0.099064104
case acc: 0.09251086
top acc: 0.1269 ::: bot acc: 0.0985
top acc: 0.0593 ::: bot acc: 0.0544
top acc: 0.0180 ::: bot acc: 0.1650
top acc: 0.0443 ::: bot acc: 0.1304
top acc: 0.1261 ::: bot acc: 0.1313
top acc: 0.1089 ::: bot acc: 0.1282
current epoch: 49
train loss is 0.005189
average val loss: 0.002784, accuracy: 0.0578
average test loss: 0.004620, accuracy: 0.0808
case acc: 0.08194389
case acc: 0.042450678
case acc: 0.0863596
case acc: 0.082518786
case acc: 0.0991468
case acc: 0.09255227
top acc: 0.1265 ::: bot acc: 0.0988
top acc: 0.0588 ::: bot acc: 0.0546
top acc: 0.0182 ::: bot acc: 0.1652
top acc: 0.0446 ::: bot acc: 0.1300
top acc: 0.1263 ::: bot acc: 0.1313
top acc: 0.1089 ::: bot acc: 0.1284
current epoch: 50
train loss is 0.005178
average val loss: 0.002779, accuracy: 0.0579
average test loss: 0.004676, accuracy: 0.0813
case acc: 0.08192528
case acc: 0.04223435
case acc: 0.08848953
case acc: 0.08335965
case acc: 0.09901769
case acc: 0.09285626
top acc: 0.1237 ::: bot acc: 0.1013
top acc: 0.0558 ::: bot acc: 0.0578
top acc: 0.0187 ::: bot acc: 0.1680
top acc: 0.0427 ::: bot acc: 0.1323
top acc: 0.1247 ::: bot acc: 0.1329
top acc: 0.1065 ::: bot acc: 0.1308
LME_Co_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5112 5112 5112
1.7082474 -0.6288155 0.48738334 -0.33910522
Validation: 570 570 570
Testing: 774 774 774
pre-processing time: 0.0001952648162841797
the split date is 2011-07-01
train dropout: 0.4 test dropout: 0.05
net initializing with time: 0.0022459030151367188
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.013107
average val loss: 0.006819, accuracy: 0.1044
average test loss: 0.007946, accuracy: 0.1094
case acc: 0.111543685
case acc: 0.15936913
case acc: 0.106806524
case acc: 0.07821264
case acc: 0.13481715
case acc: 0.06536565
top acc: 0.0409 ::: bot acc: 0.1838
top acc: 0.2244 ::: bot acc: 0.1048
top acc: 0.0408 ::: bot acc: 0.1706
top acc: 0.1174 ::: bot acc: 0.0879
top acc: 0.1009 ::: bot acc: 0.1820
top acc: 0.1269 ::: bot acc: 0.0260
current epoch: 2
train loss is 0.011533
average val loss: 0.008858, accuracy: 0.1026
average test loss: 0.010886, accuracy: 0.1191
case acc: 0.06107274
case acc: 0.24550296
case acc: 0.05238063
case acc: 0.11757202
case acc: 0.10199406
case acc: 0.13589196
top acc: 0.0786 ::: bot acc: 0.0841
top acc: 0.3104 ::: bot acc: 0.1906
top acc: 0.0690 ::: bot acc: 0.0718
top acc: 0.2112 ::: bot acc: 0.0403
top acc: 0.1843 ::: bot acc: 0.0894
top acc: 0.2151 ::: bot acc: 0.0619
current epoch: 3
train loss is 0.013409
average val loss: 0.007201, accuracy: 0.0961
average test loss: 0.008948, accuracy: 0.1092
case acc: 0.06330985
case acc: 0.21613468
case acc: 0.056270614
case acc: 0.10210667
case acc: 0.10689813
case acc: 0.11070544
top acc: 0.0628 ::: bot acc: 0.0998
top acc: 0.2812 ::: bot acc: 0.1609
top acc: 0.0511 ::: bot acc: 0.0894
top acc: 0.1906 ::: bot acc: 0.0361
top acc: 0.1643 ::: bot acc: 0.1085
top acc: 0.1896 ::: bot acc: 0.0369
current epoch: 4
train loss is 0.013714
average val loss: 0.008010, accuracy: 0.1094
average test loss: 0.008435, accuracy: 0.1104
case acc: 0.13787772
case acc: 0.08482667
case acc: 0.14508905
case acc: 0.083494894
case acc: 0.15238434
case acc: 0.058941975
top acc: 0.0563 ::: bot acc: 0.2165
top acc: 0.1499 ::: bot acc: 0.0293
top acc: 0.0735 ::: bot acc: 0.2113
top acc: 0.0686 ::: bot acc: 0.1371
top acc: 0.0728 ::: bot acc: 0.2225
top acc: 0.0667 ::: bot acc: 0.0866
current epoch: 5
train loss is 0.008888
average val loss: 0.007103, accuracy: 0.1038
average test loss: 0.007488, accuracy: 0.1048
case acc: 0.12392509
case acc: 0.08729469
case acc: 0.13558245
case acc: 0.081395656
case acc: 0.14303263
case acc: 0.05772935
top acc: 0.0463 ::: bot acc: 0.2005
top acc: 0.1521 ::: bot acc: 0.0322
top acc: 0.0645 ::: bot acc: 0.2021
top acc: 0.0798 ::: bot acc: 0.1255
top acc: 0.0861 ::: bot acc: 0.2017
top acc: 0.0778 ::: bot acc: 0.0751
current epoch: 6
train loss is 0.007424
average val loss: 0.005493, accuracy: 0.0938
average test loss: 0.006142, accuracy: 0.0956
case acc: 0.08988292
case acc: 0.12027021
case acc: 0.09903188
case acc: 0.07840766
case acc: 0.12358776
case acc: 0.062322218
top acc: 0.0334 ::: bot acc: 0.1556
top acc: 0.1854 ::: bot acc: 0.0651
top acc: 0.0353 ::: bot acc: 0.1622
top acc: 0.1209 ::: bot acc: 0.0852
top acc: 0.1224 ::: bot acc: 0.1546
top acc: 0.1177 ::: bot acc: 0.0356
current epoch: 7
train loss is 0.007533
average val loss: 0.005535, accuracy: 0.0938
average test loss: 0.006028, accuracy: 0.0948
case acc: 0.09339047
case acc: 0.103965916
case acc: 0.10722115
case acc: 0.07862475
case acc: 0.12538488
case acc: 0.059990223
top acc: 0.0332 ::: bot acc: 0.1612
top acc: 0.1688 ::: bot acc: 0.0491
top acc: 0.0405 ::: bot acc: 0.1719
top acc: 0.1115 ::: bot acc: 0.0947
top acc: 0.1186 ::: bot acc: 0.1590
top acc: 0.1071 ::: bot acc: 0.0464
current epoch: 8
train loss is 0.007328
average val loss: 0.005769, accuracy: 0.0949
average test loss: 0.006056, accuracy: 0.0945
case acc: 0.09842138
case acc: 0.08570979
case acc: 0.11739997
case acc: 0.07924712
case acc: 0.12823412
case acc: 0.057821255
top acc: 0.0342 ::: bot acc: 0.1678
top acc: 0.1505 ::: bot acc: 0.0309
top acc: 0.0483 ::: bot acc: 0.1830
top acc: 0.1002 ::: bot acc: 0.1057
top acc: 0.1134 ::: bot acc: 0.1660
top acc: 0.0950 ::: bot acc: 0.0580
current epoch: 9
train loss is 0.006821
average val loss: 0.005553, accuracy: 0.0931
average test loss: 0.005789, accuracy: 0.0920
case acc: 0.09374857
case acc: 0.081376396
case acc: 0.11483861
case acc: 0.07876586
case acc: 0.12546074
case acc: 0.057884563
top acc: 0.0338 ::: bot acc: 0.1614
top acc: 0.1462 ::: bot acc: 0.0267
top acc: 0.0462 ::: bot acc: 0.1801
top acc: 0.1034 ::: bot acc: 0.1022
top acc: 0.1192 ::: bot acc: 0.1588
top acc: 0.0973 ::: bot acc: 0.0552
current epoch: 10
train loss is 0.006546
average val loss: 0.005353, accuracy: 0.0914
average test loss: 0.005560, accuracy: 0.0898
case acc: 0.08899558
case acc: 0.077977985
case acc: 0.11158304
case acc: 0.07862528
case acc: 0.122915246
case acc: 0.058513205
top acc: 0.0330 ::: bot acc: 0.1545
top acc: 0.1427 ::: bot acc: 0.0236
top acc: 0.0433 ::: bot acc: 0.1766
top acc: 0.1075 ::: bot acc: 0.0987
top acc: 0.1243 ::: bot acc: 0.1528
top acc: 0.1005 ::: bot acc: 0.0525
current epoch: 11
train loss is 0.006306
average val loss: 0.005238, accuracy: 0.0901
average test loss: 0.005405, accuracy: 0.0881
case acc: 0.086345315
case acc: 0.07262428
case acc: 0.110890634
case acc: 0.07832186
case acc: 0.12198362
case acc: 0.05841733
top acc: 0.0330 ::: bot acc: 0.1508
top acc: 0.1369 ::: bot acc: 0.0183
top acc: 0.0431 ::: bot acc: 0.1759
top acc: 0.1081 ::: bot acc: 0.0973
top acc: 0.1265 ::: bot acc: 0.1500
top acc: 0.1005 ::: bot acc: 0.0525
current epoch: 12
train loss is 0.006134
average val loss: 0.005093, accuracy: 0.0888
average test loss: 0.005246, accuracy: 0.0863
case acc: 0.08274918
case acc: 0.06890606
case acc: 0.10869455
case acc: 0.07831423
case acc: 0.12064051
case acc: 0.05864756
top acc: 0.0331 ::: bot acc: 0.1449
top acc: 0.1333 ::: bot acc: 0.0153
top acc: 0.0413 ::: bot acc: 0.1735
top acc: 0.1109 ::: bot acc: 0.0946
top acc: 0.1296 ::: bot acc: 0.1466
top acc: 0.1025 ::: bot acc: 0.0505
current epoch: 13
train loss is 0.005997
average val loss: 0.004983, accuracy: 0.0877
average test loss: 0.005121, accuracy: 0.0849
case acc: 0.08020355
case acc: 0.06499336
case acc: 0.10706934
case acc: 0.07815561
case acc: 0.11988321
case acc: 0.058825664
top acc: 0.0335 ::: bot acc: 0.1411
top acc: 0.1288 ::: bot acc: 0.0123
top acc: 0.0400 ::: bot acc: 0.1716
top acc: 0.1133 ::: bot acc: 0.0923
top acc: 0.1311 ::: bot acc: 0.1445
top acc: 0.1037 ::: bot acc: 0.0494
current epoch: 14
train loss is 0.005850
average val loss: 0.004938, accuracy: 0.0871
average test loss: 0.005043, accuracy: 0.0839
case acc: 0.07916817
case acc: 0.05983722
case acc: 0.107282326
case acc: 0.07829501
case acc: 0.1201713
case acc: 0.05865639
top acc: 0.0342 ::: bot acc: 0.1389
top acc: 0.1224 ::: bot acc: 0.0094
top acc: 0.0404 ::: bot acc: 0.1720
top acc: 0.1137 ::: bot acc: 0.0924
top acc: 0.1309 ::: bot acc: 0.1452
top acc: 0.1021 ::: bot acc: 0.0507
current epoch: 15
train loss is 0.005663
average val loss: 0.004890, accuracy: 0.0865
average test loss: 0.004979, accuracy: 0.0831
case acc: 0.07833651
case acc: 0.05560193
case acc: 0.10746972
case acc: 0.07840457
case acc: 0.12048931
case acc: 0.058532983
top acc: 0.0351 ::: bot acc: 0.1372
top acc: 0.1162 ::: bot acc: 0.0094
top acc: 0.0404 ::: bot acc: 0.1726
top acc: 0.1136 ::: bot acc: 0.0922
top acc: 0.1301 ::: bot acc: 0.1459
top acc: 0.1015 ::: bot acc: 0.0517
current epoch: 16
train loss is 0.005562
average val loss: 0.004824, accuracy: 0.0857
average test loss: 0.004890, accuracy: 0.0821
case acc: 0.07649284
case acc: 0.052968603
case acc: 0.106506065
case acc: 0.07818493
case acc: 0.12040291
case acc: 0.05832431
top acc: 0.0354 ::: bot acc: 0.1341
top acc: 0.1116 ::: bot acc: 0.0111
top acc: 0.0396 ::: bot acc: 0.1712
top acc: 0.1147 ::: bot acc: 0.0911
top acc: 0.1301 ::: bot acc: 0.1457
top acc: 0.1010 ::: bot acc: 0.0518
current epoch: 17
train loss is 0.005418
average val loss: 0.004764, accuracy: 0.0851
average test loss: 0.004834, accuracy: 0.0816
case acc: 0.07523688
case acc: 0.050926074
case acc: 0.10609256
case acc: 0.07804108
case acc: 0.12077261
case acc: 0.058328878
top acc: 0.0363 ::: bot acc: 0.1320
top acc: 0.1069 ::: bot acc: 0.0142
top acc: 0.0392 ::: bot acc: 0.1708
top acc: 0.1151 ::: bot acc: 0.0904
top acc: 0.1302 ::: bot acc: 0.1463
top acc: 0.1010 ::: bot acc: 0.0520
current epoch: 18
train loss is 0.005334
average val loss: 0.004610, accuracy: 0.0836
average test loss: 0.004707, accuracy: 0.0803
case acc: 0.072481275
case acc: 0.050502602
case acc: 0.10216238
case acc: 0.07816002
case acc: 0.1195104
case acc: 0.05906325
top acc: 0.0399 ::: bot acc: 0.1262
top acc: 0.1060 ::: bot acc: 0.0146
top acc: 0.0368 ::: bot acc: 0.1663
top acc: 0.1197 ::: bot acc: 0.0859
top acc: 0.1326 ::: bot acc: 0.1433
top acc: 0.1043 ::: bot acc: 0.0491
current epoch: 19
train loss is 0.005264
average val loss: 0.004494, accuracy: 0.0825
average test loss: 0.004628, accuracy: 0.0795
case acc: 0.07069066
case acc: 0.05029766
case acc: 0.09929342
case acc: 0.07826698
case acc: 0.11912629
case acc: 0.059405647
top acc: 0.0433 ::: bot acc: 0.1218
top acc: 0.1052 ::: bot acc: 0.0157
top acc: 0.0352 ::: bot acc: 0.1627
top acc: 0.1231 ::: bot acc: 0.0827
top acc: 0.1338 ::: bot acc: 0.1421
top acc: 0.1061 ::: bot acc: 0.0469
current epoch: 20
train loss is 0.005221
average val loss: 0.004408, accuracy: 0.0816
average test loss: 0.004555, accuracy: 0.0788
case acc: 0.06934463
case acc: 0.049285468
case acc: 0.09734397
case acc: 0.07848432
case acc: 0.118846774
case acc: 0.05943774
top acc: 0.0463 ::: bot acc: 0.1180
top acc: 0.1029 ::: bot acc: 0.0174
top acc: 0.0344 ::: bot acc: 0.1602
top acc: 0.1254 ::: bot acc: 0.0805
top acc: 0.1340 ::: bot acc: 0.1416
top acc: 0.1070 ::: bot acc: 0.0459
current epoch: 21
train loss is 0.005159
average val loss: 0.004351, accuracy: 0.0810
average test loss: 0.004504, accuracy: 0.0783
case acc: 0.06816339
case acc: 0.04831066
case acc: 0.09600725
case acc: 0.0785758
case acc: 0.11884003
case acc: 0.05961093
top acc: 0.0490 ::: bot acc: 0.1151
top acc: 0.1002 ::: bot acc: 0.0196
top acc: 0.0337 ::: bot acc: 0.1586
top acc: 0.1272 ::: bot acc: 0.0788
top acc: 0.1342 ::: bot acc: 0.1414
top acc: 0.1077 ::: bot acc: 0.0454
current epoch: 22
train loss is 0.005110
average val loss: 0.004354, accuracy: 0.0809
average test loss: 0.004489, accuracy: 0.0781
case acc: 0.068390794
case acc: 0.046712432
case acc: 0.09642995
case acc: 0.07852463
case acc: 0.11951837
case acc: 0.059313197
top acc: 0.0494 ::: bot acc: 0.1151
top acc: 0.0956 ::: bot acc: 0.0243
top acc: 0.0335 ::: bot acc: 0.1596
top acc: 0.1261 ::: bot acc: 0.0798
top acc: 0.1326 ::: bot acc: 0.1433
top acc: 0.1058 ::: bot acc: 0.0473
current epoch: 23
train loss is 0.005055
average val loss: 0.004270, accuracy: 0.0800
average test loss: 0.004424, accuracy: 0.0774
case acc: 0.06677632
case acc: 0.04679676
case acc: 0.09379782
case acc: 0.07867267
case acc: 0.11868497
case acc: 0.05982245
top acc: 0.0534 ::: bot acc: 0.1107
top acc: 0.0950 ::: bot acc: 0.0250
top acc: 0.0325 ::: bot acc: 0.1561
top acc: 0.1292 ::: bot acc: 0.0768
top acc: 0.1344 ::: bot acc: 0.1413
top acc: 0.1081 ::: bot acc: 0.0452
current epoch: 24
train loss is 0.005012
average val loss: 0.004301, accuracy: 0.0802
average test loss: 0.004434, accuracy: 0.0777
case acc: 0.06736526
case acc: 0.04535966
case acc: 0.095533125
case acc: 0.078496106
case acc: 0.119971335
case acc: 0.059221435
top acc: 0.0521 ::: bot acc: 0.1122
top acc: 0.0894 ::: bot acc: 0.0306
top acc: 0.0330 ::: bot acc: 0.1583
top acc: 0.1270 ::: bot acc: 0.0789
top acc: 0.1313 ::: bot acc: 0.1447
top acc: 0.1049 ::: bot acc: 0.0487
current epoch: 25
train loss is 0.004961
average val loss: 0.004260, accuracy: 0.0798
average test loss: 0.004401, accuracy: 0.0773
case acc: 0.066749655
case acc: 0.045049995
case acc: 0.09434249
case acc: 0.07870999
case acc: 0.11993984
case acc: 0.0591935
top acc: 0.0541 ::: bot acc: 0.1103
top acc: 0.0877 ::: bot acc: 0.0321
top acc: 0.0325 ::: bot acc: 0.1568
top acc: 0.1284 ::: bot acc: 0.0775
top acc: 0.1313 ::: bot acc: 0.1445
top acc: 0.1055 ::: bot acc: 0.0479
current epoch: 26
train loss is 0.004934
average val loss: 0.004199, accuracy: 0.0792
average test loss: 0.004355, accuracy: 0.0769
case acc: 0.06596202
case acc: 0.045131315
case acc: 0.092317164
case acc: 0.078810416
case acc: 0.11956665
case acc: 0.059472263
top acc: 0.0572 ::: bot acc: 0.1072
top acc: 0.0873 ::: bot acc: 0.0324
top acc: 0.0319 ::: bot acc: 0.1541
top acc: 0.1305 ::: bot acc: 0.0756
top acc: 0.1323 ::: bot acc: 0.1435
top acc: 0.1066 ::: bot acc: 0.0467
current epoch: 27
train loss is 0.004911
average val loss: 0.004155, accuracy: 0.0787
average test loss: 0.004334, accuracy: 0.0766
case acc: 0.06560276
case acc: 0.044939708
case acc: 0.091091365
case acc: 0.07880205
case acc: 0.119786136
case acc: 0.05952526
top acc: 0.0593 ::: bot acc: 0.1055
top acc: 0.0864 ::: bot acc: 0.0335
top acc: 0.0313 ::: bot acc: 0.1525
top acc: 0.1317 ::: bot acc: 0.0742
top acc: 0.1322 ::: bot acc: 0.1440
top acc: 0.1069 ::: bot acc: 0.0467
current epoch: 28
train loss is 0.004889
average val loss: 0.004124, accuracy: 0.0784
average test loss: 0.004302, accuracy: 0.0763
case acc: 0.065050796
case acc: 0.044770896
case acc: 0.08998688
case acc: 0.07887344
case acc: 0.119865045
case acc: 0.05938283
top acc: 0.0607 ::: bot acc: 0.1035
top acc: 0.0853 ::: bot acc: 0.0344
top acc: 0.0310 ::: bot acc: 0.1509
top acc: 0.1326 ::: bot acc: 0.0734
top acc: 0.1319 ::: bot acc: 0.1445
top acc: 0.1065 ::: bot acc: 0.0468
current epoch: 29
train loss is 0.004897
average val loss: 0.004007, accuracy: 0.0771
average test loss: 0.004250, accuracy: 0.0756
case acc: 0.064048514
case acc: 0.045261666
case acc: 0.08579192
case acc: 0.07937834
case acc: 0.11846028
case acc: 0.06048138
top acc: 0.0668 ::: bot acc: 0.0980
top acc: 0.0887 ::: bot acc: 0.0311
top acc: 0.0300 ::: bot acc: 0.1454
top acc: 0.1378 ::: bot acc: 0.0684
top acc: 0.1355 ::: bot acc: 0.1404
top acc: 0.1106 ::: bot acc: 0.0428
current epoch: 30
train loss is 0.004881
average val loss: 0.003969, accuracy: 0.0766
average test loss: 0.004220, accuracy: 0.0753
case acc: 0.06369555
case acc: 0.045287818
case acc: 0.0844038
case acc: 0.07957528
case acc: 0.11807464
case acc: 0.060501505
top acc: 0.0687 ::: bot acc: 0.0958
top acc: 0.0886 ::: bot acc: 0.0314
top acc: 0.0299 ::: bot acc: 0.1431
top acc: 0.1392 ::: bot acc: 0.0672
top acc: 0.1357 ::: bot acc: 0.1397
top acc: 0.1109 ::: bot acc: 0.0425
current epoch: 31
train loss is 0.004871
average val loss: 0.003934, accuracy: 0.0761
average test loss: 0.004200, accuracy: 0.0750
case acc: 0.063288756
case acc: 0.0454345
case acc: 0.08284271
case acc: 0.0796901
case acc: 0.11762445
case acc: 0.060874127
top acc: 0.0711 ::: bot acc: 0.0932
top acc: 0.0892 ::: bot acc: 0.0309
top acc: 0.0300 ::: bot acc: 0.1408
top acc: 0.1410 ::: bot acc: 0.0649
top acc: 0.1373 ::: bot acc: 0.1383
top acc: 0.1122 ::: bot acc: 0.0415
current epoch: 32
train loss is 0.004875
average val loss: 0.003894, accuracy: 0.0756
average test loss: 0.004175, accuracy: 0.0746
case acc: 0.06296999
case acc: 0.04547933
case acc: 0.081202894
case acc: 0.0800161
case acc: 0.11678661
case acc: 0.061201997
top acc: 0.0739 ::: bot acc: 0.0906
top acc: 0.0898 ::: bot acc: 0.0302
top acc: 0.0304 ::: bot acc: 0.1381
top acc: 0.1432 ::: bot acc: 0.0627
top acc: 0.1391 ::: bot acc: 0.1363
top acc: 0.1137 ::: bot acc: 0.0397
current epoch: 33
train loss is 0.004877
average val loss: 0.003883, accuracy: 0.0754
average test loss: 0.004164, accuracy: 0.0745
case acc: 0.062907845
case acc: 0.045408513
case acc: 0.08071151
case acc: 0.08013276
case acc: 0.11670947
case acc: 0.06107989
top acc: 0.0747 ::: bot acc: 0.0900
top acc: 0.0888 ::: bot acc: 0.0314
top acc: 0.0303 ::: bot acc: 0.1374
top acc: 0.1437 ::: bot acc: 0.0624
top acc: 0.1392 ::: bot acc: 0.1360
top acc: 0.1135 ::: bot acc: 0.0399
current epoch: 34
train loss is 0.004853
average val loss: 0.003866, accuracy: 0.0752
average test loss: 0.004156, accuracy: 0.0744
case acc: 0.062903516
case acc: 0.045094244
case acc: 0.08043314
case acc: 0.08017497
case acc: 0.11696575
case acc: 0.06103921
top acc: 0.0749 ::: bot acc: 0.0897
top acc: 0.0876 ::: bot acc: 0.0323
top acc: 0.0303 ::: bot acc: 0.1369
top acc: 0.1439 ::: bot acc: 0.0622
top acc: 0.1392 ::: bot acc: 0.1362
top acc: 0.1131 ::: bot acc: 0.0405
current epoch: 35
train loss is 0.004852
average val loss: 0.003848, accuracy: 0.0750
average test loss: 0.004151, accuracy: 0.0744
case acc: 0.06285128
case acc: 0.04510113
case acc: 0.07986075
case acc: 0.080381915
case acc: 0.11686776
case acc: 0.061079517
top acc: 0.0757 ::: bot acc: 0.0891
top acc: 0.0872 ::: bot acc: 0.0327
top acc: 0.0307 ::: bot acc: 0.1358
top acc: 0.1447 ::: bot acc: 0.0612
top acc: 0.1394 ::: bot acc: 0.1360
top acc: 0.1136 ::: bot acc: 0.0398
current epoch: 36
train loss is 0.004843
average val loss: 0.003826, accuracy: 0.0747
average test loss: 0.004140, accuracy: 0.0742
case acc: 0.06270271
case acc: 0.045254644
case acc: 0.078918934
case acc: 0.08055836
case acc: 0.116395906
case acc: 0.061374553
top acc: 0.0769 ::: bot acc: 0.0878
top acc: 0.0877 ::: bot acc: 0.0325
top acc: 0.0307 ::: bot acc: 0.1345
top acc: 0.1461 ::: bot acc: 0.0596
top acc: 0.1402 ::: bot acc: 0.1349
top acc: 0.1144 ::: bot acc: 0.0390
current epoch: 37
train loss is 0.004828
average val loss: 0.003829, accuracy: 0.0747
average test loss: 0.004131, accuracy: 0.0741
case acc: 0.06272373
case acc: 0.045052726
case acc: 0.0789836
case acc: 0.08049908
case acc: 0.11629347
case acc: 0.061218373
top acc: 0.0768 ::: bot acc: 0.0880
top acc: 0.0866 ::: bot acc: 0.0335
top acc: 0.0307 ::: bot acc: 0.1345
top acc: 0.1458 ::: bot acc: 0.0599
top acc: 0.1401 ::: bot acc: 0.1350
top acc: 0.1139 ::: bot acc: 0.0396
current epoch: 38
train loss is 0.004831
average val loss: 0.003827, accuracy: 0.0747
average test loss: 0.004127, accuracy: 0.0741
case acc: 0.0628212
case acc: 0.044850864
case acc: 0.07887477
case acc: 0.08051994
case acc: 0.1162057
case acc: 0.06113611
top acc: 0.0767 ::: bot acc: 0.0882
top acc: 0.0856 ::: bot acc: 0.0343
top acc: 0.0307 ::: bot acc: 0.1344
top acc: 0.1460 ::: bot acc: 0.0599
top acc: 0.1404 ::: bot acc: 0.1347
top acc: 0.1137 ::: bot acc: 0.0397
current epoch: 39
train loss is 0.004809
average val loss: 0.003830, accuracy: 0.0748
average test loss: 0.004130, accuracy: 0.0742
case acc: 0.062895834
case acc: 0.04464234
case acc: 0.07967408
case acc: 0.08041661
case acc: 0.11671477
case acc: 0.060859088
top acc: 0.0754 ::: bot acc: 0.0894
top acc: 0.0835 ::: bot acc: 0.0367
top acc: 0.0306 ::: bot acc: 0.1357
top acc: 0.1451 ::: bot acc: 0.0610
top acc: 0.1393 ::: bot acc: 0.1361
top acc: 0.1126 ::: bot acc: 0.0410
current epoch: 40
train loss is 0.004802
average val loss: 0.003835, accuracy: 0.0748
average test loss: 0.004128, accuracy: 0.0742
case acc: 0.06302873
case acc: 0.044411626
case acc: 0.08013026
case acc: 0.08024343
case acc: 0.116904505
case acc: 0.06066817
top acc: 0.0748 ::: bot acc: 0.0901
top acc: 0.0818 ::: bot acc: 0.0381
top acc: 0.0305 ::: bot acc: 0.1364
top acc: 0.1445 ::: bot acc: 0.0615
top acc: 0.1387 ::: bot acc: 0.1365
top acc: 0.1120 ::: bot acc: 0.0416
current epoch: 41
train loss is 0.004790
average val loss: 0.003853, accuracy: 0.0751
average test loss: 0.004136, accuracy: 0.0743
case acc: 0.063113905
case acc: 0.044229425
case acc: 0.080946244
case acc: 0.08007917
case acc: 0.11725059
case acc: 0.060476433
top acc: 0.0732 ::: bot acc: 0.0914
top acc: 0.0799 ::: bot acc: 0.0401
top acc: 0.0301 ::: bot acc: 0.1379
top acc: 0.1436 ::: bot acc: 0.0625
top acc: 0.1381 ::: bot acc: 0.1375
top acc: 0.1109 ::: bot acc: 0.0428
current epoch: 42
train loss is 0.004793
average val loss: 0.003885, accuracy: 0.0755
average test loss: 0.004146, accuracy: 0.0746
case acc: 0.06352302
case acc: 0.04416598
case acc: 0.08236742
case acc: 0.079734504
case acc: 0.117706105
case acc: 0.060091026
top acc: 0.0716 ::: bot acc: 0.0934
top acc: 0.0771 ::: bot acc: 0.0429
top acc: 0.0303 ::: bot acc: 0.1399
top acc: 0.1421 ::: bot acc: 0.0639
top acc: 0.1368 ::: bot acc: 0.1388
top acc: 0.1094 ::: bot acc: 0.0441
current epoch: 43
train loss is 0.004787
average val loss: 0.003929, accuracy: 0.0760
average test loss: 0.004167, accuracy: 0.0749
case acc: 0.0637628
case acc: 0.04413805
case acc: 0.08382656
case acc: 0.079549246
case acc: 0.11821448
case acc: 0.05979713
top acc: 0.0695 ::: bot acc: 0.0954
top acc: 0.0745 ::: bot acc: 0.0455
top acc: 0.0300 ::: bot acc: 0.1422
top acc: 0.1404 ::: bot acc: 0.0656
top acc: 0.1362 ::: bot acc: 0.1399
top acc: 0.1081 ::: bot acc: 0.0455
current epoch: 44
train loss is 0.004766
average val loss: 0.003995, accuracy: 0.0768
average test loss: 0.004192, accuracy: 0.0753
case acc: 0.06428168
case acc: 0.04436992
case acc: 0.08602891
case acc: 0.079289034
case acc: 0.11877683
case acc: 0.059272885
top acc: 0.0665 ::: bot acc: 0.0983
top acc: 0.0709 ::: bot acc: 0.0491
top acc: 0.0298 ::: bot acc: 0.1456
top acc: 0.1377 ::: bot acc: 0.0684
top acc: 0.1345 ::: bot acc: 0.1416
top acc: 0.1059 ::: bot acc: 0.0475
current epoch: 45
train loss is 0.004762
average val loss: 0.004117, accuracy: 0.0782
average test loss: 0.004258, accuracy: 0.0762
case acc: 0.06513877
case acc: 0.044904888
case acc: 0.089708626
case acc: 0.07897566
case acc: 0.120130666
case acc: 0.058621693
top acc: 0.0619 ::: bot acc: 0.1030
top acc: 0.0653 ::: bot acc: 0.0548
top acc: 0.0306 ::: bot acc: 0.1507
top acc: 0.1334 ::: bot acc: 0.0731
top acc: 0.1316 ::: bot acc: 0.1450
top acc: 0.1020 ::: bot acc: 0.0517
current epoch: 46
train loss is 0.004763
average val loss: 0.004268, accuracy: 0.0799
average test loss: 0.004348, accuracy: 0.0775
case acc: 0.06642304
case acc: 0.045955982
case acc: 0.094211265
case acc: 0.07844573
case acc: 0.12165915
case acc: 0.058097493
top acc: 0.0568 ::: bot acc: 0.1081
top acc: 0.0594 ::: bot acc: 0.0606
top acc: 0.0325 ::: bot acc: 0.1566
top acc: 0.1283 ::: bot acc: 0.0777
top acc: 0.1282 ::: bot acc: 0.1490
top acc: 0.0975 ::: bot acc: 0.0561
current epoch: 47
train loss is 0.004785
average val loss: 0.004487, accuracy: 0.0822
average test loss: 0.004494, accuracy: 0.0791
case acc: 0.06802858
case acc: 0.04768285
case acc: 0.099584684
case acc: 0.07822384
case acc: 0.12354508
case acc: 0.057715576
top acc: 0.0508 ::: bot acc: 0.1139
top acc: 0.0526 ::: bot acc: 0.0674
top acc: 0.0350 ::: bot acc: 0.1634
top acc: 0.1226 ::: bot acc: 0.0836
top acc: 0.1241 ::: bot acc: 0.1539
top acc: 0.0927 ::: bot acc: 0.0609
current epoch: 48
train loss is 0.004815
average val loss: 0.004753, accuracy: 0.0848
average test loss: 0.004680, accuracy: 0.0812
case acc: 0.07021567
case acc: 0.050171066
case acc: 0.10567907
case acc: 0.07811364
case acc: 0.12533405
case acc: 0.057642926
top acc: 0.0451 ::: bot acc: 0.1201
top acc: 0.0457 ::: bot acc: 0.0746
top acc: 0.0387 ::: bot acc: 0.1707
top acc: 0.1165 ::: bot acc: 0.0896
top acc: 0.1202 ::: bot acc: 0.1585
top acc: 0.0878 ::: bot acc: 0.0657
current epoch: 49
train loss is 0.004885
average val loss: 0.004985, accuracy: 0.0869
average test loss: 0.004849, accuracy: 0.0829
case acc: 0.07185788
case acc: 0.05240683
case acc: 0.11101888
case acc: 0.07816914
case acc: 0.1264005
case acc: 0.05772039
top acc: 0.0417 ::: bot acc: 0.1243
top acc: 0.0409 ::: bot acc: 0.0804
top acc: 0.0426 ::: bot acc: 0.1768
top acc: 0.1122 ::: bot acc: 0.0939
top acc: 0.1182 ::: bot acc: 0.1611
top acc: 0.0848 ::: bot acc: 0.0688
current epoch: 50
train loss is 0.004962
average val loss: 0.005463, accuracy: 0.0911
average test loss: 0.005206, accuracy: 0.0864
case acc: 0.07545325
case acc: 0.056793462
case acc: 0.1204685
case acc: 0.07872061
case acc: 0.12880194
case acc: 0.05797379
top acc: 0.0362 ::: bot acc: 0.1325
top acc: 0.0341 ::: bot acc: 0.0904
top acc: 0.0502 ::: bot acc: 0.1871
top acc: 0.1038 ::: bot acc: 0.1025
top acc: 0.1132 ::: bot acc: 0.1672
top acc: 0.0784 ::: bot acc: 0.0753
LME_Co_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5142 5142 5142
1.7082474 -0.6288155 0.48738334 -0.25570297
Validation: 576 576 576
Testing: 750 750 750
pre-processing time: 0.00019431114196777344
the split date is 2012-01-01
train dropout: 0.4 test dropout: 0.05
net initializing with time: 0.002160310745239258
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012805
average val loss: 0.010264, accuracy: 0.1233
average test loss: 0.006594, accuracy: 0.0949
case acc: 0.07670705
case acc: 0.17958319
case acc: 0.0793754
case acc: 0.07925198
case acc: 0.08916119
case acc: 0.06560782
top acc: 0.0284 ::: bot acc: 0.1285
top acc: 0.2310 ::: bot acc: 0.1231
top acc: 0.1451 ::: bot acc: 0.0376
top acc: 0.1587 ::: bot acc: 0.0265
top acc: 0.1513 ::: bot acc: 0.0283
top acc: 0.1131 ::: bot acc: 0.0396
current epoch: 2
train loss is 0.010389
average val loss: 0.006262, accuracy: 0.0933
average test loss: 0.017906, accuracy: 0.1667
case acc: 0.05333843
case acc: 0.2775158
case acc: 0.16591458
case acc: 0.16933927
case acc: 0.19166668
case acc: 0.14253828
top acc: 0.0940 ::: bot acc: 0.0287
top acc: 0.3285 ::: bot acc: 0.2201
top acc: 0.2557 ::: bot acc: 0.0752
top acc: 0.2644 ::: bot acc: 0.0826
top acc: 0.2560 ::: bot acc: 0.1277
top acc: 0.2153 ::: bot acc: 0.0651
current epoch: 3
train loss is 0.012072
average val loss: 0.007233, accuracy: 0.0968
average test loss: 0.024306, accuracy: 0.2018
case acc: 0.084848076
case acc: 0.30589002
case acc: 0.20451263
case acc: 0.20706902
case acc: 0.23151344
case acc: 0.17711604
top acc: 0.1362 ::: bot acc: 0.0383
top acc: 0.3571 ::: bot acc: 0.2483
top acc: 0.2948 ::: bot acc: 0.1132
top acc: 0.3026 ::: bot acc: 0.1201
top acc: 0.2965 ::: bot acc: 0.1672
top acc: 0.2498 ::: bot acc: 0.0992
current epoch: 4
train loss is 0.013886
average val loss: 0.005436, accuracy: 0.0891
average test loss: 0.013138, accuracy: 0.1391
case acc: 0.04226373
case acc: 0.22949731
case acc: 0.13873318
case acc: 0.14167829
case acc: 0.17094597
case acc: 0.11164424
top acc: 0.0728 ::: bot acc: 0.0381
top acc: 0.2807 ::: bot acc: 0.1723
top acc: 0.2275 ::: bot acc: 0.0512
top acc: 0.2378 ::: bot acc: 0.0543
top acc: 0.2350 ::: bot acc: 0.1067
top acc: 0.1835 ::: bot acc: 0.0353
current epoch: 5
train loss is 0.011497
average val loss: 0.008886, accuracy: 0.1114
average test loss: 0.005690, accuracy: 0.0890
case acc: 0.06425646
case acc: 0.14295207
case acc: 0.08040997
case acc: 0.08054324
case acc: 0.10173231
case acc: 0.064085275
top acc: 0.0239 ::: bot acc: 0.1119
top acc: 0.1935 ::: bot acc: 0.0863
top acc: 0.1474 ::: bot acc: 0.0376
top acc: 0.1618 ::: bot acc: 0.0227
top acc: 0.1661 ::: bot acc: 0.0385
top acc: 0.1103 ::: bot acc: 0.0403
current epoch: 6
train loss is 0.007983
average val loss: 0.008505, accuracy: 0.1081
average test loss: 0.005682, accuracy: 0.0890
case acc: 0.06145921
case acc: 0.13591546
case acc: 0.07985424
case acc: 0.08181195
case acc: 0.1086327
case acc: 0.066547625
top acc: 0.0234 ::: bot acc: 0.1082
top acc: 0.1871 ::: bot acc: 0.0788
top acc: 0.1465 ::: bot acc: 0.0384
top acc: 0.1643 ::: bot acc: 0.0202
top acc: 0.1734 ::: bot acc: 0.0450
top acc: 0.1165 ::: bot acc: 0.0355
current epoch: 7
train loss is 0.006860
average val loss: 0.006301, accuracy: 0.0940
average test loss: 0.007682, accuracy: 0.1037
case acc: 0.043739375
case acc: 0.15764463
case acc: 0.095262714
case acc: 0.10251768
case acc: 0.13971612
case acc: 0.08321259
top acc: 0.0341 ::: bot acc: 0.0762
top acc: 0.2085 ::: bot acc: 0.1005
top acc: 0.1732 ::: bot acc: 0.0301
top acc: 0.1941 ::: bot acc: 0.0216
top acc: 0.2046 ::: bot acc: 0.0758
top acc: 0.1475 ::: bot acc: 0.0228
current epoch: 8
train loss is 0.006729
average val loss: 0.006058, accuracy: 0.0916
average test loss: 0.007721, accuracy: 0.1040
case acc: 0.042156726
case acc: 0.15275003
case acc: 0.09613144
case acc: 0.105289094
case acc: 0.1427419
case acc: 0.08500075
top acc: 0.0383 ::: bot acc: 0.0717
top acc: 0.2036 ::: bot acc: 0.0961
top acc: 0.1749 ::: bot acc: 0.0299
top acc: 0.1972 ::: bot acc: 0.0239
top acc: 0.2076 ::: bot acc: 0.0791
top acc: 0.1504 ::: bot acc: 0.0220
current epoch: 9
train loss is 0.006622
average val loss: 0.006594, accuracy: 0.0939
average test loss: 0.006658, accuracy: 0.0962
case acc: 0.04533061
case acc: 0.13404879
case acc: 0.08867728
case acc: 0.09682531
case acc: 0.13329946
case acc: 0.07888674
top acc: 0.0306 ::: bot acc: 0.0804
top acc: 0.1849 ::: bot acc: 0.0771
top acc: 0.1635 ::: bot acc: 0.0315
top acc: 0.1870 ::: bot acc: 0.0184
top acc: 0.1976 ::: bot acc: 0.0700
top acc: 0.1405 ::: bot acc: 0.0236
current epoch: 10
train loss is 0.006262
average val loss: 0.006328, accuracy: 0.0914
average test loss: 0.006725, accuracy: 0.0966
case acc: 0.043110568
case acc: 0.12920678
case acc: 0.08936896
case acc: 0.099232666
case acc: 0.13729623
case acc: 0.08111121
top acc: 0.0340 ::: bot acc: 0.0752
top acc: 0.1803 ::: bot acc: 0.0723
top acc: 0.1646 ::: bot acc: 0.0310
top acc: 0.1902 ::: bot acc: 0.0195
top acc: 0.2020 ::: bot acc: 0.0739
top acc: 0.1439 ::: bot acc: 0.0231
current epoch: 11
train loss is 0.006062
average val loss: 0.006510, accuracy: 0.0918
average test loss: 0.006313, accuracy: 0.0932
case acc: 0.043986395
case acc: 0.118634395
case acc: 0.08666887
case acc: 0.0970337
case acc: 0.133956
case acc: 0.079212144
top acc: 0.0330 ::: bot acc: 0.0773
top acc: 0.1697 ::: bot acc: 0.0618
top acc: 0.1602 ::: bot acc: 0.0323
top acc: 0.1873 ::: bot acc: 0.0187
top acc: 0.1986 ::: bot acc: 0.0706
top acc: 0.1413 ::: bot acc: 0.0232
current epoch: 12
train loss is 0.005876
average val loss: 0.006464, accuracy: 0.0909
average test loss: 0.006185, accuracy: 0.0921
case acc: 0.042884268
case acc: 0.112032615
case acc: 0.08614189
case acc: 0.097551994
case acc: 0.13386069
case acc: 0.08016331
top acc: 0.0339 ::: bot acc: 0.0752
top acc: 0.1631 ::: bot acc: 0.0552
top acc: 0.1591 ::: bot acc: 0.0327
top acc: 0.1878 ::: bot acc: 0.0191
top acc: 0.1986 ::: bot acc: 0.0707
top acc: 0.1427 ::: bot acc: 0.0232
current epoch: 13
train loss is 0.005715
average val loss: 0.006404, accuracy: 0.0902
average test loss: 0.006109, accuracy: 0.0914
case acc: 0.042331234
case acc: 0.1062334
case acc: 0.08634515
case acc: 0.098780125
case acc: 0.1337813
case acc: 0.08082207
top acc: 0.0366 ::: bot acc: 0.0725
top acc: 0.1572 ::: bot acc: 0.0497
top acc: 0.1593 ::: bot acc: 0.0330
top acc: 0.1893 ::: bot acc: 0.0196
top acc: 0.1981 ::: bot acc: 0.0708
top acc: 0.1437 ::: bot acc: 0.0229
current epoch: 14
train loss is 0.005569
average val loss: 0.006546, accuracy: 0.0911
average test loss: 0.005838, accuracy: 0.0889
case acc: 0.042244166
case acc: 0.098195344
case acc: 0.085067265
case acc: 0.09744766
case acc: 0.13086212
case acc: 0.07977305
top acc: 0.0363 ::: bot acc: 0.0727
top acc: 0.1491 ::: bot acc: 0.0414
top acc: 0.1569 ::: bot acc: 0.0341
top acc: 0.1875 ::: bot acc: 0.0188
top acc: 0.1953 ::: bot acc: 0.0680
top acc: 0.1420 ::: bot acc: 0.0234
current epoch: 15
train loss is 0.005465
average val loss: 0.006046, accuracy: 0.0872
average test loss: 0.006248, accuracy: 0.0921
case acc: 0.039936855
case acc: 0.09949209
case acc: 0.08883898
case acc: 0.103665605
case acc: 0.13669963
case acc: 0.084239215
top acc: 0.0452 ::: bot acc: 0.0635
top acc: 0.1504 ::: bot acc: 0.0429
top acc: 0.1638 ::: bot acc: 0.0314
top acc: 0.1950 ::: bot acc: 0.0228
top acc: 0.2011 ::: bot acc: 0.0740
top acc: 0.1491 ::: bot acc: 0.0223
current epoch: 16
train loss is 0.005284
average val loss: 0.006121, accuracy: 0.0877
average test loss: 0.006048, accuracy: 0.0903
case acc: 0.03964238
case acc: 0.09299536
case acc: 0.08850827
case acc: 0.10326191
case acc: 0.13399225
case acc: 0.08317669
top acc: 0.0458 ::: bot acc: 0.0627
top acc: 0.1438 ::: bot acc: 0.0362
top acc: 0.1631 ::: bot acc: 0.0319
top acc: 0.1941 ::: bot acc: 0.0228
top acc: 0.1984 ::: bot acc: 0.0714
top acc: 0.1476 ::: bot acc: 0.0219
current epoch: 17
train loss is 0.005211
average val loss: 0.006229, accuracy: 0.0885
average test loss: 0.005822, accuracy: 0.0882
case acc: 0.03969604
case acc: 0.08626834
case acc: 0.087393306
case acc: 0.10206899
case acc: 0.13122068
case acc: 0.08238189
top acc: 0.0462 ::: bot acc: 0.0622
top acc: 0.1372 ::: bot acc: 0.0296
top acc: 0.1613 ::: bot acc: 0.0325
top acc: 0.1929 ::: bot acc: 0.0219
top acc: 0.1956 ::: bot acc: 0.0686
top acc: 0.1464 ::: bot acc: 0.0222
current epoch: 18
train loss is 0.005112
average val loss: 0.006266, accuracy: 0.0887
average test loss: 0.005682, accuracy: 0.0867
case acc: 0.039373208
case acc: 0.08084102
case acc: 0.08720119
case acc: 0.101634
case acc: 0.12949581
case acc: 0.08165224
top acc: 0.0470 ::: bot acc: 0.0611
top acc: 0.1316 ::: bot acc: 0.0242
top acc: 0.1610 ::: bot acc: 0.0326
top acc: 0.1920 ::: bot acc: 0.0216
top acc: 0.1938 ::: bot acc: 0.0670
top acc: 0.1453 ::: bot acc: 0.0224
current epoch: 19
train loss is 0.005054
average val loss: 0.006290, accuracy: 0.0889
average test loss: 0.005565, accuracy: 0.0855
case acc: 0.03913211
case acc: 0.07655953
case acc: 0.08720016
case acc: 0.10140774
case acc: 0.12776083
case acc: 0.08120291
top acc: 0.0485 ::: bot acc: 0.0595
top acc: 0.1271 ::: bot acc: 0.0208
top acc: 0.1608 ::: bot acc: 0.0327
top acc: 0.1917 ::: bot acc: 0.0216
top acc: 0.1921 ::: bot acc: 0.0653
top acc: 0.1445 ::: bot acc: 0.0223
current epoch: 20
train loss is 0.004983
average val loss: 0.006186, accuracy: 0.0881
average test loss: 0.005602, accuracy: 0.0857
case acc: 0.03893524
case acc: 0.07430814
case acc: 0.08823796
case acc: 0.102856025
case acc: 0.12819614
case acc: 0.08180153
top acc: 0.0517 ::: bot acc: 0.0563
top acc: 0.1245 ::: bot acc: 0.0190
top acc: 0.1627 ::: bot acc: 0.0321
top acc: 0.1935 ::: bot acc: 0.0225
top acc: 0.1923 ::: bot acc: 0.0657
top acc: 0.1451 ::: bot acc: 0.0226
current epoch: 21
train loss is 0.004974
average val loss: 0.005880, accuracy: 0.0857
average test loss: 0.005878, accuracy: 0.0880
case acc: 0.03895646
case acc: 0.07513605
case acc: 0.09113808
case acc: 0.10702298
case acc: 0.13139723
case acc: 0.084529206
top acc: 0.0585 ::: bot acc: 0.0497
top acc: 0.1255 ::: bot acc: 0.0197
top acc: 0.1679 ::: bot acc: 0.0307
top acc: 0.1982 ::: bot acc: 0.0254
top acc: 0.1956 ::: bot acc: 0.0691
top acc: 0.1495 ::: bot acc: 0.0222
current epoch: 22
train loss is 0.004882
average val loss: 0.005784, accuracy: 0.0849
average test loss: 0.005945, accuracy: 0.0885
case acc: 0.039165054
case acc: 0.07336405
case acc: 0.09267342
case acc: 0.10863784
case acc: 0.13184918
case acc: 0.0854756
top acc: 0.0615 ::: bot acc: 0.0464
top acc: 0.1235 ::: bot acc: 0.0184
top acc: 0.1701 ::: bot acc: 0.0307
top acc: 0.2001 ::: bot acc: 0.0267
top acc: 0.1958 ::: bot acc: 0.0698
top acc: 0.1508 ::: bot acc: 0.0220
current epoch: 23
train loss is 0.004848
average val loss: 0.005883, accuracy: 0.0857
average test loss: 0.005757, accuracy: 0.0868
case acc: 0.03914548
case acc: 0.06932817
case acc: 0.09191347
case acc: 0.10692604
case acc: 0.1294294
case acc: 0.08411183
top acc: 0.0615 ::: bot acc: 0.0460
top acc: 0.1185 ::: bot acc: 0.0158
top acc: 0.1688 ::: bot acc: 0.0306
top acc: 0.1980 ::: bot acc: 0.0254
top acc: 0.1936 ::: bot acc: 0.0674
top acc: 0.1488 ::: bot acc: 0.0223
current epoch: 24
train loss is 0.004807
average val loss: 0.006156, accuracy: 0.0881
average test loss: 0.005416, accuracy: 0.0836
case acc: 0.038969267
case acc: 0.06353231
case acc: 0.08976206
case acc: 0.10365713
case acc: 0.124591246
case acc: 0.08122363
top acc: 0.0592 ::: bot acc: 0.0486
top acc: 0.1115 ::: bot acc: 0.0131
top acc: 0.1655 ::: bot acc: 0.0313
top acc: 0.1945 ::: bot acc: 0.0230
top acc: 0.1887 ::: bot acc: 0.0624
top acc: 0.1441 ::: bot acc: 0.0227
current epoch: 25
train loss is 0.004781
average val loss: 0.006035, accuracy: 0.0872
average test loss: 0.005512, accuracy: 0.0844
case acc: 0.03926694
case acc: 0.06298219
case acc: 0.091087855
case acc: 0.10525633
case acc: 0.12565334
case acc: 0.082306355
top acc: 0.0624 ::: bot acc: 0.0448
top acc: 0.1107 ::: bot acc: 0.0126
top acc: 0.1678 ::: bot acc: 0.0308
top acc: 0.1963 ::: bot acc: 0.0240
top acc: 0.1896 ::: bot acc: 0.0637
top acc: 0.1457 ::: bot acc: 0.0228
current epoch: 26
train loss is 0.004742
average val loss: 0.006260, accuracy: 0.0892
average test loss: 0.005253, accuracy: 0.0821
case acc: 0.039006103
case acc: 0.05914324
case acc: 0.0896656
case acc: 0.1027359
case acc: 0.12163475
case acc: 0.08022475
top acc: 0.0608 ::: bot acc: 0.0463
top acc: 0.1048 ::: bot acc: 0.0128
top acc: 0.1654 ::: bot acc: 0.0314
top acc: 0.1932 ::: bot acc: 0.0222
top acc: 0.1855 ::: bot acc: 0.0597
top acc: 0.1424 ::: bot acc: 0.0231
current epoch: 27
train loss is 0.004795
average val loss: 0.005971, accuracy: 0.0868
average test loss: 0.005495, accuracy: 0.0842
case acc: 0.040068217
case acc: 0.06006925
case acc: 0.092316054
case acc: 0.106007725
case acc: 0.1246025
case acc: 0.082213715
top acc: 0.0661 ::: bot acc: 0.0410
top acc: 0.1064 ::: bot acc: 0.0127
top acc: 0.1700 ::: bot acc: 0.0305
top acc: 0.1970 ::: bot acc: 0.0248
top acc: 0.1885 ::: bot acc: 0.0628
top acc: 0.1458 ::: bot acc: 0.0223
current epoch: 28
train loss is 0.004708
average val loss: 0.005911, accuracy: 0.0863
average test loss: 0.005533, accuracy: 0.0846
case acc: 0.040672477
case acc: 0.05942712
case acc: 0.093221135
case acc: 0.10693349
case acc: 0.124688506
case acc: 0.082607195
top acc: 0.0684 ::: bot acc: 0.0388
top acc: 0.1054 ::: bot acc: 0.0130
top acc: 0.1712 ::: bot acc: 0.0303
top acc: 0.1978 ::: bot acc: 0.0256
top acc: 0.1886 ::: bot acc: 0.0630
top acc: 0.1464 ::: bot acc: 0.0224
current epoch: 29
train loss is 0.004651
average val loss: 0.005983, accuracy: 0.0870
average test loss: 0.005438, accuracy: 0.0837
case acc: 0.040889535
case acc: 0.057553936
case acc: 0.092980966
case acc: 0.10600677
case acc: 0.12307803
case acc: 0.081897035
top acc: 0.0687 ::: bot acc: 0.0386
top acc: 0.1023 ::: bot acc: 0.0132
top acc: 0.1711 ::: bot acc: 0.0301
top acc: 0.1969 ::: bot acc: 0.0248
top acc: 0.1869 ::: bot acc: 0.0614
top acc: 0.1450 ::: bot acc: 0.0227
current epoch: 30
train loss is 0.004681
average val loss: 0.005830, accuracy: 0.0858
average test loss: 0.005568, accuracy: 0.0849
case acc: 0.041974366
case acc: 0.057753816
case acc: 0.094779484
case acc: 0.1077339
case acc: 0.12457719
case acc: 0.08278863
top acc: 0.0721 ::: bot acc: 0.0357
top acc: 0.1027 ::: bot acc: 0.0131
top acc: 0.1737 ::: bot acc: 0.0300
top acc: 0.1988 ::: bot acc: 0.0262
top acc: 0.1883 ::: bot acc: 0.0629
top acc: 0.1466 ::: bot acc: 0.0224
current epoch: 31
train loss is 0.004665
average val loss: 0.005538, accuracy: 0.0833
average test loss: 0.005885, accuracy: 0.0878
case acc: 0.044034135
case acc: 0.059550688
case acc: 0.09797105
case acc: 0.11153747
case acc: 0.12862968
case acc: 0.08533799
top acc: 0.0776 ::: bot acc: 0.0314
top acc: 0.1056 ::: bot acc: 0.0126
top acc: 0.1786 ::: bot acc: 0.0299
top acc: 0.2027 ::: bot acc: 0.0294
top acc: 0.1924 ::: bot acc: 0.0673
top acc: 0.1502 ::: bot acc: 0.0225
current epoch: 32
train loss is 0.004612
average val loss: 0.005429, accuracy: 0.0824
average test loss: 0.006003, accuracy: 0.0889
case acc: 0.04521208
case acc: 0.059754636
case acc: 0.0994941
case acc: 0.11299032
case acc: 0.12980483
case acc: 0.086434364
top acc: 0.0801 ::: bot acc: 0.0297
top acc: 0.1059 ::: bot acc: 0.0129
top acc: 0.1808 ::: bot acc: 0.0299
top acc: 0.2042 ::: bot acc: 0.0306
top acc: 0.1934 ::: bot acc: 0.0684
top acc: 0.1519 ::: bot acc: 0.0224
current epoch: 33
train loss is 0.004613
average val loss: 0.005620, accuracy: 0.0842
average test loss: 0.005752, accuracy: 0.0866
case acc: 0.044115372
case acc: 0.05683012
case acc: 0.097545646
case acc: 0.11023387
case acc: 0.12656567
case acc: 0.084340096
top acc: 0.0777 ::: bot acc: 0.0309
top acc: 0.1013 ::: bot acc: 0.0136
top acc: 0.1780 ::: bot acc: 0.0298
top acc: 0.2012 ::: bot acc: 0.0285
top acc: 0.1903 ::: bot acc: 0.0650
top acc: 0.1492 ::: bot acc: 0.0220
current epoch: 34
train loss is 0.004590
average val loss: 0.006152, accuracy: 0.0891
average test loss: 0.005170, accuracy: 0.0813
case acc: 0.041536707
case acc: 0.05196928
case acc: 0.092696816
case acc: 0.10324799
case acc: 0.1187221
case acc: 0.0796694
top acc: 0.0710 ::: bot acc: 0.0361
top acc: 0.0918 ::: bot acc: 0.0177
top acc: 0.1707 ::: bot acc: 0.0302
top acc: 0.1935 ::: bot acc: 0.0229
top acc: 0.1821 ::: bot acc: 0.0576
top acc: 0.1417 ::: bot acc: 0.0230
current epoch: 35
train loss is 0.004633
average val loss: 0.006228, accuracy: 0.0899
average test loss: 0.005078, accuracy: 0.0805
case acc: 0.0413386
case acc: 0.051105637
case acc: 0.09219794
case acc: 0.10223401
case acc: 0.11716766
case acc: 0.078978926
top acc: 0.0705 ::: bot acc: 0.0364
top acc: 0.0900 ::: bot acc: 0.0192
top acc: 0.1697 ::: bot acc: 0.0305
top acc: 0.1921 ::: bot acc: 0.0223
top acc: 0.1808 ::: bot acc: 0.0561
top acc: 0.1404 ::: bot acc: 0.0231
current epoch: 36
train loss is 0.004615
average val loss: 0.006116, accuracy: 0.0889
average test loss: 0.005194, accuracy: 0.0816
case acc: 0.042288292
case acc: 0.05129768
case acc: 0.09352466
case acc: 0.10375009
case acc: 0.11838955
case acc: 0.08007859
top acc: 0.0731 ::: bot acc: 0.0343
top acc: 0.0906 ::: bot acc: 0.0187
top acc: 0.1721 ::: bot acc: 0.0301
top acc: 0.1940 ::: bot acc: 0.0232
top acc: 0.1817 ::: bot acc: 0.0572
top acc: 0.1422 ::: bot acc: 0.0229
current epoch: 37
train loss is 0.004637
average val loss: 0.005935, accuracy: 0.0873
average test loss: 0.005365, accuracy: 0.0832
case acc: 0.04352749
case acc: 0.052353885
case acc: 0.0954778
case acc: 0.10599248
case acc: 0.12016538
case acc: 0.0814624
top acc: 0.0765 ::: bot acc: 0.0315
top acc: 0.0928 ::: bot acc: 0.0173
top acc: 0.1751 ::: bot acc: 0.0296
top acc: 0.1963 ::: bot acc: 0.0252
top acc: 0.1835 ::: bot acc: 0.0591
top acc: 0.1444 ::: bot acc: 0.0223
current epoch: 38
train loss is 0.004577
average val loss: 0.005890, accuracy: 0.0869
average test loss: 0.005399, accuracy: 0.0835
case acc: 0.043912362
case acc: 0.052385367
case acc: 0.096169226
case acc: 0.10643946
case acc: 0.12006438
case acc: 0.08184645
top acc: 0.0774 ::: bot acc: 0.0306
top acc: 0.0928 ::: bot acc: 0.0171
top acc: 0.1761 ::: bot acc: 0.0297
top acc: 0.1967 ::: bot acc: 0.0257
top acc: 0.1833 ::: bot acc: 0.0591
top acc: 0.1452 ::: bot acc: 0.0222
current epoch: 39
train loss is 0.004599
average val loss: 0.005870, accuracy: 0.0867
average test loss: 0.005401, accuracy: 0.0835
case acc: 0.044139672
case acc: 0.052116342
case acc: 0.096732534
case acc: 0.10642691
case acc: 0.11992797
case acc: 0.081635214
top acc: 0.0781 ::: bot acc: 0.0301
top acc: 0.0926 ::: bot acc: 0.0171
top acc: 0.1769 ::: bot acc: 0.0299
top acc: 0.1966 ::: bot acc: 0.0256
top acc: 0.1832 ::: bot acc: 0.0591
top acc: 0.1450 ::: bot acc: 0.0222
current epoch: 40
train loss is 0.004599
average val loss: 0.005754, accuracy: 0.0857
average test loss: 0.005508, accuracy: 0.0845
case acc: 0.0453364
case acc: 0.05255703
case acc: 0.098093696
case acc: 0.10740153
case acc: 0.121325545
case acc: 0.08243715
top acc: 0.0803 ::: bot acc: 0.0290
top acc: 0.0936 ::: bot acc: 0.0161
top acc: 0.1790 ::: bot acc: 0.0299
top acc: 0.1977 ::: bot acc: 0.0263
top acc: 0.1846 ::: bot acc: 0.0605
top acc: 0.1461 ::: bot acc: 0.0224
current epoch: 41
train loss is 0.004564
average val loss: 0.005544, accuracy: 0.0838
average test loss: 0.005743, accuracy: 0.0867
case acc: 0.04721595
case acc: 0.053894907
case acc: 0.10052631
case acc: 0.1100524
case acc: 0.12441162
case acc: 0.084227875
top acc: 0.0838 ::: bot acc: 0.0275
top acc: 0.0964 ::: bot acc: 0.0146
top acc: 0.1824 ::: bot acc: 0.0304
top acc: 0.2006 ::: bot acc: 0.0287
top acc: 0.1876 ::: bot acc: 0.0637
top acc: 0.1487 ::: bot acc: 0.0222
current epoch: 42
train loss is 0.004520
average val loss: 0.005607, accuracy: 0.0844
average test loss: 0.005644, accuracy: 0.0858
case acc: 0.046824146
case acc: 0.05315982
case acc: 0.09978513
case acc: 0.108533055
case acc: 0.12335515
case acc: 0.08314793
top acc: 0.0829 ::: bot acc: 0.0278
top acc: 0.0948 ::: bot acc: 0.0155
top acc: 0.1817 ::: bot acc: 0.0302
top acc: 0.1989 ::: bot acc: 0.0274
top acc: 0.1866 ::: bot acc: 0.0626
top acc: 0.1472 ::: bot acc: 0.0221
current epoch: 43
train loss is 0.004558
average val loss: 0.005329, accuracy: 0.0819
average test loss: 0.005969, accuracy: 0.0889
case acc: 0.049491193
case acc: 0.055106144
case acc: 0.103063986
case acc: 0.112049825
case acc: 0.12760542
case acc: 0.08580578
top acc: 0.0875 ::: bot acc: 0.0265
top acc: 0.0985 ::: bot acc: 0.0139
top acc: 0.1861 ::: bot acc: 0.0307
top acc: 0.2026 ::: bot acc: 0.0304
top acc: 0.1908 ::: bot acc: 0.0669
top acc: 0.1509 ::: bot acc: 0.0224
current epoch: 44
train loss is 0.004525
average val loss: 0.004969, accuracy: 0.0786
average test loss: 0.006469, accuracy: 0.0935
case acc: 0.05351545
case acc: 0.058655508
case acc: 0.10772476
case acc: 0.117485784
case acc: 0.13344927
case acc: 0.09016134
top acc: 0.0940 ::: bot acc: 0.0256
top acc: 0.1044 ::: bot acc: 0.0128
top acc: 0.1924 ::: bot acc: 0.0320
top acc: 0.2082 ::: bot acc: 0.0355
top acc: 0.1966 ::: bot acc: 0.0728
top acc: 0.1566 ::: bot acc: 0.0241
current epoch: 45
train loss is 0.004502
average val loss: 0.004880, accuracy: 0.0778
average test loss: 0.006604, accuracy: 0.0948
case acc: 0.054710586
case acc: 0.059505418
case acc: 0.10909106
case acc: 0.11878839
case acc: 0.13513961
case acc: 0.091286376
top acc: 0.0958 ::: bot acc: 0.0258
top acc: 0.1056 ::: bot acc: 0.0128
top acc: 0.1942 ::: bot acc: 0.0325
top acc: 0.2096 ::: bot acc: 0.0367
top acc: 0.1981 ::: bot acc: 0.0746
top acc: 0.1580 ::: bot acc: 0.0247
current epoch: 46
train loss is 0.004486
average val loss: 0.005121, accuracy: 0.0801
average test loss: 0.006211, accuracy: 0.0911
case acc: 0.051607333
case acc: 0.056243613
case acc: 0.10576414
case acc: 0.11415708
case acc: 0.13089424
case acc: 0.08794714
top acc: 0.0912 ::: bot acc: 0.0256
top acc: 0.1004 ::: bot acc: 0.0134
top acc: 0.1900 ::: bot acc: 0.0312
top acc: 0.2046 ::: bot acc: 0.0325
top acc: 0.1937 ::: bot acc: 0.0705
top acc: 0.1538 ::: bot acc: 0.0231
current epoch: 47
train loss is 0.004498
average val loss: 0.005405, accuracy: 0.0828
average test loss: 0.005826, accuracy: 0.0875
case acc: 0.048918385
case acc: 0.05338477
case acc: 0.10236261
case acc: 0.109401174
case acc: 0.12655307
case acc: 0.08462087
top acc: 0.0866 ::: bot acc: 0.0266
top acc: 0.0952 ::: bot acc: 0.0154
top acc: 0.1852 ::: bot acc: 0.0305
top acc: 0.1998 ::: bot acc: 0.0280
top acc: 0.1894 ::: bot acc: 0.0661
top acc: 0.1491 ::: bot acc: 0.0224
current epoch: 48
train loss is 0.004499
average val loss: 0.005437, accuracy: 0.0832
average test loss: 0.005790, accuracy: 0.0872
case acc: 0.04869376
case acc: 0.052738566
case acc: 0.10217163
case acc: 0.10885355
case acc: 0.1262674
case acc: 0.08436965
top acc: 0.0864 ::: bot acc: 0.0264
top acc: 0.0942 ::: bot acc: 0.0156
top acc: 0.1847 ::: bot acc: 0.0304
top acc: 0.1994 ::: bot acc: 0.0274
top acc: 0.1888 ::: bot acc: 0.0659
top acc: 0.1486 ::: bot acc: 0.0225
current epoch: 49
train loss is 0.004529
average val loss: 0.005258, accuracy: 0.0815
average test loss: 0.006009, accuracy: 0.0892
case acc: 0.050471015
case acc: 0.054186832
case acc: 0.10453151
case acc: 0.111387454
case acc: 0.1284342
case acc: 0.08636741
top acc: 0.0893 ::: bot acc: 0.0257
top acc: 0.0972 ::: bot acc: 0.0140
top acc: 0.1882 ::: bot acc: 0.0311
top acc: 0.2020 ::: bot acc: 0.0298
top acc: 0.1910 ::: bot acc: 0.0681
top acc: 0.1512 ::: bot acc: 0.0231
current epoch: 50
train loss is 0.004527
average val loss: 0.005197, accuracy: 0.0809
average test loss: 0.006093, accuracy: 0.0901
case acc: 0.051325407
case acc: 0.054895345
case acc: 0.10564855
case acc: 0.11234316
case acc: 0.12903
case acc: 0.08719115
top acc: 0.0906 ::: bot acc: 0.0254
top acc: 0.0984 ::: bot acc: 0.0138
top acc: 0.1895 ::: bot acc: 0.0316
top acc: 0.2030 ::: bot acc: 0.0307
top acc: 0.1915 ::: bot acc: 0.0688
top acc: 0.1524 ::: bot acc: 0.0233
LME_Co_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5130 5130 5130
1.7082474 -0.6288155 0.48738334 -0.27422604
Validation: 570 570 570
Testing: 768 768 768
pre-processing time: 0.00019979476928710938
the split date is 2012-07-01
train dropout: 0.4 test dropout: 0.05
net initializing with time: 0.0024003982543945312
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012664
average val loss: 0.005378, accuracy: 0.0878
average test loss: 0.006408, accuracy: 0.0961
case acc: 0.09837159
case acc: 0.14696214
case acc: 0.11381981
case acc: 0.080899596
case acc: 0.05972363
case acc: 0.07662366
top acc: 0.0737 ::: bot acc: 0.1644
top acc: 0.1932 ::: bot acc: 0.1050
top acc: 0.0418 ::: bot acc: 0.1751
top acc: 0.0340 ::: bot acc: 0.1739
top acc: 0.0554 ::: bot acc: 0.1011
top acc: 0.1169 ::: bot acc: 0.0614
current epoch: 2
train loss is 0.012758
average val loss: 0.011683, accuracy: 0.1250
average test loss: 0.014183, accuracy: 0.1391
case acc: 0.11061704
case acc: 0.2792555
case acc: 0.06234625
case acc: 0.10324492
case acc: 0.113610305
case acc: 0.16546999
top acc: 0.2175 ::: bot acc: 0.0273
top acc: 0.3251 ::: bot acc: 0.2371
top acc: 0.1382 ::: bot acc: 0.0311
top acc: 0.1688 ::: bot acc: 0.0416
top acc: 0.1906 ::: bot acc: 0.0382
top acc: 0.2450 ::: bot acc: 0.0710
current epoch: 3
train loss is 0.012368
average val loss: 0.021993, accuracy: 0.1883
average test loss: 0.025220, accuracy: 0.1992
case acc: 0.17309907
case acc: 0.3450185
case acc: 0.11849998
case acc: 0.14697403
case acc: 0.18433498
case acc: 0.22697444
top acc: 0.2951 ::: bot acc: 0.0587
top acc: 0.3910 ::: bot acc: 0.3031
top acc: 0.2149 ::: bot acc: 0.0454
top acc: 0.2378 ::: bot acc: 0.0340
top acc: 0.2621 ::: bot acc: 0.1064
top acc: 0.3069 ::: bot acc: 0.1314
current epoch: 4
train loss is 0.012900
average val loss: 0.009676, accuracy: 0.1127
average test loss: 0.011867, accuracy: 0.1272
case acc: 0.109688655
case acc: 0.25096902
case acc: 0.060497835
case acc: 0.09588267
case acc: 0.107617654
case acc: 0.13872853
top acc: 0.2163 ::: bot acc: 0.0274
top acc: 0.2969 ::: bot acc: 0.2086
top acc: 0.1313 ::: bot acc: 0.0383
top acc: 0.1511 ::: bot acc: 0.0555
top acc: 0.1837 ::: bot acc: 0.0335
top acc: 0.2165 ::: bot acc: 0.0478
current epoch: 5
train loss is 0.008935
average val loss: 0.006284, accuracy: 0.0899
average test loss: 0.007980, accuracy: 0.1069
case acc: 0.097060144
case acc: 0.20143825
case acc: 0.065143004
case acc: 0.084564194
case acc: 0.08508851
case acc: 0.10838996
top acc: 0.1819 ::: bot acc: 0.0556
top acc: 0.2473 ::: bot acc: 0.1598
top acc: 0.0918 ::: bot acc: 0.0781
top acc: 0.1099 ::: bot acc: 0.0969
top acc: 0.1534 ::: bot acc: 0.0271
top acc: 0.1742 ::: bot acc: 0.0421
current epoch: 6
train loss is 0.007286
average val loss: 0.008388, accuracy: 0.1049
average test loss: 0.010345, accuracy: 0.1203
case acc: 0.10956002
case acc: 0.22179078
case acc: 0.059518054
case acc: 0.09200468
case acc: 0.113318205
case acc: 0.12583211
top acc: 0.2156 ::: bot acc: 0.0284
top acc: 0.2677 ::: bot acc: 0.1797
top acc: 0.1214 ::: bot acc: 0.0489
top acc: 0.1379 ::: bot acc: 0.0687
top acc: 0.1907 ::: bot acc: 0.0376
top acc: 0.2003 ::: bot acc: 0.0418
current epoch: 7
train loss is 0.007472
average val loss: 0.009610, accuracy: 0.1142
average test loss: 0.011678, accuracy: 0.1287
case acc: 0.120626315
case acc: 0.22807299
case acc: 0.061835967
case acc: 0.0958194
case acc: 0.1314549
case acc: 0.13415515
top acc: 0.2338 ::: bot acc: 0.0246
top acc: 0.2737 ::: bot acc: 0.1866
top acc: 0.1360 ::: bot acc: 0.0351
top acc: 0.1505 ::: bot acc: 0.0562
top acc: 0.2094 ::: bot acc: 0.0537
top acc: 0.2113 ::: bot acc: 0.0451
current epoch: 8
train loss is 0.007266
average val loss: 0.008202, accuracy: 0.1045
average test loss: 0.010094, accuracy: 0.1198
case acc: 0.1150841
case acc: 0.20752326
case acc: 0.059508294
case acc: 0.09143831
case acc: 0.122939534
case acc: 0.12236105
top acc: 0.2253 ::: bot acc: 0.0254
top acc: 0.2532 ::: bot acc: 0.1658
top acc: 0.1231 ::: bot acc: 0.0472
top acc: 0.1365 ::: bot acc: 0.0696
top acc: 0.2009 ::: bot acc: 0.0457
top acc: 0.1952 ::: bot acc: 0.0416
current epoch: 9
train loss is 0.006666
average val loss: 0.008566, accuracy: 0.1075
average test loss: 0.010465, accuracy: 0.1224
case acc: 0.120585814
case acc: 0.20476896
case acc: 0.06023184
case acc: 0.09310861
case acc: 0.13166824
case acc: 0.124217704
top acc: 0.2341 ::: bot acc: 0.0240
top acc: 0.2506 ::: bot acc: 0.1629
top acc: 0.1288 ::: bot acc: 0.0415
top acc: 0.1417 ::: bot acc: 0.0650
top acc: 0.2097 ::: bot acc: 0.0541
top acc: 0.1978 ::: bot acc: 0.0420
current epoch: 10
train loss is 0.006560
average val loss: 0.008041, accuracy: 0.1040
average test loss: 0.009852, accuracy: 0.1190
case acc: 0.120096646
case acc: 0.19302782
case acc: 0.059629545
case acc: 0.09158918
case acc: 0.12996036
case acc: 0.119896084
top acc: 0.2335 ::: bot acc: 0.0241
top acc: 0.2390 ::: bot acc: 0.1510
top acc: 0.1252 ::: bot acc: 0.0451
top acc: 0.1363 ::: bot acc: 0.0699
top acc: 0.2079 ::: bot acc: 0.0523
top acc: 0.1913 ::: bot acc: 0.0419
current epoch: 11
train loss is 0.006206
average val loss: 0.008590, accuracy: 0.1083
average test loss: 0.010447, accuracy: 0.1229
case acc: 0.12713112
case acc: 0.19386807
case acc: 0.060910948
case acc: 0.09354562
case acc: 0.13885249
case acc: 0.123077884
top acc: 0.2433 ::: bot acc: 0.0254
top acc: 0.2395 ::: bot acc: 0.1522
top acc: 0.1326 ::: bot acc: 0.0372
top acc: 0.1431 ::: bot acc: 0.0631
top acc: 0.2168 ::: bot acc: 0.0611
top acc: 0.1962 ::: bot acc: 0.0418
current epoch: 12
train loss is 0.006116
average val loss: 0.008891, accuracy: 0.1109
average test loss: 0.010750, accuracy: 0.1251
case acc: 0.13269538
case acc: 0.19181412
case acc: 0.06286717
case acc: 0.09448066
case acc: 0.14393434
case acc: 0.12451248
top acc: 0.2500 ::: bot acc: 0.0275
top acc: 0.2376 ::: bot acc: 0.1503
top acc: 0.1382 ::: bot acc: 0.0327
top acc: 0.1468 ::: bot acc: 0.0588
top acc: 0.2220 ::: bot acc: 0.0661
top acc: 0.1981 ::: bot acc: 0.0422
current epoch: 13
train loss is 0.006019
average val loss: 0.008507, accuracy: 0.1083
average test loss: 0.010314, accuracy: 0.1225
case acc: 0.13266683
case acc: 0.18338662
case acc: 0.062267188
case acc: 0.09371756
case acc: 0.14170103
case acc: 0.121088184
top acc: 0.2501 ::: bot acc: 0.0275
top acc: 0.2290 ::: bot acc: 0.1415
top acc: 0.1369 ::: bot acc: 0.0341
top acc: 0.1437 ::: bot acc: 0.0621
top acc: 0.2198 ::: bot acc: 0.0637
top acc: 0.1933 ::: bot acc: 0.0417
current epoch: 14
train loss is 0.005895
average val loss: 0.007945, accuracy: 0.1043
average test loss: 0.009694, accuracy: 0.1186
case acc: 0.13095978
case acc: 0.17301632
case acc: 0.06116976
case acc: 0.09236715
case acc: 0.13762622
case acc: 0.11658339
top acc: 0.2482 ::: bot acc: 0.0264
top acc: 0.2186 ::: bot acc: 0.1316
top acc: 0.1335 ::: bot acc: 0.0375
top acc: 0.1395 ::: bot acc: 0.0667
top acc: 0.2159 ::: bot acc: 0.0598
top acc: 0.1869 ::: bot acc: 0.0415
current epoch: 15
train loss is 0.005629
average val loss: 0.008648, accuracy: 0.1098
average test loss: 0.010444, accuracy: 0.1235
case acc: 0.13930309
case acc: 0.17570889
case acc: 0.064338475
case acc: 0.094701275
case acc: 0.1459445
case acc: 0.12087554
top acc: 0.2587 ::: bot acc: 0.0304
top acc: 0.2216 ::: bot acc: 0.1340
top acc: 0.1428 ::: bot acc: 0.0287
top acc: 0.1475 ::: bot acc: 0.0581
top acc: 0.2244 ::: bot acc: 0.0679
top acc: 0.1935 ::: bot acc: 0.0413
current epoch: 16
train loss is 0.005636
average val loss: 0.008771, accuracy: 0.1109
average test loss: 0.010571, accuracy: 0.1244
case acc: 0.143081
case acc: 0.17301196
case acc: 0.065732576
case acc: 0.09554975
case acc: 0.14784665
case acc: 0.121272
top acc: 0.2631 ::: bot acc: 0.0329
top acc: 0.2189 ::: bot acc: 0.1312
top acc: 0.1464 ::: bot acc: 0.0261
top acc: 0.1499 ::: bot acc: 0.0560
top acc: 0.2260 ::: bot acc: 0.0700
top acc: 0.1937 ::: bot acc: 0.0416
current epoch: 17
train loss is 0.005508
average val loss: 0.009071, accuracy: 0.1133
average test loss: 0.010895, accuracy: 0.1266
case acc: 0.14793849
case acc: 0.17226967
case acc: 0.06800082
case acc: 0.09691459
case acc: 0.15137485
case acc: 0.12315468
top acc: 0.2686 ::: bot acc: 0.0365
top acc: 0.2182 ::: bot acc: 0.1306
top acc: 0.1507 ::: bot acc: 0.0232
top acc: 0.1535 ::: bot acc: 0.0520
top acc: 0.2293 ::: bot acc: 0.0736
top acc: 0.1963 ::: bot acc: 0.0418
current epoch: 18
train loss is 0.005488
average val loss: 0.008344, accuracy: 0.1080
average test loss: 0.010067, accuracy: 0.1213
case acc: 0.143803
case acc: 0.1611896
case acc: 0.06556758
case acc: 0.094926424
case acc: 0.14457425
case acc: 0.117729865
top acc: 0.2637 ::: bot acc: 0.0336
top acc: 0.2069 ::: bot acc: 0.1194
top acc: 0.1457 ::: bot acc: 0.0268
top acc: 0.1475 ::: bot acc: 0.0578
top acc: 0.2230 ::: bot acc: 0.0666
top acc: 0.1888 ::: bot acc: 0.0410
current epoch: 19
train loss is 0.005303
average val loss: 0.008648, accuracy: 0.1105
average test loss: 0.010408, accuracy: 0.1236
case acc: 0.1484454
case acc: 0.16121309
case acc: 0.06802768
case acc: 0.09624479
case acc: 0.1480551
case acc: 0.11965612
top acc: 0.2692 ::: bot acc: 0.0368
top acc: 0.2069 ::: bot acc: 0.1198
top acc: 0.1506 ::: bot acc: 0.0236
top acc: 0.1518 ::: bot acc: 0.0540
top acc: 0.2263 ::: bot acc: 0.0699
top acc: 0.1914 ::: bot acc: 0.0414
current epoch: 20
train loss is 0.005311
average val loss: 0.009026, accuracy: 0.1134
average test loss: 0.010796, accuracy: 0.1262
case acc: 0.15351486
case acc: 0.16159639
case acc: 0.07085187
case acc: 0.097656764
case acc: 0.15159324
case acc: 0.121781215
top acc: 0.2745 ::: bot acc: 0.0406
top acc: 0.2075 ::: bot acc: 0.1196
top acc: 0.1559 ::: bot acc: 0.0215
top acc: 0.1563 ::: bot acc: 0.0488
top acc: 0.2296 ::: bot acc: 0.0735
top acc: 0.1945 ::: bot acc: 0.0418
current epoch: 21
train loss is 0.005294
average val loss: 0.008470, accuracy: 0.1093
average test loss: 0.010175, accuracy: 0.1221
case acc: 0.15041953
case acc: 0.15312055
case acc: 0.06879619
case acc: 0.09627262
case acc: 0.14594236
case acc: 0.118163124
top acc: 0.2712 ::: bot acc: 0.0382
top acc: 0.1987 ::: bot acc: 0.1114
top acc: 0.1520 ::: bot acc: 0.0229
top acc: 0.1526 ::: bot acc: 0.0525
top acc: 0.2238 ::: bot acc: 0.0683
top acc: 0.1891 ::: bot acc: 0.0413
current epoch: 22
train loss is 0.005132
average val loss: 0.008309, accuracy: 0.1082
average test loss: 0.009980, accuracy: 0.1208
case acc: 0.15025969
case acc: 0.14838344
case acc: 0.06889708
case acc: 0.09606849
case acc: 0.1443553
case acc: 0.1169481
top acc: 0.2706 ::: bot acc: 0.0382
top acc: 0.1940 ::: bot acc: 0.1067
top acc: 0.1522 ::: bot acc: 0.0231
top acc: 0.1518 ::: bot acc: 0.0531
top acc: 0.2222 ::: bot acc: 0.0666
top acc: 0.1876 ::: bot acc: 0.0410
current epoch: 23
train loss is 0.005031
average val loss: 0.008436, accuracy: 0.1092
average test loss: 0.010099, accuracy: 0.1216
case acc: 0.1528756
case acc: 0.14698803
case acc: 0.07034163
case acc: 0.096782535
case acc: 0.14548728
case acc: 0.11742045
top acc: 0.2739 ::: bot acc: 0.0402
top acc: 0.1926 ::: bot acc: 0.1050
top acc: 0.1544 ::: bot acc: 0.0221
top acc: 0.1534 ::: bot acc: 0.0516
top acc: 0.2235 ::: bot acc: 0.0677
top acc: 0.1883 ::: bot acc: 0.0412
current epoch: 24
train loss is 0.005020
average val loss: 0.008691, accuracy: 0.1112
average test loss: 0.010366, accuracy: 0.1235
case acc: 0.15639971
case acc: 0.14716884
case acc: 0.07264903
case acc: 0.09814112
case acc: 0.14770108
case acc: 0.118754536
top acc: 0.2776 ::: bot acc: 0.0434
top acc: 0.1929 ::: bot acc: 0.1054
top acc: 0.1584 ::: bot acc: 0.0211
top acc: 0.1572 ::: bot acc: 0.0483
top acc: 0.2258 ::: bot acc: 0.0698
top acc: 0.1900 ::: bot acc: 0.0413
current epoch: 25
train loss is 0.005044
average val loss: 0.008702, accuracy: 0.1114
average test loss: 0.010378, accuracy: 0.1235
case acc: 0.15769026
case acc: 0.14520732
case acc: 0.07354751
case acc: 0.09817256
case acc: 0.14788657
case acc: 0.11865249
top acc: 0.2788 ::: bot acc: 0.0443
top acc: 0.1906 ::: bot acc: 0.1037
top acc: 0.1603 ::: bot acc: 0.0207
top acc: 0.1577 ::: bot acc: 0.0471
top acc: 0.2261 ::: bot acc: 0.0700
top acc: 0.1898 ::: bot acc: 0.0413
current epoch: 26
train loss is 0.005021
average val loss: 0.008507, accuracy: 0.1099
average test loss: 0.010143, accuracy: 0.1219
case acc: 0.15674683
case acc: 0.14097369
case acc: 0.0731851
case acc: 0.097773865
case acc: 0.14622027
case acc: 0.11675422
top acc: 0.2779 ::: bot acc: 0.0435
top acc: 0.1864 ::: bot acc: 0.0995
top acc: 0.1597 ::: bot acc: 0.0209
top acc: 0.1562 ::: bot acc: 0.0490
top acc: 0.2244 ::: bot acc: 0.0683
top acc: 0.1873 ::: bot acc: 0.0411
current epoch: 27
train loss is 0.004920
average val loss: 0.008502, accuracy: 0.1098
average test loss: 0.010111, accuracy: 0.1217
case acc: 0.15757546
case acc: 0.13894838
case acc: 0.07365226
case acc: 0.09775813
case acc: 0.14607513
case acc: 0.116177864
top acc: 0.2788 ::: bot acc: 0.0442
top acc: 0.1842 ::: bot acc: 0.0975
top acc: 0.1606 ::: bot acc: 0.0203
top acc: 0.1563 ::: bot acc: 0.0485
top acc: 0.2243 ::: bot acc: 0.0681
top acc: 0.1866 ::: bot acc: 0.0407
current epoch: 28
train loss is 0.004943
average val loss: 0.008989, accuracy: 0.1135
average test loss: 0.010627, accuracy: 0.1253
case acc: 0.16290173
case acc: 0.14185193
case acc: 0.07753911
case acc: 0.09989147
case acc: 0.15083596
case acc: 0.11866436
top acc: 0.2842 ::: bot acc: 0.0493
top acc: 0.1873 ::: bot acc: 0.1002
top acc: 0.1665 ::: bot acc: 0.0202
top acc: 0.1615 ::: bot acc: 0.0445
top acc: 0.2289 ::: bot acc: 0.0730
top acc: 0.1901 ::: bot acc: 0.0411
current epoch: 29
train loss is 0.004981
average val loss: 0.008658, accuracy: 0.1110
average test loss: 0.010251, accuracy: 0.1226
case acc: 0.16036204
case acc: 0.1369237
case acc: 0.07629061
case acc: 0.09865452
case acc: 0.14741626
case acc: 0.1162474
top acc: 0.2819 ::: bot acc: 0.0466
top acc: 0.1824 ::: bot acc: 0.0951
top acc: 0.1648 ::: bot acc: 0.0200
top acc: 0.1587 ::: bot acc: 0.0463
top acc: 0.2254 ::: bot acc: 0.0696
top acc: 0.1866 ::: bot acc: 0.0409
current epoch: 30
train loss is 0.004864
average val loss: 0.008797, accuracy: 0.1121
average test loss: 0.010373, accuracy: 0.1235
case acc: 0.16213864
case acc: 0.13638996
case acc: 0.07788814
case acc: 0.099404514
case acc: 0.14859226
case acc: 0.11678929
top acc: 0.2837 ::: bot acc: 0.0484
top acc: 0.1817 ::: bot acc: 0.0948
top acc: 0.1671 ::: bot acc: 0.0205
top acc: 0.1603 ::: bot acc: 0.0452
top acc: 0.2268 ::: bot acc: 0.0707
top acc: 0.1874 ::: bot acc: 0.0412
current epoch: 31
train loss is 0.004899
average val loss: 0.008381, accuracy: 0.1088
average test loss: 0.009925, accuracy: 0.1204
case acc: 0.15875164
case acc: 0.13077742
case acc: 0.075732954
case acc: 0.09827845
case acc: 0.14451304
case acc: 0.114238635
top acc: 0.2803 ::: bot acc: 0.0451
top acc: 0.1764 ::: bot acc: 0.0891
top acc: 0.1641 ::: bot acc: 0.0203
top acc: 0.1571 ::: bot acc: 0.0479
top acc: 0.2224 ::: bot acc: 0.0666
top acc: 0.1834 ::: bot acc: 0.0413
current epoch: 32
train loss is 0.004753
average val loss: 0.008143, accuracy: 0.1070
average test loss: 0.009654, accuracy: 0.1184
case acc: 0.15680137
case acc: 0.12677214
case acc: 0.07455555
case acc: 0.09758347
case acc: 0.14205678
case acc: 0.11261828
top acc: 0.2785 ::: bot acc: 0.0433
top acc: 0.1723 ::: bot acc: 0.0851
top acc: 0.1627 ::: bot acc: 0.0201
top acc: 0.1554 ::: bot acc: 0.0495
top acc: 0.2203 ::: bot acc: 0.0642
top acc: 0.1812 ::: bot acc: 0.0413
current epoch: 33
train loss is 0.004739
average val loss: 0.008978, accuracy: 0.1133
average test loss: 0.010559, accuracy: 0.1247
case acc: 0.16501322
case acc: 0.13373324
case acc: 0.08074844
case acc: 0.10121232
case acc: 0.15000418
case acc: 0.117617086
top acc: 0.2873 ::: bot acc: 0.0505
top acc: 0.1794 ::: bot acc: 0.0920
top acc: 0.1719 ::: bot acc: 0.0205
top acc: 0.1638 ::: bot acc: 0.0432
top acc: 0.2284 ::: bot acc: 0.0720
top acc: 0.1887 ::: bot acc: 0.0411
current epoch: 34
train loss is 0.004818
average val loss: 0.009440, accuracy: 0.1168
average test loss: 0.011048, accuracy: 0.1281
case acc: 0.16931784
case acc: 0.13694063
case acc: 0.08469044
case acc: 0.103278995
case acc: 0.15390335
case acc: 0.120431446
top acc: 0.2916 ::: bot acc: 0.0547
top acc: 0.1824 ::: bot acc: 0.0953
top acc: 0.1771 ::: bot acc: 0.0217
top acc: 0.1685 ::: bot acc: 0.0397
top acc: 0.2324 ::: bot acc: 0.0757
top acc: 0.1928 ::: bot acc: 0.0413
current epoch: 35
train loss is 0.004924
average val loss: 0.008785, accuracy: 0.1118
average test loss: 0.010309, accuracy: 0.1230
case acc: 0.1632273
case acc: 0.12914069
case acc: 0.080403216
case acc: 0.10067821
case acc: 0.1479505
case acc: 0.11644694
top acc: 0.2852 ::: bot acc: 0.0489
top acc: 0.1747 ::: bot acc: 0.0874
top acc: 0.1711 ::: bot acc: 0.0206
top acc: 0.1629 ::: bot acc: 0.0431
top acc: 0.2262 ::: bot acc: 0.0700
top acc: 0.1871 ::: bot acc: 0.0409
current epoch: 36
train loss is 0.004753
average val loss: 0.008270, accuracy: 0.1077
average test loss: 0.009713, accuracy: 0.1187
case acc: 0.15845776
case acc: 0.12202199
case acc: 0.07669283
case acc: 0.0986896
case acc: 0.14320144
case acc: 0.11323047
top acc: 0.2800 ::: bot acc: 0.0449
top acc: 0.1674 ::: bot acc: 0.0806
top acc: 0.1660 ::: bot acc: 0.0198
top acc: 0.1583 ::: bot acc: 0.0466
top acc: 0.2214 ::: bot acc: 0.0654
top acc: 0.1824 ::: bot acc: 0.0410
current epoch: 37
train loss is 0.004642
average val loss: 0.008461, accuracy: 0.1091
average test loss: 0.009928, accuracy: 0.1202
case acc: 0.16043448
case acc: 0.12304671
case acc: 0.07838057
case acc: 0.099582985
case acc: 0.14513081
case acc: 0.11449125
top acc: 0.2823 ::: bot acc: 0.0463
top acc: 0.1682 ::: bot acc: 0.0816
top acc: 0.1684 ::: bot acc: 0.0199
top acc: 0.1607 ::: bot acc: 0.0446
top acc: 0.2237 ::: bot acc: 0.0670
top acc: 0.1844 ::: bot acc: 0.0404
current epoch: 38
train loss is 0.004688
average val loss: 0.008196, accuracy: 0.1071
average test loss: 0.009635, accuracy: 0.1181
case acc: 0.15775234
case acc: 0.119772494
case acc: 0.07703589
case acc: 0.09880549
case acc: 0.14212674
case acc: 0.11289737
top acc: 0.2796 ::: bot acc: 0.0437
top acc: 0.1652 ::: bot acc: 0.0781
top acc: 0.1668 ::: bot acc: 0.0198
top acc: 0.1586 ::: bot acc: 0.0460
top acc: 0.2207 ::: bot acc: 0.0641
top acc: 0.1820 ::: bot acc: 0.0410
current epoch: 39
train loss is 0.004668
average val loss: 0.008535, accuracy: 0.1097
average test loss: 0.010001, accuracy: 0.1207
case acc: 0.16111487
case acc: 0.122304514
case acc: 0.079879045
case acc: 0.10055699
case acc: 0.14538732
case acc: 0.11479571
top acc: 0.2833 ::: bot acc: 0.0467
top acc: 0.1677 ::: bot acc: 0.0806
top acc: 0.1707 ::: bot acc: 0.0203
top acc: 0.1624 ::: bot acc: 0.0437
top acc: 0.2240 ::: bot acc: 0.0672
top acc: 0.1850 ::: bot acc: 0.0406
current epoch: 40
train loss is 0.004706
average val loss: 0.008801, accuracy: 0.1118
average test loss: 0.010270, accuracy: 0.1226
case acc: 0.16360697
case acc: 0.12418561
case acc: 0.08223014
case acc: 0.101603545
case acc: 0.14791541
case acc: 0.11625627
top acc: 0.2856 ::: bot acc: 0.0494
top acc: 0.1698 ::: bot acc: 0.0823
top acc: 0.1739 ::: bot acc: 0.0208
top acc: 0.1649 ::: bot acc: 0.0418
top acc: 0.2263 ::: bot acc: 0.0698
top acc: 0.1870 ::: bot acc: 0.0407
current epoch: 41
train loss is 0.004713
average val loss: 0.008960, accuracy: 0.1129
average test loss: 0.010431, accuracy: 0.1238
case acc: 0.16489719
case acc: 0.12491461
case acc: 0.08387665
case acc: 0.10241468
case acc: 0.14977399
case acc: 0.11698201
top acc: 0.2868 ::: bot acc: 0.0507
top acc: 0.1704 ::: bot acc: 0.0832
top acc: 0.1760 ::: bot acc: 0.0217
top acc: 0.1666 ::: bot acc: 0.0406
top acc: 0.2281 ::: bot acc: 0.0716
top acc: 0.1880 ::: bot acc: 0.0408
current epoch: 42
train loss is 0.004717
average val loss: 0.008669, accuracy: 0.1107
average test loss: 0.010101, accuracy: 0.1214
case acc: 0.16202238
case acc: 0.12102514
case acc: 0.081914715
case acc: 0.10131026
case acc: 0.14722842
case acc: 0.11518235
top acc: 0.2840 ::: bot acc: 0.0478
top acc: 0.1665 ::: bot acc: 0.0791
top acc: 0.1734 ::: bot acc: 0.0210
top acc: 0.1639 ::: bot acc: 0.0422
top acc: 0.2255 ::: bot acc: 0.0692
top acc: 0.1853 ::: bot acc: 0.0409
current epoch: 43
train loss is 0.004641
average val loss: 0.007945, accuracy: 0.1049
average test loss: 0.009290, accuracy: 0.1156
case acc: 0.15513317
case acc: 0.11244437
case acc: 0.07665351
case acc: 0.09821042
case acc: 0.14033478
case acc: 0.11063799
top acc: 0.2764 ::: bot acc: 0.0420
top acc: 0.1577 ::: bot acc: 0.0709
top acc: 0.1662 ::: bot acc: 0.0202
top acc: 0.1568 ::: bot acc: 0.0473
top acc: 0.2184 ::: bot acc: 0.0625
top acc: 0.1785 ::: bot acc: 0.0411
current epoch: 44
train loss is 0.004540
average val loss: 0.008351, accuracy: 0.1081
average test loss: 0.009719, accuracy: 0.1187
case acc: 0.15859139
case acc: 0.11586465
case acc: 0.07967606
case acc: 0.10009904
case acc: 0.14451429
case acc: 0.11326232
top acc: 0.2800 ::: bot acc: 0.0450
top acc: 0.1611 ::: bot acc: 0.0743
top acc: 0.1704 ::: bot acc: 0.0203
top acc: 0.1612 ::: bot acc: 0.0439
top acc: 0.2228 ::: bot acc: 0.0664
top acc: 0.1826 ::: bot acc: 0.0407
current epoch: 45
train loss is 0.004585
average val loss: 0.008700, accuracy: 0.1108
average test loss: 0.010097, accuracy: 0.1214
case acc: 0.16167784
case acc: 0.11919524
case acc: 0.082640916
case acc: 0.101777196
case acc: 0.1476072
case acc: 0.11536711
top acc: 0.2834 ::: bot acc: 0.0477
top acc: 0.1645 ::: bot acc: 0.0777
top acc: 0.1745 ::: bot acc: 0.0211
top acc: 0.1649 ::: bot acc: 0.0414
top acc: 0.2260 ::: bot acc: 0.0696
top acc: 0.1861 ::: bot acc: 0.0403
current epoch: 46
train loss is 0.004610
average val loss: 0.008565, accuracy: 0.1097
average test loss: 0.009932, accuracy: 0.1202
case acc: 0.1600977
case acc: 0.117464446
case acc: 0.08204903
case acc: 0.10125752
case acc: 0.14593074
case acc: 0.11448636
top acc: 0.2818 ::: bot acc: 0.0462
top acc: 0.1626 ::: bot acc: 0.0758
top acc: 0.1739 ::: bot acc: 0.0210
top acc: 0.1637 ::: bot acc: 0.0421
top acc: 0.2244 ::: bot acc: 0.0677
top acc: 0.1844 ::: bot acc: 0.0407
current epoch: 47
train loss is 0.004605
average val loss: 0.008563, accuracy: 0.1097
average test loss: 0.009933, accuracy: 0.1202
case acc: 0.15994278
case acc: 0.11716895
case acc: 0.082339376
case acc: 0.101457246
case acc: 0.14595036
case acc: 0.114398554
top acc: 0.2818 ::: bot acc: 0.0460
top acc: 0.1626 ::: bot acc: 0.0753
top acc: 0.1744 ::: bot acc: 0.0210
top acc: 0.1639 ::: bot acc: 0.0421
top acc: 0.2243 ::: bot acc: 0.0678
top acc: 0.1844 ::: bot acc: 0.0404
current epoch: 48
train loss is 0.004587
average val loss: 0.008431, accuracy: 0.1086
average test loss: 0.009788, accuracy: 0.1192
case acc: 0.15840325
case acc: 0.1153429
case acc: 0.08159681
case acc: 0.10098711
case acc: 0.14489475
case acc: 0.11383583
top acc: 0.2802 ::: bot acc: 0.0447
top acc: 0.1608 ::: bot acc: 0.0734
top acc: 0.1735 ::: bot acc: 0.0209
top acc: 0.1627 ::: bot acc: 0.0426
top acc: 0.2233 ::: bot acc: 0.0668
top acc: 0.1833 ::: bot acc: 0.0411
current epoch: 49
train loss is 0.004543
average val loss: 0.008727, accuracy: 0.1109
average test loss: 0.010101, accuracy: 0.1214
case acc: 0.16107105
case acc: 0.11778195
case acc: 0.08384718
case acc: 0.102285296
case acc: 0.14785996
case acc: 0.115277246
top acc: 0.2830 ::: bot acc: 0.0469
top acc: 0.1631 ::: bot acc: 0.0758
top acc: 0.1765 ::: bot acc: 0.0212
top acc: 0.1658 ::: bot acc: 0.0406
top acc: 0.2264 ::: bot acc: 0.0696
top acc: 0.1859 ::: bot acc: 0.0404
current epoch: 50
train loss is 0.004566
average val loss: 0.008496, accuracy: 0.1090
average test loss: 0.009823, accuracy: 0.1194
case acc: 0.15859151
case acc: 0.11501941
case acc: 0.08229616
case acc: 0.101281375
case acc: 0.14538452
case acc: 0.11370469
top acc: 0.2805 ::: bot acc: 0.0447
top acc: 0.1604 ::: bot acc: 0.0732
top acc: 0.1745 ::: bot acc: 0.0208
top acc: 0.1635 ::: bot acc: 0.0420
top acc: 0.2237 ::: bot acc: 0.0671
top acc: 0.1836 ::: bot acc: 0.0406

		{"drop_out": 0.4, "drop_out_mc": 0.1, "repeat_mc": 50, "hidden": 30, "embedding_size": 5, "batch": 512, "lag": 3}
LME_Co_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5112 5112 5112
1.7082474 -0.6288155 0.48142856 -0.33910522
Validation: 570 570 570
Testing: 774 774 774
pre-processing time: 0.00020241737365722656
the split date is 2010-07-01
train dropout: 0.4 test dropout: 0.1
net initializing with time: 0.0036454200744628906
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012857
average val loss: 0.008286, accuracy: 0.1102
average test loss: 0.016943, accuracy: 0.1702
case acc: 0.23394044
case acc: 0.048032124
case acc: 0.21743852
case acc: 0.18302166
case acc: 0.18394591
case acc: 0.15474398
top acc: 0.1889 ::: bot acc: 0.2935
top acc: 0.0812 ::: bot acc: 0.0246
top acc: 0.1569 ::: bot acc: 0.2724
top acc: 0.1353 ::: bot acc: 0.2242
top acc: 0.1505 ::: bot acc: 0.2165
top acc: 0.1177 ::: bot acc: 0.1879
current epoch: 2
train loss is 0.011409
average val loss: 0.010486, accuracy: 0.1076
average test loss: 0.005005, accuracy: 0.0856
case acc: 0.10305928
case acc: 0.14653774
case acc: 0.08969462
case acc: 0.064469144
case acc: 0.06513558
case acc: 0.044704452
top acc: 0.0584 ::: bot acc: 0.1626
top acc: 0.1952 ::: bot acc: 0.0909
top acc: 0.0330 ::: bot acc: 0.1428
top acc: 0.0208 ::: bot acc: 0.1042
top acc: 0.0314 ::: bot acc: 0.0966
top acc: 0.0144 ::: bot acc: 0.0752
current epoch: 3
train loss is 0.012650
average val loss: 0.022454, accuracy: 0.1785
average test loss: 0.005854, accuracy: 0.0743
case acc: 0.04079651
case acc: 0.23508841
case acc: 0.044423502
case acc: 0.040655617
case acc: 0.03516052
case acc: 0.049726725
top acc: 0.0452 ::: bot acc: 0.0590
top acc: 0.2832 ::: bot acc: 0.1794
top acc: 0.0767 ::: bot acc: 0.0410
top acc: 0.0825 ::: bot acc: 0.0126
top acc: 0.0662 ::: bot acc: 0.0111
top acc: 0.0872 ::: bot acc: 0.0186
current epoch: 4
train loss is 0.015466
average val loss: 0.014045, accuracy: 0.1298
average test loss: 0.003713, accuracy: 0.0629
case acc: 0.0516037
case acc: 0.16949987
case acc: 0.054015692
case acc: 0.040167153
case acc: 0.03232539
case acc: 0.03003205
top acc: 0.0101 ::: bot acc: 0.1100
top acc: 0.2176 ::: bot acc: 0.1138
top acc: 0.0255 ::: bot acc: 0.0944
top acc: 0.0259 ::: bot acc: 0.0654
top acc: 0.0183 ::: bot acc: 0.0543
top acc: 0.0270 ::: bot acc: 0.0471
current epoch: 5
train loss is 0.013609
average val loss: 0.006627, accuracy: 0.0906
average test loss: 0.009083, accuracy: 0.1242
case acc: 0.1577696
case acc: 0.05567472
case acc: 0.1498836
case acc: 0.13402545
case acc: 0.124002665
case acc: 0.124046594
top acc: 0.1134 ::: bot acc: 0.2177
top acc: 0.0964 ::: bot acc: 0.0154
top acc: 0.0878 ::: bot acc: 0.2069
top acc: 0.0841 ::: bot acc: 0.1770
top acc: 0.0891 ::: bot acc: 0.1565
top acc: 0.0841 ::: bot acc: 0.1595
current epoch: 6
train loss is 0.008310
average val loss: 0.007125, accuracy: 0.0871
average test loss: 0.005759, accuracy: 0.0973
case acc: 0.12042037
case acc: 0.07407259
case acc: 0.11763694
case acc: 0.100697815
case acc: 0.080554396
case acc: 0.09026643
top acc: 0.0762 ::: bot acc: 0.1804
top acc: 0.1208 ::: bot acc: 0.0221
top acc: 0.0553 ::: bot acc: 0.1743
top acc: 0.0509 ::: bot acc: 0.1437
top acc: 0.0460 ::: bot acc: 0.1127
top acc: 0.0514 ::: bot acc: 0.1252
current epoch: 7
train loss is 0.008020
average val loss: 0.009011, accuracy: 0.0980
average test loss: 0.003676, accuracy: 0.0733
case acc: 0.0823913
case acc: 0.10052824
case acc: 0.08692904
case acc: 0.06844731
case acc: 0.043509576
case acc: 0.05776014
top acc: 0.0382 ::: bot acc: 0.1423
top acc: 0.1492 ::: bot acc: 0.0438
top acc: 0.0298 ::: bot acc: 0.1415
top acc: 0.0228 ::: bot acc: 0.1089
top acc: 0.0169 ::: bot acc: 0.0717
top acc: 0.0218 ::: bot acc: 0.0916
current epoch: 8
train loss is 0.008488
average val loss: 0.007579, accuracy: 0.0880
average test loss: 0.004352, accuracy: 0.0820
case acc: 0.097268686
case acc: 0.07705125
case acc: 0.102926
case acc: 0.08542211
case acc: 0.05348602
case acc: 0.07595135
top acc: 0.0532 ::: bot acc: 0.1568
top acc: 0.1240 ::: bot acc: 0.0240
top acc: 0.0417 ::: bot acc: 0.1596
top acc: 0.0359 ::: bot acc: 0.1279
top acc: 0.0219 ::: bot acc: 0.0843
top acc: 0.0381 ::: bot acc: 0.1103
current epoch: 9
train loss is 0.007906
average val loss: 0.006923, accuracy: 0.0839
average test loss: 0.004719, accuracy: 0.0857
case acc: 0.1036545
case acc: 0.06388939
case acc: 0.11151207
case acc: 0.09350814
case acc: 0.056938387
case acc: 0.08478156
top acc: 0.0598 ::: bot acc: 0.1632
top acc: 0.1078 ::: bot acc: 0.0167
top acc: 0.0497 ::: bot acc: 0.1687
top acc: 0.0436 ::: bot acc: 0.1366
top acc: 0.0238 ::: bot acc: 0.0885
top acc: 0.0461 ::: bot acc: 0.1196
current epoch: 10
train loss is 0.007464
average val loss: 0.007096, accuracy: 0.0846
average test loss: 0.004234, accuracy: 0.0801
case acc: 0.09511451
case acc: 0.06338644
case acc: 0.10623026
case acc: 0.08799688
case acc: 0.048711807
case acc: 0.07927983
top acc: 0.0508 ::: bot acc: 0.1551
top acc: 0.1078 ::: bot acc: 0.0160
top acc: 0.0445 ::: bot acc: 0.1632
top acc: 0.0386 ::: bot acc: 0.1311
top acc: 0.0195 ::: bot acc: 0.0781
top acc: 0.0408 ::: bot acc: 0.1144
current epoch: 11
train loss is 0.007316
average val loss: 0.007311, accuracy: 0.0862
average test loss: 0.003822, accuracy: 0.0752
case acc: 0.087488726
case acc: 0.063751005
case acc: 0.10095586
case acc: 0.08245392
case acc: 0.041938405
case acc: 0.07461653
top acc: 0.0432 ::: bot acc: 0.1478
top acc: 0.1076 ::: bot acc: 0.0159
top acc: 0.0399 ::: bot acc: 0.1575
top acc: 0.0331 ::: bot acc: 0.1256
top acc: 0.0167 ::: bot acc: 0.0698
top acc: 0.0367 ::: bot acc: 0.1093
current epoch: 12
train loss is 0.007301
average val loss: 0.007389, accuracy: 0.0866
average test loss: 0.003603, accuracy: 0.0725
case acc: 0.08312347
case acc: 0.061692793
case acc: 0.09857722
case acc: 0.07995791
case acc: 0.039044697
case acc: 0.072568215
top acc: 0.0391 ::: bot acc: 0.1427
top acc: 0.1055 ::: bot acc: 0.0150
top acc: 0.0379 ::: bot acc: 0.1549
top acc: 0.0308 ::: bot acc: 0.1226
top acc: 0.0168 ::: bot acc: 0.0651
top acc: 0.0348 ::: bot acc: 0.1071
current epoch: 13
train loss is 0.007245
average val loss: 0.007307, accuracy: 0.0860
average test loss: 0.003522, accuracy: 0.0714
case acc: 0.081118226
case acc: 0.058454018
case acc: 0.09807447
case acc: 0.07971621
case acc: 0.03805574
case acc: 0.07305055
top acc: 0.0368 ::: bot acc: 0.1412
top acc: 0.1006 ::: bot acc: 0.0152
top acc: 0.0376 ::: bot acc: 0.1547
top acc: 0.0305 ::: bot acc: 0.1227
top acc: 0.0169 ::: bot acc: 0.0635
top acc: 0.0351 ::: bot acc: 0.1075
current epoch: 14
train loss is 0.007108
average val loss: 0.007146, accuracy: 0.0847
average test loss: 0.003511, accuracy: 0.0713
case acc: 0.08090858
case acc: 0.05535036
case acc: 0.09859126
case acc: 0.08024859
case acc: 0.038269315
case acc: 0.07462247
top acc: 0.0370 ::: bot acc: 0.1408
top acc: 0.0954 ::: bot acc: 0.0165
top acc: 0.0376 ::: bot acc: 0.1552
top acc: 0.0313 ::: bot acc: 0.1232
top acc: 0.0173 ::: bot acc: 0.0638
top acc: 0.0367 ::: bot acc: 0.1092
current epoch: 15
train loss is 0.006962
average val loss: 0.007094, accuracy: 0.0841
average test loss: 0.003449, accuracy: 0.0706
case acc: 0.07965395
case acc: 0.053256385
case acc: 0.09793585
case acc: 0.07992758
case acc: 0.0377432
case acc: 0.07502278
top acc: 0.0358 ::: bot acc: 0.1393
top acc: 0.0909 ::: bot acc: 0.0186
top acc: 0.0371 ::: bot acc: 0.1543
top acc: 0.0306 ::: bot acc: 0.1230
top acc: 0.0173 ::: bot acc: 0.0629
top acc: 0.0372 ::: bot acc: 0.1096
current epoch: 16
train loss is 0.006905
average val loss: 0.007074, accuracy: 0.0838
average test loss: 0.003367, accuracy: 0.0696
case acc: 0.077439114
case acc: 0.05151477
case acc: 0.09671306
case acc: 0.079335794
case acc: 0.03731521
case acc: 0.07530325
top acc: 0.0332 ::: bot acc: 0.1373
top acc: 0.0880 ::: bot acc: 0.0202
top acc: 0.0362 ::: bot acc: 0.1529
top acc: 0.0304 ::: bot acc: 0.1222
top acc: 0.0175 ::: bot acc: 0.0624
top acc: 0.0374 ::: bot acc: 0.1101
current epoch: 17
train loss is 0.006806
average val loss: 0.007096, accuracy: 0.0837
average test loss: 0.003278, accuracy: 0.0685
case acc: 0.075478084
case acc: 0.050491888
case acc: 0.09543465
case acc: 0.078062765
case acc: 0.03644151
case acc: 0.0749853
top acc: 0.0314 ::: bot acc: 0.1354
top acc: 0.0848 ::: bot acc: 0.0222
top acc: 0.0352 ::: bot acc: 0.1520
top acc: 0.0291 ::: bot acc: 0.1210
top acc: 0.0168 ::: bot acc: 0.0614
top acc: 0.0373 ::: bot acc: 0.1096
current epoch: 18
train loss is 0.006797
average val loss: 0.007305, accuracy: 0.0852
average test loss: 0.003055, accuracy: 0.0657
case acc: 0.07085067
case acc: 0.050982434
case acc: 0.091304965
case acc: 0.07448631
case acc: 0.034637768
case acc: 0.072019845
top acc: 0.0269 ::: bot acc: 0.1308
top acc: 0.0863 ::: bot acc: 0.0217
top acc: 0.0323 ::: bot acc: 0.1471
top acc: 0.0261 ::: bot acc: 0.1171
top acc: 0.0179 ::: bot acc: 0.0579
top acc: 0.0347 ::: bot acc: 0.1062
current epoch: 19
train loss is 0.006795
average val loss: 0.007469, accuracy: 0.0864
average test loss: 0.002896, accuracy: 0.0637
case acc: 0.06757195
case acc: 0.05106114
case acc: 0.0879487
case acc: 0.0720808
case acc: 0.03326962
case acc: 0.070302784
top acc: 0.0237 ::: bot acc: 0.1275
top acc: 0.0859 ::: bot acc: 0.0220
top acc: 0.0299 ::: bot acc: 0.1434
top acc: 0.0247 ::: bot acc: 0.1143
top acc: 0.0189 ::: bot acc: 0.0555
top acc: 0.0331 ::: bot acc: 0.1046
current epoch: 20
train loss is 0.006774
average val loss: 0.007525, accuracy: 0.0867
average test loss: 0.002824, accuracy: 0.0628
case acc: 0.06576918
case acc: 0.0502903
case acc: 0.08642037
case acc: 0.07083417
case acc: 0.033154428
case acc: 0.07046337
top acc: 0.0222 ::: bot acc: 0.1254
top acc: 0.0845 ::: bot acc: 0.0230
top acc: 0.0290 ::: bot acc: 0.1415
top acc: 0.0237 ::: bot acc: 0.1129
top acc: 0.0187 ::: bot acc: 0.0555
top acc: 0.0332 ::: bot acc: 0.1049
current epoch: 21
train loss is 0.006721
average val loss: 0.007434, accuracy: 0.0857
average test loss: 0.002839, accuracy: 0.0631
case acc: 0.06599418
case acc: 0.049082533
case acc: 0.08646945
case acc: 0.07150807
case acc: 0.033732608
case acc: 0.07183507
top acc: 0.0223 ::: bot acc: 0.1256
top acc: 0.0814 ::: bot acc: 0.0256
top acc: 0.0290 ::: bot acc: 0.1416
top acc: 0.0245 ::: bot acc: 0.1133
top acc: 0.0184 ::: bot acc: 0.0565
top acc: 0.0345 ::: bot acc: 0.1062
current epoch: 22
train loss is 0.006666
average val loss: 0.007285, accuracy: 0.0843
average test loss: 0.002914, accuracy: 0.0642
case acc: 0.06769361
case acc: 0.047298905
case acc: 0.08734465
case acc: 0.073210515
case acc: 0.035138775
case acc: 0.07423078
top acc: 0.0240 ::: bot acc: 0.1273
top acc: 0.0769 ::: bot acc: 0.0295
top acc: 0.0295 ::: bot acc: 0.1424
top acc: 0.0252 ::: bot acc: 0.1159
top acc: 0.0177 ::: bot acc: 0.0589
top acc: 0.0367 ::: bot acc: 0.1087
current epoch: 23
train loss is 0.006603
average val loss: 0.007456, accuracy: 0.0855
average test loss: 0.002759, accuracy: 0.0621
case acc: 0.06445356
case acc: 0.04738551
case acc: 0.084445775
case acc: 0.070822574
case acc: 0.03343375
case acc: 0.07186191
top acc: 0.0208 ::: bot acc: 0.1242
top acc: 0.0771 ::: bot acc: 0.0287
top acc: 0.0274 ::: bot acc: 0.1392
top acc: 0.0240 ::: bot acc: 0.1127
top acc: 0.0180 ::: bot acc: 0.0562
top acc: 0.0346 ::: bot acc: 0.1061
current epoch: 24
train loss is 0.006592
average val loss: 0.007325, accuracy: 0.0843
average test loss: 0.002830, accuracy: 0.0631
case acc: 0.06578419
case acc: 0.046242107
case acc: 0.08545376
case acc: 0.071958885
case acc: 0.03483891
case acc: 0.074168004
top acc: 0.0222 ::: bot acc: 0.1252
top acc: 0.0737 ::: bot acc: 0.0325
top acc: 0.0280 ::: bot acc: 0.1406
top acc: 0.0243 ::: bot acc: 0.1143
top acc: 0.0181 ::: bot acc: 0.0583
top acc: 0.0367 ::: bot acc: 0.1087
current epoch: 25
train loss is 0.006534
average val loss: 0.007449, accuracy: 0.0851
average test loss: 0.002744, accuracy: 0.0619
case acc: 0.0639681
case acc: 0.04636329
case acc: 0.08342797
case acc: 0.070477806
case acc: 0.034243777
case acc: 0.07311858
top acc: 0.0201 ::: bot acc: 0.1239
top acc: 0.0740 ::: bot acc: 0.0322
top acc: 0.0267 ::: bot acc: 0.1381
top acc: 0.0235 ::: bot acc: 0.1127
top acc: 0.0183 ::: bot acc: 0.0575
top acc: 0.0357 ::: bot acc: 0.1074
current epoch: 26
train loss is 0.006545
average val loss: 0.007602, accuracy: 0.0863
average test loss: 0.002627, accuracy: 0.0604
case acc: 0.06159278
case acc: 0.046564456
case acc: 0.08061065
case acc: 0.068626605
case acc: 0.033285897
case acc: 0.071761526
top acc: 0.0180 ::: bot acc: 0.1213
top acc: 0.0746 ::: bot acc: 0.0311
top acc: 0.0251 ::: bot acc: 0.1349
top acc: 0.0224 ::: bot acc: 0.1103
top acc: 0.0186 ::: bot acc: 0.0556
top acc: 0.0345 ::: bot acc: 0.1061
current epoch: 27
train loss is 0.006535
average val loss: 0.007684, accuracy: 0.0867
average test loss: 0.002572, accuracy: 0.0597
case acc: 0.06039325
case acc: 0.046334673
case acc: 0.07929063
case acc: 0.06742122
case acc: 0.033145025
case acc: 0.07147827
top acc: 0.0169 ::: bot acc: 0.1198
top acc: 0.0741 ::: bot acc: 0.0314
top acc: 0.0246 ::: bot acc: 0.1332
top acc: 0.0215 ::: bot acc: 0.1091
top acc: 0.0189 ::: bot acc: 0.0552
top acc: 0.0339 ::: bot acc: 0.1060
current epoch: 28
train loss is 0.006532
average val loss: 0.007691, accuracy: 0.0866
average test loss: 0.002570, accuracy: 0.0597
case acc: 0.060434487
case acc: 0.04618862
case acc: 0.07851792
case acc: 0.06741438
case acc: 0.033256847
case acc: 0.072260976
top acc: 0.0169 ::: bot acc: 0.1200
top acc: 0.0736 ::: bot acc: 0.0324
top acc: 0.0239 ::: bot acc: 0.1323
top acc: 0.0216 ::: bot acc: 0.1090
top acc: 0.0184 ::: bot acc: 0.0559
top acc: 0.0350 ::: bot acc: 0.1067
current epoch: 29
train loss is 0.006533
average val loss: 0.007934, accuracy: 0.0884
average test loss: 0.002419, accuracy: 0.0576
case acc: 0.057194356
case acc: 0.04680877
case acc: 0.07522515
case acc: 0.0648177
case acc: 0.03165104
case acc: 0.06968474
top acc: 0.0141 ::: bot acc: 0.1167
top acc: 0.0755 ::: bot acc: 0.0306
top acc: 0.0222 ::: bot acc: 0.1281
top acc: 0.0201 ::: bot acc: 0.1058
top acc: 0.0196 ::: bot acc: 0.0527
top acc: 0.0327 ::: bot acc: 0.1039
current epoch: 30
train loss is 0.006518
average val loss: 0.007852, accuracy: 0.0876
average test loss: 0.002472, accuracy: 0.0583
case acc: 0.058360606
case acc: 0.045960683
case acc: 0.07579134
case acc: 0.06587893
case acc: 0.03246996
case acc: 0.071382344
top acc: 0.0150 ::: bot acc: 0.1179
top acc: 0.0729 ::: bot acc: 0.0327
top acc: 0.0225 ::: bot acc: 0.1290
top acc: 0.0206 ::: bot acc: 0.1072
top acc: 0.0188 ::: bot acc: 0.0544
top acc: 0.0341 ::: bot acc: 0.1058
current epoch: 31
train loss is 0.006474
average val loss: 0.007751, accuracy: 0.0866
average test loss: 0.002517, accuracy: 0.0590
case acc: 0.059634484
case acc: 0.045123033
case acc: 0.07652448
case acc: 0.06680082
case acc: 0.032842916
case acc: 0.0728728
top acc: 0.0161 ::: bot acc: 0.1191
top acc: 0.0705 ::: bot acc: 0.0353
top acc: 0.0230 ::: bot acc: 0.1296
top acc: 0.0210 ::: bot acc: 0.1086
top acc: 0.0189 ::: bot acc: 0.0548
top acc: 0.0354 ::: bot acc: 0.1074
current epoch: 32
train loss is 0.006453
average val loss: 0.007758, accuracy: 0.0866
average test loss: 0.002512, accuracy: 0.0589
case acc: 0.05987228
case acc: 0.044814575
case acc: 0.07615704
case acc: 0.06685798
case acc: 0.032633644
case acc: 0.073012635
top acc: 0.0164 ::: bot acc: 0.1194
top acc: 0.0693 ::: bot acc: 0.0367
top acc: 0.0225 ::: bot acc: 0.1295
top acc: 0.0212 ::: bot acc: 0.1086
top acc: 0.0190 ::: bot acc: 0.0545
top acc: 0.0356 ::: bot acc: 0.1075
current epoch: 33
train loss is 0.006435
average val loss: 0.007717, accuracy: 0.0861
average test loss: 0.002530, accuracy: 0.0591
case acc: 0.060317144
case acc: 0.04420523
case acc: 0.07644621
case acc: 0.067193456
case acc: 0.032934505
case acc: 0.07366413
top acc: 0.0167 ::: bot acc: 0.1201
top acc: 0.0674 ::: bot acc: 0.0385
top acc: 0.0228 ::: bot acc: 0.1298
top acc: 0.0213 ::: bot acc: 0.1088
top acc: 0.0189 ::: bot acc: 0.0551
top acc: 0.0363 ::: bot acc: 0.1081
current epoch: 34
train loss is 0.006405
average val loss: 0.007629, accuracy: 0.0852
average test loss: 0.002581, accuracy: 0.0599
case acc: 0.061725885
case acc: 0.04363267
case acc: 0.076933265
case acc: 0.06822348
case acc: 0.033775274
case acc: 0.075036444
top acc: 0.0182 ::: bot acc: 0.1216
top acc: 0.0655 ::: bot acc: 0.0405
top acc: 0.0232 ::: bot acc: 0.1304
top acc: 0.0219 ::: bot acc: 0.1100
top acc: 0.0187 ::: bot acc: 0.0562
top acc: 0.0373 ::: bot acc: 0.1096
current epoch: 35
train loss is 0.006382
average val loss: 0.007621, accuracy: 0.0851
average test loss: 0.002588, accuracy: 0.0600
case acc: 0.062335767
case acc: 0.04331375
case acc: 0.076847054
case acc: 0.06831932
case acc: 0.033837948
case acc: 0.07538473
top acc: 0.0190 ::: bot acc: 0.1221
top acc: 0.0641 ::: bot acc: 0.0416
top acc: 0.0231 ::: bot acc: 0.1303
top acc: 0.0219 ::: bot acc: 0.1104
top acc: 0.0185 ::: bot acc: 0.0566
top acc: 0.0378 ::: bot acc: 0.1098
current epoch: 36
train loss is 0.006366
average val loss: 0.007667, accuracy: 0.0854
average test loss: 0.002560, accuracy: 0.0596
case acc: 0.061992742
case acc: 0.043394405
case acc: 0.07617635
case acc: 0.06755934
case acc: 0.033724427
case acc: 0.07490214
top acc: 0.0184 ::: bot acc: 0.1216
top acc: 0.0642 ::: bot acc: 0.0417
top acc: 0.0228 ::: bot acc: 0.1293
top acc: 0.0214 ::: bot acc: 0.1095
top acc: 0.0187 ::: bot acc: 0.0563
top acc: 0.0372 ::: bot acc: 0.1096
current epoch: 37
train loss is 0.006351
average val loss: 0.007675, accuracy: 0.0853
average test loss: 0.002561, accuracy: 0.0596
case acc: 0.06233551
case acc: 0.043250717
case acc: 0.07581691
case acc: 0.06770097
case acc: 0.033538226
case acc: 0.0750612
top acc: 0.0186 ::: bot acc: 0.1223
top acc: 0.0637 ::: bot acc: 0.0423
top acc: 0.0228 ::: bot acc: 0.1289
top acc: 0.0217 ::: bot acc: 0.1096
top acc: 0.0188 ::: bot acc: 0.0562
top acc: 0.0375 ::: bot acc: 0.1098
current epoch: 38
train loss is 0.006341
average val loss: 0.007661, accuracy: 0.0852
average test loss: 0.002566, accuracy: 0.0597
case acc: 0.062840685
case acc: 0.04306884
case acc: 0.07566283
case acc: 0.06777894
case acc: 0.033559684
case acc: 0.07536687
top acc: 0.0192 ::: bot acc: 0.1226
top acc: 0.0628 ::: bot acc: 0.0431
top acc: 0.0225 ::: bot acc: 0.1289
top acc: 0.0216 ::: bot acc: 0.1096
top acc: 0.0186 ::: bot acc: 0.0563
top acc: 0.0378 ::: bot acc: 0.1102
current epoch: 39
train loss is 0.006334
average val loss: 0.007679, accuracy: 0.0852
average test loss: 0.002560, accuracy: 0.0596
case acc: 0.0630575
case acc: 0.042714816
case acc: 0.07531227
case acc: 0.06743101
case acc: 0.033701357
case acc: 0.07547386
top acc: 0.0194 ::: bot acc: 0.1228
top acc: 0.0622 ::: bot acc: 0.0435
top acc: 0.0221 ::: bot acc: 0.1284
top acc: 0.0212 ::: bot acc: 0.1093
top acc: 0.0188 ::: bot acc: 0.0564
top acc: 0.0376 ::: bot acc: 0.1101
current epoch: 40
train loss is 0.006306
average val loss: 0.007661, accuracy: 0.0849
average test loss: 0.002578, accuracy: 0.0599
case acc: 0.06372892
case acc: 0.042627808
case acc: 0.07529084
case acc: 0.06793537
case acc: 0.03391647
case acc: 0.07593894
top acc: 0.0201 ::: bot acc: 0.1234
top acc: 0.0612 ::: bot acc: 0.0446
top acc: 0.0221 ::: bot acc: 0.1284
top acc: 0.0219 ::: bot acc: 0.1097
top acc: 0.0186 ::: bot acc: 0.0567
top acc: 0.0382 ::: bot acc: 0.1107
current epoch: 41
train loss is 0.006302
average val loss: 0.007631, accuracy: 0.0847
average test loss: 0.002607, accuracy: 0.0603
case acc: 0.064660214
case acc: 0.04238505
case acc: 0.07561257
case acc: 0.06846874
case acc: 0.034209445
case acc: 0.0765322
top acc: 0.0210 ::: bot acc: 0.1245
top acc: 0.0600 ::: bot acc: 0.0458
top acc: 0.0224 ::: bot acc: 0.1290
top acc: 0.0221 ::: bot acc: 0.1103
top acc: 0.0184 ::: bot acc: 0.0572
top acc: 0.0389 ::: bot acc: 0.1110
current epoch: 42
train loss is 0.006271
average val loss: 0.007607, accuracy: 0.0844
average test loss: 0.002622, accuracy: 0.0605
case acc: 0.06549902
case acc: 0.04218974
case acc: 0.07559973
case acc: 0.06870782
case acc: 0.034270957
case acc: 0.076740354
top acc: 0.0217 ::: bot acc: 0.1254
top acc: 0.0592 ::: bot acc: 0.0467
top acc: 0.0224 ::: bot acc: 0.1288
top acc: 0.0222 ::: bot acc: 0.1106
top acc: 0.0181 ::: bot acc: 0.0574
top acc: 0.0390 ::: bot acc: 0.1115
current epoch: 43
train loss is 0.006269
average val loss: 0.007546, accuracy: 0.0838
average test loss: 0.002664, accuracy: 0.0611
case acc: 0.066670924
case acc: 0.04196315
case acc: 0.076390006
case acc: 0.06933376
case acc: 0.034518585
case acc: 0.077545956
top acc: 0.0230 ::: bot acc: 0.1263
top acc: 0.0578 ::: bot acc: 0.0481
top acc: 0.0228 ::: bot acc: 0.1298
top acc: 0.0225 ::: bot acc: 0.1116
top acc: 0.0180 ::: bot acc: 0.0579
top acc: 0.0397 ::: bot acc: 0.1122
current epoch: 44
train loss is 0.006260
average val loss: 0.007480, accuracy: 0.0832
average test loss: 0.002718, accuracy: 0.0618
case acc: 0.06811482
case acc: 0.041730177
case acc: 0.07713661
case acc: 0.070181556
case acc: 0.035011828
case acc: 0.07855157
top acc: 0.0243 ::: bot acc: 0.1278
top acc: 0.0564 ::: bot acc: 0.0496
top acc: 0.0233 ::: bot acc: 0.1306
top acc: 0.0230 ::: bot acc: 0.1124
top acc: 0.0180 ::: bot acc: 0.0586
top acc: 0.0407 ::: bot acc: 0.1133
current epoch: 45
train loss is 0.006240
average val loss: 0.007383, accuracy: 0.0823
average test loss: 0.002799, accuracy: 0.0628
case acc: 0.070112966
case acc: 0.04143942
case acc: 0.07815378
case acc: 0.07162378
case acc: 0.03564953
case acc: 0.08010743
top acc: 0.0261 ::: bot acc: 0.1301
top acc: 0.0543 ::: bot acc: 0.0515
top acc: 0.0239 ::: bot acc: 0.1318
top acc: 0.0241 ::: bot acc: 0.1140
top acc: 0.0177 ::: bot acc: 0.0599
top acc: 0.0421 ::: bot acc: 0.1149
current epoch: 46
train loss is 0.006224
average val loss: 0.007399, accuracy: 0.0824
average test loss: 0.002809, accuracy: 0.0630
case acc: 0.07069644
case acc: 0.04131218
case acc: 0.07802689
case acc: 0.071710944
case acc: 0.03561276
case acc: 0.08039553
top acc: 0.0267 ::: bot acc: 0.1305
top acc: 0.0539 ::: bot acc: 0.0519
top acc: 0.0238 ::: bot acc: 0.1318
top acc: 0.0240 ::: bot acc: 0.1143
top acc: 0.0179 ::: bot acc: 0.0598
top acc: 0.0423 ::: bot acc: 0.1154
current epoch: 47
train loss is 0.006211
average val loss: 0.007358, accuracy: 0.0820
average test loss: 0.002851, accuracy: 0.0635
case acc: 0.071619436
case acc: 0.041161545
case acc: 0.078421
case acc: 0.07232795
case acc: 0.0363245
case acc: 0.08126849
top acc: 0.0276 ::: bot acc: 0.1314
top acc: 0.0529 ::: bot acc: 0.0528
top acc: 0.0239 ::: bot acc: 0.1323
top acc: 0.0245 ::: bot acc: 0.1150
top acc: 0.0175 ::: bot acc: 0.0609
top acc: 0.0431 ::: bot acc: 0.1163
current epoch: 48
train loss is 0.006208
average val loss: 0.007362, accuracy: 0.0820
average test loss: 0.002854, accuracy: 0.0636
case acc: 0.072022825
case acc: 0.041199118
case acc: 0.07800641
case acc: 0.07241616
case acc: 0.03643383
case acc: 0.081373066
top acc: 0.0280 ::: bot acc: 0.1320
top acc: 0.0528 ::: bot acc: 0.0531
top acc: 0.0237 ::: bot acc: 0.1317
top acc: 0.0245 ::: bot acc: 0.1152
top acc: 0.0173 ::: bot acc: 0.0611
top acc: 0.0432 ::: bot acc: 0.1162
current epoch: 49
train loss is 0.006198
average val loss: 0.007506, accuracy: 0.0831
average test loss: 0.002762, accuracy: 0.0624
case acc: 0.070538804
case acc: 0.04137842
case acc: 0.076375194
case acc: 0.07093051
case acc: 0.035481106
case acc: 0.07980005
top acc: 0.0265 ::: bot acc: 0.1305
top acc: 0.0544 ::: bot acc: 0.0513
top acc: 0.0231 ::: bot acc: 0.1295
top acc: 0.0236 ::: bot acc: 0.1133
top acc: 0.0179 ::: bot acc: 0.0595
top acc: 0.0418 ::: bot acc: 0.1145
current epoch: 50
train loss is 0.006196
average val loss: 0.007491, accuracy: 0.0829
average test loss: 0.002781, accuracy: 0.0627
case acc: 0.07125405
case acc: 0.041321725
case acc: 0.07611257
case acc: 0.07124566
case acc: 0.035795353
case acc: 0.080318704
top acc: 0.0274 ::: bot acc: 0.1311
top acc: 0.0541 ::: bot acc: 0.0517
top acc: 0.0228 ::: bot acc: 0.1294
top acc: 0.0236 ::: bot acc: 0.1138
top acc: 0.0178 ::: bot acc: 0.0601
top acc: 0.0422 ::: bot acc: 0.1152
LME_Co_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5136 5136 5136
1.7082474 -0.6288155 0.48738334 -0.33910522
Validation: 576 576 576
Testing: 744 744 744
pre-processing time: 0.0001850128173828125
the split date is 2011-01-01
train dropout: 0.4 test dropout: 0.1
net initializing with time: 0.0022072792053222656
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012616
average val loss: 0.016119, accuracy: 0.1645
average test loss: 0.020642, accuracy: 0.1729
case acc: 0.21061963
case acc: 0.043505337
case acc: 0.26176712
case acc: 0.20647542
case acc: 0.19726472
case acc: 0.117921196
top acc: 0.0957 ::: bot acc: 0.3205
top acc: 0.0577 ::: bot acc: 0.0588
top acc: 0.1829 ::: bot acc: 0.3459
top acc: 0.1099 ::: bot acc: 0.2829
top acc: 0.0699 ::: bot acc: 0.3283
top acc: 0.0278 ::: bot acc: 0.2176
current epoch: 2
train loss is 0.012346
average val loss: 0.007015, accuracy: 0.1041
average test loss: 0.009090, accuracy: 0.1146
case acc: 0.11703415
case acc: 0.098222926
case acc: 0.15364613
case acc: 0.110026486
case acc: 0.11646458
case acc: 0.09217709
top acc: 0.0351 ::: bot acc: 0.2103
top acc: 0.1521 ::: bot acc: 0.0396
top acc: 0.0743 ::: bot acc: 0.2378
top acc: 0.0216 ::: bot acc: 0.1824
top acc: 0.0300 ::: bot acc: 0.2268
top acc: 0.1117 ::: bot acc: 0.1260
current epoch: 3
train loss is 0.011812
average val loss: 0.008637, accuracy: 0.0874
average test loss: 0.007995, accuracy: 0.1042
case acc: 0.082166664
case acc: 0.20208885
case acc: 0.06593684
case acc: 0.06751645
case acc: 0.09983857
case acc: 0.10761569
top acc: 0.1322 ::: bot acc: 0.0926
top acc: 0.2572 ::: bot acc: 0.1427
top acc: 0.0419 ::: bot acc: 0.1227
top acc: 0.0974 ::: bot acc: 0.0754
top acc: 0.1404 ::: bot acc: 0.1174
top acc: 0.2104 ::: bot acc: 0.0294
current epoch: 4
train loss is 0.012907
average val loss: 0.008385, accuracy: 0.0877
average test loss: 0.007694, accuracy: 0.1027
case acc: 0.08309592
case acc: 0.19470689
case acc: 0.064568974
case acc: 0.06827704
case acc: 0.100072324
case acc: 0.10545254
top acc: 0.1398 ::: bot acc: 0.0848
top acc: 0.2498 ::: bot acc: 0.1358
top acc: 0.0462 ::: bot acc: 0.1179
top acc: 0.0974 ::: bot acc: 0.0769
top acc: 0.1465 ::: bot acc: 0.1103
top acc: 0.2054 ::: bot acc: 0.0338
current epoch: 5
train loss is 0.012132
average val loss: 0.005500, accuracy: 0.0867
average test loss: 0.007024, accuracy: 0.1019
case acc: 0.09617147
case acc: 0.09617554
case acc: 0.1218481
case acc: 0.09919481
case acc: 0.10579768
case acc: 0.09218283
top acc: 0.0572 ::: bot acc: 0.1680
top acc: 0.1503 ::: bot acc: 0.0380
top acc: 0.0426 ::: bot acc: 0.2061
top acc: 0.0221 ::: bot acc: 0.1662
top acc: 0.0684 ::: bot acc: 0.1885
top acc: 0.1143 ::: bot acc: 0.1236
current epoch: 6
train loss is 0.008382
average val loss: 0.005670, accuracy: 0.0926
average test loss: 0.007658, accuracy: 0.1051
case acc: 0.10189884
case acc: 0.07287206
case acc: 0.14128403
case acc: 0.112066
case acc: 0.10806771
case acc: 0.09461308
top acc: 0.0470 ::: bot acc: 0.1822
top acc: 0.1216 ::: bot acc: 0.0258
top acc: 0.0619 ::: bot acc: 0.2257
top acc: 0.0224 ::: bot acc: 0.1849
top acc: 0.0607 ::: bot acc: 0.1965
top acc: 0.0946 ::: bot acc: 0.1427
current epoch: 7
train loss is 0.007115
average val loss: 0.004995, accuracy: 0.0829
average test loss: 0.006682, accuracy: 0.0993
case acc: 0.09380539
case acc: 0.079775706
case acc: 0.12624353
case acc: 0.10098975
case acc: 0.1025181
case acc: 0.09252067
top acc: 0.0648 ::: bot acc: 0.1608
top acc: 0.1303 ::: bot acc: 0.0282
top acc: 0.0469 ::: bot acc: 0.2107
top acc: 0.0218 ::: bot acc: 0.1685
top acc: 0.0861 ::: bot acc: 0.1715
top acc: 0.1111 ::: bot acc: 0.1268
current epoch: 8
train loss is 0.007063
average val loss: 0.004695, accuracy: 0.0758
average test loss: 0.005941, accuracy: 0.0942
case acc: 0.08730844
case acc: 0.08870699
case acc: 0.10793436
case acc: 0.09117144
case acc: 0.09931015
case acc: 0.09062322
top acc: 0.0874 ::: bot acc: 0.1378
top acc: 0.1412 ::: bot acc: 0.0330
top acc: 0.0298 ::: bot acc: 0.1916
top acc: 0.0282 ::: bot acc: 0.1508
top acc: 0.1095 ::: bot acc: 0.1473
top acc: 0.1269 ::: bot acc: 0.1106
current epoch: 9
train loss is 0.007028
average val loss: 0.004559, accuracy: 0.0749
average test loss: 0.005807, accuracy: 0.0930
case acc: 0.08650084
case acc: 0.0829894
case acc: 0.10727921
case acc: 0.0909446
case acc: 0.09928043
case acc: 0.090778716
top acc: 0.0911 ::: bot acc: 0.1341
top acc: 0.1347 ::: bot acc: 0.0295
top acc: 0.0295 ::: bot acc: 0.1906
top acc: 0.0280 ::: bot acc: 0.1509
top acc: 0.1149 ::: bot acc: 0.1427
top acc: 0.1252 ::: bot acc: 0.1126
current epoch: 10
train loss is 0.006815
average val loss: 0.004425, accuracy: 0.0764
average test loss: 0.006009, accuracy: 0.0939
case acc: 0.08775347
case acc: 0.06904357
case acc: 0.117934495
case acc: 0.09714181
case acc: 0.09955028
case acc: 0.09220643
top acc: 0.0832 ::: bot acc: 0.1411
top acc: 0.1160 ::: bot acc: 0.0249
top acc: 0.0383 ::: bot acc: 0.2023
top acc: 0.0233 ::: bot acc: 0.1626
top acc: 0.1071 ::: bot acc: 0.1503
top acc: 0.1130 ::: bot acc: 0.1248
current epoch: 11
train loss is 0.006407
average val loss: 0.004287, accuracy: 0.0752
average test loss: 0.005914, accuracy: 0.0930
case acc: 0.08746382
case acc: 0.06467938
case acc: 0.11752982
case acc: 0.09676244
case acc: 0.09932117
case acc: 0.092518136
top acc: 0.0871 ::: bot acc: 0.1385
top acc: 0.1097 ::: bot acc: 0.0241
top acc: 0.0383 ::: bot acc: 0.2018
top acc: 0.0234 ::: bot acc: 0.1620
top acc: 0.1109 ::: bot acc: 0.1466
top acc: 0.1122 ::: bot acc: 0.1258
current epoch: 12
train loss is 0.006226
average val loss: 0.004140, accuracy: 0.0730
average test loss: 0.005695, accuracy: 0.0913
case acc: 0.08611053
case acc: 0.06428689
case acc: 0.112604596
case acc: 0.094437525
case acc: 0.098951876
case acc: 0.091657564
top acc: 0.0945 ::: bot acc: 0.1311
top acc: 0.1094 ::: bot acc: 0.0245
top acc: 0.0338 ::: bot acc: 0.1966
top acc: 0.0251 ::: bot acc: 0.1578
top acc: 0.1176 ::: bot acc: 0.1396
top acc: 0.1151 ::: bot acc: 0.1218
current epoch: 13
train loss is 0.006171
average val loss: 0.004024, accuracy: 0.0719
average test loss: 0.005608, accuracy: 0.0904
case acc: 0.08519804
case acc: 0.060577348
case acc: 0.11175675
case acc: 0.09414318
case acc: 0.09903045
case acc: 0.09186791
top acc: 0.0973 ::: bot acc: 0.1278
top acc: 0.1038 ::: bot acc: 0.0239
top acc: 0.0331 ::: bot acc: 0.1958
top acc: 0.0256 ::: bot acc: 0.1565
top acc: 0.1190 ::: bot acc: 0.1384
top acc: 0.1142 ::: bot acc: 0.1229
current epoch: 14
train loss is 0.006030
average val loss: 0.003918, accuracy: 0.0710
average test loss: 0.005546, accuracy: 0.0898
case acc: 0.08479871
case acc: 0.057740733
case acc: 0.1108906
case acc: 0.09394306
case acc: 0.099175155
case acc: 0.092046335
top acc: 0.0998 ::: bot acc: 0.1251
top acc: 0.0987 ::: bot acc: 0.0252
top acc: 0.0322 ::: bot acc: 0.1950
top acc: 0.0258 ::: bot acc: 0.1566
top acc: 0.1200 ::: bot acc: 0.1377
top acc: 0.1126 ::: bot acc: 0.1245
current epoch: 15
train loss is 0.005944
average val loss: 0.003806, accuracy: 0.0694
average test loss: 0.005395, accuracy: 0.0886
case acc: 0.084538534
case acc: 0.05745445
case acc: 0.10643885
case acc: 0.09198393
case acc: 0.0991477
case acc: 0.09190451
top acc: 0.1061 ::: bot acc: 0.1201
top acc: 0.0987 ::: bot acc: 0.0254
top acc: 0.0289 ::: bot acc: 0.1899
top acc: 0.0281 ::: bot acc: 0.1522
top acc: 0.1231 ::: bot acc: 0.1346
top acc: 0.1150 ::: bot acc: 0.1225
current epoch: 16
train loss is 0.005934
average val loss: 0.003717, accuracy: 0.0678
average test loss: 0.005202, accuracy: 0.0869
case acc: 0.083180115
case acc: 0.058113664
case acc: 0.10049752
case acc: 0.089027
case acc: 0.09918246
case acc: 0.09156062
top acc: 0.1134 ::: bot acc: 0.1115
top acc: 0.1000 ::: bot acc: 0.0247
top acc: 0.0248 ::: bot acc: 0.1830
top acc: 0.0305 ::: bot acc: 0.1466
top acc: 0.1281 ::: bot acc: 0.1298
top acc: 0.1188 ::: bot acc: 0.1186
current epoch: 17
train loss is 0.005937
average val loss: 0.003648, accuracy: 0.0669
average test loss: 0.005111, accuracy: 0.0860
case acc: 0.082437746
case acc: 0.05719197
case acc: 0.097891934
case acc: 0.088039905
case acc: 0.09909363
case acc: 0.09128562
top acc: 0.1164 ::: bot acc: 0.1083
top acc: 0.0981 ::: bot acc: 0.0255
top acc: 0.0226 ::: bot acc: 0.1803
top acc: 0.0318 ::: bot acc: 0.1447
top acc: 0.1289 ::: bot acc: 0.1283
top acc: 0.1191 ::: bot acc: 0.1179
current epoch: 18
train loss is 0.005860
average val loss: 0.003557, accuracy: 0.0663
average test loss: 0.005113, accuracy: 0.0859
case acc: 0.08265092
case acc: 0.053553108
case acc: 0.09908985
case acc: 0.089165606
case acc: 0.09899618
case acc: 0.09170743
top acc: 0.1160 ::: bot acc: 0.1086
top acc: 0.0915 ::: bot acc: 0.0273
top acc: 0.0235 ::: bot acc: 0.1816
top acc: 0.0310 ::: bot acc: 0.1467
top acc: 0.1261 ::: bot acc: 0.1309
top acc: 0.1160 ::: bot acc: 0.1213
current epoch: 19
train loss is 0.005769
average val loss: 0.003486, accuracy: 0.0654
average test loss: 0.005026, accuracy: 0.0850
case acc: 0.08240797
case acc: 0.053077
case acc: 0.0963757
case acc: 0.08773076
case acc: 0.09904203
case acc: 0.091577016
top acc: 0.1197 ::: bot acc: 0.1055
top acc: 0.0907 ::: bot acc: 0.0277
top acc: 0.0222 ::: bot acc: 0.1782
top acc: 0.0327 ::: bot acc: 0.1438
top acc: 0.1285 ::: bot acc: 0.1287
top acc: 0.1175 ::: bot acc: 0.1202
current epoch: 20
train loss is 0.005733
average val loss: 0.003420, accuracy: 0.0648
average test loss: 0.004978, accuracy: 0.0845
case acc: 0.08208434
case acc: 0.052063975
case acc: 0.09504898
case acc: 0.087015
case acc: 0.09913455
case acc: 0.091641136
top acc: 0.1213 ::: bot acc: 0.1035
top acc: 0.0886 ::: bot acc: 0.0289
top acc: 0.0212 ::: bot acc: 0.1766
top acc: 0.0336 ::: bot acc: 0.1421
top acc: 0.1288 ::: bot acc: 0.1288
top acc: 0.1175 ::: bot acc: 0.1200
current epoch: 21
train loss is 0.005687
average val loss: 0.003352, accuracy: 0.0642
average test loss: 0.004953, accuracy: 0.0842
case acc: 0.08223084
case acc: 0.050193533
case acc: 0.09474282
case acc: 0.08711349
case acc: 0.0992422
case acc: 0.09162006
top acc: 0.1223 ::: bot acc: 0.1029
top acc: 0.0846 ::: bot acc: 0.0304
top acc: 0.0212 ::: bot acc: 0.1763
top acc: 0.0340 ::: bot acc: 0.1421
top acc: 0.1277 ::: bot acc: 0.1300
top acc: 0.1156 ::: bot acc: 0.1213
current epoch: 22
train loss is 0.005633
average val loss: 0.003305, accuracy: 0.0637
average test loss: 0.004911, accuracy: 0.0837
case acc: 0.08205756
case acc: 0.049232405
case acc: 0.093520194
case acc: 0.08657434
case acc: 0.099125735
case acc: 0.09190901
top acc: 0.1237 ::: bot acc: 0.1013
top acc: 0.0830 ::: bot acc: 0.0318
top acc: 0.0207 ::: bot acc: 0.1743
top acc: 0.0342 ::: bot acc: 0.1412
top acc: 0.1280 ::: bot acc: 0.1297
top acc: 0.1164 ::: bot acc: 0.1211
current epoch: 23
train loss is 0.005596
average val loss: 0.003234, accuracy: 0.0633
average test loss: 0.004952, accuracy: 0.0839
case acc: 0.08218295
case acc: 0.04713666
case acc: 0.09536023
case acc: 0.0877084
case acc: 0.09920822
case acc: 0.0920242
top acc: 0.1219 ::: bot acc: 0.1031
top acc: 0.0774 ::: bot acc: 0.0363
top acc: 0.0213 ::: bot acc: 0.1769
top acc: 0.0328 ::: bot acc: 0.1436
top acc: 0.1252 ::: bot acc: 0.1327
top acc: 0.1130 ::: bot acc: 0.1242
current epoch: 24
train loss is 0.005541
average val loss: 0.003198, accuracy: 0.0629
average test loss: 0.004913, accuracy: 0.0835
case acc: 0.0821721
case acc: 0.046367906
case acc: 0.094495505
case acc: 0.08722997
case acc: 0.098880604
case acc: 0.09210853
top acc: 0.1236 ::: bot acc: 0.1018
top acc: 0.0759 ::: bot acc: 0.0376
top acc: 0.0211 ::: bot acc: 0.1757
top acc: 0.0336 ::: bot acc: 0.1425
top acc: 0.1249 ::: bot acc: 0.1320
top acc: 0.1127 ::: bot acc: 0.1245
current epoch: 25
train loss is 0.005554
average val loss: 0.003159, accuracy: 0.0622
average test loss: 0.004818, accuracy: 0.0828
case acc: 0.0820395
case acc: 0.04700069
case acc: 0.091097064
case acc: 0.085753106
case acc: 0.099096164
case acc: 0.091672026
top acc: 0.1273 ::: bot acc: 0.0978
top acc: 0.0772 ::: bot acc: 0.0364
top acc: 0.0197 ::: bot acc: 0.1714
top acc: 0.0367 ::: bot acc: 0.1387
top acc: 0.1280 ::: bot acc: 0.1294
top acc: 0.1149 ::: bot acc: 0.1221
current epoch: 26
train loss is 0.005572
average val loss: 0.003136, accuracy: 0.0619
average test loss: 0.004782, accuracy: 0.0824
case acc: 0.081966326
case acc: 0.04709465
case acc: 0.089530595
case acc: 0.08517668
case acc: 0.09898775
case acc: 0.091897234
top acc: 0.1290 ::: bot acc: 0.0961
top acc: 0.0771 ::: bot acc: 0.0369
top acc: 0.0193 ::: bot acc: 0.1693
top acc: 0.0378 ::: bot acc: 0.1375
top acc: 0.1282 ::: bot acc: 0.1291
top acc: 0.1155 ::: bot acc: 0.1220
current epoch: 27
train loss is 0.005546
average val loss: 0.003131, accuracy: 0.0618
average test loss: 0.004706, accuracy: 0.0819
case acc: 0.08208201
case acc: 0.047598552
case acc: 0.086601734
case acc: 0.08392104
case acc: 0.09929014
case acc: 0.09160828
top acc: 0.1324 ::: bot acc: 0.0925
top acc: 0.0784 ::: bot acc: 0.0358
top acc: 0.0185 ::: bot acc: 0.1652
top acc: 0.0405 ::: bot acc: 0.1342
top acc: 0.1309 ::: bot acc: 0.1268
top acc: 0.1168 ::: bot acc: 0.1203
current epoch: 28
train loss is 0.005560
average val loss: 0.003086, accuracy: 0.0613
average test loss: 0.004695, accuracy: 0.0817
case acc: 0.08229827
case acc: 0.047112267
case acc: 0.08587708
case acc: 0.083946764
case acc: 0.09940593
case acc: 0.09174227
top acc: 0.1330 ::: bot acc: 0.0922
top acc: 0.0771 ::: bot acc: 0.0367
top acc: 0.0180 ::: bot acc: 0.1646
top acc: 0.0414 ::: bot acc: 0.1338
top acc: 0.1311 ::: bot acc: 0.1267
top acc: 0.1169 ::: bot acc: 0.1208
current epoch: 29
train loss is 0.005546
average val loss: 0.003058, accuracy: 0.0611
average test loss: 0.004703, accuracy: 0.0817
case acc: 0.081969626
case acc: 0.045798134
case acc: 0.08706925
case acc: 0.08419739
case acc: 0.09911021
case acc: 0.09187165
top acc: 0.1311 ::: bot acc: 0.0936
top acc: 0.0732 ::: bot acc: 0.0406
top acc: 0.0184 ::: bot acc: 0.1660
top acc: 0.0394 ::: bot acc: 0.1352
top acc: 0.1291 ::: bot acc: 0.1283
top acc: 0.1145 ::: bot acc: 0.1226
current epoch: 30
train loss is 0.005442
average val loss: 0.003009, accuracy: 0.0609
average test loss: 0.004806, accuracy: 0.0824
case acc: 0.08219225
case acc: 0.04393365
case acc: 0.09063915
case acc: 0.08609153
case acc: 0.09916268
case acc: 0.09258637
top acc: 0.1268 ::: bot acc: 0.0988
top acc: 0.0666 ::: bot acc: 0.0468
top acc: 0.0195 ::: bot acc: 0.1708
top acc: 0.0359 ::: bot acc: 0.1397
top acc: 0.1249 ::: bot acc: 0.1328
top acc: 0.1096 ::: bot acc: 0.1278
current epoch: 31
train loss is 0.005375
average val loss: 0.002992, accuracy: 0.0610
average test loss: 0.004909, accuracy: 0.0832
case acc: 0.082255445
case acc: 0.042853564
case acc: 0.09393976
case acc: 0.088036135
case acc: 0.099029005
case acc: 0.09301474
top acc: 0.1226 ::: bot acc: 0.1028
top acc: 0.0610 ::: bot acc: 0.0526
top acc: 0.0210 ::: bot acc: 0.1751
top acc: 0.0325 ::: bot acc: 0.1444
top acc: 0.1204 ::: bot acc: 0.1369
top acc: 0.1044 ::: bot acc: 0.1327
current epoch: 32
train loss is 0.005354
average val loss: 0.002974, accuracy: 0.0605
average test loss: 0.004766, accuracy: 0.0821
case acc: 0.081985705
case acc: 0.043642923
case acc: 0.08944839
case acc: 0.085678056
case acc: 0.09907843
case acc: 0.092545465
top acc: 0.1277 ::: bot acc: 0.0975
top acc: 0.0652 ::: bot acc: 0.0482
top acc: 0.0191 ::: bot acc: 0.1691
top acc: 0.0368 ::: bot acc: 0.1387
top acc: 0.1256 ::: bot acc: 0.1321
top acc: 0.1092 ::: bot acc: 0.1281
current epoch: 33
train loss is 0.005408
average val loss: 0.002957, accuracy: 0.0601
average test loss: 0.004710, accuracy: 0.0816
case acc: 0.081893444
case acc: 0.04375617
case acc: 0.08773097
case acc: 0.08490316
case acc: 0.09903716
case acc: 0.09237678
top acc: 0.1293 ::: bot acc: 0.0954
top acc: 0.0661 ::: bot acc: 0.0472
top acc: 0.0184 ::: bot acc: 0.1670
top acc: 0.0389 ::: bot acc: 0.1363
top acc: 0.1268 ::: bot acc: 0.1305
top acc: 0.1107 ::: bot acc: 0.1268
current epoch: 34
train loss is 0.005402
average val loss: 0.002934, accuracy: 0.0598
average test loss: 0.004701, accuracy: 0.0815
case acc: 0.08201342
case acc: 0.043546204
case acc: 0.08736731
case acc: 0.0845789
case acc: 0.09930129
case acc: 0.09234583
top acc: 0.1299 ::: bot acc: 0.0952
top acc: 0.0655 ::: bot acc: 0.0479
top acc: 0.0184 ::: bot acc: 0.1663
top acc: 0.0394 ::: bot acc: 0.1357
top acc: 0.1273 ::: bot acc: 0.1308
top acc: 0.1107 ::: bot acc: 0.1267
current epoch: 35
train loss is 0.005366
average val loss: 0.002918, accuracy: 0.0598
average test loss: 0.004729, accuracy: 0.0817
case acc: 0.08214078
case acc: 0.043048628
case acc: 0.08842357
case acc: 0.08498835
case acc: 0.09917992
case acc: 0.09246226
top acc: 0.1283 ::: bot acc: 0.0971
top acc: 0.0630 ::: bot acc: 0.0502
top acc: 0.0187 ::: bot acc: 0.1680
top acc: 0.0384 ::: bot acc: 0.1369
top acc: 0.1261 ::: bot acc: 0.1315
top acc: 0.1092 ::: bot acc: 0.1282
current epoch: 36
train loss is 0.005332
average val loss: 0.002908, accuracy: 0.0595
average test loss: 0.004681, accuracy: 0.0814
case acc: 0.082188524
case acc: 0.043334346
case acc: 0.08687634
case acc: 0.084216885
case acc: 0.09911961
case acc: 0.09237786
top acc: 0.1301 ::: bot acc: 0.0954
top acc: 0.0638 ::: bot acc: 0.0497
top acc: 0.0183 ::: bot acc: 0.1659
top acc: 0.0408 ::: bot acc: 0.1345
top acc: 0.1282 ::: bot acc: 0.1292
top acc: 0.1112 ::: bot acc: 0.1264
current epoch: 37
train loss is 0.005322
average val loss: 0.002879, accuracy: 0.0592
average test loss: 0.004709, accuracy: 0.0815
case acc: 0.08208274
case acc: 0.042811494
case acc: 0.08809335
case acc: 0.084671736
case acc: 0.09904729
case acc: 0.092421524
top acc: 0.1280 ::: bot acc: 0.0971
top acc: 0.0610 ::: bot acc: 0.0523
top acc: 0.0184 ::: bot acc: 0.1675
top acc: 0.0394 ::: bot acc: 0.1358
top acc: 0.1264 ::: bot acc: 0.1313
top acc: 0.1094 ::: bot acc: 0.1280
current epoch: 38
train loss is 0.005263
average val loss: 0.002870, accuracy: 0.0593
average test loss: 0.004804, accuracy: 0.0823
case acc: 0.0819724
case acc: 0.042381305
case acc: 0.09144093
case acc: 0.08593592
case acc: 0.09906921
case acc: 0.09296421
top acc: 0.1234 ::: bot acc: 0.1017
top acc: 0.0564 ::: bot acc: 0.0573
top acc: 0.0198 ::: bot acc: 0.1719
top acc: 0.0364 ::: bot acc: 0.1393
top acc: 0.1222 ::: bot acc: 0.1352
top acc: 0.1055 ::: bot acc: 0.1318
current epoch: 39
train loss is 0.005223
average val loss: 0.002853, accuracy: 0.0592
average test loss: 0.004840, accuracy: 0.0825
case acc: 0.082154594
case acc: 0.04204714
case acc: 0.09245858
case acc: 0.08638727
case acc: 0.09906336
case acc: 0.09314796
top acc: 0.1221 ::: bot acc: 0.1032
top acc: 0.0541 ::: bot acc: 0.0595
top acc: 0.0203 ::: bot acc: 0.1733
top acc: 0.0354 ::: bot acc: 0.1403
top acc: 0.1203 ::: bot acc: 0.1372
top acc: 0.1039 ::: bot acc: 0.1335
current epoch: 40
train loss is 0.005201
average val loss: 0.002844, accuracy: 0.0589
average test loss: 0.004738, accuracy: 0.0818
case acc: 0.08192166
case acc: 0.042493653
case acc: 0.08959414
case acc: 0.08497192
case acc: 0.09901964
case acc: 0.092824556
top acc: 0.1250 ::: bot acc: 0.0999
top acc: 0.0577 ::: bot acc: 0.0561
top acc: 0.0194 ::: bot acc: 0.1693
top acc: 0.0383 ::: bot acc: 0.1369
top acc: 0.1229 ::: bot acc: 0.1344
top acc: 0.1066 ::: bot acc: 0.1306
current epoch: 41
train loss is 0.005227
average val loss: 0.002834, accuracy: 0.0586
average test loss: 0.004656, accuracy: 0.0811
case acc: 0.08197463
case acc: 0.042659417
case acc: 0.08682078
case acc: 0.08371117
case acc: 0.09897832
case acc: 0.09249426
top acc: 0.1281 ::: bot acc: 0.0969
top acc: 0.0603 ::: bot acc: 0.0527
top acc: 0.0184 ::: bot acc: 0.1658
top acc: 0.0418 ::: bot acc: 0.1332
top acc: 0.1257 ::: bot acc: 0.1316
top acc: 0.1096 ::: bot acc: 0.1276
current epoch: 42
train loss is 0.005275
average val loss: 0.002825, accuracy: 0.0585
average test loss: 0.004687, accuracy: 0.0814
case acc: 0.081919886
case acc: 0.042431846
case acc: 0.08791127
case acc: 0.08412896
case acc: 0.09906227
case acc: 0.09270641
top acc: 0.1264 ::: bot acc: 0.0987
top acc: 0.0589 ::: bot acc: 0.0544
top acc: 0.0186 ::: bot acc: 0.1672
top acc: 0.0410 ::: bot acc: 0.1343
top acc: 0.1245 ::: bot acc: 0.1330
top acc: 0.1082 ::: bot acc: 0.1293
current epoch: 43
train loss is 0.005254
average val loss: 0.002817, accuracy: 0.0586
average test loss: 0.004755, accuracy: 0.0819
case acc: 0.0820653
case acc: 0.042121906
case acc: 0.0903263
case acc: 0.08499426
case acc: 0.09907601
case acc: 0.092992485
top acc: 0.1231 ::: bot acc: 0.1019
top acc: 0.0549 ::: bot acc: 0.0586
top acc: 0.0196 ::: bot acc: 0.1704
top acc: 0.0383 ::: bot acc: 0.1368
top acc: 0.1220 ::: bot acc: 0.1356
top acc: 0.1050 ::: bot acc: 0.1321
current epoch: 44
train loss is 0.005205
average val loss: 0.002811, accuracy: 0.0586
average test loss: 0.004799, accuracy: 0.0822
case acc: 0.08229965
case acc: 0.041923963
case acc: 0.09165083
case acc: 0.08537075
case acc: 0.09900917
case acc: 0.09320112
top acc: 0.1219 ::: bot acc: 0.1037
top acc: 0.0530 ::: bot acc: 0.0608
top acc: 0.0200 ::: bot acc: 0.1720
top acc: 0.0373 ::: bot acc: 0.1380
top acc: 0.1210 ::: bot acc: 0.1363
top acc: 0.1039 ::: bot acc: 0.1335
current epoch: 45
train loss is 0.005168
average val loss: 0.002810, accuracy: 0.0587
average test loss: 0.004826, accuracy: 0.0825
case acc: 0.08236227
case acc: 0.041852508
case acc: 0.092448324
case acc: 0.08563571
case acc: 0.099059254
case acc: 0.09347005
top acc: 0.1202 ::: bot acc: 0.1051
top acc: 0.0514 ::: bot acc: 0.0624
top acc: 0.0203 ::: bot acc: 0.1731
top acc: 0.0364 ::: bot acc: 0.1390
top acc: 0.1197 ::: bot acc: 0.1377
top acc: 0.1025 ::: bot acc: 0.1348
current epoch: 46
train loss is 0.005143
average val loss: 0.002807, accuracy: 0.0586
average test loss: 0.004827, accuracy: 0.0825
case acc: 0.08234268
case acc: 0.041776247
case acc: 0.09239644
case acc: 0.08569106
case acc: 0.09907549
case acc: 0.093447044
top acc: 0.1197 ::: bot acc: 0.1055
top acc: 0.0513 ::: bot acc: 0.0624
top acc: 0.0202 ::: bot acc: 0.1731
top acc: 0.0365 ::: bot acc: 0.1387
top acc: 0.1193 ::: bot acc: 0.1383
top acc: 0.1021 ::: bot acc: 0.1351
current epoch: 47
train loss is 0.005163
average val loss: 0.002795, accuracy: 0.0582
average test loss: 0.004701, accuracy: 0.0815
case acc: 0.08199005
case acc: 0.042249937
case acc: 0.08884121
case acc: 0.083903976
case acc: 0.09919798
case acc: 0.09292039
top acc: 0.1236 ::: bot acc: 0.1016
top acc: 0.0558 ::: bot acc: 0.0578
top acc: 0.0188 ::: bot acc: 0.1682
top acc: 0.0408 ::: bot acc: 0.1340
top acc: 0.1234 ::: bot acc: 0.1343
top acc: 0.1059 ::: bot acc: 0.1313
current epoch: 48
train loss is 0.005175
average val loss: 0.002786, accuracy: 0.0579
average test loss: 0.004658, accuracy: 0.0812
case acc: 0.081907354
case acc: 0.042473335
case acc: 0.08756868
case acc: 0.08334613
case acc: 0.09907688
case acc: 0.092667125
top acc: 0.1250 ::: bot acc: 0.1000
top acc: 0.0575 ::: bot acc: 0.0562
top acc: 0.0184 ::: bot acc: 0.1667
top acc: 0.0426 ::: bot acc: 0.1323
top acc: 0.1242 ::: bot acc: 0.1332
top acc: 0.1070 ::: bot acc: 0.1298
current epoch: 49
train loss is 0.005189
average val loss: 0.002779, accuracy: 0.0579
average test loss: 0.004663, accuracy: 0.0812
case acc: 0.08206416
case acc: 0.042225204
case acc: 0.08769936
case acc: 0.08321151
case acc: 0.099118344
case acc: 0.09277373
top acc: 0.1246 ::: bot acc: 0.1007
top acc: 0.0569 ::: bot acc: 0.0565
top acc: 0.0185 ::: bot acc: 0.1670
top acc: 0.0429 ::: bot acc: 0.1319
top acc: 0.1244 ::: bot acc: 0.1332
top acc: 0.1071 ::: bot acc: 0.1303
current epoch: 50
train loss is 0.005178
average val loss: 0.002778, accuracy: 0.0580
average test loss: 0.004720, accuracy: 0.0817
case acc: 0.0820445
case acc: 0.042000283
case acc: 0.089847855
case acc: 0.08400468
case acc: 0.09894781
case acc: 0.09311533
top acc: 0.1218 ::: bot acc: 0.1033
top acc: 0.0539 ::: bot acc: 0.0595
top acc: 0.0193 ::: bot acc: 0.1699
top acc: 0.0409 ::: bot acc: 0.1340
top acc: 0.1226 ::: bot acc: 0.1347
top acc: 0.1048 ::: bot acc: 0.1327
LME_Co_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5112 5112 5112
1.7082474 -0.6288155 0.48738334 -0.33910522
Validation: 570 570 570
Testing: 774 774 774
pre-processing time: 0.00018358230590820312
the split date is 2011-07-01
train dropout: 0.4 test dropout: 0.1
net initializing with time: 0.002210855484008789
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.013107
average val loss: 0.006827, accuracy: 0.1045
average test loss: 0.007951, accuracy: 0.1092
case acc: 0.11082932
case acc: 0.16046187
case acc: 0.10627049
case acc: 0.07787524
case acc: 0.13444722
case acc: 0.06553763
top acc: 0.0402 ::: bot acc: 0.1831
top acc: 0.2254 ::: bot acc: 0.1063
top acc: 0.0399 ::: bot acc: 0.1705
top acc: 0.1182 ::: bot acc: 0.0864
top acc: 0.1007 ::: bot acc: 0.1823
top acc: 0.1268 ::: bot acc: 0.0260
current epoch: 2
train loss is 0.011533
average val loss: 0.008850, accuracy: 0.1025
average test loss: 0.010909, accuracy: 0.1192
case acc: 0.06089554
case acc: 0.24600004
case acc: 0.052491277
case acc: 0.117588505
case acc: 0.10211879
case acc: 0.135898
top acc: 0.0783 ::: bot acc: 0.0841
top acc: 0.3108 ::: bot acc: 0.1911
top acc: 0.0689 ::: bot acc: 0.0721
top acc: 0.2113 ::: bot acc: 0.0401
top acc: 0.1841 ::: bot acc: 0.0898
top acc: 0.2151 ::: bot acc: 0.0625
current epoch: 3
train loss is 0.013409
average val loss: 0.007214, accuracy: 0.0960
average test loss: 0.008984, accuracy: 0.1094
case acc: 0.06309529
case acc: 0.2166343
case acc: 0.05635166
case acc: 0.102514416
case acc: 0.106861115
case acc: 0.11111197
top acc: 0.0628 ::: bot acc: 0.0994
top acc: 0.2819 ::: bot acc: 0.1613
top acc: 0.0515 ::: bot acc: 0.0892
top acc: 0.1912 ::: bot acc: 0.0363
top acc: 0.1645 ::: bot acc: 0.1085
top acc: 0.1896 ::: bot acc: 0.0377
current epoch: 4
train loss is 0.013714
average val loss: 0.007939, accuracy: 0.1090
average test loss: 0.008380, accuracy: 0.1102
case acc: 0.13687302
case acc: 0.08568555
case acc: 0.14409947
case acc: 0.0836388
case acc: 0.15192483
case acc: 0.058962125
top acc: 0.0553 ::: bot acc: 0.2152
top acc: 0.1508 ::: bot acc: 0.0303
top acc: 0.0729 ::: bot acc: 0.2103
top acc: 0.0694 ::: bot acc: 0.1367
top acc: 0.0730 ::: bot acc: 0.2220
top acc: 0.0677 ::: bot acc: 0.0855
current epoch: 5
train loss is 0.008888
average val loss: 0.007052, accuracy: 0.1037
average test loss: 0.007419, accuracy: 0.1045
case acc: 0.12301576
case acc: 0.08852999
case acc: 0.13445991
case acc: 0.08096572
case acc: 0.14236772
case acc: 0.05742803
top acc: 0.0457 ::: bot acc: 0.1992
top acc: 0.1532 ::: bot acc: 0.0337
top acc: 0.0636 ::: bot acc: 0.2004
top acc: 0.0807 ::: bot acc: 0.1237
top acc: 0.0869 ::: bot acc: 0.2006
top acc: 0.0784 ::: bot acc: 0.0739
current epoch: 6
train loss is 0.007424
average val loss: 0.005470, accuracy: 0.0936
average test loss: 0.006133, accuracy: 0.0955
case acc: 0.08878252
case acc: 0.121825695
case acc: 0.097377405
case acc: 0.07848494
case acc: 0.12335513
case acc: 0.0629621
top acc: 0.0337 ::: bot acc: 0.1536
top acc: 0.1870 ::: bot acc: 0.0670
top acc: 0.0345 ::: bot acc: 0.1603
top acc: 0.1228 ::: bot acc: 0.0837
top acc: 0.1244 ::: bot acc: 0.1537
top acc: 0.1190 ::: bot acc: 0.0346
current epoch: 7
train loss is 0.007533
average val loss: 0.005499, accuracy: 0.0935
average test loss: 0.006014, accuracy: 0.0947
case acc: 0.09234478
case acc: 0.10604319
case acc: 0.10585473
case acc: 0.07878707
case acc: 0.12479121
case acc: 0.060150553
top acc: 0.0330 ::: bot acc: 0.1597
top acc: 0.1707 ::: bot acc: 0.0514
top acc: 0.0396 ::: bot acc: 0.1703
top acc: 0.1135 ::: bot acc: 0.0932
top acc: 0.1200 ::: bot acc: 0.1570
top acc: 0.1085 ::: bot acc: 0.0445
current epoch: 8
train loss is 0.007328
average val loss: 0.005715, accuracy: 0.0946
average test loss: 0.006021, accuracy: 0.0943
case acc: 0.09764114
case acc: 0.086953014
case acc: 0.116082035
case acc: 0.07905752
case acc: 0.127753
case acc: 0.058047175
top acc: 0.0343 ::: bot acc: 0.1664
top acc: 0.1515 ::: bot acc: 0.0324
top acc: 0.0473 ::: bot acc: 0.1815
top acc: 0.1013 ::: bot acc: 0.1044
top acc: 0.1149 ::: bot acc: 0.1645
top acc: 0.0967 ::: bot acc: 0.0569
current epoch: 9
train loss is 0.006821
average val loss: 0.005534, accuracy: 0.0930
average test loss: 0.005766, accuracy: 0.0918
case acc: 0.09283854
case acc: 0.08264109
case acc: 0.113352984
case acc: 0.07880805
case acc: 0.12509339
case acc: 0.058144007
top acc: 0.0334 ::: bot acc: 0.1602
top acc: 0.1479 ::: bot acc: 0.0279
top acc: 0.0449 ::: bot acc: 0.1785
top acc: 0.1052 ::: bot acc: 0.1009
top acc: 0.1206 ::: bot acc: 0.1574
top acc: 0.0989 ::: bot acc: 0.0539
current epoch: 10
train loss is 0.006546
average val loss: 0.005324, accuracy: 0.0912
average test loss: 0.005538, accuracy: 0.0895
case acc: 0.08804871
case acc: 0.07931758
case acc: 0.11030582
case acc: 0.07835735
case acc: 0.12251279
case acc: 0.058737148
top acc: 0.0328 ::: bot acc: 0.1532
top acc: 0.1442 ::: bot acc: 0.0248
top acc: 0.0422 ::: bot acc: 0.1754
top acc: 0.1088 ::: bot acc: 0.0972
top acc: 0.1254 ::: bot acc: 0.1518
top acc: 0.1020 ::: bot acc: 0.0511
current epoch: 11
train loss is 0.006306
average val loss: 0.005203, accuracy: 0.0899
average test loss: 0.005387, accuracy: 0.0880
case acc: 0.085338935
case acc: 0.074061714
case acc: 0.10975209
case acc: 0.078157276
case acc: 0.12151196
case acc: 0.059014868
top acc: 0.0324 ::: bot acc: 0.1496
top acc: 0.1383 ::: bot acc: 0.0196
top acc: 0.0419 ::: bot acc: 0.1748
top acc: 0.1094 ::: bot acc: 0.0959
top acc: 0.1278 ::: bot acc: 0.1487
top acc: 0.1021 ::: bot acc: 0.0516
current epoch: 12
train loss is 0.006134
average val loss: 0.005065, accuracy: 0.0887
average test loss: 0.005224, accuracy: 0.0862
case acc: 0.082177125
case acc: 0.070194244
case acc: 0.1076798
case acc: 0.078351766
case acc: 0.12004342
case acc: 0.05877236
top acc: 0.0334 ::: bot acc: 0.1438
top acc: 0.1345 ::: bot acc: 0.0167
top acc: 0.0407 ::: bot acc: 0.1723
top acc: 0.1124 ::: bot acc: 0.0934
top acc: 0.1306 ::: bot acc: 0.1453
top acc: 0.1036 ::: bot acc: 0.0489
current epoch: 13
train loss is 0.005997
average val loss: 0.004947, accuracy: 0.0874
average test loss: 0.005110, accuracy: 0.0848
case acc: 0.07958082
case acc: 0.06588917
case acc: 0.106269784
case acc: 0.07818891
case acc: 0.11950071
case acc: 0.059074994
top acc: 0.0336 ::: bot acc: 0.1400
top acc: 0.1301 ::: bot acc: 0.0127
top acc: 0.0394 ::: bot acc: 0.1708
top acc: 0.1143 ::: bot acc: 0.0914
top acc: 0.1321 ::: bot acc: 0.1436
top acc: 0.1047 ::: bot acc: 0.0482
current epoch: 14
train loss is 0.005850
average val loss: 0.004916, accuracy: 0.0870
average test loss: 0.005028, accuracy: 0.0838
case acc: 0.07864954
case acc: 0.06051672
case acc: 0.10652396
case acc: 0.07827621
case acc: 0.11975988
case acc: 0.05882961
top acc: 0.0344 ::: bot acc: 0.1378
top acc: 0.1233 ::: bot acc: 0.0097
top acc: 0.0398 ::: bot acc: 0.1710
top acc: 0.1147 ::: bot acc: 0.0915
top acc: 0.1314 ::: bot acc: 0.1443
top acc: 0.1033 ::: bot acc: 0.0495
current epoch: 15
train loss is 0.005663
average val loss: 0.004861, accuracy: 0.0864
average test loss: 0.004970, accuracy: 0.0830
case acc: 0.07789133
case acc: 0.056072444
case acc: 0.10679572
case acc: 0.07835004
case acc: 0.120327495
case acc: 0.058796726
top acc: 0.0350 ::: bot acc: 0.1366
top acc: 0.1169 ::: bot acc: 0.0090
top acc: 0.0399 ::: bot acc: 0.1717
top acc: 0.1146 ::: bot acc: 0.0914
top acc: 0.1311 ::: bot acc: 0.1453
top acc: 0.1025 ::: bot acc: 0.0508
current epoch: 16
train loss is 0.005562
average val loss: 0.004803, accuracy: 0.0856
average test loss: 0.004871, accuracy: 0.0819
case acc: 0.07607394
case acc: 0.05327566
case acc: 0.1058205
case acc: 0.07813847
case acc: 0.11987757
case acc: 0.058479253
top acc: 0.0355 ::: bot acc: 0.1334
top acc: 0.1125 ::: bot acc: 0.0107
top acc: 0.0391 ::: bot acc: 0.1704
top acc: 0.1155 ::: bot acc: 0.0904
top acc: 0.1303 ::: bot acc: 0.1448
top acc: 0.1020 ::: bot acc: 0.0509
current epoch: 17
train loss is 0.005418
average val loss: 0.004745, accuracy: 0.0849
average test loss: 0.004822, accuracy: 0.0814
case acc: 0.074851446
case acc: 0.051369086
case acc: 0.10546915
case acc: 0.07798089
case acc: 0.12044425
case acc: 0.05849533
top acc: 0.0368 ::: bot acc: 0.1313
top acc: 0.1079 ::: bot acc: 0.0137
top acc: 0.0387 ::: bot acc: 0.1699
top acc: 0.1161 ::: bot acc: 0.0895
top acc: 0.1307 ::: bot acc: 0.1455
top acc: 0.1017 ::: bot acc: 0.0513
current epoch: 18
train loss is 0.005334
average val loss: 0.004592, accuracy: 0.0835
average test loss: 0.004707, accuracy: 0.0803
case acc: 0.07223086
case acc: 0.050798293
case acc: 0.10175183
case acc: 0.078123175
case acc: 0.11933807
case acc: 0.05947096
top acc: 0.0401 ::: bot acc: 0.1257
top acc: 0.1066 ::: bot acc: 0.0140
top acc: 0.0365 ::: bot acc: 0.1658
top acc: 0.1205 ::: bot acc: 0.0855
top acc: 0.1328 ::: bot acc: 0.1431
top acc: 0.1053 ::: bot acc: 0.0487
current epoch: 19
train loss is 0.005264
average val loss: 0.004482, accuracy: 0.0824
average test loss: 0.004627, accuracy: 0.0795
case acc: 0.07062684
case acc: 0.050350018
case acc: 0.099014975
case acc: 0.07841894
case acc: 0.11899334
case acc: 0.059626166
top acc: 0.0435 ::: bot acc: 0.1216
top acc: 0.1055 ::: bot acc: 0.0154
top acc: 0.0352 ::: bot acc: 0.1622
top acc: 0.1235 ::: bot acc: 0.0826
top acc: 0.1343 ::: bot acc: 0.1417
top acc: 0.1068 ::: bot acc: 0.0467
current epoch: 20
train loss is 0.005221
average val loss: 0.004404, accuracy: 0.0816
average test loss: 0.004555, accuracy: 0.0788
case acc: 0.06929845
case acc: 0.049315777
case acc: 0.097249694
case acc: 0.07841632
case acc: 0.11885245
case acc: 0.05951166
top acc: 0.0465 ::: bot acc: 0.1178
top acc: 0.1029 ::: bot acc: 0.0171
top acc: 0.0341 ::: bot acc: 0.1603
top acc: 0.1255 ::: bot acc: 0.0802
top acc: 0.1341 ::: bot acc: 0.1416
top acc: 0.1072 ::: bot acc: 0.0460
current epoch: 21
train loss is 0.005159
average val loss: 0.004341, accuracy: 0.0809
average test loss: 0.004504, accuracy: 0.0782
case acc: 0.06816202
case acc: 0.048475813
case acc: 0.09589623
case acc: 0.07841337
case acc: 0.11889222
case acc: 0.05957352
top acc: 0.0490 ::: bot acc: 0.1150
top acc: 0.1003 ::: bot acc: 0.0195
top acc: 0.0338 ::: bot acc: 0.1584
top acc: 0.1272 ::: bot acc: 0.0788
top acc: 0.1346 ::: bot acc: 0.1414
top acc: 0.1077 ::: bot acc: 0.0453
current epoch: 22
train loss is 0.005110
average val loss: 0.004357, accuracy: 0.0809
average test loss: 0.004487, accuracy: 0.0781
case acc: 0.06834762
case acc: 0.046615977
case acc: 0.09631385
case acc: 0.078530766
case acc: 0.11949894
case acc: 0.05936158
top acc: 0.0494 ::: bot acc: 0.1153
top acc: 0.0956 ::: bot acc: 0.0241
top acc: 0.0335 ::: bot acc: 0.1596
top acc: 0.1261 ::: bot acc: 0.0797
top acc: 0.1326 ::: bot acc: 0.1434
top acc: 0.1059 ::: bot acc: 0.0475
current epoch: 23
train loss is 0.005055
average val loss: 0.004272, accuracy: 0.0800
average test loss: 0.004430, accuracy: 0.0775
case acc: 0.06683979
case acc: 0.046837755
case acc: 0.093820654
case acc: 0.07870577
case acc: 0.118778065
case acc: 0.05986973
top acc: 0.0531 ::: bot acc: 0.1109
top acc: 0.0950 ::: bot acc: 0.0254
top acc: 0.0322 ::: bot acc: 0.1563
top acc: 0.1290 ::: bot acc: 0.0770
top acc: 0.1344 ::: bot acc: 0.1415
top acc: 0.1082 ::: bot acc: 0.0452
current epoch: 24
train loss is 0.005012
average val loss: 0.004308, accuracy: 0.0803
average test loss: 0.004439, accuracy: 0.0777
case acc: 0.067265294
case acc: 0.04532798
case acc: 0.09577587
case acc: 0.078484364
case acc: 0.120010525
case acc: 0.05925725
top acc: 0.0516 ::: bot acc: 0.1125
top acc: 0.0890 ::: bot acc: 0.0309
top acc: 0.0330 ::: bot acc: 0.1587
top acc: 0.1268 ::: bot acc: 0.0791
top acc: 0.1312 ::: bot acc: 0.1446
top acc: 0.1050 ::: bot acc: 0.0489
current epoch: 25
train loss is 0.004961
average val loss: 0.004267, accuracy: 0.0798
average test loss: 0.004407, accuracy: 0.0774
case acc: 0.06689297
case acc: 0.044944637
case acc: 0.09459859
case acc: 0.078588106
case acc: 0.120185025
case acc: 0.05916028
top acc: 0.0537 ::: bot acc: 0.1108
top acc: 0.0874 ::: bot acc: 0.0322
top acc: 0.0325 ::: bot acc: 0.1571
top acc: 0.1278 ::: bot acc: 0.0780
top acc: 0.1312 ::: bot acc: 0.1450
top acc: 0.1051 ::: bot acc: 0.0484
current epoch: 26
train loss is 0.004934
average val loss: 0.004211, accuracy: 0.0793
average test loss: 0.004366, accuracy: 0.0770
case acc: 0.06611265
case acc: 0.04516125
case acc: 0.09264606
case acc: 0.07881342
case acc: 0.11986421
case acc: 0.059406642
top acc: 0.0565 ::: bot acc: 0.1079
top acc: 0.0869 ::: bot acc: 0.0332
top acc: 0.0319 ::: bot acc: 0.1547
top acc: 0.1299 ::: bot acc: 0.0763
top acc: 0.1317 ::: bot acc: 0.1444
top acc: 0.1060 ::: bot acc: 0.0475
current epoch: 27
train loss is 0.004911
average val loss: 0.004169, accuracy: 0.0788
average test loss: 0.004336, accuracy: 0.0767
case acc: 0.06563859
case acc: 0.04483817
case acc: 0.09137022
case acc: 0.07872662
case acc: 0.120032206
case acc: 0.05937543
top acc: 0.0585 ::: bot acc: 0.1060
top acc: 0.0858 ::: bot acc: 0.0343
top acc: 0.0312 ::: bot acc: 0.1529
top acc: 0.1310 ::: bot acc: 0.0747
top acc: 0.1316 ::: bot acc: 0.1446
top acc: 0.1060 ::: bot acc: 0.0474
current epoch: 28
train loss is 0.004889
average val loss: 0.004134, accuracy: 0.0785
average test loss: 0.004306, accuracy: 0.0764
case acc: 0.06506099
case acc: 0.044694073
case acc: 0.09038726
case acc: 0.07896217
case acc: 0.12012522
case acc: 0.059205584
top acc: 0.0598 ::: bot acc: 0.1041
top acc: 0.0847 ::: bot acc: 0.0349
top acc: 0.0310 ::: bot acc: 0.1515
top acc: 0.1321 ::: bot acc: 0.0741
top acc: 0.1313 ::: bot acc: 0.1451
top acc: 0.1058 ::: bot acc: 0.0473
current epoch: 29
train loss is 0.004897
average val loss: 0.004016, accuracy: 0.0772
average test loss: 0.004250, accuracy: 0.0756
case acc: 0.064068414
case acc: 0.045095608
case acc: 0.08620082
case acc: 0.07923547
case acc: 0.11870369
case acc: 0.060283195
top acc: 0.0661 ::: bot acc: 0.0986
top acc: 0.0879 ::: bot acc: 0.0318
top acc: 0.0299 ::: bot acc: 0.1460
top acc: 0.1370 ::: bot acc: 0.0689
top acc: 0.1350 ::: bot acc: 0.1411
top acc: 0.1101 ::: bot acc: 0.0433
current epoch: 30
train loss is 0.004881
average val loss: 0.003977, accuracy: 0.0767
average test loss: 0.004222, accuracy: 0.0753
case acc: 0.063809514
case acc: 0.045200538
case acc: 0.08482114
case acc: 0.07945122
case acc: 0.11840223
case acc: 0.060319576
top acc: 0.0679 ::: bot acc: 0.0965
top acc: 0.0881 ::: bot acc: 0.0319
top acc: 0.0299 ::: bot acc: 0.1437
top acc: 0.1383 ::: bot acc: 0.0679
top acc: 0.1351 ::: bot acc: 0.1405
top acc: 0.1101 ::: bot acc: 0.0432
current epoch: 31
train loss is 0.004871
average val loss: 0.003942, accuracy: 0.0763
average test loss: 0.004203, accuracy: 0.0751
case acc: 0.06347886
case acc: 0.04533217
case acc: 0.08341229
case acc: 0.07962867
case acc: 0.11786698
case acc: 0.06060541
top acc: 0.0705 ::: bot acc: 0.0941
top acc: 0.0884 ::: bot acc: 0.0318
top acc: 0.0302 ::: bot acc: 0.1416
top acc: 0.1401 ::: bot acc: 0.0658
top acc: 0.1366 ::: bot acc: 0.1390
top acc: 0.1115 ::: bot acc: 0.0421
current epoch: 32
train loss is 0.004875
average val loss: 0.003902, accuracy: 0.0757
average test loss: 0.004173, accuracy: 0.0747
case acc: 0.062959656
case acc: 0.045245133
case acc: 0.08173733
case acc: 0.079923175
case acc: 0.11711274
case acc: 0.06092864
top acc: 0.0725 ::: bot acc: 0.0917
top acc: 0.0888 ::: bot acc: 0.0309
top acc: 0.0303 ::: bot acc: 0.1390
top acc: 0.1423 ::: bot acc: 0.0637
top acc: 0.1383 ::: bot acc: 0.1373
top acc: 0.1128 ::: bot acc: 0.0404
current epoch: 33
train loss is 0.004877
average val loss: 0.003894, accuracy: 0.0756
average test loss: 0.004166, accuracy: 0.0746
case acc: 0.06285701
case acc: 0.045207825
case acc: 0.081231065
case acc: 0.080064036
case acc: 0.11711767
case acc: 0.060887195
top acc: 0.0737 ::: bot acc: 0.0908
top acc: 0.0877 ::: bot acc: 0.0322
top acc: 0.0301 ::: bot acc: 0.1382
top acc: 0.1428 ::: bot acc: 0.0634
top acc: 0.1385 ::: bot acc: 0.1371
top acc: 0.1127 ::: bot acc: 0.0408
current epoch: 34
train loss is 0.004853
average val loss: 0.003879, accuracy: 0.0754
average test loss: 0.004156, accuracy: 0.0745
case acc: 0.0631015
case acc: 0.044926643
case acc: 0.08089027
case acc: 0.08000363
case acc: 0.117335126
case acc: 0.06078633
top acc: 0.0741 ::: bot acc: 0.0905
top acc: 0.0867 ::: bot acc: 0.0332
top acc: 0.0301 ::: bot acc: 0.1378
top acc: 0.1429 ::: bot acc: 0.0632
top acc: 0.1385 ::: bot acc: 0.1370
top acc: 0.1123 ::: bot acc: 0.0413
current epoch: 35
train loss is 0.004852
average val loss: 0.003862, accuracy: 0.0752
average test loss: 0.004149, accuracy: 0.0744
case acc: 0.06293749
case acc: 0.045030285
case acc: 0.0804678
case acc: 0.08014119
case acc: 0.11715873
case acc: 0.060736354
top acc: 0.0745 ::: bot acc: 0.0902
top acc: 0.0863 ::: bot acc: 0.0338
top acc: 0.0304 ::: bot acc: 0.1368
top acc: 0.1435 ::: bot acc: 0.0625
top acc: 0.1384 ::: bot acc: 0.1370
top acc: 0.1126 ::: bot acc: 0.0407
current epoch: 36
train loss is 0.004843
average val loss: 0.003835, accuracy: 0.0749
average test loss: 0.004140, accuracy: 0.0743
case acc: 0.0628482
case acc: 0.04507188
case acc: 0.07957867
case acc: 0.08035392
case acc: 0.11679193
case acc: 0.061062723
top acc: 0.0759 ::: bot acc: 0.0889
top acc: 0.0866 ::: bot acc: 0.0337
top acc: 0.0306 ::: bot acc: 0.1354
top acc: 0.1450 ::: bot acc: 0.0608
top acc: 0.1391 ::: bot acc: 0.1361
top acc: 0.1133 ::: bot acc: 0.0401
current epoch: 37
train loss is 0.004828
average val loss: 0.003837, accuracy: 0.0749
average test loss: 0.004134, accuracy: 0.0742
case acc: 0.06290609
case acc: 0.044832554
case acc: 0.07960456
case acc: 0.08030498
case acc: 0.11668956
case acc: 0.06087626
top acc: 0.0757 ::: bot acc: 0.0892
top acc: 0.0855 ::: bot acc: 0.0345
top acc: 0.0304 ::: bot acc: 0.1357
top acc: 0.1449 ::: bot acc: 0.0609
top acc: 0.1391 ::: bot acc: 0.1362
top acc: 0.1128 ::: bot acc: 0.0405
current epoch: 38
train loss is 0.004831
average val loss: 0.003840, accuracy: 0.0749
average test loss: 0.004124, accuracy: 0.0741
case acc: 0.0629468
case acc: 0.044709835
case acc: 0.07946155
case acc: 0.08025059
case acc: 0.11659725
case acc: 0.06089228
top acc: 0.0757 ::: bot acc: 0.0893
top acc: 0.0845 ::: bot acc: 0.0355
top acc: 0.0304 ::: bot acc: 0.1355
top acc: 0.1447 ::: bot acc: 0.0609
top acc: 0.1393 ::: bot acc: 0.1357
top acc: 0.1126 ::: bot acc: 0.0409
current epoch: 39
train loss is 0.004809
average val loss: 0.003845, accuracy: 0.0750
average test loss: 0.004129, accuracy: 0.0743
case acc: 0.06312359
case acc: 0.044457808
case acc: 0.080312625
case acc: 0.08009236
case acc: 0.11709849
case acc: 0.06053553
top acc: 0.0742 ::: bot acc: 0.0907
top acc: 0.0822 ::: bot acc: 0.0379
top acc: 0.0304 ::: bot acc: 0.1366
top acc: 0.1439 ::: bot acc: 0.0619
top acc: 0.1381 ::: bot acc: 0.1371
top acc: 0.1113 ::: bot acc: 0.0424
current epoch: 40
train loss is 0.004802
average val loss: 0.003851, accuracy: 0.0751
average test loss: 0.004129, accuracy: 0.0743
case acc: 0.06320276
case acc: 0.044265196
case acc: 0.08076401
case acc: 0.08015709
case acc: 0.11725291
case acc: 0.06038027
top acc: 0.0736 ::: bot acc: 0.0913
top acc: 0.0805 ::: bot acc: 0.0393
top acc: 0.0303 ::: bot acc: 0.1374
top acc: 0.1434 ::: bot acc: 0.0628
top acc: 0.1375 ::: bot acc: 0.1377
top acc: 0.1107 ::: bot acc: 0.0428
current epoch: 41
train loss is 0.004790
average val loss: 0.003869, accuracy: 0.0753
average test loss: 0.004139, accuracy: 0.0745
case acc: 0.06326003
case acc: 0.04421133
case acc: 0.08170071
case acc: 0.07975706
case acc: 0.11765188
case acc: 0.06031945
top acc: 0.0722 ::: bot acc: 0.0926
top acc: 0.0787 ::: bot acc: 0.0414
top acc: 0.0302 ::: bot acc: 0.1388
top acc: 0.1424 ::: bot acc: 0.0634
top acc: 0.1369 ::: bot acc: 0.1387
top acc: 0.1101 ::: bot acc: 0.0440
current epoch: 42
train loss is 0.004793
average val loss: 0.003901, accuracy: 0.0757
average test loss: 0.004149, accuracy: 0.0747
case acc: 0.0636107
case acc: 0.04411281
case acc: 0.083032295
case acc: 0.07954285
case acc: 0.118157916
case acc: 0.05979392
top acc: 0.0702 ::: bot acc: 0.0946
top acc: 0.0760 ::: bot acc: 0.0441
top acc: 0.0302 ::: bot acc: 0.1409
top acc: 0.1407 ::: bot acc: 0.0652
top acc: 0.1358 ::: bot acc: 0.1400
top acc: 0.1084 ::: bot acc: 0.0452
current epoch: 43
train loss is 0.004787
average val loss: 0.003950, accuracy: 0.0763
average test loss: 0.004174, accuracy: 0.0750
case acc: 0.063881926
case acc: 0.04413095
case acc: 0.084551126
case acc: 0.07933851
case acc: 0.1187203
case acc: 0.059520107
top acc: 0.0682 ::: bot acc: 0.0965
top acc: 0.0732 ::: bot acc: 0.0468
top acc: 0.0300 ::: bot acc: 0.1432
top acc: 0.1391 ::: bot acc: 0.0667
top acc: 0.1352 ::: bot acc: 0.1412
top acc: 0.1070 ::: bot acc: 0.0465
current epoch: 44
train loss is 0.004766
average val loss: 0.004019, accuracy: 0.0771
average test loss: 0.004204, accuracy: 0.0755
case acc: 0.064463936
case acc: 0.04440236
case acc: 0.08676062
case acc: 0.07915253
case acc: 0.11929513
case acc: 0.058987394
top acc: 0.0653 ::: bot acc: 0.0995
top acc: 0.0697 ::: bot acc: 0.0503
top acc: 0.0298 ::: bot acc: 0.1468
top acc: 0.1365 ::: bot acc: 0.0694
top acc: 0.1336 ::: bot acc: 0.1429
top acc: 0.1047 ::: bot acc: 0.0486
current epoch: 45
train loss is 0.004762
average val loss: 0.004148, accuracy: 0.0786
average test loss: 0.004272, accuracy: 0.0765
case acc: 0.06542197
case acc: 0.045104038
case acc: 0.090578154
case acc: 0.078756995
case acc: 0.12052175
case acc: 0.05845137
top acc: 0.0606 ::: bot acc: 0.1043
top acc: 0.0641 ::: bot acc: 0.0560
top acc: 0.0309 ::: bot acc: 0.1518
top acc: 0.1321 ::: bot acc: 0.0742
top acc: 0.1305 ::: bot acc: 0.1461
top acc: 0.1007 ::: bot acc: 0.0531
current epoch: 46
train loss is 0.004763
average val loss: 0.004308, accuracy: 0.0803
average test loss: 0.004370, accuracy: 0.0777
case acc: 0.066757575
case acc: 0.046214227
case acc: 0.09524875
case acc: 0.07829089
case acc: 0.12203874
case acc: 0.057890702
top acc: 0.0553 ::: bot acc: 0.1095
top acc: 0.0581 ::: bot acc: 0.0619
top acc: 0.0329 ::: bot acc: 0.1579
top acc: 0.1269 ::: bot acc: 0.0789
top acc: 0.1270 ::: bot acc: 0.1503
top acc: 0.0961 ::: bot acc: 0.0572
current epoch: 47
train loss is 0.004785
average val loss: 0.004532, accuracy: 0.0826
average test loss: 0.004529, accuracy: 0.0795
case acc: 0.068508446
case acc: 0.04809657
case acc: 0.10063652
case acc: 0.07829335
case acc: 0.12404538
case acc: 0.057604037
top acc: 0.0497 ::: bot acc: 0.1153
top acc: 0.0514 ::: bot acc: 0.0687
top acc: 0.0356 ::: bot acc: 0.1646
top acc: 0.1215 ::: bot acc: 0.0850
top acc: 0.1230 ::: bot acc: 0.1552
top acc: 0.0915 ::: bot acc: 0.0620
current epoch: 48
train loss is 0.004815
average val loss: 0.004803, accuracy: 0.0852
average test loss: 0.004719, accuracy: 0.0816
case acc: 0.07054386
case acc: 0.050569512
case acc: 0.10665143
case acc: 0.07819948
case acc: 0.12586716
case acc: 0.057696365
top acc: 0.0439 ::: bot acc: 0.1212
top acc: 0.0445 ::: bot acc: 0.0758
top acc: 0.0393 ::: bot acc: 0.1718
top acc: 0.1153 ::: bot acc: 0.0909
top acc: 0.1191 ::: bot acc: 0.1599
top acc: 0.0866 ::: bot acc: 0.0669
current epoch: 49
train loss is 0.004885
average val loss: 0.005041, accuracy: 0.0874
average test loss: 0.004898, accuracy: 0.0834
case acc: 0.07238055
case acc: 0.052931003
case acc: 0.112183355
case acc: 0.07824903
case acc: 0.12703101
case acc: 0.05774972
top acc: 0.0406 ::: bot acc: 0.1257
top acc: 0.0399 ::: bot acc: 0.0816
top acc: 0.0436 ::: bot acc: 0.1780
top acc: 0.1109 ::: bot acc: 0.0953
top acc: 0.1173 ::: bot acc: 0.1627
top acc: 0.0835 ::: bot acc: 0.0700
current epoch: 50
train loss is 0.004962
average val loss: 0.005533, accuracy: 0.0917
average test loss: 0.005262, accuracy: 0.0869
case acc: 0.07617182
case acc: 0.057485823
case acc: 0.12156303
case acc: 0.078795694
case acc: 0.12935005
case acc: 0.058072787
top acc: 0.0356 ::: bot acc: 0.1338
top acc: 0.0336 ::: bot acc: 0.0917
top acc: 0.0511 ::: bot acc: 0.1883
top acc: 0.1023 ::: bot acc: 0.1038
top acc: 0.1121 ::: bot acc: 0.1686
top acc: 0.0771 ::: bot acc: 0.0766
LME_Co_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5142 5142 5142
1.7082474 -0.6288155 0.48738334 -0.25570297
Validation: 576 576 576
Testing: 750 750 750
pre-processing time: 0.00019979476928710938
the split date is 2012-01-01
train dropout: 0.4 test dropout: 0.1
net initializing with time: 0.0022788047790527344
preparing training and testing date with time: 0.0
current epoch: 1
train loss is 0.012805
average val loss: 0.010191, accuracy: 0.1230
average test loss: 0.006643, accuracy: 0.0952
case acc: 0.07624997
case acc: 0.18081774
case acc: 0.080236256
case acc: 0.07916056
case acc: 0.08970744
case acc: 0.06530991
top acc: 0.0288 ::: bot acc: 0.1282
top acc: 0.2327 ::: bot acc: 0.1239
top acc: 0.1462 ::: bot acc: 0.0376
top acc: 0.1590 ::: bot acc: 0.0250
top acc: 0.1511 ::: bot acc: 0.0301
top acc: 0.1132 ::: bot acc: 0.0382
current epoch: 2
train loss is 0.010389
average val loss: 0.006271, accuracy: 0.0934
average test loss: 0.017922, accuracy: 0.1668
case acc: 0.053135164
case acc: 0.27804142
case acc: 0.16588746
case acc: 0.16950251
case acc: 0.19145845
case acc: 0.14257865
top acc: 0.0927 ::: bot acc: 0.0288
top acc: 0.3287 ::: bot acc: 0.2205
top acc: 0.2553 ::: bot acc: 0.0758
top acc: 0.2645 ::: bot acc: 0.0827
top acc: 0.2559 ::: bot acc: 0.1270
top acc: 0.2154 ::: bot acc: 0.0651
current epoch: 3
train loss is 0.012072
average val loss: 0.007221, accuracy: 0.0967
average test loss: 0.024273, accuracy: 0.2017
case acc: 0.08464141
case acc: 0.3059834
case acc: 0.20404042
case acc: 0.20697746
case acc: 0.23127668
case acc: 0.17704567
top acc: 0.1360 ::: bot acc: 0.0379
top acc: 0.3568 ::: bot acc: 0.2484
top acc: 0.2938 ::: bot acc: 0.1130
top acc: 0.3025 ::: bot acc: 0.1205
top acc: 0.2964 ::: bot acc: 0.1667
top acc: 0.2494 ::: bot acc: 0.0991
current epoch: 4
train loss is 0.013886
average val loss: 0.005423, accuracy: 0.0889
average test loss: 0.013185, accuracy: 0.1394
case acc: 0.042556863
case acc: 0.22983828
case acc: 0.13928017
case acc: 0.14166918
case acc: 0.17121777
case acc: 0.11195711
top acc: 0.0734 ::: bot acc: 0.0377
top acc: 0.2810 ::: bot acc: 0.1723
top acc: 0.2283 ::: bot acc: 0.0515
top acc: 0.2379 ::: bot acc: 0.0540
top acc: 0.2354 ::: bot acc: 0.1069
top acc: 0.1840 ::: bot acc: 0.0354
current epoch: 5
train loss is 0.011497
average val loss: 0.008822, accuracy: 0.1111
average test loss: 0.005735, accuracy: 0.0893
case acc: 0.0638992
case acc: 0.14361729
case acc: 0.08073492
case acc: 0.08098786
case acc: 0.102554575
case acc: 0.06417079
top acc: 0.0239 ::: bot acc: 0.1115
top acc: 0.1943 ::: bot acc: 0.0869
top acc: 0.1481 ::: bot acc: 0.0372
top acc: 0.1622 ::: bot acc: 0.0224
top acc: 0.1670 ::: bot acc: 0.0391
top acc: 0.1106 ::: bot acc: 0.0395
current epoch: 6
train loss is 0.007983
average val loss: 0.008415, accuracy: 0.1076
average test loss: 0.005750, accuracy: 0.0895
case acc: 0.060754556
case acc: 0.13695331
case acc: 0.08034355
case acc: 0.08242849
case acc: 0.10961788
case acc: 0.06715544
top acc: 0.0234 ::: bot acc: 0.1072
top acc: 0.1881 ::: bot acc: 0.0798
top acc: 0.1479 ::: bot acc: 0.0374
top acc: 0.1657 ::: bot acc: 0.0195
top acc: 0.1740 ::: bot acc: 0.0461
top acc: 0.1178 ::: bot acc: 0.0351
current epoch: 7
train loss is 0.006860
average val loss: 0.006207, accuracy: 0.0933
average test loss: 0.007828, accuracy: 0.1047
case acc: 0.043210056
case acc: 0.15947576
case acc: 0.096121505
case acc: 0.10380953
case acc: 0.14128625
case acc: 0.08429391
top acc: 0.0355 ::: bot acc: 0.0749
top acc: 0.2107 ::: bot acc: 0.1024
top acc: 0.1748 ::: bot acc: 0.0296
top acc: 0.1954 ::: bot acc: 0.0225
top acc: 0.2063 ::: bot acc: 0.0774
top acc: 0.1490 ::: bot acc: 0.0227
current epoch: 8
train loss is 0.006729
average val loss: 0.005964, accuracy: 0.0909
average test loss: 0.007874, accuracy: 0.1051
case acc: 0.041571762
case acc: 0.15436864
case acc: 0.097382955
case acc: 0.10661034
case acc: 0.14447126
case acc: 0.086230084
top acc: 0.0398 ::: bot acc: 0.0700
top acc: 0.2053 ::: bot acc: 0.0978
top acc: 0.1769 ::: bot acc: 0.0301
top acc: 0.1989 ::: bot acc: 0.0248
top acc: 0.2094 ::: bot acc: 0.0808
top acc: 0.1524 ::: bot acc: 0.0222
current epoch: 9
train loss is 0.006622
average val loss: 0.006510, accuracy: 0.0934
average test loss: 0.006762, accuracy: 0.0969
case acc: 0.044736363
case acc: 0.13535626
case acc: 0.089450374
case acc: 0.09783877
case acc: 0.13472265
case acc: 0.07955256
top acc: 0.0313 ::: bot acc: 0.0794
top acc: 0.1863 ::: bot acc: 0.0785
top acc: 0.1653 ::: bot acc: 0.0309
top acc: 0.1882 ::: bot acc: 0.0189
top acc: 0.1990 ::: bot acc: 0.0713
top acc: 0.1419 ::: bot acc: 0.0233
current epoch: 10
train loss is 0.006262
average val loss: 0.006232, accuracy: 0.0908
average test loss: 0.006824, accuracy: 0.0973
case acc: 0.042502124
case acc: 0.13039163
case acc: 0.09026468
case acc: 0.10036919
case acc: 0.13840464
case acc: 0.08197761
top acc: 0.0349 ::: bot acc: 0.0737
top acc: 0.1817 ::: bot acc: 0.0734
top acc: 0.1659 ::: bot acc: 0.0307
top acc: 0.1911 ::: bot acc: 0.0207
top acc: 0.2031 ::: bot acc: 0.0749
top acc: 0.1455 ::: bot acc: 0.0228
current epoch: 11
train loss is 0.006062
average val loss: 0.006424, accuracy: 0.0913
average test loss: 0.006424, accuracy: 0.0941
case acc: 0.043351363
case acc: 0.12010515
case acc: 0.087249465
case acc: 0.0984492
case acc: 0.13546304
case acc: 0.08010222
top acc: 0.0341 ::: bot acc: 0.0757
top acc: 0.1712 ::: bot acc: 0.0634
top acc: 0.1615 ::: bot acc: 0.0316
top acc: 0.1889 ::: bot acc: 0.0197
top acc: 0.2001 ::: bot acc: 0.0721
top acc: 0.1429 ::: bot acc: 0.0228
current epoch: 12
train loss is 0.005876
average val loss: 0.006404, accuracy: 0.0906
average test loss: 0.006250, accuracy: 0.0926
case acc: 0.042695146
case acc: 0.112958595
case acc: 0.08660908
case acc: 0.098008364
case acc: 0.13469127
case acc: 0.08079442
top acc: 0.0347 ::: bot acc: 0.0746
top acc: 0.1641 ::: bot acc: 0.0561
top acc: 0.1602 ::: bot acc: 0.0325
top acc: 0.1883 ::: bot acc: 0.0195
top acc: 0.1993 ::: bot acc: 0.0718
top acc: 0.1440 ::: bot acc: 0.0232
current epoch: 13
train loss is 0.005715
average val loss: 0.006395, accuracy: 0.0900
average test loss: 0.006121, accuracy: 0.0915
case acc: 0.042227935
case acc: 0.1064797
case acc: 0.08635227
case acc: 0.098865464
case acc: 0.13391164
case acc: 0.08095451
top acc: 0.0366 ::: bot acc: 0.0722
top acc: 0.1571 ::: bot acc: 0.0501
top acc: 0.1595 ::: bot acc: 0.0329
top acc: 0.1894 ::: bot acc: 0.0196
top acc: 0.1981 ::: bot acc: 0.0710
top acc: 0.1441 ::: bot acc: 0.0226
current epoch: 14
train loss is 0.005569
average val loss: 0.006518, accuracy: 0.0908
average test loss: 0.005874, accuracy: 0.0892
case acc: 0.042079438
case acc: 0.09873538
case acc: 0.08525053
case acc: 0.097798474
case acc: 0.13131575
case acc: 0.080066904
top acc: 0.0367 ::: bot acc: 0.0723
top acc: 0.1497 ::: bot acc: 0.0421
top acc: 0.1574 ::: bot acc: 0.0337
top acc: 0.1881 ::: bot acc: 0.0188
top acc: 0.1958 ::: bot acc: 0.0683
top acc: 0.1425 ::: bot acc: 0.0233
current epoch: 15
train loss is 0.005465
average val loss: 0.006039, accuracy: 0.0871
average test loss: 0.006272, accuracy: 0.0923
case acc: 0.039810874
case acc: 0.09984381
case acc: 0.08899156
case acc: 0.1040375
case acc: 0.13681804
case acc: 0.08441457
top acc: 0.0453 ::: bot acc: 0.0635
top acc: 0.1508 ::: bot acc: 0.0429
top acc: 0.1641 ::: bot acc: 0.0313
top acc: 0.1953 ::: bot acc: 0.0232
top acc: 0.2013 ::: bot acc: 0.0741
top acc: 0.1493 ::: bot acc: 0.0223
current epoch: 16
train loss is 0.005284
average val loss: 0.006125, accuracy: 0.0877
average test loss: 0.006043, accuracy: 0.0903
case acc: 0.039630156
case acc: 0.092917584
case acc: 0.08843479
case acc: 0.10310868
case acc: 0.13422856
case acc: 0.08319701
top acc: 0.0459 ::: bot acc: 0.0628
top acc: 0.1441 ::: bot acc: 0.0359
top acc: 0.1628 ::: bot acc: 0.0320
top acc: 0.1939 ::: bot acc: 0.0227
top acc: 0.1984 ::: bot acc: 0.0719
top acc: 0.1473 ::: bot acc: 0.0224
current epoch: 17
train loss is 0.005211
average val loss: 0.006259, accuracy: 0.0887
average test loss: 0.005797, accuracy: 0.0880
case acc: 0.039756063
case acc: 0.08602063
case acc: 0.08719383
case acc: 0.10183014
case acc: 0.13085952
case acc: 0.08220208
top acc: 0.0457 ::: bot acc: 0.0627
top acc: 0.1369 ::: bot acc: 0.0293
top acc: 0.1606 ::: bot acc: 0.0326
top acc: 0.1926 ::: bot acc: 0.0218
top acc: 0.1953 ::: bot acc: 0.0682
top acc: 0.1459 ::: bot acc: 0.0225
current epoch: 18
train loss is 0.005112
average val loss: 0.006273, accuracy: 0.0887
average test loss: 0.005670, accuracy: 0.0866
case acc: 0.039323404
case acc: 0.08084376
case acc: 0.08693318
case acc: 0.101538956
case acc: 0.12934612
case acc: 0.08174099
top acc: 0.0469 ::: bot acc: 0.0609
top acc: 0.1315 ::: bot acc: 0.0242
top acc: 0.1609 ::: bot acc: 0.0322
top acc: 0.1917 ::: bot acc: 0.0218
top acc: 0.1935 ::: bot acc: 0.0669
top acc: 0.1453 ::: bot acc: 0.0228
current epoch: 19
train loss is 0.005054
average val loss: 0.006286, accuracy: 0.0889
average test loss: 0.005567, accuracy: 0.0856
case acc: 0.03911718
case acc: 0.076510474
case acc: 0.08710776
case acc: 0.10141935
case acc: 0.12789255
case acc: 0.08129095
top acc: 0.0486 ::: bot acc: 0.0595
top acc: 0.1270 ::: bot acc: 0.0207
top acc: 0.1608 ::: bot acc: 0.0325
top acc: 0.1917 ::: bot acc: 0.0216
top acc: 0.1923 ::: bot acc: 0.0654
top acc: 0.1444 ::: bot acc: 0.0226
current epoch: 20
train loss is 0.004983
average val loss: 0.006195, accuracy: 0.0882
average test loss: 0.005593, accuracy: 0.0856
case acc: 0.038910266
case acc: 0.074332945
case acc: 0.0879837
case acc: 0.10275405
case acc: 0.12806477
case acc: 0.08183846
top acc: 0.0518 ::: bot acc: 0.0562
top acc: 0.1245 ::: bot acc: 0.0189
top acc: 0.1625 ::: bot acc: 0.0318
top acc: 0.1935 ::: bot acc: 0.0224
top acc: 0.1922 ::: bot acc: 0.0655
top acc: 0.1449 ::: bot acc: 0.0227
current epoch: 21
train loss is 0.004974
average val loss: 0.005937, accuracy: 0.0861
average test loss: 0.005817, accuracy: 0.0875
case acc: 0.038888212
case acc: 0.0744218
case acc: 0.090726316
case acc: 0.10644721
case acc: 0.13061659
case acc: 0.08394311
top acc: 0.0575 ::: bot acc: 0.0505
top acc: 0.1247 ::: bot acc: 0.0193
top acc: 0.1669 ::: bot acc: 0.0310
top acc: 0.1976 ::: bot acc: 0.0249
top acc: 0.1946 ::: bot acc: 0.0685
top acc: 0.1487 ::: bot acc: 0.0223
current epoch: 22
train loss is 0.004882
average val loss: 0.005867, accuracy: 0.0856
average test loss: 0.005848, accuracy: 0.0877
case acc: 0.03912878
case acc: 0.07228702
case acc: 0.09193994
case acc: 0.1074233
case acc: 0.1305694
case acc: 0.08460202
top acc: 0.0603 ::: bot acc: 0.0478
top acc: 0.1222 ::: bot acc: 0.0177
top acc: 0.1690 ::: bot acc: 0.0309
top acc: 0.1990 ::: bot acc: 0.0256
top acc: 0.1946 ::: bot acc: 0.0683
top acc: 0.1492 ::: bot acc: 0.0221
current epoch: 23
train loss is 0.004848
average val loss: 0.005950, accuracy: 0.0863
average test loss: 0.005684, accuracy: 0.0861
case acc: 0.039055288
case acc: 0.06836091
case acc: 0.09134395
case acc: 0.10603254
case acc: 0.12844472
case acc: 0.08361881
top acc: 0.0603 ::: bot acc: 0.0471
top acc: 0.1175 ::: bot acc: 0.0152
top acc: 0.1680 ::: bot acc: 0.0307
top acc: 0.1972 ::: bot acc: 0.0247
top acc: 0.1926 ::: bot acc: 0.0666
top acc: 0.1480 ::: bot acc: 0.0225
current epoch: 24
train loss is 0.004807
average val loss: 0.006201, accuracy: 0.0885
average test loss: 0.005373, accuracy: 0.0832
case acc: 0.038913194
case acc: 0.06305969
case acc: 0.089390494
case acc: 0.10304139
case acc: 0.12404156
case acc: 0.08083811
top acc: 0.0587 ::: bot acc: 0.0491
top acc: 0.1109 ::: bot acc: 0.0129
top acc: 0.1648 ::: bot acc: 0.0316
top acc: 0.1939 ::: bot acc: 0.0226
top acc: 0.1881 ::: bot acc: 0.0618
top acc: 0.1435 ::: bot acc: 0.0229
current epoch: 25
train loss is 0.004781
average val loss: 0.006076, accuracy: 0.0875
average test loss: 0.005465, accuracy: 0.0840
case acc: 0.039233647
case acc: 0.06257786
case acc: 0.09068477
case acc: 0.10469576
case acc: 0.12494887
case acc: 0.081816144
top acc: 0.0618 ::: bot acc: 0.0455
top acc: 0.1103 ::: bot acc: 0.0124
top acc: 0.1672 ::: bot acc: 0.0307
top acc: 0.1957 ::: bot acc: 0.0237
top acc: 0.1890 ::: bot acc: 0.0628
top acc: 0.1451 ::: bot acc: 0.0227
current epoch: 26
train loss is 0.004742
average val loss: 0.006308, accuracy: 0.0897
average test loss: 0.005205, accuracy: 0.0817
case acc: 0.039041676
case acc: 0.0586807
case acc: 0.08933803
case acc: 0.10212538
case acc: 0.12086938
case acc: 0.07995038
top acc: 0.0602 ::: bot acc: 0.0470
top acc: 0.1041 ::: bot acc: 0.0128
top acc: 0.1649 ::: bot acc: 0.0317
top acc: 0.1926 ::: bot acc: 0.0218
top acc: 0.1846 ::: bot acc: 0.0589
top acc: 0.1416 ::: bot acc: 0.0236
current epoch: 27
train loss is 0.004795
average val loss: 0.006021, accuracy: 0.0873
average test loss: 0.005443, accuracy: 0.0838
case acc: 0.03995764
case acc: 0.059589706
case acc: 0.09196262
case acc: 0.10540631
case acc: 0.12396891
case acc: 0.08179365
top acc: 0.0656 ::: bot acc: 0.0417
top acc: 0.1055 ::: bot acc: 0.0128
top acc: 0.1692 ::: bot acc: 0.0307
top acc: 0.1963 ::: bot acc: 0.0243
top acc: 0.1879 ::: bot acc: 0.0620
top acc: 0.1449 ::: bot acc: 0.0226
current epoch: 28
train loss is 0.004708
average val loss: 0.005940, accuracy: 0.0866
average test loss: 0.005494, accuracy: 0.0843
case acc: 0.040584728
case acc: 0.059140187
case acc: 0.0930144
case acc: 0.106400765
case acc: 0.12408113
case acc: 0.08231633
top acc: 0.0680 ::: bot acc: 0.0393
top acc: 0.1049 ::: bot acc: 0.0130
top acc: 0.1708 ::: bot acc: 0.0304
top acc: 0.1972 ::: bot acc: 0.0252
top acc: 0.1880 ::: bot acc: 0.0624
top acc: 0.1460 ::: bot acc: 0.0225
current epoch: 29
train loss is 0.004651
average val loss: 0.006000, accuracy: 0.0872
average test loss: 0.005420, accuracy: 0.0836
case acc: 0.04084171
case acc: 0.057259083
case acc: 0.092929825
case acc: 0.105690576
case acc: 0.12276831
case acc: 0.0818537
top acc: 0.0684 ::: bot acc: 0.0388
top acc: 0.1021 ::: bot acc: 0.0129
top acc: 0.1709 ::: bot acc: 0.0303
top acc: 0.1966 ::: bot acc: 0.0245
top acc: 0.1866 ::: bot acc: 0.0611
top acc: 0.1449 ::: bot acc: 0.0229
current epoch: 30
train loss is 0.004681
average val loss: 0.005868, accuracy: 0.0862
average test loss: 0.005523, accuracy: 0.0845
case acc: 0.041745517
case acc: 0.057426155
case acc: 0.09426944
case acc: 0.10725577
case acc: 0.12395953
case acc: 0.08250922
top acc: 0.0713 ::: bot acc: 0.0361
top acc: 0.1019 ::: bot acc: 0.0133
top acc: 0.1729 ::: bot acc: 0.0298
top acc: 0.1982 ::: bot acc: 0.0260
top acc: 0.1878 ::: bot acc: 0.0621
top acc: 0.1462 ::: bot acc: 0.0225
current epoch: 31
train loss is 0.004665
average val loss: 0.005591, accuracy: 0.0837
average test loss: 0.005821, accuracy: 0.0873
case acc: 0.04375422
case acc: 0.059168812
case acc: 0.09740319
case acc: 0.11070942
case acc: 0.12778965
case acc: 0.0849282
top acc: 0.0771 ::: bot acc: 0.0321
top acc: 0.1047 ::: bot acc: 0.0131
top acc: 0.1780 ::: bot acc: 0.0297
top acc: 0.2016 ::: bot acc: 0.0289
top acc: 0.1917 ::: bot acc: 0.0664
top acc: 0.1493 ::: bot acc: 0.0227
current epoch: 32
train loss is 0.004612
average val loss: 0.005500, accuracy: 0.0830
average test loss: 0.005906, accuracy: 0.0881
case acc: 0.044724055
case acc: 0.059032347
case acc: 0.09865012
case acc: 0.11192632
case acc: 0.12852179
case acc: 0.08572073
top acc: 0.0789 ::: bot acc: 0.0308
top acc: 0.1046 ::: bot acc: 0.0131
top acc: 0.1794 ::: bot acc: 0.0300
top acc: 0.2030 ::: bot acc: 0.0298
top acc: 0.1921 ::: bot acc: 0.0671
top acc: 0.1508 ::: bot acc: 0.0224
current epoch: 33
train loss is 0.004613
average val loss: 0.005683, accuracy: 0.0848
average test loss: 0.005666, accuracy: 0.0858
case acc: 0.043721065
case acc: 0.056129735
case acc: 0.0968286
case acc: 0.10920951
case acc: 0.12547112
case acc: 0.08366928
top acc: 0.0766 ::: bot acc: 0.0318
top acc: 0.0998 ::: bot acc: 0.0139
top acc: 0.1767 ::: bot acc: 0.0300
top acc: 0.2003 ::: bot acc: 0.0276
top acc: 0.1891 ::: bot acc: 0.0639
top acc: 0.1481 ::: bot acc: 0.0222
current epoch: 34
train loss is 0.004590
average val loss: 0.006200, accuracy: 0.0896
average test loss: 0.005124, accuracy: 0.0809
case acc: 0.041357085
case acc: 0.05152388
case acc: 0.09238251
case acc: 0.10265858
case acc: 0.11801219
case acc: 0.0793113
top acc: 0.0703 ::: bot acc: 0.0366
top acc: 0.0908 ::: bot acc: 0.0181
top acc: 0.1701 ::: bot acc: 0.0304
top acc: 0.1928 ::: bot acc: 0.0225
top acc: 0.1817 ::: bot acc: 0.0568
top acc: 0.1411 ::: bot acc: 0.0232
current epoch: 35
train loss is 0.004633
average val loss: 0.006254, accuracy: 0.0902
average test loss: 0.005056, accuracy: 0.0803
case acc: 0.0411831
case acc: 0.05096656
case acc: 0.09199136
case acc: 0.10183556
case acc: 0.11695036
case acc: 0.07865365
top acc: 0.0701 ::: bot acc: 0.0367
top acc: 0.0897 ::: bot acc: 0.0196
top acc: 0.1695 ::: bot acc: 0.0305
top acc: 0.1917 ::: bot acc: 0.0219
top acc: 0.1804 ::: bot acc: 0.0557
top acc: 0.1399 ::: bot acc: 0.0232
current epoch: 36
train loss is 0.004615
average val loss: 0.006158, accuracy: 0.0893
average test loss: 0.005153, accuracy: 0.0812
case acc: 0.042112537
case acc: 0.051076446
case acc: 0.093200125
case acc: 0.10331515
case acc: 0.11770591
case acc: 0.07980648
top acc: 0.0724 ::: bot acc: 0.0349
top acc: 0.0899 ::: bot acc: 0.0192
top acc: 0.1717 ::: bot acc: 0.0303
top acc: 0.1935 ::: bot acc: 0.0231
top acc: 0.1809 ::: bot acc: 0.0565
top acc: 0.1418 ::: bot acc: 0.0232
current epoch: 37
train loss is 0.004637
average val loss: 0.005987, accuracy: 0.0877
average test loss: 0.005316, accuracy: 0.0827
case acc: 0.04321
case acc: 0.05208907
case acc: 0.09501506
case acc: 0.10536523
case acc: 0.11955088
case acc: 0.08109404
top acc: 0.0758 ::: bot acc: 0.0320
top acc: 0.0922 ::: bot acc: 0.0177
top acc: 0.1744 ::: bot acc: 0.0296
top acc: 0.1955 ::: bot acc: 0.0247
top acc: 0.1829 ::: bot acc: 0.0586
top acc: 0.1437 ::: bot acc: 0.0226
current epoch: 38
train loss is 0.004577
average val loss: 0.005929, accuracy: 0.0872
average test loss: 0.005365, accuracy: 0.0832
case acc: 0.043789774
case acc: 0.05206276
case acc: 0.09588644
case acc: 0.105973735
case acc: 0.119703494
case acc: 0.08152533
top acc: 0.0769 ::: bot acc: 0.0312
top acc: 0.0924 ::: bot acc: 0.0173
top acc: 0.1757 ::: bot acc: 0.0298
top acc: 0.1961 ::: bot acc: 0.0254
top acc: 0.1829 ::: bot acc: 0.0585
top acc: 0.1447 ::: bot acc: 0.0223
current epoch: 39
train loss is 0.004599
average val loss: 0.005901, accuracy: 0.0869
average test loss: 0.005372, accuracy: 0.0832
case acc: 0.043935884
case acc: 0.05190442
case acc: 0.096424736
case acc: 0.10608421
case acc: 0.119588606
case acc: 0.08136536
top acc: 0.0777 ::: bot acc: 0.0302
top acc: 0.0921 ::: bot acc: 0.0172
top acc: 0.1766 ::: bot acc: 0.0299
top acc: 0.1962 ::: bot acc: 0.0254
top acc: 0.1827 ::: bot acc: 0.0588
top acc: 0.1444 ::: bot acc: 0.0222
current epoch: 40
train loss is 0.004599
average val loss: 0.005800, accuracy: 0.0861
average test loss: 0.005463, accuracy: 0.0841
case acc: 0.045042306
case acc: 0.05228251
case acc: 0.097690694
case acc: 0.10671022
case acc: 0.12070367
case acc: 0.082090616
top acc: 0.0797 ::: bot acc: 0.0292
top acc: 0.0931 ::: bot acc: 0.0165
top acc: 0.1785 ::: bot acc: 0.0299
top acc: 0.1970 ::: bot acc: 0.0257
top acc: 0.1839 ::: bot acc: 0.0597
top acc: 0.1454 ::: bot acc: 0.0227
current epoch: 41
train loss is 0.004564
average val loss: 0.005591, accuracy: 0.0843
average test loss: 0.005690, accuracy: 0.0862
case acc: 0.046808045
case acc: 0.053483207
case acc: 0.10010739
case acc: 0.10933394
case acc: 0.12379522
case acc: 0.08372575
top acc: 0.0830 ::: bot acc: 0.0278
top acc: 0.0958 ::: bot acc: 0.0149
top acc: 0.1818 ::: bot acc: 0.0303
top acc: 0.1998 ::: bot acc: 0.0281
top acc: 0.1869 ::: bot acc: 0.0630
top acc: 0.1481 ::: bot acc: 0.0221
current epoch: 42
train loss is 0.004520
average val loss: 0.005658, accuracy: 0.0849
average test loss: 0.005586, accuracy: 0.0853
case acc: 0.04638321
case acc: 0.052835986
case acc: 0.099327765
case acc: 0.10784788
case acc: 0.12253111
case acc: 0.08272757
top acc: 0.0822 ::: bot acc: 0.0281
top acc: 0.0941 ::: bot acc: 0.0161
top acc: 0.1809 ::: bot acc: 0.0301
top acc: 0.1981 ::: bot acc: 0.0268
top acc: 0.1856 ::: bot acc: 0.0618
top acc: 0.1465 ::: bot acc: 0.0222
current epoch: 43
train loss is 0.004558
average val loss: 0.005407, accuracy: 0.0826
average test loss: 0.005863, accuracy: 0.0879
case acc: 0.04866917
case acc: 0.054501176
case acc: 0.102223516
case acc: 0.110792674
case acc: 0.12625922
case acc: 0.084916934
top acc: 0.0862 ::: bot acc: 0.0266
top acc: 0.0974 ::: bot acc: 0.0146
top acc: 0.1847 ::: bot acc: 0.0307
top acc: 0.2013 ::: bot acc: 0.0293
top acc: 0.1894 ::: bot acc: 0.0655
top acc: 0.1497 ::: bot acc: 0.0222
current epoch: 44
train loss is 0.004525
average val loss: 0.005074, accuracy: 0.0796
average test loss: 0.006315, accuracy: 0.0921
case acc: 0.05247388
case acc: 0.057579186
case acc: 0.10636484
case acc: 0.115663424
case acc: 0.1316907
case acc: 0.08880053
top acc: 0.0923 ::: bot acc: 0.0259
top acc: 0.1026 ::: bot acc: 0.0133
top acc: 0.1907 ::: bot acc: 0.0316
top acc: 0.2063 ::: bot acc: 0.0338
top acc: 0.1947 ::: bot acc: 0.0710
top acc: 0.1548 ::: bot acc: 0.0235
current epoch: 45
train loss is 0.004502
average val loss: 0.004984, accuracy: 0.0788
average test loss: 0.006448, accuracy: 0.0933
case acc: 0.053455424
case acc: 0.0583269
case acc: 0.10770647
case acc: 0.117043264
case acc: 0.13345438
case acc: 0.08980655
top acc: 0.0939 ::: bot acc: 0.0256
top acc: 0.1039 ::: bot acc: 0.0130
top acc: 0.1923 ::: bot acc: 0.0320
top acc: 0.2080 ::: bot acc: 0.0350
top acc: 0.1964 ::: bot acc: 0.0729
top acc: 0.1561 ::: bot acc: 0.0240
current epoch: 46
train loss is 0.004486
average val loss: 0.005209, accuracy: 0.0810
average test loss: 0.006085, accuracy: 0.0899
case acc: 0.050746035
case acc: 0.05534652
case acc: 0.10467619
case acc: 0.11267009
case acc: 0.12939562
case acc: 0.08674763
top acc: 0.0896 ::: bot acc: 0.0260
top acc: 0.0990 ::: bot acc: 0.0138
top acc: 0.1885 ::: bot acc: 0.0310
top acc: 0.2031 ::: bot acc: 0.0310
top acc: 0.1922 ::: bot acc: 0.0689
top acc: 0.1523 ::: bot acc: 0.0225
current epoch: 47
train loss is 0.004498
average val loss: 0.005481, accuracy: 0.0835
average test loss: 0.005733, accuracy: 0.0867
case acc: 0.048235826
case acc: 0.052925218
case acc: 0.10152833
case acc: 0.10826815
case acc: 0.12542099
case acc: 0.08378029
top acc: 0.0855 ::: bot acc: 0.0269
top acc: 0.0941 ::: bot acc: 0.0162
top acc: 0.1839 ::: bot acc: 0.0306
top acc: 0.1986 ::: bot acc: 0.0271
top acc: 0.1884 ::: bot acc: 0.0647
top acc: 0.1478 ::: bot acc: 0.0222
current epoch: 48
train loss is 0.004499
average val loss: 0.005535, accuracy: 0.0841
average test loss: 0.005674, accuracy: 0.0861
case acc: 0.04788923
case acc: 0.05220688
case acc: 0.1010723
case acc: 0.10746936
case acc: 0.12473831
case acc: 0.083395675
top acc: 0.0849 ::: bot acc: 0.0268
top acc: 0.0928 ::: bot acc: 0.0168
top acc: 0.1832 ::: bot acc: 0.0301
top acc: 0.1979 ::: bot acc: 0.0263
top acc: 0.1872 ::: bot acc: 0.0644
top acc: 0.1470 ::: bot acc: 0.0225
current epoch: 49
train loss is 0.004529
average val loss: 0.005368, accuracy: 0.0825
average test loss: 0.005868, accuracy: 0.0879
case acc: 0.049363963
case acc: 0.053233232
case acc: 0.10335508
case acc: 0.109719194
case acc: 0.12666741
case acc: 0.08518587
top acc: 0.0875 ::: bot acc: 0.0261
top acc: 0.0953 ::: bot acc: 0.0148
top acc: 0.1866 ::: bot acc: 0.0309
top acc: 0.2003 ::: bot acc: 0.0283
top acc: 0.1892 ::: bot acc: 0.0664
top acc: 0.1496 ::: bot acc: 0.0228
current epoch: 50
train loss is 0.004527
average val loss: 0.005292, accuracy: 0.0818
average test loss: 0.005964, accuracy: 0.0889
case acc: 0.05028162
case acc: 0.054045796
case acc: 0.10445813
case acc: 0.110801674
case acc: 0.12748373
case acc: 0.0860434
top acc: 0.0890 ::: bot acc: 0.0257
top acc: 0.0970 ::: bot acc: 0.0143
top acc: 0.1878 ::: bot acc: 0.0313
top acc: 0.2015 ::: bot acc: 0.0294
top acc: 0.1898 ::: bot acc: 0.0674
top acc: 0.1509 ::: bot acc: 0.0229
LME_Co_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5130 5130 5130
1.7082474 -0.6288155 0.48738334 -0.27422604
Validation: 570 570 570
Testing: 768 768 768
pre-processing time: 0.00020360946655273438
the split date is 2012-07-01
train dropout: 0.4 test dropout: 0.1
net initializing with time: 0.0023119449615478516
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012664
average val loss: 0.005367, accuracy: 0.0876
average test loss: 0.006390, accuracy: 0.0960
case acc: 0.097745575
case acc: 0.14840698
case acc: 0.11294622
case acc: 0.080484174
case acc: 0.059293874
case acc: 0.07689019
top acc: 0.0742 ::: bot acc: 0.1626
top acc: 0.1941 ::: bot acc: 0.1064
top acc: 0.0422 ::: bot acc: 0.1735
top acc: 0.0351 ::: bot acc: 0.1724
top acc: 0.0557 ::: bot acc: 0.1006
top acc: 0.1184 ::: bot acc: 0.0609
current epoch: 2
train loss is 0.012758
average val loss: 0.011725, accuracy: 0.1252
average test loss: 0.014273, accuracy: 0.1396
case acc: 0.11131898
case acc: 0.2801822
case acc: 0.06263939
case acc: 0.103716485
case acc: 0.1138666
case acc: 0.16591592
top acc: 0.2186 ::: bot acc: 0.0276
top acc: 0.3261 ::: bot acc: 0.2380
top acc: 0.1385 ::: bot acc: 0.0312
top acc: 0.1694 ::: bot acc: 0.0417
top acc: 0.1909 ::: bot acc: 0.0388
top acc: 0.2454 ::: bot acc: 0.0715
current epoch: 3
train loss is 0.012368
average val loss: 0.022165, accuracy: 0.1892
average test loss: 0.025400, accuracy: 0.2000
case acc: 0.17394996
case acc: 0.3461043
case acc: 0.11907163
case acc: 0.14778978
case acc: 0.18512522
case acc: 0.22777563
top acc: 0.2958 ::: bot acc: 0.0594
top acc: 0.3920 ::: bot acc: 0.3043
top acc: 0.2159 ::: bot acc: 0.0455
top acc: 0.2387 ::: bot acc: 0.0345
top acc: 0.2627 ::: bot acc: 0.1075
top acc: 0.3082 ::: bot acc: 0.1320
current epoch: 4
train loss is 0.012900
average val loss: 0.009860, accuracy: 0.1140
average test loss: 0.012041, accuracy: 0.1283
case acc: 0.11055517
case acc: 0.2525277
case acc: 0.06118185
case acc: 0.09610156
case acc: 0.10909414
case acc: 0.14011481
top acc: 0.2178 ::: bot acc: 0.0269
top acc: 0.2985 ::: bot acc: 0.2105
top acc: 0.1333 ::: bot acc: 0.0368
top acc: 0.1524 ::: bot acc: 0.0540
top acc: 0.1852 ::: bot acc: 0.0343
top acc: 0.2177 ::: bot acc: 0.0493
current epoch: 5
train loss is 0.008935
average val loss: 0.006389, accuracy: 0.0906
average test loss: 0.008113, accuracy: 0.1078
case acc: 0.09723153
case acc: 0.20325851
case acc: 0.06471883
case acc: 0.08517979
case acc: 0.08658243
case acc: 0.1095525
top acc: 0.1833 ::: bot acc: 0.0534
top acc: 0.2492 ::: bot acc: 0.1615
top acc: 0.0936 ::: bot acc: 0.0764
top acc: 0.1116 ::: bot acc: 0.0954
top acc: 0.1553 ::: bot acc: 0.0279
top acc: 0.1761 ::: bot acc: 0.0422
current epoch: 6
train loss is 0.007286
average val loss: 0.008566, accuracy: 0.1062
average test loss: 0.010518, accuracy: 0.1213
case acc: 0.11059499
case acc: 0.22347347
case acc: 0.05969549
case acc: 0.092455655
case acc: 0.11496534
case acc: 0.1268733
top acc: 0.2174 ::: bot acc: 0.0278
top acc: 0.2694 ::: bot acc: 0.1813
top acc: 0.1235 ::: bot acc: 0.0468
top acc: 0.1397 ::: bot acc: 0.0671
top acc: 0.1922 ::: bot acc: 0.0391
top acc: 0.2014 ::: bot acc: 0.0421
current epoch: 7
train loss is 0.007472
average val loss: 0.009762, accuracy: 0.1153
average test loss: 0.011852, accuracy: 0.1297
case acc: 0.12146276
case acc: 0.22987105
case acc: 0.062342837
case acc: 0.096233174
case acc: 0.13280255
case acc: 0.1354945
top acc: 0.2348 ::: bot acc: 0.0250
top acc: 0.2757 ::: bot acc: 0.1879
top acc: 0.1372 ::: bot acc: 0.0339
top acc: 0.1519 ::: bot acc: 0.0545
top acc: 0.2111 ::: bot acc: 0.0548
top acc: 0.2126 ::: bot acc: 0.0460
current epoch: 8
train loss is 0.007266
average val loss: 0.008316, accuracy: 0.1054
average test loss: 0.010228, accuracy: 0.1206
case acc: 0.11598611
case acc: 0.20870969
case acc: 0.059809264
case acc: 0.09174075
case acc: 0.124444366
case acc: 0.12305988
top acc: 0.2267 ::: bot acc: 0.0254
top acc: 0.2542 ::: bot acc: 0.1672
top acc: 0.1246 ::: bot acc: 0.0462
top acc: 0.1379 ::: bot acc: 0.0683
top acc: 0.2023 ::: bot acc: 0.0470
top acc: 0.1961 ::: bot acc: 0.0415
current epoch: 9
train loss is 0.006666
average val loss: 0.008731, accuracy: 0.1087
average test loss: 0.010644, accuracy: 0.1235
case acc: 0.12167274
case acc: 0.20638612
case acc: 0.060402542
case acc: 0.09354445
case acc: 0.13346264
case acc: 0.12567523
top acc: 0.2353 ::: bot acc: 0.0244
top acc: 0.2522 ::: bot acc: 0.1642
top acc: 0.1304 ::: bot acc: 0.0392
top acc: 0.1432 ::: bot acc: 0.0631
top acc: 0.2116 ::: bot acc: 0.0559
top acc: 0.1994 ::: bot acc: 0.0429
current epoch: 10
train loss is 0.006560
average val loss: 0.008180, accuracy: 0.1050
average test loss: 0.010008, accuracy: 0.1200
case acc: 0.12115572
case acc: 0.19451955
case acc: 0.059792224
case acc: 0.09209865
case acc: 0.13134241
case acc: 0.120897785
top acc: 0.2350 ::: bot acc: 0.0242
top acc: 0.2402 ::: bot acc: 0.1527
top acc: 0.1267 ::: bot acc: 0.0436
top acc: 0.1375 ::: bot acc: 0.0685
top acc: 0.2095 ::: bot acc: 0.0534
top acc: 0.1928 ::: bot acc: 0.0419
current epoch: 11
train loss is 0.006206
average val loss: 0.008724, accuracy: 0.1094
average test loss: 0.010598, accuracy: 0.1238
case acc: 0.12809786
case acc: 0.19530576
case acc: 0.061124492
case acc: 0.094133034
case acc: 0.14031483
case acc: 0.12399985
top acc: 0.2443 ::: bot acc: 0.0258
top acc: 0.2407 ::: bot acc: 0.1537
top acc: 0.1338 ::: bot acc: 0.0355
top acc: 0.1451 ::: bot acc: 0.0617
top acc: 0.2184 ::: bot acc: 0.0625
top acc: 0.1974 ::: bot acc: 0.0422
current epoch: 12
train loss is 0.006116
average val loss: 0.009010, accuracy: 0.1118
average test loss: 0.010875, accuracy: 0.1258
case acc: 0.13339454
case acc: 0.1930424
case acc: 0.0633516
case acc: 0.09466615
case acc: 0.14519721
case acc: 0.12536483
top acc: 0.2507 ::: bot acc: 0.0280
top acc: 0.2388 ::: bot acc: 0.1516
top acc: 0.1393 ::: bot acc: 0.0318
top acc: 0.1478 ::: bot acc: 0.0577
top acc: 0.2236 ::: bot acc: 0.0672
top acc: 0.1990 ::: bot acc: 0.0427
current epoch: 13
train loss is 0.006019
average val loss: 0.008639, accuracy: 0.1093
average test loss: 0.010456, accuracy: 0.1233
case acc: 0.1337215
case acc: 0.18471637
case acc: 0.06260546
case acc: 0.094120346
case acc: 0.14296332
case acc: 0.12191357
top acc: 0.2517 ::: bot acc: 0.0279
top acc: 0.2303 ::: bot acc: 0.1429
top acc: 0.1379 ::: bot acc: 0.0330
top acc: 0.1451 ::: bot acc: 0.0608
top acc: 0.2210 ::: bot acc: 0.0650
top acc: 0.1947 ::: bot acc: 0.0416
current epoch: 14
train loss is 0.005895
average val loss: 0.008097, accuracy: 0.1054
average test loss: 0.009873, accuracy: 0.1197
case acc: 0.13215283
case acc: 0.17480695
case acc: 0.061505876
case acc: 0.09299362
case acc: 0.13925162
case acc: 0.11775292
top acc: 0.2497 ::: bot acc: 0.0267
top acc: 0.2204 ::: bot acc: 0.1333
top acc: 0.1351 ::: bot acc: 0.0356
top acc: 0.1415 ::: bot acc: 0.0651
top acc: 0.2177 ::: bot acc: 0.0614
top acc: 0.1883 ::: bot acc: 0.0417
current epoch: 15
train loss is 0.005629
average val loss: 0.008751, accuracy: 0.1106
average test loss: 0.010550, accuracy: 0.1241
case acc: 0.13990083
case acc: 0.17674145
case acc: 0.06477238
case acc: 0.09499307
case acc: 0.1469165
case acc: 0.12155838
top acc: 0.2594 ::: bot acc: 0.0309
top acc: 0.2227 ::: bot acc: 0.1352
top acc: 0.1440 ::: bot acc: 0.0280
top acc: 0.1484 ::: bot acc: 0.0569
top acc: 0.2256 ::: bot acc: 0.0690
top acc: 0.1944 ::: bot acc: 0.0415
current epoch: 16
train loss is 0.005636
average val loss: 0.008849, accuracy: 0.1115
average test loss: 0.010665, accuracy: 0.1250
case acc: 0.14379036
case acc: 0.17388636
case acc: 0.06618064
case acc: 0.095860176
case acc: 0.14868745
case acc: 0.12178469
top acc: 0.2637 ::: bot acc: 0.0337
top acc: 0.2197 ::: bot acc: 0.1322
top acc: 0.1475 ::: bot acc: 0.0257
top acc: 0.1506 ::: bot acc: 0.0555
top acc: 0.2269 ::: bot acc: 0.0708
top acc: 0.1945 ::: bot acc: 0.0417
current epoch: 17
train loss is 0.005508
average val loss: 0.009116, accuracy: 0.1137
average test loss: 0.010961, accuracy: 0.1270
case acc: 0.14838773
case acc: 0.17303804
case acc: 0.068074666
case acc: 0.097118765
case acc: 0.15188125
case acc: 0.12361641
top acc: 0.2689 ::: bot acc: 0.0371
top acc: 0.2187 ::: bot acc: 0.1313
top acc: 0.1508 ::: bot acc: 0.0227
top acc: 0.1542 ::: bot acc: 0.0514
top acc: 0.2298 ::: bot acc: 0.0739
top acc: 0.1971 ::: bot acc: 0.0420
current epoch: 18
train loss is 0.005488
average val loss: 0.008421, accuracy: 0.1085
average test loss: 0.010153, accuracy: 0.1219
case acc: 0.14445446
case acc: 0.16202876
case acc: 0.066032514
case acc: 0.095169425
case acc: 0.14526975
case acc: 0.118227646
top acc: 0.2646 ::: bot acc: 0.0342
top acc: 0.2079 ::: bot acc: 0.1202
top acc: 0.1465 ::: bot acc: 0.0264
top acc: 0.1482 ::: bot acc: 0.0572
top acc: 0.2237 ::: bot acc: 0.0671
top acc: 0.1896 ::: bot acc: 0.0410
current epoch: 19
train loss is 0.005303
average val loss: 0.008735, accuracy: 0.1111
average test loss: 0.010499, accuracy: 0.1242
case acc: 0.14902213
case acc: 0.16199419
case acc: 0.06846124
case acc: 0.0964874
case acc: 0.1490345
case acc: 0.12010644
top acc: 0.2697 ::: bot acc: 0.0373
top acc: 0.2077 ::: bot acc: 0.1203
top acc: 0.1515 ::: bot acc: 0.0234
top acc: 0.1528 ::: bot acc: 0.0531
top acc: 0.2273 ::: bot acc: 0.0710
top acc: 0.1923 ::: bot acc: 0.0410
current epoch: 20
train loss is 0.005311
average val loss: 0.009081, accuracy: 0.1138
average test loss: 0.010865, accuracy: 0.1266
case acc: 0.15406191
case acc: 0.16223249
case acc: 0.07124208
case acc: 0.09794186
case acc: 0.15196738
case acc: 0.12221037
top acc: 0.2754 ::: bot acc: 0.0411
top acc: 0.2083 ::: bot acc: 0.1203
top acc: 0.1565 ::: bot acc: 0.0215
top acc: 0.1570 ::: bot acc: 0.0483
top acc: 0.2301 ::: bot acc: 0.0738
top acc: 0.1950 ::: bot acc: 0.0419
current epoch: 21
train loss is 0.005294
average val loss: 0.008531, accuracy: 0.1098
average test loss: 0.010246, accuracy: 0.1226
case acc: 0.15093744
case acc: 0.15378395
case acc: 0.06902008
case acc: 0.096588105
case acc: 0.1467054
case acc: 0.11849135
top acc: 0.2716 ::: bot acc: 0.0384
top acc: 0.1996 ::: bot acc: 0.1119
top acc: 0.1525 ::: bot acc: 0.0225
top acc: 0.1533 ::: bot acc: 0.0522
top acc: 0.2246 ::: bot acc: 0.0691
top acc: 0.1895 ::: bot acc: 0.0413
current epoch: 22
train loss is 0.005132
average val loss: 0.008408, accuracy: 0.1090
average test loss: 0.010084, accuracy: 0.1215
case acc: 0.1510197
case acc: 0.149493
case acc: 0.069461845
case acc: 0.09631798
case acc: 0.14521125
case acc: 0.11752688
top acc: 0.2715 ::: bot acc: 0.0388
top acc: 0.1949 ::: bot acc: 0.1079
top acc: 0.1536 ::: bot acc: 0.0226
top acc: 0.1525 ::: bot acc: 0.0523
top acc: 0.2235 ::: bot acc: 0.0673
top acc: 0.1884 ::: bot acc: 0.0413
current epoch: 23
train loss is 0.005031
average val loss: 0.008492, accuracy: 0.1096
average test loss: 0.010161, accuracy: 0.1221
case acc: 0.15326342
case acc: 0.1473549
case acc: 0.07084784
case acc: 0.0969494
case acc: 0.14611417
case acc: 0.11780953
top acc: 0.2741 ::: bot acc: 0.0406
top acc: 0.1931 ::: bot acc: 0.1054
top acc: 0.1555 ::: bot acc: 0.0222
top acc: 0.1540 ::: bot acc: 0.0512
top acc: 0.2243 ::: bot acc: 0.0683
top acc: 0.1891 ::: bot acc: 0.0411
current epoch: 24
train loss is 0.005020
average val loss: 0.008735, accuracy: 0.1116
average test loss: 0.010434, accuracy: 0.1239
case acc: 0.15689424
case acc: 0.14779457
case acc: 0.07288749
case acc: 0.098287076
case acc: 0.14847931
case acc: 0.11913428
top acc: 0.2781 ::: bot acc: 0.0437
top acc: 0.1936 ::: bot acc: 0.1061
top acc: 0.1589 ::: bot acc: 0.0211
top acc: 0.1575 ::: bot acc: 0.0478
top acc: 0.2267 ::: bot acc: 0.0705
top acc: 0.1906 ::: bot acc: 0.0414
current epoch: 25
train loss is 0.005044
average val loss: 0.008764, accuracy: 0.1118
average test loss: 0.010448, accuracy: 0.1240
case acc: 0.15813348
case acc: 0.14584514
case acc: 0.073996276
case acc: 0.09851935
case acc: 0.14837734
case acc: 0.119112045
top acc: 0.2796 ::: bot acc: 0.0446
top acc: 0.1914 ::: bot acc: 0.1043
top acc: 0.1611 ::: bot acc: 0.0207
top acc: 0.1584 ::: bot acc: 0.0469
top acc: 0.2265 ::: bot acc: 0.0705
top acc: 0.1905 ::: bot acc: 0.0412
current epoch: 26
train loss is 0.005021
average val loss: 0.008553, accuracy: 0.1103
average test loss: 0.010201, accuracy: 0.1223
case acc: 0.15714037
case acc: 0.14153606
case acc: 0.0736389
case acc: 0.09790624
case acc: 0.14669251
case acc: 0.117109165
top acc: 0.2783 ::: bot acc: 0.0437
top acc: 0.1870 ::: bot acc: 0.1000
top acc: 0.1604 ::: bot acc: 0.0209
top acc: 0.1567 ::: bot acc: 0.0486
top acc: 0.2249 ::: bot acc: 0.0688
top acc: 0.1879 ::: bot acc: 0.0410
current epoch: 27
train loss is 0.004920
average val loss: 0.008599, accuracy: 0.1105
average test loss: 0.010203, accuracy: 0.1223
case acc: 0.15832238
case acc: 0.1395411
case acc: 0.07436219
case acc: 0.09817483
case acc: 0.14676058
case acc: 0.11679734
top acc: 0.2797 ::: bot acc: 0.0448
top acc: 0.1847 ::: bot acc: 0.0981
top acc: 0.1618 ::: bot acc: 0.0202
top acc: 0.1573 ::: bot acc: 0.0479
top acc: 0.2253 ::: bot acc: 0.0686
top acc: 0.1874 ::: bot acc: 0.0409
current epoch: 28
train loss is 0.004943
average val loss: 0.009057, accuracy: 0.1140
average test loss: 0.010709, accuracy: 0.1258
case acc: 0.16350523
case acc: 0.14266595
case acc: 0.078119375
case acc: 0.10026189
case acc: 0.15137483
case acc: 0.119152576
top acc: 0.2849 ::: bot acc: 0.0497
top acc: 0.1882 ::: bot acc: 0.1009
top acc: 0.1673 ::: bot acc: 0.0204
top acc: 0.1621 ::: bot acc: 0.0442
top acc: 0.2295 ::: bot acc: 0.0734
top acc: 0.1908 ::: bot acc: 0.0410
current epoch: 29
train loss is 0.004981
average val loss: 0.008723, accuracy: 0.1115
average test loss: 0.010325, accuracy: 0.1232
case acc: 0.16110408
case acc: 0.13762036
case acc: 0.0767259
case acc: 0.098774776
case acc: 0.14802144
case acc: 0.11674167
top acc: 0.2826 ::: bot acc: 0.0473
top acc: 0.1829 ::: bot acc: 0.0960
top acc: 0.1655 ::: bot acc: 0.0201
top acc: 0.1591 ::: bot acc: 0.0458
top acc: 0.2261 ::: bot acc: 0.0703
top acc: 0.1874 ::: bot acc: 0.0410
current epoch: 30
train loss is 0.004864
average val loss: 0.008825, accuracy: 0.1123
average test loss: 0.010417, accuracy: 0.1238
case acc: 0.16249366
case acc: 0.13690016
case acc: 0.07796492
case acc: 0.09953976
case acc: 0.14895612
case acc: 0.117053635
top acc: 0.2840 ::: bot acc: 0.0487
top acc: 0.1823 ::: bot acc: 0.0954
top acc: 0.1672 ::: bot acc: 0.0204
top acc: 0.1605 ::: bot acc: 0.0450
top acc: 0.2272 ::: bot acc: 0.0711
top acc: 0.1880 ::: bot acc: 0.0411
current epoch: 31
train loss is 0.004899
average val loss: 0.008400, accuracy: 0.1090
average test loss: 0.009946, accuracy: 0.1205
case acc: 0.1587849
case acc: 0.13083152
case acc: 0.07588514
case acc: 0.09836617
case acc: 0.14484742
case acc: 0.114494555
top acc: 0.2804 ::: bot acc: 0.0450
top acc: 0.1763 ::: bot acc: 0.0893
top acc: 0.1642 ::: bot acc: 0.0202
top acc: 0.1574 ::: bot acc: 0.0479
top acc: 0.2226 ::: bot acc: 0.0670
top acc: 0.1839 ::: bot acc: 0.0413
current epoch: 32
train loss is 0.004753
average val loss: 0.008145, accuracy: 0.1070
average test loss: 0.009657, accuracy: 0.1184
case acc: 0.15673336
case acc: 0.12682486
case acc: 0.07447109
case acc: 0.09761733
case acc: 0.14225413
case acc: 0.11264002
top acc: 0.2783 ::: bot acc: 0.0432
top acc: 0.1723 ::: bot acc: 0.0852
top acc: 0.1626 ::: bot acc: 0.0200
top acc: 0.1554 ::: bot acc: 0.0495
top acc: 0.2205 ::: bot acc: 0.0645
top acc: 0.1813 ::: bot acc: 0.0410
current epoch: 33
train loss is 0.004739
average val loss: 0.008935, accuracy: 0.1130
average test loss: 0.010524, accuracy: 0.1245
case acc: 0.164745
case acc: 0.13342547
case acc: 0.08039689
case acc: 0.10118018
case acc: 0.14970393
case acc: 0.11737775
top acc: 0.2870 ::: bot acc: 0.0502
top acc: 0.1790 ::: bot acc: 0.0916
top acc: 0.1715 ::: bot acc: 0.0204
top acc: 0.1634 ::: bot acc: 0.0437
top acc: 0.2283 ::: bot acc: 0.0716
top acc: 0.1882 ::: bot acc: 0.0411
current epoch: 34
train loss is 0.004818
average val loss: 0.009400, accuracy: 0.1164
average test loss: 0.011020, accuracy: 0.1279
case acc: 0.16910687
case acc: 0.13659854
case acc: 0.08462592
case acc: 0.10324623
case acc: 0.15361941
case acc: 0.12035808
top acc: 0.2915 ::: bot acc: 0.0544
top acc: 0.1820 ::: bot acc: 0.0951
top acc: 0.1770 ::: bot acc: 0.0217
top acc: 0.1683 ::: bot acc: 0.0404
top acc: 0.2322 ::: bot acc: 0.0755
top acc: 0.1926 ::: bot acc: 0.0416
current epoch: 35
train loss is 0.004924
average val loss: 0.008720, accuracy: 0.1113
average test loss: 0.010236, accuracy: 0.1225
case acc: 0.1625272
case acc: 0.12845391
case acc: 0.079927176
case acc: 0.10033691
case acc: 0.14745937
case acc: 0.116058476
top acc: 0.2844 ::: bot acc: 0.0482
top acc: 0.1738 ::: bot acc: 0.0867
top acc: 0.1707 ::: bot acc: 0.0203
top acc: 0.1622 ::: bot acc: 0.0438
top acc: 0.2257 ::: bot acc: 0.0695
top acc: 0.1865 ::: bot acc: 0.0408
current epoch: 36
train loss is 0.004753
average val loss: 0.008209, accuracy: 0.1073
average test loss: 0.009649, accuracy: 0.1182
case acc: 0.1579062
case acc: 0.12141304
case acc: 0.07624333
case acc: 0.09838852
case acc: 0.14262676
case acc: 0.11291801
top acc: 0.2793 ::: bot acc: 0.0444
top acc: 0.1669 ::: bot acc: 0.0799
top acc: 0.1654 ::: bot acc: 0.0199
top acc: 0.1576 ::: bot acc: 0.0471
top acc: 0.2208 ::: bot acc: 0.0648
top acc: 0.1821 ::: bot acc: 0.0410
current epoch: 37
train loss is 0.004642
average val loss: 0.008456, accuracy: 0.1091
average test loss: 0.009934, accuracy: 0.1202
case acc: 0.16036673
case acc: 0.12309319
case acc: 0.07842788
case acc: 0.09965405
case acc: 0.14518824
case acc: 0.11454916
top acc: 0.2824 ::: bot acc: 0.0461
top acc: 0.1682 ::: bot acc: 0.0817
top acc: 0.1686 ::: bot acc: 0.0197
top acc: 0.1607 ::: bot acc: 0.0447
top acc: 0.2237 ::: bot acc: 0.0670
top acc: 0.1846 ::: bot acc: 0.0404
current epoch: 38
train loss is 0.004688
average val loss: 0.008223, accuracy: 0.1073
average test loss: 0.009663, accuracy: 0.1182
case acc: 0.15795124
case acc: 0.11998823
case acc: 0.07718085
case acc: 0.09891633
case acc: 0.14230618
case acc: 0.11313835
top acc: 0.2799 ::: bot acc: 0.0439
top acc: 0.1656 ::: bot acc: 0.0782
top acc: 0.1672 ::: bot acc: 0.0197
top acc: 0.1586 ::: bot acc: 0.0460
top acc: 0.2210 ::: bot acc: 0.0642
top acc: 0.1822 ::: bot acc: 0.0412
current epoch: 39
train loss is 0.004668
average val loss: 0.008531, accuracy: 0.1097
average test loss: 0.009993, accuracy: 0.1207
case acc: 0.16105814
case acc: 0.122214794
case acc: 0.07992104
case acc: 0.10049049
case acc: 0.14544821
case acc: 0.11480218
top acc: 0.2832 ::: bot acc: 0.0469
top acc: 0.1676 ::: bot acc: 0.0805
top acc: 0.1707 ::: bot acc: 0.0203
top acc: 0.1622 ::: bot acc: 0.0436
top acc: 0.2239 ::: bot acc: 0.0675
top acc: 0.1847 ::: bot acc: 0.0408
current epoch: 40
train loss is 0.004706
average val loss: 0.008740, accuracy: 0.1113
average test loss: 0.010209, accuracy: 0.1222
case acc: 0.1630538
case acc: 0.12357727
case acc: 0.08189614
case acc: 0.10129034
case acc: 0.14747274
case acc: 0.115943484
top acc: 0.2849 ::: bot acc: 0.0488
top acc: 0.1692 ::: bot acc: 0.0816
top acc: 0.1737 ::: bot acc: 0.0207
top acc: 0.1640 ::: bot acc: 0.0421
top acc: 0.2259 ::: bot acc: 0.0693
top acc: 0.1865 ::: bot acc: 0.0407
current epoch: 41
train loss is 0.004713
average val loss: 0.008887, accuracy: 0.1124
average test loss: 0.010358, accuracy: 0.1233
case acc: 0.16424313
case acc: 0.12431626
case acc: 0.08339508
case acc: 0.10211172
case acc: 0.14914504
case acc: 0.11660825
top acc: 0.2862 ::: bot acc: 0.0501
top acc: 0.1698 ::: bot acc: 0.0828
top acc: 0.1754 ::: bot acc: 0.0215
top acc: 0.1657 ::: bot acc: 0.0410
top acc: 0.2276 ::: bot acc: 0.0712
top acc: 0.1873 ::: bot acc: 0.0410
current epoch: 42
train loss is 0.004717
average val loss: 0.008606, accuracy: 0.1102
average test loss: 0.010022, accuracy: 0.1209
case acc: 0.16133928
case acc: 0.12035833
case acc: 0.081412226
case acc: 0.10098591
case acc: 0.14654279
case acc: 0.11476757
top acc: 0.2831 ::: bot acc: 0.0475
top acc: 0.1658 ::: bot acc: 0.0784
top acc: 0.1728 ::: bot acc: 0.0208
top acc: 0.1632 ::: bot acc: 0.0428
top acc: 0.2249 ::: bot acc: 0.0686
top acc: 0.1848 ::: bot acc: 0.0410
current epoch: 43
train loss is 0.004641
average val loss: 0.007891, accuracy: 0.1045
average test loss: 0.009215, accuracy: 0.1150
case acc: 0.15448058
case acc: 0.11173694
case acc: 0.07620467
case acc: 0.0978549
case acc: 0.1395478
case acc: 0.110296965
top acc: 0.2757 ::: bot acc: 0.0416
top acc: 0.1568 ::: bot acc: 0.0705
top acc: 0.1653 ::: bot acc: 0.0203
top acc: 0.1561 ::: bot acc: 0.0475
top acc: 0.2178 ::: bot acc: 0.0617
top acc: 0.1780 ::: bot acc: 0.0412
current epoch: 44
train loss is 0.004540
average val loss: 0.008298, accuracy: 0.1077
average test loss: 0.009661, accuracy: 0.1183
case acc: 0.15816589
case acc: 0.11534906
case acc: 0.07935037
case acc: 0.099855915
case acc: 0.14389178
case acc: 0.11290776
top acc: 0.2796 ::: bot acc: 0.0446
top acc: 0.1604 ::: bot acc: 0.0740
top acc: 0.1698 ::: bot acc: 0.0203
top acc: 0.1607 ::: bot acc: 0.0441
top acc: 0.2222 ::: bot acc: 0.0658
top acc: 0.1821 ::: bot acc: 0.0407
current epoch: 45
train loss is 0.004585
average val loss: 0.008614, accuracy: 0.1101
average test loss: 0.010010, accuracy: 0.1208
case acc: 0.16095617
case acc: 0.11840886
case acc: 0.08198395
case acc: 0.10153572
case acc: 0.14691542
case acc: 0.11472048
top acc: 0.2826 ::: bot acc: 0.0472
top acc: 0.1639 ::: bot acc: 0.0768
top acc: 0.1737 ::: bot acc: 0.0208
top acc: 0.1641 ::: bot acc: 0.0419
top acc: 0.2254 ::: bot acc: 0.0688
top acc: 0.1852 ::: bot acc: 0.0402
current epoch: 46
train loss is 0.004610
average val loss: 0.008479, accuracy: 0.1090
average test loss: 0.009844, accuracy: 0.1196
case acc: 0.15929067
case acc: 0.11672176
case acc: 0.081383035
case acc: 0.10084305
case acc: 0.14517298
case acc: 0.11410259
top acc: 0.2809 ::: bot acc: 0.0455
top acc: 0.1619 ::: bot acc: 0.0751
top acc: 0.1730 ::: bot acc: 0.0207
top acc: 0.1628 ::: bot acc: 0.0426
top acc: 0.2236 ::: bot acc: 0.0670
top acc: 0.1840 ::: bot acc: 0.0409
current epoch: 47
train loss is 0.004605
average val loss: 0.008484, accuracy: 0.1091
average test loss: 0.009850, accuracy: 0.1196
case acc: 0.15917
case acc: 0.11641838
case acc: 0.081765994
case acc: 0.10123703
case acc: 0.14516927
case acc: 0.114018396
top acc: 0.2810 ::: bot acc: 0.0452
top acc: 0.1618 ::: bot acc: 0.0745
top acc: 0.1736 ::: bot acc: 0.0209
top acc: 0.1632 ::: bot acc: 0.0428
top acc: 0.2236 ::: bot acc: 0.0671
top acc: 0.1839 ::: bot acc: 0.0406
current epoch: 48
train loss is 0.004587
average val loss: 0.008308, accuracy: 0.1076
average test loss: 0.009640, accuracy: 0.1181
case acc: 0.15730259
case acc: 0.11403198
case acc: 0.08061513
case acc: 0.10037434
case acc: 0.14350015
case acc: 0.11294766
top acc: 0.2790 ::: bot acc: 0.0439
top acc: 0.1594 ::: bot acc: 0.0720
top acc: 0.1720 ::: bot acc: 0.0207
top acc: 0.1616 ::: bot acc: 0.0433
top acc: 0.2217 ::: bot acc: 0.0655
top acc: 0.1819 ::: bot acc: 0.0412
current epoch: 49
train loss is 0.004543
average val loss: 0.008590, accuracy: 0.1098
average test loss: 0.009951, accuracy: 0.1203
case acc: 0.1598065
case acc: 0.11642897
case acc: 0.08289896
case acc: 0.10166583
case acc: 0.1464521
case acc: 0.114454225
top acc: 0.2817 ::: bot acc: 0.0457
top acc: 0.1615 ::: bot acc: 0.0746
top acc: 0.1753 ::: bot acc: 0.0210
top acc: 0.1644 ::: bot acc: 0.0415
top acc: 0.2252 ::: bot acc: 0.0682
top acc: 0.1847 ::: bot acc: 0.0405
current epoch: 50
train loss is 0.004566
average val loss: 0.008383, accuracy: 0.1081
average test loss: 0.009695, accuracy: 0.1185
case acc: 0.15748
case acc: 0.113872066
case acc: 0.08155026
case acc: 0.100779556
case acc: 0.14419319
case acc: 0.11290406
top acc: 0.2794 ::: bot acc: 0.0438
top acc: 0.1592 ::: bot acc: 0.0721
top acc: 0.1734 ::: bot acc: 0.0206
top acc: 0.1624 ::: bot acc: 0.0428
top acc: 0.2223 ::: bot acc: 0.0662
top acc: 0.1825 ::: bot acc: 0.0406

		{"drop_out": 0.4, "drop_out_mc": 0.15, "repeat_mc": 50, "hidden": 30, "embedding_size": 5, "batch": 512, "lag": 3}
LME_Co_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5112 5112 5112
1.7082474 -0.6288155 0.48142856 -0.33910522
Validation: 570 570 570
Testing: 774 774 774
pre-processing time: 0.00019359588623046875
the split date is 2010-07-01
train dropout: 0.4 test dropout: 0.15
net initializing with time: 0.003431081771850586
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012857
average val loss: 0.008267, accuracy: 0.1100
average test loss: 0.016803, accuracy: 0.1695
case acc: 0.23283741
case acc: 0.04806058
case acc: 0.21659218
case acc: 0.18198939
case acc: 0.18322673
case acc: 0.1539985
top acc: 0.1884 ::: bot acc: 0.2917
top acc: 0.0819 ::: bot acc: 0.0234
top acc: 0.1554 ::: bot acc: 0.2716
top acc: 0.1344 ::: bot acc: 0.2229
top acc: 0.1498 ::: bot acc: 0.2162
top acc: 0.1178 ::: bot acc: 0.1870
current epoch: 2
train loss is 0.011409
average val loss: 0.010503, accuracy: 0.1077
average test loss: 0.005020, accuracy: 0.0856
case acc: 0.10301194
case acc: 0.14685085
case acc: 0.09012095
case acc: 0.064084046
case acc: 0.06475678
case acc: 0.044688977
top acc: 0.0584 ::: bot acc: 0.1630
top acc: 0.1957 ::: bot acc: 0.0912
top acc: 0.0330 ::: bot acc: 0.1440
top acc: 0.0198 ::: bot acc: 0.1043
top acc: 0.0312 ::: bot acc: 0.0963
top acc: 0.0142 ::: bot acc: 0.0753
current epoch: 3
train loss is 0.012650
average val loss: 0.022536, accuracy: 0.1788
average test loss: 0.005883, accuracy: 0.0746
case acc: 0.040746786
case acc: 0.2355773
case acc: 0.0446707
case acc: 0.04067424
case acc: 0.03543783
case acc: 0.05028265
top acc: 0.0456 ::: bot acc: 0.0581
top acc: 0.2841 ::: bot acc: 0.1797
top acc: 0.0776 ::: bot acc: 0.0410
top acc: 0.0822 ::: bot acc: 0.0122
top acc: 0.0667 ::: bot acc: 0.0112
top acc: 0.0875 ::: bot acc: 0.0194
current epoch: 4
train loss is 0.015466
average val loss: 0.014123, accuracy: 0.1302
average test loss: 0.003731, accuracy: 0.0630
case acc: 0.051127367
case acc: 0.17032738
case acc: 0.05399508
case acc: 0.04023196
case acc: 0.032133084
case acc: 0.029990263
top acc: 0.0101 ::: bot acc: 0.1094
top acc: 0.2184 ::: bot acc: 0.1146
top acc: 0.0261 ::: bot acc: 0.0942
top acc: 0.0272 ::: bot acc: 0.0651
top acc: 0.0185 ::: bot acc: 0.0543
top acc: 0.0275 ::: bot acc: 0.0469
current epoch: 5
train loss is 0.013609
average val loss: 0.006633, accuracy: 0.0905
average test loss: 0.009008, accuracy: 0.1237
case acc: 0.15708429
case acc: 0.056364
case acc: 0.14922245
case acc: 0.13322464
case acc: 0.12308956
case acc: 0.123152174
top acc: 0.1123 ::: bot acc: 0.2173
top acc: 0.0978 ::: bot acc: 0.0147
top acc: 0.0869 ::: bot acc: 0.2063
top acc: 0.0834 ::: bot acc: 0.1760
top acc: 0.0879 ::: bot acc: 0.1556
top acc: 0.0836 ::: bot acc: 0.1582
current epoch: 6
train loss is 0.008310
average val loss: 0.007182, accuracy: 0.0873
average test loss: 0.005688, accuracy: 0.0965
case acc: 0.11948297
case acc: 0.07489123
case acc: 0.11659771
case acc: 0.099678054
case acc: 0.07941746
case acc: 0.08920557
top acc: 0.0749 ::: bot acc: 0.1797
top acc: 0.1218 ::: bot acc: 0.0229
top acc: 0.0539 ::: bot acc: 0.1737
top acc: 0.0499 ::: bot acc: 0.1424
top acc: 0.0447 ::: bot acc: 0.1121
top acc: 0.0505 ::: bot acc: 0.1239
current epoch: 7
train loss is 0.008020
average val loss: 0.009099, accuracy: 0.0986
average test loss: 0.003648, accuracy: 0.0728
case acc: 0.08118386
case acc: 0.10162235
case acc: 0.08627046
case acc: 0.06772962
case acc: 0.04277953
case acc: 0.057037752
top acc: 0.0368 ::: bot acc: 0.1411
top acc: 0.1504 ::: bot acc: 0.0448
top acc: 0.0293 ::: bot acc: 0.1408
top acc: 0.0223 ::: bot acc: 0.1081
top acc: 0.0167 ::: bot acc: 0.0708
top acc: 0.0214 ::: bot acc: 0.0909
current epoch: 8
train loss is 0.008488
average val loss: 0.007624, accuracy: 0.0884
average test loss: 0.004310, accuracy: 0.0816
case acc: 0.09638406
case acc: 0.07789601
case acc: 0.10208575
case acc: 0.08473707
case acc: 0.053001087
case acc: 0.07520979
top acc: 0.0521 ::: bot acc: 0.1558
top acc: 0.1250 ::: bot acc: 0.0246
top acc: 0.0409 ::: bot acc: 0.1586
top acc: 0.0348 ::: bot acc: 0.1274
top acc: 0.0214 ::: bot acc: 0.0841
top acc: 0.0375 ::: bot acc: 0.1090
current epoch: 9
train loss is 0.007906
average val loss: 0.006964, accuracy: 0.0841
average test loss: 0.004685, accuracy: 0.0853
case acc: 0.10301201
case acc: 0.0644764
case acc: 0.11083585
case acc: 0.09296182
case acc: 0.056347173
case acc: 0.08424684
top acc: 0.0591 ::: bot acc: 0.1624
top acc: 0.1090 ::: bot acc: 0.0166
top acc: 0.0490 ::: bot acc: 0.1683
top acc: 0.0428 ::: bot acc: 0.1364
top acc: 0.0235 ::: bot acc: 0.0878
top acc: 0.0456 ::: bot acc: 0.1190
current epoch: 10
train loss is 0.007464
average val loss: 0.007135, accuracy: 0.0849
average test loss: 0.004203, accuracy: 0.0798
case acc: 0.09432722
case acc: 0.06393487
case acc: 0.10557022
case acc: 0.08744259
case acc: 0.048346426
case acc: 0.0788834
top acc: 0.0499 ::: bot acc: 0.1544
top acc: 0.1084 ::: bot acc: 0.0160
top acc: 0.0437 ::: bot acc: 0.1629
top acc: 0.0381 ::: bot acc: 0.1305
top acc: 0.0198 ::: bot acc: 0.0775
top acc: 0.0401 ::: bot acc: 0.1141
current epoch: 11
train loss is 0.007316
average val loss: 0.007344, accuracy: 0.0864
average test loss: 0.003784, accuracy: 0.0748
case acc: 0.08680018
case acc: 0.06420224
case acc: 0.10027084
case acc: 0.08180762
case acc: 0.041741785
case acc: 0.07405006
top acc: 0.0429 ::: bot acc: 0.1468
top acc: 0.1083 ::: bot acc: 0.0162
top acc: 0.0395 ::: bot acc: 0.1566
top acc: 0.0324 ::: bot acc: 0.1248
top acc: 0.0170 ::: bot acc: 0.0696
top acc: 0.0362 ::: bot acc: 0.1085
current epoch: 12
train loss is 0.007301
average val loss: 0.007405, accuracy: 0.0868
average test loss: 0.003582, accuracy: 0.0722
case acc: 0.082756475
case acc: 0.061946847
case acc: 0.098180585
case acc: 0.07955656
case acc: 0.03878665
case acc: 0.07206597
top acc: 0.0386 ::: bot acc: 0.1425
top acc: 0.1061 ::: bot acc: 0.0149
top acc: 0.0376 ::: bot acc: 0.1543
top acc: 0.0306 ::: bot acc: 0.1221
top acc: 0.0169 ::: bot acc: 0.0646
top acc: 0.0343 ::: bot acc: 0.1064
current epoch: 13
train loss is 0.007245
average val loss: 0.007320, accuracy: 0.0860
average test loss: 0.003505, accuracy: 0.0712
case acc: 0.08074194
case acc: 0.058414463
case acc: 0.09792094
case acc: 0.07932823
case acc: 0.037935376
case acc: 0.072860286
top acc: 0.0366 ::: bot acc: 0.1407
top acc: 0.1006 ::: bot acc: 0.0149
top acc: 0.0375 ::: bot acc: 0.1547
top acc: 0.0302 ::: bot acc: 0.1221
top acc: 0.0169 ::: bot acc: 0.0633
top acc: 0.0348 ::: bot acc: 0.1073
current epoch: 14
train loss is 0.007108
average val loss: 0.007159, accuracy: 0.0848
average test loss: 0.003504, accuracy: 0.0712
case acc: 0.080641694
case acc: 0.05533032
case acc: 0.09846581
case acc: 0.07989048
case acc: 0.03818313
case acc: 0.07473552
top acc: 0.0365 ::: bot acc: 0.1409
top acc: 0.0954 ::: bot acc: 0.0165
top acc: 0.0375 ::: bot acc: 0.1552
top acc: 0.0312 ::: bot acc: 0.1226
top acc: 0.0173 ::: bot acc: 0.0636
top acc: 0.0368 ::: bot acc: 0.1095
current epoch: 15
train loss is 0.006962
average val loss: 0.007102, accuracy: 0.0842
average test loss: 0.003439, accuracy: 0.0705
case acc: 0.07944166
case acc: 0.05338914
case acc: 0.097823046
case acc: 0.079755686
case acc: 0.037719566
case acc: 0.07474812
top acc: 0.0356 ::: bot acc: 0.1394
top acc: 0.0911 ::: bot acc: 0.0186
top acc: 0.0372 ::: bot acc: 0.1539
top acc: 0.0306 ::: bot acc: 0.1230
top acc: 0.0172 ::: bot acc: 0.0629
top acc: 0.0369 ::: bot acc: 0.1093
current epoch: 16
train loss is 0.006905
average val loss: 0.007082, accuracy: 0.0839
average test loss: 0.003362, accuracy: 0.0695
case acc: 0.07744583
case acc: 0.05153632
case acc: 0.09660888
case acc: 0.079330646
case acc: 0.03712462
case acc: 0.07509201
top acc: 0.0333 ::: bot acc: 0.1372
top acc: 0.0881 ::: bot acc: 0.0199
top acc: 0.0363 ::: bot acc: 0.1529
top acc: 0.0304 ::: bot acc: 0.1222
top acc: 0.0174 ::: bot acc: 0.0621
top acc: 0.0372 ::: bot acc: 0.1100
current epoch: 17
train loss is 0.006806
average val loss: 0.007099, accuracy: 0.0837
average test loss: 0.003277, accuracy: 0.0685
case acc: 0.07568055
case acc: 0.050626613
case acc: 0.0954212
case acc: 0.07802664
case acc: 0.03620541
case acc: 0.0748537
top acc: 0.0316 ::: bot acc: 0.1354
top acc: 0.0853 ::: bot acc: 0.0221
top acc: 0.0351 ::: bot acc: 0.1517
top acc: 0.0292 ::: bot acc: 0.1210
top acc: 0.0165 ::: bot acc: 0.0613
top acc: 0.0373 ::: bot acc: 0.1094
current epoch: 18
train loss is 0.006797
average val loss: 0.007293, accuracy: 0.0851
average test loss: 0.003061, accuracy: 0.0658
case acc: 0.07065368
case acc: 0.05085589
case acc: 0.09162722
case acc: 0.074557565
case acc: 0.03464738
case acc: 0.072186224
top acc: 0.0266 ::: bot acc: 0.1307
top acc: 0.0860 ::: bot acc: 0.0219
top acc: 0.0325 ::: bot acc: 0.1475
top acc: 0.0263 ::: bot acc: 0.1171
top acc: 0.0177 ::: bot acc: 0.0581
top acc: 0.0347 ::: bot acc: 0.1064
current epoch: 19
train loss is 0.006795
average val loss: 0.007448, accuracy: 0.0863
average test loss: 0.002904, accuracy: 0.0638
case acc: 0.067812294
case acc: 0.050826915
case acc: 0.08814028
case acc: 0.072192445
case acc: 0.03343053
case acc: 0.07054792
top acc: 0.0240 ::: bot acc: 0.1279
top acc: 0.0856 ::: bot acc: 0.0222
top acc: 0.0302 ::: bot acc: 0.1433
top acc: 0.0248 ::: bot acc: 0.1142
top acc: 0.0191 ::: bot acc: 0.0558
top acc: 0.0330 ::: bot acc: 0.1050
current epoch: 20
train loss is 0.006774
average val loss: 0.007505, accuracy: 0.0865
average test loss: 0.002839, accuracy: 0.0630
case acc: 0.06616948
case acc: 0.05011055
case acc: 0.08663022
case acc: 0.07097261
case acc: 0.03342959
case acc: 0.07083118
top acc: 0.0226 ::: bot acc: 0.1258
top acc: 0.0841 ::: bot acc: 0.0235
top acc: 0.0291 ::: bot acc: 0.1418
top acc: 0.0236 ::: bot acc: 0.1130
top acc: 0.0187 ::: bot acc: 0.0559
top acc: 0.0336 ::: bot acc: 0.1051
current epoch: 21
train loss is 0.006721
average val loss: 0.007410, accuracy: 0.0855
average test loss: 0.002850, accuracy: 0.0633
case acc: 0.066242166
case acc: 0.049096752
case acc: 0.08663425
case acc: 0.07174418
case acc: 0.03390888
case acc: 0.07209667
top acc: 0.0226 ::: bot acc: 0.1258
top acc: 0.0813 ::: bot acc: 0.0258
top acc: 0.0291 ::: bot acc: 0.1418
top acc: 0.0249 ::: bot acc: 0.1137
top acc: 0.0182 ::: bot acc: 0.0568
top acc: 0.0348 ::: bot acc: 0.1066
current epoch: 22
train loss is 0.006666
average val loss: 0.007254, accuracy: 0.0841
average test loss: 0.002929, accuracy: 0.0643
case acc: 0.06792892
case acc: 0.047247764
case acc: 0.08769279
case acc: 0.07338813
case acc: 0.03521047
case acc: 0.074480884
top acc: 0.0242 ::: bot acc: 0.1276
top acc: 0.0765 ::: bot acc: 0.0298
top acc: 0.0298 ::: bot acc: 0.1430
top acc: 0.0252 ::: bot acc: 0.1163
top acc: 0.0178 ::: bot acc: 0.0590
top acc: 0.0371 ::: bot acc: 0.1089
current epoch: 23
train loss is 0.006603
average val loss: 0.007435, accuracy: 0.0853
average test loss: 0.002780, accuracy: 0.0624
case acc: 0.0647881
case acc: 0.047232144
case acc: 0.08471671
case acc: 0.07141638
case acc: 0.03369061
case acc: 0.072294846
top acc: 0.0209 ::: bot acc: 0.1245
top acc: 0.0765 ::: bot acc: 0.0291
top acc: 0.0275 ::: bot acc: 0.1395
top acc: 0.0245 ::: bot acc: 0.1133
top acc: 0.0179 ::: bot acc: 0.0566
top acc: 0.0348 ::: bot acc: 0.1067
current epoch: 24
train loss is 0.006592
average val loss: 0.007290, accuracy: 0.0840
average test loss: 0.002852, accuracy: 0.0634
case acc: 0.06612632
case acc: 0.046166357
case acc: 0.085872546
case acc: 0.07238717
case acc: 0.03509869
case acc: 0.07464062
top acc: 0.0227 ::: bot acc: 0.1255
top acc: 0.0735 ::: bot acc: 0.0331
top acc: 0.0283 ::: bot acc: 0.1412
top acc: 0.0247 ::: bot acc: 0.1147
top acc: 0.0178 ::: bot acc: 0.0587
top acc: 0.0371 ::: bot acc: 0.1092
current epoch: 25
train loss is 0.006534
average val loss: 0.007404, accuracy: 0.0847
average test loss: 0.002771, accuracy: 0.0623
case acc: 0.06463353
case acc: 0.046069745
case acc: 0.08394328
case acc: 0.07081759
case acc: 0.03456662
case acc: 0.0737408
top acc: 0.0208 ::: bot acc: 0.1245
top acc: 0.0737 ::: bot acc: 0.0325
top acc: 0.0271 ::: bot acc: 0.1389
top acc: 0.0240 ::: bot acc: 0.1129
top acc: 0.0181 ::: bot acc: 0.0582
top acc: 0.0363 ::: bot acc: 0.1082
current epoch: 26
train loss is 0.006545
average val loss: 0.007554, accuracy: 0.0859
average test loss: 0.002659, accuracy: 0.0608
case acc: 0.062258165
case acc: 0.04630403
case acc: 0.08115294
case acc: 0.0691117
case acc: 0.033709846
case acc: 0.07239561
top acc: 0.0185 ::: bot acc: 0.1222
top acc: 0.0740 ::: bot acc: 0.0319
top acc: 0.0253 ::: bot acc: 0.1357
top acc: 0.0225 ::: bot acc: 0.1108
top acc: 0.0182 ::: bot acc: 0.0565
top acc: 0.0352 ::: bot acc: 0.1067
current epoch: 27
train loss is 0.006535
average val loss: 0.007628, accuracy: 0.0863
average test loss: 0.002606, accuracy: 0.0602
case acc: 0.06129822
case acc: 0.04610907
case acc: 0.079799294
case acc: 0.06815156
case acc: 0.033549443
case acc: 0.072273046
top acc: 0.0177 ::: bot acc: 0.1206
top acc: 0.0733 ::: bot acc: 0.0321
top acc: 0.0247 ::: bot acc: 0.1340
top acc: 0.0221 ::: bot acc: 0.1099
top acc: 0.0187 ::: bot acc: 0.0560
top acc: 0.0348 ::: bot acc: 0.1067
current epoch: 28
train loss is 0.006532
average val loss: 0.007640, accuracy: 0.0861
average test loss: 0.002601, accuracy: 0.0601
case acc: 0.06111919
case acc: 0.04599791
case acc: 0.07907241
case acc: 0.06795188
case acc: 0.033607706
case acc: 0.07277094
top acc: 0.0173 ::: bot acc: 0.1209
top acc: 0.0728 ::: bot acc: 0.0331
top acc: 0.0241 ::: bot acc: 0.1331
top acc: 0.0222 ::: bot acc: 0.1096
top acc: 0.0181 ::: bot acc: 0.0566
top acc: 0.0354 ::: bot acc: 0.1072
current epoch: 29
train loss is 0.006533
average val loss: 0.007880, accuracy: 0.0879
average test loss: 0.002454, accuracy: 0.0581
case acc: 0.05780638
case acc: 0.04651771
case acc: 0.076003574
case acc: 0.0655389
case acc: 0.03212274
case acc: 0.070497036
top acc: 0.0146 ::: bot acc: 0.1172
top acc: 0.0749 ::: bot acc: 0.0314
top acc: 0.0228 ::: bot acc: 0.1290
top acc: 0.0206 ::: bot acc: 0.1066
top acc: 0.0194 ::: bot acc: 0.0535
top acc: 0.0333 ::: bot acc: 0.1045
current epoch: 30
train loss is 0.006518
average val loss: 0.007790, accuracy: 0.0871
average test loss: 0.002510, accuracy: 0.0588
case acc: 0.059049696
case acc: 0.045475777
case acc: 0.076681234
case acc: 0.06644277
case acc: 0.032983508
case acc: 0.072286405
top acc: 0.0157 ::: bot acc: 0.1186
top acc: 0.0719 ::: bot acc: 0.0333
top acc: 0.0229 ::: bot acc: 0.1302
top acc: 0.0209 ::: bot acc: 0.1078
top acc: 0.0189 ::: bot acc: 0.0551
top acc: 0.0348 ::: bot acc: 0.1070
current epoch: 31
train loss is 0.006474
average val loss: 0.007701, accuracy: 0.0862
average test loss: 0.002551, accuracy: 0.0594
case acc: 0.06033225
case acc: 0.04485693
case acc: 0.07718368
case acc: 0.067387484
case acc: 0.03320399
case acc: 0.073639594
top acc: 0.0168 ::: bot acc: 0.1199
top acc: 0.0697 ::: bot acc: 0.0361
top acc: 0.0233 ::: bot acc: 0.1305
top acc: 0.0215 ::: bot acc: 0.1091
top acc: 0.0185 ::: bot acc: 0.0556
top acc: 0.0362 ::: bot acc: 0.1081
current epoch: 32
train loss is 0.006453
average val loss: 0.007699, accuracy: 0.0861
average test loss: 0.002552, accuracy: 0.0595
case acc: 0.06079723
case acc: 0.044650745
case acc: 0.076903775
case acc: 0.06749981
case acc: 0.03309448
case acc: 0.07377375
top acc: 0.0173 ::: bot acc: 0.1204
top acc: 0.0686 ::: bot acc: 0.0376
top acc: 0.0229 ::: bot acc: 0.1304
top acc: 0.0215 ::: bot acc: 0.1092
top acc: 0.0187 ::: bot acc: 0.0552
top acc: 0.0362 ::: bot acc: 0.1084
current epoch: 33
train loss is 0.006435
average val loss: 0.007652, accuracy: 0.0855
average test loss: 0.002575, accuracy: 0.0597
case acc: 0.061296806
case acc: 0.043957822
case acc: 0.077197365
case acc: 0.06794813
case acc: 0.033401996
case acc: 0.07468487
top acc: 0.0175 ::: bot acc: 0.1212
top acc: 0.0666 ::: bot acc: 0.0393
top acc: 0.0233 ::: bot acc: 0.1307
top acc: 0.0218 ::: bot acc: 0.1098
top acc: 0.0184 ::: bot acc: 0.0560
top acc: 0.0372 ::: bot acc: 0.1094
current epoch: 34
train loss is 0.006405
average val loss: 0.007562, accuracy: 0.0846
average test loss: 0.002623, accuracy: 0.0605
case acc: 0.06260674
case acc: 0.043353885
case acc: 0.07768565
case acc: 0.06904191
case acc: 0.034216207
case acc: 0.07586053
top acc: 0.0191 ::: bot acc: 0.1224
top acc: 0.0647 ::: bot acc: 0.0413
top acc: 0.0237 ::: bot acc: 0.1313
top acc: 0.0224 ::: bot acc: 0.1111
top acc: 0.0184 ::: bot acc: 0.0570
top acc: 0.0380 ::: bot acc: 0.1106
current epoch: 35
train loss is 0.006382
average val loss: 0.007550, accuracy: 0.0845
average test loss: 0.002637, accuracy: 0.0607
case acc: 0.06338122
case acc: 0.04310106
case acc: 0.07771467
case acc: 0.06905503
case acc: 0.03450318
case acc: 0.07637546
top acc: 0.0200 ::: bot acc: 0.1232
top acc: 0.0630 ::: bot acc: 0.0428
top acc: 0.0235 ::: bot acc: 0.1312
top acc: 0.0224 ::: bot acc: 0.1112
top acc: 0.0183 ::: bot acc: 0.0577
top acc: 0.0388 ::: bot acc: 0.1108
current epoch: 36
train loss is 0.006366
average val loss: 0.007593, accuracy: 0.0848
average test loss: 0.002611, accuracy: 0.0604
case acc: 0.06302967
case acc: 0.043102335
case acc: 0.07702745
case acc: 0.0684619
case acc: 0.034488983
case acc: 0.075992785
top acc: 0.0194 ::: bot acc: 0.1229
top acc: 0.0630 ::: bot acc: 0.0428
top acc: 0.0234 ::: bot acc: 0.1304
top acc: 0.0221 ::: bot acc: 0.1104
top acc: 0.0187 ::: bot acc: 0.0575
top acc: 0.0380 ::: bot acc: 0.1108
current epoch: 37
train loss is 0.006351
average val loss: 0.007601, accuracy: 0.0847
average test loss: 0.002612, accuracy: 0.0603
case acc: 0.06331009
case acc: 0.04293383
case acc: 0.07672869
case acc: 0.06861392
case acc: 0.034143187
case acc: 0.07621694
top acc: 0.0197 ::: bot acc: 0.1232
top acc: 0.0626 ::: bot acc: 0.0433
top acc: 0.0232 ::: bot acc: 0.1300
top acc: 0.0222 ::: bot acc: 0.1107
top acc: 0.0185 ::: bot acc: 0.0572
top acc: 0.0387 ::: bot acc: 0.1109
current epoch: 38
train loss is 0.006341
average val loss: 0.007592, accuracy: 0.0846
average test loss: 0.002616, accuracy: 0.0604
case acc: 0.063849054
case acc: 0.04277805
case acc: 0.07648764
case acc: 0.06855321
case acc: 0.034244195
case acc: 0.076428704
top acc: 0.0200 ::: bot acc: 0.1236
top acc: 0.0617 ::: bot acc: 0.0443
top acc: 0.0228 ::: bot acc: 0.1301
top acc: 0.0222 ::: bot acc: 0.1106
top acc: 0.0184 ::: bot acc: 0.0574
top acc: 0.0388 ::: bot acc: 0.1113
current epoch: 39
train loss is 0.006334
average val loss: 0.007604, accuracy: 0.0846
average test loss: 0.002615, accuracy: 0.0604
case acc: 0.06411901
case acc: 0.04241919
case acc: 0.07628464
case acc: 0.068471745
case acc: 0.03431888
case acc: 0.07655936
top acc: 0.0205 ::: bot acc: 0.1239
top acc: 0.0608 ::: bot acc: 0.0446
top acc: 0.0226 ::: bot acc: 0.1297
top acc: 0.0221 ::: bot acc: 0.1103
top acc: 0.0183 ::: bot acc: 0.0574
top acc: 0.0386 ::: bot acc: 0.1113
current epoch: 40
train loss is 0.006306
average val loss: 0.007587, accuracy: 0.0843
average test loss: 0.002635, accuracy: 0.0607
case acc: 0.06480195
case acc: 0.042376805
case acc: 0.076231375
case acc: 0.068878
case acc: 0.03463121
case acc: 0.077035636
top acc: 0.0211 ::: bot acc: 0.1248
top acc: 0.0600 ::: bot acc: 0.0458
top acc: 0.0227 ::: bot acc: 0.1296
top acc: 0.0224 ::: bot acc: 0.1110
top acc: 0.0182 ::: bot acc: 0.0580
top acc: 0.0391 ::: bot acc: 0.1120
current epoch: 41
train loss is 0.006302
average val loss: 0.007552, accuracy: 0.0840
average test loss: 0.002661, accuracy: 0.0611
case acc: 0.06579707
case acc: 0.0422324
case acc: 0.07655001
case acc: 0.069312684
case acc: 0.034869023
case acc: 0.07757706
top acc: 0.0220 ::: bot acc: 0.1256
top acc: 0.0590 ::: bot acc: 0.0470
top acc: 0.0229 ::: bot acc: 0.1302
top acc: 0.0227 ::: bot acc: 0.1112
top acc: 0.0182 ::: bot acc: 0.0583
top acc: 0.0400 ::: bot acc: 0.1121
current epoch: 42
train loss is 0.006271
average val loss: 0.007529, accuracy: 0.0837
average test loss: 0.002678, accuracy: 0.0613
case acc: 0.06654989
case acc: 0.041973595
case acc: 0.07647463
case acc: 0.06967755
case acc: 0.03501588
case acc: 0.07784845
top acc: 0.0226 ::: bot acc: 0.1264
top acc: 0.0581 ::: bot acc: 0.0479
top acc: 0.0228 ::: bot acc: 0.1300
top acc: 0.0230 ::: bot acc: 0.1117
top acc: 0.0178 ::: bot acc: 0.0588
top acc: 0.0399 ::: bot acc: 0.1127
current epoch: 43
train loss is 0.006269
average val loss: 0.007477, accuracy: 0.0832
average test loss: 0.002720, accuracy: 0.0618
case acc: 0.06783709
case acc: 0.04183613
case acc: 0.077249594
case acc: 0.07024759
case acc: 0.035143882
case acc: 0.07863172
top acc: 0.0242 ::: bot acc: 0.1274
top acc: 0.0568 ::: bot acc: 0.0493
top acc: 0.0232 ::: bot acc: 0.1310
top acc: 0.0230 ::: bot acc: 0.1128
top acc: 0.0178 ::: bot acc: 0.0590
top acc: 0.0408 ::: bot acc: 0.1133
current epoch: 44
train loss is 0.006260
average val loss: 0.007416, accuracy: 0.0826
average test loss: 0.002773, accuracy: 0.0625
case acc: 0.06919256
case acc: 0.041526668
case acc: 0.07800412
case acc: 0.07107995
case acc: 0.035658192
case acc: 0.07958681
top acc: 0.0253 ::: bot acc: 0.1290
top acc: 0.0553 ::: bot acc: 0.0506
top acc: 0.0239 ::: bot acc: 0.1317
top acc: 0.0236 ::: bot acc: 0.1135
top acc: 0.0178 ::: bot acc: 0.0597
top acc: 0.0415 ::: bot acc: 0.1145
current epoch: 45
train loss is 0.006240
average val loss: 0.007312, accuracy: 0.0818
average test loss: 0.002857, accuracy: 0.0636
case acc: 0.07128645
case acc: 0.0413
case acc: 0.0790194
case acc: 0.07247271
case acc: 0.036226805
case acc: 0.081146844
top acc: 0.0273 ::: bot acc: 0.1313
top acc: 0.0533 ::: bot acc: 0.0525
top acc: 0.0243 ::: bot acc: 0.1330
top acc: 0.0247 ::: bot acc: 0.1150
top acc: 0.0174 ::: bot acc: 0.0610
top acc: 0.0430 ::: bot acc: 0.1160
current epoch: 46
train loss is 0.006224
average val loss: 0.007324, accuracy: 0.0818
average test loss: 0.002873, accuracy: 0.0638
case acc: 0.07199422
case acc: 0.04108219
case acc: 0.07897649
case acc: 0.07270951
case acc: 0.036401436
case acc: 0.08159431
top acc: 0.0281 ::: bot acc: 0.1318
top acc: 0.0527 ::: bot acc: 0.0531
top acc: 0.0244 ::: bot acc: 0.1329
top acc: 0.0248 ::: bot acc: 0.1154
top acc: 0.0177 ::: bot acc: 0.0611
top acc: 0.0433 ::: bot acc: 0.1166
current epoch: 47
train loss is 0.006211
average val loss: 0.007284, accuracy: 0.0814
average test loss: 0.002919, accuracy: 0.0644
case acc: 0.0728296
case acc: 0.04103225
case acc: 0.07955965
case acc: 0.07339676
case acc: 0.037203144
case acc: 0.0822541
top acc: 0.0287 ::: bot acc: 0.1327
top acc: 0.0516 ::: bot acc: 0.0543
top acc: 0.0247 ::: bot acc: 0.1336
top acc: 0.0252 ::: bot acc: 0.1161
top acc: 0.0177 ::: bot acc: 0.0621
top acc: 0.0439 ::: bot acc: 0.1174
current epoch: 48
train loss is 0.006208
average val loss: 0.007283, accuracy: 0.0813
average test loss: 0.002920, accuracy: 0.0644
case acc: 0.0731581
case acc: 0.04105552
case acc: 0.07903176
case acc: 0.07358354
case acc: 0.03721121
case acc: 0.08244652
top acc: 0.0292 ::: bot acc: 0.1331
top acc: 0.0516 ::: bot acc: 0.0543
top acc: 0.0242 ::: bot acc: 0.1330
top acc: 0.0253 ::: bot acc: 0.1164
top acc: 0.0172 ::: bot acc: 0.0624
top acc: 0.0441 ::: bot acc: 0.1173
current epoch: 49
train loss is 0.006198
average val loss: 0.007428, accuracy: 0.0824
average test loss: 0.002829, accuracy: 0.0633
case acc: 0.0718052
case acc: 0.041147146
case acc: 0.07730864
case acc: 0.071985036
case acc: 0.03623274
case acc: 0.08108031
top acc: 0.0278 ::: bot acc: 0.1317
top acc: 0.0531 ::: bot acc: 0.0524
top acc: 0.0234 ::: bot acc: 0.1308
top acc: 0.0242 ::: bot acc: 0.1147
top acc: 0.0175 ::: bot acc: 0.0608
top acc: 0.0429 ::: bot acc: 0.1158
current epoch: 50
train loss is 0.006196
average val loss: 0.007412, accuracy: 0.0822
average test loss: 0.002848, accuracy: 0.0635
case acc: 0.07261623
case acc: 0.041110754
case acc: 0.07709096
case acc: 0.07231343
case acc: 0.036612257
case acc: 0.081492156
top acc: 0.0287 ::: bot acc: 0.1325
top acc: 0.0528 ::: bot acc: 0.0530
top acc: 0.0232 ::: bot acc: 0.1307
top acc: 0.0244 ::: bot acc: 0.1151
top acc: 0.0177 ::: bot acc: 0.0614
top acc: 0.0434 ::: bot acc: 0.1164
LME_Co_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5136 5136 5136
1.7082474 -0.6288155 0.48738334 -0.33910522
Validation: 576 576 576
Testing: 744 744 744
pre-processing time: 0.0002865791320800781
the split date is 2011-01-01
train dropout: 0.4 test dropout: 0.15
net initializing with time: 0.0024797916412353516
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012616
average val loss: 0.015995, accuracy: 0.1638
average test loss: 0.020451, accuracy: 0.1721
case acc: 0.20937878
case acc: 0.04370744
case acc: 0.2605043
case acc: 0.20514905
case acc: 0.19629475
case acc: 0.117558144
top acc: 0.0953 ::: bot acc: 0.3189
top acc: 0.0591 ::: bot acc: 0.0575
top acc: 0.1814 ::: bot acc: 0.3442
top acc: 0.1089 ::: bot acc: 0.2814
top acc: 0.0691 ::: bot acc: 0.3277
top acc: 0.0283 ::: bot acc: 0.2167
current epoch: 2
train loss is 0.012346
average val loss: 0.007001, accuracy: 0.1039
average test loss: 0.009083, accuracy: 0.1147
case acc: 0.117136165
case acc: 0.09889
case acc: 0.15399773
case acc: 0.10935451
case acc: 0.11637757
case acc: 0.09217057
top acc: 0.0358 ::: bot acc: 0.2100
top acc: 0.1528 ::: bot acc: 0.0398
top acc: 0.0748 ::: bot acc: 0.2379
top acc: 0.0215 ::: bot acc: 0.1812
top acc: 0.0306 ::: bot acc: 0.2261
top acc: 0.1117 ::: bot acc: 0.1263
current epoch: 3
train loss is 0.011812
average val loss: 0.008656, accuracy: 0.0875
average test loss: 0.008013, accuracy: 0.1042
case acc: 0.082091704
case acc: 0.20258692
case acc: 0.06589425
case acc: 0.067408554
case acc: 0.0997869
case acc: 0.10769754
top acc: 0.1329 ::: bot acc: 0.0918
top acc: 0.2581 ::: bot acc: 0.1429
top acc: 0.0423 ::: bot acc: 0.1222
top acc: 0.0972 ::: bot acc: 0.0751
top acc: 0.1404 ::: bot acc: 0.1172
top acc: 0.2105 ::: bot acc: 0.0291
current epoch: 4
train loss is 0.012907
average val loss: 0.008445, accuracy: 0.0882
average test loss: 0.007744, accuracy: 0.1029
case acc: 0.083081685
case acc: 0.19573905
case acc: 0.06419484
case acc: 0.06805928
case acc: 0.100637145
case acc: 0.10563262
top acc: 0.1403 ::: bot acc: 0.0841
top acc: 0.2514 ::: bot acc: 0.1367
top acc: 0.0463 ::: bot acc: 0.1170
top acc: 0.0987 ::: bot acc: 0.0755
top acc: 0.1474 ::: bot acc: 0.1096
top acc: 0.2063 ::: bot acc: 0.0332
current epoch: 5
train loss is 0.012132
average val loss: 0.005485, accuracy: 0.0863
average test loss: 0.007002, accuracy: 0.1018
case acc: 0.096013136
case acc: 0.09772539
case acc: 0.120770276
case acc: 0.098492414
case acc: 0.10568679
case acc: 0.09203072
top acc: 0.0584 ::: bot acc: 0.1671
top acc: 0.1525 ::: bot acc: 0.0388
top acc: 0.0415 ::: bot acc: 0.2052
top acc: 0.0220 ::: bot acc: 0.1653
top acc: 0.0696 ::: bot acc: 0.1876
top acc: 0.1151 ::: bot acc: 0.1224
current epoch: 6
train loss is 0.008382
average val loss: 0.005655, accuracy: 0.0923
average test loss: 0.007611, accuracy: 0.1049
case acc: 0.10163824
case acc: 0.07334902
case acc: 0.14035462
case acc: 0.11139644
case acc: 0.10781161
case acc: 0.094584405
top acc: 0.0475 ::: bot acc: 0.1818
top acc: 0.1225 ::: bot acc: 0.0255
top acc: 0.0613 ::: bot acc: 0.2244
top acc: 0.0221 ::: bot acc: 0.1838
top acc: 0.0618 ::: bot acc: 0.1956
top acc: 0.0956 ::: bot acc: 0.1418
current epoch: 7
train loss is 0.007115
average val loss: 0.004987, accuracy: 0.0827
average test loss: 0.006665, accuracy: 0.0992
case acc: 0.093573816
case acc: 0.080362156
case acc: 0.12584569
case acc: 0.10050586
case acc: 0.10257965
case acc: 0.092323616
top acc: 0.0658 ::: bot acc: 0.1600
top acc: 0.1314 ::: bot acc: 0.0287
top acc: 0.0468 ::: bot acc: 0.2101
top acc: 0.0214 ::: bot acc: 0.1680
top acc: 0.0869 ::: bot acc: 0.1712
top acc: 0.1112 ::: bot acc: 0.1264
current epoch: 8
train loss is 0.007063
average val loss: 0.004694, accuracy: 0.0756
average test loss: 0.005940, accuracy: 0.0941
case acc: 0.08714145
case acc: 0.08892933
case acc: 0.10757617
case acc: 0.09112193
case acc: 0.09943286
case acc: 0.09047319
top acc: 0.0875 ::: bot acc: 0.1374
top acc: 0.1415 ::: bot acc: 0.0334
top acc: 0.0292 ::: bot acc: 0.1914
top acc: 0.0281 ::: bot acc: 0.1508
top acc: 0.1099 ::: bot acc: 0.1475
top acc: 0.1269 ::: bot acc: 0.1101
current epoch: 9
train loss is 0.007028
average val loss: 0.004561, accuracy: 0.0750
average test loss: 0.005808, accuracy: 0.0929
case acc: 0.08626647
case acc: 0.0831038
case acc: 0.107209794
case acc: 0.09102467
case acc: 0.099195555
case acc: 0.09088487
top acc: 0.0909 ::: bot acc: 0.1335
top acc: 0.1351 ::: bot acc: 0.0293
top acc: 0.0297 ::: bot acc: 0.1905
top acc: 0.0284 ::: bot acc: 0.1507
top acc: 0.1149 ::: bot acc: 0.1426
top acc: 0.1252 ::: bot acc: 0.1129
current epoch: 10
train loss is 0.006815
average val loss: 0.004418, accuracy: 0.0764
average test loss: 0.006000, accuracy: 0.0939
case acc: 0.08746674
case acc: 0.0689368
case acc: 0.11795744
case acc: 0.09710943
case acc: 0.09964076
case acc: 0.09228749
top acc: 0.0826 ::: bot acc: 0.1410
top acc: 0.1158 ::: bot acc: 0.0249
top acc: 0.0386 ::: bot acc: 0.2021
top acc: 0.0234 ::: bot acc: 0.1624
top acc: 0.1071 ::: bot acc: 0.1504
top acc: 0.1127 ::: bot acc: 0.1250
current epoch: 11
train loss is 0.006407
average val loss: 0.004294, accuracy: 0.0754
average test loss: 0.005916, accuracy: 0.0930
case acc: 0.08739551
case acc: 0.064567074
case acc: 0.11760787
case acc: 0.09691476
case acc: 0.09909237
case acc: 0.09241767
top acc: 0.0865 ::: bot acc: 0.1387
top acc: 0.1096 ::: bot acc: 0.0241
top acc: 0.0383 ::: bot acc: 0.2020
top acc: 0.0232 ::: bot acc: 0.1624
top acc: 0.1105 ::: bot acc: 0.1468
top acc: 0.1120 ::: bot acc: 0.1259
current epoch: 12
train loss is 0.006226
average val loss: 0.004137, accuracy: 0.0731
average test loss: 0.005717, accuracy: 0.0915
case acc: 0.086151026
case acc: 0.06390546
case acc: 0.11328545
case acc: 0.09477444
case acc: 0.09922273
case acc: 0.091542244
top acc: 0.0939 ::: bot acc: 0.1313
top acc: 0.1089 ::: bot acc: 0.0244
top acc: 0.0343 ::: bot acc: 0.1976
top acc: 0.0250 ::: bot acc: 0.1585
top acc: 0.1172 ::: bot acc: 0.1407
top acc: 0.1143 ::: bot acc: 0.1225
current epoch: 13
train loss is 0.006171
average val loss: 0.004024, accuracy: 0.0719
average test loss: 0.005619, accuracy: 0.0905
case acc: 0.08532401
case acc: 0.060305875
case acc: 0.11222705
case acc: 0.09431763
case acc: 0.09895391
case acc: 0.091798715
top acc: 0.0970 ::: bot acc: 0.1281
top acc: 0.1031 ::: bot acc: 0.0240
top acc: 0.0335 ::: bot acc: 0.1966
top acc: 0.0254 ::: bot acc: 0.1570
top acc: 0.1186 ::: bot acc: 0.1384
top acc: 0.1138 ::: bot acc: 0.1231
current epoch: 14
train loss is 0.006030
average val loss: 0.003922, accuracy: 0.0710
average test loss: 0.005558, accuracy: 0.0898
case acc: 0.084843576
case acc: 0.057641417
case acc: 0.111234985
case acc: 0.09379437
case acc: 0.09917721
case acc: 0.09227446
top acc: 0.0998 ::: bot acc: 0.1252
top acc: 0.0985 ::: bot acc: 0.0252
top acc: 0.0323 ::: bot acc: 0.1955
top acc: 0.0255 ::: bot acc: 0.1564
top acc: 0.1196 ::: bot acc: 0.1383
top acc: 0.1125 ::: bot acc: 0.1251
current epoch: 15
train loss is 0.005944
average val loss: 0.003806, accuracy: 0.0694
average test loss: 0.005397, accuracy: 0.0886
case acc: 0.0845764
case acc: 0.057151668
case acc: 0.10660723
case acc: 0.09227498
case acc: 0.09909492
case acc: 0.091779225
top acc: 0.1057 ::: bot acc: 0.1203
top acc: 0.0984 ::: bot acc: 0.0254
top acc: 0.0293 ::: bot acc: 0.1899
top acc: 0.0278 ::: bot acc: 0.1529
top acc: 0.1224 ::: bot acc: 0.1349
top acc: 0.1146 ::: bot acc: 0.1227
current epoch: 16
train loss is 0.005934
average val loss: 0.003712, accuracy: 0.0678
average test loss: 0.005209, accuracy: 0.0870
case acc: 0.08328127
case acc: 0.057750877
case acc: 0.10085003
case acc: 0.089478776
case acc: 0.09909205
case acc: 0.091544725
top acc: 0.1127 ::: bot acc: 0.1122
top acc: 0.0995 ::: bot acc: 0.0248
top acc: 0.0250 ::: bot acc: 0.1835
top acc: 0.0306 ::: bot acc: 0.1472
top acc: 0.1275 ::: bot acc: 0.1300
top acc: 0.1184 ::: bot acc: 0.1189
current epoch: 17
train loss is 0.005937
average val loss: 0.003653, accuracy: 0.0670
average test loss: 0.005126, accuracy: 0.0861
case acc: 0.08268313
case acc: 0.05709784
case acc: 0.09807635
case acc: 0.088401854
case acc: 0.09910951
case acc: 0.09129911
top acc: 0.1165 ::: bot acc: 0.1089
top acc: 0.0981 ::: bot acc: 0.0255
top acc: 0.0227 ::: bot acc: 0.1805
top acc: 0.0317 ::: bot acc: 0.1454
top acc: 0.1284 ::: bot acc: 0.1287
top acc: 0.1189 ::: bot acc: 0.1182
current epoch: 18
train loss is 0.005860
average val loss: 0.003559, accuracy: 0.0664
average test loss: 0.005125, accuracy: 0.0860
case acc: 0.08271226
case acc: 0.05332168
case acc: 0.09960739
case acc: 0.08926201
case acc: 0.09915172
case acc: 0.09164783
top acc: 0.1155 ::: bot acc: 0.1091
top acc: 0.0911 ::: bot acc: 0.0275
top acc: 0.0240 ::: bot acc: 0.1823
top acc: 0.0307 ::: bot acc: 0.1468
top acc: 0.1260 ::: bot acc: 0.1314
top acc: 0.1154 ::: bot acc: 0.1217
current epoch: 19
train loss is 0.005769
average val loss: 0.003482, accuracy: 0.0655
average test loss: 0.005039, accuracy: 0.0851
case acc: 0.082454376
case acc: 0.052711386
case acc: 0.09694932
case acc: 0.08809106
case acc: 0.09901476
case acc: 0.09157566
top acc: 0.1190 ::: bot acc: 0.1062
top acc: 0.0902 ::: bot acc: 0.0280
top acc: 0.0224 ::: bot acc: 0.1789
top acc: 0.0323 ::: bot acc: 0.1445
top acc: 0.1277 ::: bot acc: 0.1293
top acc: 0.1169 ::: bot acc: 0.1206
current epoch: 20
train loss is 0.005733
average val loss: 0.003415, accuracy: 0.0650
average test loss: 0.004996, accuracy: 0.0846
case acc: 0.082219176
case acc: 0.05170635
case acc: 0.09557446
case acc: 0.08731216
case acc: 0.099219106
case acc: 0.09173173
top acc: 0.1206 ::: bot acc: 0.1043
top acc: 0.0879 ::: bot acc: 0.0293
top acc: 0.0213 ::: bot acc: 0.1774
top acc: 0.0330 ::: bot acc: 0.1426
top acc: 0.1282 ::: bot acc: 0.1295
top acc: 0.1169 ::: bot acc: 0.1209
current epoch: 21
train loss is 0.005687
average val loss: 0.003349, accuracy: 0.0643
average test loss: 0.004978, accuracy: 0.0844
case acc: 0.08240562
case acc: 0.049950957
case acc: 0.09537
case acc: 0.08746985
case acc: 0.09933153
case acc: 0.091938786
top acc: 0.1217 ::: bot acc: 0.1039
top acc: 0.0842 ::: bot acc: 0.0308
top acc: 0.0216 ::: bot acc: 0.1770
top acc: 0.0335 ::: bot acc: 0.1427
top acc: 0.1270 ::: bot acc: 0.1308
top acc: 0.1152 ::: bot acc: 0.1221
current epoch: 22
train loss is 0.005633
average val loss: 0.003304, accuracy: 0.0638
average test loss: 0.004932, accuracy: 0.0839
case acc: 0.08218594
case acc: 0.049084153
case acc: 0.09432083
case acc: 0.08695043
case acc: 0.09896842
case acc: 0.092083365
top acc: 0.1234 ::: bot acc: 0.1020
top acc: 0.0822 ::: bot acc: 0.0327
top acc: 0.0213 ::: bot acc: 0.1750
top acc: 0.0336 ::: bot acc: 0.1421
top acc: 0.1273 ::: bot acc: 0.1300
top acc: 0.1154 ::: bot acc: 0.1222
current epoch: 23
train loss is 0.005596
average val loss: 0.003229, accuracy: 0.0633
average test loss: 0.004963, accuracy: 0.0840
case acc: 0.08219639
case acc: 0.046920158
case acc: 0.09575687
case acc: 0.08780686
case acc: 0.09915849
case acc: 0.092170194
top acc: 0.1212 ::: bot acc: 0.1039
top acc: 0.0767 ::: bot acc: 0.0370
top acc: 0.0217 ::: bot acc: 0.1775
top acc: 0.0321 ::: bot acc: 0.1441
top acc: 0.1245 ::: bot acc: 0.1334
top acc: 0.1126 ::: bot acc: 0.1247
current epoch: 24
train loss is 0.005541
average val loss: 0.003195, accuracy: 0.0629
average test loss: 0.004927, accuracy: 0.0836
case acc: 0.0820728
case acc: 0.046271298
case acc: 0.095013216
case acc: 0.08731788
case acc: 0.09904811
case acc: 0.09196562
top acc: 0.1230 ::: bot acc: 0.1022
top acc: 0.0758 ::: bot acc: 0.0380
top acc: 0.0214 ::: bot acc: 0.1764
top acc: 0.0331 ::: bot acc: 0.1430
top acc: 0.1248 ::: bot acc: 0.1325
top acc: 0.1119 ::: bot acc: 0.1249
current epoch: 25
train loss is 0.005554
average val loss: 0.003155, accuracy: 0.0622
average test loss: 0.004837, accuracy: 0.0829
case acc: 0.08206276
case acc: 0.046691082
case acc: 0.09156368
case acc: 0.08625756
case acc: 0.09907853
case acc: 0.09182928
top acc: 0.1270 ::: bot acc: 0.0983
top acc: 0.0768 ::: bot acc: 0.0366
top acc: 0.0199 ::: bot acc: 0.1721
top acc: 0.0365 ::: bot acc: 0.1395
top acc: 0.1274 ::: bot acc: 0.1298
top acc: 0.1146 ::: bot acc: 0.1227
current epoch: 26
train loss is 0.005572
average val loss: 0.003135, accuracy: 0.0619
average test loss: 0.004789, accuracy: 0.0825
case acc: 0.08196235
case acc: 0.047042802
case acc: 0.08980042
case acc: 0.085429154
case acc: 0.09891003
case acc: 0.091839746
top acc: 0.1287 ::: bot acc: 0.0965
top acc: 0.0766 ::: bot acc: 0.0375
top acc: 0.0191 ::: bot acc: 0.1697
top acc: 0.0374 ::: bot acc: 0.1379
top acc: 0.1276 ::: bot acc: 0.1297
top acc: 0.1149 ::: bot acc: 0.1223
current epoch: 27
train loss is 0.005546
average val loss: 0.003126, accuracy: 0.0617
average test loss: 0.004714, accuracy: 0.0819
case acc: 0.082080536
case acc: 0.047193423
case acc: 0.08694898
case acc: 0.084149286
case acc: 0.099250816
case acc: 0.09174921
top acc: 0.1318 ::: bot acc: 0.0932
top acc: 0.0779 ::: bot acc: 0.0361
top acc: 0.0187 ::: bot acc: 0.1656
top acc: 0.0400 ::: bot acc: 0.1348
top acc: 0.1303 ::: bot acc: 0.1272
top acc: 0.1163 ::: bot acc: 0.1210
current epoch: 28
train loss is 0.005560
average val loss: 0.003077, accuracy: 0.0613
average test loss: 0.004706, accuracy: 0.0818
case acc: 0.08212653
case acc: 0.04696662
case acc: 0.08644125
case acc: 0.08415416
case acc: 0.0993155
case acc: 0.091753714
top acc: 0.1323 ::: bot acc: 0.0926
top acc: 0.0766 ::: bot acc: 0.0374
top acc: 0.0181 ::: bot acc: 0.1653
top acc: 0.0409 ::: bot acc: 0.1343
top acc: 0.1303 ::: bot acc: 0.1274
top acc: 0.1162 ::: bot acc: 0.1213
current epoch: 29
train loss is 0.005546
average val loss: 0.003047, accuracy: 0.0610
average test loss: 0.004714, accuracy: 0.0818
case acc: 0.08190287
case acc: 0.045576062
case acc: 0.087736055
case acc: 0.08460497
case acc: 0.09898861
case acc: 0.09183359
top acc: 0.1301 ::: bot acc: 0.0945
top acc: 0.0727 ::: bot acc: 0.0414
top acc: 0.0187 ::: bot acc: 0.1668
top acc: 0.0386 ::: bot acc: 0.1359
top acc: 0.1281 ::: bot acc: 0.1290
top acc: 0.1134 ::: bot acc: 0.1235
current epoch: 30
train loss is 0.005442
average val loss: 0.003007, accuracy: 0.0610
average test loss: 0.004827, accuracy: 0.0826
case acc: 0.08231828
case acc: 0.043519355
case acc: 0.09139769
case acc: 0.086459406
case acc: 0.09915572
case acc: 0.09256339
top acc: 0.1261 ::: bot acc: 0.0998
top acc: 0.0657 ::: bot acc: 0.0473
top acc: 0.0197 ::: bot acc: 0.1717
top acc: 0.0352 ::: bot acc: 0.1405
top acc: 0.1239 ::: bot acc: 0.1338
top acc: 0.1088 ::: bot acc: 0.1284
current epoch: 31
train loss is 0.005375
average val loss: 0.002997, accuracy: 0.0611
average test loss: 0.004915, accuracy: 0.0832
case acc: 0.08226023
case acc: 0.04283228
case acc: 0.09412358
case acc: 0.08803941
case acc: 0.09914487
case acc: 0.09293384
top acc: 0.1222 ::: bot acc: 0.1032
top acc: 0.0607 ::: bot acc: 0.0529
top acc: 0.0212 ::: bot acc: 0.1751
top acc: 0.0321 ::: bot acc: 0.1446
top acc: 0.1203 ::: bot acc: 0.1373
top acc: 0.1041 ::: bot acc: 0.1330
current epoch: 32
train loss is 0.005354
average val loss: 0.002972, accuracy: 0.0604
average test loss: 0.004771, accuracy: 0.0821
case acc: 0.08210584
case acc: 0.043585546
case acc: 0.089639604
case acc: 0.08579069
case acc: 0.09913954
case acc: 0.09249934
top acc: 0.1275 ::: bot acc: 0.0979
top acc: 0.0650 ::: bot acc: 0.0484
top acc: 0.0191 ::: bot acc: 0.1694
top acc: 0.0369 ::: bot acc: 0.1388
top acc: 0.1253 ::: bot acc: 0.1324
top acc: 0.1087 ::: bot acc: 0.1284
current epoch: 33
train loss is 0.005408
average val loss: 0.002957, accuracy: 0.0601
average test loss: 0.004713, accuracy: 0.0817
case acc: 0.0818339
case acc: 0.043813694
case acc: 0.08783841
case acc: 0.08496874
case acc: 0.09896211
case acc: 0.09251479
top acc: 0.1289 ::: bot acc: 0.0957
top acc: 0.0660 ::: bot acc: 0.0478
top acc: 0.0181 ::: bot acc: 0.1673
top acc: 0.0385 ::: bot acc: 0.1365
top acc: 0.1264 ::: bot acc: 0.1306
top acc: 0.1103 ::: bot acc: 0.1271
current epoch: 34
train loss is 0.005402
average val loss: 0.002933, accuracy: 0.0598
average test loss: 0.004715, accuracy: 0.0816
case acc: 0.0819673
case acc: 0.0435211
case acc: 0.087873854
case acc: 0.08480149
case acc: 0.09934763
case acc: 0.09235598
top acc: 0.1293 ::: bot acc: 0.0958
top acc: 0.0648 ::: bot acc: 0.0485
top acc: 0.0188 ::: bot acc: 0.1670
top acc: 0.0388 ::: bot acc: 0.1363
top acc: 0.1267 ::: bot acc: 0.1314
top acc: 0.1101 ::: bot acc: 0.1271
current epoch: 35
train loss is 0.005366
average val loss: 0.002918, accuracy: 0.0598
average test loss: 0.004745, accuracy: 0.0818
case acc: 0.08214798
case acc: 0.04292601
case acc: 0.088931635
case acc: 0.08529903
case acc: 0.09919573
case acc: 0.092533596
top acc: 0.1275 ::: bot acc: 0.0980
top acc: 0.0623 ::: bot acc: 0.0510
top acc: 0.0188 ::: bot acc: 0.1687
top acc: 0.0376 ::: bot acc: 0.1376
top acc: 0.1253 ::: bot acc: 0.1324
top acc: 0.1084 ::: bot acc: 0.1289
current epoch: 36
train loss is 0.005332
average val loss: 0.002906, accuracy: 0.0595
average test loss: 0.004704, accuracy: 0.0815
case acc: 0.08217861
case acc: 0.043074332
case acc: 0.08775547
case acc: 0.084595814
case acc: 0.09904972
case acc: 0.09260566
top acc: 0.1291 ::: bot acc: 0.0965
top acc: 0.0625 ::: bot acc: 0.0509
top acc: 0.0187 ::: bot acc: 0.1671
top acc: 0.0399 ::: bot acc: 0.1352
top acc: 0.1270 ::: bot acc: 0.1302
top acc: 0.1100 ::: bot acc: 0.1276
current epoch: 37
train loss is 0.005322
average val loss: 0.002878, accuracy: 0.0592
average test loss: 0.004736, accuracy: 0.0817
case acc: 0.082061335
case acc: 0.042720128
case acc: 0.08902537
case acc: 0.0851501
case acc: 0.09894769
case acc: 0.09259414
top acc: 0.1267 ::: bot acc: 0.0983
top acc: 0.0597 ::: bot acc: 0.0539
top acc: 0.0189 ::: bot acc: 0.1687
top acc: 0.0385 ::: bot acc: 0.1370
top acc: 0.1251 ::: bot acc: 0.1323
top acc: 0.1081 ::: bot acc: 0.1293
current epoch: 38
train loss is 0.005263
average val loss: 0.002876, accuracy: 0.0595
average test loss: 0.004836, accuracy: 0.0825
case acc: 0.08197838
case acc: 0.042109575
case acc: 0.09236477
case acc: 0.0863907
case acc: 0.099025376
case acc: 0.09327656
top acc: 0.1224 ::: bot acc: 0.1027
top acc: 0.0551 ::: bot acc: 0.0584
top acc: 0.0201 ::: bot acc: 0.1731
top acc: 0.0356 ::: bot acc: 0.1402
top acc: 0.1210 ::: bot acc: 0.1364
top acc: 0.1045 ::: bot acc: 0.1331
current epoch: 39
train loss is 0.005223
average val loss: 0.002864, accuracy: 0.0594
average test loss: 0.004871, accuracy: 0.0828
case acc: 0.08226785
case acc: 0.042015087
case acc: 0.093186334
case acc: 0.08675888
case acc: 0.09896744
case acc: 0.09347199
top acc: 0.1209 ::: bot acc: 0.1043
top acc: 0.0530 ::: bot acc: 0.0607
top acc: 0.0203 ::: bot acc: 0.1742
top acc: 0.0345 ::: bot acc: 0.1413
top acc: 0.1191 ::: bot acc: 0.1382
top acc: 0.1028 ::: bot acc: 0.1348
current epoch: 40
train loss is 0.005201
average val loss: 0.002844, accuracy: 0.0590
average test loss: 0.004767, accuracy: 0.0820
case acc: 0.08200193
case acc: 0.04222716
case acc: 0.09041581
case acc: 0.0853563
case acc: 0.09904203
case acc: 0.0930161
top acc: 0.1239 ::: bot acc: 0.1011
top acc: 0.0564 ::: bot acc: 0.0572
top acc: 0.0197 ::: bot acc: 0.1705
top acc: 0.0372 ::: bot acc: 0.1381
top acc: 0.1219 ::: bot acc: 0.1354
top acc: 0.1054 ::: bot acc: 0.1319
current epoch: 41
train loss is 0.005227
average val loss: 0.002828, accuracy: 0.0586
average test loss: 0.004673, accuracy: 0.0813
case acc: 0.081881024
case acc: 0.042510595
case acc: 0.08763824
case acc: 0.084062316
case acc: 0.09887036
case acc: 0.09257766
top acc: 0.1268 ::: bot acc: 0.0980
top acc: 0.0593 ::: bot acc: 0.0538
top acc: 0.0188 ::: bot acc: 0.1668
top acc: 0.0407 ::: bot acc: 0.1341
top acc: 0.1245 ::: bot acc: 0.1326
top acc: 0.1085 ::: bot acc: 0.1285
current epoch: 42
train loss is 0.005275
average val loss: 0.002824, accuracy: 0.0586
average test loss: 0.004709, accuracy: 0.0815
case acc: 0.08198652
case acc: 0.04235138
case acc: 0.08861281
case acc: 0.0843646
case acc: 0.09901002
case acc: 0.0928404
top acc: 0.1255 ::: bot acc: 0.0997
top acc: 0.0578 ::: bot acc: 0.0554
top acc: 0.0188 ::: bot acc: 0.1683
top acc: 0.0401 ::: bot acc: 0.1350
top acc: 0.1238 ::: bot acc: 0.1338
top acc: 0.1076 ::: bot acc: 0.1302
current epoch: 43
train loss is 0.005254
average val loss: 0.002818, accuracy: 0.0587
average test loss: 0.004779, accuracy: 0.0821
case acc: 0.08227715
case acc: 0.041944988
case acc: 0.0908759
case acc: 0.0854187
case acc: 0.0989638
case acc: 0.09307439
top acc: 0.1225 ::: bot acc: 0.1029
top acc: 0.0537 ::: bot acc: 0.0595
top acc: 0.0198 ::: bot acc: 0.1711
top acc: 0.0376 ::: bot acc: 0.1378
top acc: 0.1209 ::: bot acc: 0.1365
top acc: 0.1041 ::: bot acc: 0.1332
current epoch: 44
train loss is 0.005205
average val loss: 0.002817, accuracy: 0.0587
average test loss: 0.004831, accuracy: 0.0825
case acc: 0.082329385
case acc: 0.041858528
case acc: 0.09253479
case acc: 0.085809164
case acc: 0.099033825
case acc: 0.093426816
top acc: 0.1207 ::: bot acc: 0.1048
top acc: 0.0519 ::: bot acc: 0.0620
top acc: 0.0204 ::: bot acc: 0.1732
top acc: 0.0364 ::: bot acc: 0.1389
top acc: 0.1200 ::: bot acc: 0.1374
top acc: 0.1028 ::: bot acc: 0.1347
current epoch: 45
train loss is 0.005168
average val loss: 0.002815, accuracy: 0.0588
average test loss: 0.004854, accuracy: 0.0827
case acc: 0.08241639
case acc: 0.041758254
case acc: 0.09325676
case acc: 0.08615877
case acc: 0.09885189
case acc: 0.09370519
top acc: 0.1189 ::: bot acc: 0.1063
top acc: 0.0503 ::: bot acc: 0.0635
top acc: 0.0207 ::: bot acc: 0.1742
top acc: 0.0355 ::: bot acc: 0.1400
top acc: 0.1183 ::: bot acc: 0.1386
top acc: 0.1013 ::: bot acc: 0.1360
current epoch: 46
train loss is 0.005143
average val loss: 0.002812, accuracy: 0.0587
average test loss: 0.004860, accuracy: 0.0827
case acc: 0.0824767
case acc: 0.04167173
case acc: 0.09336367
case acc: 0.085988514
case acc: 0.09904955
case acc: 0.093612395
top acc: 0.1184 ::: bot acc: 0.1068
top acc: 0.0503 ::: bot acc: 0.0634
top acc: 0.0204 ::: bot acc: 0.1744
top acc: 0.0354 ::: bot acc: 0.1397
top acc: 0.1182 ::: bot acc: 0.1395
top acc: 0.1010 ::: bot acc: 0.1362
current epoch: 47
train loss is 0.005163
average val loss: 0.002796, accuracy: 0.0583
average test loss: 0.004733, accuracy: 0.0818
case acc: 0.08203186
case acc: 0.04213454
case acc: 0.08965664
case acc: 0.084411494
case acc: 0.099262066
case acc: 0.09313814
top acc: 0.1223 ::: bot acc: 0.1028
top acc: 0.0547 ::: bot acc: 0.0591
top acc: 0.0190 ::: bot acc: 0.1695
top acc: 0.0397 ::: bot acc: 0.1353
top acc: 0.1221 ::: bot acc: 0.1355
top acc: 0.1048 ::: bot acc: 0.1325
current epoch: 48
train loss is 0.005175
average val loss: 0.002783, accuracy: 0.0580
average test loss: 0.004690, accuracy: 0.0814
case acc: 0.08198906
case acc: 0.042316057
case acc: 0.08855291
case acc: 0.083779946
case acc: 0.099094145
case acc: 0.09284392
top acc: 0.1237 ::: bot acc: 0.1015
top acc: 0.0560 ::: bot acc: 0.0577
top acc: 0.0186 ::: bot acc: 0.1681
top acc: 0.0414 ::: bot acc: 0.1335
top acc: 0.1230 ::: bot acc: 0.1343
top acc: 0.1060 ::: bot acc: 0.1311
current epoch: 49
train loss is 0.005189
average val loss: 0.002780, accuracy: 0.0580
average test loss: 0.004696, accuracy: 0.0814
case acc: 0.08210067
case acc: 0.042063728
case acc: 0.0886874
case acc: 0.0837145
case acc: 0.09907407
case acc: 0.092969626
top acc: 0.1233 ::: bot acc: 0.1020
top acc: 0.0555 ::: bot acc: 0.0578
top acc: 0.0185 ::: bot acc: 0.1684
top acc: 0.0416 ::: bot acc: 0.1334
top acc: 0.1229 ::: bot acc: 0.1346
top acc: 0.1058 ::: bot acc: 0.1317
current epoch: 50
train loss is 0.005178
average val loss: 0.002785, accuracy: 0.0582
average test loss: 0.004753, accuracy: 0.0819
case acc: 0.08218604
case acc: 0.04187878
case acc: 0.090818726
case acc: 0.08449032
case acc: 0.09894469
case acc: 0.093261525
top acc: 0.1204 ::: bot acc: 0.1047
top acc: 0.0525 ::: bot acc: 0.0609
top acc: 0.0197 ::: bot acc: 0.1712
top acc: 0.0396 ::: bot acc: 0.1353
top acc: 0.1211 ::: bot acc: 0.1361
top acc: 0.1035 ::: bot acc: 0.1340
LME_Co_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5112 5112 5112
1.7082474 -0.6288155 0.48738334 -0.33910522
Validation: 570 570 570
Testing: 774 774 774
pre-processing time: 0.0002079010009765625
the split date is 2011-07-01
train dropout: 0.4 test dropout: 0.15
net initializing with time: 0.002193927764892578
preparing training and testing date with time: 4.76837158203125e-07
current epoch: 1
train loss is 0.013107
average val loss: 0.006791, accuracy: 0.1042
average test loss: 0.007942, accuracy: 0.1092
case acc: 0.110285886
case acc: 0.16130328
case acc: 0.105278976
case acc: 0.07820855
case acc: 0.13389887
case acc: 0.066108964
top acc: 0.0407 ::: bot acc: 0.1814
top acc: 0.2266 ::: bot acc: 0.1073
top acc: 0.0393 ::: bot acc: 0.1691
top acc: 0.1201 ::: bot acc: 0.0858
top acc: 0.1011 ::: bot acc: 0.1811
top acc: 0.1277 ::: bot acc: 0.0259
current epoch: 2
train loss is 0.011533
average val loss: 0.008907, accuracy: 0.1028
average test loss: 0.010934, accuracy: 0.1193
case acc: 0.060950056
case acc: 0.2463799
case acc: 0.052409653
case acc: 0.117781155
case acc: 0.102332935
case acc: 0.13588734
top acc: 0.0785 ::: bot acc: 0.0842
top acc: 0.3116 ::: bot acc: 0.1914
top acc: 0.0684 ::: bot acc: 0.0723
top acc: 0.2112 ::: bot acc: 0.0404
top acc: 0.1849 ::: bot acc: 0.0897
top acc: 0.2142 ::: bot acc: 0.0625
current epoch: 3
train loss is 0.013409
average val loss: 0.007223, accuracy: 0.0960
average test loss: 0.009021, accuracy: 0.1096
case acc: 0.06302222
case acc: 0.21707898
case acc: 0.055953085
case acc: 0.102990106
case acc: 0.10689269
case acc: 0.11159664
top acc: 0.0634 ::: bot acc: 0.0994
top acc: 0.2823 ::: bot acc: 0.1618
top acc: 0.0517 ::: bot acc: 0.0887
top acc: 0.1915 ::: bot acc: 0.0369
top acc: 0.1650 ::: bot acc: 0.1085
top acc: 0.1900 ::: bot acc: 0.0381
current epoch: 4
train loss is 0.013714
average val loss: 0.007903, accuracy: 0.1089
average test loss: 0.008340, accuracy: 0.1100
case acc: 0.13625287
case acc: 0.086600944
case acc: 0.14361216
case acc: 0.08312082
case acc: 0.15140872
case acc: 0.058791544
top acc: 0.0549 ::: bot acc: 0.2145
top acc: 0.1518 ::: bot acc: 0.0316
top acc: 0.0717 ::: bot acc: 0.2101
top acc: 0.0700 ::: bot acc: 0.1350
top acc: 0.0729 ::: bot acc: 0.2212
top acc: 0.0681 ::: bot acc: 0.0846
current epoch: 5
train loss is 0.008888
average val loss: 0.007007, accuracy: 0.1035
average test loss: 0.007394, accuracy: 0.1044
case acc: 0.1223838
case acc: 0.08978417
case acc: 0.13375677
case acc: 0.08074831
case acc: 0.14196886
case acc: 0.05773583
top acc: 0.0451 ::: bot acc: 0.1985
top acc: 0.1541 ::: bot acc: 0.0350
top acc: 0.0633 ::: bot acc: 0.1996
top acc: 0.0815 ::: bot acc: 0.1231
top acc: 0.0877 ::: bot acc: 0.1995
top acc: 0.0797 ::: bot acc: 0.0732
current epoch: 6
train loss is 0.007424
average val loss: 0.005451, accuracy: 0.0935
average test loss: 0.006134, accuracy: 0.0954
case acc: 0.08805701
case acc: 0.12276885
case acc: 0.09636783
case acc: 0.0786393
case acc: 0.12294126
case acc: 0.06333549
top acc: 0.0333 ::: bot acc: 0.1528
top acc: 0.1881 ::: bot acc: 0.0680
top acc: 0.0339 ::: bot acc: 0.1589
top acc: 0.1242 ::: bot acc: 0.0829
top acc: 0.1254 ::: bot acc: 0.1527
top acc: 0.1202 ::: bot acc: 0.0335
current epoch: 7
train loss is 0.007533
average val loss: 0.005476, accuracy: 0.0933
average test loss: 0.006002, accuracy: 0.0946
case acc: 0.091415256
case acc: 0.107093275
case acc: 0.10522113
case acc: 0.07875593
case acc: 0.12455686
case acc: 0.06033432
top acc: 0.0329 ::: bot acc: 0.1583
top acc: 0.1716 ::: bot acc: 0.0528
top acc: 0.0392 ::: bot acc: 0.1696
top acc: 0.1143 ::: bot acc: 0.0923
top acc: 0.1210 ::: bot acc: 0.1561
top acc: 0.1093 ::: bot acc: 0.0435
current epoch: 8
train loss is 0.007328
average val loss: 0.005682, accuracy: 0.0944
average test loss: 0.005996, accuracy: 0.0941
case acc: 0.09713306
case acc: 0.08780936
case acc: 0.115346715
case acc: 0.07867136
case acc: 0.12738334
case acc: 0.058253504
top acc: 0.0345 ::: bot acc: 0.1653
top acc: 0.1523 ::: bot acc: 0.0332
top acc: 0.0467 ::: bot acc: 0.1809
top acc: 0.1018 ::: bot acc: 0.1030
top acc: 0.1153 ::: bot acc: 0.1636
top acc: 0.0976 ::: bot acc: 0.0562
current epoch: 9
train loss is 0.006821
average val loss: 0.005516, accuracy: 0.0929
average test loss: 0.005741, accuracy: 0.0917
case acc: 0.09247103
case acc: 0.08327708
case acc: 0.1127047
case acc: 0.078543566
case acc: 0.12470034
case acc: 0.05833367
top acc: 0.0334 ::: bot acc: 0.1594
top acc: 0.1481 ::: bot acc: 0.0291
top acc: 0.0442 ::: bot acc: 0.1779
top acc: 0.1055 ::: bot acc: 0.1003
top acc: 0.1214 ::: bot acc: 0.1564
top acc: 0.0998 ::: bot acc: 0.0532
current epoch: 10
train loss is 0.006546
average val loss: 0.005303, accuracy: 0.0911
average test loss: 0.005517, accuracy: 0.0894
case acc: 0.08737617
case acc: 0.07997985
case acc: 0.10973166
case acc: 0.078125015
case acc: 0.12226959
case acc: 0.05868262
top acc: 0.0326 ::: bot acc: 0.1521
top acc: 0.1450 ::: bot acc: 0.0254
top acc: 0.0421 ::: bot acc: 0.1746
top acc: 0.1093 ::: bot acc: 0.0964
top acc: 0.1258 ::: bot acc: 0.1517
top acc: 0.1022 ::: bot acc: 0.0507
current epoch: 11
train loss is 0.006306
average val loss: 0.005185, accuracy: 0.0898
average test loss: 0.005366, accuracy: 0.0878
case acc: 0.08457274
case acc: 0.07455969
case acc: 0.10905119
case acc: 0.07816031
case acc: 0.12111548
case acc: 0.059067637
top acc: 0.0321 ::: bot acc: 0.1486
top acc: 0.1388 ::: bot acc: 0.0203
top acc: 0.0413 ::: bot acc: 0.1742
top acc: 0.1102 ::: bot acc: 0.0953
top acc: 0.1279 ::: bot acc: 0.1483
top acc: 0.1029 ::: bot acc: 0.0509
current epoch: 12
train loss is 0.006134
average val loss: 0.005044, accuracy: 0.0885
average test loss: 0.005222, accuracy: 0.0861
case acc: 0.081707194
case acc: 0.0706758
case acc: 0.107090026
case acc: 0.07834417
case acc: 0.120130494
case acc: 0.058895223
top acc: 0.0333 ::: bot acc: 0.1431
top acc: 0.1351 ::: bot acc: 0.0174
top acc: 0.0400 ::: bot acc: 0.1716
top acc: 0.1130 ::: bot acc: 0.0931
top acc: 0.1315 ::: bot acc: 0.1452
top acc: 0.1045 ::: bot acc: 0.0480
current epoch: 13
train loss is 0.005997
average val loss: 0.004943, accuracy: 0.0874
average test loss: 0.005108, accuracy: 0.0847
case acc: 0.07930853
case acc: 0.06618885
case acc: 0.10585585
case acc: 0.07809787
case acc: 0.11932693
case acc: 0.059341453
top acc: 0.0339 ::: bot acc: 0.1396
top acc: 0.1307 ::: bot acc: 0.0126
top acc: 0.0390 ::: bot acc: 0.1703
top acc: 0.1148 ::: bot acc: 0.0908
top acc: 0.1326 ::: bot acc: 0.1434
top acc: 0.1052 ::: bot acc: 0.0479
current epoch: 14
train loss is 0.005850
average val loss: 0.004907, accuracy: 0.0869
average test loss: 0.005022, accuracy: 0.0837
case acc: 0.07839707
case acc: 0.06071253
case acc: 0.106261656
case acc: 0.0783399
case acc: 0.119730204
case acc: 0.058664806
top acc: 0.0345 ::: bot acc: 0.1376
top acc: 0.1234 ::: bot acc: 0.0099
top acc: 0.0395 ::: bot acc: 0.1707
top acc: 0.1155 ::: bot acc: 0.0911
top acc: 0.1318 ::: bot acc: 0.1440
top acc: 0.1033 ::: bot acc: 0.0493
current epoch: 15
train loss is 0.005663
average val loss: 0.004854, accuracy: 0.0863
average test loss: 0.004973, accuracy: 0.0830
case acc: 0.077656105
case acc: 0.056124195
case acc: 0.10663697
case acc: 0.078472175
case acc: 0.120326966
case acc: 0.05879136
top acc: 0.0349 ::: bot acc: 0.1366
top acc: 0.1170 ::: bot acc: 0.0089
top acc: 0.0400 ::: bot acc: 0.1716
top acc: 0.1151 ::: bot acc: 0.0913
top acc: 0.1313 ::: bot acc: 0.1453
top acc: 0.1031 ::: bot acc: 0.0503
current epoch: 16
train loss is 0.005562
average val loss: 0.004793, accuracy: 0.0855
average test loss: 0.004866, accuracy: 0.0819
case acc: 0.07595683
case acc: 0.05349659
case acc: 0.105556026
case acc: 0.07801869
case acc: 0.11965556
case acc: 0.05863835
top acc: 0.0356 ::: bot acc: 0.1332
top acc: 0.1127 ::: bot acc: 0.0108
top acc: 0.0389 ::: bot acc: 0.1702
top acc: 0.1156 ::: bot acc: 0.0901
top acc: 0.1305 ::: bot acc: 0.1446
top acc: 0.1023 ::: bot acc: 0.0508
current epoch: 17
train loss is 0.005418
average val loss: 0.004749, accuracy: 0.0850
average test loss: 0.004833, accuracy: 0.0815
case acc: 0.07502549
case acc: 0.05144286
case acc: 0.10543079
case acc: 0.077892125
case acc: 0.12070217
case acc: 0.058612436
top acc: 0.0369 ::: bot acc: 0.1315
top acc: 0.1083 ::: bot acc: 0.0134
top acc: 0.0388 ::: bot acc: 0.1696
top acc: 0.1164 ::: bot acc: 0.0893
top acc: 0.1313 ::: bot acc: 0.1459
top acc: 0.1021 ::: bot acc: 0.0511
current epoch: 18
train loss is 0.005334
average val loss: 0.004591, accuracy: 0.0835
average test loss: 0.004717, accuracy: 0.0803
case acc: 0.07215137
case acc: 0.050717887
case acc: 0.10204961
case acc: 0.07825603
case acc: 0.11928273
case acc: 0.05954824
top acc: 0.0404 ::: bot acc: 0.1256
top acc: 0.1064 ::: bot acc: 0.0141
top acc: 0.0366 ::: bot acc: 0.1662
top acc: 0.1206 ::: bot acc: 0.0857
top acc: 0.1325 ::: bot acc: 0.1431
top acc: 0.1055 ::: bot acc: 0.0489
current epoch: 19
train loss is 0.005264
average val loss: 0.004489, accuracy: 0.0825
average test loss: 0.004630, accuracy: 0.0795
case acc: 0.07083495
case acc: 0.050320767
case acc: 0.09910797
case acc: 0.07831079
case acc: 0.11894129
case acc: 0.05963415
top acc: 0.0436 ::: bot acc: 0.1218
top acc: 0.1053 ::: bot acc: 0.0158
top acc: 0.0350 ::: bot acc: 0.1625
top acc: 0.1234 ::: bot acc: 0.0826
top acc: 0.1341 ::: bot acc: 0.1417
top acc: 0.1068 ::: bot acc: 0.0469
current epoch: 20
train loss is 0.005221
average val loss: 0.004413, accuracy: 0.0816
average test loss: 0.004553, accuracy: 0.0787
case acc: 0.06935429
case acc: 0.048996646
case acc: 0.09732122
case acc: 0.07828959
case acc: 0.11896018
case acc: 0.059424818
top acc: 0.0460 ::: bot acc: 0.1182
top acc: 0.1025 ::: bot acc: 0.0172
top acc: 0.0340 ::: bot acc: 0.1604
top acc: 0.1252 ::: bot acc: 0.0804
top acc: 0.1339 ::: bot acc: 0.1421
top acc: 0.1069 ::: bot acc: 0.0462
current epoch: 21
train loss is 0.005159
average val loss: 0.004351, accuracy: 0.0810
average test loss: 0.004510, accuracy: 0.0783
case acc: 0.06825109
case acc: 0.04839011
case acc: 0.09608128
case acc: 0.07836607
case acc: 0.11916008
case acc: 0.05952368
top acc: 0.0488 ::: bot acc: 0.1153
top acc: 0.1000 ::: bot acc: 0.0195
top acc: 0.0340 ::: bot acc: 0.1584
top acc: 0.1271 ::: bot acc: 0.0790
top acc: 0.1345 ::: bot acc: 0.1418
top acc: 0.1076 ::: bot acc: 0.0456
current epoch: 22
train loss is 0.005110
average val loss: 0.004370, accuracy: 0.0811
average test loss: 0.004488, accuracy: 0.0782
case acc: 0.06853737
case acc: 0.046700817
case acc: 0.09644167
case acc: 0.07845108
case acc: 0.11939521
case acc: 0.059530184
top acc: 0.0494 ::: bot acc: 0.1155
top acc: 0.0954 ::: bot acc: 0.0244
top acc: 0.0334 ::: bot acc: 0.1597
top acc: 0.1258 ::: bot acc: 0.0796
top acc: 0.1323 ::: bot acc: 0.1435
top acc: 0.1060 ::: bot acc: 0.0479
current epoch: 23
train loss is 0.005055
average val loss: 0.004283, accuracy: 0.0801
average test loss: 0.004431, accuracy: 0.0775
case acc: 0.06698571
case acc: 0.046714548
case acc: 0.09390297
case acc: 0.07860719
case acc: 0.11900383
case acc: 0.059771497
top acc: 0.0529 ::: bot acc: 0.1113
top acc: 0.0945 ::: bot acc: 0.0259
top acc: 0.0322 ::: bot acc: 0.1566
top acc: 0.1285 ::: bot acc: 0.0775
top acc: 0.1341 ::: bot acc: 0.1420
top acc: 0.1079 ::: bot acc: 0.0457
current epoch: 24
train loss is 0.005012
average val loss: 0.004319, accuracy: 0.0804
average test loss: 0.004445, accuracy: 0.0777
case acc: 0.067541406
case acc: 0.04502299
case acc: 0.09603641
case acc: 0.078414224
case acc: 0.120236374
case acc: 0.059206676
top acc: 0.0514 ::: bot acc: 0.1129
top acc: 0.0884 ::: bot acc: 0.0310
top acc: 0.0330 ::: bot acc: 0.1591
top acc: 0.1263 ::: bot acc: 0.0797
top acc: 0.1307 ::: bot acc: 0.1452
top acc: 0.1045 ::: bot acc: 0.0493
current epoch: 25
train loss is 0.004961
average val loss: 0.004283, accuracy: 0.0800
average test loss: 0.004415, accuracy: 0.0775
case acc: 0.06714469
case acc: 0.044924524
case acc: 0.09501639
case acc: 0.07868842
case acc: 0.12034678
case acc: 0.059049312
top acc: 0.0532 ::: bot acc: 0.1115
top acc: 0.0871 ::: bot acc: 0.0329
top acc: 0.0329 ::: bot acc: 0.1576
top acc: 0.1276 ::: bot acc: 0.0785
top acc: 0.1307 ::: bot acc: 0.1456
top acc: 0.1044 ::: bot acc: 0.0491
current epoch: 26
train loss is 0.004934
average val loss: 0.004232, accuracy: 0.0795
average test loss: 0.004379, accuracy: 0.0772
case acc: 0.06634274
case acc: 0.045051116
case acc: 0.09319143
case acc: 0.078724965
case acc: 0.12022392
case acc: 0.059403814
top acc: 0.0559 ::: bot acc: 0.1085
top acc: 0.0861 ::: bot acc: 0.0339
top acc: 0.0321 ::: bot acc: 0.1554
top acc: 0.1293 ::: bot acc: 0.0769
top acc: 0.1310 ::: bot acc: 0.1453
top acc: 0.1057 ::: bot acc: 0.0483
current epoch: 27
train loss is 0.004911
average val loss: 0.004181, accuracy: 0.0790
average test loss: 0.004350, accuracy: 0.0769
case acc: 0.065992475
case acc: 0.044800863
case acc: 0.09187188
case acc: 0.07870427
case acc: 0.12042797
case acc: 0.059358552
top acc: 0.0577 ::: bot acc: 0.1070
top acc: 0.0852 ::: bot acc: 0.0350
top acc: 0.0313 ::: bot acc: 0.1536
top acc: 0.1302 ::: bot acc: 0.0755
top acc: 0.1311 ::: bot acc: 0.1455
top acc: 0.1055 ::: bot acc: 0.0483
current epoch: 28
train loss is 0.004889
average val loss: 0.004150, accuracy: 0.0786
average test loss: 0.004318, accuracy: 0.0766
case acc: 0.065316856
case acc: 0.044567797
case acc: 0.090878464
case acc: 0.07899592
case acc: 0.12045222
case acc: 0.05911225
top acc: 0.0592 ::: bot acc: 0.1050
top acc: 0.0840 ::: bot acc: 0.0357
top acc: 0.0312 ::: bot acc: 0.1520
top acc: 0.1316 ::: bot acc: 0.0748
top acc: 0.1306 ::: bot acc: 0.1460
top acc: 0.1052 ::: bot acc: 0.0478
current epoch: 29
train loss is 0.004897
average val loss: 0.004032, accuracy: 0.0773
average test loss: 0.004246, accuracy: 0.0756
case acc: 0.064132996
case acc: 0.044845264
case acc: 0.08656858
case acc: 0.07903071
case acc: 0.118894726
case acc: 0.060073145
top acc: 0.0652 ::: bot acc: 0.0993
top acc: 0.0870 ::: bot acc: 0.0323
top acc: 0.0296 ::: bot acc: 0.1466
top acc: 0.1361 ::: bot acc: 0.0696
top acc: 0.1343 ::: bot acc: 0.1416
top acc: 0.1093 ::: bot acc: 0.0440
current epoch: 30
train loss is 0.004881
average val loss: 0.003988, accuracy: 0.0768
average test loss: 0.004228, accuracy: 0.0755
case acc: 0.064018115
case acc: 0.045286734
case acc: 0.08542139
case acc: 0.079311475
case acc: 0.11868044
case acc: 0.060051404
top acc: 0.0673 ::: bot acc: 0.0972
top acc: 0.0876 ::: bot acc: 0.0330
top acc: 0.0300 ::: bot acc: 0.1444
top acc: 0.1375 ::: bot acc: 0.0683
top acc: 0.1345 ::: bot acc: 0.1412
top acc: 0.1092 ::: bot acc: 0.0441
current epoch: 31
train loss is 0.004871
average val loss: 0.003957, accuracy: 0.0764
average test loss: 0.004205, accuracy: 0.0751
case acc: 0.06350079
case acc: 0.04527755
case acc: 0.083880976
case acc: 0.07961859
case acc: 0.11813506
case acc: 0.060353834
top acc: 0.0697 ::: bot acc: 0.0949
top acc: 0.0878 ::: bot acc: 0.0326
top acc: 0.0302 ::: bot acc: 0.1423
top acc: 0.1394 ::: bot acc: 0.0666
top acc: 0.1359 ::: bot acc: 0.1398
top acc: 0.1108 ::: bot acc: 0.0427
current epoch: 32
train loss is 0.004875
average val loss: 0.003911, accuracy: 0.0758
average test loss: 0.004177, accuracy: 0.0748
case acc: 0.06299129
case acc: 0.04512355
case acc: 0.08235155
case acc: 0.07984372
case acc: 0.11748417
case acc: 0.06070686
top acc: 0.0715 ::: bot acc: 0.0928
top acc: 0.0878 ::: bot acc: 0.0319
top acc: 0.0303 ::: bot acc: 0.1398
top acc: 0.1415 ::: bot acc: 0.0648
top acc: 0.1375 ::: bot acc: 0.1383
top acc: 0.1118 ::: bot acc: 0.0412
current epoch: 33
train loss is 0.004877
average val loss: 0.003906, accuracy: 0.0757
average test loss: 0.004166, accuracy: 0.0746
case acc: 0.062975824
case acc: 0.045081016
case acc: 0.08154912
case acc: 0.07988066
case acc: 0.11756358
case acc: 0.060630858
top acc: 0.0727 ::: bot acc: 0.0918
top acc: 0.0868 ::: bot acc: 0.0331
top acc: 0.0296 ::: bot acc: 0.1390
top acc: 0.1420 ::: bot acc: 0.0643
top acc: 0.1378 ::: bot acc: 0.1380
top acc: 0.1117 ::: bot acc: 0.0419
current epoch: 34
train loss is 0.004853
average val loss: 0.003892, accuracy: 0.0756
average test loss: 0.004160, accuracy: 0.0746
case acc: 0.06318379
case acc: 0.044953316
case acc: 0.081545174
case acc: 0.07987934
case acc: 0.1177267
case acc: 0.06049446
top acc: 0.0731 ::: bot acc: 0.0916
top acc: 0.0861 ::: bot acc: 0.0342
top acc: 0.0303 ::: bot acc: 0.1386
top acc: 0.1418 ::: bot acc: 0.0643
top acc: 0.1378 ::: bot acc: 0.1379
top acc: 0.1114 ::: bot acc: 0.0419
current epoch: 35
train loss is 0.004852
average val loss: 0.003874, accuracy: 0.0754
average test loss: 0.004150, accuracy: 0.0745
case acc: 0.06313164
case acc: 0.04479063
case acc: 0.08095907
case acc: 0.08003996
case acc: 0.11753461
case acc: 0.060488924
top acc: 0.0736 ::: bot acc: 0.0913
top acc: 0.0851 ::: bot acc: 0.0346
top acc: 0.0301 ::: bot acc: 0.1377
top acc: 0.1428 ::: bot acc: 0.0633
top acc: 0.1375 ::: bot acc: 0.1379
top acc: 0.1117 ::: bot acc: 0.0416
current epoch: 36
train loss is 0.004843
average val loss: 0.003845, accuracy: 0.0750
average test loss: 0.004140, accuracy: 0.0744
case acc: 0.063065805
case acc: 0.044937465
case acc: 0.08032878
case acc: 0.08025375
case acc: 0.11711851
case acc: 0.06077102
top acc: 0.0750 ::: bot acc: 0.0900
top acc: 0.0854 ::: bot acc: 0.0348
top acc: 0.0307 ::: bot acc: 0.1364
top acc: 0.1440 ::: bot acc: 0.0619
top acc: 0.1380 ::: bot acc: 0.1371
top acc: 0.1123 ::: bot acc: 0.0411
current epoch: 37
train loss is 0.004828
average val loss: 0.003847, accuracy: 0.0750
average test loss: 0.004139, accuracy: 0.0743
case acc: 0.063087955
case acc: 0.044709943
case acc: 0.08036544
case acc: 0.080145165
case acc: 0.11702713
case acc: 0.060749937
top acc: 0.0749 ::: bot acc: 0.0902
top acc: 0.0845 ::: bot acc: 0.0358
top acc: 0.0304 ::: bot acc: 0.1368
top acc: 0.1439 ::: bot acc: 0.0620
top acc: 0.1382 ::: bot acc: 0.1371
top acc: 0.1119 ::: bot acc: 0.0418
current epoch: 38
train loss is 0.004831
average val loss: 0.003845, accuracy: 0.0750
average test loss: 0.004127, accuracy: 0.0742
case acc: 0.063082404
case acc: 0.0445879
case acc: 0.08010143
case acc: 0.08017587
case acc: 0.1169224
case acc: 0.06056464
top acc: 0.0747 ::: bot acc: 0.0903
top acc: 0.0835 ::: bot acc: 0.0366
top acc: 0.0302 ::: bot acc: 0.1365
top acc: 0.1439 ::: bot acc: 0.0619
top acc: 0.1383 ::: bot acc: 0.1367
top acc: 0.1116 ::: bot acc: 0.0419
current epoch: 39
train loss is 0.004809
average val loss: 0.003856, accuracy: 0.0751
average test loss: 0.004133, accuracy: 0.0744
case acc: 0.06325219
case acc: 0.044358123
case acc: 0.08094733
case acc: 0.07994964
case acc: 0.1175626
case acc: 0.060311783
top acc: 0.0733 ::: bot acc: 0.0918
top acc: 0.0812 ::: bot acc: 0.0390
top acc: 0.0301 ::: bot acc: 0.1376
top acc: 0.1429 ::: bot acc: 0.0630
top acc: 0.1373 ::: bot acc: 0.1382
top acc: 0.1103 ::: bot acc: 0.0434
current epoch: 40
train loss is 0.004802
average val loss: 0.003866, accuracy: 0.0753
average test loss: 0.004132, accuracy: 0.0744
case acc: 0.06331664
case acc: 0.04414803
case acc: 0.08135357
case acc: 0.080002956
case acc: 0.117651165
case acc: 0.06007327
top acc: 0.0727 ::: bot acc: 0.0922
top acc: 0.0794 ::: bot acc: 0.0404
top acc: 0.0301 ::: bot acc: 0.1383
top acc: 0.1424 ::: bot acc: 0.0638
top acc: 0.1365 ::: bot acc: 0.1390
top acc: 0.1096 ::: bot acc: 0.0438
current epoch: 41
train loss is 0.004790
average val loss: 0.003886, accuracy: 0.0755
average test loss: 0.004144, accuracy: 0.0746
case acc: 0.06340044
case acc: 0.044098847
case acc: 0.08234512
case acc: 0.079583414
case acc: 0.11810338
case acc: 0.060059022
top acc: 0.0711 ::: bot acc: 0.0938
top acc: 0.0775 ::: bot acc: 0.0424
top acc: 0.0303 ::: bot acc: 0.1398
top acc: 0.1414 ::: bot acc: 0.0644
top acc: 0.1361 ::: bot acc: 0.1398
top acc: 0.1090 ::: bot acc: 0.0449
current epoch: 42
train loss is 0.004793
average val loss: 0.003923, accuracy: 0.0760
average test loss: 0.004157, accuracy: 0.0749
case acc: 0.063830115
case acc: 0.04414679
case acc: 0.08361568
case acc: 0.07951729
case acc: 0.118612066
case acc: 0.05954174
top acc: 0.0693 ::: bot acc: 0.0957
top acc: 0.0748 ::: bot acc: 0.0452
top acc: 0.0300 ::: bot acc: 0.1420
top acc: 0.1398 ::: bot acc: 0.0663
top acc: 0.1349 ::: bot acc: 0.1411
top acc: 0.1073 ::: bot acc: 0.0461
current epoch: 43
train loss is 0.004787
average val loss: 0.003970, accuracy: 0.0765
average test loss: 0.004186, accuracy: 0.0752
case acc: 0.06412843
case acc: 0.04421725
case acc: 0.08522088
case acc: 0.07929108
case acc: 0.11924899
case acc: 0.05931448
top acc: 0.0671 ::: bot acc: 0.0976
top acc: 0.0722 ::: bot acc: 0.0478
top acc: 0.0299 ::: bot acc: 0.1442
top acc: 0.1380 ::: bot acc: 0.0679
top acc: 0.1344 ::: bot acc: 0.1424
top acc: 0.1060 ::: bot acc: 0.0476
current epoch: 44
train loss is 0.004766
average val loss: 0.004044, accuracy: 0.0774
average test loss: 0.004213, accuracy: 0.0757
case acc: 0.06454983
case acc: 0.04447851
case acc: 0.08747757
case acc: 0.07912276
case acc: 0.11965218
case acc: 0.05875045
top acc: 0.0641 ::: bot acc: 0.1004
top acc: 0.0686 ::: bot acc: 0.0514
top acc: 0.0299 ::: bot acc: 0.1477
top acc: 0.1354 ::: bot acc: 0.0705
top acc: 0.1326 ::: bot acc: 0.1440
top acc: 0.1034 ::: bot acc: 0.0496
current epoch: 45
train loss is 0.004762
average val loss: 0.004174, accuracy: 0.0789
average test loss: 0.004287, accuracy: 0.0767
case acc: 0.06573581
case acc: 0.045263793
case acc: 0.091296986
case acc: 0.07867669
case acc: 0.120898314
case acc: 0.058355022
top acc: 0.0595 ::: bot acc: 0.1055
top acc: 0.0631 ::: bot acc: 0.0570
top acc: 0.0312 ::: bot acc: 0.1527
top acc: 0.1310 ::: bot acc: 0.0752
top acc: 0.1295 ::: bot acc: 0.1472
top acc: 0.0996 ::: bot acc: 0.0541
current epoch: 46
train loss is 0.004763
average val loss: 0.004342, accuracy: 0.0807
average test loss: 0.004392, accuracy: 0.0780
case acc: 0.06707903
case acc: 0.04653773
case acc: 0.09605415
case acc: 0.078248814
case acc: 0.122386605
case acc: 0.05792339
top acc: 0.0540 ::: bot acc: 0.1108
top acc: 0.0570 ::: bot acc: 0.0630
top acc: 0.0332 ::: bot acc: 0.1589
top acc: 0.1258 ::: bot acc: 0.0800
top acc: 0.1258 ::: bot acc: 0.1514
top acc: 0.0951 ::: bot acc: 0.0585
current epoch: 47
train loss is 0.004785
average val loss: 0.004571, accuracy: 0.0830
average test loss: 0.004557, accuracy: 0.0798
case acc: 0.068882346
case acc: 0.04841364
case acc: 0.10152662
case acc: 0.07828582
case acc: 0.12439124
case acc: 0.05753234
top acc: 0.0486 ::: bot acc: 0.1164
top acc: 0.0501 ::: bot acc: 0.0698
top acc: 0.0359 ::: bot acc: 0.1659
top acc: 0.1203 ::: bot acc: 0.0861
top acc: 0.1222 ::: bot acc: 0.1561
top acc: 0.0905 ::: bot acc: 0.0628
current epoch: 48
train loss is 0.004815
average val loss: 0.004853, accuracy: 0.0857
average test loss: 0.004753, accuracy: 0.0820
case acc: 0.07094345
case acc: 0.050936226
case acc: 0.107651085
case acc: 0.07824162
case acc: 0.12633824
case acc: 0.057672385
top acc: 0.0430 ::: bot acc: 0.1221
top acc: 0.0436 ::: bot acc: 0.0768
top acc: 0.0401 ::: bot acc: 0.1729
top acc: 0.1142 ::: bot acc: 0.0920
top acc: 0.1183 ::: bot acc: 0.1609
top acc: 0.0856 ::: bot acc: 0.0678
current epoch: 49
train loss is 0.004885
average val loss: 0.005086, accuracy: 0.0878
average test loss: 0.004941, accuracy: 0.0838
case acc: 0.07283394
case acc: 0.053432602
case acc: 0.11312549
case acc: 0.07828903
case acc: 0.1274786
case acc: 0.057893116
top acc: 0.0397 ::: bot acc: 0.1270
top acc: 0.0391 ::: bot acc: 0.0828
top acc: 0.0444 ::: bot acc: 0.1790
top acc: 0.1098 ::: bot acc: 0.0964
top acc: 0.1164 ::: bot acc: 0.1639
top acc: 0.0824 ::: bot acc: 0.0713
current epoch: 50
train loss is 0.004962
average val loss: 0.005597, accuracy: 0.0922
average test loss: 0.005313, accuracy: 0.0874
case acc: 0.07686951
case acc: 0.057973403
case acc: 0.12262893
case acc: 0.07887626
case acc: 0.12976685
case acc: 0.058161095
top acc: 0.0353 ::: bot acc: 0.1349
top acc: 0.0329 ::: bot acc: 0.0927
top acc: 0.0520 ::: bot acc: 0.1894
top acc: 0.1011 ::: bot acc: 0.1049
top acc: 0.1110 ::: bot acc: 0.1698
top acc: 0.0760 ::: bot acc: 0.0776
LME_Co_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5142 5142 5142
1.7082474 -0.6288155 0.48738334 -0.25570297
Validation: 576 576 576
Testing: 750 750 750
pre-processing time: 0.0001926422119140625
the split date is 2012-01-01
train dropout: 0.4 test dropout: 0.15
net initializing with time: 0.0023872852325439453
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012805
average val loss: 0.010135, accuracy: 0.1227
average test loss: 0.006712, accuracy: 0.0956
case acc: 0.07509568
case acc: 0.18191676
case acc: 0.08104911
case acc: 0.07948452
case acc: 0.09032494
case acc: 0.06560384
top acc: 0.0282 ::: bot acc: 0.1267
top acc: 0.2341 ::: bot acc: 0.1246
top acc: 0.1483 ::: bot acc: 0.0374
top acc: 0.1603 ::: bot acc: 0.0234
top acc: 0.1518 ::: bot acc: 0.0306
top acc: 0.1144 ::: bot acc: 0.0376
current epoch: 2
train loss is 0.010389
average val loss: 0.006292, accuracy: 0.0936
average test loss: 0.017957, accuracy: 0.1670
case acc: 0.053336184
case acc: 0.2785273
case acc: 0.16534409
case acc: 0.17017174
case acc: 0.19185865
case acc: 0.14265734
top acc: 0.0932 ::: bot acc: 0.0290
top acc: 0.3294 ::: bot acc: 0.2208
top acc: 0.2545 ::: bot acc: 0.0753
top acc: 0.2648 ::: bot acc: 0.0844
top acc: 0.2564 ::: bot acc: 0.1279
top acc: 0.2155 ::: bot acc: 0.0656
current epoch: 3
train loss is 0.012072
average val loss: 0.007209, accuracy: 0.0966
average test loss: 0.024283, accuracy: 0.2017
case acc: 0.0849462
case acc: 0.30613303
case acc: 0.20397331
case acc: 0.20684607
case acc: 0.23117779
case acc: 0.1772527
top acc: 0.1367 ::: bot acc: 0.0380
top acc: 0.3568 ::: bot acc: 0.2485
top acc: 0.2940 ::: bot acc: 0.1131
top acc: 0.3022 ::: bot acc: 0.1203
top acc: 0.2964 ::: bot acc: 0.1671
top acc: 0.2497 ::: bot acc: 0.0994
current epoch: 4
train loss is 0.013886
average val loss: 0.005422, accuracy: 0.0889
average test loss: 0.013229, accuracy: 0.1398
case acc: 0.042833608
case acc: 0.2303402
case acc: 0.13937642
case acc: 0.14237177
case acc: 0.17152724
case acc: 0.112125136
top acc: 0.0735 ::: bot acc: 0.0378
top acc: 0.2813 ::: bot acc: 0.1731
top acc: 0.2288 ::: bot acc: 0.0516
top acc: 0.2383 ::: bot acc: 0.0553
top acc: 0.2354 ::: bot acc: 0.1077
top acc: 0.1837 ::: bot acc: 0.0359
current epoch: 5
train loss is 0.011497
average val loss: 0.008776, accuracy: 0.1108
average test loss: 0.005760, accuracy: 0.0895
case acc: 0.06365454
case acc: 0.14431313
case acc: 0.08088696
case acc: 0.080966346
case acc: 0.102976166
case acc: 0.064471334
top acc: 0.0237 ::: bot acc: 0.1111
top acc: 0.1949 ::: bot acc: 0.0874
top acc: 0.1481 ::: bot acc: 0.0373
top acc: 0.1619 ::: bot acc: 0.0223
top acc: 0.1674 ::: bot acc: 0.0393
top acc: 0.1114 ::: bot acc: 0.0394
current epoch: 6
train loss is 0.007983
average val loss: 0.008354, accuracy: 0.1072
average test loss: 0.005792, accuracy: 0.0898
case acc: 0.06031314
case acc: 0.13755526
case acc: 0.08079392
case acc: 0.082585335
case acc: 0.11029183
case acc: 0.06745221
top acc: 0.0235 ::: bot acc: 0.1063
top acc: 0.1888 ::: bot acc: 0.0804
top acc: 0.1489 ::: bot acc: 0.0371
top acc: 0.1662 ::: bot acc: 0.0189
top acc: 0.1748 ::: bot acc: 0.0465
top acc: 0.1184 ::: bot acc: 0.0343
current epoch: 7
train loss is 0.006860
average val loss: 0.006153, accuracy: 0.0930
average test loss: 0.007922, accuracy: 0.1054
case acc: 0.042918447
case acc: 0.16063602
case acc: 0.09669509
case acc: 0.10476694
case acc: 0.14233153
case acc: 0.08497548
top acc: 0.0368 ::: bot acc: 0.0738
top acc: 0.2117 ::: bot acc: 0.1032
top acc: 0.1757 ::: bot acc: 0.0297
top acc: 0.1962 ::: bot acc: 0.0233
top acc: 0.2072 ::: bot acc: 0.0787
top acc: 0.1499 ::: bot acc: 0.0227
current epoch: 8
train loss is 0.006729
average val loss: 0.005911, accuracy: 0.0906
average test loss: 0.007956, accuracy: 0.1057
case acc: 0.04133172
case acc: 0.15531626
case acc: 0.09806473
case acc: 0.10738509
case acc: 0.14527802
case acc: 0.08685486
top acc: 0.0408 ::: bot acc: 0.0688
top acc: 0.2059 ::: bot acc: 0.0989
top acc: 0.1780 ::: bot acc: 0.0301
top acc: 0.1997 ::: bot acc: 0.0253
top acc: 0.2102 ::: bot acc: 0.0819
top acc: 0.1532 ::: bot acc: 0.0222
current epoch: 9
train loss is 0.006622
average val loss: 0.006466, accuracy: 0.0932
average test loss: 0.006828, accuracy: 0.0975
case acc: 0.044730037
case acc: 0.13627267
case acc: 0.089842916
case acc: 0.098516256
case acc: 0.13559379
case acc: 0.07988499
top acc: 0.0323 ::: bot acc: 0.0791
top acc: 0.1869 ::: bot acc: 0.0794
top acc: 0.1658 ::: bot acc: 0.0308
top acc: 0.1890 ::: bot acc: 0.0193
top acc: 0.1998 ::: bot acc: 0.0722
top acc: 0.1424 ::: bot acc: 0.0233
current epoch: 10
train loss is 0.006262
average val loss: 0.006182, accuracy: 0.0904
average test loss: 0.006881, accuracy: 0.0977
case acc: 0.042025115
case acc: 0.13101205
case acc: 0.090784855
case acc: 0.10110229
case acc: 0.13905984
case acc: 0.082248956
top acc: 0.0352 ::: bot acc: 0.0728
top acc: 0.1823 ::: bot acc: 0.0741
top acc: 0.1667 ::: bot acc: 0.0305
top acc: 0.1920 ::: bot acc: 0.0213
top acc: 0.2038 ::: bot acc: 0.0756
top acc: 0.1460 ::: bot acc: 0.0226
current epoch: 11
train loss is 0.006062
average val loss: 0.006379, accuracy: 0.0910
average test loss: 0.006474, accuracy: 0.0945
case acc: 0.042817492
case acc: 0.12087223
case acc: 0.08762432
case acc: 0.09894074
case acc: 0.13616647
case acc: 0.080562145
top acc: 0.0343 ::: bot acc: 0.0750
top acc: 0.1720 ::: bot acc: 0.0640
top acc: 0.1624 ::: bot acc: 0.0312
top acc: 0.1893 ::: bot acc: 0.0205
top acc: 0.2009 ::: bot acc: 0.0732
top acc: 0.1433 ::: bot acc: 0.0229
current epoch: 12
train loss is 0.005876
average val loss: 0.006395, accuracy: 0.0905
average test loss: 0.006262, accuracy: 0.0927
case acc: 0.0426151
case acc: 0.11303923
case acc: 0.086534835
case acc: 0.09808801
case acc: 0.1348538
case acc: 0.08101741
top acc: 0.0348 ::: bot acc: 0.0743
top acc: 0.1639 ::: bot acc: 0.0563
top acc: 0.1601 ::: bot acc: 0.0322
top acc: 0.1887 ::: bot acc: 0.0191
top acc: 0.1993 ::: bot acc: 0.0721
top acc: 0.1443 ::: bot acc: 0.0232
current epoch: 13
train loss is 0.005715
average val loss: 0.006415, accuracy: 0.0902
average test loss: 0.006101, accuracy: 0.0913
case acc: 0.04213576
case acc: 0.10619221
case acc: 0.08614949
case acc: 0.09869202
case acc: 0.13374439
case acc: 0.08084871
top acc: 0.0361 ::: bot acc: 0.0725
top acc: 0.1569 ::: bot acc: 0.0498
top acc: 0.1591 ::: bot acc: 0.0329
top acc: 0.1888 ::: bot acc: 0.0198
top acc: 0.1980 ::: bot acc: 0.0706
top acc: 0.1439 ::: bot acc: 0.0228
current epoch: 14
train loss is 0.005569
average val loss: 0.006508, accuracy: 0.0908
average test loss: 0.005863, accuracy: 0.0891
case acc: 0.042001642
case acc: 0.098543376
case acc: 0.08509813
case acc: 0.09790063
case acc: 0.13120387
case acc: 0.079908684
top acc: 0.0368 ::: bot acc: 0.0719
top acc: 0.1495 ::: bot acc: 0.0424
top acc: 0.1572 ::: bot acc: 0.0337
top acc: 0.1883 ::: bot acc: 0.0192
top acc: 0.1960 ::: bot acc: 0.0683
top acc: 0.1422 ::: bot acc: 0.0233
current epoch: 15
train loss is 0.005465
average val loss: 0.006047, accuracy: 0.0872
average test loss: 0.006252, accuracy: 0.0921
case acc: 0.039814208
case acc: 0.099547476
case acc: 0.08876238
case acc: 0.10371207
case acc: 0.13648252
case acc: 0.084329255
top acc: 0.0453 ::: bot acc: 0.0633
top acc: 0.1508 ::: bot acc: 0.0423
top acc: 0.1642 ::: bot acc: 0.0311
top acc: 0.1950 ::: bot acc: 0.0228
top acc: 0.2007 ::: bot acc: 0.0738
top acc: 0.1491 ::: bot acc: 0.0222
current epoch: 16
train loss is 0.005284
average val loss: 0.006148, accuracy: 0.0879
average test loss: 0.006014, accuracy: 0.0900
case acc: 0.03957226
case acc: 0.092460126
case acc: 0.0881643
case acc: 0.10278212
case acc: 0.13385871
case acc: 0.0831963
top acc: 0.0451 ::: bot acc: 0.0630
top acc: 0.1437 ::: bot acc: 0.0356
top acc: 0.1625 ::: bot acc: 0.0320
top acc: 0.1933 ::: bot acc: 0.0224
top acc: 0.1979 ::: bot acc: 0.0714
top acc: 0.1469 ::: bot acc: 0.0229
current epoch: 17
train loss is 0.005211
average val loss: 0.006312, accuracy: 0.0891
average test loss: 0.005745, accuracy: 0.0875
case acc: 0.039906677
case acc: 0.08552272
case acc: 0.086684614
case acc: 0.10126356
case acc: 0.12997745
case acc: 0.08189877
top acc: 0.0449 ::: bot acc: 0.0635
top acc: 0.1363 ::: bot acc: 0.0290
top acc: 0.1600 ::: bot acc: 0.0323
top acc: 0.1919 ::: bot acc: 0.0216
top acc: 0.1945 ::: bot acc: 0.0673
top acc: 0.1453 ::: bot acc: 0.0230
current epoch: 18
train loss is 0.005112
average val loss: 0.006298, accuracy: 0.0889
average test loss: 0.005631, accuracy: 0.0863
case acc: 0.039293468
case acc: 0.08032619
case acc: 0.08671231
case acc: 0.10111854
case acc: 0.12891069
case acc: 0.081223845
top acc: 0.0465 ::: bot acc: 0.0613
top acc: 0.1309 ::: bot acc: 0.0238
top acc: 0.1604 ::: bot acc: 0.0328
top acc: 0.1911 ::: bot acc: 0.0216
top acc: 0.1930 ::: bot acc: 0.0663
top acc: 0.1447 ::: bot acc: 0.0226
current epoch: 19
train loss is 0.005054
average val loss: 0.006315, accuracy: 0.0891
average test loss: 0.005547, accuracy: 0.0853
case acc: 0.03906818
case acc: 0.07625075
case acc: 0.08685619
case acc: 0.1013387
case acc: 0.12752555
case acc: 0.080860086
top acc: 0.0483 ::: bot acc: 0.0597
top acc: 0.1265 ::: bot acc: 0.0204
top acc: 0.1606 ::: bot acc: 0.0322
top acc: 0.1915 ::: bot acc: 0.0215
top acc: 0.1921 ::: bot acc: 0.0649
top acc: 0.1439 ::: bot acc: 0.0221
current epoch: 20
train loss is 0.004983
average val loss: 0.006216, accuracy: 0.0883
average test loss: 0.005568, accuracy: 0.0855
case acc: 0.038849976
case acc: 0.07401423
case acc: 0.087931454
case acc: 0.10253621
case acc: 0.12771359
case acc: 0.08173466
top acc: 0.0512 ::: bot acc: 0.0564
top acc: 0.1239 ::: bot acc: 0.0189
top acc: 0.1622 ::: bot acc: 0.0319
top acc: 0.1934 ::: bot acc: 0.0223
top acc: 0.1916 ::: bot acc: 0.0654
top acc: 0.1448 ::: bot acc: 0.0229
current epoch: 21
train loss is 0.004974
average val loss: 0.006003, accuracy: 0.0867
average test loss: 0.005746, accuracy: 0.0869
case acc: 0.038963698
case acc: 0.0736975
case acc: 0.09009915
case acc: 0.10559608
case acc: 0.12971804
case acc: 0.08348817
top acc: 0.0567 ::: bot acc: 0.0516
top acc: 0.1240 ::: bot acc: 0.0185
top acc: 0.1660 ::: bot acc: 0.0311
top acc: 0.1964 ::: bot acc: 0.0244
top acc: 0.1936 ::: bot acc: 0.0677
top acc: 0.1478 ::: bot acc: 0.0226
current epoch: 22
train loss is 0.004882
average val loss: 0.005947, accuracy: 0.0863
average test loss: 0.005757, accuracy: 0.0868
case acc: 0.0391139
case acc: 0.071294196
case acc: 0.09116015
case acc: 0.106422104
case acc: 0.1294963
case acc: 0.08353667
top acc: 0.0596 ::: bot acc: 0.0488
top acc: 0.1210 ::: bot acc: 0.0170
top acc: 0.1678 ::: bot acc: 0.0312
top acc: 0.1978 ::: bot acc: 0.0249
top acc: 0.1936 ::: bot acc: 0.0672
top acc: 0.1478 ::: bot acc: 0.0219
current epoch: 23
train loss is 0.004848
average val loss: 0.006014, accuracy: 0.0868
average test loss: 0.005616, accuracy: 0.0855
case acc: 0.039027326
case acc: 0.067538366
case acc: 0.09090343
case acc: 0.10507043
case acc: 0.12753516
case acc: 0.08304769
top acc: 0.0594 ::: bot acc: 0.0481
top acc: 0.1168 ::: bot acc: 0.0146
top acc: 0.1675 ::: bot acc: 0.0307
top acc: 0.1961 ::: bot acc: 0.0238
top acc: 0.1917 ::: bot acc: 0.0657
top acc: 0.1470 ::: bot acc: 0.0229
current epoch: 24
train loss is 0.004807
average val loss: 0.006241, accuracy: 0.0888
average test loss: 0.005340, accuracy: 0.0829
case acc: 0.039050266
case acc: 0.06275097
case acc: 0.08913481
case acc: 0.10265381
case acc: 0.123488784
case acc: 0.08058401
top acc: 0.0584 ::: bot acc: 0.0498
top acc: 0.1106 ::: bot acc: 0.0127
top acc: 0.1643 ::: bot acc: 0.0320
top acc: 0.1936 ::: bot acc: 0.0222
top acc: 0.1876 ::: bot acc: 0.0612
top acc: 0.1430 ::: bot acc: 0.0228
current epoch: 25
train loss is 0.004781
average val loss: 0.006116, accuracy: 0.0878
average test loss: 0.005425, accuracy: 0.0836
case acc: 0.0391398
case acc: 0.062066603
case acc: 0.09025426
case acc: 0.10423984
case acc: 0.12450534
case acc: 0.081447214
top acc: 0.0614 ::: bot acc: 0.0460
top acc: 0.1097 ::: bot acc: 0.0123
top acc: 0.1665 ::: bot acc: 0.0308
top acc: 0.1951 ::: bot acc: 0.0234
top acc: 0.1888 ::: bot acc: 0.0622
top acc: 0.1444 ::: bot acc: 0.0228
current epoch: 26
train loss is 0.004742
average val loss: 0.006358, accuracy: 0.0901
average test loss: 0.005159, accuracy: 0.0813
case acc: 0.03909124
case acc: 0.05818379
case acc: 0.08898471
case acc: 0.10171752
case acc: 0.120012894
case acc: 0.07956224
top acc: 0.0595 ::: bot acc: 0.0478
top acc: 0.1035 ::: bot acc: 0.0126
top acc: 0.1641 ::: bot acc: 0.0318
top acc: 0.1921 ::: bot acc: 0.0214
top acc: 0.1838 ::: bot acc: 0.0580
top acc: 0.1409 ::: bot acc: 0.0237
current epoch: 27
train loss is 0.004795
average val loss: 0.006075, accuracy: 0.0877
average test loss: 0.005394, accuracy: 0.0834
case acc: 0.040126987
case acc: 0.05914017
case acc: 0.091659024
case acc: 0.104793325
case acc: 0.1232407
case acc: 0.081290476
top acc: 0.0650 ::: bot acc: 0.0430
top acc: 0.1049 ::: bot acc: 0.0129
top acc: 0.1687 ::: bot acc: 0.0310
top acc: 0.1955 ::: bot acc: 0.0239
top acc: 0.1873 ::: bot acc: 0.0613
top acc: 0.1442 ::: bot acc: 0.0227
current epoch: 28
train loss is 0.004708
average val loss: 0.005983, accuracy: 0.0870
average test loss: 0.005454, accuracy: 0.0839
case acc: 0.04037938
case acc: 0.058733437
case acc: 0.092663586
case acc: 0.10600064
case acc: 0.123466834
case acc: 0.082159564
top acc: 0.0674 ::: bot acc: 0.0396
top acc: 0.1044 ::: bot acc: 0.0129
top acc: 0.1702 ::: bot acc: 0.0307
top acc: 0.1967 ::: bot acc: 0.0248
top acc: 0.1872 ::: bot acc: 0.0617
top acc: 0.1458 ::: bot acc: 0.0227
current epoch: 29
train loss is 0.004651
average val loss: 0.006029, accuracy: 0.0874
average test loss: 0.005394, accuracy: 0.0833
case acc: 0.040644385
case acc: 0.056927815
case acc: 0.09276
case acc: 0.1053033
case acc: 0.122505195
case acc: 0.08153327
top acc: 0.0680 ::: bot acc: 0.0391
top acc: 0.1015 ::: bot acc: 0.0129
top acc: 0.1707 ::: bot acc: 0.0303
top acc: 0.1962 ::: bot acc: 0.0242
top acc: 0.1863 ::: bot acc: 0.0606
top acc: 0.1445 ::: bot acc: 0.0228
current epoch: 30
train loss is 0.004681
average val loss: 0.005921, accuracy: 0.0866
average test loss: 0.005468, accuracy: 0.0841
case acc: 0.04151962
case acc: 0.05701642
case acc: 0.09382219
case acc: 0.106588475
case acc: 0.12321893
case acc: 0.08221683
top acc: 0.0707 ::: bot acc: 0.0368
top acc: 0.1012 ::: bot acc: 0.0134
top acc: 0.1722 ::: bot acc: 0.0299
top acc: 0.1974 ::: bot acc: 0.0256
top acc: 0.1869 ::: bot acc: 0.0614
top acc: 0.1456 ::: bot acc: 0.0227
current epoch: 31
train loss is 0.004665
average val loss: 0.005647, accuracy: 0.0842
average test loss: 0.005755, accuracy: 0.0867
case acc: 0.043528643
case acc: 0.058592238
case acc: 0.09686078
case acc: 0.10980839
case acc: 0.12695715
case acc: 0.08441599
top acc: 0.0762 ::: bot acc: 0.0328
top acc: 0.1038 ::: bot acc: 0.0132
top acc: 0.1771 ::: bot acc: 0.0298
top acc: 0.2009 ::: bot acc: 0.0279
top acc: 0.1908 ::: bot acc: 0.0656
top acc: 0.1484 ::: bot acc: 0.0227
current epoch: 32
train loss is 0.004612
average val loss: 0.005567, accuracy: 0.0836
average test loss: 0.005818, accuracy: 0.0873
case acc: 0.04429605
case acc: 0.058335815
case acc: 0.097914495
case acc: 0.11101412
case acc: 0.12739497
case acc: 0.084922105
top acc: 0.0778 ::: bot acc: 0.0317
top acc: 0.1034 ::: bot acc: 0.0133
top acc: 0.1783 ::: bot acc: 0.0300
top acc: 0.2020 ::: bot acc: 0.0290
top acc: 0.1909 ::: bot acc: 0.0659
top acc: 0.1497 ::: bot acc: 0.0226
current epoch: 33
train loss is 0.004613
average val loss: 0.005746, accuracy: 0.0854
average test loss: 0.005586, accuracy: 0.0851
case acc: 0.043323893
case acc: 0.055589985
case acc: 0.09621007
case acc: 0.108311996
case acc: 0.12441797
case acc: 0.08290589
top acc: 0.0757 ::: bot acc: 0.0324
top acc: 0.0990 ::: bot acc: 0.0144
top acc: 0.1758 ::: bot acc: 0.0300
top acc: 0.1994 ::: bot acc: 0.0270
top acc: 0.1879 ::: bot acc: 0.0630
top acc: 0.1469 ::: bot acc: 0.0221
current epoch: 34
train loss is 0.004590
average val loss: 0.006249, accuracy: 0.0900
average test loss: 0.005073, accuracy: 0.0804
case acc: 0.04109116
case acc: 0.051360644
case acc: 0.09190987
case acc: 0.10219348
case acc: 0.1171268
case acc: 0.07888725
top acc: 0.0697 ::: bot acc: 0.0371
top acc: 0.0902 ::: bot acc: 0.0188
top acc: 0.1693 ::: bot acc: 0.0305
top acc: 0.1922 ::: bot acc: 0.0222
top acc: 0.1809 ::: bot acc: 0.0558
top acc: 0.1404 ::: bot acc: 0.0233
current epoch: 35
train loss is 0.004633
average val loss: 0.006282, accuracy: 0.0904
average test loss: 0.005034, accuracy: 0.0800
case acc: 0.0411043
case acc: 0.05079712
case acc: 0.091832206
case acc: 0.10152003
case acc: 0.116543345
case acc: 0.07845266
top acc: 0.0698 ::: bot acc: 0.0371
top acc: 0.0895 ::: bot acc: 0.0199
top acc: 0.1691 ::: bot acc: 0.0308
top acc: 0.1916 ::: bot acc: 0.0214
top acc: 0.1800 ::: bot acc: 0.0552
top acc: 0.1396 ::: bot acc: 0.0232
current epoch: 36
train loss is 0.004615
average val loss: 0.006203, accuracy: 0.0897
average test loss: 0.005116, accuracy: 0.0809
case acc: 0.04191936
case acc: 0.05083499
case acc: 0.09285718
case acc: 0.10287716
case acc: 0.11722849
case acc: 0.07948381
top acc: 0.0720 ::: bot acc: 0.0354
top acc: 0.0893 ::: bot acc: 0.0196
top acc: 0.1712 ::: bot acc: 0.0303
top acc: 0.1928 ::: bot acc: 0.0229
top acc: 0.1805 ::: bot acc: 0.0558
top acc: 0.1411 ::: bot acc: 0.0235
current epoch: 37
train loss is 0.004637
average val loss: 0.006037, accuracy: 0.0881
average test loss: 0.005266, accuracy: 0.0823
case acc: 0.042972576
case acc: 0.051617127
case acc: 0.09461551
case acc: 0.10479535
case acc: 0.118994325
case acc: 0.080577135
top acc: 0.0751 ::: bot acc: 0.0325
top acc: 0.0912 ::: bot acc: 0.0183
top acc: 0.1739 ::: bot acc: 0.0298
top acc: 0.1949 ::: bot acc: 0.0244
top acc: 0.1821 ::: bot acc: 0.0582
top acc: 0.1428 ::: bot acc: 0.0227
current epoch: 38
train loss is 0.004577
average val loss: 0.005965, accuracy: 0.0875
average test loss: 0.005328, accuracy: 0.0828
case acc: 0.04352002
case acc: 0.051813427
case acc: 0.09564005
case acc: 0.10561509
case acc: 0.11919364
case acc: 0.081134245
top acc: 0.0764 ::: bot acc: 0.0314
top acc: 0.0918 ::: bot acc: 0.0177
top acc: 0.1753 ::: bot acc: 0.0300
top acc: 0.1959 ::: bot acc: 0.0250
top acc: 0.1823 ::: bot acc: 0.0582
top acc: 0.1441 ::: bot acc: 0.0226
current epoch: 39
train loss is 0.004599
average val loss: 0.005933, accuracy: 0.0872
average test loss: 0.005340, accuracy: 0.0829
case acc: 0.043800477
case acc: 0.051887713
case acc: 0.09620645
case acc: 0.10568727
case acc: 0.1190074
case acc: 0.080998376
top acc: 0.0772 ::: bot acc: 0.0306
top acc: 0.0917 ::: bot acc: 0.0178
top acc: 0.1762 ::: bot acc: 0.0299
top acc: 0.1959 ::: bot acc: 0.0249
top acc: 0.1820 ::: bot acc: 0.0583
top acc: 0.1440 ::: bot acc: 0.0222
current epoch: 40
train loss is 0.004599
average val loss: 0.005838, accuracy: 0.0864
average test loss: 0.005426, accuracy: 0.0837
case acc: 0.044692148
case acc: 0.05209159
case acc: 0.0972808
case acc: 0.10628428
case acc: 0.120275766
case acc: 0.08178018
top acc: 0.0791 ::: bot acc: 0.0294
top acc: 0.0926 ::: bot acc: 0.0169
top acc: 0.1781 ::: bot acc: 0.0296
top acc: 0.1967 ::: bot acc: 0.0254
top acc: 0.1834 ::: bot acc: 0.0595
top acc: 0.1448 ::: bot acc: 0.0228
current epoch: 41
train loss is 0.004564
average val loss: 0.005631, accuracy: 0.0847
average test loss: 0.005643, accuracy: 0.0858
case acc: 0.046496738
case acc: 0.053200703
case acc: 0.09975097
case acc: 0.10873857
case acc: 0.12315813
case acc: 0.08329469
top acc: 0.0824 ::: bot acc: 0.0280
top acc: 0.0950 ::: bot acc: 0.0153
top acc: 0.1812 ::: bot acc: 0.0305
top acc: 0.1993 ::: bot acc: 0.0275
top acc: 0.1863 ::: bot acc: 0.0623
top acc: 0.1474 ::: bot acc: 0.0217
current epoch: 42
train loss is 0.004520
average val loss: 0.005707, accuracy: 0.0853
average test loss: 0.005535, accuracy: 0.0848
case acc: 0.046083007
case acc: 0.052467685
case acc: 0.098856755
case acc: 0.10719178
case acc: 0.12205716
case acc: 0.08225795
top acc: 0.0814 ::: bot acc: 0.0285
top acc: 0.0934 ::: bot acc: 0.0166
top acc: 0.1800 ::: bot acc: 0.0302
top acc: 0.1974 ::: bot acc: 0.0264
top acc: 0.1851 ::: bot acc: 0.0613
top acc: 0.1457 ::: bot acc: 0.0224
current epoch: 43
train loss is 0.004558
average val loss: 0.005475, accuracy: 0.0832
average test loss: 0.005774, accuracy: 0.0871
case acc: 0.048047192
case acc: 0.05389041
case acc: 0.10141973
case acc: 0.10973007
case acc: 0.12516882
case acc: 0.08419132
top acc: 0.0850 ::: bot acc: 0.0270
top acc: 0.0963 ::: bot acc: 0.0150
top acc: 0.1836 ::: bot acc: 0.0306
top acc: 0.2003 ::: bot acc: 0.0283
top acc: 0.1882 ::: bot acc: 0.0644
top acc: 0.1484 ::: bot acc: 0.0224
current epoch: 44
train loss is 0.004525
average val loss: 0.005165, accuracy: 0.0804
average test loss: 0.006188, accuracy: 0.0909
case acc: 0.051523734
case acc: 0.056563612
case acc: 0.105321944
case acc: 0.11427394
case acc: 0.13025776
case acc: 0.0876138
top acc: 0.0908 ::: bot acc: 0.0261
top acc: 0.1012 ::: bot acc: 0.0135
top acc: 0.1891 ::: bot acc: 0.0313
top acc: 0.2048 ::: bot acc: 0.0326
top acc: 0.1931 ::: bot acc: 0.0693
top acc: 0.1533 ::: bot acc: 0.0229
current epoch: 45
train loss is 0.004502
average val loss: 0.005075, accuracy: 0.0796
average test loss: 0.006314, accuracy: 0.0921
case acc: 0.052349847
case acc: 0.05743882
case acc: 0.106487975
case acc: 0.115555085
case acc: 0.13191032
case acc: 0.08857885
top acc: 0.0921 ::: bot acc: 0.0258
top acc: 0.1025 ::: bot acc: 0.0132
top acc: 0.1908 ::: bot acc: 0.0316
top acc: 0.2065 ::: bot acc: 0.0337
top acc: 0.1948 ::: bot acc: 0.0714
top acc: 0.1547 ::: bot acc: 0.0234
current epoch: 46
train loss is 0.004486
average val loss: 0.005279, accuracy: 0.0816
average test loss: 0.005982, accuracy: 0.0890
case acc: 0.04989531
case acc: 0.054638643
case acc: 0.10373406
case acc: 0.11151345
case acc: 0.12816696
case acc: 0.085839845
top acc: 0.0883 ::: bot acc: 0.0262
top acc: 0.0978 ::: bot acc: 0.0142
top acc: 0.1873 ::: bot acc: 0.0306
top acc: 0.2020 ::: bot acc: 0.0300
top acc: 0.1909 ::: bot acc: 0.0678
top acc: 0.1509 ::: bot acc: 0.0224
current epoch: 47
train loss is 0.004498
average val loss: 0.005551, accuracy: 0.0842
average test loss: 0.005650, accuracy: 0.0859
case acc: 0.047564607
case acc: 0.052424874
case acc: 0.10074322
case acc: 0.107346125
case acc: 0.1242605
case acc: 0.08311784
top acc: 0.0842 ::: bot acc: 0.0273
top acc: 0.0931 ::: bot acc: 0.0169
top acc: 0.1829 ::: bot acc: 0.0304
top acc: 0.1976 ::: bot acc: 0.0262
top acc: 0.1873 ::: bot acc: 0.0634
top acc: 0.1468 ::: bot acc: 0.0224
current epoch: 48
train loss is 0.004499
average val loss: 0.005621, accuracy: 0.0849
average test loss: 0.005570, accuracy: 0.0852
case acc: 0.04715178
case acc: 0.051613364
case acc: 0.10021788
case acc: 0.10621765
case acc: 0.12347765
case acc: 0.08252124
top acc: 0.0835 ::: bot acc: 0.0272
top acc: 0.0915 ::: bot acc: 0.0179
top acc: 0.1819 ::: bot acc: 0.0301
top acc: 0.1965 ::: bot acc: 0.0252
top acc: 0.1859 ::: bot acc: 0.0631
top acc: 0.1457 ::: bot acc: 0.0227
current epoch: 49
train loss is 0.004529
average val loss: 0.005468, accuracy: 0.0834
average test loss: 0.005749, accuracy: 0.0868
case acc: 0.04839618
case acc: 0.052525762
case acc: 0.10232652
case acc: 0.10834856
case acc: 0.12512478
case acc: 0.08418842
top acc: 0.0861 ::: bot acc: 0.0262
top acc: 0.0938 ::: bot acc: 0.0156
top acc: 0.1853 ::: bot acc: 0.0308
top acc: 0.1988 ::: bot acc: 0.0271
top acc: 0.1876 ::: bot acc: 0.0648
top acc: 0.1480 ::: bot acc: 0.0228
current epoch: 50
train loss is 0.004527
average val loss: 0.005383, accuracy: 0.0826
average test loss: 0.005849, accuracy: 0.0878
case acc: 0.049489714
case acc: 0.053281855
case acc: 0.10347482
case acc: 0.109394506
case acc: 0.1260482
case acc: 0.0850261
top acc: 0.0876 ::: bot acc: 0.0261
top acc: 0.0955 ::: bot acc: 0.0151
top acc: 0.1865 ::: bot acc: 0.0310
top acc: 0.2001 ::: bot acc: 0.0279
top acc: 0.1883 ::: bot acc: 0.0659
top acc: 0.1493 ::: bot acc: 0.0228
LME_Co_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5130 5130 5130
1.7082474 -0.6288155 0.48738334 -0.27422604
Validation: 570 570 570
Testing: 768 768 768
pre-processing time: 0.00020170211791992188
the split date is 2012-07-01
train dropout: 0.4 test dropout: 0.15
net initializing with time: 0.002137899398803711
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012664
average val loss: 0.005355, accuracy: 0.0875
average test loss: 0.006376, accuracy: 0.0959
case acc: 0.09735703
case acc: 0.1495457
case acc: 0.11195125
case acc: 0.08028037
case acc: 0.05908385
case acc: 0.0774079
top acc: 0.0756 ::: bot acc: 0.1610
top acc: 0.1946 ::: bot acc: 0.1081
top acc: 0.0419 ::: bot acc: 0.1723
top acc: 0.0365 ::: bot acc: 0.1708
top acc: 0.0566 ::: bot acc: 0.0997
top acc: 0.1194 ::: bot acc: 0.0603
current epoch: 2
train loss is 0.012758
average val loss: 0.011807, accuracy: 0.1258
average test loss: 0.014373, accuracy: 0.1401
case acc: 0.111285314
case acc: 0.28106195
case acc: 0.06273685
case acc: 0.10478052
case acc: 0.114513054
case acc: 0.16650805
top acc: 0.2187 ::: bot acc: 0.0269
top acc: 0.3270 ::: bot acc: 0.2387
top acc: 0.1385 ::: bot acc: 0.0313
top acc: 0.1708 ::: bot acc: 0.0419
top acc: 0.1919 ::: bot acc: 0.0388
top acc: 0.2464 ::: bot acc: 0.0721
current epoch: 3
train loss is 0.012368
average val loss: 0.022329, accuracy: 0.1899
average test loss: 0.025574, accuracy: 0.2008
case acc: 0.17488381
case acc: 0.34704232
case acc: 0.119943745
case acc: 0.14857537
case acc: 0.18584415
case acc: 0.22876664
top acc: 0.2965 ::: bot acc: 0.0603
top acc: 0.3929 ::: bot acc: 0.3058
top acc: 0.2173 ::: bot acc: 0.0463
top acc: 0.2393 ::: bot acc: 0.0353
top acc: 0.2635 ::: bot acc: 0.1081
top acc: 0.3087 ::: bot acc: 0.1331
current epoch: 4
train loss is 0.012900
average val loss: 0.009977, accuracy: 0.1148
average test loss: 0.012185, accuracy: 0.1291
case acc: 0.111013785
case acc: 0.2539116
case acc: 0.0618485
case acc: 0.09687117
case acc: 0.11004062
case acc: 0.14113115
top acc: 0.2187 ::: bot acc: 0.0264
top acc: 0.2999 ::: bot acc: 0.2120
top acc: 0.1336 ::: bot acc: 0.0365
top acc: 0.1539 ::: bot acc: 0.0529
top acc: 0.1866 ::: bot acc: 0.0348
top acc: 0.2191 ::: bot acc: 0.0498
current epoch: 5
train loss is 0.008935
average val loss: 0.006456, accuracy: 0.0910
average test loss: 0.008210, accuracy: 0.1084
case acc: 0.09752458
case acc: 0.20471868
case acc: 0.064688146
case acc: 0.08545183
case acc: 0.0874847
case acc: 0.11040053
top acc: 0.1842 ::: bot acc: 0.0524
top acc: 0.2504 ::: bot acc: 0.1636
top acc: 0.0947 ::: bot acc: 0.0758
top acc: 0.1127 ::: bot acc: 0.0942
top acc: 0.1567 ::: bot acc: 0.0280
top acc: 0.1769 ::: bot acc: 0.0429
current epoch: 6
train loss is 0.007286
average val loss: 0.008690, accuracy: 0.1070
average test loss: 0.010634, accuracy: 0.1220
case acc: 0.11102139
case acc: 0.22465317
case acc: 0.059761368
case acc: 0.0926237
case acc: 0.11611045
case acc: 0.1277564
top acc: 0.2180 ::: bot acc: 0.0274
top acc: 0.2705 ::: bot acc: 0.1826
top acc: 0.1246 ::: bot acc: 0.0456
top acc: 0.1407 ::: bot acc: 0.0654
top acc: 0.1937 ::: bot acc: 0.0397
top acc: 0.2023 ::: bot acc: 0.0424
current epoch: 7
train loss is 0.007472
average val loss: 0.009891, accuracy: 0.1162
average test loss: 0.011990, accuracy: 0.1305
case acc: 0.12242189
case acc: 0.23100325
case acc: 0.062575996
case acc: 0.0967943
case acc: 0.13391641
case acc: 0.1364294
top acc: 0.2365 ::: bot acc: 0.0254
top acc: 0.2768 ::: bot acc: 0.1892
top acc: 0.1378 ::: bot acc: 0.0331
top acc: 0.1530 ::: bot acc: 0.0539
top acc: 0.2121 ::: bot acc: 0.0558
top acc: 0.2140 ::: bot acc: 0.0465
current epoch: 8
train loss is 0.007266
average val loss: 0.008401, accuracy: 0.1060
average test loss: 0.010310, accuracy: 0.1211
case acc: 0.11646794
case acc: 0.20958754
case acc: 0.06014828
case acc: 0.091921136
case acc: 0.12512472
case acc: 0.12354138
top acc: 0.2274 ::: bot acc: 0.0252
top acc: 0.2551 ::: bot acc: 0.1682
top acc: 0.1259 ::: bot acc: 0.0452
top acc: 0.1383 ::: bot acc: 0.0677
top acc: 0.2031 ::: bot acc: 0.0475
top acc: 0.1964 ::: bot acc: 0.0415
current epoch: 9
train loss is 0.006666
average val loss: 0.008848, accuracy: 0.1095
average test loss: 0.010760, accuracy: 0.1242
case acc: 0.12244411
case acc: 0.2075365
case acc: 0.060575113
case acc: 0.09393818
case acc: 0.13460726
case acc: 0.12632035
top acc: 0.2364 ::: bot acc: 0.0242
top acc: 0.2531 ::: bot acc: 0.1655
top acc: 0.1313 ::: bot acc: 0.0383
top acc: 0.1442 ::: bot acc: 0.0625
top acc: 0.2127 ::: bot acc: 0.0574
top acc: 0.2003 ::: bot acc: 0.0431
current epoch: 10
train loss is 0.006560
average val loss: 0.008253, accuracy: 0.1055
average test loss: 0.010098, accuracy: 0.1205
case acc: 0.12171783
case acc: 0.19533925
case acc: 0.05976972
case acc: 0.092589445
case acc: 0.13212833
case acc: 0.12137745
top acc: 0.2358 ::: bot acc: 0.0239
top acc: 0.2412 ::: bot acc: 0.1535
top acc: 0.1273 ::: bot acc: 0.0424
top acc: 0.1388 ::: bot acc: 0.0681
top acc: 0.2103 ::: bot acc: 0.0540
top acc: 0.1935 ::: bot acc: 0.0420
current epoch: 11
train loss is 0.006206
average val loss: 0.008808, accuracy: 0.1099
average test loss: 0.010701, accuracy: 0.1245
case acc: 0.1287585
case acc: 0.196228
case acc: 0.061544538
case acc: 0.09445895
case acc: 0.14101595
case acc: 0.124763496
top acc: 0.2454 ::: bot acc: 0.0258
top acc: 0.2417 ::: bot acc: 0.1545
top acc: 0.1350 ::: bot acc: 0.0346
top acc: 0.1464 ::: bot acc: 0.0610
top acc: 0.2194 ::: bot acc: 0.0633
top acc: 0.1986 ::: bot acc: 0.0424
current epoch: 12
train loss is 0.006116
average val loss: 0.009078, accuracy: 0.1123
average test loss: 0.010948, accuracy: 0.1263
case acc: 0.13391688
case acc: 0.19369286
case acc: 0.0635933
case acc: 0.09485446
case acc: 0.14571442
case acc: 0.12592448
top acc: 0.2515 ::: bot acc: 0.0283
top acc: 0.2396 ::: bot acc: 0.1521
top acc: 0.1400 ::: bot acc: 0.0314
top acc: 0.1485 ::: bot acc: 0.0571
top acc: 0.2239 ::: bot acc: 0.0681
top acc: 0.1996 ::: bot acc: 0.0426
current epoch: 13
train loss is 0.006019
average val loss: 0.008720, accuracy: 0.1098
average test loss: 0.010553, accuracy: 0.1240
case acc: 0.13444558
case acc: 0.18546098
case acc: 0.062841274
case acc: 0.094699696
case acc: 0.14393562
case acc: 0.122439854
top acc: 0.2525 ::: bot acc: 0.0280
top acc: 0.2309 ::: bot acc: 0.1435
top acc: 0.1386 ::: bot acc: 0.0322
top acc: 0.1465 ::: bot acc: 0.0603
top acc: 0.2220 ::: bot acc: 0.0660
top acc: 0.1952 ::: bot acc: 0.0418
current epoch: 14
train loss is 0.005895
average val loss: 0.008196, accuracy: 0.1062
average test loss: 0.009965, accuracy: 0.1204
case acc: 0.13294733
case acc: 0.17567964
case acc: 0.061578736
case acc: 0.093466714
case acc: 0.14011726
case acc: 0.11837223
top acc: 0.2506 ::: bot acc: 0.0272
top acc: 0.2212 ::: bot acc: 0.1343
top acc: 0.1356 ::: bot acc: 0.0343
top acc: 0.1428 ::: bot acc: 0.0640
top acc: 0.2182 ::: bot acc: 0.0623
top acc: 0.1890 ::: bot acc: 0.0417
current epoch: 15
train loss is 0.005629
average val loss: 0.008812, accuracy: 0.1110
average test loss: 0.010602, accuracy: 0.1245
case acc: 0.1402728
case acc: 0.17707984
case acc: 0.065049656
case acc: 0.09517772
case acc: 0.14744124
case acc: 0.121906266
top acc: 0.2598 ::: bot acc: 0.0315
top acc: 0.2231 ::: bot acc: 0.1355
top acc: 0.1447 ::: bot acc: 0.0277
top acc: 0.1488 ::: bot acc: 0.0565
top acc: 0.2259 ::: bot acc: 0.0697
top acc: 0.1951 ::: bot acc: 0.0414
current epoch: 16
train loss is 0.005636
average val loss: 0.008878, accuracy: 0.1117
average test loss: 0.010692, accuracy: 0.1252
case acc: 0.14387973
case acc: 0.17395112
case acc: 0.06643841
case acc: 0.096132316
case acc: 0.14907207
case acc: 0.1218629
top acc: 0.2638 ::: bot acc: 0.0337
top acc: 0.2200 ::: bot acc: 0.1320
top acc: 0.1478 ::: bot acc: 0.0257
top acc: 0.1509 ::: bot acc: 0.0556
top acc: 0.2274 ::: bot acc: 0.0711
top acc: 0.1948 ::: bot acc: 0.0415
current epoch: 17
train loss is 0.005508
average val loss: 0.009126, accuracy: 0.1137
average test loss: 0.010972, accuracy: 0.1271
case acc: 0.14819211
case acc: 0.17325044
case acc: 0.0681362
case acc: 0.09727272
case acc: 0.15195519
case acc: 0.12382181
top acc: 0.2688 ::: bot acc: 0.0370
top acc: 0.2189 ::: bot acc: 0.1316
top acc: 0.1510 ::: bot acc: 0.0226
top acc: 0.1545 ::: bot acc: 0.0514
top acc: 0.2298 ::: bot acc: 0.0739
top acc: 0.1971 ::: bot acc: 0.0423
current epoch: 18
train loss is 0.005488
average val loss: 0.008461, accuracy: 0.1088
average test loss: 0.010177, accuracy: 0.1220
case acc: 0.14475498
case acc: 0.16215192
case acc: 0.06592575
case acc: 0.09539665
case acc: 0.1454357
case acc: 0.118424535
top acc: 0.2649 ::: bot acc: 0.0343
top acc: 0.2080 ::: bot acc: 0.1203
top acc: 0.1464 ::: bot acc: 0.0264
top acc: 0.1488 ::: bot acc: 0.0570
top acc: 0.2240 ::: bot acc: 0.0672
top acc: 0.1895 ::: bot acc: 0.0414
current epoch: 19
train loss is 0.005303
average val loss: 0.008782, accuracy: 0.1115
average test loss: 0.010546, accuracy: 0.1245
case acc: 0.14937349
case acc: 0.16227102
case acc: 0.068732634
case acc: 0.096827
case acc: 0.14951086
case acc: 0.12024154
top acc: 0.2702 ::: bot acc: 0.0375
top acc: 0.2080 ::: bot acc: 0.1206
top acc: 0.1521 ::: bot acc: 0.0233
top acc: 0.1529 ::: bot acc: 0.0531
top acc: 0.2278 ::: bot acc: 0.0712
top acc: 0.1923 ::: bot acc: 0.0409
current epoch: 20
train loss is 0.005311
average val loss: 0.009110, accuracy: 0.1140
average test loss: 0.010876, accuracy: 0.1267
case acc: 0.15406738
case acc: 0.16232577
case acc: 0.07114768
case acc: 0.09813133
case acc: 0.1521331
case acc: 0.12232477
top acc: 0.2753 ::: bot acc: 0.0412
top acc: 0.2081 ::: bot acc: 0.1206
top acc: 0.1566 ::: bot acc: 0.0211
top acc: 0.1571 ::: bot acc: 0.0481
top acc: 0.2304 ::: bot acc: 0.0740
top acc: 0.1951 ::: bot acc: 0.0419
current epoch: 21
train loss is 0.005294
average val loss: 0.008547, accuracy: 0.1099
average test loss: 0.010272, accuracy: 0.1228
case acc: 0.15109536
case acc: 0.15422142
case acc: 0.069201544
case acc: 0.09681367
case acc: 0.14690295
case acc: 0.11868662
top acc: 0.2717 ::: bot acc: 0.0388
top acc: 0.1999 ::: bot acc: 0.1120
top acc: 0.1527 ::: bot acc: 0.0226
top acc: 0.1535 ::: bot acc: 0.0523
top acc: 0.2247 ::: bot acc: 0.0694
top acc: 0.1898 ::: bot acc: 0.0416
current epoch: 22
train loss is 0.005132
average val loss: 0.008454, accuracy: 0.1094
average test loss: 0.010150, accuracy: 0.1219
case acc: 0.15155609
case acc: 0.14998293
case acc: 0.069748595
case acc: 0.0963753
case acc: 0.14597285
case acc: 0.11779344
top acc: 0.2721 ::: bot acc: 0.0390
top acc: 0.1956 ::: bot acc: 0.1083
top acc: 0.1542 ::: bot acc: 0.0223
top acc: 0.1530 ::: bot acc: 0.0515
top acc: 0.2244 ::: bot acc: 0.0679
top acc: 0.1888 ::: bot acc: 0.0414
current epoch: 23
train loss is 0.005031
average val loss: 0.008512, accuracy: 0.1097
average test loss: 0.010179, accuracy: 0.1222
case acc: 0.15360019
case acc: 0.14747491
case acc: 0.07116521
case acc: 0.09682277
case acc: 0.14626014
case acc: 0.11804118
top acc: 0.2744 ::: bot acc: 0.0410
top acc: 0.1933 ::: bot acc: 0.1055
top acc: 0.1558 ::: bot acc: 0.0223
top acc: 0.1537 ::: bot acc: 0.0512
top acc: 0.2245 ::: bot acc: 0.0685
top acc: 0.1893 ::: bot acc: 0.0413
current epoch: 24
train loss is 0.005020
average val loss: 0.008756, accuracy: 0.1117
average test loss: 0.010467, accuracy: 0.1241
case acc: 0.15711935
case acc: 0.14803216
case acc: 0.072971016
case acc: 0.09861477
case acc: 0.14860225
case acc: 0.11948283
top acc: 0.2784 ::: bot acc: 0.0438
top acc: 0.1938 ::: bot acc: 0.1063
top acc: 0.1592 ::: bot acc: 0.0210
top acc: 0.1579 ::: bot acc: 0.0481
top acc: 0.2270 ::: bot acc: 0.0705
top acc: 0.1912 ::: bot acc: 0.0417
current epoch: 25
train loss is 0.005044
average val loss: 0.008788, accuracy: 0.1120
average test loss: 0.010478, accuracy: 0.1242
case acc: 0.15836583
case acc: 0.14620966
case acc: 0.07412949
case acc: 0.09876029
case acc: 0.14866692
case acc: 0.119221896
top acc: 0.2796 ::: bot acc: 0.0450
top acc: 0.1916 ::: bot acc: 0.1046
top acc: 0.1613 ::: bot acc: 0.0207
top acc: 0.1586 ::: bot acc: 0.0468
top acc: 0.2268 ::: bot acc: 0.0707
top acc: 0.1908 ::: bot acc: 0.0411
current epoch: 26
train loss is 0.005021
average val loss: 0.008563, accuracy: 0.1104
average test loss: 0.010225, accuracy: 0.1225
case acc: 0.15737735
case acc: 0.14171836
case acc: 0.07368909
case acc: 0.09802841
case acc: 0.14669682
case acc: 0.117431
top acc: 0.2786 ::: bot acc: 0.0437
top acc: 0.1872 ::: bot acc: 0.1003
top acc: 0.1605 ::: bot acc: 0.0208
top acc: 0.1569 ::: bot acc: 0.0484
top acc: 0.2250 ::: bot acc: 0.0687
top acc: 0.1880 ::: bot acc: 0.0414
current epoch: 27
train loss is 0.004920
average val loss: 0.008639, accuracy: 0.1108
average test loss: 0.010250, accuracy: 0.1227
case acc: 0.15849762
case acc: 0.1399312
case acc: 0.07468011
case acc: 0.09842765
case acc: 0.14730516
case acc: 0.117181875
top acc: 0.2800 ::: bot acc: 0.0448
top acc: 0.1851 ::: bot acc: 0.0984
top acc: 0.1623 ::: bot acc: 0.0202
top acc: 0.1576 ::: bot acc: 0.0477
top acc: 0.2259 ::: bot acc: 0.0691
top acc: 0.1876 ::: bot acc: 0.0413
current epoch: 28
train loss is 0.004943
average val loss: 0.009089, accuracy: 0.1142
average test loss: 0.010746, accuracy: 0.1261
case acc: 0.16380164
case acc: 0.14295134
case acc: 0.07834235
case acc: 0.10044722
case acc: 0.15162143
case acc: 0.11939495
top acc: 0.2851 ::: bot acc: 0.0499
top acc: 0.1885 ::: bot acc: 0.1012
top acc: 0.1675 ::: bot acc: 0.0206
top acc: 0.1626 ::: bot acc: 0.0440
top acc: 0.2297 ::: bot acc: 0.0738
top acc: 0.1914 ::: bot acc: 0.0408
current epoch: 29
train loss is 0.004981
average val loss: 0.008748, accuracy: 0.1117
average test loss: 0.010356, accuracy: 0.1234
case acc: 0.16131963
case acc: 0.13797551
case acc: 0.07692088
case acc: 0.09889549
case acc: 0.14837429
case acc: 0.11687054
top acc: 0.2827 ::: bot acc: 0.0477
top acc: 0.1832 ::: bot acc: 0.0962
top acc: 0.1659 ::: bot acc: 0.0201
top acc: 0.1591 ::: bot acc: 0.0457
top acc: 0.2264 ::: bot acc: 0.0708
top acc: 0.1877 ::: bot acc: 0.0408
current epoch: 30
train loss is 0.004864
average val loss: 0.008834, accuracy: 0.1123
average test loss: 0.010430, accuracy: 0.1239
case acc: 0.16252834
case acc: 0.13705549
case acc: 0.07801915
case acc: 0.0996137
case acc: 0.14903772
case acc: 0.11719534
top acc: 0.2843 ::: bot acc: 0.0486
top acc: 0.1826 ::: bot acc: 0.0954
top acc: 0.1673 ::: bot acc: 0.0204
top acc: 0.1606 ::: bot acc: 0.0450
top acc: 0.2275 ::: bot acc: 0.0712
top acc: 0.1880 ::: bot acc: 0.0412
current epoch: 31
train loss is 0.004899
average val loss: 0.008404, accuracy: 0.1090
average test loss: 0.009957, accuracy: 0.1206
case acc: 0.15905501
case acc: 0.13084346
case acc: 0.07586784
case acc: 0.09834051
case acc: 0.14485262
case acc: 0.11466375
top acc: 0.2807 ::: bot acc: 0.0451
top acc: 0.1764 ::: bot acc: 0.0893
top acc: 0.1643 ::: bot acc: 0.0202
top acc: 0.1574 ::: bot acc: 0.0478
top acc: 0.2227 ::: bot acc: 0.0672
top acc: 0.1840 ::: bot acc: 0.0418
current epoch: 32
train loss is 0.004753
average val loss: 0.008137, accuracy: 0.1070
average test loss: 0.009648, accuracy: 0.1184
case acc: 0.15658194
case acc: 0.12679113
case acc: 0.07434603
case acc: 0.09767258
case acc: 0.142093
case acc: 0.11264131
top acc: 0.2782 ::: bot acc: 0.0429
top acc: 0.1723 ::: bot acc: 0.0852
top acc: 0.1623 ::: bot acc: 0.0200
top acc: 0.1555 ::: bot acc: 0.0497
top acc: 0.2204 ::: bot acc: 0.0643
top acc: 0.1812 ::: bot acc: 0.0410
current epoch: 33
train loss is 0.004739
average val loss: 0.008888, accuracy: 0.1127
average test loss: 0.010468, accuracy: 0.1241
case acc: 0.16424662
case acc: 0.13306248
case acc: 0.08021431
case acc: 0.10092779
case acc: 0.14925653
case acc: 0.11690423
top acc: 0.2866 ::: bot acc: 0.0499
top acc: 0.1785 ::: bot acc: 0.0913
top acc: 0.1711 ::: bot acc: 0.0206
top acc: 0.1627 ::: bot acc: 0.0440
top acc: 0.2276 ::: bot acc: 0.0712
top acc: 0.1876 ::: bot acc: 0.0407
current epoch: 34
train loss is 0.004818
average val loss: 0.009341, accuracy: 0.1160
average test loss: 0.010972, accuracy: 0.1276
case acc: 0.16871446
case acc: 0.13629599
case acc: 0.08429113
case acc: 0.10299677
case acc: 0.1532633
case acc: 0.12015221
top acc: 0.2911 ::: bot acc: 0.0540
top acc: 0.1818 ::: bot acc: 0.0947
top acc: 0.1766 ::: bot acc: 0.0216
top acc: 0.1679 ::: bot acc: 0.0407
top acc: 0.2316 ::: bot acc: 0.0752
top acc: 0.1921 ::: bot acc: 0.0417
current epoch: 35
train loss is 0.004924
average val loss: 0.008659, accuracy: 0.1109
average test loss: 0.010155, accuracy: 0.1219
case acc: 0.16190946
case acc: 0.12776361
case acc: 0.07939787
case acc: 0.099964954
case acc: 0.14663962
case acc: 0.11557609
top acc: 0.2839 ::: bot acc: 0.0476
top acc: 0.1730 ::: bot acc: 0.0861
top acc: 0.1701 ::: bot acc: 0.0199
top acc: 0.1614 ::: bot acc: 0.0441
top acc: 0.2250 ::: bot acc: 0.0687
top acc: 0.1858 ::: bot acc: 0.0408
current epoch: 36
train loss is 0.004753
average val loss: 0.008148, accuracy: 0.1068
average test loss: 0.009578, accuracy: 0.1178
case acc: 0.15725261
case acc: 0.120724246
case acc: 0.07593103
case acc: 0.098196596
case acc: 0.14212234
case acc: 0.11238475
top acc: 0.2786 ::: bot acc: 0.0438
top acc: 0.1664 ::: bot acc: 0.0790
top acc: 0.1649 ::: bot acc: 0.0202
top acc: 0.1569 ::: bot acc: 0.0478
top acc: 0.2202 ::: bot acc: 0.0644
top acc: 0.1813 ::: bot acc: 0.0412
current epoch: 37
train loss is 0.004642
average val loss: 0.008429, accuracy: 0.1089
average test loss: 0.009909, accuracy: 0.1201
case acc: 0.16020215
case acc: 0.12280919
case acc: 0.0783999
case acc: 0.09954767
case acc: 0.14499357
case acc: 0.11452888
top acc: 0.2820 ::: bot acc: 0.0460
top acc: 0.1679 ::: bot acc: 0.0816
top acc: 0.1684 ::: bot acc: 0.0199
top acc: 0.1603 ::: bot acc: 0.0449
top acc: 0.2236 ::: bot acc: 0.0669
top acc: 0.1843 ::: bot acc: 0.0407
current epoch: 38
train loss is 0.004688
average val loss: 0.008224, accuracy: 0.1073
average test loss: 0.009669, accuracy: 0.1183
case acc: 0.15805295
case acc: 0.11989714
case acc: 0.07722868
case acc: 0.09895107
case acc: 0.14240906
case acc: 0.11317708
top acc: 0.2799 ::: bot acc: 0.0440
top acc: 0.1655 ::: bot acc: 0.0782
top acc: 0.1674 ::: bot acc: 0.0198
top acc: 0.1586 ::: bot acc: 0.0459
top acc: 0.2210 ::: bot acc: 0.0645
top acc: 0.1823 ::: bot acc: 0.0412
current epoch: 39
train loss is 0.004668
average val loss: 0.008517, accuracy: 0.1096
average test loss: 0.009982, accuracy: 0.1206
case acc: 0.16103941
case acc: 0.1220451
case acc: 0.079826534
case acc: 0.100518346
case acc: 0.14522639
case acc: 0.11473347
top acc: 0.2833 ::: bot acc: 0.0468
top acc: 0.1674 ::: bot acc: 0.0801
top acc: 0.1706 ::: bot acc: 0.0203
top acc: 0.1622 ::: bot acc: 0.0439
top acc: 0.2238 ::: bot acc: 0.0673
top acc: 0.1845 ::: bot acc: 0.0409
current epoch: 40
train loss is 0.004706
average val loss: 0.008679, accuracy: 0.1108
average test loss: 0.010149, accuracy: 0.1218
case acc: 0.16257851
case acc: 0.123048894
case acc: 0.08148797
case acc: 0.10109333
case acc: 0.14684594
case acc: 0.11559649
top acc: 0.2845 ::: bot acc: 0.0484
top acc: 0.1685 ::: bot acc: 0.0812
top acc: 0.1732 ::: bot acc: 0.0206
top acc: 0.1634 ::: bot acc: 0.0427
top acc: 0.2254 ::: bot acc: 0.0687
top acc: 0.1860 ::: bot acc: 0.0407
current epoch: 41
train loss is 0.004713
average val loss: 0.008808, accuracy: 0.1118
average test loss: 0.010274, accuracy: 0.1227
case acc: 0.16334696
case acc: 0.123676255
case acc: 0.08283743
case acc: 0.10191661
case acc: 0.14847799
case acc: 0.11616332
top acc: 0.2852 ::: bot acc: 0.0492
top acc: 0.1690 ::: bot acc: 0.0822
top acc: 0.1746 ::: bot acc: 0.0213
top acc: 0.1652 ::: bot acc: 0.0415
top acc: 0.2269 ::: bot acc: 0.0707
top acc: 0.1866 ::: bot acc: 0.0410
current epoch: 42
train loss is 0.004717
average val loss: 0.008533, accuracy: 0.1096
average test loss: 0.009938, accuracy: 0.1203
case acc: 0.16062845
case acc: 0.11952047
case acc: 0.08094977
case acc: 0.10062152
case acc: 0.14585678
case acc: 0.114208244
top acc: 0.2823 ::: bot acc: 0.0468
top acc: 0.1649 ::: bot acc: 0.0777
top acc: 0.1723 ::: bot acc: 0.0207
top acc: 0.1623 ::: bot acc: 0.0433
top acc: 0.2241 ::: bot acc: 0.0679
top acc: 0.1840 ::: bot acc: 0.0409
current epoch: 43
train loss is 0.004641
average val loss: 0.007824, accuracy: 0.1040
average test loss: 0.009139, accuracy: 0.1145
case acc: 0.15371606
case acc: 0.111121684
case acc: 0.075799674
case acc: 0.09745443
case acc: 0.1387844
case acc: 0.109843
top acc: 0.2750 ::: bot acc: 0.0408
top acc: 0.1561 ::: bot acc: 0.0697
top acc: 0.1646 ::: bot acc: 0.0205
top acc: 0.1555 ::: bot acc: 0.0478
top acc: 0.2172 ::: bot acc: 0.0610
top acc: 0.1771 ::: bot acc: 0.0413
current epoch: 44
train loss is 0.004540
average val loss: 0.008238, accuracy: 0.1072
average test loss: 0.009600, accuracy: 0.1178
case acc: 0.15755367
case acc: 0.11467231
case acc: 0.079036996
case acc: 0.099638246
case acc: 0.1433633
case acc: 0.11259967
top acc: 0.2789 ::: bot acc: 0.0440
top acc: 0.1598 ::: bot acc: 0.0734
top acc: 0.1694 ::: bot acc: 0.0203
top acc: 0.1600 ::: bot acc: 0.0446
top acc: 0.2217 ::: bot acc: 0.0653
top acc: 0.1817 ::: bot acc: 0.0407
current epoch: 45
train loss is 0.004585
average val loss: 0.008531, accuracy: 0.1095
average test loss: 0.009925, accuracy: 0.1202
case acc: 0.1600813
case acc: 0.117732845
case acc: 0.08140491
case acc: 0.1012301
case acc: 0.14610982
case acc: 0.11434774
top acc: 0.2818 ::: bot acc: 0.0463
top acc: 0.1632 ::: bot acc: 0.0760
top acc: 0.1728 ::: bot acc: 0.0206
top acc: 0.1633 ::: bot acc: 0.0425
top acc: 0.2247 ::: bot acc: 0.0681
top acc: 0.1845 ::: bot acc: 0.0406
current epoch: 46
train loss is 0.004610
average val loss: 0.008405, accuracy: 0.1085
average test loss: 0.009756, accuracy: 0.1189
case acc: 0.1584936
case acc: 0.115918644
case acc: 0.08082196
case acc: 0.10049524
case acc: 0.14433545
case acc: 0.11352817
top acc: 0.2801 ::: bot acc: 0.0447
top acc: 0.1610 ::: bot acc: 0.0745
top acc: 0.1725 ::: bot acc: 0.0204
top acc: 0.1621 ::: bot acc: 0.0432
top acc: 0.2228 ::: bot acc: 0.0662
top acc: 0.1831 ::: bot acc: 0.0409
current epoch: 47
train loss is 0.004605
average val loss: 0.008407, accuracy: 0.1085
average test loss: 0.009776, accuracy: 0.1191
case acc: 0.1584925
case acc: 0.1156795
case acc: 0.081277885
case acc: 0.10078616
case acc: 0.14452001
case acc: 0.11367446
top acc: 0.2803 ::: bot acc: 0.0445
top acc: 0.1611 ::: bot acc: 0.0738
top acc: 0.1728 ::: bot acc: 0.0210
top acc: 0.1624 ::: bot acc: 0.0431
top acc: 0.2233 ::: bot acc: 0.0663
top acc: 0.1833 ::: bot acc: 0.0407
current epoch: 48
train loss is 0.004587
average val loss: 0.008201, accuracy: 0.1068
average test loss: 0.009528, accuracy: 0.1173
case acc: 0.1563614
case acc: 0.11305097
case acc: 0.07975663
case acc: 0.09983456
case acc: 0.14243704
case acc: 0.112292364
top acc: 0.2781 ::: bot acc: 0.0431
top acc: 0.1585 ::: bot acc: 0.0710
top acc: 0.1710 ::: bot acc: 0.0203
top acc: 0.1603 ::: bot acc: 0.0441
top acc: 0.2206 ::: bot acc: 0.0645
top acc: 0.1809 ::: bot acc: 0.0411
current epoch: 49
train loss is 0.004543
average val loss: 0.008487, accuracy: 0.1090
average test loss: 0.009821, accuracy: 0.1193
case acc: 0.1586161
case acc: 0.11533949
case acc: 0.08205789
case acc: 0.10124963
case acc: 0.14518349
case acc: 0.11357172
top acc: 0.2806 ::: bot acc: 0.0445
top acc: 0.1607 ::: bot acc: 0.0735
top acc: 0.1741 ::: bot acc: 0.0208
top acc: 0.1636 ::: bot acc: 0.0423
top acc: 0.2240 ::: bot acc: 0.0671
top acc: 0.1835 ::: bot acc: 0.0403
current epoch: 50
train loss is 0.004566
average val loss: 0.008289, accuracy: 0.1074
average test loss: 0.009595, accuracy: 0.1177
case acc: 0.15652506
case acc: 0.11292165
case acc: 0.08086808
case acc: 0.10044049
case acc: 0.1433414
case acc: 0.11224432
top acc: 0.2785 ::: bot acc: 0.0430
top acc: 0.1583 ::: bot acc: 0.0711
top acc: 0.1725 ::: bot acc: 0.0205
top acc: 0.1616 ::: bot acc: 0.0434
top acc: 0.2216 ::: bot acc: 0.0652
top acc: 0.1815 ::: bot acc: 0.0405
