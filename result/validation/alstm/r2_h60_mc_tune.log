
		{"drop_out": 0.4, "drop_out_mc": 0.05, "repeat_mc": 50, "hidden": 30, "embedding_size": 5, "batch": 512, "lag": 3}
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
LME_Co_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5112 5112 5112
1.7082474 -0.6288155 0.48142856 -0.33910522
Validation: 570 570 570
Testing: 774 774 774
pre-processing time: 0.0002636909484863281
the split date is 2010-07-01
net initializing with time: 0.004233360290527344
preparing training and testing date with time: 0.0
current epoch: 1
train loss is 0.140225
average val loss: 0.114828, accuracy: 0.1156
average test loss: 0.174725, accuracy: 0.1804
case acc: 0.24796191
case acc: 0.04365469
case acc: 0.23042159
case acc: 0.19569041
case acc: 0.19725072
case acc: 0.1671868
top acc: 0.2028 ::: bot acc: 0.3073
top acc: 0.0688 ::: bot acc: 0.0351
top acc: 0.1701 ::: bot acc: 0.2856
top acc: 0.1483 ::: bot acc: 0.2372
top acc: 0.1632 ::: bot acc: 0.2290
top acc: 0.1302 ::: bot acc: 0.2011
current epoch: 2
train loss is 0.137464
average val loss: 0.137843, accuracy: 0.1427
average test loss: 0.067964, accuracy: 0.0682
case acc: 0.056930512
case acc: 0.20644723
case acc: 0.053399242
case acc: 0.03654847
case acc: 0.028600568
case acc: 0.027266195
top acc: 0.0137 ::: bot acc: 0.1157
top acc: 0.2551 ::: bot acc: 0.1509
top acc: 0.0268 ::: bot acc: 0.0922
top acc: 0.0373 ::: bot acc: 0.0526
top acc: 0.0206 ::: bot acc: 0.0474
top acc: 0.0488 ::: bot acc: 0.0227
current epoch: 3
train loss is 0.146084
average val loss: 0.146036, accuracy: 0.1497
average test loss: 0.067007, accuracy: 0.0654
case acc: 0.045015167
case acc: 0.21078543
case acc: 0.047798518
case acc: 0.035055283
case acc: 0.025114758
case acc: 0.02864634
top acc: 0.0092 ::: bot acc: 0.1000
top acc: 0.2595 ::: bot acc: 0.1551
top acc: 0.0406 ::: bot acc: 0.0767
top acc: 0.0476 ::: bot acc: 0.0430
top acc: 0.0287 ::: bot acc: 0.0377
top acc: 0.0540 ::: bot acc: 0.0184
current epoch: 4
train loss is 0.138690
average val loss: 0.087187, accuracy: 0.0939
average test loss: 0.107469, accuracy: 0.1164
case acc: 0.15005279
case acc: 0.08854685
case acc: 0.13373068
case acc: 0.114969105
case acc: 0.11164077
case acc: 0.09946958
top acc: 0.1052 ::: bot acc: 0.2097
top acc: 0.1368 ::: bot acc: 0.0337
top acc: 0.0723 ::: bot acc: 0.1901
top acc: 0.0663 ::: bot acc: 0.1574
top acc: 0.0772 ::: bot acc: 0.1436
top acc: 0.0614 ::: bot acc: 0.1340
current epoch: 5
train loss is 0.109604
average val loss: 0.088290, accuracy: 0.0941
average test loss: 0.086201, accuracy: 0.0950
case acc: 0.11912658
case acc: 0.10529701
case acc: 0.108369514
case acc: 0.088542074
case acc: 0.07535559
case acc: 0.073131256
top acc: 0.0745 ::: bot acc: 0.1788
top acc: 0.1540 ::: bot acc: 0.0490
top acc: 0.0470 ::: bot acc: 0.1648
top acc: 0.0398 ::: bot acc: 0.1310
top acc: 0.0411 ::: bot acc: 0.1073
top acc: 0.0355 ::: bot acc: 0.1074
current epoch: 6
train loss is 0.110096
average val loss: 0.099255, accuracy: 0.1042
average test loss: 0.067870, accuracy: 0.0752
case acc: 0.08655726
case acc: 0.124286324
case acc: 0.08332386
case acc: 0.064237066
case acc: 0.043136235
case acc: 0.049809344
top acc: 0.0422 ::: bot acc: 0.1463
top acc: 0.1729 ::: bot acc: 0.0679
top acc: 0.0278 ::: bot acc: 0.1370
top acc: 0.0204 ::: bot acc: 0.1042
top acc: 0.0168 ::: bot acc: 0.0711
top acc: 0.0165 ::: bot acc: 0.0820
current epoch: 7
train loss is 0.111499
average val loss: 0.089164, accuracy: 0.0941
average test loss: 0.073971, accuracy: 0.0822
case acc: 0.09903712
case acc: 0.099003755
case acc: 0.097549126
case acc: 0.07976529
case acc: 0.05020982
case acc: 0.067436986
top acc: 0.0547 ::: bot acc: 0.1588
top acc: 0.1476 ::: bot acc: 0.0426
top acc: 0.0378 ::: bot acc: 0.1534
top acc: 0.0314 ::: bot acc: 0.1221
top acc: 0.0198 ::: bot acc: 0.0801
top acc: 0.0298 ::: bot acc: 0.1017
current epoch: 8
train loss is 0.107291
average val loss: 0.083763, accuracy: 0.0886
average test loss: 0.077881, accuracy: 0.0860
case acc: 0.10575171
case acc: 0.08133245
case acc: 0.10725788
case acc: 0.09019985
case acc: 0.05270141
case acc: 0.078639165
top acc: 0.0615 ::: bot acc: 0.1655
top acc: 0.1288 ::: bot acc: 0.0270
top acc: 0.0459 ::: bot acc: 0.1640
top acc: 0.0412 ::: bot acc: 0.1330
top acc: 0.0212 ::: bot acc: 0.0832
top acc: 0.0397 ::: bot acc: 0.1136
current epoch: 9
train loss is 0.103190
average val loss: 0.084402, accuracy: 0.0888
average test loss: 0.073039, accuracy: 0.0810
case acc: 0.0981177
case acc: 0.07806223
case acc: 0.10342261
case acc: 0.08637587
case acc: 0.044363774
case acc: 0.07552812
top acc: 0.0540 ::: bot acc: 0.1578
top acc: 0.1251 ::: bot acc: 0.0246
top acc: 0.0424 ::: bot acc: 0.1599
top acc: 0.0375 ::: bot acc: 0.1291
top acc: 0.0172 ::: bot acc: 0.0727
top acc: 0.0369 ::: bot acc: 0.1103
current epoch: 10
train loss is 0.100628
average val loss: 0.085355, accuracy: 0.0895
average test loss: 0.068873, accuracy: 0.0766
case acc: 0.09083497
case acc: 0.07507772
case acc: 0.099530324
case acc: 0.08293762
case acc: 0.03826778
case acc: 0.07315088
top acc: 0.0468 ::: bot acc: 0.1506
top acc: 0.1218 ::: bot acc: 0.0225
top acc: 0.0390 ::: bot acc: 0.1558
top acc: 0.0341 ::: bot acc: 0.1255
top acc: 0.0169 ::: bot acc: 0.0637
top acc: 0.0349 ::: bot acc: 0.1078
current epoch: 11
train loss is 0.099761
average val loss: 0.085998, accuracy: 0.0899
average test loss: 0.065847, accuracy: 0.0734
case acc: 0.08506652
case acc: 0.0715295
case acc: 0.096514665
case acc: 0.08072712
case acc: 0.034054972
case acc: 0.07228448
top acc: 0.0410 ::: bot acc: 0.1448
top acc: 0.1178 ::: bot acc: 0.0200
top acc: 0.0366 ::: bot acc: 0.1525
top acc: 0.0320 ::: bot acc: 0.1232
top acc: 0.0177 ::: bot acc: 0.0571
top acc: 0.0344 ::: bot acc: 0.1068
current epoch: 12
train loss is 0.098149
average val loss: 0.085977, accuracy: 0.0897
average test loss: 0.063997, accuracy: 0.0712
case acc: 0.08121968
case acc: 0.067287825
case acc: 0.09473292
case acc: 0.079771936
case acc: 0.031730276
case acc: 0.07273735
top acc: 0.0372 ::: bot acc: 0.1410
top acc: 0.1127 ::: bot acc: 0.0174
top acc: 0.0351 ::: bot acc: 0.1505
top acc: 0.0311 ::: bot acc: 0.1223
top acc: 0.0189 ::: bot acc: 0.0531
top acc: 0.0350 ::: bot acc: 0.1072
current epoch: 13
train loss is 0.096731
average val loss: 0.083819, accuracy: 0.0874
average test loss: 0.064825, accuracy: 0.0718
case acc: 0.081989504
case acc: 0.060755733
case acc: 0.09652905
case acc: 0.082484476
case acc: 0.032386705
case acc: 0.07667588
top acc: 0.0380 ::: bot acc: 0.1417
top acc: 0.1043 ::: bot acc: 0.0147
top acc: 0.0364 ::: bot acc: 0.1526
top acc: 0.0334 ::: bot acc: 0.1253
top acc: 0.0185 ::: bot acc: 0.0542
top acc: 0.0386 ::: bot acc: 0.1113
current epoch: 14
train loss is 0.095794
average val loss: 0.083728, accuracy: 0.0872
average test loss: 0.063560, accuracy: 0.0703
case acc: 0.079209134
case acc: 0.05802933
case acc: 0.09479916
case acc: 0.08146885
case acc: 0.03132682
case acc: 0.076883405
top acc: 0.0352 ::: bot acc: 0.1390
top acc: 0.1001 ::: bot acc: 0.0148
top acc: 0.0350 ::: bot acc: 0.1507
top acc: 0.0323 ::: bot acc: 0.1244
top acc: 0.0192 ::: bot acc: 0.0524
top acc: 0.0387 ::: bot acc: 0.1116
current epoch: 15
train loss is 0.094799
average val loss: 0.083576, accuracy: 0.0869
average test loss: 0.062684, accuracy: 0.0692
case acc: 0.077022366
case acc: 0.055991225
case acc: 0.09310471
case acc: 0.0807033
case acc: 0.030860346
case acc: 0.07746733
top acc: 0.0330 ::: bot acc: 0.1368
top acc: 0.0965 ::: bot acc: 0.0158
top acc: 0.0337 ::: bot acc: 0.1489
top acc: 0.0316 ::: bot acc: 0.1236
top acc: 0.0196 ::: bot acc: 0.0515
top acc: 0.0391 ::: bot acc: 0.1123
current epoch: 16
train loss is 0.093492
average val loss: 0.083398, accuracy: 0.0867
average test loss: 0.062122, accuracy: 0.0684
case acc: 0.07517936
case acc: 0.05446281
case acc: 0.09129522
case acc: 0.08017528
case acc: 0.030861693
case acc: 0.078557186
top acc: 0.0312 ::: bot acc: 0.1350
top acc: 0.0936 ::: bot acc: 0.0171
top acc: 0.0323 ::: bot acc: 0.1469
top acc: 0.0311 ::: bot acc: 0.1232
top acc: 0.0196 ::: bot acc: 0.0514
top acc: 0.0402 ::: bot acc: 0.1134
current epoch: 17
train loss is 0.093013
average val loss: 0.086368, accuracy: 0.0894
average test loss: 0.058495, accuracy: 0.0645
case acc: 0.0682929
case acc: 0.056110967
case acc: 0.08469222
case acc: 0.074680075
case acc: 0.02856661
case acc: 0.07451442
top acc: 0.0245 ::: bot acc: 0.1280
top acc: 0.0967 ::: bot acc: 0.0158
top acc: 0.0278 ::: bot acc: 0.1393
top acc: 0.0265 ::: bot acc: 0.1172
top acc: 0.0220 ::: bot acc: 0.0468
top acc: 0.0366 ::: bot acc: 0.1092
current epoch: 18
train loss is 0.093335
average val loss: 0.088801, accuracy: 0.0917
average test loss: 0.056001, accuracy: 0.0616
case acc: 0.06294247
case acc: 0.05735824
case acc: 0.079439595
case acc: 0.07074021
case acc: 0.027339198
case acc: 0.07192681
top acc: 0.0194 ::: bot acc: 0.1226
top acc: 0.0989 ::: bot acc: 0.0151
top acc: 0.0247 ::: bot acc: 0.1330
top acc: 0.0237 ::: bot acc: 0.1127
top acc: 0.0242 ::: bot acc: 0.0439
top acc: 0.0343 ::: bot acc: 0.1064
current epoch: 19
train loss is 0.092811
average val loss: 0.090547, accuracy: 0.0934
average test loss: 0.054374, accuracy: 0.0597
case acc: 0.05912364
case acc: 0.05802958
case acc: 0.07547201
case acc: 0.06805168
case acc: 0.026687317
case acc: 0.070544004
top acc: 0.0158 ::: bot acc: 0.1187
top acc: 0.1000 ::: bot acc: 0.0148
top acc: 0.0226 ::: bot acc: 0.1281
top acc: 0.0219 ::: bot acc: 0.1095
top acc: 0.0258 ::: bot acc: 0.0421
top acc: 0.0330 ::: bot acc: 0.1050
current epoch: 20
train loss is 0.092750
average val loss: 0.091669, accuracy: 0.0944
average test loss: 0.053306, accuracy: 0.0583
case acc: 0.056596994
case acc: 0.058055725
case acc: 0.07259039
case acc: 0.0662358
case acc: 0.026271567
case acc: 0.06992275
top acc: 0.0136 ::: bot acc: 0.1160
top acc: 0.1001 ::: bot acc: 0.0148
top acc: 0.0212 ::: bot acc: 0.1245
top acc: 0.0207 ::: bot acc: 0.1074
top acc: 0.0270 ::: bot acc: 0.0409
top acc: 0.0325 ::: bot acc: 0.1043
current epoch: 21
train loss is 0.092883
average val loss: 0.092044, accuracy: 0.0948
average test loss: 0.052758, accuracy: 0.0575
case acc: 0.055370115
case acc: 0.057339393
case acc: 0.07100181
case acc: 0.065326415
case acc: 0.026116718
case acc: 0.0699216
top acc: 0.0127 ::: bot acc: 0.1146
top acc: 0.0989 ::: bot acc: 0.0151
top acc: 0.0205 ::: bot acc: 0.1225
top acc: 0.0202 ::: bot acc: 0.1064
top acc: 0.0275 ::: bot acc: 0.0404
top acc: 0.0326 ::: bot acc: 0.1043
current epoch: 22
train loss is 0.092532
average val loss: 0.091095, accuracy: 0.0938
average test loss: 0.053037, accuracy: 0.0577
case acc: 0.056072466
case acc: 0.055426907
case acc: 0.071295075
case acc: 0.06582979
case acc: 0.026503237
case acc: 0.07107967
top acc: 0.0132 ::: bot acc: 0.1154
top acc: 0.0954 ::: bot acc: 0.0163
top acc: 0.0206 ::: bot acc: 0.1229
top acc: 0.0204 ::: bot acc: 0.1070
top acc: 0.0265 ::: bot acc: 0.0415
top acc: 0.0336 ::: bot acc: 0.1055
current epoch: 23
train loss is 0.092324
average val loss: 0.089501, accuracy: 0.0923
average test loss: 0.053769, accuracy: 0.0584
case acc: 0.057880998
case acc: 0.05312831
case acc: 0.07267272
case acc: 0.067004755
case acc: 0.027152553
case acc: 0.07260676
top acc: 0.0147 ::: bot acc: 0.1174
top acc: 0.0908 ::: bot acc: 0.0186
top acc: 0.0212 ::: bot acc: 0.1247
top acc: 0.0211 ::: bot acc: 0.1085
top acc: 0.0248 ::: bot acc: 0.0434
top acc: 0.0351 ::: bot acc: 0.1071
current epoch: 24
train loss is 0.091465
average val loss: 0.087290, accuracy: 0.0901
average test loss: 0.055122, accuracy: 0.0598
case acc: 0.060811196
case acc: 0.05068968
case acc: 0.07497994
case acc: 0.0690557
case acc: 0.02826754
case acc: 0.07495055
top acc: 0.0173 ::: bot acc: 0.1205
top acc: 0.0853 ::: bot acc: 0.0223
top acc: 0.0223 ::: bot acc: 0.1277
top acc: 0.0224 ::: bot acc: 0.1109
top acc: 0.0227 ::: bot acc: 0.0461
top acc: 0.0372 ::: bot acc: 0.1096
current epoch: 25
train loss is 0.090713
average val loss: 0.085766, accuracy: 0.0887
average test loss: 0.056091, accuracy: 0.0607
case acc: 0.06302352
case acc: 0.049023923
case acc: 0.07645088
case acc: 0.07027984
case acc: 0.02922961
case acc: 0.07640173
top acc: 0.0194 ::: bot acc: 0.1228
top acc: 0.0812 ::: bot acc: 0.0255
top acc: 0.0230 ::: bot acc: 0.1295
top acc: 0.0232 ::: bot acc: 0.1124
top acc: 0.0214 ::: bot acc: 0.0482
top acc: 0.0384 ::: bot acc: 0.1111
current epoch: 26
train loss is 0.089736
average val loss: 0.085268, accuracy: 0.0882
average test loss: 0.056332, accuracy: 0.0609
case acc: 0.063653424
case acc: 0.0482536
case acc: 0.07641303
case acc: 0.07036996
case acc: 0.029724488
case acc: 0.07685995
top acc: 0.0200 ::: bot acc: 0.1234
top acc: 0.0792 ::: bot acc: 0.0271
top acc: 0.0230 ::: bot acc: 0.1295
top acc: 0.0233 ::: bot acc: 0.1125
top acc: 0.0208 ::: bot acc: 0.0492
top acc: 0.0388 ::: bot acc: 0.1116
current epoch: 27
train loss is 0.089832
average val loss: 0.086698, accuracy: 0.0896
average test loss: 0.054981, accuracy: 0.0593
case acc: 0.061210882
case acc: 0.04890015
case acc: 0.07362783
case acc: 0.068102226
case acc: 0.029035565
case acc: 0.07501688
top acc: 0.0177 ::: bot acc: 0.1209
top acc: 0.0808 ::: bot acc: 0.0258
top acc: 0.0215 ::: bot acc: 0.1261
top acc: 0.0218 ::: bot acc: 0.1099
top acc: 0.0217 ::: bot acc: 0.0477
top acc: 0.0371 ::: bot acc: 0.1097
current epoch: 28
train loss is 0.089839
average val loss: 0.089699, accuracy: 0.0925
average test loss: 0.052679, accuracy: 0.0567
case acc: 0.05676265
case acc: 0.050692666
case acc: 0.069061235
case acc: 0.064209945
case acc: 0.027521415
case acc: 0.07166219
top acc: 0.0137 ::: bot acc: 0.1162
top acc: 0.0852 ::: bot acc: 0.0224
top acc: 0.0199 ::: bot acc: 0.1201
top acc: 0.0194 ::: bot acc: 0.1052
top acc: 0.0242 ::: bot acc: 0.0442
top acc: 0.0342 ::: bot acc: 0.1062
current epoch: 29
train loss is 0.090017
average val loss: 0.090087, accuracy: 0.0930
average test loss: 0.052593, accuracy: 0.0564
case acc: 0.05611152
case acc: 0.050959215
case acc: 0.067528516
case acc: 0.06382791
case acc: 0.027699977
case acc: 0.07250525
top acc: 0.0132 ::: bot acc: 0.1156
top acc: 0.0859 ::: bot acc: 0.0219
top acc: 0.0197 ::: bot acc: 0.1179
top acc: 0.0191 ::: bot acc: 0.1048
top acc: 0.0238 ::: bot acc: 0.0447
top acc: 0.0350 ::: bot acc: 0.1070
current epoch: 30
train loss is 0.090082
average val loss: 0.092226, accuracy: 0.0950
average test loss: 0.051329, accuracy: 0.0549
case acc: 0.053355154
case acc: 0.052337777
case acc: 0.064507425
case acc: 0.061486147
case acc: 0.026802197
case acc: 0.07086302
top acc: 0.0111 ::: bot acc: 0.1125
top acc: 0.0890 ::: bot acc: 0.0197
top acc: 0.0198 ::: bot acc: 0.1133
top acc: 0.0179 ::: bot acc: 0.1019
top acc: 0.0259 ::: bot acc: 0.0423
top acc: 0.0335 ::: bot acc: 0.1053
current epoch: 31
train loss is 0.090173
average val loss: 0.094171, accuracy: 0.0969
average test loss: 0.050227, accuracy: 0.0535
case acc: 0.051163387
case acc: 0.053496487
case acc: 0.06220108
case acc: 0.059431534
case acc: 0.025952747
case acc: 0.06899354
top acc: 0.0097 ::: bot acc: 0.1099
top acc: 0.0915 ::: bot acc: 0.0183
top acc: 0.0204 ::: bot acc: 0.1095
top acc: 0.0171 ::: bot acc: 0.0992
top acc: 0.0285 ::: bot acc: 0.0398
top acc: 0.0319 ::: bot acc: 0.1033
current epoch: 32
train loss is 0.090538
average val loss: 0.093995, accuracy: 0.0968
average test loss: 0.050241, accuracy: 0.0535
case acc: 0.051654674
case acc: 0.052987203
case acc: 0.062116195
case acc: 0.059420902
case acc: 0.025864905
case acc: 0.06914144
top acc: 0.0100 ::: bot acc: 0.1105
top acc: 0.0904 ::: bot acc: 0.0189
top acc: 0.0205 ::: bot acc: 0.1094
top acc: 0.0171 ::: bot acc: 0.0992
top acc: 0.0288 ::: bot acc: 0.0395
top acc: 0.0320 ::: bot acc: 0.1035
current epoch: 33
train loss is 0.090559
average val loss: 0.093190, accuracy: 0.0961
average test loss: 0.050454, accuracy: 0.0538
case acc: 0.053063944
case acc: 0.05188582
case acc: 0.06289035
case acc: 0.059838265
case acc: 0.02586845
case acc: 0.06923479
top acc: 0.0109 ::: bot acc: 0.1122
top acc: 0.0879 ::: bot acc: 0.0205
top acc: 0.0202 ::: bot acc: 0.1107
top acc: 0.0172 ::: bot acc: 0.0998
top acc: 0.0288 ::: bot acc: 0.0395
top acc: 0.0321 ::: bot acc: 0.1036
current epoch: 34
train loss is 0.090302
average val loss: 0.088176, accuracy: 0.0914
average test loss: 0.053510, accuracy: 0.0573
case acc: 0.06034838
case acc: 0.048191372
case acc: 0.0681489
case acc: 0.06482309
case acc: 0.027959608
case acc: 0.07415909
top acc: 0.0168 ::: bot acc: 0.1201
top acc: 0.0789 ::: bot acc: 0.0274
top acc: 0.0198 ::: bot acc: 0.1188
top acc: 0.0197 ::: bot acc: 0.1061
top acc: 0.0235 ::: bot acc: 0.0454
top acc: 0.0365 ::: bot acc: 0.1088
current epoch: 35
train loss is 0.089297
average val loss: 0.084634, accuracy: 0.0880
average test loss: 0.056067, accuracy: 0.0600
case acc: 0.066446446
case acc: 0.04558346
case acc: 0.072898574
case acc: 0.068462774
case acc: 0.029808914
case acc: 0.07703748
top acc: 0.0227 ::: bot acc: 0.1263
top acc: 0.0716 ::: bot acc: 0.0343
top acc: 0.0211 ::: bot acc: 0.1253
top acc: 0.0219 ::: bot acc: 0.1104
top acc: 0.0209 ::: bot acc: 0.0494
top acc: 0.0391 ::: bot acc: 0.1118
current epoch: 36
train loss is 0.088473
average val loss: 0.081620, accuracy: 0.0851
average test loss: 0.058752, accuracy: 0.0628
case acc: 0.07196383
case acc: 0.043690156
case acc: 0.07736221
case acc: 0.07181453
case acc: 0.032112777
case acc: 0.07984655
top acc: 0.0281 ::: bot acc: 0.1320
top acc: 0.0652 ::: bot acc: 0.0407
top acc: 0.0233 ::: bot acc: 0.1309
top acc: 0.0239 ::: bot acc: 0.1144
top acc: 0.0194 ::: bot acc: 0.0537
top acc: 0.0417 ::: bot acc: 0.1147
current epoch: 37
train loss is 0.087963
average val loss: 0.079442, accuracy: 0.0832
average test loss: 0.061129, accuracy: 0.0651
case acc: 0.076148264
case acc: 0.042578675
case acc: 0.08028162
case acc: 0.074424684
case acc: 0.034575358
case acc: 0.08252135
top acc: 0.0322 ::: bot acc: 0.1362
top acc: 0.0607 ::: bot acc: 0.0452
top acc: 0.0248 ::: bot acc: 0.1345
top acc: 0.0255 ::: bot acc: 0.1175
top acc: 0.0182 ::: bot acc: 0.0580
top acc: 0.0441 ::: bot acc: 0.1175
current epoch: 38
train loss is 0.087956
average val loss: 0.078923, accuracy: 0.0828
average test loss: 0.061930, accuracy: 0.0658
case acc: 0.07710729
case acc: 0.042423375
case acc: 0.08001508
case acc: 0.07506034
case acc: 0.036102332
case acc: 0.084111534
top acc: 0.0331 ::: bot acc: 0.1372
top acc: 0.0599 ::: bot acc: 0.0460
top acc: 0.0247 ::: bot acc: 0.1342
top acc: 0.0259 ::: bot acc: 0.1182
top acc: 0.0176 ::: bot acc: 0.0605
top acc: 0.0455 ::: bot acc: 0.1192
current epoch: 39
train loss is 0.088072
average val loss: 0.082404, accuracy: 0.0861
average test loss: 0.058281, accuracy: 0.0621
case acc: 0.07061966
case acc: 0.044153243
case acc: 0.07334661
case acc: 0.07010358
case acc: 0.03382794
case acc: 0.08039506
top acc: 0.0268 ::: bot acc: 0.1307
top acc: 0.0668 ::: bot acc: 0.0390
top acc: 0.0213 ::: bot acc: 0.1259
top acc: 0.0226 ::: bot acc: 0.1124
top acc: 0.0185 ::: bot acc: 0.0567
top acc: 0.0422 ::: bot acc: 0.1153
current epoch: 40
train loss is 0.088430
average val loss: 0.089016, accuracy: 0.0924
average test loss: 0.052978, accuracy: 0.0562
case acc: 0.05991595
case acc: 0.04808406
case acc: 0.06421769
case acc: 0.06209258
case acc: 0.02935951
case acc: 0.07340292
top acc: 0.0164 ::: bot acc: 0.1198
top acc: 0.0785 ::: bot acc: 0.0277
top acc: 0.0199 ::: bot acc: 0.1129
top acc: 0.0179 ::: bot acc: 0.1027
top acc: 0.0215 ::: bot acc: 0.0485
top acc: 0.0359 ::: bot acc: 0.1079
current epoch: 41
train loss is 0.088934
average val loss: 0.095241, accuracy: 0.0984
average test loss: 0.049536, accuracy: 0.0521
case acc: 0.051640444
case acc: 0.052193306
case acc: 0.058132544
case acc: 0.0561953
case acc: 0.026304882
case acc: 0.06800007
top acc: 0.0099 ::: bot acc: 0.1106
top acc: 0.0885 ::: bot acc: 0.0201
top acc: 0.0241 ::: bot acc: 0.1017
top acc: 0.0160 ::: bot acc: 0.0949
top acc: 0.0273 ::: bot acc: 0.0409
top acc: 0.0311 ::: bot acc: 0.1022
current epoch: 42
train loss is 0.089770
average val loss: 0.098855, accuracy: 0.1018
average test loss: 0.048071, accuracy: 0.0503
case acc: 0.04790359
case acc: 0.054749187
case acc: 0.055397738
case acc: 0.053606603
case acc: 0.024973411
case acc: 0.06543098
top acc: 0.0083 ::: bot acc: 0.1058
top acc: 0.0939 ::: bot acc: 0.0171
top acc: 0.0284 ::: bot acc: 0.0955
top acc: 0.0164 ::: bot acc: 0.0908
top acc: 0.0325 ::: bot acc: 0.0358
top acc: 0.0288 ::: bot acc: 0.0995
current epoch: 43
train loss is 0.091099
average val loss: 0.100014, accuracy: 0.1030
average test loss: 0.047388, accuracy: 0.0496
case acc: 0.04729292
case acc: 0.054980457
case acc: 0.054913033
case acc: 0.052644156
case acc: 0.024475379
case acc: 0.06356508
top acc: 0.0081 ::: bot acc: 0.1050
top acc: 0.0943 ::: bot acc: 0.0168
top acc: 0.0293 ::: bot acc: 0.0943
top acc: 0.0167 ::: bot acc: 0.0893
top acc: 0.0360 ::: bot acc: 0.0323
top acc: 0.0272 ::: bot acc: 0.0976
current epoch: 44
train loss is 0.092494
average val loss: 0.094034, accuracy: 0.0975
average test loss: 0.049601, accuracy: 0.0524
case acc: 0.054699462
case acc: 0.049972773
case acc: 0.059539188
case acc: 0.05702328
case acc: 0.025205385
case acc: 0.06776805
top acc: 0.0120 ::: bot acc: 0.1141
top acc: 0.0833 ::: bot acc: 0.0239
top acc: 0.0226 ::: bot acc: 0.1046
top acc: 0.0163 ::: bot acc: 0.0961
top acc: 0.0314 ::: bot acc: 0.0369
top acc: 0.0309 ::: bot acc: 0.1020
current epoch: 45
train loss is 0.091524
average val loss: 0.082815, accuracy: 0.0867
average test loss: 0.057100, accuracy: 0.0607
case acc: 0.07222505
case acc: 0.04294754
case acc: 0.07266361
case acc: 0.06839464
case acc: 0.029856442
case acc: 0.07785761
top acc: 0.0283 ::: bot acc: 0.1323
top acc: 0.0622 ::: bot acc: 0.0437
top acc: 0.0211 ::: bot acc: 0.1251
top acc: 0.0217 ::: bot acc: 0.1105
top acc: 0.0209 ::: bot acc: 0.0495
top acc: 0.0400 ::: bot acc: 0.1126
current epoch: 46
train loss is 0.088369
average val loss: 0.073604, accuracy: 0.0777
average test loss: 0.068652, accuracy: 0.0715
case acc: 0.09107896
case acc: 0.039913345
case acc: 0.0901684
case acc: 0.08147654
case acc: 0.038012482
case acc: 0.08827804
top acc: 0.0471 ::: bot acc: 0.1511
top acc: 0.0407 ::: bot acc: 0.0652
top acc: 0.0310 ::: bot acc: 0.1464
top acc: 0.0312 ::: bot acc: 0.1253
top acc: 0.0174 ::: bot acc: 0.0635
top acc: 0.0495 ::: bot acc: 0.1235
current epoch: 47
train loss is 0.087113
average val loss: 0.068410, accuracy: 0.0728
average test loss: 0.080973, accuracy: 0.0825
case acc: 0.107132666
case acc: 0.04125015
case acc: 0.10564038
case acc: 0.09410351
case acc: 0.04826866
case acc: 0.09864628
top acc: 0.0632 ::: bot acc: 0.1672
top acc: 0.0234 ::: bot acc: 0.0825
top acc: 0.0428 ::: bot acc: 0.1637
top acc: 0.0429 ::: bot acc: 0.1383
top acc: 0.0196 ::: bot acc: 0.0778
top acc: 0.0596 ::: bot acc: 0.1340
current epoch: 48
train loss is 0.089140
average val loss: 0.067211, accuracy: 0.0721
average test loss: 0.086177, accuracy: 0.0873
case acc: 0.11279085
case acc: 0.04287487
case acc: 0.11035387
case acc: 0.09886298
case acc: 0.055264525
case acc: 0.10352695
top acc: 0.0688 ::: bot acc: 0.1729
top acc: 0.0180 ::: bot acc: 0.0880
top acc: 0.0469 ::: bot acc: 0.1687
top acc: 0.0476 ::: bot acc: 0.1431
top acc: 0.0232 ::: bot acc: 0.0865
top acc: 0.0643 ::: bot acc: 0.1389
current epoch: 49
train loss is 0.091897
average val loss: 0.070171, accuracy: 0.0747
average test loss: 0.076240, accuracy: 0.0783
case acc: 0.09993997
case acc: 0.03970892
case acc: 0.09611643
case acc: 0.087827355
case acc: 0.05044031
case acc: 0.09566531
top acc: 0.0560 ::: bot acc: 0.1600
top acc: 0.0324 ::: bot acc: 0.0735
top acc: 0.0352 ::: bot acc: 0.1532
top acc: 0.0370 ::: bot acc: 0.1319
top acc: 0.0207 ::: bot acc: 0.0805
top acc: 0.0567 ::: bot acc: 0.1309
current epoch: 50
train loss is 0.091213
average val loss: 0.081457, accuracy: 0.0856
average test loss: 0.058750, accuracy: 0.0619
case acc: 0.07374457
case acc: 0.04255523
case acc: 0.071144275
case acc: 0.06749182
case acc: 0.03729601
case acc: 0.07893803
top acc: 0.0298 ::: bot acc: 0.1338
top acc: 0.0607 ::: bot acc: 0.0451
top acc: 0.0205 ::: bot acc: 0.1231
top acc: 0.0210 ::: bot acc: 0.1093
top acc: 0.0174 ::: bot acc: 0.0624
top acc: 0.0410 ::: bot acc: 0.1136
LME_Co_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5136 5136 5136
1.7082474 -0.6288155 0.48738334 -0.33910522
Validation: 576 576 576
Testing: 744 744 744
pre-processing time: 0.0002593994140625
the split date is 2011-01-01
net initializing with time: 0.002992868423461914
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.221345
average val loss: 0.099322, accuracy: 0.1259
average test loss: 0.112874, accuracy: 0.1288
case acc: 0.18357827
case acc: 0.07435581
case acc: 0.21263741
case acc: 0.10780769
case acc: 0.10213123
case acc: 0.09206917
top acc: 0.3015 ::: bot acc: 0.0721
top acc: 0.1193 ::: bot acc: 0.0272
top acc: 0.1347 ::: bot acc: 0.2954
top acc: 0.0217 ::: bot acc: 0.1786
top acc: 0.1548 ::: bot acc: 0.1048
top acc: 0.1083 ::: bot acc: 0.1261
current epoch: 2
train loss is 0.208538
average val loss: 0.116871, accuracy: 0.1358
average test loss: 0.119948, accuracy: 0.1364
case acc: 0.25263816
case acc: 0.14089641
case acc: 0.13096192
case acc: 0.07496408
case acc: 0.12071744
case acc: 0.098068126
top acc: 0.3697 ::: bot acc: 0.1418
top acc: 0.1942 ::: bot acc: 0.0797
top acc: 0.0523 ::: bot acc: 0.2143
top acc: 0.0698 ::: bot acc: 0.1047
top acc: 0.2264 ::: bot acc: 0.0339
top acc: 0.1816 ::: bot acc: 0.0533
current epoch: 3
train loss is 0.142007
average val loss: 0.112342, accuracy: 0.1268
average test loss: 0.134733, accuracy: 0.1407
case acc: 0.10501171
case acc: 0.044478394
case acc: 0.28522855
case acc: 0.18657763
case acc: 0.10797559
case acc: 0.11503287
top acc: 0.1997 ::: bot acc: 0.0390
top acc: 0.0300 ::: bot acc: 0.0852
top acc: 0.2064 ::: bot acc: 0.3688
top acc: 0.0887 ::: bot acc: 0.2639
top acc: 0.0651 ::: bot acc: 0.1941
top acc: 0.0313 ::: bot acc: 0.2127
current epoch: 4
train loss is 0.131315
average val loss: 0.096164, accuracy: 0.1199
average test loss: 0.106492, accuracy: 0.1227
case acc: 0.19927606
case acc: 0.09617441
case acc: 0.15502484
case acc: 0.08650651
case acc: 0.10861466
case acc: 0.09060529
top acc: 0.3152 ::: bot acc: 0.0890
top acc: 0.1493 ::: bot acc: 0.0380
top acc: 0.0758 ::: bot acc: 0.2389
top acc: 0.0363 ::: bot acc: 0.1405
top acc: 0.1867 ::: bot acc: 0.0721
top acc: 0.1465 ::: bot acc: 0.0894
current epoch: 5
train loss is 0.135629
average val loss: 0.087128, accuracy: 0.1117
average test loss: 0.103925, accuracy: 0.1165
case acc: 0.14842463
case acc: 0.059631318
case acc: 0.18948695
case acc: 0.10894453
case acc: 0.09998668
case acc: 0.09246729
top acc: 0.2638 ::: bot acc: 0.0389
top acc: 0.1019 ::: bot acc: 0.0242
top acc: 0.1101 ::: bot acc: 0.2736
top acc: 0.0207 ::: bot acc: 0.1819
top acc: 0.1431 ::: bot acc: 0.1156
top acc: 0.1049 ::: bot acc: 0.1312
current epoch: 6
train loss is 0.118483
average val loss: 0.083886, accuracy: 0.1072
average test loss: 0.102452, accuracy: 0.1135
case acc: 0.13430232
case acc: 0.051828124
case acc: 0.18875624
case acc: 0.1135819
case acc: 0.09934024
case acc: 0.093278676
top acc: 0.2470 ::: bot acc: 0.0298
top acc: 0.0890 ::: bot acc: 0.0274
top acc: 0.1092 ::: bot acc: 0.2730
top acc: 0.0225 ::: bot acc: 0.1881
top acc: 0.1349 ::: bot acc: 0.1237
top acc: 0.0990 ::: bot acc: 0.1374
current epoch: 7
train loss is 0.120205
average val loss: 0.081483, accuracy: 0.1050
average test loss: 0.097578, accuracy: 0.1096
case acc: 0.14805888
case acc: 0.06342773
case acc: 0.1565655
case acc: 0.09720869
case acc: 0.101954915
case acc: 0.09047605
top acc: 0.2626 ::: bot acc: 0.0395
top acc: 0.1090 ::: bot acc: 0.0228
top acc: 0.0769 ::: bot acc: 0.2409
top acc: 0.0243 ::: bot acc: 0.1627
top acc: 0.1581 ::: bot acc: 0.1005
top acc: 0.1244 ::: bot acc: 0.1123
current epoch: 8
train loss is 0.116971
average val loss: 0.077500, accuracy: 0.0999
average test loss: 0.096518, accuracy: 0.1068
case acc: 0.12922277
case acc: 0.05214654
case acc: 0.16267256
case acc: 0.10474287
case acc: 0.100039005
case acc: 0.09181952
top acc: 0.2401 ::: bot acc: 0.0282
top acc: 0.0903 ::: bot acc: 0.0267
top acc: 0.0830 ::: bot acc: 0.2470
top acc: 0.0219 ::: bot acc: 0.1753
top acc: 0.1435 ::: bot acc: 0.1152
top acc: 0.1118 ::: bot acc: 0.1250
current epoch: 9
train loss is 0.112633
average val loss: 0.074967, accuracy: 0.0967
average test loss: 0.093806, accuracy: 0.1035
case acc: 0.12818292
case acc: 0.05340437
case acc: 0.14838585
case acc: 0.09973155
case acc: 0.10073449
case acc: 0.09084939
top acc: 0.2387 ::: bot acc: 0.0279
top acc: 0.0928 ::: bot acc: 0.0256
top acc: 0.0687 ::: bot acc: 0.2329
top acc: 0.0233 ::: bot acc: 0.1672
top acc: 0.1496 ::: bot acc: 0.1090
top acc: 0.1197 ::: bot acc: 0.1169
current epoch: 10
train loss is 0.110687
average val loss: 0.072173, accuracy: 0.0929
average test loss: 0.092453, accuracy: 0.1012
case acc: 0.12015952
case acc: 0.049256925
case acc: 0.14508802
case acc: 0.10128854
case acc: 0.100158185
case acc: 0.09118418
top acc: 0.2267 ::: bot acc: 0.0277
top acc: 0.0850 ::: bot acc: 0.0289
top acc: 0.0654 ::: bot acc: 0.2296
top acc: 0.0229 ::: bot acc: 0.1697
top acc: 0.1448 ::: bot acc: 0.1136
top acc: 0.1170 ::: bot acc: 0.1198
current epoch: 11
train loss is 0.106403
average val loss: 0.069728, accuracy: 0.0894
average test loss: 0.091191, accuracy: 0.0990
case acc: 0.114252046
case acc: 0.046607524
case acc: 0.14034495
case acc: 0.101793274
case acc: 0.09981698
case acc: 0.091250576
top acc: 0.2169 ::: bot acc: 0.0296
top acc: 0.0793 ::: bot acc: 0.0327
top acc: 0.0605 ::: bot acc: 0.2249
top acc: 0.0230 ::: bot acc: 0.1705
top acc: 0.1419 ::: bot acc: 0.1163
top acc: 0.1159 ::: bot acc: 0.1209
current epoch: 12
train loss is 0.104279
average val loss: 0.067587, accuracy: 0.0863
average test loss: 0.089790, accuracy: 0.0968
case acc: 0.11020515
case acc: 0.045393877
case acc: 0.1333926
case acc: 0.10081717
case acc: 0.099787794
case acc: 0.09114141
top acc: 0.2100 ::: bot acc: 0.0313
top acc: 0.0765 ::: bot acc: 0.0348
top acc: 0.0536 ::: bot acc: 0.2179
top acc: 0.0233 ::: bot acc: 0.1689
top acc: 0.1415 ::: bot acc: 0.1167
top acc: 0.1174 ::: bot acc: 0.1196
current epoch: 13
train loss is 0.102047
average val loss: 0.065595, accuracy: 0.0829
average test loss: 0.089667, accuracy: 0.0957
case acc: 0.10334168
case acc: 0.042891946
case acc: 0.13325489
case acc: 0.10391949
case acc: 0.09932619
case acc: 0.0917621
top acc: 0.1967 ::: bot acc: 0.0372
top acc: 0.0674 ::: bot acc: 0.0437
top acc: 0.0534 ::: bot acc: 0.2177
top acc: 0.0228 ::: bot acc: 0.1738
top acc: 0.1346 ::: bot acc: 0.1237
top acc: 0.1123 ::: bot acc: 0.1248
current epoch: 14
train loss is 0.098576
average val loss: 0.064303, accuracy: 0.0800
average test loss: 0.090284, accuracy: 0.0955
case acc: 0.0975991
case acc: 0.041370116
case acc: 0.13449255
case acc: 0.10772156
case acc: 0.099045575
case acc: 0.0925617
top acc: 0.1831 ::: bot acc: 0.0475
top acc: 0.0577 ::: bot acc: 0.0532
top acc: 0.0546 ::: bot acc: 0.2190
top acc: 0.0224 ::: bot acc: 0.1796
top acc: 0.1266 ::: bot acc: 0.1316
top acc: 0.1062 ::: bot acc: 0.1310
current epoch: 15
train loss is 0.097182
average val loss: 0.062665, accuracy: 0.0776
average test loss: 0.089086, accuracy: 0.0936
case acc: 0.09586494
case acc: 0.041362163
case acc: 0.12717567
case acc: 0.105821
case acc: 0.099103056
case acc: 0.092285946
top acc: 0.1789 ::: bot acc: 0.0508
top acc: 0.0575 ::: bot acc: 0.0535
top acc: 0.0474 ::: bot acc: 0.2116
top acc: 0.0224 ::: bot acc: 0.1767
top acc: 0.1275 ::: bot acc: 0.1309
top acc: 0.1087 ::: bot acc: 0.1285
current epoch: 16
train loss is 0.095425
average val loss: 0.061870, accuracy: 0.0753
average test loss: 0.089416, accuracy: 0.0931
case acc: 0.091962315
case acc: 0.04113407
case acc: 0.12596396
case acc: 0.10772628
case acc: 0.09907175
case acc: 0.09271163
top acc: 0.1694 ::: bot acc: 0.0584
top acc: 0.0517 ::: bot acc: 0.0593
top acc: 0.0463 ::: bot acc: 0.2104
top acc: 0.0223 ::: bot acc: 0.1796
top acc: 0.1226 ::: bot acc: 0.1359
top acc: 0.1055 ::: bot acc: 0.1317
current epoch: 17
train loss is 0.093045
average val loss: 0.061100, accuracy: 0.0733
average test loss: 0.089305, accuracy: 0.0922
case acc: 0.089252755
case acc: 0.041264784
case acc: 0.12282557
case acc: 0.10814546
case acc: 0.099040054
case acc: 0.09286933
top acc: 0.1625 ::: bot acc: 0.0640
top acc: 0.0487 ::: bot acc: 0.0625
top acc: 0.0433 ::: bot acc: 0.2072
top acc: 0.0223 ::: bot acc: 0.1801
top acc: 0.1201 ::: bot acc: 0.1383
top acc: 0.1045 ::: bot acc: 0.1328
current epoch: 18
train loss is 0.091882
average val loss: 0.060165, accuracy: 0.0715
average test loss: 0.088740, accuracy: 0.0910
case acc: 0.087798074
case acc: 0.0412824
case acc: 0.118063815
case acc: 0.10725605
case acc: 0.09904558
case acc: 0.092773296
top acc: 0.1581 ::: bot acc: 0.0679
top acc: 0.0480 ::: bot acc: 0.0633
top acc: 0.0387 ::: bot acc: 0.2024
top acc: 0.0223 ::: bot acc: 0.1788
top acc: 0.1195 ::: bot acc: 0.1388
top acc: 0.1054 ::: bot acc: 0.1319
current epoch: 19
train loss is 0.090531
average val loss: 0.059784, accuracy: 0.0699
average test loss: 0.088972, accuracy: 0.0906
case acc: 0.085903354
case acc: 0.041433107
case acc: 0.116240196
case acc: 0.108068146
case acc: 0.09910222
case acc: 0.093011856
top acc: 0.1515 ::: bot acc: 0.0738
top acc: 0.0449 ::: bot acc: 0.0663
top acc: 0.0370 ::: bot acc: 0.2005
top acc: 0.0224 ::: bot acc: 0.1800
top acc: 0.1165 ::: bot acc: 0.1418
top acc: 0.1038 ::: bot acc: 0.1336
current epoch: 20
train loss is 0.089552
average val loss: 0.059034, accuracy: 0.0685
average test loss: 0.088479, accuracy: 0.0896
case acc: 0.08498608
case acc: 0.041467253
case acc: 0.11219807
case acc: 0.107024685
case acc: 0.09911675
case acc: 0.092858635
top acc: 0.1482 ::: bot acc: 0.0768
top acc: 0.0450 ::: bot acc: 0.0664
top acc: 0.0335 ::: bot acc: 0.1963
top acc: 0.0223 ::: bot acc: 0.1785
top acc: 0.1162 ::: bot acc: 0.1420
top acc: 0.1047 ::: bot acc: 0.1325
current epoch: 21
train loss is 0.088963
average val loss: 0.058479, accuracy: 0.0674
average test loss: 0.088180, accuracy: 0.0888
case acc: 0.08415675
case acc: 0.04146258
case acc: 0.10896272
case acc: 0.10631037
case acc: 0.09908812
case acc: 0.092763096
top acc: 0.1448 ::: bot acc: 0.0800
top acc: 0.0447 ::: bot acc: 0.0667
top acc: 0.0308 ::: bot acc: 0.1927
top acc: 0.0221 ::: bot acc: 0.1775
top acc: 0.1155 ::: bot acc: 0.1426
top acc: 0.1050 ::: bot acc: 0.1322
current epoch: 22
train loss is 0.087685
average val loss: 0.058134, accuracy: 0.0664
average test loss: 0.088129, accuracy: 0.0882
case acc: 0.08333832
case acc: 0.041501615
case acc: 0.106655635
case acc: 0.106116794
case acc: 0.099106416
case acc: 0.09274534
top acc: 0.1410 ::: bot acc: 0.0834
top acc: 0.0439 ::: bot acc: 0.0675
top acc: 0.0290 ::: bot acc: 0.1902
top acc: 0.0220 ::: bot acc: 0.1772
top acc: 0.1141 ::: bot acc: 0.1440
top acc: 0.1046 ::: bot acc: 0.1325
current epoch: 23
train loss is 0.087327
average val loss: 0.057905, accuracy: 0.0656
average test loss: 0.088179, accuracy: 0.0879
case acc: 0.082745746
case acc: 0.04161728
case acc: 0.104805335
case acc: 0.106105104
case acc: 0.09917819
case acc: 0.09281013
top acc: 0.1370 ::: bot acc: 0.0874
top acc: 0.0431 ::: bot acc: 0.0684
top acc: 0.0276 ::: bot acc: 0.1881
top acc: 0.0219 ::: bot acc: 0.1771
top acc: 0.1125 ::: bot acc: 0.1454
top acc: 0.1041 ::: bot acc: 0.1330
current epoch: 24
train loss is 0.086423
average val loss: 0.057909, accuracy: 0.0649
average test loss: 0.088464, accuracy: 0.0878
case acc: 0.0822052
case acc: 0.041703872
case acc: 0.103842095
case acc: 0.106484376
case acc: 0.099332385
case acc: 0.09297028
top acc: 0.1328 ::: bot acc: 0.0917
top acc: 0.0418 ::: bot acc: 0.0697
top acc: 0.0269 ::: bot acc: 0.1870
top acc: 0.0219 ::: bot acc: 0.1778
top acc: 0.1105 ::: bot acc: 0.1476
top acc: 0.1029 ::: bot acc: 0.1342
current epoch: 25
train loss is 0.085768
average val loss: 0.058057, accuracy: 0.0646
average test loss: 0.088839, accuracy: 0.0878
case acc: 0.081896596
case acc: 0.04187786
case acc: 0.103228346
case acc: 0.10706418
case acc: 0.099535495
case acc: 0.09326431
top acc: 0.1285 ::: bot acc: 0.0958
top acc: 0.0405 ::: bot acc: 0.0710
top acc: 0.0266 ::: bot acc: 0.1862
top acc: 0.0220 ::: bot acc: 0.1786
top acc: 0.1084 ::: bot acc: 0.1499
top acc: 0.1016 ::: bot acc: 0.1357
current epoch: 26
train loss is 0.085286
average val loss: 0.057573, accuracy: 0.0640
average test loss: 0.088335, accuracy: 0.0870
case acc: 0.08182745
case acc: 0.041708425
case acc: 0.10033676
case acc: 0.105782785
case acc: 0.099490546
case acc: 0.09308154
top acc: 0.1274 ::: bot acc: 0.0969
top acc: 0.0422 ::: bot acc: 0.0694
top acc: 0.0246 ::: bot acc: 0.1829
top acc: 0.0221 ::: bot acc: 0.1766
top acc: 0.1091 ::: bot acc: 0.1492
top acc: 0.1030 ::: bot acc: 0.1343
current epoch: 27
train loss is 0.084923
average val loss: 0.057261, accuracy: 0.0635
average test loss: 0.088023, accuracy: 0.0865
case acc: 0.08172009
case acc: 0.0415815
case acc: 0.098195456
case acc: 0.10488613
case acc: 0.099477895
case acc: 0.09302107
top acc: 0.1259 ::: bot acc: 0.0983
top acc: 0.0434 ::: bot acc: 0.0682
top acc: 0.0233 ::: bot acc: 0.1803
top acc: 0.0223 ::: bot acc: 0.1751
top acc: 0.1093 ::: bot acc: 0.1490
top acc: 0.1039 ::: bot acc: 0.1335
current epoch: 28
train loss is 0.084775
average val loss: 0.057485, accuracy: 0.0635
average test loss: 0.088407, accuracy: 0.0866
case acc: 0.081785746
case acc: 0.041685063
case acc: 0.0980614
case acc: 0.105374575
case acc: 0.099621765
case acc: 0.09326274
top acc: 0.1224 ::: bot acc: 0.1017
top acc: 0.0425 ::: bot acc: 0.0691
top acc: 0.0233 ::: bot acc: 0.1801
top acc: 0.0223 ::: bot acc: 0.1759
top acc: 0.1074 ::: bot acc: 0.1509
top acc: 0.1026 ::: bot acc: 0.1349
current epoch: 29
train loss is 0.084330
average val loss: 0.057659, accuracy: 0.0634
average test loss: 0.088680, accuracy: 0.0867
case acc: 0.082068354
case acc: 0.04170834
case acc: 0.097782545
case acc: 0.105601765
case acc: 0.09972044
case acc: 0.09338719
top acc: 0.1194 ::: bot acc: 0.1047
top acc: 0.0421 ::: bot acc: 0.0695
top acc: 0.0231 ::: bot acc: 0.1798
top acc: 0.0222 ::: bot acc: 0.1762
top acc: 0.1060 ::: bot acc: 0.1523
top acc: 0.1017 ::: bot acc: 0.1359
current epoch: 30
train loss is 0.084141
average val loss: 0.057981, accuracy: 0.0636
average test loss: 0.089100, accuracy: 0.0870
case acc: 0.0824194
case acc: 0.041786604
case acc: 0.09795998
case acc: 0.10610261
case acc: 0.09988424
case acc: 0.09360127
top acc: 0.1163 ::: bot acc: 0.1078
top acc: 0.0413 ::: bot acc: 0.0702
top acc: 0.0232 ::: bot acc: 0.1800
top acc: 0.0221 ::: bot acc: 0.1770
top acc: 0.1041 ::: bot acc: 0.1542
top acc: 0.1003 ::: bot acc: 0.1373
current epoch: 31
train loss is 0.083665
average val loss: 0.058108, accuracy: 0.0637
average test loss: 0.089248, accuracy: 0.0869
case acc: 0.08262415
case acc: 0.04175131
case acc: 0.09752554
case acc: 0.10609414
case acc: 0.09997298
case acc: 0.09370322
top acc: 0.1143 ::: bot acc: 0.1098
top acc: 0.0416 ::: bot acc: 0.0699
top acc: 0.0229 ::: bot acc: 0.1795
top acc: 0.0221 ::: bot acc: 0.1770
top acc: 0.1031 ::: bot acc: 0.1551
top acc: 0.0997 ::: bot acc: 0.1380
current epoch: 32
train loss is 0.083697
average val loss: 0.057863, accuracy: 0.0635
average test loss: 0.088936, accuracy: 0.0865
case acc: 0.08270251
case acc: 0.04159939
case acc: 0.09609043
case acc: 0.10517173
case acc: 0.0999315
case acc: 0.09358915
top acc: 0.1139 ::: bot acc: 0.1102
top acc: 0.0434 ::: bot acc: 0.0683
top acc: 0.0222 ::: bot acc: 0.1777
top acc: 0.0222 ::: bot acc: 0.1755
top acc: 0.1036 ::: bot acc: 0.1546
top acc: 0.1004 ::: bot acc: 0.1372
current epoch: 33
train loss is 0.083699
average val loss: 0.057571, accuracy: 0.0633
average test loss: 0.088569, accuracy: 0.0860
case acc: 0.082733296
case acc: 0.04148927
case acc: 0.09458873
case acc: 0.10413885
case acc: 0.09985663
case acc: 0.09346033
top acc: 0.1134 ::: bot acc: 0.1106
top acc: 0.0451 ::: bot acc: 0.0666
top acc: 0.0215 ::: bot acc: 0.1758
top acc: 0.0223 ::: bot acc: 0.1739
top acc: 0.1043 ::: bot acc: 0.1539
top acc: 0.1015 ::: bot acc: 0.1362
current epoch: 34
train loss is 0.083266
average val loss: 0.057879, accuracy: 0.0635
average test loss: 0.088965, accuracy: 0.0863
case acc: 0.08317925
case acc: 0.041559543
case acc: 0.09503674
case acc: 0.104474485
case acc: 0.09999465
case acc: 0.0935787
top acc: 0.1105 ::: bot acc: 0.1137
top acc: 0.0444 ::: bot acc: 0.0675
top acc: 0.0216 ::: bot acc: 0.1764
top acc: 0.0221 ::: bot acc: 0.1745
top acc: 0.1028 ::: bot acc: 0.1553
top acc: 0.1002 ::: bot acc: 0.1373
current epoch: 35
train loss is 0.083082
average val loss: 0.058417, accuracy: 0.0639
average test loss: 0.089637, accuracy: 0.0868
case acc: 0.083657056
case acc: 0.041718468
case acc: 0.09622741
case acc: 0.10530121
case acc: 0.100223854
case acc: 0.09380658
top acc: 0.1071 ::: bot acc: 0.1174
top acc: 0.0429 ::: bot acc: 0.0691
top acc: 0.0221 ::: bot acc: 0.1780
top acc: 0.0219 ::: bot acc: 0.1758
top acc: 0.1006 ::: bot acc: 0.1574
top acc: 0.0982 ::: bot acc: 0.1392
current epoch: 36
train loss is 0.082808
average val loss: 0.057782, accuracy: 0.0634
average test loss: 0.088868, accuracy: 0.0860
case acc: 0.083521195
case acc: 0.04149321
case acc: 0.09413293
case acc: 0.10351025
case acc: 0.09997769
case acc: 0.0935034
top acc: 0.1081 ::: bot acc: 0.1164
top acc: 0.0457 ::: bot acc: 0.0662
top acc: 0.0210 ::: bot acc: 0.1753
top acc: 0.0221 ::: bot acc: 0.1731
top acc: 0.1026 ::: bot acc: 0.1554
top acc: 0.1005 ::: bot acc: 0.1370
current epoch: 37
train loss is 0.082702
average val loss: 0.057943, accuracy: 0.0636
average test loss: 0.089042, accuracy: 0.0861
case acc: 0.08367188
case acc: 0.04141601
case acc: 0.09434531
case acc: 0.10352238
case acc: 0.10007911
case acc: 0.093558975
top acc: 0.1065 ::: bot acc: 0.1179
top acc: 0.0459 ::: bot acc: 0.0659
top acc: 0.0211 ::: bot acc: 0.1756
top acc: 0.0221 ::: bot acc: 0.1731
top acc: 0.1018 ::: bot acc: 0.1562
top acc: 0.0998 ::: bot acc: 0.1377
current epoch: 38
train loss is 0.082678
average val loss: 0.058054, accuracy: 0.0638
average test loss: 0.089144, accuracy: 0.0862
case acc: 0.08381559
case acc: 0.041395262
case acc: 0.09445553
case acc: 0.10347772
case acc: 0.10021324
case acc: 0.0937083
top acc: 0.1051 ::: bot acc: 0.1191
top acc: 0.0462 ::: bot acc: 0.0656
top acc: 0.0213 ::: bot acc: 0.1756
top acc: 0.0222 ::: bot acc: 0.1730
top acc: 0.1014 ::: bot acc: 0.1569
top acc: 0.0996 ::: bot acc: 0.1380
current epoch: 39
train loss is 0.082428
average val loss: 0.058370, accuracy: 0.0641
average test loss: 0.089515, accuracy: 0.0865
case acc: 0.083994046
case acc: 0.041437764
case acc: 0.09519075
case acc: 0.103947245
case acc: 0.10036662
case acc: 0.09395448
top acc: 0.1030 ::: bot acc: 0.1210
top acc: 0.0458 ::: bot acc: 0.0660
top acc: 0.0217 ::: bot acc: 0.1765
top acc: 0.0223 ::: bot acc: 0.1736
top acc: 0.1001 ::: bot acc: 0.1582
top acc: 0.0986 ::: bot acc: 0.1392
current epoch: 40
train loss is 0.082289
average val loss: 0.058701, accuracy: 0.0644
average test loss: 0.089888, accuracy: 0.0868
case acc: 0.08424527
case acc: 0.0414361
case acc: 0.09592684
case acc: 0.104326226
case acc: 0.10056567
case acc: 0.094123274
top acc: 0.1010 ::: bot acc: 0.1230
top acc: 0.0452 ::: bot acc: 0.0665
top acc: 0.0221 ::: bot acc: 0.1775
top acc: 0.0223 ::: bot acc: 0.1741
top acc: 0.0989 ::: bot acc: 0.1594
top acc: 0.0975 ::: bot acc: 0.1404
current epoch: 41
train loss is 0.082266
average val loss: 0.059328, accuracy: 0.0650
average test loss: 0.090590, accuracy: 0.0874
case acc: 0.084676675
case acc: 0.041535087
case acc: 0.097379014
case acc: 0.105262935
case acc: 0.10095846
case acc: 0.09441488
top acc: 0.0981 ::: bot acc: 0.1258
top acc: 0.0438 ::: bot acc: 0.0680
top acc: 0.0229 ::: bot acc: 0.1792
top acc: 0.0221 ::: bot acc: 0.1756
top acc: 0.0968 ::: bot acc: 0.1615
top acc: 0.0955 ::: bot acc: 0.1423
current epoch: 42
train loss is 0.081884
average val loss: 0.058757, accuracy: 0.0646
average test loss: 0.089925, accuracy: 0.0867
case acc: 0.08445271
case acc: 0.04140486
case acc: 0.09586535
case acc: 0.10383758
case acc: 0.10068798
case acc: 0.09414977
top acc: 0.0995 ::: bot acc: 0.1244
top acc: 0.0463 ::: bot acc: 0.0655
top acc: 0.0222 ::: bot acc: 0.1773
top acc: 0.0223 ::: bot acc: 0.1734
top acc: 0.0985 ::: bot acc: 0.1599
top acc: 0.0973 ::: bot acc: 0.1405
current epoch: 43
train loss is 0.082157
average val loss: 0.058558, accuracy: 0.0645
average test loss: 0.089667, accuracy: 0.0865
case acc: 0.08438713
case acc: 0.041384183
case acc: 0.09526343
case acc: 0.103135526
case acc: 0.100576706
case acc: 0.09412642
top acc: 0.0996 ::: bot acc: 0.1242
top acc: 0.0477 ::: bot acc: 0.0642
top acc: 0.0219 ::: bot acc: 0.1765
top acc: 0.0224 ::: bot acc: 0.1723
top acc: 0.0991 ::: bot acc: 0.1593
top acc: 0.0980 ::: bot acc: 0.1400
current epoch: 44
train loss is 0.081987
average val loss: 0.057905, accuracy: 0.0639
average test loss: 0.088846, accuracy: 0.0857
case acc: 0.08411935
case acc: 0.04125919
case acc: 0.093388304
case acc: 0.10132412
case acc: 0.10022907
case acc: 0.09370826
top acc: 0.1015 ::: bot acc: 0.1225
top acc: 0.0505 ::: bot acc: 0.0612
top acc: 0.0209 ::: bot acc: 0.1743
top acc: 0.0227 ::: bot acc: 0.1694
top acc: 0.1014 ::: bot acc: 0.1569
top acc: 0.1003 ::: bot acc: 0.1377
current epoch: 45
train loss is 0.081924
average val loss: 0.057851, accuracy: 0.0639
average test loss: 0.088761, accuracy: 0.0856
case acc: 0.08421556
case acc: 0.041266475
case acc: 0.0931872
case acc: 0.100932874
case acc: 0.100169934
case acc: 0.09367258
top acc: 0.1012 ::: bot acc: 0.1228
top acc: 0.0512 ::: bot acc: 0.0604
top acc: 0.0207 ::: bot acc: 0.1741
top acc: 0.0228 ::: bot acc: 0.1688
top acc: 0.1016 ::: bot acc: 0.1566
top acc: 0.1003 ::: bot acc: 0.1376
current epoch: 46
train loss is 0.081849
average val loss: 0.057683, accuracy: 0.0638
average test loss: 0.088510, accuracy: 0.0853
case acc: 0.08416756
case acc: 0.04125677
case acc: 0.092571735
case acc: 0.10025068
case acc: 0.10004967
case acc: 0.0935232
top acc: 0.1017 ::: bot acc: 0.1225
top acc: 0.0525 ::: bot acc: 0.0591
top acc: 0.0204 ::: bot acc: 0.1733
top acc: 0.0230 ::: bot acc: 0.1678
top acc: 0.1022 ::: bot acc: 0.1558
top acc: 0.1009 ::: bot acc: 0.1370
current epoch: 47
train loss is 0.081959
average val loss: 0.057494, accuracy: 0.0636
average test loss: 0.088222, accuracy: 0.0850
case acc: 0.084103435
case acc: 0.041340813
case acc: 0.09180141
case acc: 0.09953494
case acc: 0.099977955
case acc: 0.09347016
top acc: 0.1022 ::: bot acc: 0.1220
top acc: 0.0538 ::: bot acc: 0.0578
top acc: 0.0201 ::: bot acc: 0.1723
top acc: 0.0233 ::: bot acc: 0.1666
top acc: 0.1030 ::: bot acc: 0.1551
top acc: 0.1017 ::: bot acc: 0.1363
current epoch: 48
train loss is 0.081662
average val loss: 0.058194, accuracy: 0.0643
average test loss: 0.089108, accuracy: 0.0858
case acc: 0.08458495
case acc: 0.041265607
case acc: 0.09383209
case acc: 0.100931756
case acc: 0.10036758
case acc: 0.09395761
top acc: 0.0989 ::: bot acc: 0.1251
top acc: 0.0514 ::: bot acc: 0.0603
top acc: 0.0210 ::: bot acc: 0.1749
top acc: 0.0228 ::: bot acc: 0.1688
top acc: 0.1002 ::: bot acc: 0.1580
top acc: 0.0990 ::: bot acc: 0.1390
current epoch: 49
train loss is 0.081537
average val loss: 0.058550, accuracy: 0.0646
average test loss: 0.089543, accuracy: 0.0862
case acc: 0.08481675
case acc: 0.041298494
case acc: 0.09491705
case acc: 0.101497285
case acc: 0.100567885
case acc: 0.0941981
top acc: 0.0971 ::: bot acc: 0.1268
top acc: 0.0504 ::: bot acc: 0.0614
top acc: 0.0216 ::: bot acc: 0.1762
top acc: 0.0226 ::: bot acc: 0.1697
top acc: 0.0989 ::: bot acc: 0.1594
top acc: 0.0978 ::: bot acc: 0.1403
current epoch: 50
train loss is 0.081655
average val loss: 0.058185, accuracy: 0.0644
average test loss: 0.089100, accuracy: 0.0859
case acc: 0.084737375
case acc: 0.041461136
case acc: 0.09401681
case acc: 0.10052132
case acc: 0.100367144
case acc: 0.094149426
top acc: 0.0980 ::: bot acc: 0.1260
top acc: 0.0522 ::: bot acc: 0.0600
top acc: 0.0213 ::: bot acc: 0.1750
top acc: 0.0231 ::: bot acc: 0.1680
top acc: 0.1002 ::: bot acc: 0.1580
top acc: 0.0991 ::: bot acc: 0.1391
LME_Co_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5112 5112 5112
1.7082474 -0.6288155 0.48738334 -0.33910522
Validation: 570 570 570
Testing: 774 774 774
pre-processing time: 0.00042557716369628906
the split date is 2011-07-01
net initializing with time: 0.003695964813232422
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.288951
average val loss: 0.107504, accuracy: 0.0853
average test loss: 0.109594, accuracy: 0.1145
case acc: 0.075850785
case acc: 0.17266816
case acc: 0.055529676
case acc: 0.16615416
case acc: 0.12666154
case acc: 0.090244636
top acc: 0.0388 ::: bot acc: 0.1301
top acc: 0.1078 ::: bot acc: 0.2255
top acc: 0.1006 ::: bot acc: 0.0416
top acc: 0.2738 ::: bot acc: 0.0678
top acc: 0.1161 ::: bot acc: 0.1610
top acc: 0.1647 ::: bot acc: 0.0236
current epoch: 2
train loss is 0.211425
average val loss: 0.144262, accuracy: 0.1307
average test loss: 0.139638, accuracy: 0.1429
case acc: 0.15850541
case acc: 0.28572193
case acc: 0.08860934
case acc: 0.08682892
case acc: 0.17830555
case acc: 0.05943976
top acc: 0.0734 ::: bot acc: 0.2373
top acc: 0.2210 ::: bot acc: 0.3392
top acc: 0.0319 ::: bot acc: 0.1493
top acc: 0.1663 ::: bot acc: 0.0434
top acc: 0.0586 ::: bot acc: 0.2678
top acc: 0.0594 ::: bot acc: 0.0930
current epoch: 3
train loss is 0.167410
average val loss: 0.111139, accuracy: 0.0951
average test loss: 0.115810, accuracy: 0.1189
case acc: 0.061610058
case acc: 0.11837695
case acc: 0.07862638
case acc: 0.21069255
case acc: 0.10886781
case acc: 0.13514999
top acc: 0.0840 ::: bot acc: 0.0801
top acc: 0.0537 ::: bot acc: 0.1721
top acc: 0.1467 ::: bot acc: 0.0206
top acc: 0.3179 ::: bot acc: 0.1107
top acc: 0.1610 ::: bot acc: 0.1129
top acc: 0.2139 ::: bot acc: 0.0610
current epoch: 4
train loss is 0.133301
average val loss: 0.128480, accuracy: 0.1138
average test loss: 0.124503, accuracy: 0.1286
case acc: 0.135631
case acc: 0.23984438
case acc: 0.079940505
case acc: 0.091814056
case acc: 0.16647951
case acc: 0.057920545
top acc: 0.0537 ::: bot acc: 0.2137
top acc: 0.1750 ::: bot acc: 0.2940
top acc: 0.0307 ::: bot acc: 0.1367
top acc: 0.1751 ::: bot acc: 0.0388
top acc: 0.0628 ::: bot acc: 0.2487
top acc: 0.0751 ::: bot acc: 0.0780
current epoch: 5
train loss is 0.133474
average val loss: 0.097342, accuracy: 0.0767
average test loss: 0.099266, accuracy: 0.1036
case acc: 0.07212918
case acc: 0.1361577
case acc: 0.052652936
case acc: 0.14983176
case acc: 0.12727995
case acc: 0.08373646
top acc: 0.0415 ::: bot acc: 0.1247
top acc: 0.0712 ::: bot acc: 0.1905
top acc: 0.0866 ::: bot acc: 0.0565
top acc: 0.2534 ::: bot acc: 0.0560
top acc: 0.1158 ::: bot acc: 0.1636
top acc: 0.1579 ::: bot acc: 0.0196
current epoch: 6
train loss is 0.114222
average val loss: 0.106228, accuracy: 0.0903
average test loss: 0.103852, accuracy: 0.1085
case acc: 0.10175344
case acc: 0.17083822
case acc: 0.06716224
case acc: 0.10299209
case acc: 0.14917077
case acc: 0.058935292
top acc: 0.0343 ::: bot acc: 0.1727
top acc: 0.1059 ::: bot acc: 0.2252
top acc: 0.0355 ::: bot acc: 0.1150
top acc: 0.1927 ::: bot acc: 0.0366
top acc: 0.0774 ::: bot acc: 0.2158
top acc: 0.1023 ::: bot acc: 0.0512
current epoch: 7
train loss is 0.116965
average val loss: 0.093734, accuracy: 0.0773
average test loss: 0.093190, accuracy: 0.0972
case acc: 0.08046672
case acc: 0.12591974
case acc: 0.05785448
case acc: 0.11648751
case acc: 0.1374873
case acc: 0.06526189
top acc: 0.0338 ::: bot acc: 0.1411
top acc: 0.0609 ::: bot acc: 0.1802
top acc: 0.0485 ::: bot acc: 0.0945
top acc: 0.2113 ::: bot acc: 0.0398
top acc: 0.0959 ::: bot acc: 0.1891
top acc: 0.1264 ::: bot acc: 0.0272
current epoch: 8
train loss is 0.104537
average val loss: 0.091060, accuracy: 0.0764
average test loss: 0.090089, accuracy: 0.0936
case acc: 0.078657985
case acc: 0.11064176
case acc: 0.060875416
case acc: 0.10926823
case acc: 0.13812704
case acc: 0.06382889
top acc: 0.0342 ::: bot acc: 0.1381
top acc: 0.0467 ::: bot acc: 0.1645
top acc: 0.0423 ::: bot acc: 0.1021
top acc: 0.2016 ::: bot acc: 0.0375
top acc: 0.0948 ::: bot acc: 0.1907
top acc: 0.1218 ::: bot acc: 0.0319
current epoch: 9
train loss is 0.100940
average val loss: 0.090058, accuracy: 0.0773
average test loss: 0.088459, accuracy: 0.0914
case acc: 0.079177015
case acc: 0.10063959
case acc: 0.066272445
case acc: 0.10025815
case acc: 0.14032376
case acc: 0.061507612
top acc: 0.0339 ::: bot acc: 0.1391
top acc: 0.0389 ::: bot acc: 0.1534
top acc: 0.0361 ::: bot acc: 0.1132
top acc: 0.1885 ::: bot acc: 0.0365
top acc: 0.0912 ::: bot acc: 0.1959
top acc: 0.1136 ::: bot acc: 0.0401
current epoch: 10
train loss is 0.097009
average val loss: 0.085562, accuracy: 0.0730
average test loss: 0.084285, accuracy: 0.0866
case acc: 0.07288629
case acc: 0.0813468
case acc: 0.06504603
case acc: 0.10068246
case acc: 0.1368926
case acc: 0.06297602
top acc: 0.0397 ::: bot acc: 0.1268
top acc: 0.0286 ::: bot acc: 0.1297
top acc: 0.0372 ::: bot acc: 0.1107
top acc: 0.1891 ::: bot acc: 0.0365
top acc: 0.0972 ::: bot acc: 0.1878
top acc: 0.1190 ::: bot acc: 0.0348
current epoch: 11
train loss is 0.091851
average val loss: 0.082949, accuracy: 0.0708
average test loss: 0.081727, accuracy: 0.0836
case acc: 0.06999604
case acc: 0.0684948
case acc: 0.06593115
case acc: 0.09842314
case acc: 0.13540663
case acc: 0.06320427
top acc: 0.0455 ::: bot acc: 0.1195
top acc: 0.0263 ::: bot acc: 0.1116
top acc: 0.0363 ::: bot acc: 0.1124
top acc: 0.1855 ::: bot acc: 0.0366
top acc: 0.1000 ::: bot acc: 0.1842
top acc: 0.1197 ::: bot acc: 0.0342
current epoch: 12
train loss is 0.087781
average val loss: 0.081596, accuracy: 0.0703
average test loss: 0.080269, accuracy: 0.0817
case acc: 0.0687613
case acc: 0.061069924
case acc: 0.06840306
case acc: 0.09452857
case acc: 0.13523726
case acc: 0.06248812
top acc: 0.0487 ::: bot acc: 0.1160
top acc: 0.0301 ::: bot acc: 0.0985
top acc: 0.0344 ::: bot acc: 0.1171
top acc: 0.1793 ::: bot acc: 0.0374
top acc: 0.1004 ::: bot acc: 0.1838
top acc: 0.1171 ::: bot acc: 0.0368
current epoch: 13
train loss is 0.084453
average val loss: 0.079100, accuracy: 0.0680
average test loss: 0.078134, accuracy: 0.0795
case acc: 0.0661028
case acc: 0.0526645
case acc: 0.06728641
case acc: 0.09494531
case acc: 0.1324173
case acc: 0.06359675
top acc: 0.0577 ::: bot acc: 0.1069
top acc: 0.0399 ::: bot acc: 0.0810
top acc: 0.0353 ::: bot acc: 0.1149
top acc: 0.1800 ::: bot acc: 0.0373
top acc: 0.1056 ::: bot acc: 0.1770
top acc: 0.1208 ::: bot acc: 0.0332
current epoch: 14
train loss is 0.081931
average val loss: 0.077634, accuracy: 0.0672
average test loss: 0.076880, accuracy: 0.0781
case acc: 0.06464568
case acc: 0.047846723
case acc: 0.067200795
case acc: 0.094281025
case acc: 0.13064116
case acc: 0.063956015
top acc: 0.0635 ::: bot acc: 0.1011
top acc: 0.0513 ::: bot acc: 0.0681
top acc: 0.0355 ::: bot acc: 0.1147
top acc: 0.1788 ::: bot acc: 0.0376
top acc: 0.1092 ::: bot acc: 0.1725
top acc: 0.1220 ::: bot acc: 0.0320
current epoch: 15
train loss is 0.080066
average val loss: 0.076427, accuracy: 0.0666
average test loss: 0.075967, accuracy: 0.0771
case acc: 0.06368943
case acc: 0.045176644
case acc: 0.066499665
case acc: 0.09441658
case acc: 0.12863068
case acc: 0.06447854
top acc: 0.0690 ::: bot acc: 0.0957
top acc: 0.0622 ::: bot acc: 0.0572
top acc: 0.0361 ::: bot acc: 0.1132
top acc: 0.1790 ::: bot acc: 0.0375
top acc: 0.1134 ::: bot acc: 0.1674
top acc: 0.1238 ::: bot acc: 0.0303
current epoch: 16
train loss is 0.079322
average val loss: 0.076024, accuracy: 0.0669
average test loss: 0.075587, accuracy: 0.0767
case acc: 0.0634852
case acc: 0.044418775
case acc: 0.06712671
case acc: 0.09311655
case acc: 0.1278135
case acc: 0.06405451
top acc: 0.0706 ::: bot acc: 0.0941
top acc: 0.0674 ::: bot acc: 0.0521
top acc: 0.0356 ::: bot acc: 0.1144
top acc: 0.1768 ::: bot acc: 0.0381
top acc: 0.1151 ::: bot acc: 0.1653
top acc: 0.1224 ::: bot acc: 0.0316
current epoch: 17
train loss is 0.078878
average val loss: 0.075396, accuracy: 0.0667
average test loss: 0.075153, accuracy: 0.0762
case acc: 0.06311471
case acc: 0.043958426
case acc: 0.06635465
case acc: 0.09347523
case acc: 0.12606308
case acc: 0.064362265
top acc: 0.0738 ::: bot acc: 0.0909
top acc: 0.0728 ::: bot acc: 0.0466
top acc: 0.0364 ::: bot acc: 0.1129
top acc: 0.1774 ::: bot acc: 0.0379
top acc: 0.1188 ::: bot acc: 0.1609
top acc: 0.1235 ::: bot acc: 0.0307
current epoch: 18
train loss is 0.078818
average val loss: 0.074935, accuracy: 0.0666
average test loss: 0.074846, accuracy: 0.0759
case acc: 0.06285177
case acc: 0.043871105
case acc: 0.06563509
case acc: 0.093832426
case acc: 0.12446704
case acc: 0.06455792
top acc: 0.0761 ::: bot acc: 0.0886
top acc: 0.0764 ::: bot acc: 0.0431
top acc: 0.0372 ::: bot acc: 0.1114
top acc: 0.1780 ::: bot acc: 0.0377
top acc: 0.1221 ::: bot acc: 0.1568
top acc: 0.1241 ::: bot acc: 0.0300
current epoch: 19
train loss is 0.078680
average val loss: 0.074973, accuracy: 0.0669
average test loss: 0.074761, accuracy: 0.0756
case acc: 0.06298812
case acc: 0.04387634
case acc: 0.06640804
case acc: 0.092597686
case acc: 0.1241196
case acc: 0.06379811
top acc: 0.0747 ::: bot acc: 0.0899
top acc: 0.0754 ::: bot acc: 0.0441
top acc: 0.0366 ::: bot acc: 0.1128
top acc: 0.1758 ::: bot acc: 0.0384
top acc: 0.1228 ::: bot acc: 0.1559
top acc: 0.1217 ::: bot acc: 0.0325
current epoch: 20
train loss is 0.078609
average val loss: 0.075122, accuracy: 0.0673
average test loss: 0.074749, accuracy: 0.0754
case acc: 0.06318987
case acc: 0.0439479
case acc: 0.067290515
case acc: 0.09132548
case acc: 0.12394504
case acc: 0.06298823
top acc: 0.0727 ::: bot acc: 0.0919
top acc: 0.0732 ::: bot acc: 0.0464
top acc: 0.0358 ::: bot acc: 0.1145
top acc: 0.1733 ::: bot acc: 0.0394
top acc: 0.1232 ::: bot acc: 0.1554
top acc: 0.1190 ::: bot acc: 0.0352
current epoch: 21
train loss is 0.078315
average val loss: 0.075036, accuracy: 0.0674
average test loss: 0.074635, accuracy: 0.0752
case acc: 0.063194074
case acc: 0.043971483
case acc: 0.06734034
case acc: 0.09096474
case acc: 0.123120874
case acc: 0.06269119
top acc: 0.0726 ::: bot acc: 0.0920
top acc: 0.0724 ::: bot acc: 0.0471
top acc: 0.0358 ::: bot acc: 0.1146
top acc: 0.1726 ::: bot acc: 0.0397
top acc: 0.1250 ::: bot acc: 0.1532
top acc: 0.1180 ::: bot acc: 0.0361
current epoch: 22
train loss is 0.078193
average val loss: 0.075420, accuracy: 0.0680
average test loss: 0.074772, accuracy: 0.0752
case acc: 0.06357661
case acc: 0.044216815
case acc: 0.06877836
case acc: 0.08931978
case acc: 0.12336459
case acc: 0.061657324
top acc: 0.0699 ::: bot acc: 0.0947
top acc: 0.0688 ::: bot acc: 0.0507
top acc: 0.0347 ::: bot acc: 0.1173
top acc: 0.1692 ::: bot acc: 0.0414
top acc: 0.1245 ::: bot acc: 0.1538
top acc: 0.1146 ::: bot acc: 0.0395
current epoch: 23
train loss is 0.078128
average val loss: 0.075693, accuracy: 0.0686
average test loss: 0.074877, accuracy: 0.0751
case acc: 0.06384712
case acc: 0.04454788
case acc: 0.06978038
case acc: 0.0881794
case acc: 0.12328115
case acc: 0.06096491
top acc: 0.0683 ::: bot acc: 0.0963
top acc: 0.0661 ::: bot acc: 0.0535
top acc: 0.0340 ::: bot acc: 0.1192
top acc: 0.1669 ::: bot acc: 0.0428
top acc: 0.1247 ::: bot acc: 0.1536
top acc: 0.1122 ::: bot acc: 0.0418
current epoch: 24
train loss is 0.077786
average val loss: 0.076110, accuracy: 0.0694
average test loss: 0.075089, accuracy: 0.0752
case acc: 0.06420801
case acc: 0.04502922
case acc: 0.07119708
case acc: 0.086788446
case acc: 0.123467855
case acc: 0.060249113
top acc: 0.0663 ::: bot acc: 0.0983
top acc: 0.0631 ::: bot acc: 0.0564
top acc: 0.0330 ::: bot acc: 0.1218
top acc: 0.1638 ::: bot acc: 0.0447
top acc: 0.1243 ::: bot acc: 0.1540
top acc: 0.1092 ::: bot acc: 0.0448
current epoch: 25
train loss is 0.077607
average val loss: 0.076466, accuracy: 0.0701
average test loss: 0.075284, accuracy: 0.0753
case acc: 0.06448101
case acc: 0.045480855
case acc: 0.07258177
case acc: 0.08568253
case acc: 0.12356814
case acc: 0.05971441
top acc: 0.0649 ::: bot acc: 0.0998
top acc: 0.0609 ::: bot acc: 0.0586
top acc: 0.0325 ::: bot acc: 0.1241
top acc: 0.1611 ::: bot acc: 0.0469
top acc: 0.1241 ::: bot acc: 0.1542
top acc: 0.1069 ::: bot acc: 0.0471
current epoch: 26
train loss is 0.077369
average val loss: 0.076436, accuracy: 0.0704
average test loss: 0.075235, accuracy: 0.0751
case acc: 0.06440494
case acc: 0.04556565
case acc: 0.072986916
case acc: 0.08527529
case acc: 0.123044744
case acc: 0.05958018
top acc: 0.0652 ::: bot acc: 0.0994
top acc: 0.0603 ::: bot acc: 0.0591
top acc: 0.0323 ::: bot acc: 0.1248
top acc: 0.1600 ::: bot acc: 0.0478
top acc: 0.1253 ::: bot acc: 0.1529
top acc: 0.1064 ::: bot acc: 0.0476
current epoch: 27
train loss is 0.077324
average val loss: 0.076595, accuracy: 0.0709
average test loss: 0.075317, accuracy: 0.0751
case acc: 0.06448059
case acc: 0.045754477
case acc: 0.07392098
case acc: 0.084522225
case acc: 0.12290664
case acc: 0.059297066
top acc: 0.0648 ::: bot acc: 0.0999
top acc: 0.0593 ::: bot acc: 0.0602
top acc: 0.0320 ::: bot acc: 0.1263
top acc: 0.1580 ::: bot acc: 0.0494
top acc: 0.1256 ::: bot acc: 0.1526
top acc: 0.1052 ::: bot acc: 0.0488
current epoch: 28
train loss is 0.077204
average val loss: 0.077069, accuracy: 0.0718
average test loss: 0.075617, accuracy: 0.0753
case acc: 0.06476665
case acc: 0.046168372
case acc: 0.07564647
case acc: 0.083312154
case acc: 0.12330121
case acc: 0.058852497
top acc: 0.0633 ::: bot acc: 0.1014
top acc: 0.0574 ::: bot acc: 0.0621
top acc: 0.0314 ::: bot acc: 0.1292
top acc: 0.1548 ::: bot acc: 0.0521
top acc: 0.1248 ::: bot acc: 0.1535
top acc: 0.1027 ::: bot acc: 0.0512
current epoch: 29
train loss is 0.077019
average val loss: 0.077235, accuracy: 0.0723
average test loss: 0.075708, accuracy: 0.0754
case acc: 0.06483141
case acc: 0.046299823
case acc: 0.076599956
case acc: 0.08264952
case acc: 0.1232404
case acc: 0.058713567
top acc: 0.0632 ::: bot acc: 0.1016
top acc: 0.0569 ::: bot acc: 0.0626
top acc: 0.0311 ::: bot acc: 0.1308
top acc: 0.1531 ::: bot acc: 0.0538
top acc: 0.1249 ::: bot acc: 0.1534
top acc: 0.1017 ::: bot acc: 0.0522
current epoch: 30
train loss is 0.076735
average val loss: 0.077744, accuracy: 0.0732
average test loss: 0.076050, accuracy: 0.0757
case acc: 0.0651619
case acc: 0.046643846
case acc: 0.07835201
case acc: 0.081618056
case acc: 0.12372716
case acc: 0.058421955
top acc: 0.0617 ::: bot acc: 0.1031
top acc: 0.0555 ::: bot acc: 0.0640
top acc: 0.0307 ::: bot acc: 0.1337
top acc: 0.1500 ::: bot acc: 0.0567
top acc: 0.1239 ::: bot acc: 0.1546
top acc: 0.0995 ::: bot acc: 0.0544
current epoch: 31
train loss is 0.076720
average val loss: 0.077576, accuracy: 0.0733
average test loss: 0.075912, accuracy: 0.0755
case acc: 0.064928114
case acc: 0.04625366
case acc: 0.07865221
case acc: 0.08141985
case acc: 0.123312846
case acc: 0.058422178
top acc: 0.0627 ::: bot acc: 0.1021
top acc: 0.0571 ::: bot acc: 0.0624
top acc: 0.0306 ::: bot acc: 0.1341
top acc: 0.1492 ::: bot acc: 0.0575
top acc: 0.1248 ::: bot acc: 0.1536
top acc: 0.0997 ::: bot acc: 0.0542
current epoch: 32
train loss is 0.076479
average val loss: 0.077346, accuracy: 0.0734
average test loss: 0.075732, accuracy: 0.0753
case acc: 0.06471704
case acc: 0.045885272
case acc: 0.078738816
case acc: 0.081332095
case acc: 0.12282063
case acc: 0.05847635
top acc: 0.0639 ::: bot acc: 0.1009
top acc: 0.0587 ::: bot acc: 0.0608
top acc: 0.0306 ::: bot acc: 0.1343
top acc: 0.1489 ::: bot acc: 0.0578
top acc: 0.1258 ::: bot acc: 0.1523
top acc: 0.1002 ::: bot acc: 0.0536
current epoch: 33
train loss is 0.076385
average val loss: 0.076945, accuracy: 0.0732
average test loss: 0.075434, accuracy: 0.0750
case acc: 0.064436965
case acc: 0.045362376
case acc: 0.07832604
case acc: 0.08144734
case acc: 0.122081965
case acc: 0.058611188
top acc: 0.0656 ::: bot acc: 0.0993
top acc: 0.0612 ::: bot acc: 0.0583
top acc: 0.0307 ::: bot acc: 0.1336
top acc: 0.1493 ::: bot acc: 0.0574
top acc: 0.1274 ::: bot acc: 0.1504
top acc: 0.1013 ::: bot acc: 0.0525
current epoch: 34
train loss is 0.076297
average val loss: 0.076843, accuracy: 0.0733
average test loss: 0.075351, accuracy: 0.0749
case acc: 0.06437971
case acc: 0.04513437
case acc: 0.07842213
case acc: 0.081343286
case acc: 0.12177807
case acc: 0.058600727
top acc: 0.0659 ::: bot acc: 0.0990
top acc: 0.0624 ::: bot acc: 0.0571
top acc: 0.0306 ::: bot acc: 0.1337
top acc: 0.1489 ::: bot acc: 0.0578
top acc: 0.1281 ::: bot acc: 0.1496
top acc: 0.1014 ::: bot acc: 0.0525
current epoch: 35
train loss is 0.076237
average val loss: 0.076681, accuracy: 0.0732
average test loss: 0.075230, accuracy: 0.0748
case acc: 0.06430914
case acc: 0.044881582
case acc: 0.078215964
case acc: 0.0813601
case acc: 0.12137243
case acc: 0.058622483
top acc: 0.0662 ::: bot acc: 0.0987
top acc: 0.0637 ::: bot acc: 0.0559
top acc: 0.0307 ::: bot acc: 0.1334
top acc: 0.1489 ::: bot acc: 0.0577
top acc: 0.1289 ::: bot acc: 0.1486
top acc: 0.1016 ::: bot acc: 0.0522
current epoch: 36
train loss is 0.076164
average val loss: 0.076498, accuracy: 0.0730
average test loss: 0.075097, accuracy: 0.0746
case acc: 0.06425686
case acc: 0.04462958
case acc: 0.077870235
case acc: 0.08144067
case acc: 0.12096166
case acc: 0.05866045
top acc: 0.0665 ::: bot acc: 0.0984
top acc: 0.0652 ::: bot acc: 0.0544
top acc: 0.0307 ::: bot acc: 0.1329
top acc: 0.1491 ::: bot acc: 0.0575
top acc: 0.1298 ::: bot acc: 0.1475
top acc: 0.1019 ::: bot acc: 0.0519
current epoch: 37
train loss is 0.076112
average val loss: 0.076473, accuracy: 0.0730
average test loss: 0.075072, accuracy: 0.0746
case acc: 0.064328074
case acc: 0.04449548
case acc: 0.077883594
case acc: 0.081367716
case acc: 0.12082907
case acc: 0.05862054
top acc: 0.0661 ::: bot acc: 0.0988
top acc: 0.0662 ::: bot acc: 0.0534
top acc: 0.0307 ::: bot acc: 0.1329
top acc: 0.1489 ::: bot acc: 0.0577
top acc: 0.1301 ::: bot acc: 0.1471
top acc: 0.1015 ::: bot acc: 0.0523
current epoch: 38
train loss is 0.076080
average val loss: 0.076366, accuracy: 0.0729
average test loss: 0.074993, accuracy: 0.0745
case acc: 0.064339414
case acc: 0.04436524
case acc: 0.077665456
case acc: 0.08139488
case acc: 0.1205835
case acc: 0.05862755
top acc: 0.0661 ::: bot acc: 0.0988
top acc: 0.0673 ::: bot acc: 0.0522
top acc: 0.0308 ::: bot acc: 0.1325
top acc: 0.1490 ::: bot acc: 0.0576
top acc: 0.1307 ::: bot acc: 0.1465
top acc: 0.1016 ::: bot acc: 0.0522
current epoch: 39
train loss is 0.076081
average val loss: 0.076271, accuracy: 0.0728
average test loss: 0.074924, accuracy: 0.0744
case acc: 0.06437431
case acc: 0.044274203
case acc: 0.0773724
case acc: 0.08145351
case acc: 0.1203441
case acc: 0.05863614
top acc: 0.0659 ::: bot acc: 0.0990
top acc: 0.0682 ::: bot acc: 0.0513
top acc: 0.0309 ::: bot acc: 0.1320
top acc: 0.1493 ::: bot acc: 0.0573
top acc: 0.1312 ::: bot acc: 0.1458
top acc: 0.1016 ::: bot acc: 0.0522
current epoch: 40
train loss is 0.076115
average val loss: 0.075906, accuracy: 0.0723
average test loss: 0.074679, accuracy: 0.0742
case acc: 0.0642157
case acc: 0.04413348
case acc: 0.07628675
case acc: 0.08188808
case acc: 0.11966369
case acc: 0.058824178
top acc: 0.0668 ::: bot acc: 0.0981
top acc: 0.0701 ::: bot acc: 0.0494
top acc: 0.0312 ::: bot acc: 0.1303
top acc: 0.1509 ::: bot acc: 0.0557
top acc: 0.1328 ::: bot acc: 0.1440
top acc: 0.1028 ::: bot acc: 0.0510
current epoch: 41
train loss is 0.076041
average val loss: 0.075511, accuracy: 0.0717
average test loss: 0.074426, accuracy: 0.0739
case acc: 0.06404391
case acc: 0.044042002
case acc: 0.0749749
case acc: 0.08257391
case acc: 0.11886443
case acc: 0.059087407
top acc: 0.0677 ::: bot acc: 0.0971
top acc: 0.0719 ::: bot acc: 0.0476
top acc: 0.0316 ::: bot acc: 0.1281
top acc: 0.1529 ::: bot acc: 0.0539
top acc: 0.1345 ::: bot acc: 0.1420
top acc: 0.1043 ::: bot acc: 0.0495
current epoch: 42
train loss is 0.076089
average val loss: 0.074833, accuracy: 0.0708
average test loss: 0.074017, accuracy: 0.0736
case acc: 0.063694455
case acc: 0.043922137
case acc: 0.07263921
case acc: 0.0839313
case acc: 0.11742368
case acc: 0.05969004
top acc: 0.0700 ::: bot acc: 0.0948
top acc: 0.0748 ::: bot acc: 0.0448
top acc: 0.0324 ::: bot acc: 0.1242
top acc: 0.1565 ::: bot acc: 0.0507
top acc: 0.1378 ::: bot acc: 0.1382
top acc: 0.1073 ::: bot acc: 0.0466
current epoch: 43
train loss is 0.076127
average val loss: 0.074682, accuracy: 0.0704
average test loss: 0.073931, accuracy: 0.0734
case acc: 0.06373641
case acc: 0.04390673
case acc: 0.071652375
case acc: 0.08448587
case acc: 0.11684579
case acc: 0.059792988
top acc: 0.0696 ::: bot acc: 0.0952
top acc: 0.0748 ::: bot acc: 0.0448
top acc: 0.0328 ::: bot acc: 0.1225
top acc: 0.1578 ::: bot acc: 0.0496
top acc: 0.1390 ::: bot acc: 0.1367
top acc: 0.1077 ::: bot acc: 0.0461
current epoch: 44
train loss is 0.076270
average val loss: 0.074256, accuracy: 0.0697
average test loss: 0.073702, accuracy: 0.0731
case acc: 0.063515924
case acc: 0.043865412
case acc: 0.069771595
case acc: 0.08574952
case acc: 0.11560999
case acc: 0.06030504
top acc: 0.0709 ::: bot acc: 0.0939
top acc: 0.0758 ::: bot acc: 0.0437
top acc: 0.0340 ::: bot acc: 0.1190
top acc: 0.1609 ::: bot acc: 0.0470
top acc: 0.1420 ::: bot acc: 0.1334
top acc: 0.1099 ::: bot acc: 0.0441
current epoch: 45
train loss is 0.076437
average val loss: 0.073872, accuracy: 0.0689
average test loss: 0.073528, accuracy: 0.0729
case acc: 0.063311376
case acc: 0.04385322
case acc: 0.06778306
case acc: 0.08717569
case acc: 0.11429441
case acc: 0.060881667
top acc: 0.0722 ::: bot acc: 0.0927
top acc: 0.0762 ::: bot acc: 0.0433
top acc: 0.0355 ::: bot acc: 0.1153
top acc: 0.1644 ::: bot acc: 0.0443
top acc: 0.1452 ::: bot acc: 0.1298
top acc: 0.1121 ::: bot acc: 0.0418
current epoch: 46
train loss is 0.076558
average val loss: 0.073837, accuracy: 0.0685
average test loss: 0.073533, accuracy: 0.0728
case acc: 0.06342996
case acc: 0.04389386
case acc: 0.066775925
case acc: 0.087941624
case acc: 0.11365199
case acc: 0.060971253
top acc: 0.0714 ::: bot acc: 0.0934
top acc: 0.0740 ::: bot acc: 0.0456
top acc: 0.0363 ::: bot acc: 0.1134
top acc: 0.1662 ::: bot acc: 0.0433
top acc: 0.1469 ::: bot acc: 0.1280
top acc: 0.1125 ::: bot acc: 0.0415
current epoch: 47
train loss is 0.076721
average val loss: 0.074101, accuracy: 0.0684
average test loss: 0.073677, accuracy: 0.0728
case acc: 0.06380283
case acc: 0.044127956
case acc: 0.06670346
case acc: 0.08791148
case acc: 0.113593444
case acc: 0.06064735
top acc: 0.0689 ::: bot acc: 0.0959
top acc: 0.0695 ::: bot acc: 0.0500
top acc: 0.0364 ::: bot acc: 0.1133
top acc: 0.1661 ::: bot acc: 0.0433
top acc: 0.1470 ::: bot acc: 0.1278
top acc: 0.1111 ::: bot acc: 0.0429
current epoch: 48
train loss is 0.077124
average val loss: 0.074339, accuracy: 0.0684
average test loss: 0.073832, accuracy: 0.0729
case acc: 0.06409194
case acc: 0.044663392
case acc: 0.06649166
case acc: 0.08805065
case acc: 0.11334787
case acc: 0.060475133
top acc: 0.0671 ::: bot acc: 0.0977
top acc: 0.0649 ::: bot acc: 0.0547
top acc: 0.0366 ::: bot acc: 0.1129
top acc: 0.1664 ::: bot acc: 0.0431
top acc: 0.1478 ::: bot acc: 0.1271
top acc: 0.1104 ::: bot acc: 0.0436
current epoch: 49
train loss is 0.077812
average val loss: 0.074715, accuracy: 0.0687
average test loss: 0.074083, accuracy: 0.0731
case acc: 0.064418405
case acc: 0.04589205
case acc: 0.066620804
case acc: 0.087947905
case acc: 0.113214925
case acc: 0.060285646
top acc: 0.0654 ::: bot acc: 0.0995
top acc: 0.0587 ::: bot acc: 0.0608
top acc: 0.0365 ::: bot acc: 0.1131
top acc: 0.1662 ::: bot acc: 0.0432
top acc: 0.1482 ::: bot acc: 0.1267
top acc: 0.1095 ::: bot acc: 0.0445
current epoch: 50
train loss is 0.078510
average val loss: 0.076235, accuracy: 0.0705
average test loss: 0.075053, accuracy: 0.0741
case acc: 0.06581655
case acc: 0.04958712
case acc: 0.06963009
case acc: 0.085656084
case acc: 0.11487856
case acc: 0.059120614
top acc: 0.0590 ::: bot acc: 0.1059
top acc: 0.0469 ::: bot acc: 0.0729
top acc: 0.0342 ::: bot acc: 0.1188
top acc: 0.1608 ::: bot acc: 0.0472
top acc: 0.1439 ::: bot acc: 0.1313
top acc: 0.1040 ::: bot acc: 0.0500
LME_Co_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5142 5142 5142
1.7082474 -0.6288155 0.48738334 -0.25570297
Validation: 576 576 576
Testing: 750 750 750
pre-processing time: 0.00045299530029296875
the split date is 2012-01-01
net initializing with time: 0.003644704818725586
preparing training and testing date with time: 4.76837158203125e-07
current epoch: 1
train loss is 0.304478
average val loss: 0.157194, accuracy: 0.1543
average test loss: 0.299543, accuracy: 0.2995
case acc: 0.29288647
case acc: 0.21161656
case acc: 0.42121902
case acc: 0.2951225
case acc: 0.382695
case acc: 0.19371648
top acc: 0.3479 ::: bot acc: 0.2379
top acc: 0.2621 ::: bot acc: 0.1568
top acc: 0.5108 ::: bot acc: 0.3307
top acc: 0.3899 ::: bot acc: 0.2101
top acc: 0.4456 ::: bot acc: 0.3187
top acc: 0.2644 ::: bot acc: 0.1184
current epoch: 2
train loss is 0.244934
average val loss: 0.232521, accuracy: 0.2322
average test loss: 0.109904, accuracy: 0.1143
case acc: 0.09988389
case acc: 0.17568003
case acc: 0.07072097
case acc: 0.10862864
case acc: 0.04561348
case acc: 0.1850304
top acc: 0.0457 ::: bot acc: 0.1543
top acc: 0.1250 ::: bot acc: 0.2313
top acc: 0.1194 ::: bot acc: 0.0616
top acc: 0.0418 ::: bot acc: 0.1802
top acc: 0.0677 ::: bot acc: 0.0593
top acc: 0.1138 ::: bot acc: 0.2609
current epoch: 3
train loss is 0.116216
average val loss: 0.073897, accuracy: 0.0534
average test loss: 0.175334, accuracy: 0.1752
case acc: 0.16310753
case acc: 0.08618476
case acc: 0.2898779
case acc: 0.16175717
case acc: 0.26356724
case acc: 0.08684897
top acc: 0.2178 ::: bot acc: 0.1083
top acc: 0.1370 ::: bot acc: 0.0303
top acc: 0.3802 ::: bot acc: 0.1991
top acc: 0.2548 ::: bot acc: 0.0775
top acc: 0.3274 ::: bot acc: 0.2000
top acc: 0.1528 ::: bot acc: 0.0218
current epoch: 4
train loss is 0.113842
average val loss: 0.088859, accuracy: 0.0757
average test loss: 0.099811, accuracy: 0.1017
case acc: 0.0734528
case acc: 0.040475335
case acc: 0.19147167
case acc: 0.07907433
case acc: 0.17225002
case acc: 0.053701937
top acc: 0.1211 ::: bot acc: 0.0327
top acc: 0.0449 ::: bot acc: 0.0624
top acc: 0.2817 ::: bot acc: 0.1012
top acc: 0.1587 ::: bot acc: 0.0209
top acc: 0.2363 ::: bot acc: 0.1088
top acc: 0.0626 ::: bot acc: 0.0847
current epoch: 5
train loss is 0.096007
average val loss: 0.094974, accuracy: 0.0839
average test loss: 0.092314, accuracy: 0.0945
case acc: 0.061443165
case acc: 0.04133867
case acc: 0.17368168
case acc: 0.072674684
case acc: 0.16357867
case acc: 0.054178957
top acc: 0.1051 ::: bot acc: 0.0288
top acc: 0.0338 ::: bot acc: 0.0737
top acc: 0.2634 ::: bot acc: 0.0847
top acc: 0.1432 ::: bot acc: 0.0323
top acc: 0.2279 ::: bot acc: 0.1002
top acc: 0.0562 ::: bot acc: 0.0911
current epoch: 6
train loss is 0.086320
average val loss: 0.077698, accuracy: 0.0630
average test loss: 0.109886, accuracy: 0.1109
case acc: 0.08361941
case acc: 0.043386675
case acc: 0.19812241
case acc: 0.086016916
case acc: 0.19630508
case acc: 0.058146313
top acc: 0.1336 ::: bot acc: 0.0378
top acc: 0.0669 ::: bot acc: 0.0406
top acc: 0.2887 ::: bot acc: 0.1076
top acc: 0.1710 ::: bot acc: 0.0160
top acc: 0.2607 ::: bot acc: 0.1330
top acc: 0.0926 ::: bot acc: 0.0549
current epoch: 7
train loss is 0.086208
average val loss: 0.074806, accuracy: 0.0599
average test loss: 0.112453, accuracy: 0.1131
case acc: 0.085220546
case acc: 0.0451364
case acc: 0.19700663
case acc: 0.08784196
case acc: 0.2027439
case acc: 0.060389988
top acc: 0.1355 ::: bot acc: 0.0387
top acc: 0.0738 ::: bot acc: 0.0338
top acc: 0.2877 ::: bot acc: 0.1065
top acc: 0.1737 ::: bot acc: 0.0161
top acc: 0.2670 ::: bot acc: 0.1397
top acc: 0.1016 ::: bot acc: 0.0456
current epoch: 8
train loss is 0.086441
average val loss: 0.078754, accuracy: 0.0658
average test loss: 0.104126, accuracy: 0.1050
case acc: 0.07376698
case acc: 0.04276034
case acc: 0.18074243
case acc: 0.08018544
case acc: 0.19406645
case acc: 0.058485497
top acc: 0.1213 ::: bot acc: 0.0328
top acc: 0.0646 ::: bot acc: 0.0427
top acc: 0.2710 ::: bot acc: 0.0913
top acc: 0.1609 ::: bot acc: 0.0188
top acc: 0.2582 ::: bot acc: 0.1313
top acc: 0.0950 ::: bot acc: 0.0520
current epoch: 9
train loss is 0.084340
average val loss: 0.080691, accuracy: 0.0690
average test loss: 0.100142, accuracy: 0.1010
case acc: 0.06763764
case acc: 0.042006854
case acc: 0.17046243
case acc: 0.07693952
case acc: 0.19092706
case acc: 0.058203768
top acc: 0.1133 ::: bot acc: 0.0304
top acc: 0.0612 ::: bot acc: 0.0458
top acc: 0.2605 ::: bot acc: 0.0816
top acc: 0.1541 ::: bot acc: 0.0231
top acc: 0.2548 ::: bot acc: 0.1283
top acc: 0.0942 ::: bot acc: 0.0525
current epoch: 10
train loss is 0.082420
average val loss: 0.078964, accuracy: 0.0674
average test loss: 0.101120, accuracy: 0.1017
case acc: 0.06776546
case acc: 0.042805776
case acc: 0.16795157
case acc: 0.077265725
case acc: 0.19439149
case acc: 0.05979088
top acc: 0.1135 ::: bot acc: 0.0303
top acc: 0.0654 ::: bot acc: 0.0413
top acc: 0.2581 ::: bot acc: 0.0791
top acc: 0.1550 ::: bot acc: 0.0228
top acc: 0.2581 ::: bot acc: 0.1319
top acc: 0.1004 ::: bot acc: 0.0460
current epoch: 11
train loss is 0.081366
average val loss: 0.078115, accuracy: 0.0669
average test loss: 0.101183, accuracy: 0.1015
case acc: 0.067081
case acc: 0.04347592
case acc: 0.16448784
case acc: 0.07691955
case acc: 0.19569957
case acc: 0.06111278
top acc: 0.1126 ::: bot acc: 0.0300
top acc: 0.0683 ::: bot acc: 0.0384
top acc: 0.2546 ::: bot acc: 0.0760
top acc: 0.1542 ::: bot acc: 0.0232
top acc: 0.2593 ::: bot acc: 0.1333
top acc: 0.1047 ::: bot acc: 0.0417
current epoch: 12
train loss is 0.080783
average val loss: 0.079463, accuracy: 0.0692
average test loss: 0.098690, accuracy: 0.0989
case acc: 0.06360902
case acc: 0.043256357
case acc: 0.15762134
case acc: 0.07505536
case acc: 0.19274943
case acc: 0.061110675
top acc: 0.1080 ::: bot acc: 0.0288
top acc: 0.0672 ::: bot acc: 0.0396
top acc: 0.2474 ::: bot acc: 0.0699
top acc: 0.1499 ::: bot acc: 0.0265
top acc: 0.2564 ::: bot acc: 0.1304
top acc: 0.1047 ::: bot acc: 0.0417
current epoch: 13
train loss is 0.080415
average val loss: 0.079617, accuracy: 0.0699
average test loss: 0.097829, accuracy: 0.0978
case acc: 0.06206546
case acc: 0.043455485
case acc: 0.15340678
case acc: 0.074475504
case acc: 0.19190945
case acc: 0.061746508
top acc: 0.1060 ::: bot acc: 0.0281
top acc: 0.0683 ::: bot acc: 0.0382
top acc: 0.2430 ::: bot acc: 0.0660
top acc: 0.1484 ::: bot acc: 0.0280
top acc: 0.2553 ::: bot acc: 0.1297
top acc: 0.1066 ::: bot acc: 0.0397
current epoch: 14
train loss is 0.079895
average val loss: 0.080795, accuracy: 0.0719
average test loss: 0.095948, accuracy: 0.0958
case acc: 0.05944353
case acc: 0.04313034
case acc: 0.14794154
case acc: 0.07354942
case acc: 0.18908268
case acc: 0.061544403
top acc: 0.1026 ::: bot acc: 0.0271
top acc: 0.0678 ::: bot acc: 0.0382
top acc: 0.2374 ::: bot acc: 0.0610
top acc: 0.1457 ::: bot acc: 0.0311
top acc: 0.2521 ::: bot acc: 0.1272
top acc: 0.1064 ::: bot acc: 0.0394
current epoch: 15
train loss is 0.079625
average val loss: 0.078465, accuracy: 0.0694
average test loss: 0.098012, accuracy: 0.0975
case acc: 0.061700925
case acc: 0.044679895
case acc: 0.14869195
case acc: 0.074647486
case acc: 0.19171922
case acc: 0.06368762
top acc: 0.1056 ::: bot acc: 0.0277
top acc: 0.0734 ::: bot acc: 0.0328
top acc: 0.2383 ::: bot acc: 0.0617
top acc: 0.1488 ::: bot acc: 0.0281
top acc: 0.2547 ::: bot acc: 0.1300
top acc: 0.1119 ::: bot acc: 0.0345
current epoch: 16
train loss is 0.079247
average val loss: 0.077590, accuracy: 0.0687
average test loss: 0.098545, accuracy: 0.0979
case acc: 0.062246323
case acc: 0.045644637
case acc: 0.14749207
case acc: 0.07488395
case acc: 0.19180842
case acc: 0.06507847
top acc: 0.1063 ::: bot acc: 0.0280
top acc: 0.0764 ::: bot acc: 0.0299
top acc: 0.2370 ::: bot acc: 0.0606
top acc: 0.1496 ::: bot acc: 0.0271
top acc: 0.2549 ::: bot acc: 0.1300
top acc: 0.1149 ::: bot acc: 0.0327
current epoch: 17
train loss is 0.078702
average val loss: 0.077453, accuracy: 0.0689
average test loss: 0.098238, accuracy: 0.0974
case acc: 0.061871752
case acc: 0.04624832
case acc: 0.14534448
case acc: 0.074779145
case acc: 0.19054534
case acc: 0.065699056
top acc: 0.1058 ::: bot acc: 0.0279
top acc: 0.0780 ::: bot acc: 0.0285
top acc: 0.2348 ::: bot acc: 0.0587
top acc: 0.1493 ::: bot acc: 0.0271
top acc: 0.2538 ::: bot acc: 0.1287
top acc: 0.1163 ::: bot acc: 0.0318
current epoch: 18
train loss is 0.078606
average val loss: 0.077250, accuracy: 0.0690
average test loss: 0.098053, accuracy: 0.0971
case acc: 0.061684463
case acc: 0.04688312
case acc: 0.14358293
case acc: 0.074854866
case acc: 0.18925518
case acc: 0.066295534
top acc: 0.1055 ::: bot acc: 0.0279
top acc: 0.0797 ::: bot acc: 0.0270
top acc: 0.2329 ::: bot acc: 0.0572
top acc: 0.1495 ::: bot acc: 0.0269
top acc: 0.2525 ::: bot acc: 0.1275
top acc: 0.1175 ::: bot acc: 0.0311
current epoch: 19
train loss is 0.078426
average val loss: 0.074474, accuracy: 0.0658
average test loss: 0.101104, accuracy: 0.0999
case acc: 0.06508695
case acc: 0.049310997
case acc: 0.14657314
case acc: 0.07692102
case acc: 0.19234128
case acc: 0.06901497
top acc: 0.1099 ::: bot acc: 0.0290
top acc: 0.0860 ::: bot acc: 0.0216
top acc: 0.2361 ::: bot acc: 0.0598
top acc: 0.1545 ::: bot acc: 0.0231
top acc: 0.2555 ::: bot acc: 0.1307
top acc: 0.1231 ::: bot acc: 0.0279
current epoch: 20
train loss is 0.078136
average val loss: 0.074007, accuracy: 0.0656
average test loss: 0.101322, accuracy: 0.1000
case acc: 0.06538794
case acc: 0.050137665
case acc: 0.14591455
case acc: 0.077391066
case acc: 0.19124874
case acc: 0.06966517
top acc: 0.1103 ::: bot acc: 0.0290
top acc: 0.0879 ::: bot acc: 0.0201
top acc: 0.2356 ::: bot acc: 0.0592
top acc: 0.1554 ::: bot acc: 0.0225
top acc: 0.2543 ::: bot acc: 0.1297
top acc: 0.1243 ::: bot acc: 0.0273
current epoch: 21
train loss is 0.077937
average val loss: 0.073821, accuracy: 0.0656
average test loss: 0.101229, accuracy: 0.0997
case acc: 0.065261774
case acc: 0.050662346
case acc: 0.14505333
case acc: 0.07774872
case acc: 0.18978132
case acc: 0.0699769
top acc: 0.1101 ::: bot acc: 0.0289
top acc: 0.0893 ::: bot acc: 0.0190
top acc: 0.2348 ::: bot acc: 0.0584
top acc: 0.1561 ::: bot acc: 0.0221
top acc: 0.2527 ::: bot acc: 0.1284
top acc: 0.1250 ::: bot acc: 0.0269
current epoch: 22
train loss is 0.077648
average val loss: 0.073191, accuracy: 0.0651
average test loss: 0.101752, accuracy: 0.1001
case acc: 0.06576217
case acc: 0.051547978
case acc: 0.14506142
case acc: 0.078476176
case acc: 0.18930553
case acc: 0.07072615
top acc: 0.1108 ::: bot acc: 0.0291
top acc: 0.0913 ::: bot acc: 0.0176
top acc: 0.2348 ::: bot acc: 0.0584
top acc: 0.1576 ::: bot acc: 0.0212
top acc: 0.2522 ::: bot acc: 0.1280
top acc: 0.1264 ::: bot acc: 0.0263
current epoch: 23
train loss is 0.077590
average val loss: 0.071238, accuracy: 0.0629
average test loss: 0.104197, accuracy: 0.1024
case acc: 0.068160556
case acc: 0.05361358
case acc: 0.14759669
case acc: 0.08049344
case acc: 0.19159418
case acc: 0.07309079
top acc: 0.1139 ::: bot acc: 0.0300
top acc: 0.0957 ::: bot acc: 0.0149
top acc: 0.2374 ::: bot acc: 0.0607
top acc: 0.1617 ::: bot acc: 0.0189
top acc: 0.2545 ::: bot acc: 0.1303
top acc: 0.1305 ::: bot acc: 0.0251
current epoch: 24
train loss is 0.077389
average val loss: 0.071318, accuracy: 0.0633
average test loss: 0.103811, accuracy: 0.1020
case acc: 0.06753971
case acc: 0.05380957
case acc: 0.14657736
case acc: 0.08059065
case acc: 0.19014667
case acc: 0.073266976
top acc: 0.1131 ::: bot acc: 0.0297
top acc: 0.0961 ::: bot acc: 0.0147
top acc: 0.2363 ::: bot acc: 0.0598
top acc: 0.1619 ::: bot acc: 0.0187
top acc: 0.2530 ::: bot acc: 0.1289
top acc: 0.1308 ::: bot acc: 0.0250
current epoch: 25
train loss is 0.077105
average val loss: 0.071266, accuracy: 0.0635
average test loss: 0.103631, accuracy: 0.1018
case acc: 0.06707306
case acc: 0.054172862
case acc: 0.14584485
case acc: 0.08079944
case acc: 0.18906485
case acc: 0.0735769
top acc: 0.1124 ::: bot acc: 0.0296
top acc: 0.0967 ::: bot acc: 0.0145
top acc: 0.2356 ::: bot acc: 0.0592
top acc: 0.1624 ::: bot acc: 0.0184
top acc: 0.2519 ::: bot acc: 0.1279
top acc: 0.1312 ::: bot acc: 0.0249
current epoch: 26
train loss is 0.077021
average val loss: 0.071187, accuracy: 0.0636
average test loss: 0.103544, accuracy: 0.1016
case acc: 0.06668516
case acc: 0.05447596
case acc: 0.14532137
case acc: 0.081191964
case acc: 0.1880761
case acc: 0.07390666
top acc: 0.1119 ::: bot acc: 0.0294
top acc: 0.0973 ::: bot acc: 0.0143
top acc: 0.2351 ::: bot acc: 0.0586
top acc: 0.1632 ::: bot acc: 0.0181
top acc: 0.2508 ::: bot acc: 0.1270
top acc: 0.1318 ::: bot acc: 0.0249
current epoch: 27
train loss is 0.076807
average val loss: 0.071462, accuracy: 0.0641
average test loss: 0.102954, accuracy: 0.1010
case acc: 0.06579158
case acc: 0.054429725
case acc: 0.14410526
case acc: 0.081184074
case acc: 0.18643065
case acc: 0.07381834
top acc: 0.1108 ::: bot acc: 0.0290
top acc: 0.0972 ::: bot acc: 0.0142
top acc: 0.2338 ::: bot acc: 0.0575
top acc: 0.1632 ::: bot acc: 0.0182
top acc: 0.2490 ::: bot acc: 0.1254
top acc: 0.1316 ::: bot acc: 0.0249
current epoch: 28
train loss is 0.076800
average val loss: 0.070919, accuracy: 0.0637
average test loss: 0.103532, accuracy: 0.1015
case acc: 0.06614951
case acc: 0.055307463
case acc: 0.14446923
case acc: 0.081977695
case acc: 0.18628952
case acc: 0.074657224
top acc: 0.1113 ::: bot acc: 0.0290
top acc: 0.0987 ::: bot acc: 0.0138
top acc: 0.2343 ::: bot acc: 0.0577
top acc: 0.1649 ::: bot acc: 0.0174
top acc: 0.2489 ::: bot acc: 0.1253
top acc: 0.1330 ::: bot acc: 0.0246
current epoch: 29
train loss is 0.076533
average val loss: 0.070161, accuracy: 0.0630
average test loss: 0.104457, accuracy: 0.1023
case acc: 0.06686524
case acc: 0.056454495
case acc: 0.14535293
case acc: 0.08289252
case acc: 0.18666959
case acc: 0.07576098
top acc: 0.1122 ::: bot acc: 0.0293
top acc: 0.1006 ::: bot acc: 0.0135
top acc: 0.2352 ::: bot acc: 0.0585
top acc: 0.1666 ::: bot acc: 0.0164
top acc: 0.2492 ::: bot acc: 0.1258
top acc: 0.1348 ::: bot acc: 0.0242
current epoch: 30
train loss is 0.076534
average val loss: 0.070912, accuracy: 0.0641
average test loss: 0.103205, accuracy: 0.1011
case acc: 0.06532871
case acc: 0.05576371
case acc: 0.14356893
case acc: 0.08237657
case acc: 0.18422559
case acc: 0.07511262
top acc: 0.1102 ::: bot acc: 0.0287
top acc: 0.0994 ::: bot acc: 0.0137
top acc: 0.2334 ::: bot acc: 0.0569
top acc: 0.1657 ::: bot acc: 0.0169
top acc: 0.2468 ::: bot acc: 0.1234
top acc: 0.1337 ::: bot acc: 0.0245
current epoch: 31
train loss is 0.076224
average val loss: 0.069686, accuracy: 0.0628
average test loss: 0.104859, accuracy: 0.1027
case acc: 0.066721596
case acc: 0.057454165
case acc: 0.1454113
case acc: 0.08398511
case acc: 0.18564662
case acc: 0.07677689
top acc: 0.1120 ::: bot acc: 0.0292
top acc: 0.1021 ::: bot acc: 0.0134
top acc: 0.2353 ::: bot acc: 0.0585
top acc: 0.1687 ::: bot acc: 0.0158
top acc: 0.2481 ::: bot acc: 0.1248
top acc: 0.1363 ::: bot acc: 0.0240
current epoch: 32
train loss is 0.076296
average val loss: 0.070576, accuracy: 0.0640
average test loss: 0.103417, accuracy: 0.1012
case acc: 0.06492397
case acc: 0.05652462
case acc: 0.14340818
case acc: 0.08323991
case acc: 0.18307978
case acc: 0.07597933
top acc: 0.1097 ::: bot acc: 0.0285
top acc: 0.1006 ::: bot acc: 0.0136
top acc: 0.2332 ::: bot acc: 0.0567
top acc: 0.1674 ::: bot acc: 0.0163
top acc: 0.2455 ::: bot acc: 0.1222
top acc: 0.1350 ::: bot acc: 0.0243
current epoch: 33
train loss is 0.075894
average val loss: 0.070619, accuracy: 0.0642
average test loss: 0.103236, accuracy: 0.1010
case acc: 0.064453594
case acc: 0.05662606
case acc: 0.14299779
case acc: 0.08330413
case acc: 0.1821518
case acc: 0.076212995
top acc: 0.1090 ::: bot acc: 0.0284
top acc: 0.1007 ::: bot acc: 0.0135
top acc: 0.2328 ::: bot acc: 0.0564
top acc: 0.1674 ::: bot acc: 0.0161
top acc: 0.2446 ::: bot acc: 0.1214
top acc: 0.1354 ::: bot acc: 0.0241
current epoch: 34
train loss is 0.075978
average val loss: 0.069366, accuracy: 0.0630
average test loss: 0.104982, accuracy: 0.1026
case acc: 0.065895446
case acc: 0.058274016
case acc: 0.14507246
case acc: 0.084902376
case acc: 0.18377581
case acc: 0.07794425
top acc: 0.1109 ::: bot acc: 0.0288
top acc: 0.1033 ::: bot acc: 0.0132
top acc: 0.2350 ::: bot acc: 0.0583
top acc: 0.1701 ::: bot acc: 0.0154
top acc: 0.2462 ::: bot acc: 0.1231
top acc: 0.1383 ::: bot acc: 0.0234
current epoch: 35
train loss is 0.075843
average val loss: 0.068557, accuracy: 0.0622
average test loss: 0.106126, accuracy: 0.1038
case acc: 0.06670835
case acc: 0.059470527
case acc: 0.14631416
case acc: 0.0860855
case acc: 0.18467304
case acc: 0.079250544
top acc: 0.1119 ::: bot acc: 0.0292
top acc: 0.1052 ::: bot acc: 0.0131
top acc: 0.2363 ::: bot acc: 0.0594
top acc: 0.1719 ::: bot acc: 0.0152
top acc: 0.2470 ::: bot acc: 0.1240
top acc: 0.1404 ::: bot acc: 0.0231
current epoch: 36
train loss is 0.075602
average val loss: 0.068321, accuracy: 0.0621
average test loss: 0.106420, accuracy: 0.1040
case acc: 0.066622585
case acc: 0.05992436
case acc: 0.14653538
case acc: 0.086620584
case acc: 0.18451925
case acc: 0.07990859
top acc: 0.1118 ::: bot acc: 0.0291
top acc: 0.1058 ::: bot acc: 0.0132
top acc: 0.2366 ::: bot acc: 0.0596
top acc: 0.1728 ::: bot acc: 0.0151
top acc: 0.2468 ::: bot acc: 0.1239
top acc: 0.1413 ::: bot acc: 0.0231
current epoch: 37
train loss is 0.075683
average val loss: 0.069532, accuracy: 0.0636
average test loss: 0.104455, accuracy: 0.1020
case acc: 0.064323395
case acc: 0.058480155
case acc: 0.1439313
case acc: 0.085218355
case acc: 0.18147653
case acc: 0.07874442
top acc: 0.1087 ::: bot acc: 0.0283
top acc: 0.1035 ::: bot acc: 0.0134
top acc: 0.2338 ::: bot acc: 0.0572
top acc: 0.1706 ::: bot acc: 0.0153
top acc: 0.2438 ::: bot acc: 0.1209
top acc: 0.1394 ::: bot acc: 0.0234
current epoch: 38
train loss is 0.075440
average val loss: 0.070222, accuracy: 0.0644
average test loss: 0.103357, accuracy: 0.1009
case acc: 0.06297249
case acc: 0.0577658
case acc: 0.14243414
case acc: 0.084458105
case acc: 0.17939416
case acc: 0.07821698
top acc: 0.1069 ::: bot acc: 0.0279
top acc: 0.1024 ::: bot acc: 0.0135
top acc: 0.2323 ::: bot acc: 0.0559
top acc: 0.1693 ::: bot acc: 0.0155
top acc: 0.2417 ::: bot acc: 0.1189
top acc: 0.1386 ::: bot acc: 0.0235
current epoch: 39
train loss is 0.075538
average val loss: 0.069518, accuracy: 0.0638
average test loss: 0.104348, accuracy: 0.1018
case acc: 0.06381813
case acc: 0.05876658
case acc: 0.14367151
case acc: 0.08535037
case acc: 0.17989677
case acc: 0.07938761
top acc: 0.1079 ::: bot acc: 0.0282
top acc: 0.1040 ::: bot acc: 0.0133
top acc: 0.2336 ::: bot acc: 0.0571
top acc: 0.1706 ::: bot acc: 0.0152
top acc: 0.2422 ::: bot acc: 0.1195
top acc: 0.1405 ::: bot acc: 0.0231
current epoch: 40
train loss is 0.075594
average val loss: 0.068331, accuracy: 0.0626
average test loss: 0.106125, accuracy: 0.1036
case acc: 0.065391056
case acc: 0.06058694
case acc: 0.14581908
case acc: 0.087019846
case acc: 0.1813981
case acc: 0.08117872
top acc: 0.1100 ::: bot acc: 0.0288
top acc: 0.1066 ::: bot acc: 0.0135
top acc: 0.2358 ::: bot acc: 0.0591
top acc: 0.1730 ::: bot acc: 0.0151
top acc: 0.2437 ::: bot acc: 0.1211
top acc: 0.1433 ::: bot acc: 0.0228
current epoch: 41
train loss is 0.075500
average val loss: 0.067342, accuracy: 0.0616
average test loss: 0.107689, accuracy: 0.1051
case acc: 0.06670335
case acc: 0.062249947
case acc: 0.14774916
case acc: 0.08871283
case acc: 0.18265563
case acc: 0.08277203
top acc: 0.1116 ::: bot acc: 0.0292
top acc: 0.1089 ::: bot acc: 0.0140
top acc: 0.2379 ::: bot acc: 0.0608
top acc: 0.1754 ::: bot acc: 0.0154
top acc: 0.2450 ::: bot acc: 0.1224
top acc: 0.1457 ::: bot acc: 0.0228
current epoch: 42
train loss is 0.075556
average val loss: 0.068453, accuracy: 0.0629
average test loss: 0.105812, accuracy: 0.1032
case acc: 0.064580895
case acc: 0.060612738
case acc: 0.1455087
case acc: 0.08733815
case acc: 0.17963424
case acc: 0.081351556
top acc: 0.1088 ::: bot acc: 0.0284
top acc: 0.1066 ::: bot acc: 0.0135
top acc: 0.2356 ::: bot acc: 0.0588
top acc: 0.1734 ::: bot acc: 0.0152
top acc: 0.2419 ::: bot acc: 0.1195
top acc: 0.1436 ::: bot acc: 0.0227
current epoch: 43
train loss is 0.075311
average val loss: 0.069865, accuracy: 0.0646
average test loss: 0.103620, accuracy: 0.1009
case acc: 0.062184755
case acc: 0.058825046
case acc: 0.14277677
case acc: 0.08570303
case acc: 0.17613634
case acc: 0.07976534
top acc: 0.1056 ::: bot acc: 0.0276
top acc: 0.1039 ::: bot acc: 0.0135
top acc: 0.2327 ::: bot acc: 0.0563
top acc: 0.1709 ::: bot acc: 0.0152
top acc: 0.2384 ::: bot acc: 0.1160
top acc: 0.1411 ::: bot acc: 0.0230
current epoch: 44
train loss is 0.075087
average val loss: 0.068532, accuracy: 0.0632
average test loss: 0.105627, accuracy: 0.1029
case acc: 0.064006954
case acc: 0.060762126
case acc: 0.14529918
case acc: 0.0876802
case acc: 0.17801361
case acc: 0.08173371
top acc: 0.1081 ::: bot acc: 0.0281
top acc: 0.1067 ::: bot acc: 0.0136
top acc: 0.2354 ::: bot acc: 0.0586
top acc: 0.1739 ::: bot acc: 0.0152
top acc: 0.2402 ::: bot acc: 0.1179
top acc: 0.1440 ::: bot acc: 0.0229
current epoch: 45
train loss is 0.075330
average val loss: 0.067944, accuracy: 0.0627
average test loss: 0.106531, accuracy: 0.1038
case acc: 0.064607196
case acc: 0.06174411
case acc: 0.1464152
case acc: 0.08868779
case acc: 0.17866002
case acc: 0.08284983
top acc: 0.1088 ::: bot acc: 0.0283
top acc: 0.1081 ::: bot acc: 0.0139
top acc: 0.2366 ::: bot acc: 0.0596
top acc: 0.1752 ::: bot acc: 0.0154
top acc: 0.2408 ::: bot acc: 0.1187
top acc: 0.1456 ::: bot acc: 0.0230
current epoch: 46
train loss is 0.075270
average val loss: 0.067643, accuracy: 0.0625
average test loss: 0.106991, accuracy: 0.1043
case acc: 0.064709105
case acc: 0.062289383
case acc: 0.14698167
case acc: 0.08938556
case acc: 0.17888759
case acc: 0.083580784
top acc: 0.1088 ::: bot acc: 0.0284
top acc: 0.1087 ::: bot acc: 0.0142
top acc: 0.2372 ::: bot acc: 0.0601
top acc: 0.1761 ::: bot acc: 0.0156
top acc: 0.2410 ::: bot acc: 0.1190
top acc: 0.1466 ::: bot acc: 0.0232
current epoch: 47
train loss is 0.075151
average val loss: 0.068227, accuracy: 0.0632
average test loss: 0.105984, accuracy: 0.1032
case acc: 0.0632919
case acc: 0.061363466
case acc: 0.14568207
case acc: 0.08868844
case acc: 0.17741846
case acc: 0.08299699
top acc: 0.1069 ::: bot acc: 0.0280
top acc: 0.1074 ::: bot acc: 0.0140
top acc: 0.2358 ::: bot acc: 0.0589
top acc: 0.1751 ::: bot acc: 0.0154
top acc: 0.2395 ::: bot acc: 0.1175
top acc: 0.1457 ::: bot acc: 0.0232
current epoch: 48
train loss is 0.075052
average val loss: 0.068854, accuracy: 0.0640
average test loss: 0.104945, accuracy: 0.1021
case acc: 0.061729804
case acc: 0.06044268
case acc: 0.1442014
case acc: 0.08799942
case acc: 0.17605087
case acc: 0.082421206
top acc: 0.1049 ::: bot acc: 0.0274
top acc: 0.1061 ::: bot acc: 0.0138
top acc: 0.2342 ::: bot acc: 0.0576
top acc: 0.1742 ::: bot acc: 0.0153
top acc: 0.2381 ::: bot acc: 0.1162
top acc: 0.1448 ::: bot acc: 0.0231
current epoch: 49
train loss is 0.075137
average val loss: 0.069137, accuracy: 0.0644
average test loss: 0.104448, accuracy: 0.1016
case acc: 0.06073012
case acc: 0.059994824
case acc: 0.14337717
case acc: 0.08774993
case acc: 0.17544821
case acc: 0.0823973
top acc: 0.1035 ::: bot acc: 0.0271
top acc: 0.1056 ::: bot acc: 0.0137
top acc: 0.2334 ::: bot acc: 0.0569
top acc: 0.1738 ::: bot acc: 0.0152
top acc: 0.2375 ::: bot acc: 0.1157
top acc: 0.1448 ::: bot acc: 0.0231
current epoch: 50
train loss is 0.074825
average val loss: 0.071358, accuracy: 0.0668
average test loss: 0.101235, accuracy: 0.0983
case acc: 0.05705577
case acc: 0.05729113
case acc: 0.13910311
case acc: 0.0850598
case acc: 0.17114334
case acc: 0.07989002
top acc: 0.0986 ::: bot acc: 0.0259
top acc: 0.1014 ::: bot acc: 0.0140
top acc: 0.2289 ::: bot acc: 0.0531
top acc: 0.1697 ::: bot acc: 0.0153
top acc: 0.2331 ::: bot acc: 0.1115
top acc: 0.1412 ::: bot acc: 0.0229
LME_Co_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5130 5130 5130
1.7082474 -0.6288155 0.48738334 -0.27422604
Validation: 570 570 570
Testing: 768 768 768
pre-processing time: 0.0005099773406982422
the split date is 2012-07-01
net initializing with time: 0.0038535594940185547
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.215200
average val loss: 0.239056, accuracy: 0.2391
average test loss: 0.249903, accuracy: 0.2499
case acc: 0.36516482
case acc: 0.17016381
case acc: 0.25087434
case acc: 0.19061083
case acc: 0.2825433
case acc: 0.23981386
top acc: 0.4834 ::: bot acc: 0.2526
top acc: 0.2155 ::: bot acc: 0.1285
top acc: 0.3456 ::: bot acc: 0.1769
top acc: 0.2818 ::: bot acc: 0.0765
top acc: 0.3599 ::: bot acc: 0.2058
top acc: 0.3197 ::: bot acc: 0.1436
current epoch: 2
train loss is 0.120605
average val loss: 0.142187, accuracy: 0.1419
average test loss: 0.154784, accuracy: 0.1524
case acc: 0.2562099
case acc: 0.07052751
case acc: 0.15288155
case acc: 0.10935068
case acc: 0.18671587
case acc: 0.13887072
top acc: 0.3754 ::: bot acc: 0.1430
top acc: 0.1162 ::: bot acc: 0.0282
top acc: 0.2482 ::: bot acc: 0.0794
top acc: 0.1823 ::: bot acc: 0.0317
top acc: 0.2641 ::: bot acc: 0.1100
top acc: 0.2158 ::: bot acc: 0.0489
current epoch: 3
train loss is 0.093863
average val loss: 0.107618, accuracy: 0.1079
average test loss: 0.120443, accuracy: 0.1159
case acc: 0.20531577
case acc: 0.038044438
case acc: 0.108438924
case acc: 0.09250889
case acc: 0.14544675
case acc: 0.10586445
top acc: 0.3250 ::: bot acc: 0.0918
top acc: 0.0732 ::: bot acc: 0.0173
top acc: 0.2034 ::: bot acc: 0.0363
top acc: 0.1392 ::: bot acc: 0.0667
top acc: 0.2230 ::: bot acc: 0.0686
top acc: 0.1700 ::: bot acc: 0.0421
current epoch: 4
train loss is 0.091739
average val loss: 0.118665, accuracy: 0.1190
average test loss: 0.131585, accuracy: 0.1279
case acc: 0.21738322
case acc: 0.048885867
case acc: 0.12439565
case acc: 0.09809141
case acc: 0.16269837
case acc: 0.115981445
top acc: 0.3375 ::: bot acc: 0.1037
top acc: 0.0914 ::: bot acc: 0.0131
top acc: 0.2202 ::: bot acc: 0.0511
top acc: 0.1567 ::: bot acc: 0.0495
top acc: 0.2403 ::: bot acc: 0.0858
top acc: 0.1854 ::: bot acc: 0.0414
current epoch: 5
train loss is 0.093542
average val loss: 0.136004, accuracy: 0.1362
average test loss: 0.148739, accuracy: 0.1462
case acc: 0.23565377
case acc: 0.069645286
case acc: 0.14819583
case acc: 0.10856062
case acc: 0.18489574
case acc: 0.13045573
top acc: 0.3561 ::: bot acc: 0.1218
top acc: 0.1156 ::: bot acc: 0.0270
top acc: 0.2444 ::: bot acc: 0.0747
top acc: 0.1800 ::: bot acc: 0.0339
top acc: 0.2626 ::: bot acc: 0.1078
top acc: 0.2061 ::: bot acc: 0.0433
current epoch: 6
train loss is 0.091128
average val loss: 0.129909, accuracy: 0.1303
average test loss: 0.142785, accuracy: 0.1400
case acc: 0.22411156
case acc: 0.06475881
case acc: 0.14290455
case acc: 0.1058971
case acc: 0.17824173
case acc: 0.12421371
top acc: 0.3447 ::: bot acc: 0.1102
top acc: 0.1106 ::: bot acc: 0.0224
top acc: 0.2393 ::: bot acc: 0.0693
top acc: 0.1740 ::: bot acc: 0.0379
top acc: 0.2561 ::: bot acc: 0.1011
top acc: 0.1976 ::: bot acc: 0.0419
current epoch: 7
train loss is 0.088417
average val loss: 0.122062, accuracy: 0.1228
average test loss: 0.135023, accuracy: 0.1318
case acc: 0.21008343
case acc: 0.05805331
case acc: 0.13426092
case acc: 0.102137536
case acc: 0.16934223
case acc: 0.11686349
top acc: 0.3307 ::: bot acc: 0.0962
top acc: 0.1032 ::: bot acc: 0.0171
top acc: 0.2307 ::: bot acc: 0.0606
top acc: 0.1657 ::: bot acc: 0.0434
top acc: 0.2472 ::: bot acc: 0.0922
top acc: 0.1869 ::: bot acc: 0.0414
current epoch: 8
train loss is 0.086772
average val loss: 0.118593, accuracy: 0.1195
average test loss: 0.131573, accuracy: 0.1281
case acc: 0.20192581
case acc: 0.05633018
case acc: 0.13038561
case acc: 0.10074512
case acc: 0.16571215
case acc: 0.1135228
top acc: 0.3224 ::: bot acc: 0.0882
top acc: 0.1011 ::: bot acc: 0.0159
top acc: 0.2269 ::: bot acc: 0.0565
top acc: 0.1628 ::: bot acc: 0.0451
top acc: 0.2436 ::: bot acc: 0.0885
top acc: 0.1819 ::: bot acc: 0.0415
current epoch: 9
train loss is 0.085484
average val loss: 0.114036, accuracy: 0.1154
average test loss: 0.127050, accuracy: 0.1232
case acc: 0.19272283
case acc: 0.053444237
case acc: 0.124649085
case acc: 0.09867287
case acc: 0.16021755
case acc: 0.10943529
top acc: 0.3131 ::: bot acc: 0.0790
top acc: 0.0976 ::: bot acc: 0.0144
top acc: 0.2212 ::: bot acc: 0.0509
top acc: 0.1582 ::: bot acc: 0.0480
top acc: 0.2381 ::: bot acc: 0.0830
top acc: 0.1756 ::: bot acc: 0.0420
current epoch: 10
train loss is 0.084669
average val loss: 0.112165, accuracy: 0.1138
average test loss: 0.125204, accuracy: 0.1212
case acc: 0.187577
case acc: 0.05340085
case acc: 0.12223481
case acc: 0.098184854
case acc: 0.15793273
case acc: 0.107669264
top acc: 0.3078 ::: bot acc: 0.0738
top acc: 0.0974 ::: bot acc: 0.0145
top acc: 0.2186 ::: bot acc: 0.0486
top acc: 0.1571 ::: bot acc: 0.0487
top acc: 0.2357 ::: bot acc: 0.0808
top acc: 0.1730 ::: bot acc: 0.0421
current epoch: 11
train loss is 0.084053
average val loss: 0.110003, accuracy: 0.1119
average test loss: 0.123069, accuracy: 0.1188
case acc: 0.18207464
case acc: 0.05304987
case acc: 0.11946131
case acc: 0.09766835
case acc: 0.15509361
case acc: 0.10561653
top acc: 0.3023 ::: bot acc: 0.0684
top acc: 0.0970 ::: bot acc: 0.0143
top acc: 0.2158 ::: bot acc: 0.0459
top acc: 0.1554 ::: bot acc: 0.0501
top acc: 0.2327 ::: bot acc: 0.0780
top acc: 0.1698 ::: bot acc: 0.0424
current epoch: 12
train loss is 0.082720
average val loss: 0.109860, accuracy: 0.1118
average test loss: 0.122990, accuracy: 0.1188
case acc: 0.17965066
case acc: 0.055024426
case acc: 0.119074374
case acc: 0.09827756
case acc: 0.1552288
case acc: 0.1055277
top acc: 0.3000 ::: bot acc: 0.0657
top acc: 0.0994 ::: bot acc: 0.0153
top acc: 0.2155 ::: bot acc: 0.0456
top acc: 0.1568 ::: bot acc: 0.0491
top acc: 0.2329 ::: bot acc: 0.0781
top acc: 0.1696 ::: bot acc: 0.0424
current epoch: 13
train loss is 0.082605
average val loss: 0.107874, accuracy: 0.1100
average test loss: 0.121068, accuracy: 0.1167
case acc: 0.1747715
case acc: 0.054893196
case acc: 0.1159884
case acc: 0.09775217
case acc: 0.15288198
case acc: 0.10391801
top acc: 0.2954 ::: bot acc: 0.0607
top acc: 0.0992 ::: bot acc: 0.0152
top acc: 0.2127 ::: bot acc: 0.0426
top acc: 0.1555 ::: bot acc: 0.0501
top acc: 0.2308 ::: bot acc: 0.0757
top acc: 0.1670 ::: bot acc: 0.0429
current epoch: 14
train loss is 0.081482
average val loss: 0.106117, accuracy: 0.1084
average test loss: 0.119359, accuracy: 0.1148
case acc: 0.1706644
case acc: 0.055031464
case acc: 0.1127356
case acc: 0.09741187
case acc: 0.15054864
case acc: 0.10267776
top acc: 0.2913 ::: bot acc: 0.0566
top acc: 0.0994 ::: bot acc: 0.0153
top acc: 0.2093 ::: bot acc: 0.0396
top acc: 0.1546 ::: bot acc: 0.0508
top acc: 0.2285 ::: bot acc: 0.0733
top acc: 0.1650 ::: bot acc: 0.0433
current epoch: 15
train loss is 0.081076
average val loss: 0.105540, accuracy: 0.1079
average test loss: 0.118824, accuracy: 0.1143
case acc: 0.16844723
case acc: 0.05640667
case acc: 0.11111775
case acc: 0.09762056
case acc: 0.14972663
case acc: 0.102376424
top acc: 0.2891 ::: bot acc: 0.0543
top acc: 0.1011 ::: bot acc: 0.0160
top acc: 0.2077 ::: bot acc: 0.0382
top acc: 0.1551 ::: bot acc: 0.0502
top acc: 0.2277 ::: bot acc: 0.0725
top acc: 0.1646 ::: bot acc: 0.0433
current epoch: 16
train loss is 0.080365
average val loss: 0.102946, accuracy: 0.1055
average test loss: 0.116230, accuracy: 0.1113
case acc: 0.16341424
case acc: 0.055120282
case acc: 0.1069438
case acc: 0.09665455
case acc: 0.14588378
case acc: 0.10006188
top acc: 0.2840 ::: bot acc: 0.0494
top acc: 0.0995 ::: bot acc: 0.0152
top acc: 0.2032 ::: bot acc: 0.0345
top acc: 0.1525 ::: bot acc: 0.0525
top acc: 0.2238 ::: bot acc: 0.0687
top acc: 0.1609 ::: bot acc: 0.0438
current epoch: 17
train loss is 0.079970
average val loss: 0.102339, accuracy: 0.1050
average test loss: 0.115686, accuracy: 0.1108
case acc: 0.16116318
case acc: 0.056352884
case acc: 0.10569911
case acc: 0.09681289
case acc: 0.14497982
case acc: 0.09969637
top acc: 0.2817 ::: bot acc: 0.0473
top acc: 0.1009 ::: bot acc: 0.0160
top acc: 0.2018 ::: bot acc: 0.0335
top acc: 0.1528 ::: bot acc: 0.0522
top acc: 0.2229 ::: bot acc: 0.0679
top acc: 0.1604 ::: bot acc: 0.0438
current epoch: 18
train loss is 0.079326
average val loss: 0.101431, accuracy: 0.1042
average test loss: 0.114819, accuracy: 0.1099
case acc: 0.15847425
case acc: 0.057150334
case acc: 0.10414033
case acc: 0.09679724
case acc: 0.14363986
case acc: 0.09900095
top acc: 0.2787 ::: bot acc: 0.0450
top acc: 0.1018 ::: bot acc: 0.0165
top acc: 0.2000 ::: bot acc: 0.0324
top acc: 0.1526 ::: bot acc: 0.0523
top acc: 0.2215 ::: bot acc: 0.0666
top acc: 0.1593 ::: bot acc: 0.0440
current epoch: 19
train loss is 0.079127
average val loss: 0.100654, accuracy: 0.1035
average test loss: 0.114082, accuracy: 0.1091
case acc: 0.15607491
case acc: 0.05820209
case acc: 0.102601975
case acc: 0.096861586
case acc: 0.14256907
case acc: 0.09849346
top acc: 0.2760 ::: bot acc: 0.0432
top acc: 0.1031 ::: bot acc: 0.0172
top acc: 0.1984 ::: bot acc: 0.0313
top acc: 0.1526 ::: bot acc: 0.0523
top acc: 0.2204 ::: bot acc: 0.0654
top acc: 0.1584 ::: bot acc: 0.0443
current epoch: 20
train loss is 0.078421
average val loss: 0.098338, accuracy: 0.1013
average test loss: 0.111803, accuracy: 0.1066
case acc: 0.15189064
case acc: 0.057126805
case acc: 0.09899685
case acc: 0.09612924
case acc: 0.1391071
case acc: 0.096609876
top acc: 0.2715 ::: bot acc: 0.0400
top acc: 0.1018 ::: bot acc: 0.0165
top acc: 0.1943 ::: bot acc: 0.0290
top acc: 0.1502 ::: bot acc: 0.0546
top acc: 0.2170 ::: bot acc: 0.0619
top acc: 0.1553 ::: bot acc: 0.0450
current epoch: 21
train loss is 0.078236
average val loss: 0.097711, accuracy: 0.1007
average test loss: 0.111226, accuracy: 0.1061
case acc: 0.15025756
case acc: 0.05815538
case acc: 0.09752086
case acc: 0.09618288
case acc: 0.13808545
case acc: 0.0962372
top acc: 0.2700 ::: bot acc: 0.0387
top acc: 0.1030 ::: bot acc: 0.0171
top acc: 0.1927 ::: bot acc: 0.0281
top acc: 0.1503 ::: bot acc: 0.0546
top acc: 0.2161 ::: bot acc: 0.0608
top acc: 0.1547 ::: bot acc: 0.0452
current epoch: 22
train loss is 0.078131
average val loss: 0.096900, accuracy: 0.0999
average test loss: 0.110463, accuracy: 0.1053
case acc: 0.14839727
case acc: 0.05887205
case acc: 0.09597923
case acc: 0.096122086
case acc: 0.13669106
case acc: 0.0956685
top acc: 0.2679 ::: bot acc: 0.0374
top acc: 0.1038 ::: bot acc: 0.0176
top acc: 0.1911 ::: bot acc: 0.0272
top acc: 0.1500 ::: bot acc: 0.0549
top acc: 0.2148 ::: bot acc: 0.0594
top acc: 0.1538 ::: bot acc: 0.0455
current epoch: 23
train loss is 0.077571
average val loss: 0.096493, accuracy: 0.0996
average test loss: 0.110084, accuracy: 0.1049
case acc: 0.14706288
case acc: 0.059926093
case acc: 0.09512297
case acc: 0.09614269
case acc: 0.1358642
case acc: 0.095354855
top acc: 0.2665 ::: bot acc: 0.0364
top acc: 0.1051 ::: bot acc: 0.0183
top acc: 0.1902 ::: bot acc: 0.0267
top acc: 0.1501 ::: bot acc: 0.0547
top acc: 0.2140 ::: bot acc: 0.0585
top acc: 0.1532 ::: bot acc: 0.0456
current epoch: 24
train loss is 0.077074
average val loss: 0.094810, accuracy: 0.0980
average test loss: 0.108439, accuracy: 0.1031
case acc: 0.14403662
case acc: 0.059303753
case acc: 0.09257215
case acc: 0.095608935
case acc: 0.13311756
case acc: 0.09396349
top acc: 0.2629 ::: bot acc: 0.0344
top acc: 0.1043 ::: bot acc: 0.0179
top acc: 0.1869 ::: bot acc: 0.0255
top acc: 0.1483 ::: bot acc: 0.0564
top acc: 0.2112 ::: bot acc: 0.0560
top acc: 0.1510 ::: bot acc: 0.0461
current epoch: 25
train loss is 0.076973
average val loss: 0.094536, accuracy: 0.0977
average test loss: 0.108202, accuracy: 0.1029
case acc: 0.14292645
case acc: 0.06055219
case acc: 0.0918828
case acc: 0.09573412
case acc: 0.13252613
case acc: 0.09383267
top acc: 0.2615 ::: bot acc: 0.0336
top acc: 0.1057 ::: bot acc: 0.0188
top acc: 0.1861 ::: bot acc: 0.0251
top acc: 0.1486 ::: bot acc: 0.0560
top acc: 0.2106 ::: bot acc: 0.0553
top acc: 0.1507 ::: bot acc: 0.0463
current epoch: 26
train loss is 0.076881
average val loss: 0.096790, accuracy: 0.1000
average test loss: 0.110480, accuracy: 0.1056
case acc: 0.14520809
case acc: 0.06533109
case acc: 0.09440598
case acc: 0.09708808
case acc: 0.13571602
case acc: 0.09590685
top acc: 0.2641 ::: bot acc: 0.0351
top acc: 0.1108 ::: bot acc: 0.0229
top acc: 0.1894 ::: bot acc: 0.0263
top acc: 0.1527 ::: bot acc: 0.0519
top acc: 0.2140 ::: bot acc: 0.0584
top acc: 0.1542 ::: bot acc: 0.0453
current epoch: 27
train loss is 0.076319
average val loss: 0.095396, accuracy: 0.0987
average test loss: 0.109117, accuracy: 0.1041
case acc: 0.142513
case acc: 0.06489755
case acc: 0.09246681
case acc: 0.096523374
case acc: 0.13353635
case acc: 0.09471848
top acc: 0.2608 ::: bot acc: 0.0335
top acc: 0.1102 ::: bot acc: 0.0225
top acc: 0.1868 ::: bot acc: 0.0255
top acc: 0.1511 ::: bot acc: 0.0532
top acc: 0.2116 ::: bot acc: 0.0564
top acc: 0.1523 ::: bot acc: 0.0458
current epoch: 28
train loss is 0.076343
average val loss: 0.093529, accuracy: 0.0969
average test loss: 0.107277, accuracy: 0.1020
case acc: 0.1393176
case acc: 0.06365189
case acc: 0.0898592
case acc: 0.09572922
case acc: 0.13051498
case acc: 0.09307916
top acc: 0.2569 ::: bot acc: 0.0315
top acc: 0.1088 ::: bot acc: 0.0215
top acc: 0.1836 ::: bot acc: 0.0242
top acc: 0.1488 ::: bot acc: 0.0553
top acc: 0.2086 ::: bot acc: 0.0535
top acc: 0.1495 ::: bot acc: 0.0465
current epoch: 29
train loss is 0.075909
average val loss: 0.094796, accuracy: 0.0982
average test loss: 0.108579, accuracy: 0.1036
case acc: 0.14031893
case acc: 0.06702846
case acc: 0.0911652
case acc: 0.09654799
case acc: 0.13226143
case acc: 0.09424772
top acc: 0.2580 ::: bot acc: 0.0321
top acc: 0.1123 ::: bot acc: 0.0247
top acc: 0.1853 ::: bot acc: 0.0247
top acc: 0.1513 ::: bot acc: 0.0528
top acc: 0.2104 ::: bot acc: 0.0552
top acc: 0.1516 ::: bot acc: 0.0459
current epoch: 30
train loss is 0.075712
average val loss: 0.094728, accuracy: 0.0982
average test loss: 0.108536, accuracy: 0.1036
case acc: 0.13954951
case acc: 0.06834748
case acc: 0.09091081
case acc: 0.096710816
case acc: 0.13194142
case acc: 0.09414253
top acc: 0.2572 ::: bot acc: 0.0317
top acc: 0.1136 ::: bot acc: 0.0259
top acc: 0.1851 ::: bot acc: 0.0245
top acc: 0.1516 ::: bot acc: 0.0524
top acc: 0.2102 ::: bot acc: 0.0548
top acc: 0.1514 ::: bot acc: 0.0460
current epoch: 31
train loss is 0.075533
average val loss: 0.095216, accuracy: 0.0988
average test loss: 0.109040, accuracy: 0.1042
case acc: 0.139456
case acc: 0.070472084
case acc: 0.091367066
case acc: 0.09709796
case acc: 0.13251948
case acc: 0.09457128
top acc: 0.2570 ::: bot acc: 0.0317
top acc: 0.1158 ::: bot acc: 0.0281
top acc: 0.1858 ::: bot acc: 0.0247
top acc: 0.1527 ::: bot acc: 0.0511
top acc: 0.2107 ::: bot acc: 0.0553
top acc: 0.1522 ::: bot acc: 0.0458
current epoch: 32
train loss is 0.075197
average val loss: 0.095000, accuracy: 0.0986
average test loss: 0.108848, accuracy: 0.1041
case acc: 0.13851584
case acc: 0.071493015
case acc: 0.09088555
case acc: 0.09714545
case acc: 0.13200769
case acc: 0.09434011
top acc: 0.2558 ::: bot acc: 0.0312
top acc: 0.1167 ::: bot acc: 0.0292
top acc: 0.1850 ::: bot acc: 0.0245
top acc: 0.1528 ::: bot acc: 0.0508
top acc: 0.2101 ::: bot acc: 0.0550
top acc: 0.1520 ::: bot acc: 0.0458
current epoch: 33
train loss is 0.075126
average val loss: 0.095420, accuracy: 0.0991
average test loss: 0.109283, accuracy: 0.1046
case acc: 0.13838007
case acc: 0.073442765
case acc: 0.0910763
case acc: 0.097576834
case acc: 0.1325913
case acc: 0.094726875
top acc: 0.2557 ::: bot acc: 0.0311
top acc: 0.1187 ::: bot acc: 0.0312
top acc: 0.1854 ::: bot acc: 0.0246
top acc: 0.1539 ::: bot acc: 0.0497
top acc: 0.2108 ::: bot acc: 0.0555
top acc: 0.1526 ::: bot acc: 0.0457
current epoch: 34
train loss is 0.075056
average val loss: 0.094215, accuracy: 0.0980
average test loss: 0.108099, accuracy: 0.1033
case acc: 0.13615966
case acc: 0.07288261
case acc: 0.089338005
case acc: 0.097048745
case acc: 0.13074316
case acc: 0.09366742
top acc: 0.2527 ::: bot acc: 0.0300
top acc: 0.1181 ::: bot acc: 0.0307
top acc: 0.1831 ::: bot acc: 0.0238
top acc: 0.1525 ::: bot acc: 0.0509
top acc: 0.2088 ::: bot acc: 0.0537
top acc: 0.1509 ::: bot acc: 0.0460
current epoch: 35
train loss is 0.075048
average val loss: 0.096557, accuracy: 0.1004
average test loss: 0.110431, accuracy: 0.1061
case acc: 0.13847023
case acc: 0.077649236
case acc: 0.09174417
case acc: 0.09859394
case acc: 0.13424936
case acc: 0.095828116
top acc: 0.2553 ::: bot acc: 0.0314
top acc: 0.1228 ::: bot acc: 0.0354
top acc: 0.1861 ::: bot acc: 0.0250
top acc: 0.1564 ::: bot acc: 0.0473
top acc: 0.2123 ::: bot acc: 0.0572
top acc: 0.1546 ::: bot acc: 0.0451
current epoch: 36
train loss is 0.074529
average val loss: 0.095957, accuracy: 0.0999
average test loss: 0.109847, accuracy: 0.1055
case acc: 0.13720411
case acc: 0.07790759
case acc: 0.09058894
case acc: 0.09840552
case acc: 0.13333155
case acc: 0.09533776
top acc: 0.2537 ::: bot acc: 0.0307
top acc: 0.1230 ::: bot acc: 0.0357
top acc: 0.1847 ::: bot acc: 0.0244
top acc: 0.1559 ::: bot acc: 0.0475
top acc: 0.2113 ::: bot acc: 0.0563
top acc: 0.1538 ::: bot acc: 0.0452
current epoch: 37
train loss is 0.074651
average val loss: 0.096349, accuracy: 0.1003
average test loss: 0.110237, accuracy: 0.1060
case acc: 0.13722245
case acc: 0.07958098
case acc: 0.09063089
case acc: 0.09876664
case acc: 0.13379954
case acc: 0.09573277
top acc: 0.2539 ::: bot acc: 0.0306
top acc: 0.1247 ::: bot acc: 0.0375
top acc: 0.1848 ::: bot acc: 0.0244
top acc: 0.1569 ::: bot acc: 0.0466
top acc: 0.2118 ::: bot acc: 0.0567
top acc: 0.1545 ::: bot acc: 0.0451
current epoch: 38
train loss is 0.074818
average val loss: 0.095542, accuracy: 0.0995
average test loss: 0.109457, accuracy: 0.1051
case acc: 0.13580833
case acc: 0.0793823
case acc: 0.0893524
case acc: 0.09839557
case acc: 0.13253736
case acc: 0.09503952
top acc: 0.2522 ::: bot acc: 0.0299
top acc: 0.1244 ::: bot acc: 0.0374
top acc: 0.1833 ::: bot acc: 0.0237
top acc: 0.1560 ::: bot acc: 0.0471
top acc: 0.2107 ::: bot acc: 0.0554
top acc: 0.1535 ::: bot acc: 0.0453
current epoch: 39
train loss is 0.074576
average val loss: 0.094554, accuracy: 0.0986
average test loss: 0.108502, accuracy: 0.1040
case acc: 0.13433973
case acc: 0.078837916
case acc: 0.08786863
case acc: 0.09796477
case acc: 0.13092752
case acc: 0.09419957
top acc: 0.2503 ::: bot acc: 0.0292
top acc: 0.1239 ::: bot acc: 0.0367
top acc: 0.1816 ::: bot acc: 0.0231
top acc: 0.1547 ::: bot acc: 0.0481
top acc: 0.2090 ::: bot acc: 0.0539
top acc: 0.1521 ::: bot acc: 0.0456
current epoch: 40
train loss is 0.074402
average val loss: 0.094132, accuracy: 0.0982
average test loss: 0.108133, accuracy: 0.1036
case acc: 0.13354982
case acc: 0.079080485
case acc: 0.08724333
case acc: 0.09783934
case acc: 0.13026914
case acc: 0.09385598
top acc: 0.2491 ::: bot acc: 0.0291
top acc: 0.1241 ::: bot acc: 0.0369
top acc: 0.1809 ::: bot acc: 0.0229
top acc: 0.1541 ::: bot acc: 0.0484
top acc: 0.2083 ::: bot acc: 0.0532
top acc: 0.1516 ::: bot acc: 0.0458
current epoch: 41
train loss is 0.074260
average val loss: 0.095864, accuracy: 0.0999
average test loss: 0.109883, accuracy: 0.1057
case acc: 0.1354293
case acc: 0.08257188
case acc: 0.08918123
case acc: 0.09904553
case acc: 0.13265213
case acc: 0.09545268
top acc: 0.2514 ::: bot acc: 0.0299
top acc: 0.1276 ::: bot acc: 0.0404
top acc: 0.1835 ::: bot acc: 0.0238
top acc: 0.1568 ::: bot acc: 0.0460
top acc: 0.2108 ::: bot acc: 0.0555
top acc: 0.1542 ::: bot acc: 0.0453
current epoch: 42
train loss is 0.074300
average val loss: 0.096739, accuracy: 0.1008
average test loss: 0.110762, accuracy: 0.1068
case acc: 0.13614129
case acc: 0.08468206
case acc: 0.09017421
case acc: 0.099697046
case acc: 0.13369699
case acc: 0.09618707
top acc: 0.2523 ::: bot acc: 0.0303
top acc: 0.1297 ::: bot acc: 0.0424
top acc: 0.1848 ::: bot acc: 0.0243
top acc: 0.1581 ::: bot acc: 0.0449
top acc: 0.2119 ::: bot acc: 0.0564
top acc: 0.1555 ::: bot acc: 0.0451
current epoch: 43
train loss is 0.074305
average val loss: 0.098210, accuracy: 0.1023
average test loss: 0.112218, accuracy: 0.1084
case acc: 0.13755268
case acc: 0.08751796
case acc: 0.0918918
case acc: 0.10067906
case acc: 0.13556248
case acc: 0.0974133
top acc: 0.2539 ::: bot acc: 0.0311
top acc: 0.1325 ::: bot acc: 0.0453
top acc: 0.1870 ::: bot acc: 0.0250
top acc: 0.1603 ::: bot acc: 0.0433
top acc: 0.2138 ::: bot acc: 0.0583
top acc: 0.1575 ::: bot acc: 0.0447
current epoch: 44
train loss is 0.074414
average val loss: 0.099371, accuracy: 0.1034
average test loss: 0.113390, accuracy: 0.1098
case acc: 0.1385165
case acc: 0.089879364
case acc: 0.0932691
case acc: 0.10155541
case acc: 0.13708016
case acc: 0.09835898
top acc: 0.2551 ::: bot acc: 0.0317
top acc: 0.1349 ::: bot acc: 0.0476
top acc: 0.1889 ::: bot acc: 0.0256
top acc: 0.1620 ::: bot acc: 0.0421
top acc: 0.2154 ::: bot acc: 0.0596
top acc: 0.1591 ::: bot acc: 0.0445
current epoch: 45
train loss is 0.074447
average val loss: 0.098054, accuracy: 0.1022
average test loss: 0.112115, accuracy: 0.1083
case acc: 0.13658708
case acc: 0.088655114
case acc: 0.091771744
case acc: 0.10079177
case acc: 0.13514334
case acc: 0.09714133
top acc: 0.2526 ::: bot acc: 0.0307
top acc: 0.1336 ::: bot acc: 0.0464
top acc: 0.1872 ::: bot acc: 0.0249
top acc: 0.1601 ::: bot acc: 0.0433
top acc: 0.2135 ::: bot acc: 0.0577
top acc: 0.1572 ::: bot acc: 0.0448
current epoch: 46
train loss is 0.074493
average val loss: 0.098643, accuracy: 0.1028
average test loss: 0.112705, accuracy: 0.1090
case acc: 0.13685371
case acc: 0.09011182
case acc: 0.09232819
case acc: 0.101252854
case acc: 0.13595945
case acc: 0.097572275
top acc: 0.2529 ::: bot acc: 0.0309
top acc: 0.1351 ::: bot acc: 0.0479
top acc: 0.1880 ::: bot acc: 0.0251
top acc: 0.1611 ::: bot acc: 0.0425
top acc: 0.2143 ::: bot acc: 0.0585
top acc: 0.1580 ::: bot acc: 0.0445
current epoch: 47
train loss is 0.074401
average val loss: 0.097766, accuracy: 0.1020
average test loss: 0.111868, accuracy: 0.1081
case acc: 0.13548094
case acc: 0.08945094
case acc: 0.09114076
case acc: 0.10072631
case acc: 0.13481422
case acc: 0.09674838
top acc: 0.2512 ::: bot acc: 0.0301
top acc: 0.1342 ::: bot acc: 0.0474
top acc: 0.1865 ::: bot acc: 0.0245
top acc: 0.1600 ::: bot acc: 0.0430
top acc: 0.2132 ::: bot acc: 0.0575
top acc: 0.1568 ::: bot acc: 0.0446
current epoch: 48
train loss is 0.074481
average val loss: 0.095398, accuracy: 0.0998
average test loss: 0.109542, accuracy: 0.1054
case acc: 0.13248721
case acc: 0.086535394
case acc: 0.08811156
case acc: 0.09922442
case acc: 0.13140836
case acc: 0.09461744
top acc: 0.2472 ::: bot acc: 0.0290
top acc: 0.1312 ::: bot acc: 0.0446
top acc: 0.1827 ::: bot acc: 0.0231
top acc: 0.1567 ::: bot acc: 0.0450
top acc: 0.2097 ::: bot acc: 0.0542
top acc: 0.1535 ::: bot acc: 0.0450
current epoch: 49
train loss is 0.074256
average val loss: 0.095296, accuracy: 0.0997
average test loss: 0.109449, accuracy: 0.1053
case acc: 0.13225034
case acc: 0.08694273
case acc: 0.08767942
case acc: 0.09923788
case acc: 0.13117968
case acc: 0.09451474
top acc: 0.2468 ::: bot acc: 0.0289
top acc: 0.1316 ::: bot acc: 0.0450
top acc: 0.1823 ::: bot acc: 0.0228
top acc: 0.1566 ::: bot acc: 0.0449
top acc: 0.2095 ::: bot acc: 0.0540
top acc: 0.1534 ::: bot acc: 0.0449
current epoch: 50
train loss is 0.074274
average val loss: 0.095639, accuracy: 0.1000
average test loss: 0.109839, accuracy: 0.1058
case acc: 0.13266394
case acc: 0.08798718
case acc: 0.08785394
case acc: 0.09961866
case acc: 0.13158965
case acc: 0.094830886
top acc: 0.2476 ::: bot acc: 0.0290
top acc: 0.1327 ::: bot acc: 0.0461
top acc: 0.1829 ::: bot acc: 0.0226
top acc: 0.1574 ::: bot acc: 0.0445
top acc: 0.2101 ::: bot acc: 0.0543
top acc: 0.1541 ::: bot acc: 0.0447

		{"drop_out": 0.4, "drop_out_mc": 0.1, "repeat_mc": 50, "hidden": 30, "embedding_size": 5, "batch": 512, "lag": 3}
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
LME_Co_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5112 5112 5112
1.7082474 -0.6288155 0.48142856 -0.33910522
Validation: 570 570 570
Testing: 774 774 774
pre-processing time: 0.0002856254577636719
the split date is 2010-07-01
net initializing with time: 0.004424333572387695
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.140225
average val loss: 0.114828, accuracy: 0.1156
average test loss: 0.174725, accuracy: 0.1804
case acc: 0.24796191
case acc: 0.04365469
case acc: 0.23042159
case acc: 0.19569041
case acc: 0.19725072
case acc: 0.1671868
top acc: 0.2028 ::: bot acc: 0.3073
top acc: 0.0688 ::: bot acc: 0.0351
top acc: 0.1701 ::: bot acc: 0.2856
top acc: 0.1483 ::: bot acc: 0.2372
top acc: 0.1632 ::: bot acc: 0.2290
top acc: 0.1302 ::: bot acc: 0.2011
current epoch: 2
train loss is 0.137464
average val loss: 0.137843, accuracy: 0.1427
average test loss: 0.067964, accuracy: 0.0682
case acc: 0.056930512
case acc: 0.20644723
case acc: 0.053399242
case acc: 0.03654847
case acc: 0.028600568
case acc: 0.027266195
top acc: 0.0137 ::: bot acc: 0.1157
top acc: 0.2551 ::: bot acc: 0.1509
top acc: 0.0268 ::: bot acc: 0.0922
top acc: 0.0373 ::: bot acc: 0.0526
top acc: 0.0206 ::: bot acc: 0.0474
top acc: 0.0488 ::: bot acc: 0.0227
current epoch: 3
train loss is 0.146084
average val loss: 0.146036, accuracy: 0.1497
average test loss: 0.067007, accuracy: 0.0654
case acc: 0.045015167
case acc: 0.21078543
case acc: 0.047798518
case acc: 0.035055283
case acc: 0.025114758
case acc: 0.02864634
top acc: 0.0092 ::: bot acc: 0.1000
top acc: 0.2595 ::: bot acc: 0.1551
top acc: 0.0406 ::: bot acc: 0.0767
top acc: 0.0476 ::: bot acc: 0.0430
top acc: 0.0287 ::: bot acc: 0.0377
top acc: 0.0540 ::: bot acc: 0.0184
current epoch: 4
train loss is 0.138690
average val loss: 0.087187, accuracy: 0.0939
average test loss: 0.107469, accuracy: 0.1164
case acc: 0.15005279
case acc: 0.08854685
case acc: 0.13373068
case acc: 0.114969105
case acc: 0.11164077
case acc: 0.09946958
top acc: 0.1052 ::: bot acc: 0.2097
top acc: 0.1368 ::: bot acc: 0.0337
top acc: 0.0723 ::: bot acc: 0.1901
top acc: 0.0663 ::: bot acc: 0.1574
top acc: 0.0772 ::: bot acc: 0.1436
top acc: 0.0614 ::: bot acc: 0.1340
current epoch: 5
train loss is 0.109604
average val loss: 0.088290, accuracy: 0.0941
average test loss: 0.086201, accuracy: 0.0950
case acc: 0.11912658
case acc: 0.10529701
case acc: 0.108369514
case acc: 0.088542074
case acc: 0.07535559
case acc: 0.073131256
top acc: 0.0745 ::: bot acc: 0.1788
top acc: 0.1540 ::: bot acc: 0.0490
top acc: 0.0470 ::: bot acc: 0.1648
top acc: 0.0398 ::: bot acc: 0.1310
top acc: 0.0411 ::: bot acc: 0.1073
top acc: 0.0355 ::: bot acc: 0.1074
current epoch: 6
train loss is 0.110096
average val loss: 0.099255, accuracy: 0.1042
average test loss: 0.067870, accuracy: 0.0752
case acc: 0.08655726
case acc: 0.124286324
case acc: 0.08332386
case acc: 0.064237066
case acc: 0.043136235
case acc: 0.049809344
top acc: 0.0422 ::: bot acc: 0.1463
top acc: 0.1729 ::: bot acc: 0.0679
top acc: 0.0278 ::: bot acc: 0.1370
top acc: 0.0204 ::: bot acc: 0.1042
top acc: 0.0168 ::: bot acc: 0.0711
top acc: 0.0165 ::: bot acc: 0.0820
current epoch: 7
train loss is 0.111499
average val loss: 0.089164, accuracy: 0.0941
average test loss: 0.073971, accuracy: 0.0822
case acc: 0.09903712
case acc: 0.099003755
case acc: 0.097549126
case acc: 0.07976529
case acc: 0.05020982
case acc: 0.067436986
top acc: 0.0547 ::: bot acc: 0.1588
top acc: 0.1476 ::: bot acc: 0.0426
top acc: 0.0378 ::: bot acc: 0.1534
top acc: 0.0314 ::: bot acc: 0.1221
top acc: 0.0198 ::: bot acc: 0.0801
top acc: 0.0298 ::: bot acc: 0.1017
current epoch: 8
train loss is 0.107291
average val loss: 0.083763, accuracy: 0.0886
average test loss: 0.077881, accuracy: 0.0860
case acc: 0.10575171
case acc: 0.08133245
case acc: 0.10725788
case acc: 0.09019985
case acc: 0.05270141
case acc: 0.078639165
top acc: 0.0615 ::: bot acc: 0.1655
top acc: 0.1288 ::: bot acc: 0.0270
top acc: 0.0459 ::: bot acc: 0.1640
top acc: 0.0412 ::: bot acc: 0.1330
top acc: 0.0212 ::: bot acc: 0.0832
top acc: 0.0397 ::: bot acc: 0.1136
current epoch: 9
train loss is 0.103190
average val loss: 0.084402, accuracy: 0.0888
average test loss: 0.073039, accuracy: 0.0810
case acc: 0.0981177
case acc: 0.07806223
case acc: 0.10342261
case acc: 0.08637587
case acc: 0.044363774
case acc: 0.07552812
top acc: 0.0540 ::: bot acc: 0.1578
top acc: 0.1251 ::: bot acc: 0.0246
top acc: 0.0424 ::: bot acc: 0.1599
top acc: 0.0375 ::: bot acc: 0.1291
top acc: 0.0172 ::: bot acc: 0.0727
top acc: 0.0369 ::: bot acc: 0.1103
current epoch: 10
train loss is 0.100628
average val loss: 0.085355, accuracy: 0.0895
average test loss: 0.068873, accuracy: 0.0766
case acc: 0.09083497
case acc: 0.07507772
case acc: 0.099530324
case acc: 0.08293762
case acc: 0.03826778
case acc: 0.07315088
top acc: 0.0468 ::: bot acc: 0.1506
top acc: 0.1218 ::: bot acc: 0.0225
top acc: 0.0390 ::: bot acc: 0.1558
top acc: 0.0341 ::: bot acc: 0.1255
top acc: 0.0169 ::: bot acc: 0.0637
top acc: 0.0349 ::: bot acc: 0.1078
current epoch: 11
train loss is 0.099761
average val loss: 0.085998, accuracy: 0.0899
average test loss: 0.065847, accuracy: 0.0734
case acc: 0.08506652
case acc: 0.0715295
case acc: 0.096514665
case acc: 0.08072712
case acc: 0.034054972
case acc: 0.07228448
top acc: 0.0410 ::: bot acc: 0.1448
top acc: 0.1178 ::: bot acc: 0.0200
top acc: 0.0366 ::: bot acc: 0.1525
top acc: 0.0320 ::: bot acc: 0.1232
top acc: 0.0177 ::: bot acc: 0.0571
top acc: 0.0344 ::: bot acc: 0.1068
current epoch: 12
train loss is 0.098149
average val loss: 0.085977, accuracy: 0.0897
average test loss: 0.063997, accuracy: 0.0712
case acc: 0.08121968
case acc: 0.067287825
case acc: 0.09473292
case acc: 0.079771936
case acc: 0.031730276
case acc: 0.07273735
top acc: 0.0372 ::: bot acc: 0.1410
top acc: 0.1127 ::: bot acc: 0.0174
top acc: 0.0351 ::: bot acc: 0.1505
top acc: 0.0311 ::: bot acc: 0.1223
top acc: 0.0189 ::: bot acc: 0.0531
top acc: 0.0350 ::: bot acc: 0.1072
current epoch: 13
train loss is 0.096731
average val loss: 0.083819, accuracy: 0.0874
average test loss: 0.064825, accuracy: 0.0718
case acc: 0.081989504
case acc: 0.060755733
case acc: 0.09652905
case acc: 0.082484476
case acc: 0.032386705
case acc: 0.07667588
top acc: 0.0380 ::: bot acc: 0.1417
top acc: 0.1043 ::: bot acc: 0.0147
top acc: 0.0364 ::: bot acc: 0.1526
top acc: 0.0334 ::: bot acc: 0.1253
top acc: 0.0185 ::: bot acc: 0.0542
top acc: 0.0386 ::: bot acc: 0.1113
current epoch: 14
train loss is 0.095794
average val loss: 0.083728, accuracy: 0.0872
average test loss: 0.063560, accuracy: 0.0703
case acc: 0.079209134
case acc: 0.05802933
case acc: 0.09479916
case acc: 0.08146885
case acc: 0.03132682
case acc: 0.076883405
top acc: 0.0352 ::: bot acc: 0.1390
top acc: 0.1001 ::: bot acc: 0.0148
top acc: 0.0350 ::: bot acc: 0.1507
top acc: 0.0323 ::: bot acc: 0.1244
top acc: 0.0192 ::: bot acc: 0.0524
top acc: 0.0387 ::: bot acc: 0.1116
current epoch: 15
train loss is 0.094799
average val loss: 0.083576, accuracy: 0.0869
average test loss: 0.062684, accuracy: 0.0692
case acc: 0.077022366
case acc: 0.055991225
case acc: 0.09310471
case acc: 0.0807033
case acc: 0.030860346
case acc: 0.07746733
top acc: 0.0330 ::: bot acc: 0.1368
top acc: 0.0965 ::: bot acc: 0.0158
top acc: 0.0337 ::: bot acc: 0.1489
top acc: 0.0316 ::: bot acc: 0.1236
top acc: 0.0196 ::: bot acc: 0.0515
top acc: 0.0391 ::: bot acc: 0.1123
current epoch: 16
train loss is 0.093492
average val loss: 0.083398, accuracy: 0.0867
average test loss: 0.062122, accuracy: 0.0684
case acc: 0.07517936
case acc: 0.05446281
case acc: 0.09129522
case acc: 0.08017528
case acc: 0.030861693
case acc: 0.078557186
top acc: 0.0312 ::: bot acc: 0.1350
top acc: 0.0936 ::: bot acc: 0.0171
top acc: 0.0323 ::: bot acc: 0.1469
top acc: 0.0311 ::: bot acc: 0.1232
top acc: 0.0196 ::: bot acc: 0.0514
top acc: 0.0402 ::: bot acc: 0.1134
current epoch: 17
train loss is 0.093013
average val loss: 0.086368, accuracy: 0.0894
average test loss: 0.058495, accuracy: 0.0645
case acc: 0.0682929
case acc: 0.056110967
case acc: 0.08469222
case acc: 0.074680075
case acc: 0.02856661
case acc: 0.07451442
top acc: 0.0245 ::: bot acc: 0.1280
top acc: 0.0967 ::: bot acc: 0.0158
top acc: 0.0278 ::: bot acc: 0.1393
top acc: 0.0265 ::: bot acc: 0.1172
top acc: 0.0220 ::: bot acc: 0.0468
top acc: 0.0366 ::: bot acc: 0.1092
current epoch: 18
train loss is 0.093335
average val loss: 0.088801, accuracy: 0.0917
average test loss: 0.056001, accuracy: 0.0616
case acc: 0.06294247
case acc: 0.05735824
case acc: 0.079439595
case acc: 0.07074021
case acc: 0.027339198
case acc: 0.07192681
top acc: 0.0194 ::: bot acc: 0.1226
top acc: 0.0989 ::: bot acc: 0.0151
top acc: 0.0247 ::: bot acc: 0.1330
top acc: 0.0237 ::: bot acc: 0.1127
top acc: 0.0242 ::: bot acc: 0.0439
top acc: 0.0343 ::: bot acc: 0.1064
current epoch: 19
train loss is 0.092811
average val loss: 0.090547, accuracy: 0.0934
average test loss: 0.054374, accuracy: 0.0597
case acc: 0.05912364
case acc: 0.05802958
case acc: 0.07547201
case acc: 0.06805168
case acc: 0.026687317
case acc: 0.070544004
top acc: 0.0158 ::: bot acc: 0.1187
top acc: 0.1000 ::: bot acc: 0.0148
top acc: 0.0226 ::: bot acc: 0.1281
top acc: 0.0219 ::: bot acc: 0.1095
top acc: 0.0258 ::: bot acc: 0.0421
top acc: 0.0330 ::: bot acc: 0.1050
current epoch: 20
train loss is 0.092750
average val loss: 0.091669, accuracy: 0.0944
average test loss: 0.053306, accuracy: 0.0583
case acc: 0.056596994
case acc: 0.058055725
case acc: 0.07259039
case acc: 0.0662358
case acc: 0.026271567
case acc: 0.06992275
top acc: 0.0136 ::: bot acc: 0.1160
top acc: 0.1001 ::: bot acc: 0.0148
top acc: 0.0212 ::: bot acc: 0.1245
top acc: 0.0207 ::: bot acc: 0.1074
top acc: 0.0270 ::: bot acc: 0.0409
top acc: 0.0325 ::: bot acc: 0.1043
current epoch: 21
train loss is 0.092883
average val loss: 0.092044, accuracy: 0.0948
average test loss: 0.052758, accuracy: 0.0575
case acc: 0.055370115
case acc: 0.057339393
case acc: 0.07100181
case acc: 0.065326415
case acc: 0.026116718
case acc: 0.0699216
top acc: 0.0127 ::: bot acc: 0.1146
top acc: 0.0989 ::: bot acc: 0.0151
top acc: 0.0205 ::: bot acc: 0.1225
top acc: 0.0202 ::: bot acc: 0.1064
top acc: 0.0275 ::: bot acc: 0.0404
top acc: 0.0326 ::: bot acc: 0.1043
current epoch: 22
train loss is 0.092532
average val loss: 0.091095, accuracy: 0.0938
average test loss: 0.053037, accuracy: 0.0577
case acc: 0.056072466
case acc: 0.055426907
case acc: 0.071295075
case acc: 0.06582979
case acc: 0.026503237
case acc: 0.07107967
top acc: 0.0132 ::: bot acc: 0.1154
top acc: 0.0954 ::: bot acc: 0.0163
top acc: 0.0206 ::: bot acc: 0.1229
top acc: 0.0204 ::: bot acc: 0.1070
top acc: 0.0265 ::: bot acc: 0.0415
top acc: 0.0336 ::: bot acc: 0.1055
current epoch: 23
train loss is 0.092324
average val loss: 0.089501, accuracy: 0.0923
average test loss: 0.053769, accuracy: 0.0584
case acc: 0.057880998
case acc: 0.05312831
case acc: 0.07267272
case acc: 0.067004755
case acc: 0.027152553
case acc: 0.07260676
top acc: 0.0147 ::: bot acc: 0.1174
top acc: 0.0908 ::: bot acc: 0.0186
top acc: 0.0212 ::: bot acc: 0.1247
top acc: 0.0211 ::: bot acc: 0.1085
top acc: 0.0248 ::: bot acc: 0.0434
top acc: 0.0351 ::: bot acc: 0.1071
current epoch: 24
train loss is 0.091465
average val loss: 0.087290, accuracy: 0.0901
average test loss: 0.055122, accuracy: 0.0598
case acc: 0.060811196
case acc: 0.05068968
case acc: 0.07497994
case acc: 0.0690557
case acc: 0.02826754
case acc: 0.07495055
top acc: 0.0173 ::: bot acc: 0.1205
top acc: 0.0853 ::: bot acc: 0.0223
top acc: 0.0223 ::: bot acc: 0.1277
top acc: 0.0224 ::: bot acc: 0.1109
top acc: 0.0227 ::: bot acc: 0.0461
top acc: 0.0372 ::: bot acc: 0.1096
current epoch: 25
train loss is 0.090713
average val loss: 0.085766, accuracy: 0.0887
average test loss: 0.056091, accuracy: 0.0607
case acc: 0.06302352
case acc: 0.049023923
case acc: 0.07645088
case acc: 0.07027984
case acc: 0.02922961
case acc: 0.07640173
top acc: 0.0194 ::: bot acc: 0.1228
top acc: 0.0812 ::: bot acc: 0.0255
top acc: 0.0230 ::: bot acc: 0.1295
top acc: 0.0232 ::: bot acc: 0.1124
top acc: 0.0214 ::: bot acc: 0.0482
top acc: 0.0384 ::: bot acc: 0.1111
current epoch: 26
train loss is 0.089736
average val loss: 0.085268, accuracy: 0.0882
average test loss: 0.056332, accuracy: 0.0609
case acc: 0.063653424
case acc: 0.0482536
case acc: 0.07641303
case acc: 0.07036996
case acc: 0.029724488
case acc: 0.07685995
top acc: 0.0200 ::: bot acc: 0.1234
top acc: 0.0792 ::: bot acc: 0.0271
top acc: 0.0230 ::: bot acc: 0.1295
top acc: 0.0233 ::: bot acc: 0.1125
top acc: 0.0208 ::: bot acc: 0.0492
top acc: 0.0388 ::: bot acc: 0.1116
current epoch: 27
train loss is 0.089832
average val loss: 0.086698, accuracy: 0.0896
average test loss: 0.054981, accuracy: 0.0593
case acc: 0.061210882
case acc: 0.04890015
case acc: 0.07362783
case acc: 0.068102226
case acc: 0.029035565
case acc: 0.07501688
top acc: 0.0177 ::: bot acc: 0.1209
top acc: 0.0808 ::: bot acc: 0.0258
top acc: 0.0215 ::: bot acc: 0.1261
top acc: 0.0218 ::: bot acc: 0.1099
top acc: 0.0217 ::: bot acc: 0.0477
top acc: 0.0371 ::: bot acc: 0.1097
current epoch: 28
train loss is 0.089839
average val loss: 0.089699, accuracy: 0.0925
average test loss: 0.052679, accuracy: 0.0567
case acc: 0.05676265
case acc: 0.050692666
case acc: 0.069061235
case acc: 0.064209945
case acc: 0.027521415
case acc: 0.07166219
top acc: 0.0137 ::: bot acc: 0.1162
top acc: 0.0852 ::: bot acc: 0.0224
top acc: 0.0199 ::: bot acc: 0.1201
top acc: 0.0194 ::: bot acc: 0.1052
top acc: 0.0242 ::: bot acc: 0.0442
top acc: 0.0342 ::: bot acc: 0.1062
current epoch: 29
train loss is 0.090017
average val loss: 0.090087, accuracy: 0.0930
average test loss: 0.052593, accuracy: 0.0564
case acc: 0.05611152
case acc: 0.050959215
case acc: 0.067528516
case acc: 0.06382791
case acc: 0.027699977
case acc: 0.07250525
top acc: 0.0132 ::: bot acc: 0.1156
top acc: 0.0859 ::: bot acc: 0.0219
top acc: 0.0197 ::: bot acc: 0.1179
top acc: 0.0191 ::: bot acc: 0.1048
top acc: 0.0238 ::: bot acc: 0.0447
top acc: 0.0350 ::: bot acc: 0.1070
current epoch: 30
train loss is 0.090082
average val loss: 0.092226, accuracy: 0.0950
average test loss: 0.051329, accuracy: 0.0549
case acc: 0.053355154
case acc: 0.052337777
case acc: 0.064507425
case acc: 0.061486147
case acc: 0.026802197
case acc: 0.07086302
top acc: 0.0111 ::: bot acc: 0.1125
top acc: 0.0890 ::: bot acc: 0.0197
top acc: 0.0198 ::: bot acc: 0.1133
top acc: 0.0179 ::: bot acc: 0.1019
top acc: 0.0259 ::: bot acc: 0.0423
top acc: 0.0335 ::: bot acc: 0.1053
current epoch: 31
train loss is 0.090173
average val loss: 0.094171, accuracy: 0.0969
average test loss: 0.050227, accuracy: 0.0535
case acc: 0.051163387
case acc: 0.053496487
case acc: 0.06220108
case acc: 0.059431534
case acc: 0.025952747
case acc: 0.06899354
top acc: 0.0097 ::: bot acc: 0.1099
top acc: 0.0915 ::: bot acc: 0.0183
top acc: 0.0204 ::: bot acc: 0.1095
top acc: 0.0171 ::: bot acc: 0.0992
top acc: 0.0285 ::: bot acc: 0.0398
top acc: 0.0319 ::: bot acc: 0.1033
current epoch: 32
train loss is 0.090538
average val loss: 0.093995, accuracy: 0.0968
average test loss: 0.050241, accuracy: 0.0535
case acc: 0.051654674
case acc: 0.052987203
case acc: 0.062116195
case acc: 0.059420902
case acc: 0.025864905
case acc: 0.06914144
top acc: 0.0100 ::: bot acc: 0.1105
top acc: 0.0904 ::: bot acc: 0.0189
top acc: 0.0205 ::: bot acc: 0.1094
top acc: 0.0171 ::: bot acc: 0.0992
top acc: 0.0288 ::: bot acc: 0.0395
top acc: 0.0320 ::: bot acc: 0.1035
current epoch: 33
train loss is 0.090559
average val loss: 0.093190, accuracy: 0.0961
average test loss: 0.050454, accuracy: 0.0538
case acc: 0.053063944
case acc: 0.05188582
case acc: 0.06289035
case acc: 0.059838265
case acc: 0.02586845
case acc: 0.06923479
top acc: 0.0109 ::: bot acc: 0.1122
top acc: 0.0879 ::: bot acc: 0.0205
top acc: 0.0202 ::: bot acc: 0.1107
top acc: 0.0172 ::: bot acc: 0.0998
top acc: 0.0288 ::: bot acc: 0.0395
top acc: 0.0321 ::: bot acc: 0.1036
current epoch: 34
train loss is 0.090302
average val loss: 0.088176, accuracy: 0.0914
average test loss: 0.053510, accuracy: 0.0573
case acc: 0.06034838
case acc: 0.048191372
case acc: 0.0681489
case acc: 0.06482309
case acc: 0.027959608
case acc: 0.07415909
top acc: 0.0168 ::: bot acc: 0.1201
top acc: 0.0789 ::: bot acc: 0.0274
top acc: 0.0198 ::: bot acc: 0.1188
top acc: 0.0197 ::: bot acc: 0.1061
top acc: 0.0235 ::: bot acc: 0.0454
top acc: 0.0365 ::: bot acc: 0.1088
current epoch: 35
train loss is 0.089297
average val loss: 0.084634, accuracy: 0.0880
average test loss: 0.056067, accuracy: 0.0600
case acc: 0.066446446
case acc: 0.04558346
case acc: 0.072898574
case acc: 0.068462774
case acc: 0.029808914
case acc: 0.07703748
top acc: 0.0227 ::: bot acc: 0.1263
top acc: 0.0716 ::: bot acc: 0.0343
top acc: 0.0211 ::: bot acc: 0.1253
top acc: 0.0219 ::: bot acc: 0.1104
top acc: 0.0209 ::: bot acc: 0.0494
top acc: 0.0391 ::: bot acc: 0.1118
current epoch: 36
train loss is 0.088473
average val loss: 0.081620, accuracy: 0.0851
average test loss: 0.058752, accuracy: 0.0628
case acc: 0.07196383
case acc: 0.043690156
case acc: 0.07736221
case acc: 0.07181453
case acc: 0.032112777
case acc: 0.07984655
top acc: 0.0281 ::: bot acc: 0.1320
top acc: 0.0652 ::: bot acc: 0.0407
top acc: 0.0233 ::: bot acc: 0.1309
top acc: 0.0239 ::: bot acc: 0.1144
top acc: 0.0194 ::: bot acc: 0.0537
top acc: 0.0417 ::: bot acc: 0.1147
current epoch: 37
train loss is 0.087963
average val loss: 0.079442, accuracy: 0.0832
average test loss: 0.061129, accuracy: 0.0651
case acc: 0.076148264
case acc: 0.042578675
case acc: 0.08028162
case acc: 0.074424684
case acc: 0.034575358
case acc: 0.08252135
top acc: 0.0322 ::: bot acc: 0.1362
top acc: 0.0607 ::: bot acc: 0.0452
top acc: 0.0248 ::: bot acc: 0.1345
top acc: 0.0255 ::: bot acc: 0.1175
top acc: 0.0182 ::: bot acc: 0.0580
top acc: 0.0441 ::: bot acc: 0.1175
current epoch: 38
train loss is 0.087956
average val loss: 0.078923, accuracy: 0.0828
average test loss: 0.061930, accuracy: 0.0658
case acc: 0.07710729
case acc: 0.042423375
case acc: 0.08001508
case acc: 0.07506034
case acc: 0.036102332
case acc: 0.084111534
top acc: 0.0331 ::: bot acc: 0.1372
top acc: 0.0599 ::: bot acc: 0.0460
top acc: 0.0247 ::: bot acc: 0.1342
top acc: 0.0259 ::: bot acc: 0.1182
top acc: 0.0176 ::: bot acc: 0.0605
top acc: 0.0455 ::: bot acc: 0.1192
current epoch: 39
train loss is 0.088072
average val loss: 0.082404, accuracy: 0.0861
average test loss: 0.058281, accuracy: 0.0621
case acc: 0.07061966
case acc: 0.044153243
case acc: 0.07334661
case acc: 0.07010358
case acc: 0.03382794
case acc: 0.08039506
top acc: 0.0268 ::: bot acc: 0.1307
top acc: 0.0668 ::: bot acc: 0.0390
top acc: 0.0213 ::: bot acc: 0.1259
top acc: 0.0226 ::: bot acc: 0.1124
top acc: 0.0185 ::: bot acc: 0.0567
top acc: 0.0422 ::: bot acc: 0.1153
current epoch: 40
train loss is 0.088430
average val loss: 0.089016, accuracy: 0.0924
average test loss: 0.052978, accuracy: 0.0562
case acc: 0.05991595
case acc: 0.04808406
case acc: 0.06421769
case acc: 0.06209258
case acc: 0.02935951
case acc: 0.07340292
top acc: 0.0164 ::: bot acc: 0.1198
top acc: 0.0785 ::: bot acc: 0.0277
top acc: 0.0199 ::: bot acc: 0.1129
top acc: 0.0179 ::: bot acc: 0.1027
top acc: 0.0215 ::: bot acc: 0.0485
top acc: 0.0359 ::: bot acc: 0.1079
current epoch: 41
train loss is 0.088934
average val loss: 0.095241, accuracy: 0.0984
average test loss: 0.049536, accuracy: 0.0521
case acc: 0.051640444
case acc: 0.052193306
case acc: 0.058132544
case acc: 0.0561953
case acc: 0.026304882
case acc: 0.06800007
top acc: 0.0099 ::: bot acc: 0.1106
top acc: 0.0885 ::: bot acc: 0.0201
top acc: 0.0241 ::: bot acc: 0.1017
top acc: 0.0160 ::: bot acc: 0.0949
top acc: 0.0273 ::: bot acc: 0.0409
top acc: 0.0311 ::: bot acc: 0.1022
current epoch: 42
train loss is 0.089770
average val loss: 0.098855, accuracy: 0.1018
average test loss: 0.048071, accuracy: 0.0503
case acc: 0.04790359
case acc: 0.054749187
case acc: 0.055397738
case acc: 0.053606603
case acc: 0.024973411
case acc: 0.06543098
top acc: 0.0083 ::: bot acc: 0.1058
top acc: 0.0939 ::: bot acc: 0.0171
top acc: 0.0284 ::: bot acc: 0.0955
top acc: 0.0164 ::: bot acc: 0.0908
top acc: 0.0325 ::: bot acc: 0.0358
top acc: 0.0288 ::: bot acc: 0.0995
current epoch: 43
train loss is 0.091099
average val loss: 0.100014, accuracy: 0.1030
average test loss: 0.047388, accuracy: 0.0496
case acc: 0.04729292
case acc: 0.054980457
case acc: 0.054913033
case acc: 0.052644156
case acc: 0.024475379
case acc: 0.06356508
top acc: 0.0081 ::: bot acc: 0.1050
top acc: 0.0943 ::: bot acc: 0.0168
top acc: 0.0293 ::: bot acc: 0.0943
top acc: 0.0167 ::: bot acc: 0.0893
top acc: 0.0360 ::: bot acc: 0.0323
top acc: 0.0272 ::: bot acc: 0.0976
current epoch: 44
train loss is 0.092494
average val loss: 0.094034, accuracy: 0.0975
average test loss: 0.049601, accuracy: 0.0524
case acc: 0.054699462
case acc: 0.049972773
case acc: 0.059539188
case acc: 0.05702328
case acc: 0.025205385
case acc: 0.06776805
top acc: 0.0120 ::: bot acc: 0.1141
top acc: 0.0833 ::: bot acc: 0.0239
top acc: 0.0226 ::: bot acc: 0.1046
top acc: 0.0163 ::: bot acc: 0.0961
top acc: 0.0314 ::: bot acc: 0.0369
top acc: 0.0309 ::: bot acc: 0.1020
current epoch: 45
train loss is 0.091524
average val loss: 0.082815, accuracy: 0.0867
average test loss: 0.057100, accuracy: 0.0607
case acc: 0.07222505
case acc: 0.04294754
case acc: 0.07266361
case acc: 0.06839464
case acc: 0.029856442
case acc: 0.07785761
top acc: 0.0283 ::: bot acc: 0.1323
top acc: 0.0622 ::: bot acc: 0.0437
top acc: 0.0211 ::: bot acc: 0.1251
top acc: 0.0217 ::: bot acc: 0.1105
top acc: 0.0209 ::: bot acc: 0.0495
top acc: 0.0400 ::: bot acc: 0.1126
current epoch: 46
train loss is 0.088369
average val loss: 0.073604, accuracy: 0.0777
average test loss: 0.068652, accuracy: 0.0715
case acc: 0.09107896
case acc: 0.039913345
case acc: 0.0901684
case acc: 0.08147654
case acc: 0.038012482
case acc: 0.08827804
top acc: 0.0471 ::: bot acc: 0.1511
top acc: 0.0407 ::: bot acc: 0.0652
top acc: 0.0310 ::: bot acc: 0.1464
top acc: 0.0312 ::: bot acc: 0.1253
top acc: 0.0174 ::: bot acc: 0.0635
top acc: 0.0495 ::: bot acc: 0.1235
current epoch: 47
train loss is 0.087113
average val loss: 0.068410, accuracy: 0.0728
average test loss: 0.080973, accuracy: 0.0825
case acc: 0.107132666
case acc: 0.04125015
case acc: 0.10564038
case acc: 0.09410351
case acc: 0.04826866
case acc: 0.09864628
top acc: 0.0632 ::: bot acc: 0.1672
top acc: 0.0234 ::: bot acc: 0.0825
top acc: 0.0428 ::: bot acc: 0.1637
top acc: 0.0429 ::: bot acc: 0.1383
top acc: 0.0196 ::: bot acc: 0.0778
top acc: 0.0596 ::: bot acc: 0.1340
current epoch: 48
train loss is 0.089140
average val loss: 0.067211, accuracy: 0.0721
average test loss: 0.086177, accuracy: 0.0873
case acc: 0.11279085
case acc: 0.04287487
case acc: 0.11035387
case acc: 0.09886298
case acc: 0.055264525
case acc: 0.10352695
top acc: 0.0688 ::: bot acc: 0.1729
top acc: 0.0180 ::: bot acc: 0.0880
top acc: 0.0469 ::: bot acc: 0.1687
top acc: 0.0476 ::: bot acc: 0.1431
top acc: 0.0232 ::: bot acc: 0.0865
top acc: 0.0643 ::: bot acc: 0.1389
current epoch: 49
train loss is 0.091897
average val loss: 0.070171, accuracy: 0.0747
average test loss: 0.076240, accuracy: 0.0783
case acc: 0.09993997
case acc: 0.03970892
case acc: 0.09611643
case acc: 0.087827355
case acc: 0.05044031
case acc: 0.09566531
top acc: 0.0560 ::: bot acc: 0.1600
top acc: 0.0324 ::: bot acc: 0.0735
top acc: 0.0352 ::: bot acc: 0.1532
top acc: 0.0370 ::: bot acc: 0.1319
top acc: 0.0207 ::: bot acc: 0.0805
top acc: 0.0567 ::: bot acc: 0.1309
current epoch: 50
train loss is 0.091213
average val loss: 0.081457, accuracy: 0.0856
average test loss: 0.058750, accuracy: 0.0619
case acc: 0.07374457
case acc: 0.04255523
case acc: 0.071144275
case acc: 0.06749182
case acc: 0.03729601
case acc: 0.07893803
top acc: 0.0298 ::: bot acc: 0.1338
top acc: 0.0607 ::: bot acc: 0.0451
top acc: 0.0205 ::: bot acc: 0.1231
top acc: 0.0210 ::: bot acc: 0.1093
top acc: 0.0174 ::: bot acc: 0.0624
top acc: 0.0410 ::: bot acc: 0.1136
LME_Co_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5136 5136 5136
1.7082474 -0.6288155 0.48738334 -0.33910522
Validation: 576 576 576
Testing: 744 744 744
pre-processing time: 0.00048470497131347656
the split date is 2011-01-01
net initializing with time: 0.003736257553100586
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.221345
average val loss: 0.099322, accuracy: 0.1259
average test loss: 0.112874, accuracy: 0.1288
case acc: 0.18357827
case acc: 0.07435581
case acc: 0.21263741
case acc: 0.10780769
case acc: 0.10213123
case acc: 0.09206917
top acc: 0.3015 ::: bot acc: 0.0721
top acc: 0.1193 ::: bot acc: 0.0272
top acc: 0.1347 ::: bot acc: 0.2954
top acc: 0.0217 ::: bot acc: 0.1786
top acc: 0.1548 ::: bot acc: 0.1048
top acc: 0.1083 ::: bot acc: 0.1261
current epoch: 2
train loss is 0.208538
average val loss: 0.116871, accuracy: 0.1358
average test loss: 0.119948, accuracy: 0.1364
case acc: 0.25263816
case acc: 0.14089641
case acc: 0.13096192
case acc: 0.07496408
case acc: 0.12071744
case acc: 0.098068126
top acc: 0.3697 ::: bot acc: 0.1418
top acc: 0.1942 ::: bot acc: 0.0797
top acc: 0.0523 ::: bot acc: 0.2143
top acc: 0.0698 ::: bot acc: 0.1047
top acc: 0.2264 ::: bot acc: 0.0339
top acc: 0.1816 ::: bot acc: 0.0533
current epoch: 3
train loss is 0.142007
average val loss: 0.112342, accuracy: 0.1268
average test loss: 0.134733, accuracy: 0.1407
case acc: 0.10501171
case acc: 0.044478394
case acc: 0.28522855
case acc: 0.18657763
case acc: 0.10797559
case acc: 0.11503287
top acc: 0.1997 ::: bot acc: 0.0390
top acc: 0.0300 ::: bot acc: 0.0852
top acc: 0.2064 ::: bot acc: 0.3688
top acc: 0.0887 ::: bot acc: 0.2639
top acc: 0.0651 ::: bot acc: 0.1941
top acc: 0.0313 ::: bot acc: 0.2127
current epoch: 4
train loss is 0.131315
average val loss: 0.096164, accuracy: 0.1199
average test loss: 0.106492, accuracy: 0.1227
case acc: 0.19927606
case acc: 0.09617441
case acc: 0.15502484
case acc: 0.08650651
case acc: 0.10861466
case acc: 0.09060529
top acc: 0.3152 ::: bot acc: 0.0890
top acc: 0.1493 ::: bot acc: 0.0380
top acc: 0.0758 ::: bot acc: 0.2389
top acc: 0.0363 ::: bot acc: 0.1405
top acc: 0.1867 ::: bot acc: 0.0721
top acc: 0.1465 ::: bot acc: 0.0894
current epoch: 5
train loss is 0.135629
average val loss: 0.087128, accuracy: 0.1117
average test loss: 0.103925, accuracy: 0.1165
case acc: 0.14842463
case acc: 0.059631318
case acc: 0.18948695
case acc: 0.10894453
case acc: 0.09998668
case acc: 0.09246729
top acc: 0.2638 ::: bot acc: 0.0389
top acc: 0.1019 ::: bot acc: 0.0242
top acc: 0.1101 ::: bot acc: 0.2736
top acc: 0.0207 ::: bot acc: 0.1819
top acc: 0.1431 ::: bot acc: 0.1156
top acc: 0.1049 ::: bot acc: 0.1312
current epoch: 6
train loss is 0.118483
average val loss: 0.083886, accuracy: 0.1072
average test loss: 0.102452, accuracy: 0.1135
case acc: 0.13430232
case acc: 0.051828124
case acc: 0.18875624
case acc: 0.1135819
case acc: 0.09934024
case acc: 0.093278676
top acc: 0.2470 ::: bot acc: 0.0298
top acc: 0.0890 ::: bot acc: 0.0274
top acc: 0.1092 ::: bot acc: 0.2730
top acc: 0.0225 ::: bot acc: 0.1881
top acc: 0.1349 ::: bot acc: 0.1237
top acc: 0.0990 ::: bot acc: 0.1374
current epoch: 7
train loss is 0.120205
average val loss: 0.081483, accuracy: 0.1050
average test loss: 0.097578, accuracy: 0.1096
case acc: 0.14805888
case acc: 0.06342773
case acc: 0.1565655
case acc: 0.09720869
case acc: 0.101954915
case acc: 0.09047605
top acc: 0.2626 ::: bot acc: 0.0395
top acc: 0.1090 ::: bot acc: 0.0228
top acc: 0.0769 ::: bot acc: 0.2409
top acc: 0.0243 ::: bot acc: 0.1627
top acc: 0.1581 ::: bot acc: 0.1005
top acc: 0.1244 ::: bot acc: 0.1123
current epoch: 8
train loss is 0.116971
average val loss: 0.077500, accuracy: 0.0999
average test loss: 0.096518, accuracy: 0.1068
case acc: 0.12922277
case acc: 0.05214654
case acc: 0.16267256
case acc: 0.10474287
case acc: 0.100039005
case acc: 0.09181952
top acc: 0.2401 ::: bot acc: 0.0282
top acc: 0.0903 ::: bot acc: 0.0267
top acc: 0.0830 ::: bot acc: 0.2470
top acc: 0.0219 ::: bot acc: 0.1753
top acc: 0.1435 ::: bot acc: 0.1152
top acc: 0.1118 ::: bot acc: 0.1250
current epoch: 9
train loss is 0.112633
average val loss: 0.074967, accuracy: 0.0967
average test loss: 0.093806, accuracy: 0.1035
case acc: 0.12818292
case acc: 0.05340437
case acc: 0.14838585
case acc: 0.09973155
case acc: 0.10073449
case acc: 0.09084939
top acc: 0.2387 ::: bot acc: 0.0279
top acc: 0.0928 ::: bot acc: 0.0256
top acc: 0.0687 ::: bot acc: 0.2329
top acc: 0.0233 ::: bot acc: 0.1672
top acc: 0.1496 ::: bot acc: 0.1090
top acc: 0.1197 ::: bot acc: 0.1169
current epoch: 10
train loss is 0.110687
average val loss: 0.072173, accuracy: 0.0929
average test loss: 0.092453, accuracy: 0.1012
case acc: 0.12015952
case acc: 0.049256925
case acc: 0.14508802
case acc: 0.10128854
case acc: 0.100158185
case acc: 0.09118418
top acc: 0.2267 ::: bot acc: 0.0277
top acc: 0.0850 ::: bot acc: 0.0289
top acc: 0.0654 ::: bot acc: 0.2296
top acc: 0.0229 ::: bot acc: 0.1697
top acc: 0.1448 ::: bot acc: 0.1136
top acc: 0.1170 ::: bot acc: 0.1198
current epoch: 11
train loss is 0.106403
average val loss: 0.069728, accuracy: 0.0894
average test loss: 0.091191, accuracy: 0.0990
case acc: 0.114252046
case acc: 0.046607524
case acc: 0.14034495
case acc: 0.101793274
case acc: 0.09981698
case acc: 0.091250576
top acc: 0.2169 ::: bot acc: 0.0296
top acc: 0.0793 ::: bot acc: 0.0327
top acc: 0.0605 ::: bot acc: 0.2249
top acc: 0.0230 ::: bot acc: 0.1705
top acc: 0.1419 ::: bot acc: 0.1163
top acc: 0.1159 ::: bot acc: 0.1209
current epoch: 12
train loss is 0.104279
average val loss: 0.067587, accuracy: 0.0863
average test loss: 0.089790, accuracy: 0.0968
case acc: 0.11020515
case acc: 0.045393877
case acc: 0.1333926
case acc: 0.10081717
case acc: 0.099787794
case acc: 0.09114141
top acc: 0.2100 ::: bot acc: 0.0313
top acc: 0.0765 ::: bot acc: 0.0348
top acc: 0.0536 ::: bot acc: 0.2179
top acc: 0.0233 ::: bot acc: 0.1689
top acc: 0.1415 ::: bot acc: 0.1167
top acc: 0.1174 ::: bot acc: 0.1196
current epoch: 13
train loss is 0.102047
average val loss: 0.065595, accuracy: 0.0829
average test loss: 0.089667, accuracy: 0.0957
case acc: 0.10334168
case acc: 0.042891946
case acc: 0.13325489
case acc: 0.10391949
case acc: 0.09932619
case acc: 0.0917621
top acc: 0.1967 ::: bot acc: 0.0372
top acc: 0.0674 ::: bot acc: 0.0437
top acc: 0.0534 ::: bot acc: 0.2177
top acc: 0.0228 ::: bot acc: 0.1738
top acc: 0.1346 ::: bot acc: 0.1237
top acc: 0.1123 ::: bot acc: 0.1248
current epoch: 14
train loss is 0.098576
average val loss: 0.064303, accuracy: 0.0800
average test loss: 0.090284, accuracy: 0.0955
case acc: 0.0975991
case acc: 0.041370116
case acc: 0.13449255
case acc: 0.10772156
case acc: 0.099045575
case acc: 0.0925617
top acc: 0.1831 ::: bot acc: 0.0475
top acc: 0.0577 ::: bot acc: 0.0532
top acc: 0.0546 ::: bot acc: 0.2190
top acc: 0.0224 ::: bot acc: 0.1796
top acc: 0.1266 ::: bot acc: 0.1316
top acc: 0.1062 ::: bot acc: 0.1310
current epoch: 15
train loss is 0.097182
average val loss: 0.062665, accuracy: 0.0776
average test loss: 0.089086, accuracy: 0.0936
case acc: 0.09586494
case acc: 0.041362163
case acc: 0.12717567
case acc: 0.105821
case acc: 0.099103056
case acc: 0.092285946
top acc: 0.1789 ::: bot acc: 0.0508
top acc: 0.0575 ::: bot acc: 0.0535
top acc: 0.0474 ::: bot acc: 0.2116
top acc: 0.0224 ::: bot acc: 0.1767
top acc: 0.1275 ::: bot acc: 0.1309
top acc: 0.1087 ::: bot acc: 0.1285
current epoch: 16
train loss is 0.095425
average val loss: 0.061870, accuracy: 0.0753
average test loss: 0.089416, accuracy: 0.0931
case acc: 0.091962315
case acc: 0.04113407
case acc: 0.12596396
case acc: 0.10772628
case acc: 0.09907175
case acc: 0.09271163
top acc: 0.1694 ::: bot acc: 0.0584
top acc: 0.0517 ::: bot acc: 0.0593
top acc: 0.0463 ::: bot acc: 0.2104
top acc: 0.0223 ::: bot acc: 0.1796
top acc: 0.1226 ::: bot acc: 0.1359
top acc: 0.1055 ::: bot acc: 0.1317
current epoch: 17
train loss is 0.093045
average val loss: 0.061100, accuracy: 0.0733
average test loss: 0.089305, accuracy: 0.0922
case acc: 0.089252755
case acc: 0.041264784
case acc: 0.12282557
case acc: 0.10814546
case acc: 0.099040054
case acc: 0.09286933
top acc: 0.1625 ::: bot acc: 0.0640
top acc: 0.0487 ::: bot acc: 0.0625
top acc: 0.0433 ::: bot acc: 0.2072
top acc: 0.0223 ::: bot acc: 0.1801
top acc: 0.1201 ::: bot acc: 0.1383
top acc: 0.1045 ::: bot acc: 0.1328
current epoch: 18
train loss is 0.091882
average val loss: 0.060165, accuracy: 0.0715
average test loss: 0.088740, accuracy: 0.0910
case acc: 0.087798074
case acc: 0.0412824
case acc: 0.118063815
case acc: 0.10725605
case acc: 0.09904558
case acc: 0.092773296
top acc: 0.1581 ::: bot acc: 0.0679
top acc: 0.0480 ::: bot acc: 0.0633
top acc: 0.0387 ::: bot acc: 0.2024
top acc: 0.0223 ::: bot acc: 0.1788
top acc: 0.1195 ::: bot acc: 0.1388
top acc: 0.1054 ::: bot acc: 0.1319
current epoch: 19
train loss is 0.090531
average val loss: 0.059784, accuracy: 0.0699
average test loss: 0.088972, accuracy: 0.0906
case acc: 0.085903354
case acc: 0.041433107
case acc: 0.116240196
case acc: 0.108068146
case acc: 0.09910222
case acc: 0.093011856
top acc: 0.1515 ::: bot acc: 0.0738
top acc: 0.0449 ::: bot acc: 0.0663
top acc: 0.0370 ::: bot acc: 0.2005
top acc: 0.0224 ::: bot acc: 0.1800
top acc: 0.1165 ::: bot acc: 0.1418
top acc: 0.1038 ::: bot acc: 0.1336
current epoch: 20
train loss is 0.089552
average val loss: 0.059034, accuracy: 0.0685
average test loss: 0.088479, accuracy: 0.0896
case acc: 0.08498608
case acc: 0.041467253
case acc: 0.11219807
case acc: 0.107024685
case acc: 0.09911675
case acc: 0.092858635
top acc: 0.1482 ::: bot acc: 0.0768
top acc: 0.0450 ::: bot acc: 0.0664
top acc: 0.0335 ::: bot acc: 0.1963
top acc: 0.0223 ::: bot acc: 0.1785
top acc: 0.1162 ::: bot acc: 0.1420
top acc: 0.1047 ::: bot acc: 0.1325
current epoch: 21
train loss is 0.088963
average val loss: 0.058479, accuracy: 0.0674
average test loss: 0.088180, accuracy: 0.0888
case acc: 0.08415675
case acc: 0.04146258
case acc: 0.10896272
case acc: 0.10631037
case acc: 0.09908812
case acc: 0.092763096
top acc: 0.1448 ::: bot acc: 0.0800
top acc: 0.0447 ::: bot acc: 0.0667
top acc: 0.0308 ::: bot acc: 0.1927
top acc: 0.0221 ::: bot acc: 0.1775
top acc: 0.1155 ::: bot acc: 0.1426
top acc: 0.1050 ::: bot acc: 0.1322
current epoch: 22
train loss is 0.087685
average val loss: 0.058134, accuracy: 0.0664
average test loss: 0.088129, accuracy: 0.0882
case acc: 0.08333832
case acc: 0.041501615
case acc: 0.106655635
case acc: 0.106116794
case acc: 0.099106416
case acc: 0.09274534
top acc: 0.1410 ::: bot acc: 0.0834
top acc: 0.0439 ::: bot acc: 0.0675
top acc: 0.0290 ::: bot acc: 0.1902
top acc: 0.0220 ::: bot acc: 0.1772
top acc: 0.1141 ::: bot acc: 0.1440
top acc: 0.1046 ::: bot acc: 0.1325
current epoch: 23
train loss is 0.087327
average val loss: 0.057905, accuracy: 0.0656
average test loss: 0.088179, accuracy: 0.0879
case acc: 0.082745746
case acc: 0.04161728
case acc: 0.104805335
case acc: 0.106105104
case acc: 0.09917819
case acc: 0.09281013
top acc: 0.1370 ::: bot acc: 0.0874
top acc: 0.0431 ::: bot acc: 0.0684
top acc: 0.0276 ::: bot acc: 0.1881
top acc: 0.0219 ::: bot acc: 0.1771
top acc: 0.1125 ::: bot acc: 0.1454
top acc: 0.1041 ::: bot acc: 0.1330
current epoch: 24
train loss is 0.086423
average val loss: 0.057909, accuracy: 0.0649
average test loss: 0.088464, accuracy: 0.0878
case acc: 0.0822052
case acc: 0.041703872
case acc: 0.103842095
case acc: 0.106484376
case acc: 0.099332385
case acc: 0.09297028
top acc: 0.1328 ::: bot acc: 0.0917
top acc: 0.0418 ::: bot acc: 0.0697
top acc: 0.0269 ::: bot acc: 0.1870
top acc: 0.0219 ::: bot acc: 0.1778
top acc: 0.1105 ::: bot acc: 0.1476
top acc: 0.1029 ::: bot acc: 0.1342
current epoch: 25
train loss is 0.085768
average val loss: 0.058057, accuracy: 0.0646
average test loss: 0.088839, accuracy: 0.0878
case acc: 0.081896596
case acc: 0.04187786
case acc: 0.103228346
case acc: 0.10706418
case acc: 0.099535495
case acc: 0.09326431
top acc: 0.1285 ::: bot acc: 0.0958
top acc: 0.0405 ::: bot acc: 0.0710
top acc: 0.0266 ::: bot acc: 0.1862
top acc: 0.0220 ::: bot acc: 0.1786
top acc: 0.1084 ::: bot acc: 0.1499
top acc: 0.1016 ::: bot acc: 0.1357
current epoch: 26
train loss is 0.085286
average val loss: 0.057573, accuracy: 0.0640
average test loss: 0.088335, accuracy: 0.0870
case acc: 0.08182745
case acc: 0.041708425
case acc: 0.10033676
case acc: 0.105782785
case acc: 0.099490546
case acc: 0.09308154
top acc: 0.1274 ::: bot acc: 0.0969
top acc: 0.0422 ::: bot acc: 0.0694
top acc: 0.0246 ::: bot acc: 0.1829
top acc: 0.0221 ::: bot acc: 0.1766
top acc: 0.1091 ::: bot acc: 0.1492
top acc: 0.1030 ::: bot acc: 0.1343
current epoch: 27
train loss is 0.084923
average val loss: 0.057261, accuracy: 0.0635
average test loss: 0.088023, accuracy: 0.0865
case acc: 0.08172009
case acc: 0.0415815
case acc: 0.098195456
case acc: 0.10488613
case acc: 0.099477895
case acc: 0.09302107
top acc: 0.1259 ::: bot acc: 0.0983
top acc: 0.0434 ::: bot acc: 0.0682
top acc: 0.0233 ::: bot acc: 0.1803
top acc: 0.0223 ::: bot acc: 0.1751
top acc: 0.1093 ::: bot acc: 0.1490
top acc: 0.1039 ::: bot acc: 0.1335
current epoch: 28
train loss is 0.084775
average val loss: 0.057485, accuracy: 0.0635
average test loss: 0.088407, accuracy: 0.0866
case acc: 0.081785746
case acc: 0.041685063
case acc: 0.0980614
case acc: 0.105374575
case acc: 0.099621765
case acc: 0.09326274
top acc: 0.1224 ::: bot acc: 0.1017
top acc: 0.0425 ::: bot acc: 0.0691
top acc: 0.0233 ::: bot acc: 0.1801
top acc: 0.0223 ::: bot acc: 0.1759
top acc: 0.1074 ::: bot acc: 0.1509
top acc: 0.1026 ::: bot acc: 0.1349
current epoch: 29
train loss is 0.084330
average val loss: 0.057659, accuracy: 0.0634
average test loss: 0.088680, accuracy: 0.0867
case acc: 0.082068354
case acc: 0.04170834
case acc: 0.097782545
case acc: 0.105601765
case acc: 0.09972044
case acc: 0.09338719
top acc: 0.1194 ::: bot acc: 0.1047
top acc: 0.0421 ::: bot acc: 0.0695
top acc: 0.0231 ::: bot acc: 0.1798
top acc: 0.0222 ::: bot acc: 0.1762
top acc: 0.1060 ::: bot acc: 0.1523
top acc: 0.1017 ::: bot acc: 0.1359
current epoch: 30
train loss is 0.084141
average val loss: 0.057981, accuracy: 0.0636
average test loss: 0.089100, accuracy: 0.0870
case acc: 0.0824194
case acc: 0.041786604
case acc: 0.09795998
case acc: 0.10610261
case acc: 0.09988424
case acc: 0.09360127
top acc: 0.1163 ::: bot acc: 0.1078
top acc: 0.0413 ::: bot acc: 0.0702
top acc: 0.0232 ::: bot acc: 0.1800
top acc: 0.0221 ::: bot acc: 0.1770
top acc: 0.1041 ::: bot acc: 0.1542
top acc: 0.1003 ::: bot acc: 0.1373
current epoch: 31
train loss is 0.083665
average val loss: 0.058108, accuracy: 0.0637
average test loss: 0.089248, accuracy: 0.0869
case acc: 0.08262415
case acc: 0.04175131
case acc: 0.09752554
case acc: 0.10609414
case acc: 0.09997298
case acc: 0.09370322
top acc: 0.1143 ::: bot acc: 0.1098
top acc: 0.0416 ::: bot acc: 0.0699
top acc: 0.0229 ::: bot acc: 0.1795
top acc: 0.0221 ::: bot acc: 0.1770
top acc: 0.1031 ::: bot acc: 0.1551
top acc: 0.0997 ::: bot acc: 0.1380
current epoch: 32
train loss is 0.083697
average val loss: 0.057863, accuracy: 0.0635
average test loss: 0.088936, accuracy: 0.0865
case acc: 0.08270251
case acc: 0.04159939
case acc: 0.09609043
case acc: 0.10517173
case acc: 0.0999315
case acc: 0.09358915
top acc: 0.1139 ::: bot acc: 0.1102
top acc: 0.0434 ::: bot acc: 0.0683
top acc: 0.0222 ::: bot acc: 0.1777
top acc: 0.0222 ::: bot acc: 0.1755
top acc: 0.1036 ::: bot acc: 0.1546
top acc: 0.1004 ::: bot acc: 0.1372
current epoch: 33
train loss is 0.083699
average val loss: 0.057571, accuracy: 0.0633
average test loss: 0.088569, accuracy: 0.0860
case acc: 0.082733296
case acc: 0.04148927
case acc: 0.09458873
case acc: 0.10413885
case acc: 0.09985663
case acc: 0.09346033
top acc: 0.1134 ::: bot acc: 0.1106
top acc: 0.0451 ::: bot acc: 0.0666
top acc: 0.0215 ::: bot acc: 0.1758
top acc: 0.0223 ::: bot acc: 0.1739
top acc: 0.1043 ::: bot acc: 0.1539
top acc: 0.1015 ::: bot acc: 0.1362
current epoch: 34
train loss is 0.083266
average val loss: 0.057879, accuracy: 0.0635
average test loss: 0.088965, accuracy: 0.0863
case acc: 0.08317925
case acc: 0.041559543
case acc: 0.09503674
case acc: 0.104474485
case acc: 0.09999465
case acc: 0.0935787
top acc: 0.1105 ::: bot acc: 0.1137
top acc: 0.0444 ::: bot acc: 0.0675
top acc: 0.0216 ::: bot acc: 0.1764
top acc: 0.0221 ::: bot acc: 0.1745
top acc: 0.1028 ::: bot acc: 0.1553
top acc: 0.1002 ::: bot acc: 0.1373
current epoch: 35
train loss is 0.083082
average val loss: 0.058417, accuracy: 0.0639
average test loss: 0.089637, accuracy: 0.0868
case acc: 0.083657056
case acc: 0.041718468
case acc: 0.09622741
case acc: 0.10530121
case acc: 0.100223854
case acc: 0.09380658
top acc: 0.1071 ::: bot acc: 0.1174
top acc: 0.0429 ::: bot acc: 0.0691
top acc: 0.0221 ::: bot acc: 0.1780
top acc: 0.0219 ::: bot acc: 0.1758
top acc: 0.1006 ::: bot acc: 0.1574
top acc: 0.0982 ::: bot acc: 0.1392
current epoch: 36
train loss is 0.082808
average val loss: 0.057782, accuracy: 0.0634
average test loss: 0.088868, accuracy: 0.0860
case acc: 0.083521195
case acc: 0.04149321
case acc: 0.09413293
case acc: 0.10351025
case acc: 0.09997769
case acc: 0.0935034
top acc: 0.1081 ::: bot acc: 0.1164
top acc: 0.0457 ::: bot acc: 0.0662
top acc: 0.0210 ::: bot acc: 0.1753
top acc: 0.0221 ::: bot acc: 0.1731
top acc: 0.1026 ::: bot acc: 0.1554
top acc: 0.1005 ::: bot acc: 0.1370
current epoch: 37
train loss is 0.082702
average val loss: 0.057943, accuracy: 0.0636
average test loss: 0.089042, accuracy: 0.0861
case acc: 0.08367188
case acc: 0.04141601
case acc: 0.09434531
case acc: 0.10352238
case acc: 0.10007911
case acc: 0.093558975
top acc: 0.1065 ::: bot acc: 0.1179
top acc: 0.0459 ::: bot acc: 0.0659
top acc: 0.0211 ::: bot acc: 0.1756
top acc: 0.0221 ::: bot acc: 0.1731
top acc: 0.1018 ::: bot acc: 0.1562
top acc: 0.0998 ::: bot acc: 0.1377
current epoch: 38
train loss is 0.082678
average val loss: 0.058054, accuracy: 0.0638
average test loss: 0.089144, accuracy: 0.0862
case acc: 0.08381559
case acc: 0.041395262
case acc: 0.09445553
case acc: 0.10347772
case acc: 0.10021324
case acc: 0.0937083
top acc: 0.1051 ::: bot acc: 0.1191
top acc: 0.0462 ::: bot acc: 0.0656
top acc: 0.0213 ::: bot acc: 0.1756
top acc: 0.0222 ::: bot acc: 0.1730
top acc: 0.1014 ::: bot acc: 0.1569
top acc: 0.0996 ::: bot acc: 0.1380
current epoch: 39
train loss is 0.082428
average val loss: 0.058370, accuracy: 0.0641
average test loss: 0.089515, accuracy: 0.0865
case acc: 0.083994046
case acc: 0.041437764
case acc: 0.09519075
case acc: 0.103947245
case acc: 0.10036662
case acc: 0.09395448
top acc: 0.1030 ::: bot acc: 0.1210
top acc: 0.0458 ::: bot acc: 0.0660
top acc: 0.0217 ::: bot acc: 0.1765
top acc: 0.0223 ::: bot acc: 0.1736
top acc: 0.1001 ::: bot acc: 0.1582
top acc: 0.0986 ::: bot acc: 0.1392
current epoch: 40
train loss is 0.082289
average val loss: 0.058701, accuracy: 0.0644
average test loss: 0.089888, accuracy: 0.0868
case acc: 0.08424527
case acc: 0.0414361
case acc: 0.09592684
case acc: 0.104326226
case acc: 0.10056567
case acc: 0.094123274
top acc: 0.1010 ::: bot acc: 0.1230
top acc: 0.0452 ::: bot acc: 0.0665
top acc: 0.0221 ::: bot acc: 0.1775
top acc: 0.0223 ::: bot acc: 0.1741
top acc: 0.0989 ::: bot acc: 0.1594
top acc: 0.0975 ::: bot acc: 0.1404
current epoch: 41
train loss is 0.082266
average val loss: 0.059328, accuracy: 0.0650
average test loss: 0.090590, accuracy: 0.0874
case acc: 0.084676675
case acc: 0.041535087
case acc: 0.097379014
case acc: 0.105262935
case acc: 0.10095846
case acc: 0.09441488
top acc: 0.0981 ::: bot acc: 0.1258
top acc: 0.0438 ::: bot acc: 0.0680
top acc: 0.0229 ::: bot acc: 0.1792
top acc: 0.0221 ::: bot acc: 0.1756
top acc: 0.0968 ::: bot acc: 0.1615
top acc: 0.0955 ::: bot acc: 0.1423
current epoch: 42
train loss is 0.081884
average val loss: 0.058757, accuracy: 0.0646
average test loss: 0.089925, accuracy: 0.0867
case acc: 0.08445271
case acc: 0.04140486
case acc: 0.09586535
case acc: 0.10383758
case acc: 0.10068798
case acc: 0.09414977
top acc: 0.0995 ::: bot acc: 0.1244
top acc: 0.0463 ::: bot acc: 0.0655
top acc: 0.0222 ::: bot acc: 0.1773
top acc: 0.0223 ::: bot acc: 0.1734
top acc: 0.0985 ::: bot acc: 0.1599
top acc: 0.0973 ::: bot acc: 0.1405
current epoch: 43
train loss is 0.082157
average val loss: 0.058558, accuracy: 0.0645
average test loss: 0.089667, accuracy: 0.0865
case acc: 0.08438713
case acc: 0.041384183
case acc: 0.09526343
case acc: 0.103135526
case acc: 0.100576706
case acc: 0.09412642
top acc: 0.0996 ::: bot acc: 0.1242
top acc: 0.0477 ::: bot acc: 0.0642
top acc: 0.0219 ::: bot acc: 0.1765
top acc: 0.0224 ::: bot acc: 0.1723
top acc: 0.0991 ::: bot acc: 0.1593
top acc: 0.0980 ::: bot acc: 0.1400
current epoch: 44
train loss is 0.081987
average val loss: 0.057905, accuracy: 0.0639
average test loss: 0.088846, accuracy: 0.0857
case acc: 0.08411935
case acc: 0.04125919
case acc: 0.093388304
case acc: 0.10132412
case acc: 0.10022907
case acc: 0.09370826
top acc: 0.1015 ::: bot acc: 0.1225
top acc: 0.0505 ::: bot acc: 0.0612
top acc: 0.0209 ::: bot acc: 0.1743
top acc: 0.0227 ::: bot acc: 0.1694
top acc: 0.1014 ::: bot acc: 0.1569
top acc: 0.1003 ::: bot acc: 0.1377
current epoch: 45
train loss is 0.081924
average val loss: 0.057851, accuracy: 0.0639
average test loss: 0.088761, accuracy: 0.0856
case acc: 0.08421556
case acc: 0.041266475
case acc: 0.0931872
case acc: 0.100932874
case acc: 0.100169934
case acc: 0.09367258
top acc: 0.1012 ::: bot acc: 0.1228
top acc: 0.0512 ::: bot acc: 0.0604
top acc: 0.0207 ::: bot acc: 0.1741
top acc: 0.0228 ::: bot acc: 0.1688
top acc: 0.1016 ::: bot acc: 0.1566
top acc: 0.1003 ::: bot acc: 0.1376
current epoch: 46
train loss is 0.081849
average val loss: 0.057683, accuracy: 0.0638
average test loss: 0.088510, accuracy: 0.0853
case acc: 0.08416756
case acc: 0.04125677
case acc: 0.092571735
case acc: 0.10025068
case acc: 0.10004967
case acc: 0.0935232
top acc: 0.1017 ::: bot acc: 0.1225
top acc: 0.0525 ::: bot acc: 0.0591
top acc: 0.0204 ::: bot acc: 0.1733
top acc: 0.0230 ::: bot acc: 0.1678
top acc: 0.1022 ::: bot acc: 0.1558
top acc: 0.1009 ::: bot acc: 0.1370
current epoch: 47
train loss is 0.081959
average val loss: 0.057494, accuracy: 0.0636
average test loss: 0.088222, accuracy: 0.0850
case acc: 0.084103435
case acc: 0.041340813
case acc: 0.09180141
case acc: 0.09953494
case acc: 0.099977955
case acc: 0.09347016
top acc: 0.1022 ::: bot acc: 0.1220
top acc: 0.0538 ::: bot acc: 0.0578
top acc: 0.0201 ::: bot acc: 0.1723
top acc: 0.0233 ::: bot acc: 0.1666
top acc: 0.1030 ::: bot acc: 0.1551
top acc: 0.1017 ::: bot acc: 0.1363
current epoch: 48
train loss is 0.081662
average val loss: 0.058194, accuracy: 0.0643
average test loss: 0.089108, accuracy: 0.0858
case acc: 0.08458495
case acc: 0.041265607
case acc: 0.09383209
case acc: 0.100931756
case acc: 0.10036758
case acc: 0.09395761
top acc: 0.0989 ::: bot acc: 0.1251
top acc: 0.0514 ::: bot acc: 0.0603
top acc: 0.0210 ::: bot acc: 0.1749
top acc: 0.0228 ::: bot acc: 0.1688
top acc: 0.1002 ::: bot acc: 0.1580
top acc: 0.0990 ::: bot acc: 0.1390
current epoch: 49
train loss is 0.081537
average val loss: 0.058550, accuracy: 0.0646
average test loss: 0.089543, accuracy: 0.0862
case acc: 0.08481675
case acc: 0.041298494
case acc: 0.09491705
case acc: 0.101497285
case acc: 0.100567885
case acc: 0.0941981
top acc: 0.0971 ::: bot acc: 0.1268
top acc: 0.0504 ::: bot acc: 0.0614
top acc: 0.0216 ::: bot acc: 0.1762
top acc: 0.0226 ::: bot acc: 0.1697
top acc: 0.0989 ::: bot acc: 0.1594
top acc: 0.0978 ::: bot acc: 0.1403
current epoch: 50
train loss is 0.081655
average val loss: 0.058185, accuracy: 0.0644
average test loss: 0.089100, accuracy: 0.0859
case acc: 0.084737375
case acc: 0.041461136
case acc: 0.09401681
case acc: 0.10052132
case acc: 0.100367144
case acc: 0.094149426
top acc: 0.0980 ::: bot acc: 0.1260
top acc: 0.0522 ::: bot acc: 0.0600
top acc: 0.0213 ::: bot acc: 0.1750
top acc: 0.0231 ::: bot acc: 0.1680
top acc: 0.1002 ::: bot acc: 0.1580
top acc: 0.0991 ::: bot acc: 0.1391
LME_Co_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5112 5112 5112
1.7082474 -0.6288155 0.48738334 -0.33910522
Validation: 570 570 570
Testing: 774 774 774
pre-processing time: 0.0004570484161376953
the split date is 2011-07-01
net initializing with time: 0.0035212039947509766
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.288951
average val loss: 0.107504, accuracy: 0.0853
average test loss: 0.109594, accuracy: 0.1145
case acc: 0.075850785
case acc: 0.17266816
case acc: 0.055529676
case acc: 0.16615416
case acc: 0.12666154
case acc: 0.090244636
top acc: 0.0388 ::: bot acc: 0.1301
top acc: 0.1078 ::: bot acc: 0.2255
top acc: 0.1006 ::: bot acc: 0.0416
top acc: 0.2738 ::: bot acc: 0.0678
top acc: 0.1161 ::: bot acc: 0.1610
top acc: 0.1647 ::: bot acc: 0.0236
current epoch: 2
train loss is 0.211425
average val loss: 0.144262, accuracy: 0.1307
average test loss: 0.139638, accuracy: 0.1429
case acc: 0.15850541
case acc: 0.28572193
case acc: 0.08860934
case acc: 0.08682892
case acc: 0.17830555
case acc: 0.05943976
top acc: 0.0734 ::: bot acc: 0.2373
top acc: 0.2210 ::: bot acc: 0.3392
top acc: 0.0319 ::: bot acc: 0.1493
top acc: 0.1663 ::: bot acc: 0.0434
top acc: 0.0586 ::: bot acc: 0.2678
top acc: 0.0594 ::: bot acc: 0.0930
current epoch: 3
train loss is 0.167410
average val loss: 0.111139, accuracy: 0.0951
average test loss: 0.115810, accuracy: 0.1189
case acc: 0.061610058
case acc: 0.11837695
case acc: 0.07862638
case acc: 0.21069255
case acc: 0.10886781
case acc: 0.13514999
top acc: 0.0840 ::: bot acc: 0.0801
top acc: 0.0537 ::: bot acc: 0.1721
top acc: 0.1467 ::: bot acc: 0.0206
top acc: 0.3179 ::: bot acc: 0.1107
top acc: 0.1610 ::: bot acc: 0.1129
top acc: 0.2139 ::: bot acc: 0.0610
current epoch: 4
train loss is 0.133301
average val loss: 0.128480, accuracy: 0.1138
average test loss: 0.124503, accuracy: 0.1286
case acc: 0.135631
case acc: 0.23984438
case acc: 0.079940505
case acc: 0.091814056
case acc: 0.16647951
case acc: 0.057920545
top acc: 0.0537 ::: bot acc: 0.2137
top acc: 0.1750 ::: bot acc: 0.2940
top acc: 0.0307 ::: bot acc: 0.1367
top acc: 0.1751 ::: bot acc: 0.0388
top acc: 0.0628 ::: bot acc: 0.2487
top acc: 0.0751 ::: bot acc: 0.0780
current epoch: 5
train loss is 0.133474
average val loss: 0.097342, accuracy: 0.0767
average test loss: 0.099266, accuracy: 0.1036
case acc: 0.07212918
case acc: 0.1361577
case acc: 0.052652936
case acc: 0.14983176
case acc: 0.12727995
case acc: 0.08373646
top acc: 0.0415 ::: bot acc: 0.1247
top acc: 0.0712 ::: bot acc: 0.1905
top acc: 0.0866 ::: bot acc: 0.0565
top acc: 0.2534 ::: bot acc: 0.0560
top acc: 0.1158 ::: bot acc: 0.1636
top acc: 0.1579 ::: bot acc: 0.0196
current epoch: 6
train loss is 0.114222
average val loss: 0.106228, accuracy: 0.0903
average test loss: 0.103852, accuracy: 0.1085
case acc: 0.10175344
case acc: 0.17083822
case acc: 0.06716224
case acc: 0.10299209
case acc: 0.14917077
case acc: 0.058935292
top acc: 0.0343 ::: bot acc: 0.1727
top acc: 0.1059 ::: bot acc: 0.2252
top acc: 0.0355 ::: bot acc: 0.1150
top acc: 0.1927 ::: bot acc: 0.0366
top acc: 0.0774 ::: bot acc: 0.2158
top acc: 0.1023 ::: bot acc: 0.0512
current epoch: 7
train loss is 0.116965
average val loss: 0.093734, accuracy: 0.0773
average test loss: 0.093190, accuracy: 0.0972
case acc: 0.08046672
case acc: 0.12591974
case acc: 0.05785448
case acc: 0.11648751
case acc: 0.1374873
case acc: 0.06526189
top acc: 0.0338 ::: bot acc: 0.1411
top acc: 0.0609 ::: bot acc: 0.1802
top acc: 0.0485 ::: bot acc: 0.0945
top acc: 0.2113 ::: bot acc: 0.0398
top acc: 0.0959 ::: bot acc: 0.1891
top acc: 0.1264 ::: bot acc: 0.0272
current epoch: 8
train loss is 0.104537
average val loss: 0.091060, accuracy: 0.0764
average test loss: 0.090089, accuracy: 0.0936
case acc: 0.078657985
case acc: 0.11064176
case acc: 0.060875416
case acc: 0.10926823
case acc: 0.13812704
case acc: 0.06382889
top acc: 0.0342 ::: bot acc: 0.1381
top acc: 0.0467 ::: bot acc: 0.1645
top acc: 0.0423 ::: bot acc: 0.1021
top acc: 0.2016 ::: bot acc: 0.0375
top acc: 0.0948 ::: bot acc: 0.1907
top acc: 0.1218 ::: bot acc: 0.0319
current epoch: 9
train loss is 0.100940
average val loss: 0.090058, accuracy: 0.0773
average test loss: 0.088459, accuracy: 0.0914
case acc: 0.079177015
case acc: 0.10063959
case acc: 0.066272445
case acc: 0.10025815
case acc: 0.14032376
case acc: 0.061507612
top acc: 0.0339 ::: bot acc: 0.1391
top acc: 0.0389 ::: bot acc: 0.1534
top acc: 0.0361 ::: bot acc: 0.1132
top acc: 0.1885 ::: bot acc: 0.0365
top acc: 0.0912 ::: bot acc: 0.1959
top acc: 0.1136 ::: bot acc: 0.0401
current epoch: 10
train loss is 0.097009
average val loss: 0.085562, accuracy: 0.0730
average test loss: 0.084285, accuracy: 0.0866
case acc: 0.07288629
case acc: 0.0813468
case acc: 0.06504603
case acc: 0.10068246
case acc: 0.1368926
case acc: 0.06297602
top acc: 0.0397 ::: bot acc: 0.1268
top acc: 0.0286 ::: bot acc: 0.1297
top acc: 0.0372 ::: bot acc: 0.1107
top acc: 0.1891 ::: bot acc: 0.0365
top acc: 0.0972 ::: bot acc: 0.1878
top acc: 0.1190 ::: bot acc: 0.0348
current epoch: 11
train loss is 0.091851
average val loss: 0.082949, accuracy: 0.0708
average test loss: 0.081727, accuracy: 0.0836
case acc: 0.06999604
case acc: 0.0684948
case acc: 0.06593115
case acc: 0.09842314
case acc: 0.13540663
case acc: 0.06320427
top acc: 0.0455 ::: bot acc: 0.1195
top acc: 0.0263 ::: bot acc: 0.1116
top acc: 0.0363 ::: bot acc: 0.1124
top acc: 0.1855 ::: bot acc: 0.0366
top acc: 0.1000 ::: bot acc: 0.1842
top acc: 0.1197 ::: bot acc: 0.0342
current epoch: 12
train loss is 0.087781
average val loss: 0.081596, accuracy: 0.0703
average test loss: 0.080269, accuracy: 0.0817
case acc: 0.0687613
case acc: 0.061069924
case acc: 0.06840306
case acc: 0.09452857
case acc: 0.13523726
case acc: 0.06248812
top acc: 0.0487 ::: bot acc: 0.1160
top acc: 0.0301 ::: bot acc: 0.0985
top acc: 0.0344 ::: bot acc: 0.1171
top acc: 0.1793 ::: bot acc: 0.0374
top acc: 0.1004 ::: bot acc: 0.1838
top acc: 0.1171 ::: bot acc: 0.0368
current epoch: 13
train loss is 0.084453
average val loss: 0.079100, accuracy: 0.0680
average test loss: 0.078134, accuracy: 0.0795
case acc: 0.0661028
case acc: 0.0526645
case acc: 0.06728641
case acc: 0.09494531
case acc: 0.1324173
case acc: 0.06359675
top acc: 0.0577 ::: bot acc: 0.1069
top acc: 0.0399 ::: bot acc: 0.0810
top acc: 0.0353 ::: bot acc: 0.1149
top acc: 0.1800 ::: bot acc: 0.0373
top acc: 0.1056 ::: bot acc: 0.1770
top acc: 0.1208 ::: bot acc: 0.0332
current epoch: 14
train loss is 0.081931
average val loss: 0.077634, accuracy: 0.0672
average test loss: 0.076880, accuracy: 0.0781
case acc: 0.06464568
case acc: 0.047846723
case acc: 0.067200795
case acc: 0.094281025
case acc: 0.13064116
case acc: 0.063956015
top acc: 0.0635 ::: bot acc: 0.1011
top acc: 0.0513 ::: bot acc: 0.0681
top acc: 0.0355 ::: bot acc: 0.1147
top acc: 0.1788 ::: bot acc: 0.0376
top acc: 0.1092 ::: bot acc: 0.1725
top acc: 0.1220 ::: bot acc: 0.0320
current epoch: 15
train loss is 0.080066
average val loss: 0.076427, accuracy: 0.0666
average test loss: 0.075967, accuracy: 0.0771
case acc: 0.06368943
case acc: 0.045176644
case acc: 0.066499665
case acc: 0.09441658
case acc: 0.12863068
case acc: 0.06447854
top acc: 0.0690 ::: bot acc: 0.0957
top acc: 0.0622 ::: bot acc: 0.0572
top acc: 0.0361 ::: bot acc: 0.1132
top acc: 0.1790 ::: bot acc: 0.0375
top acc: 0.1134 ::: bot acc: 0.1674
top acc: 0.1238 ::: bot acc: 0.0303
current epoch: 16
train loss is 0.079322
average val loss: 0.076024, accuracy: 0.0669
average test loss: 0.075587, accuracy: 0.0767
case acc: 0.0634852
case acc: 0.044418775
case acc: 0.06712671
case acc: 0.09311655
case acc: 0.1278135
case acc: 0.06405451
top acc: 0.0706 ::: bot acc: 0.0941
top acc: 0.0674 ::: bot acc: 0.0521
top acc: 0.0356 ::: bot acc: 0.1144
top acc: 0.1768 ::: bot acc: 0.0381
top acc: 0.1151 ::: bot acc: 0.1653
top acc: 0.1224 ::: bot acc: 0.0316
current epoch: 17
train loss is 0.078878
average val loss: 0.075396, accuracy: 0.0667
average test loss: 0.075153, accuracy: 0.0762
case acc: 0.06311471
case acc: 0.043958426
case acc: 0.06635465
case acc: 0.09347523
case acc: 0.12606308
case acc: 0.064362265
top acc: 0.0738 ::: bot acc: 0.0909
top acc: 0.0728 ::: bot acc: 0.0466
top acc: 0.0364 ::: bot acc: 0.1129
top acc: 0.1774 ::: bot acc: 0.0379
top acc: 0.1188 ::: bot acc: 0.1609
top acc: 0.1235 ::: bot acc: 0.0307
current epoch: 18
train loss is 0.078818
average val loss: 0.074935, accuracy: 0.0666
average test loss: 0.074846, accuracy: 0.0759
case acc: 0.06285177
case acc: 0.043871105
case acc: 0.06563509
case acc: 0.093832426
case acc: 0.12446704
case acc: 0.06455792
top acc: 0.0761 ::: bot acc: 0.0886
top acc: 0.0764 ::: bot acc: 0.0431
top acc: 0.0372 ::: bot acc: 0.1114
top acc: 0.1780 ::: bot acc: 0.0377
top acc: 0.1221 ::: bot acc: 0.1568
top acc: 0.1241 ::: bot acc: 0.0300
current epoch: 19
train loss is 0.078680
average val loss: 0.074973, accuracy: 0.0669
average test loss: 0.074761, accuracy: 0.0756
case acc: 0.06298812
case acc: 0.04387634
case acc: 0.06640804
case acc: 0.092597686
case acc: 0.1241196
case acc: 0.06379811
top acc: 0.0747 ::: bot acc: 0.0899
top acc: 0.0754 ::: bot acc: 0.0441
top acc: 0.0366 ::: bot acc: 0.1128
top acc: 0.1758 ::: bot acc: 0.0384
top acc: 0.1228 ::: bot acc: 0.1559
top acc: 0.1217 ::: bot acc: 0.0325
current epoch: 20
train loss is 0.078609
average val loss: 0.075122, accuracy: 0.0673
average test loss: 0.074749, accuracy: 0.0754
case acc: 0.06318987
case acc: 0.0439479
case acc: 0.067290515
case acc: 0.09132548
case acc: 0.12394504
case acc: 0.06298823
top acc: 0.0727 ::: bot acc: 0.0919
top acc: 0.0732 ::: bot acc: 0.0464
top acc: 0.0358 ::: bot acc: 0.1145
top acc: 0.1733 ::: bot acc: 0.0394
top acc: 0.1232 ::: bot acc: 0.1554
top acc: 0.1190 ::: bot acc: 0.0352
current epoch: 21
train loss is 0.078315
average val loss: 0.075036, accuracy: 0.0674
average test loss: 0.074635, accuracy: 0.0752
case acc: 0.063194074
case acc: 0.043971483
case acc: 0.06734034
case acc: 0.09096474
case acc: 0.123120874
case acc: 0.06269119
top acc: 0.0726 ::: bot acc: 0.0920
top acc: 0.0724 ::: bot acc: 0.0471
top acc: 0.0358 ::: bot acc: 0.1146
top acc: 0.1726 ::: bot acc: 0.0397
top acc: 0.1250 ::: bot acc: 0.1532
top acc: 0.1180 ::: bot acc: 0.0361
current epoch: 22
train loss is 0.078193
average val loss: 0.075420, accuracy: 0.0680
average test loss: 0.074772, accuracy: 0.0752
case acc: 0.06357661
case acc: 0.044216815
case acc: 0.06877836
case acc: 0.08931978
case acc: 0.12336459
case acc: 0.061657324
top acc: 0.0699 ::: bot acc: 0.0947
top acc: 0.0688 ::: bot acc: 0.0507
top acc: 0.0347 ::: bot acc: 0.1173
top acc: 0.1692 ::: bot acc: 0.0414
top acc: 0.1245 ::: bot acc: 0.1538
top acc: 0.1146 ::: bot acc: 0.0395
current epoch: 23
train loss is 0.078128
average val loss: 0.075693, accuracy: 0.0686
average test loss: 0.074877, accuracy: 0.0751
case acc: 0.06384712
case acc: 0.04454788
case acc: 0.06978038
case acc: 0.0881794
case acc: 0.12328115
case acc: 0.06096491
top acc: 0.0683 ::: bot acc: 0.0963
top acc: 0.0661 ::: bot acc: 0.0535
top acc: 0.0340 ::: bot acc: 0.1192
top acc: 0.1669 ::: bot acc: 0.0428
top acc: 0.1247 ::: bot acc: 0.1536
top acc: 0.1122 ::: bot acc: 0.0418
current epoch: 24
train loss is 0.077786
average val loss: 0.076110, accuracy: 0.0694
average test loss: 0.075089, accuracy: 0.0752
case acc: 0.06420801
case acc: 0.04502922
case acc: 0.07119708
case acc: 0.086788446
case acc: 0.123467855
case acc: 0.060249113
top acc: 0.0663 ::: bot acc: 0.0983
top acc: 0.0631 ::: bot acc: 0.0564
top acc: 0.0330 ::: bot acc: 0.1218
top acc: 0.1638 ::: bot acc: 0.0447
top acc: 0.1243 ::: bot acc: 0.1540
top acc: 0.1092 ::: bot acc: 0.0448
current epoch: 25
train loss is 0.077607
average val loss: 0.076466, accuracy: 0.0701
average test loss: 0.075284, accuracy: 0.0753
case acc: 0.06448101
case acc: 0.045480855
case acc: 0.07258177
case acc: 0.08568253
case acc: 0.12356814
case acc: 0.05971441
top acc: 0.0649 ::: bot acc: 0.0998
top acc: 0.0609 ::: bot acc: 0.0586
top acc: 0.0325 ::: bot acc: 0.1241
top acc: 0.1611 ::: bot acc: 0.0469
top acc: 0.1241 ::: bot acc: 0.1542
top acc: 0.1069 ::: bot acc: 0.0471
current epoch: 26
train loss is 0.077369
average val loss: 0.076436, accuracy: 0.0704
average test loss: 0.075235, accuracy: 0.0751
case acc: 0.06440494
case acc: 0.04556565
case acc: 0.072986916
case acc: 0.08527529
case acc: 0.123044744
case acc: 0.05958018
top acc: 0.0652 ::: bot acc: 0.0994
top acc: 0.0603 ::: bot acc: 0.0591
top acc: 0.0323 ::: bot acc: 0.1248
top acc: 0.1600 ::: bot acc: 0.0478
top acc: 0.1253 ::: bot acc: 0.1529
top acc: 0.1064 ::: bot acc: 0.0476
current epoch: 27
train loss is 0.077324
average val loss: 0.076595, accuracy: 0.0709
average test loss: 0.075317, accuracy: 0.0751
case acc: 0.06448059
case acc: 0.045754477
case acc: 0.07392098
case acc: 0.084522225
case acc: 0.12290664
case acc: 0.059297066
top acc: 0.0648 ::: bot acc: 0.0999
top acc: 0.0593 ::: bot acc: 0.0602
top acc: 0.0320 ::: bot acc: 0.1263
top acc: 0.1580 ::: bot acc: 0.0494
top acc: 0.1256 ::: bot acc: 0.1526
top acc: 0.1052 ::: bot acc: 0.0488
current epoch: 28
train loss is 0.077204
average val loss: 0.077069, accuracy: 0.0718
average test loss: 0.075617, accuracy: 0.0753
case acc: 0.06476665
case acc: 0.046168372
case acc: 0.07564647
case acc: 0.083312154
case acc: 0.12330121
case acc: 0.058852497
top acc: 0.0633 ::: bot acc: 0.1014
top acc: 0.0574 ::: bot acc: 0.0621
top acc: 0.0314 ::: bot acc: 0.1292
top acc: 0.1548 ::: bot acc: 0.0521
top acc: 0.1248 ::: bot acc: 0.1535
top acc: 0.1027 ::: bot acc: 0.0512
current epoch: 29
train loss is 0.077019
average val loss: 0.077235, accuracy: 0.0723
average test loss: 0.075708, accuracy: 0.0754
case acc: 0.06483141
case acc: 0.046299823
case acc: 0.076599956
case acc: 0.08264952
case acc: 0.1232404
case acc: 0.058713567
top acc: 0.0632 ::: bot acc: 0.1016
top acc: 0.0569 ::: bot acc: 0.0626
top acc: 0.0311 ::: bot acc: 0.1308
top acc: 0.1531 ::: bot acc: 0.0538
top acc: 0.1249 ::: bot acc: 0.1534
top acc: 0.1017 ::: bot acc: 0.0522
current epoch: 30
train loss is 0.076735
average val loss: 0.077744, accuracy: 0.0732
average test loss: 0.076050, accuracy: 0.0757
case acc: 0.0651619
case acc: 0.046643846
case acc: 0.07835201
case acc: 0.081618056
case acc: 0.12372716
case acc: 0.058421955
top acc: 0.0617 ::: bot acc: 0.1031
top acc: 0.0555 ::: bot acc: 0.0640
top acc: 0.0307 ::: bot acc: 0.1337
top acc: 0.1500 ::: bot acc: 0.0567
top acc: 0.1239 ::: bot acc: 0.1546
top acc: 0.0995 ::: bot acc: 0.0544
current epoch: 31
train loss is 0.076720
average val loss: 0.077576, accuracy: 0.0733
average test loss: 0.075912, accuracy: 0.0755
case acc: 0.064928114
case acc: 0.04625366
case acc: 0.07865221
case acc: 0.08141985
case acc: 0.123312846
case acc: 0.058422178
top acc: 0.0627 ::: bot acc: 0.1021
top acc: 0.0571 ::: bot acc: 0.0624
top acc: 0.0306 ::: bot acc: 0.1341
top acc: 0.1492 ::: bot acc: 0.0575
top acc: 0.1248 ::: bot acc: 0.1536
top acc: 0.0997 ::: bot acc: 0.0542
current epoch: 32
train loss is 0.076479
average val loss: 0.077346, accuracy: 0.0734
average test loss: 0.075732, accuracy: 0.0753
case acc: 0.06471704
case acc: 0.045885272
case acc: 0.078738816
case acc: 0.081332095
case acc: 0.12282063
case acc: 0.05847635
top acc: 0.0639 ::: bot acc: 0.1009
top acc: 0.0587 ::: bot acc: 0.0608
top acc: 0.0306 ::: bot acc: 0.1343
top acc: 0.1489 ::: bot acc: 0.0578
top acc: 0.1258 ::: bot acc: 0.1523
top acc: 0.1002 ::: bot acc: 0.0536
current epoch: 33
train loss is 0.076385
average val loss: 0.076945, accuracy: 0.0732
average test loss: 0.075434, accuracy: 0.0750
case acc: 0.064436965
case acc: 0.045362376
case acc: 0.07832604
case acc: 0.08144734
case acc: 0.122081965
case acc: 0.058611188
top acc: 0.0656 ::: bot acc: 0.0993
top acc: 0.0612 ::: bot acc: 0.0583
top acc: 0.0307 ::: bot acc: 0.1336
top acc: 0.1493 ::: bot acc: 0.0574
top acc: 0.1274 ::: bot acc: 0.1504
top acc: 0.1013 ::: bot acc: 0.0525
current epoch: 34
train loss is 0.076297
average val loss: 0.076843, accuracy: 0.0733
average test loss: 0.075351, accuracy: 0.0749
case acc: 0.06437971
case acc: 0.04513437
case acc: 0.07842213
case acc: 0.081343286
case acc: 0.12177807
case acc: 0.058600727
top acc: 0.0659 ::: bot acc: 0.0990
top acc: 0.0624 ::: bot acc: 0.0571
top acc: 0.0306 ::: bot acc: 0.1337
top acc: 0.1489 ::: bot acc: 0.0578
top acc: 0.1281 ::: bot acc: 0.1496
top acc: 0.1014 ::: bot acc: 0.0525
current epoch: 35
train loss is 0.076237
average val loss: 0.076681, accuracy: 0.0732
average test loss: 0.075230, accuracy: 0.0748
case acc: 0.06430914
case acc: 0.044881582
case acc: 0.078215964
case acc: 0.0813601
case acc: 0.12137243
case acc: 0.058622483
top acc: 0.0662 ::: bot acc: 0.0987
top acc: 0.0637 ::: bot acc: 0.0559
top acc: 0.0307 ::: bot acc: 0.1334
top acc: 0.1489 ::: bot acc: 0.0577
top acc: 0.1289 ::: bot acc: 0.1486
top acc: 0.1016 ::: bot acc: 0.0522
current epoch: 36
train loss is 0.076164
average val loss: 0.076498, accuracy: 0.0730
average test loss: 0.075097, accuracy: 0.0746
case acc: 0.06425686
case acc: 0.04462958
case acc: 0.077870235
case acc: 0.08144067
case acc: 0.12096166
case acc: 0.05866045
top acc: 0.0665 ::: bot acc: 0.0984
top acc: 0.0652 ::: bot acc: 0.0544
top acc: 0.0307 ::: bot acc: 0.1329
top acc: 0.1491 ::: bot acc: 0.0575
top acc: 0.1298 ::: bot acc: 0.1475
top acc: 0.1019 ::: bot acc: 0.0519
current epoch: 37
train loss is 0.076112
average val loss: 0.076473, accuracy: 0.0730
average test loss: 0.075072, accuracy: 0.0746
case acc: 0.064328074
case acc: 0.04449548
case acc: 0.077883594
case acc: 0.081367716
case acc: 0.12082907
case acc: 0.05862054
top acc: 0.0661 ::: bot acc: 0.0988
top acc: 0.0662 ::: bot acc: 0.0534
top acc: 0.0307 ::: bot acc: 0.1329
top acc: 0.1489 ::: bot acc: 0.0577
top acc: 0.1301 ::: bot acc: 0.1471
top acc: 0.1015 ::: bot acc: 0.0523
current epoch: 38
train loss is 0.076080
average val loss: 0.076366, accuracy: 0.0729
average test loss: 0.074993, accuracy: 0.0745
case acc: 0.064339414
case acc: 0.04436524
case acc: 0.077665456
case acc: 0.08139488
case acc: 0.1205835
case acc: 0.05862755
top acc: 0.0661 ::: bot acc: 0.0988
top acc: 0.0673 ::: bot acc: 0.0522
top acc: 0.0308 ::: bot acc: 0.1325
top acc: 0.1490 ::: bot acc: 0.0576
top acc: 0.1307 ::: bot acc: 0.1465
top acc: 0.1016 ::: bot acc: 0.0522
current epoch: 39
train loss is 0.076081
average val loss: 0.076271, accuracy: 0.0728
average test loss: 0.074924, accuracy: 0.0744
case acc: 0.06437431
case acc: 0.044274203
case acc: 0.0773724
case acc: 0.08145351
case acc: 0.1203441
case acc: 0.05863614
top acc: 0.0659 ::: bot acc: 0.0990
top acc: 0.0682 ::: bot acc: 0.0513
top acc: 0.0309 ::: bot acc: 0.1320
top acc: 0.1493 ::: bot acc: 0.0573
top acc: 0.1312 ::: bot acc: 0.1458
top acc: 0.1016 ::: bot acc: 0.0522
current epoch: 40
train loss is 0.076115
average val loss: 0.075906, accuracy: 0.0723
average test loss: 0.074679, accuracy: 0.0742
case acc: 0.0642157
case acc: 0.04413348
case acc: 0.07628675
case acc: 0.08188808
case acc: 0.11966369
case acc: 0.058824178
top acc: 0.0668 ::: bot acc: 0.0981
top acc: 0.0701 ::: bot acc: 0.0494
top acc: 0.0312 ::: bot acc: 0.1303
top acc: 0.1509 ::: bot acc: 0.0557
top acc: 0.1328 ::: bot acc: 0.1440
top acc: 0.1028 ::: bot acc: 0.0510
current epoch: 41
train loss is 0.076041
average val loss: 0.075511, accuracy: 0.0717
average test loss: 0.074426, accuracy: 0.0739
case acc: 0.06404391
case acc: 0.044042002
case acc: 0.0749749
case acc: 0.08257391
case acc: 0.11886443
case acc: 0.059087407
top acc: 0.0677 ::: bot acc: 0.0971
top acc: 0.0719 ::: bot acc: 0.0476
top acc: 0.0316 ::: bot acc: 0.1281
top acc: 0.1529 ::: bot acc: 0.0539
top acc: 0.1345 ::: bot acc: 0.1420
top acc: 0.1043 ::: bot acc: 0.0495
current epoch: 42
train loss is 0.076089
average val loss: 0.074833, accuracy: 0.0708
average test loss: 0.074017, accuracy: 0.0736
case acc: 0.063694455
case acc: 0.043922137
case acc: 0.07263921
case acc: 0.0839313
case acc: 0.11742368
case acc: 0.05969004
top acc: 0.0700 ::: bot acc: 0.0948
top acc: 0.0748 ::: bot acc: 0.0448
top acc: 0.0324 ::: bot acc: 0.1242
top acc: 0.1565 ::: bot acc: 0.0507
top acc: 0.1378 ::: bot acc: 0.1382
top acc: 0.1073 ::: bot acc: 0.0466
current epoch: 43
train loss is 0.076127
average val loss: 0.074682, accuracy: 0.0704
average test loss: 0.073931, accuracy: 0.0734
case acc: 0.06373641
case acc: 0.04390673
case acc: 0.071652375
case acc: 0.08448587
case acc: 0.11684579
case acc: 0.059792988
top acc: 0.0696 ::: bot acc: 0.0952
top acc: 0.0748 ::: bot acc: 0.0448
top acc: 0.0328 ::: bot acc: 0.1225
top acc: 0.1578 ::: bot acc: 0.0496
top acc: 0.1390 ::: bot acc: 0.1367
top acc: 0.1077 ::: bot acc: 0.0461
current epoch: 44
train loss is 0.076270
average val loss: 0.074256, accuracy: 0.0697
average test loss: 0.073702, accuracy: 0.0731
case acc: 0.063515924
case acc: 0.043865412
case acc: 0.069771595
case acc: 0.08574952
case acc: 0.11560999
case acc: 0.06030504
top acc: 0.0709 ::: bot acc: 0.0939
top acc: 0.0758 ::: bot acc: 0.0437
top acc: 0.0340 ::: bot acc: 0.1190
top acc: 0.1609 ::: bot acc: 0.0470
top acc: 0.1420 ::: bot acc: 0.1334
top acc: 0.1099 ::: bot acc: 0.0441
current epoch: 45
train loss is 0.076437
average val loss: 0.073872, accuracy: 0.0689
average test loss: 0.073528, accuracy: 0.0729
case acc: 0.063311376
case acc: 0.04385322
case acc: 0.06778306
case acc: 0.08717569
case acc: 0.11429441
case acc: 0.060881667
top acc: 0.0722 ::: bot acc: 0.0927
top acc: 0.0762 ::: bot acc: 0.0433
top acc: 0.0355 ::: bot acc: 0.1153
top acc: 0.1644 ::: bot acc: 0.0443
top acc: 0.1452 ::: bot acc: 0.1298
top acc: 0.1121 ::: bot acc: 0.0418
current epoch: 46
train loss is 0.076558
average val loss: 0.073837, accuracy: 0.0685
average test loss: 0.073533, accuracy: 0.0728
case acc: 0.06342996
case acc: 0.04389386
case acc: 0.066775925
case acc: 0.087941624
case acc: 0.11365199
case acc: 0.060971253
top acc: 0.0714 ::: bot acc: 0.0934
top acc: 0.0740 ::: bot acc: 0.0456
top acc: 0.0363 ::: bot acc: 0.1134
top acc: 0.1662 ::: bot acc: 0.0433
top acc: 0.1469 ::: bot acc: 0.1280
top acc: 0.1125 ::: bot acc: 0.0415
current epoch: 47
train loss is 0.076721
average val loss: 0.074101, accuracy: 0.0684
average test loss: 0.073677, accuracy: 0.0728
case acc: 0.06380283
case acc: 0.044127956
case acc: 0.06670346
case acc: 0.08791148
case acc: 0.113593444
case acc: 0.06064735
top acc: 0.0689 ::: bot acc: 0.0959
top acc: 0.0695 ::: bot acc: 0.0500
top acc: 0.0364 ::: bot acc: 0.1133
top acc: 0.1661 ::: bot acc: 0.0433
top acc: 0.1470 ::: bot acc: 0.1278
top acc: 0.1111 ::: bot acc: 0.0429
current epoch: 48
train loss is 0.077124
average val loss: 0.074339, accuracy: 0.0684
average test loss: 0.073832, accuracy: 0.0729
case acc: 0.06409194
case acc: 0.044663392
case acc: 0.06649166
case acc: 0.08805065
case acc: 0.11334787
case acc: 0.060475133
top acc: 0.0671 ::: bot acc: 0.0977
top acc: 0.0649 ::: bot acc: 0.0547
top acc: 0.0366 ::: bot acc: 0.1129
top acc: 0.1664 ::: bot acc: 0.0431
top acc: 0.1478 ::: bot acc: 0.1271
top acc: 0.1104 ::: bot acc: 0.0436
current epoch: 49
train loss is 0.077812
average val loss: 0.074715, accuracy: 0.0687
average test loss: 0.074083, accuracy: 0.0731
case acc: 0.064418405
case acc: 0.04589205
case acc: 0.066620804
case acc: 0.087947905
case acc: 0.113214925
case acc: 0.060285646
top acc: 0.0654 ::: bot acc: 0.0995
top acc: 0.0587 ::: bot acc: 0.0608
top acc: 0.0365 ::: bot acc: 0.1131
top acc: 0.1662 ::: bot acc: 0.0432
top acc: 0.1482 ::: bot acc: 0.1267
top acc: 0.1095 ::: bot acc: 0.0445
current epoch: 50
train loss is 0.078510
average val loss: 0.076235, accuracy: 0.0705
average test loss: 0.075053, accuracy: 0.0741
case acc: 0.06581655
case acc: 0.04958712
case acc: 0.06963009
case acc: 0.085656084
case acc: 0.11487856
case acc: 0.059120614
top acc: 0.0590 ::: bot acc: 0.1059
top acc: 0.0469 ::: bot acc: 0.0729
top acc: 0.0342 ::: bot acc: 0.1188
top acc: 0.1608 ::: bot acc: 0.0472
top acc: 0.1439 ::: bot acc: 0.1313
top acc: 0.1040 ::: bot acc: 0.0500
LME_Co_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5142 5142 5142
1.7082474 -0.6288155 0.48738334 -0.25570297
Validation: 576 576 576
Testing: 750 750 750
pre-processing time: 0.00041031837463378906
the split date is 2012-01-01
net initializing with time: 0.003713369369506836
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.304478
average val loss: 0.157194, accuracy: 0.1543
average test loss: 0.299543, accuracy: 0.2995
case acc: 0.29288647
case acc: 0.21161656
case acc: 0.42121902
case acc: 0.2951225
case acc: 0.382695
case acc: 0.19371648
top acc: 0.3479 ::: bot acc: 0.2379
top acc: 0.2621 ::: bot acc: 0.1568
top acc: 0.5108 ::: bot acc: 0.3307
top acc: 0.3899 ::: bot acc: 0.2101
top acc: 0.4456 ::: bot acc: 0.3187
top acc: 0.2644 ::: bot acc: 0.1184
current epoch: 2
train loss is 0.244934
average val loss: 0.232521, accuracy: 0.2322
average test loss: 0.109904, accuracy: 0.1143
case acc: 0.09988389
case acc: 0.17568003
case acc: 0.07072097
case acc: 0.10862864
case acc: 0.04561348
case acc: 0.1850304
top acc: 0.0457 ::: bot acc: 0.1543
top acc: 0.1250 ::: bot acc: 0.2313
top acc: 0.1194 ::: bot acc: 0.0616
top acc: 0.0418 ::: bot acc: 0.1802
top acc: 0.0677 ::: bot acc: 0.0593
top acc: 0.1138 ::: bot acc: 0.2609
current epoch: 3
train loss is 0.116216
average val loss: 0.073897, accuracy: 0.0534
average test loss: 0.175334, accuracy: 0.1752
case acc: 0.16310753
case acc: 0.08618476
case acc: 0.2898779
case acc: 0.16175717
case acc: 0.26356724
case acc: 0.08684897
top acc: 0.2178 ::: bot acc: 0.1083
top acc: 0.1370 ::: bot acc: 0.0303
top acc: 0.3802 ::: bot acc: 0.1991
top acc: 0.2548 ::: bot acc: 0.0775
top acc: 0.3274 ::: bot acc: 0.2000
top acc: 0.1528 ::: bot acc: 0.0218
current epoch: 4
train loss is 0.113842
average val loss: 0.088859, accuracy: 0.0757
average test loss: 0.099811, accuracy: 0.1017
case acc: 0.0734528
case acc: 0.040475335
case acc: 0.19147167
case acc: 0.07907433
case acc: 0.17225002
case acc: 0.053701937
top acc: 0.1211 ::: bot acc: 0.0327
top acc: 0.0449 ::: bot acc: 0.0624
top acc: 0.2817 ::: bot acc: 0.1012
top acc: 0.1587 ::: bot acc: 0.0209
top acc: 0.2363 ::: bot acc: 0.1088
top acc: 0.0626 ::: bot acc: 0.0847
current epoch: 5
train loss is 0.096007
average val loss: 0.094974, accuracy: 0.0839
average test loss: 0.092314, accuracy: 0.0945
case acc: 0.061443165
case acc: 0.04133867
case acc: 0.17368168
case acc: 0.072674684
case acc: 0.16357867
case acc: 0.054178957
top acc: 0.1051 ::: bot acc: 0.0288
top acc: 0.0338 ::: bot acc: 0.0737
top acc: 0.2634 ::: bot acc: 0.0847
top acc: 0.1432 ::: bot acc: 0.0323
top acc: 0.2279 ::: bot acc: 0.1002
top acc: 0.0562 ::: bot acc: 0.0911
current epoch: 6
train loss is 0.086320
average val loss: 0.077698, accuracy: 0.0630
average test loss: 0.109886, accuracy: 0.1109
case acc: 0.08361941
case acc: 0.043386675
case acc: 0.19812241
case acc: 0.086016916
case acc: 0.19630508
case acc: 0.058146313
top acc: 0.1336 ::: bot acc: 0.0378
top acc: 0.0669 ::: bot acc: 0.0406
top acc: 0.2887 ::: bot acc: 0.1076
top acc: 0.1710 ::: bot acc: 0.0160
top acc: 0.2607 ::: bot acc: 0.1330
top acc: 0.0926 ::: bot acc: 0.0549
current epoch: 7
train loss is 0.086208
average val loss: 0.074806, accuracy: 0.0599
average test loss: 0.112453, accuracy: 0.1131
case acc: 0.085220546
case acc: 0.0451364
case acc: 0.19700663
case acc: 0.08784196
case acc: 0.2027439
case acc: 0.060389988
top acc: 0.1355 ::: bot acc: 0.0387
top acc: 0.0738 ::: bot acc: 0.0338
top acc: 0.2877 ::: bot acc: 0.1065
top acc: 0.1737 ::: bot acc: 0.0161
top acc: 0.2670 ::: bot acc: 0.1397
top acc: 0.1016 ::: bot acc: 0.0456
current epoch: 8
train loss is 0.086441
average val loss: 0.078754, accuracy: 0.0658
average test loss: 0.104126, accuracy: 0.1050
case acc: 0.07376698
case acc: 0.04276034
case acc: 0.18074243
case acc: 0.08018544
case acc: 0.19406645
case acc: 0.058485497
top acc: 0.1213 ::: bot acc: 0.0328
top acc: 0.0646 ::: bot acc: 0.0427
top acc: 0.2710 ::: bot acc: 0.0913
top acc: 0.1609 ::: bot acc: 0.0188
top acc: 0.2582 ::: bot acc: 0.1313
top acc: 0.0950 ::: bot acc: 0.0520
current epoch: 9
train loss is 0.084340
average val loss: 0.080691, accuracy: 0.0690
average test loss: 0.100142, accuracy: 0.1010
case acc: 0.06763764
case acc: 0.042006854
case acc: 0.17046243
case acc: 0.07693952
case acc: 0.19092706
case acc: 0.058203768
top acc: 0.1133 ::: bot acc: 0.0304
top acc: 0.0612 ::: bot acc: 0.0458
top acc: 0.2605 ::: bot acc: 0.0816
top acc: 0.1541 ::: bot acc: 0.0231
top acc: 0.2548 ::: bot acc: 0.1283
top acc: 0.0942 ::: bot acc: 0.0525
current epoch: 10
train loss is 0.082420
average val loss: 0.078964, accuracy: 0.0674
average test loss: 0.101120, accuracy: 0.1017
case acc: 0.06776546
case acc: 0.042805776
case acc: 0.16795157
case acc: 0.077265725
case acc: 0.19439149
case acc: 0.05979088
top acc: 0.1135 ::: bot acc: 0.0303
top acc: 0.0654 ::: bot acc: 0.0413
top acc: 0.2581 ::: bot acc: 0.0791
top acc: 0.1550 ::: bot acc: 0.0228
top acc: 0.2581 ::: bot acc: 0.1319
top acc: 0.1004 ::: bot acc: 0.0460
current epoch: 11
train loss is 0.081366
average val loss: 0.078115, accuracy: 0.0669
average test loss: 0.101183, accuracy: 0.1015
case acc: 0.067081
case acc: 0.04347592
case acc: 0.16448784
case acc: 0.07691955
case acc: 0.19569957
case acc: 0.06111278
top acc: 0.1126 ::: bot acc: 0.0300
top acc: 0.0683 ::: bot acc: 0.0384
top acc: 0.2546 ::: bot acc: 0.0760
top acc: 0.1542 ::: bot acc: 0.0232
top acc: 0.2593 ::: bot acc: 0.1333
top acc: 0.1047 ::: bot acc: 0.0417
current epoch: 12
train loss is 0.080783
average val loss: 0.079463, accuracy: 0.0692
average test loss: 0.098690, accuracy: 0.0989
case acc: 0.06360902
case acc: 0.043256357
case acc: 0.15762134
case acc: 0.07505536
case acc: 0.19274943
case acc: 0.061110675
top acc: 0.1080 ::: bot acc: 0.0288
top acc: 0.0672 ::: bot acc: 0.0396
top acc: 0.2474 ::: bot acc: 0.0699
top acc: 0.1499 ::: bot acc: 0.0265
top acc: 0.2564 ::: bot acc: 0.1304
top acc: 0.1047 ::: bot acc: 0.0417
current epoch: 13
train loss is 0.080415
average val loss: 0.079617, accuracy: 0.0699
average test loss: 0.097829, accuracy: 0.0978
case acc: 0.06206546
case acc: 0.043455485
case acc: 0.15340678
case acc: 0.074475504
case acc: 0.19190945
case acc: 0.061746508
top acc: 0.1060 ::: bot acc: 0.0281
top acc: 0.0683 ::: bot acc: 0.0382
top acc: 0.2430 ::: bot acc: 0.0660
top acc: 0.1484 ::: bot acc: 0.0280
top acc: 0.2553 ::: bot acc: 0.1297
top acc: 0.1066 ::: bot acc: 0.0397
current epoch: 14
train loss is 0.079895
average val loss: 0.080795, accuracy: 0.0719
average test loss: 0.095948, accuracy: 0.0958
case acc: 0.05944353
case acc: 0.04313034
case acc: 0.14794154
case acc: 0.07354942
case acc: 0.18908268
case acc: 0.061544403
top acc: 0.1026 ::: bot acc: 0.0271
top acc: 0.0678 ::: bot acc: 0.0382
top acc: 0.2374 ::: bot acc: 0.0610
top acc: 0.1457 ::: bot acc: 0.0311
top acc: 0.2521 ::: bot acc: 0.1272
top acc: 0.1064 ::: bot acc: 0.0394
current epoch: 15
train loss is 0.079625
average val loss: 0.078465, accuracy: 0.0694
average test loss: 0.098012, accuracy: 0.0975
case acc: 0.061700925
case acc: 0.044679895
case acc: 0.14869195
case acc: 0.074647486
case acc: 0.19171922
case acc: 0.06368762
top acc: 0.1056 ::: bot acc: 0.0277
top acc: 0.0734 ::: bot acc: 0.0328
top acc: 0.2383 ::: bot acc: 0.0617
top acc: 0.1488 ::: bot acc: 0.0281
top acc: 0.2547 ::: bot acc: 0.1300
top acc: 0.1119 ::: bot acc: 0.0345
current epoch: 16
train loss is 0.079247
average val loss: 0.077590, accuracy: 0.0687
average test loss: 0.098545, accuracy: 0.0979
case acc: 0.062246323
case acc: 0.045644637
case acc: 0.14749207
case acc: 0.07488395
case acc: 0.19180842
case acc: 0.06507847
top acc: 0.1063 ::: bot acc: 0.0280
top acc: 0.0764 ::: bot acc: 0.0299
top acc: 0.2370 ::: bot acc: 0.0606
top acc: 0.1496 ::: bot acc: 0.0271
top acc: 0.2549 ::: bot acc: 0.1300
top acc: 0.1149 ::: bot acc: 0.0327
current epoch: 17
train loss is 0.078702
average val loss: 0.077453, accuracy: 0.0689
average test loss: 0.098238, accuracy: 0.0974
case acc: 0.061871752
case acc: 0.04624832
case acc: 0.14534448
case acc: 0.074779145
case acc: 0.19054534
case acc: 0.065699056
top acc: 0.1058 ::: bot acc: 0.0279
top acc: 0.0780 ::: bot acc: 0.0285
top acc: 0.2348 ::: bot acc: 0.0587
top acc: 0.1493 ::: bot acc: 0.0271
top acc: 0.2538 ::: bot acc: 0.1287
top acc: 0.1163 ::: bot acc: 0.0318
current epoch: 18
train loss is 0.078606
average val loss: 0.077250, accuracy: 0.0690
average test loss: 0.098053, accuracy: 0.0971
case acc: 0.061684463
case acc: 0.04688312
case acc: 0.14358293
case acc: 0.074854866
case acc: 0.18925518
case acc: 0.066295534
top acc: 0.1055 ::: bot acc: 0.0279
top acc: 0.0797 ::: bot acc: 0.0270
top acc: 0.2329 ::: bot acc: 0.0572
top acc: 0.1495 ::: bot acc: 0.0269
top acc: 0.2525 ::: bot acc: 0.1275
top acc: 0.1175 ::: bot acc: 0.0311
current epoch: 19
train loss is 0.078426
average val loss: 0.074474, accuracy: 0.0658
average test loss: 0.101104, accuracy: 0.0999
case acc: 0.06508695
case acc: 0.049310997
case acc: 0.14657314
case acc: 0.07692102
case acc: 0.19234128
case acc: 0.06901497
top acc: 0.1099 ::: bot acc: 0.0290
top acc: 0.0860 ::: bot acc: 0.0216
top acc: 0.2361 ::: bot acc: 0.0598
top acc: 0.1545 ::: bot acc: 0.0231
top acc: 0.2555 ::: bot acc: 0.1307
top acc: 0.1231 ::: bot acc: 0.0279
current epoch: 20
train loss is 0.078136
average val loss: 0.074007, accuracy: 0.0656
average test loss: 0.101322, accuracy: 0.1000
case acc: 0.06538794
case acc: 0.050137665
case acc: 0.14591455
case acc: 0.077391066
case acc: 0.19124874
case acc: 0.06966517
top acc: 0.1103 ::: bot acc: 0.0290
top acc: 0.0879 ::: bot acc: 0.0201
top acc: 0.2356 ::: bot acc: 0.0592
top acc: 0.1554 ::: bot acc: 0.0225
top acc: 0.2543 ::: bot acc: 0.1297
top acc: 0.1243 ::: bot acc: 0.0273
current epoch: 21
train loss is 0.077937
average val loss: 0.073821, accuracy: 0.0656
average test loss: 0.101229, accuracy: 0.0997
case acc: 0.065261774
case acc: 0.050662346
case acc: 0.14505333
case acc: 0.07774872
case acc: 0.18978132
case acc: 0.0699769
top acc: 0.1101 ::: bot acc: 0.0289
top acc: 0.0893 ::: bot acc: 0.0190
top acc: 0.2348 ::: bot acc: 0.0584
top acc: 0.1561 ::: bot acc: 0.0221
top acc: 0.2527 ::: bot acc: 0.1284
top acc: 0.1250 ::: bot acc: 0.0269
current epoch: 22
train loss is 0.077648
average val loss: 0.073191, accuracy: 0.0651
average test loss: 0.101752, accuracy: 0.1001
case acc: 0.06576217
case acc: 0.051547978
case acc: 0.14506142
case acc: 0.078476176
case acc: 0.18930553
case acc: 0.07072615
top acc: 0.1108 ::: bot acc: 0.0291
top acc: 0.0913 ::: bot acc: 0.0176
top acc: 0.2348 ::: bot acc: 0.0584
top acc: 0.1576 ::: bot acc: 0.0212
top acc: 0.2522 ::: bot acc: 0.1280
top acc: 0.1264 ::: bot acc: 0.0263
current epoch: 23
train loss is 0.077590
average val loss: 0.071238, accuracy: 0.0629
average test loss: 0.104197, accuracy: 0.1024
case acc: 0.068160556
case acc: 0.05361358
case acc: 0.14759669
case acc: 0.08049344
case acc: 0.19159418
case acc: 0.07309079
top acc: 0.1139 ::: bot acc: 0.0300
top acc: 0.0957 ::: bot acc: 0.0149
top acc: 0.2374 ::: bot acc: 0.0607
top acc: 0.1617 ::: bot acc: 0.0189
top acc: 0.2545 ::: bot acc: 0.1303
top acc: 0.1305 ::: bot acc: 0.0251
current epoch: 24
train loss is 0.077389
average val loss: 0.071318, accuracy: 0.0633
average test loss: 0.103811, accuracy: 0.1020
case acc: 0.06753971
case acc: 0.05380957
case acc: 0.14657736
case acc: 0.08059065
case acc: 0.19014667
case acc: 0.073266976
top acc: 0.1131 ::: bot acc: 0.0297
top acc: 0.0961 ::: bot acc: 0.0147
top acc: 0.2363 ::: bot acc: 0.0598
top acc: 0.1619 ::: bot acc: 0.0187
top acc: 0.2530 ::: bot acc: 0.1289
top acc: 0.1308 ::: bot acc: 0.0250
current epoch: 25
train loss is 0.077105
average val loss: 0.071266, accuracy: 0.0635
average test loss: 0.103631, accuracy: 0.1018
case acc: 0.06707306
case acc: 0.054172862
case acc: 0.14584485
case acc: 0.08079944
case acc: 0.18906485
case acc: 0.0735769
top acc: 0.1124 ::: bot acc: 0.0296
top acc: 0.0967 ::: bot acc: 0.0145
top acc: 0.2356 ::: bot acc: 0.0592
top acc: 0.1624 ::: bot acc: 0.0184
top acc: 0.2519 ::: bot acc: 0.1279
top acc: 0.1312 ::: bot acc: 0.0249
current epoch: 26
train loss is 0.077021
average val loss: 0.071187, accuracy: 0.0636
average test loss: 0.103544, accuracy: 0.1016
case acc: 0.06668516
case acc: 0.05447596
case acc: 0.14532137
case acc: 0.081191964
case acc: 0.1880761
case acc: 0.07390666
top acc: 0.1119 ::: bot acc: 0.0294
top acc: 0.0973 ::: bot acc: 0.0143
top acc: 0.2351 ::: bot acc: 0.0586
top acc: 0.1632 ::: bot acc: 0.0181
top acc: 0.2508 ::: bot acc: 0.1270
top acc: 0.1318 ::: bot acc: 0.0249
current epoch: 27
train loss is 0.076807
average val loss: 0.071462, accuracy: 0.0641
average test loss: 0.102954, accuracy: 0.1010
case acc: 0.06579158
case acc: 0.054429725
case acc: 0.14410526
case acc: 0.081184074
case acc: 0.18643065
case acc: 0.07381834
top acc: 0.1108 ::: bot acc: 0.0290
top acc: 0.0972 ::: bot acc: 0.0142
top acc: 0.2338 ::: bot acc: 0.0575
top acc: 0.1632 ::: bot acc: 0.0182
top acc: 0.2490 ::: bot acc: 0.1254
top acc: 0.1316 ::: bot acc: 0.0249
current epoch: 28
train loss is 0.076800
average val loss: 0.070919, accuracy: 0.0637
average test loss: 0.103532, accuracy: 0.1015
case acc: 0.06614951
case acc: 0.055307463
case acc: 0.14446923
case acc: 0.081977695
case acc: 0.18628952
case acc: 0.074657224
top acc: 0.1113 ::: bot acc: 0.0290
top acc: 0.0987 ::: bot acc: 0.0138
top acc: 0.2343 ::: bot acc: 0.0577
top acc: 0.1649 ::: bot acc: 0.0174
top acc: 0.2489 ::: bot acc: 0.1253
top acc: 0.1330 ::: bot acc: 0.0246
current epoch: 29
train loss is 0.076533
average val loss: 0.070161, accuracy: 0.0630
average test loss: 0.104457, accuracy: 0.1023
case acc: 0.06686524
case acc: 0.056454495
case acc: 0.14535293
case acc: 0.08289252
case acc: 0.18666959
case acc: 0.07576098
top acc: 0.1122 ::: bot acc: 0.0293
top acc: 0.1006 ::: bot acc: 0.0135
top acc: 0.2352 ::: bot acc: 0.0585
top acc: 0.1666 ::: bot acc: 0.0164
top acc: 0.2492 ::: bot acc: 0.1258
top acc: 0.1348 ::: bot acc: 0.0242
current epoch: 30
train loss is 0.076534
average val loss: 0.070912, accuracy: 0.0641
average test loss: 0.103205, accuracy: 0.1011
case acc: 0.06532871
case acc: 0.05576371
case acc: 0.14356893
case acc: 0.08237657
case acc: 0.18422559
case acc: 0.07511262
top acc: 0.1102 ::: bot acc: 0.0287
top acc: 0.0994 ::: bot acc: 0.0137
top acc: 0.2334 ::: bot acc: 0.0569
top acc: 0.1657 ::: bot acc: 0.0169
top acc: 0.2468 ::: bot acc: 0.1234
top acc: 0.1337 ::: bot acc: 0.0245
current epoch: 31
train loss is 0.076224
average val loss: 0.069686, accuracy: 0.0628
average test loss: 0.104859, accuracy: 0.1027
case acc: 0.066721596
case acc: 0.057454165
case acc: 0.1454113
case acc: 0.08398511
case acc: 0.18564662
case acc: 0.07677689
top acc: 0.1120 ::: bot acc: 0.0292
top acc: 0.1021 ::: bot acc: 0.0134
top acc: 0.2353 ::: bot acc: 0.0585
top acc: 0.1687 ::: bot acc: 0.0158
top acc: 0.2481 ::: bot acc: 0.1248
top acc: 0.1363 ::: bot acc: 0.0240
current epoch: 32
train loss is 0.076296
average val loss: 0.070576, accuracy: 0.0640
average test loss: 0.103417, accuracy: 0.1012
case acc: 0.06492397
case acc: 0.05652462
case acc: 0.14340818
case acc: 0.08323991
case acc: 0.18307978
case acc: 0.07597933
top acc: 0.1097 ::: bot acc: 0.0285
top acc: 0.1006 ::: bot acc: 0.0136
top acc: 0.2332 ::: bot acc: 0.0567
top acc: 0.1674 ::: bot acc: 0.0163
top acc: 0.2455 ::: bot acc: 0.1222
top acc: 0.1350 ::: bot acc: 0.0243
current epoch: 33
train loss is 0.075894
average val loss: 0.070619, accuracy: 0.0642
average test loss: 0.103236, accuracy: 0.1010
case acc: 0.064453594
case acc: 0.05662606
case acc: 0.14299779
case acc: 0.08330413
case acc: 0.1821518
case acc: 0.076212995
top acc: 0.1090 ::: bot acc: 0.0284
top acc: 0.1007 ::: bot acc: 0.0135
top acc: 0.2328 ::: bot acc: 0.0564
top acc: 0.1674 ::: bot acc: 0.0161
top acc: 0.2446 ::: bot acc: 0.1214
top acc: 0.1354 ::: bot acc: 0.0241
current epoch: 34
train loss is 0.075978
average val loss: 0.069366, accuracy: 0.0630
average test loss: 0.104982, accuracy: 0.1026
case acc: 0.065895446
case acc: 0.058274016
case acc: 0.14507246
case acc: 0.084902376
case acc: 0.18377581
case acc: 0.07794425
top acc: 0.1109 ::: bot acc: 0.0288
top acc: 0.1033 ::: bot acc: 0.0132
top acc: 0.2350 ::: bot acc: 0.0583
top acc: 0.1701 ::: bot acc: 0.0154
top acc: 0.2462 ::: bot acc: 0.1231
top acc: 0.1383 ::: bot acc: 0.0234
current epoch: 35
train loss is 0.075843
average val loss: 0.068557, accuracy: 0.0622
average test loss: 0.106126, accuracy: 0.1038
case acc: 0.06670835
case acc: 0.059470527
case acc: 0.14631416
case acc: 0.0860855
case acc: 0.18467304
case acc: 0.079250544
top acc: 0.1119 ::: bot acc: 0.0292
top acc: 0.1052 ::: bot acc: 0.0131
top acc: 0.2363 ::: bot acc: 0.0594
top acc: 0.1719 ::: bot acc: 0.0152
top acc: 0.2470 ::: bot acc: 0.1240
top acc: 0.1404 ::: bot acc: 0.0231
current epoch: 36
train loss is 0.075602
average val loss: 0.068321, accuracy: 0.0621
average test loss: 0.106420, accuracy: 0.1040
case acc: 0.066622585
case acc: 0.05992436
case acc: 0.14653538
case acc: 0.086620584
case acc: 0.18451925
case acc: 0.07990859
top acc: 0.1118 ::: bot acc: 0.0291
top acc: 0.1058 ::: bot acc: 0.0132
top acc: 0.2366 ::: bot acc: 0.0596
top acc: 0.1728 ::: bot acc: 0.0151
top acc: 0.2468 ::: bot acc: 0.1239
top acc: 0.1413 ::: bot acc: 0.0231
current epoch: 37
train loss is 0.075683
average val loss: 0.069532, accuracy: 0.0636
average test loss: 0.104455, accuracy: 0.1020
case acc: 0.064323395
case acc: 0.058480155
case acc: 0.1439313
case acc: 0.085218355
case acc: 0.18147653
case acc: 0.07874442
top acc: 0.1087 ::: bot acc: 0.0283
top acc: 0.1035 ::: bot acc: 0.0134
top acc: 0.2338 ::: bot acc: 0.0572
top acc: 0.1706 ::: bot acc: 0.0153
top acc: 0.2438 ::: bot acc: 0.1209
top acc: 0.1394 ::: bot acc: 0.0234
current epoch: 38
train loss is 0.075440
average val loss: 0.070222, accuracy: 0.0644
average test loss: 0.103357, accuracy: 0.1009
case acc: 0.06297249
case acc: 0.0577658
case acc: 0.14243414
case acc: 0.084458105
case acc: 0.17939416
case acc: 0.07821698
top acc: 0.1069 ::: bot acc: 0.0279
top acc: 0.1024 ::: bot acc: 0.0135
top acc: 0.2323 ::: bot acc: 0.0559
top acc: 0.1693 ::: bot acc: 0.0155
top acc: 0.2417 ::: bot acc: 0.1189
top acc: 0.1386 ::: bot acc: 0.0235
current epoch: 39
train loss is 0.075538
average val loss: 0.069518, accuracy: 0.0638
average test loss: 0.104348, accuracy: 0.1018
case acc: 0.06381813
case acc: 0.05876658
case acc: 0.14367151
case acc: 0.08535037
case acc: 0.17989677
case acc: 0.07938761
top acc: 0.1079 ::: bot acc: 0.0282
top acc: 0.1040 ::: bot acc: 0.0133
top acc: 0.2336 ::: bot acc: 0.0571
top acc: 0.1706 ::: bot acc: 0.0152
top acc: 0.2422 ::: bot acc: 0.1195
top acc: 0.1405 ::: bot acc: 0.0231
current epoch: 40
train loss is 0.075594
average val loss: 0.068331, accuracy: 0.0626
average test loss: 0.106125, accuracy: 0.1036
case acc: 0.065391056
case acc: 0.06058694
case acc: 0.14581908
case acc: 0.087019846
case acc: 0.1813981
case acc: 0.08117872
top acc: 0.1100 ::: bot acc: 0.0288
top acc: 0.1066 ::: bot acc: 0.0135
top acc: 0.2358 ::: bot acc: 0.0591
top acc: 0.1730 ::: bot acc: 0.0151
top acc: 0.2437 ::: bot acc: 0.1211
top acc: 0.1433 ::: bot acc: 0.0228
current epoch: 41
train loss is 0.075500
average val loss: 0.067342, accuracy: 0.0616
average test loss: 0.107689, accuracy: 0.1051
case acc: 0.06670335
case acc: 0.062249947
case acc: 0.14774916
case acc: 0.08871283
case acc: 0.18265563
case acc: 0.08277203
top acc: 0.1116 ::: bot acc: 0.0292
top acc: 0.1089 ::: bot acc: 0.0140
top acc: 0.2379 ::: bot acc: 0.0608
top acc: 0.1754 ::: bot acc: 0.0154
top acc: 0.2450 ::: bot acc: 0.1224
top acc: 0.1457 ::: bot acc: 0.0228
current epoch: 42
train loss is 0.075556
average val loss: 0.068453, accuracy: 0.0629
average test loss: 0.105812, accuracy: 0.1032
case acc: 0.064580895
case acc: 0.060612738
case acc: 0.1455087
case acc: 0.08733815
case acc: 0.17963424
case acc: 0.081351556
top acc: 0.1088 ::: bot acc: 0.0284
top acc: 0.1066 ::: bot acc: 0.0135
top acc: 0.2356 ::: bot acc: 0.0588
top acc: 0.1734 ::: bot acc: 0.0152
top acc: 0.2419 ::: bot acc: 0.1195
top acc: 0.1436 ::: bot acc: 0.0227
current epoch: 43
train loss is 0.075311
average val loss: 0.069865, accuracy: 0.0646
average test loss: 0.103620, accuracy: 0.1009
case acc: 0.062184755
case acc: 0.058825046
case acc: 0.14277677
case acc: 0.08570303
case acc: 0.17613634
case acc: 0.07976534
top acc: 0.1056 ::: bot acc: 0.0276
top acc: 0.1039 ::: bot acc: 0.0135
top acc: 0.2327 ::: bot acc: 0.0563
top acc: 0.1709 ::: bot acc: 0.0152
top acc: 0.2384 ::: bot acc: 0.1160
top acc: 0.1411 ::: bot acc: 0.0230
current epoch: 44
train loss is 0.075087
average val loss: 0.068532, accuracy: 0.0632
average test loss: 0.105627, accuracy: 0.1029
case acc: 0.064006954
case acc: 0.060762126
case acc: 0.14529918
case acc: 0.0876802
case acc: 0.17801361
case acc: 0.08173371
top acc: 0.1081 ::: bot acc: 0.0281
top acc: 0.1067 ::: bot acc: 0.0136
top acc: 0.2354 ::: bot acc: 0.0586
top acc: 0.1739 ::: bot acc: 0.0152
top acc: 0.2402 ::: bot acc: 0.1179
top acc: 0.1440 ::: bot acc: 0.0229
current epoch: 45
train loss is 0.075330
average val loss: 0.067944, accuracy: 0.0627
average test loss: 0.106531, accuracy: 0.1038
case acc: 0.064607196
case acc: 0.06174411
case acc: 0.1464152
case acc: 0.08868779
case acc: 0.17866002
case acc: 0.08284983
top acc: 0.1088 ::: bot acc: 0.0283
top acc: 0.1081 ::: bot acc: 0.0139
top acc: 0.2366 ::: bot acc: 0.0596
top acc: 0.1752 ::: bot acc: 0.0154
top acc: 0.2408 ::: bot acc: 0.1187
top acc: 0.1456 ::: bot acc: 0.0230
current epoch: 46
train loss is 0.075270
average val loss: 0.067643, accuracy: 0.0625
average test loss: 0.106991, accuracy: 0.1043
case acc: 0.064709105
case acc: 0.062289383
case acc: 0.14698167
case acc: 0.08938556
case acc: 0.17888759
case acc: 0.083580784
top acc: 0.1088 ::: bot acc: 0.0284
top acc: 0.1087 ::: bot acc: 0.0142
top acc: 0.2372 ::: bot acc: 0.0601
top acc: 0.1761 ::: bot acc: 0.0156
top acc: 0.2410 ::: bot acc: 0.1190
top acc: 0.1466 ::: bot acc: 0.0232
current epoch: 47
train loss is 0.075151
average val loss: 0.068227, accuracy: 0.0632
average test loss: 0.105984, accuracy: 0.1032
case acc: 0.0632919
case acc: 0.061363466
case acc: 0.14568207
case acc: 0.08868844
case acc: 0.17741846
case acc: 0.08299699
top acc: 0.1069 ::: bot acc: 0.0280
top acc: 0.1074 ::: bot acc: 0.0140
top acc: 0.2358 ::: bot acc: 0.0589
top acc: 0.1751 ::: bot acc: 0.0154
top acc: 0.2395 ::: bot acc: 0.1175
top acc: 0.1457 ::: bot acc: 0.0232
current epoch: 48
train loss is 0.075052
average val loss: 0.068854, accuracy: 0.0640
average test loss: 0.104945, accuracy: 0.1021
case acc: 0.061729804
case acc: 0.06044268
case acc: 0.1442014
case acc: 0.08799942
case acc: 0.17605087
case acc: 0.082421206
top acc: 0.1049 ::: bot acc: 0.0274
top acc: 0.1061 ::: bot acc: 0.0138
top acc: 0.2342 ::: bot acc: 0.0576
top acc: 0.1742 ::: bot acc: 0.0153
top acc: 0.2381 ::: bot acc: 0.1162
top acc: 0.1448 ::: bot acc: 0.0231
current epoch: 49
train loss is 0.075137
average val loss: 0.069137, accuracy: 0.0644
average test loss: 0.104448, accuracy: 0.1016
case acc: 0.06073012
case acc: 0.059994824
case acc: 0.14337717
case acc: 0.08774993
case acc: 0.17544821
case acc: 0.0823973
top acc: 0.1035 ::: bot acc: 0.0271
top acc: 0.1056 ::: bot acc: 0.0137
top acc: 0.2334 ::: bot acc: 0.0569
top acc: 0.1738 ::: bot acc: 0.0152
top acc: 0.2375 ::: bot acc: 0.1157
top acc: 0.1448 ::: bot acc: 0.0231
current epoch: 50
train loss is 0.074825
average val loss: 0.071358, accuracy: 0.0668
average test loss: 0.101235, accuracy: 0.0983
case acc: 0.05705577
case acc: 0.05729113
case acc: 0.13910311
case acc: 0.0850598
case acc: 0.17114334
case acc: 0.07989002
top acc: 0.0986 ::: bot acc: 0.0259
top acc: 0.1014 ::: bot acc: 0.0140
top acc: 0.2289 ::: bot acc: 0.0531
top acc: 0.1697 ::: bot acc: 0.0153
top acc: 0.2331 ::: bot acc: 0.1115
top acc: 0.1412 ::: bot acc: 0.0229
LME_Co_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5130 5130 5130
1.7082474 -0.6288155 0.48738334 -0.27422604
Validation: 570 570 570
Testing: 768 768 768
pre-processing time: 0.00036215782165527344
the split date is 2012-07-01
net initializing with time: 0.003293752670288086
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.215200
average val loss: 0.239056, accuracy: 0.2391
average test loss: 0.249903, accuracy: 0.2499
case acc: 0.36516482
case acc: 0.17016381
case acc: 0.25087434
case acc: 0.19061083
case acc: 0.2825433
case acc: 0.23981386
top acc: 0.4834 ::: bot acc: 0.2526
top acc: 0.2155 ::: bot acc: 0.1285
top acc: 0.3456 ::: bot acc: 0.1769
top acc: 0.2818 ::: bot acc: 0.0765
top acc: 0.3599 ::: bot acc: 0.2058
top acc: 0.3197 ::: bot acc: 0.1436
current epoch: 2
train loss is 0.120605
average val loss: 0.142187, accuracy: 0.1419
average test loss: 0.154784, accuracy: 0.1524
case acc: 0.2562099
case acc: 0.07052751
case acc: 0.15288155
case acc: 0.10935068
case acc: 0.18671587
case acc: 0.13887072
top acc: 0.3754 ::: bot acc: 0.1430
top acc: 0.1162 ::: bot acc: 0.0282
top acc: 0.2482 ::: bot acc: 0.0794
top acc: 0.1823 ::: bot acc: 0.0317
top acc: 0.2641 ::: bot acc: 0.1100
top acc: 0.2158 ::: bot acc: 0.0489
current epoch: 3
train loss is 0.093863
average val loss: 0.107618, accuracy: 0.1079
average test loss: 0.120443, accuracy: 0.1159
case acc: 0.20531577
case acc: 0.038044438
case acc: 0.108438924
case acc: 0.09250889
case acc: 0.14544675
case acc: 0.10586445
top acc: 0.3250 ::: bot acc: 0.0918
top acc: 0.0732 ::: bot acc: 0.0173
top acc: 0.2034 ::: bot acc: 0.0363
top acc: 0.1392 ::: bot acc: 0.0667
top acc: 0.2230 ::: bot acc: 0.0686
top acc: 0.1700 ::: bot acc: 0.0421
current epoch: 4
train loss is 0.091739
average val loss: 0.118665, accuracy: 0.1190
average test loss: 0.131585, accuracy: 0.1279
case acc: 0.21738322
case acc: 0.048885867
case acc: 0.12439565
case acc: 0.09809141
case acc: 0.16269837
case acc: 0.115981445
top acc: 0.3375 ::: bot acc: 0.1037
top acc: 0.0914 ::: bot acc: 0.0131
top acc: 0.2202 ::: bot acc: 0.0511
top acc: 0.1567 ::: bot acc: 0.0495
top acc: 0.2403 ::: bot acc: 0.0858
top acc: 0.1854 ::: bot acc: 0.0414
current epoch: 5
train loss is 0.093542
average val loss: 0.136004, accuracy: 0.1362
average test loss: 0.148739, accuracy: 0.1462
case acc: 0.23565377
case acc: 0.069645286
case acc: 0.14819583
case acc: 0.10856062
case acc: 0.18489574
case acc: 0.13045573
top acc: 0.3561 ::: bot acc: 0.1218
top acc: 0.1156 ::: bot acc: 0.0270
top acc: 0.2444 ::: bot acc: 0.0747
top acc: 0.1800 ::: bot acc: 0.0339
top acc: 0.2626 ::: bot acc: 0.1078
top acc: 0.2061 ::: bot acc: 0.0433
current epoch: 6
train loss is 0.091128
average val loss: 0.129909, accuracy: 0.1303
average test loss: 0.142785, accuracy: 0.1400
case acc: 0.22411156
case acc: 0.06475881
case acc: 0.14290455
case acc: 0.1058971
case acc: 0.17824173
case acc: 0.12421371
top acc: 0.3447 ::: bot acc: 0.1102
top acc: 0.1106 ::: bot acc: 0.0224
top acc: 0.2393 ::: bot acc: 0.0693
top acc: 0.1740 ::: bot acc: 0.0379
top acc: 0.2561 ::: bot acc: 0.1011
top acc: 0.1976 ::: bot acc: 0.0419
current epoch: 7
train loss is 0.088417
average val loss: 0.122062, accuracy: 0.1228
average test loss: 0.135023, accuracy: 0.1318
case acc: 0.21008343
case acc: 0.05805331
case acc: 0.13426092
case acc: 0.102137536
case acc: 0.16934223
case acc: 0.11686349
top acc: 0.3307 ::: bot acc: 0.0962
top acc: 0.1032 ::: bot acc: 0.0171
top acc: 0.2307 ::: bot acc: 0.0606
top acc: 0.1657 ::: bot acc: 0.0434
top acc: 0.2472 ::: bot acc: 0.0922
top acc: 0.1869 ::: bot acc: 0.0414
current epoch: 8
train loss is 0.086772
average val loss: 0.118593, accuracy: 0.1195
average test loss: 0.131573, accuracy: 0.1281
case acc: 0.20192581
case acc: 0.05633018
case acc: 0.13038561
case acc: 0.10074512
case acc: 0.16571215
case acc: 0.1135228
top acc: 0.3224 ::: bot acc: 0.0882
top acc: 0.1011 ::: bot acc: 0.0159
top acc: 0.2269 ::: bot acc: 0.0565
top acc: 0.1628 ::: bot acc: 0.0451
top acc: 0.2436 ::: bot acc: 0.0885
top acc: 0.1819 ::: bot acc: 0.0415
current epoch: 9
train loss is 0.085484
average val loss: 0.114036, accuracy: 0.1154
average test loss: 0.127050, accuracy: 0.1232
case acc: 0.19272283
case acc: 0.053444237
case acc: 0.124649085
case acc: 0.09867287
case acc: 0.16021755
case acc: 0.10943529
top acc: 0.3131 ::: bot acc: 0.0790
top acc: 0.0976 ::: bot acc: 0.0144
top acc: 0.2212 ::: bot acc: 0.0509
top acc: 0.1582 ::: bot acc: 0.0480
top acc: 0.2381 ::: bot acc: 0.0830
top acc: 0.1756 ::: bot acc: 0.0420
current epoch: 10
train loss is 0.084669
average val loss: 0.112165, accuracy: 0.1138
average test loss: 0.125204, accuracy: 0.1212
case acc: 0.187577
case acc: 0.05340085
case acc: 0.12223481
case acc: 0.098184854
case acc: 0.15793273
case acc: 0.107669264
top acc: 0.3078 ::: bot acc: 0.0738
top acc: 0.0974 ::: bot acc: 0.0145
top acc: 0.2186 ::: bot acc: 0.0486
top acc: 0.1571 ::: bot acc: 0.0487
top acc: 0.2357 ::: bot acc: 0.0808
top acc: 0.1730 ::: bot acc: 0.0421
current epoch: 11
train loss is 0.084053
average val loss: 0.110003, accuracy: 0.1119
average test loss: 0.123069, accuracy: 0.1188
case acc: 0.18207464
case acc: 0.05304987
case acc: 0.11946131
case acc: 0.09766835
case acc: 0.15509361
case acc: 0.10561653
top acc: 0.3023 ::: bot acc: 0.0684
top acc: 0.0970 ::: bot acc: 0.0143
top acc: 0.2158 ::: bot acc: 0.0459
top acc: 0.1554 ::: bot acc: 0.0501
top acc: 0.2327 ::: bot acc: 0.0780
top acc: 0.1698 ::: bot acc: 0.0424
current epoch: 12
train loss is 0.082720
average val loss: 0.109860, accuracy: 0.1118
average test loss: 0.122990, accuracy: 0.1188
case acc: 0.17965066
case acc: 0.055024426
case acc: 0.119074374
case acc: 0.09827756
case acc: 0.1552288
case acc: 0.1055277
top acc: 0.3000 ::: bot acc: 0.0657
top acc: 0.0994 ::: bot acc: 0.0153
top acc: 0.2155 ::: bot acc: 0.0456
top acc: 0.1568 ::: bot acc: 0.0491
top acc: 0.2329 ::: bot acc: 0.0781
top acc: 0.1696 ::: bot acc: 0.0424
current epoch: 13
train loss is 0.082605
average val loss: 0.107874, accuracy: 0.1100
average test loss: 0.121068, accuracy: 0.1167
case acc: 0.1747715
case acc: 0.054893196
case acc: 0.1159884
case acc: 0.09775217
case acc: 0.15288198
case acc: 0.10391801
top acc: 0.2954 ::: bot acc: 0.0607
top acc: 0.0992 ::: bot acc: 0.0152
top acc: 0.2127 ::: bot acc: 0.0426
top acc: 0.1555 ::: bot acc: 0.0501
top acc: 0.2308 ::: bot acc: 0.0757
top acc: 0.1670 ::: bot acc: 0.0429
current epoch: 14
train loss is 0.081482
average val loss: 0.106117, accuracy: 0.1084
average test loss: 0.119359, accuracy: 0.1148
case acc: 0.1706644
case acc: 0.055031464
case acc: 0.1127356
case acc: 0.09741187
case acc: 0.15054864
case acc: 0.10267776
top acc: 0.2913 ::: bot acc: 0.0566
top acc: 0.0994 ::: bot acc: 0.0153
top acc: 0.2093 ::: bot acc: 0.0396
top acc: 0.1546 ::: bot acc: 0.0508
top acc: 0.2285 ::: bot acc: 0.0733
top acc: 0.1650 ::: bot acc: 0.0433
current epoch: 15
train loss is 0.081076
average val loss: 0.105540, accuracy: 0.1079
average test loss: 0.118824, accuracy: 0.1143
case acc: 0.16844723
case acc: 0.05640667
case acc: 0.11111775
case acc: 0.09762056
case acc: 0.14972663
case acc: 0.102376424
top acc: 0.2891 ::: bot acc: 0.0543
top acc: 0.1011 ::: bot acc: 0.0160
top acc: 0.2077 ::: bot acc: 0.0382
top acc: 0.1551 ::: bot acc: 0.0502
top acc: 0.2277 ::: bot acc: 0.0725
top acc: 0.1646 ::: bot acc: 0.0433
current epoch: 16
train loss is 0.080365
average val loss: 0.102946, accuracy: 0.1055
average test loss: 0.116230, accuracy: 0.1113
case acc: 0.16341424
case acc: 0.055120282
case acc: 0.1069438
case acc: 0.09665455
case acc: 0.14588378
case acc: 0.10006188
top acc: 0.2840 ::: bot acc: 0.0494
top acc: 0.0995 ::: bot acc: 0.0152
top acc: 0.2032 ::: bot acc: 0.0345
top acc: 0.1525 ::: bot acc: 0.0525
top acc: 0.2238 ::: bot acc: 0.0687
top acc: 0.1609 ::: bot acc: 0.0438
current epoch: 17
train loss is 0.079970
average val loss: 0.102339, accuracy: 0.1050
average test loss: 0.115686, accuracy: 0.1108
case acc: 0.16116318
case acc: 0.056352884
case acc: 0.10569911
case acc: 0.09681289
case acc: 0.14497982
case acc: 0.09969637
top acc: 0.2817 ::: bot acc: 0.0473
top acc: 0.1009 ::: bot acc: 0.0160
top acc: 0.2018 ::: bot acc: 0.0335
top acc: 0.1528 ::: bot acc: 0.0522
top acc: 0.2229 ::: bot acc: 0.0679
top acc: 0.1604 ::: bot acc: 0.0438
current epoch: 18
train loss is 0.079326
average val loss: 0.101431, accuracy: 0.1042
average test loss: 0.114819, accuracy: 0.1099
case acc: 0.15847425
case acc: 0.057150334
case acc: 0.10414033
case acc: 0.09679724
case acc: 0.14363986
case acc: 0.09900095
top acc: 0.2787 ::: bot acc: 0.0450
top acc: 0.1018 ::: bot acc: 0.0165
top acc: 0.2000 ::: bot acc: 0.0324
top acc: 0.1526 ::: bot acc: 0.0523
top acc: 0.2215 ::: bot acc: 0.0666
top acc: 0.1593 ::: bot acc: 0.0440
current epoch: 19
train loss is 0.079127
average val loss: 0.100654, accuracy: 0.1035
average test loss: 0.114082, accuracy: 0.1091
case acc: 0.15607491
case acc: 0.05820209
case acc: 0.102601975
case acc: 0.096861586
case acc: 0.14256907
case acc: 0.09849346
top acc: 0.2760 ::: bot acc: 0.0432
top acc: 0.1031 ::: bot acc: 0.0172
top acc: 0.1984 ::: bot acc: 0.0313
top acc: 0.1526 ::: bot acc: 0.0523
top acc: 0.2204 ::: bot acc: 0.0654
top acc: 0.1584 ::: bot acc: 0.0443
current epoch: 20
train loss is 0.078421
average val loss: 0.098338, accuracy: 0.1013
average test loss: 0.111803, accuracy: 0.1066
case acc: 0.15189064
case acc: 0.057126805
case acc: 0.09899685
case acc: 0.09612924
case acc: 0.1391071
case acc: 0.096609876
top acc: 0.2715 ::: bot acc: 0.0400
top acc: 0.1018 ::: bot acc: 0.0165
top acc: 0.1943 ::: bot acc: 0.0290
top acc: 0.1502 ::: bot acc: 0.0546
top acc: 0.2170 ::: bot acc: 0.0619
top acc: 0.1553 ::: bot acc: 0.0450
current epoch: 21
train loss is 0.078236
average val loss: 0.097711, accuracy: 0.1007
average test loss: 0.111226, accuracy: 0.1061
case acc: 0.15025756
case acc: 0.05815538
case acc: 0.09752086
case acc: 0.09618288
case acc: 0.13808545
case acc: 0.0962372
top acc: 0.2700 ::: bot acc: 0.0387
top acc: 0.1030 ::: bot acc: 0.0171
top acc: 0.1927 ::: bot acc: 0.0281
top acc: 0.1503 ::: bot acc: 0.0546
top acc: 0.2161 ::: bot acc: 0.0608
top acc: 0.1547 ::: bot acc: 0.0452
current epoch: 22
train loss is 0.078131
average val loss: 0.096900, accuracy: 0.0999
average test loss: 0.110463, accuracy: 0.1053
case acc: 0.14839727
case acc: 0.05887205
case acc: 0.09597923
case acc: 0.096122086
case acc: 0.13669106
case acc: 0.0956685
top acc: 0.2679 ::: bot acc: 0.0374
top acc: 0.1038 ::: bot acc: 0.0176
top acc: 0.1911 ::: bot acc: 0.0272
top acc: 0.1500 ::: bot acc: 0.0549
top acc: 0.2148 ::: bot acc: 0.0594
top acc: 0.1538 ::: bot acc: 0.0455
current epoch: 23
train loss is 0.077571
average val loss: 0.096493, accuracy: 0.0996
average test loss: 0.110084, accuracy: 0.1049
case acc: 0.14706288
case acc: 0.059926093
case acc: 0.09512297
case acc: 0.09614269
case acc: 0.1358642
case acc: 0.095354855
top acc: 0.2665 ::: bot acc: 0.0364
top acc: 0.1051 ::: bot acc: 0.0183
top acc: 0.1902 ::: bot acc: 0.0267
top acc: 0.1501 ::: bot acc: 0.0547
top acc: 0.2140 ::: bot acc: 0.0585
top acc: 0.1532 ::: bot acc: 0.0456
current epoch: 24
train loss is 0.077074
average val loss: 0.094810, accuracy: 0.0980
average test loss: 0.108439, accuracy: 0.1031
case acc: 0.14403662
case acc: 0.059303753
case acc: 0.09257215
case acc: 0.095608935
case acc: 0.13311756
case acc: 0.09396349
top acc: 0.2629 ::: bot acc: 0.0344
top acc: 0.1043 ::: bot acc: 0.0179
top acc: 0.1869 ::: bot acc: 0.0255
top acc: 0.1483 ::: bot acc: 0.0564
top acc: 0.2112 ::: bot acc: 0.0560
top acc: 0.1510 ::: bot acc: 0.0461
current epoch: 25
train loss is 0.076973
average val loss: 0.094536, accuracy: 0.0977
average test loss: 0.108202, accuracy: 0.1029
case acc: 0.14292645
case acc: 0.06055219
case acc: 0.0918828
case acc: 0.09573412
case acc: 0.13252613
case acc: 0.09383267
top acc: 0.2615 ::: bot acc: 0.0336
top acc: 0.1057 ::: bot acc: 0.0188
top acc: 0.1861 ::: bot acc: 0.0251
top acc: 0.1486 ::: bot acc: 0.0560
top acc: 0.2106 ::: bot acc: 0.0553
top acc: 0.1507 ::: bot acc: 0.0463
current epoch: 26
train loss is 0.076881
average val loss: 0.096790, accuracy: 0.1000
average test loss: 0.110480, accuracy: 0.1056
case acc: 0.14520809
case acc: 0.06533109
case acc: 0.09440598
case acc: 0.09708808
case acc: 0.13571602
case acc: 0.09590685
top acc: 0.2641 ::: bot acc: 0.0351
top acc: 0.1108 ::: bot acc: 0.0229
top acc: 0.1894 ::: bot acc: 0.0263
top acc: 0.1527 ::: bot acc: 0.0519
top acc: 0.2140 ::: bot acc: 0.0584
top acc: 0.1542 ::: bot acc: 0.0453
current epoch: 27
train loss is 0.076319
average val loss: 0.095396, accuracy: 0.0987
average test loss: 0.109117, accuracy: 0.1041
case acc: 0.142513
case acc: 0.06489755
case acc: 0.09246681
case acc: 0.096523374
case acc: 0.13353635
case acc: 0.09471848
top acc: 0.2608 ::: bot acc: 0.0335
top acc: 0.1102 ::: bot acc: 0.0225
top acc: 0.1868 ::: bot acc: 0.0255
top acc: 0.1511 ::: bot acc: 0.0532
top acc: 0.2116 ::: bot acc: 0.0564
top acc: 0.1523 ::: bot acc: 0.0458
current epoch: 28
train loss is 0.076343
average val loss: 0.093529, accuracy: 0.0969
average test loss: 0.107277, accuracy: 0.1020
case acc: 0.1393176
case acc: 0.06365189
case acc: 0.0898592
case acc: 0.09572922
case acc: 0.13051498
case acc: 0.09307916
top acc: 0.2569 ::: bot acc: 0.0315
top acc: 0.1088 ::: bot acc: 0.0215
top acc: 0.1836 ::: bot acc: 0.0242
top acc: 0.1488 ::: bot acc: 0.0553
top acc: 0.2086 ::: bot acc: 0.0535
top acc: 0.1495 ::: bot acc: 0.0465
current epoch: 29
train loss is 0.075909
average val loss: 0.094796, accuracy: 0.0982
average test loss: 0.108579, accuracy: 0.1036
case acc: 0.14031893
case acc: 0.06702846
case acc: 0.0911652
case acc: 0.09654799
case acc: 0.13226143
case acc: 0.09424772
top acc: 0.2580 ::: bot acc: 0.0321
top acc: 0.1123 ::: bot acc: 0.0247
top acc: 0.1853 ::: bot acc: 0.0247
top acc: 0.1513 ::: bot acc: 0.0528
top acc: 0.2104 ::: bot acc: 0.0552
top acc: 0.1516 ::: bot acc: 0.0459
current epoch: 30
train loss is 0.075712
average val loss: 0.094728, accuracy: 0.0982
average test loss: 0.108536, accuracy: 0.1036
case acc: 0.13954951
case acc: 0.06834748
case acc: 0.09091081
case acc: 0.096710816
case acc: 0.13194142
case acc: 0.09414253
top acc: 0.2572 ::: bot acc: 0.0317
top acc: 0.1136 ::: bot acc: 0.0259
top acc: 0.1851 ::: bot acc: 0.0245
top acc: 0.1516 ::: bot acc: 0.0524
top acc: 0.2102 ::: bot acc: 0.0548
top acc: 0.1514 ::: bot acc: 0.0460
current epoch: 31
train loss is 0.075533
average val loss: 0.095216, accuracy: 0.0988
average test loss: 0.109040, accuracy: 0.1042
case acc: 0.139456
case acc: 0.070472084
case acc: 0.091367066
case acc: 0.09709796
case acc: 0.13251948
case acc: 0.09457128
top acc: 0.2570 ::: bot acc: 0.0317
top acc: 0.1158 ::: bot acc: 0.0281
top acc: 0.1858 ::: bot acc: 0.0247
top acc: 0.1527 ::: bot acc: 0.0511
top acc: 0.2107 ::: bot acc: 0.0553
top acc: 0.1522 ::: bot acc: 0.0458
current epoch: 32
train loss is 0.075197
average val loss: 0.095000, accuracy: 0.0986
average test loss: 0.108848, accuracy: 0.1041
case acc: 0.13851584
case acc: 0.071493015
case acc: 0.09088555
case acc: 0.09714545
case acc: 0.13200769
case acc: 0.09434011
top acc: 0.2558 ::: bot acc: 0.0312
top acc: 0.1167 ::: bot acc: 0.0292
top acc: 0.1850 ::: bot acc: 0.0245
top acc: 0.1528 ::: bot acc: 0.0508
top acc: 0.2101 ::: bot acc: 0.0550
top acc: 0.1520 ::: bot acc: 0.0458
current epoch: 33
train loss is 0.075126
average val loss: 0.095420, accuracy: 0.0991
average test loss: 0.109283, accuracy: 0.1046
case acc: 0.13838007
case acc: 0.073442765
case acc: 0.0910763
case acc: 0.097576834
case acc: 0.1325913
case acc: 0.094726875
top acc: 0.2557 ::: bot acc: 0.0311
top acc: 0.1187 ::: bot acc: 0.0312
top acc: 0.1854 ::: bot acc: 0.0246
top acc: 0.1539 ::: bot acc: 0.0497
top acc: 0.2108 ::: bot acc: 0.0555
top acc: 0.1526 ::: bot acc: 0.0457
current epoch: 34
train loss is 0.075056
average val loss: 0.094215, accuracy: 0.0980
average test loss: 0.108099, accuracy: 0.1033
case acc: 0.13615966
case acc: 0.07288261
case acc: 0.089338005
case acc: 0.097048745
case acc: 0.13074316
case acc: 0.09366742
top acc: 0.2527 ::: bot acc: 0.0300
top acc: 0.1181 ::: bot acc: 0.0307
top acc: 0.1831 ::: bot acc: 0.0238
top acc: 0.1525 ::: bot acc: 0.0509
top acc: 0.2088 ::: bot acc: 0.0537
top acc: 0.1509 ::: bot acc: 0.0460
current epoch: 35
train loss is 0.075048
average val loss: 0.096557, accuracy: 0.1004
average test loss: 0.110431, accuracy: 0.1061
case acc: 0.13847023
case acc: 0.077649236
case acc: 0.09174417
case acc: 0.09859394
case acc: 0.13424936
case acc: 0.095828116
top acc: 0.2553 ::: bot acc: 0.0314
top acc: 0.1228 ::: bot acc: 0.0354
top acc: 0.1861 ::: bot acc: 0.0250
top acc: 0.1564 ::: bot acc: 0.0473
top acc: 0.2123 ::: bot acc: 0.0572
top acc: 0.1546 ::: bot acc: 0.0451
current epoch: 36
train loss is 0.074529
average val loss: 0.095957, accuracy: 0.0999
average test loss: 0.109847, accuracy: 0.1055
case acc: 0.13720411
case acc: 0.07790759
case acc: 0.09058894
case acc: 0.09840552
case acc: 0.13333155
case acc: 0.09533776
top acc: 0.2537 ::: bot acc: 0.0307
top acc: 0.1230 ::: bot acc: 0.0357
top acc: 0.1847 ::: bot acc: 0.0244
top acc: 0.1559 ::: bot acc: 0.0475
top acc: 0.2113 ::: bot acc: 0.0563
top acc: 0.1538 ::: bot acc: 0.0452
current epoch: 37
train loss is 0.074651
average val loss: 0.096349, accuracy: 0.1003
average test loss: 0.110237, accuracy: 0.1060
case acc: 0.13722245
case acc: 0.07958098
case acc: 0.09063089
case acc: 0.09876664
case acc: 0.13379954
case acc: 0.09573277
top acc: 0.2539 ::: bot acc: 0.0306
top acc: 0.1247 ::: bot acc: 0.0375
top acc: 0.1848 ::: bot acc: 0.0244
top acc: 0.1569 ::: bot acc: 0.0466
top acc: 0.2118 ::: bot acc: 0.0567
top acc: 0.1545 ::: bot acc: 0.0451
current epoch: 38
train loss is 0.074818
average val loss: 0.095542, accuracy: 0.0995
average test loss: 0.109457, accuracy: 0.1051
case acc: 0.13580833
case acc: 0.0793823
case acc: 0.0893524
case acc: 0.09839557
case acc: 0.13253736
case acc: 0.09503952
top acc: 0.2522 ::: bot acc: 0.0299
top acc: 0.1244 ::: bot acc: 0.0374
top acc: 0.1833 ::: bot acc: 0.0237
top acc: 0.1560 ::: bot acc: 0.0471
top acc: 0.2107 ::: bot acc: 0.0554
top acc: 0.1535 ::: bot acc: 0.0453
current epoch: 39
train loss is 0.074576
average val loss: 0.094554, accuracy: 0.0986
average test loss: 0.108502, accuracy: 0.1040
case acc: 0.13433973
case acc: 0.078837916
case acc: 0.08786863
case acc: 0.09796477
case acc: 0.13092752
case acc: 0.09419957
top acc: 0.2503 ::: bot acc: 0.0292
top acc: 0.1239 ::: bot acc: 0.0367
top acc: 0.1816 ::: bot acc: 0.0231
top acc: 0.1547 ::: bot acc: 0.0481
top acc: 0.2090 ::: bot acc: 0.0539
top acc: 0.1521 ::: bot acc: 0.0456
current epoch: 40
train loss is 0.074402
average val loss: 0.094132, accuracy: 0.0982
average test loss: 0.108133, accuracy: 0.1036
case acc: 0.13354982
case acc: 0.079080485
case acc: 0.08724333
case acc: 0.09783934
case acc: 0.13026914
case acc: 0.09385598
top acc: 0.2491 ::: bot acc: 0.0291
top acc: 0.1241 ::: bot acc: 0.0369
top acc: 0.1809 ::: bot acc: 0.0229
top acc: 0.1541 ::: bot acc: 0.0484
top acc: 0.2083 ::: bot acc: 0.0532
top acc: 0.1516 ::: bot acc: 0.0458
current epoch: 41
train loss is 0.074260
average val loss: 0.095864, accuracy: 0.0999
average test loss: 0.109883, accuracy: 0.1057
case acc: 0.1354293
case acc: 0.08257188
case acc: 0.08918123
case acc: 0.09904553
case acc: 0.13265213
case acc: 0.09545268
top acc: 0.2514 ::: bot acc: 0.0299
top acc: 0.1276 ::: bot acc: 0.0404
top acc: 0.1835 ::: bot acc: 0.0238
top acc: 0.1568 ::: bot acc: 0.0460
top acc: 0.2108 ::: bot acc: 0.0555
top acc: 0.1542 ::: bot acc: 0.0453
current epoch: 42
train loss is 0.074300
average val loss: 0.096739, accuracy: 0.1008
average test loss: 0.110762, accuracy: 0.1068
case acc: 0.13614129
case acc: 0.08468206
case acc: 0.09017421
case acc: 0.099697046
case acc: 0.13369699
case acc: 0.09618707
top acc: 0.2523 ::: bot acc: 0.0303
top acc: 0.1297 ::: bot acc: 0.0424
top acc: 0.1848 ::: bot acc: 0.0243
top acc: 0.1581 ::: bot acc: 0.0449
top acc: 0.2119 ::: bot acc: 0.0564
top acc: 0.1555 ::: bot acc: 0.0451
current epoch: 43
train loss is 0.074305
average val loss: 0.098210, accuracy: 0.1023
average test loss: 0.112218, accuracy: 0.1084
case acc: 0.13755268
case acc: 0.08751796
case acc: 0.0918918
case acc: 0.10067906
case acc: 0.13556248
case acc: 0.0974133
top acc: 0.2539 ::: bot acc: 0.0311
top acc: 0.1325 ::: bot acc: 0.0453
top acc: 0.1870 ::: bot acc: 0.0250
top acc: 0.1603 ::: bot acc: 0.0433
top acc: 0.2138 ::: bot acc: 0.0583
top acc: 0.1575 ::: bot acc: 0.0447
current epoch: 44
train loss is 0.074414
average val loss: 0.099371, accuracy: 0.1034
average test loss: 0.113390, accuracy: 0.1098
case acc: 0.1385165
case acc: 0.089879364
case acc: 0.0932691
case acc: 0.10155541
case acc: 0.13708016
case acc: 0.09835898
top acc: 0.2551 ::: bot acc: 0.0317
top acc: 0.1349 ::: bot acc: 0.0476
top acc: 0.1889 ::: bot acc: 0.0256
top acc: 0.1620 ::: bot acc: 0.0421
top acc: 0.2154 ::: bot acc: 0.0596
top acc: 0.1591 ::: bot acc: 0.0445
current epoch: 45
train loss is 0.074447
average val loss: 0.098054, accuracy: 0.1022
average test loss: 0.112115, accuracy: 0.1083
case acc: 0.13658708
case acc: 0.088655114
case acc: 0.091771744
case acc: 0.10079177
case acc: 0.13514334
case acc: 0.09714133
top acc: 0.2526 ::: bot acc: 0.0307
top acc: 0.1336 ::: bot acc: 0.0464
top acc: 0.1872 ::: bot acc: 0.0249
top acc: 0.1601 ::: bot acc: 0.0433
top acc: 0.2135 ::: bot acc: 0.0577
top acc: 0.1572 ::: bot acc: 0.0448
current epoch: 46
train loss is 0.074493
average val loss: 0.098643, accuracy: 0.1028
average test loss: 0.112705, accuracy: 0.1090
case acc: 0.13685371
case acc: 0.09011182
case acc: 0.09232819
case acc: 0.101252854
case acc: 0.13595945
case acc: 0.097572275
top acc: 0.2529 ::: bot acc: 0.0309
top acc: 0.1351 ::: bot acc: 0.0479
top acc: 0.1880 ::: bot acc: 0.0251
top acc: 0.1611 ::: bot acc: 0.0425
top acc: 0.2143 ::: bot acc: 0.0585
top acc: 0.1580 ::: bot acc: 0.0445
current epoch: 47
train loss is 0.074401
average val loss: 0.097766, accuracy: 0.1020
average test loss: 0.111868, accuracy: 0.1081
case acc: 0.13548094
case acc: 0.08945094
case acc: 0.09114076
case acc: 0.10072631
case acc: 0.13481422
case acc: 0.09674838
top acc: 0.2512 ::: bot acc: 0.0301
top acc: 0.1342 ::: bot acc: 0.0474
top acc: 0.1865 ::: bot acc: 0.0245
top acc: 0.1600 ::: bot acc: 0.0430
top acc: 0.2132 ::: bot acc: 0.0575
top acc: 0.1568 ::: bot acc: 0.0446
current epoch: 48
train loss is 0.074481
average val loss: 0.095398, accuracy: 0.0998
average test loss: 0.109542, accuracy: 0.1054
case acc: 0.13248721
case acc: 0.086535394
case acc: 0.08811156
case acc: 0.09922442
case acc: 0.13140836
case acc: 0.09461744
top acc: 0.2472 ::: bot acc: 0.0290
top acc: 0.1312 ::: bot acc: 0.0446
top acc: 0.1827 ::: bot acc: 0.0231
top acc: 0.1567 ::: bot acc: 0.0450
top acc: 0.2097 ::: bot acc: 0.0542
top acc: 0.1535 ::: bot acc: 0.0450
current epoch: 49
train loss is 0.074256
average val loss: 0.095296, accuracy: 0.0997
average test loss: 0.109449, accuracy: 0.1053
case acc: 0.13225034
case acc: 0.08694273
case acc: 0.08767942
case acc: 0.09923788
case acc: 0.13117968
case acc: 0.09451474
top acc: 0.2468 ::: bot acc: 0.0289
top acc: 0.1316 ::: bot acc: 0.0450
top acc: 0.1823 ::: bot acc: 0.0228
top acc: 0.1566 ::: bot acc: 0.0449
top acc: 0.2095 ::: bot acc: 0.0540
top acc: 0.1534 ::: bot acc: 0.0449
current epoch: 50
train loss is 0.074274
average val loss: 0.095639, accuracy: 0.1000
average test loss: 0.109839, accuracy: 0.1058
case acc: 0.13266394
case acc: 0.08798718
case acc: 0.08785394
case acc: 0.09961866
case acc: 0.13158965
case acc: 0.094830886
top acc: 0.2476 ::: bot acc: 0.0290
top acc: 0.1327 ::: bot acc: 0.0461
top acc: 0.1829 ::: bot acc: 0.0226
top acc: 0.1574 ::: bot acc: 0.0445
top acc: 0.2101 ::: bot acc: 0.0543
top acc: 0.1541 ::: bot acc: 0.0447

		{"drop_out": 0.4, "drop_out_mc": 0.15, "repeat_mc": 50, "hidden": 30, "embedding_size": 5, "batch": 512, "lag": 3}
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
LME_Co_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5112 5112 5112
1.7082474 -0.6288155 0.48142856 -0.33910522
Validation: 570 570 570
Testing: 774 774 774
pre-processing time: 0.0002987384796142578
the split date is 2010-07-01
net initializing with time: 0.004767179489135742
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.140225
average val loss: 0.114828, accuracy: 0.1156
average test loss: 0.174725, accuracy: 0.1804
case acc: 0.24796191
case acc: 0.04365469
case acc: 0.23042159
case acc: 0.19569041
case acc: 0.19725072
case acc: 0.1671868
top acc: 0.2028 ::: bot acc: 0.3073
top acc: 0.0688 ::: bot acc: 0.0351
top acc: 0.1701 ::: bot acc: 0.2856
top acc: 0.1483 ::: bot acc: 0.2372
top acc: 0.1632 ::: bot acc: 0.2290
top acc: 0.1302 ::: bot acc: 0.2011
current epoch: 2
train loss is 0.137464
average val loss: 0.137843, accuracy: 0.1427
average test loss: 0.067964, accuracy: 0.0682
case acc: 0.056930512
case acc: 0.20644723
case acc: 0.053399242
case acc: 0.03654847
case acc: 0.028600568
case acc: 0.027266195
top acc: 0.0137 ::: bot acc: 0.1157
top acc: 0.2551 ::: bot acc: 0.1509
top acc: 0.0268 ::: bot acc: 0.0922
top acc: 0.0373 ::: bot acc: 0.0526
top acc: 0.0206 ::: bot acc: 0.0474
top acc: 0.0488 ::: bot acc: 0.0227
current epoch: 3
train loss is 0.146084
average val loss: 0.146036, accuracy: 0.1497
average test loss: 0.067007, accuracy: 0.0654
case acc: 0.045015167
case acc: 0.21078543
case acc: 0.047798518
case acc: 0.035055283
case acc: 0.025114758
case acc: 0.02864634
top acc: 0.0092 ::: bot acc: 0.1000
top acc: 0.2595 ::: bot acc: 0.1551
top acc: 0.0406 ::: bot acc: 0.0767
top acc: 0.0476 ::: bot acc: 0.0430
top acc: 0.0287 ::: bot acc: 0.0377
top acc: 0.0540 ::: bot acc: 0.0184
current epoch: 4
train loss is 0.138690
average val loss: 0.087187, accuracy: 0.0939
average test loss: 0.107469, accuracy: 0.1164
case acc: 0.15005279
case acc: 0.08854685
case acc: 0.13373068
case acc: 0.114969105
case acc: 0.11164077
case acc: 0.09946958
top acc: 0.1052 ::: bot acc: 0.2097
top acc: 0.1368 ::: bot acc: 0.0337
top acc: 0.0723 ::: bot acc: 0.1901
top acc: 0.0663 ::: bot acc: 0.1574
top acc: 0.0772 ::: bot acc: 0.1436
top acc: 0.0614 ::: bot acc: 0.1340
current epoch: 5
train loss is 0.109604
average val loss: 0.088290, accuracy: 0.0941
average test loss: 0.086201, accuracy: 0.0950
case acc: 0.11912658
case acc: 0.10529701
case acc: 0.108369514
case acc: 0.088542074
case acc: 0.07535559
case acc: 0.073131256
top acc: 0.0745 ::: bot acc: 0.1788
top acc: 0.1540 ::: bot acc: 0.0490
top acc: 0.0470 ::: bot acc: 0.1648
top acc: 0.0398 ::: bot acc: 0.1310
top acc: 0.0411 ::: bot acc: 0.1073
top acc: 0.0355 ::: bot acc: 0.1074
current epoch: 6
train loss is 0.110096
average val loss: 0.099255, accuracy: 0.1042
average test loss: 0.067870, accuracy: 0.0752
case acc: 0.08655726
case acc: 0.124286324
case acc: 0.08332386
case acc: 0.064237066
case acc: 0.043136235
case acc: 0.049809344
top acc: 0.0422 ::: bot acc: 0.1463
top acc: 0.1729 ::: bot acc: 0.0679
top acc: 0.0278 ::: bot acc: 0.1370
top acc: 0.0204 ::: bot acc: 0.1042
top acc: 0.0168 ::: bot acc: 0.0711
top acc: 0.0165 ::: bot acc: 0.0820
current epoch: 7
train loss is 0.111499
average val loss: 0.089164, accuracy: 0.0941
average test loss: 0.073971, accuracy: 0.0822
case acc: 0.09903712
case acc: 0.099003755
case acc: 0.097549126
case acc: 0.07976529
case acc: 0.05020982
case acc: 0.067436986
top acc: 0.0547 ::: bot acc: 0.1588
top acc: 0.1476 ::: bot acc: 0.0426
top acc: 0.0378 ::: bot acc: 0.1534
top acc: 0.0314 ::: bot acc: 0.1221
top acc: 0.0198 ::: bot acc: 0.0801
top acc: 0.0298 ::: bot acc: 0.1017
current epoch: 8
train loss is 0.107291
average val loss: 0.083763, accuracy: 0.0886
average test loss: 0.077881, accuracy: 0.0860
case acc: 0.10575171
case acc: 0.08133245
case acc: 0.10725788
case acc: 0.09019985
case acc: 0.05270141
case acc: 0.078639165
top acc: 0.0615 ::: bot acc: 0.1655
top acc: 0.1288 ::: bot acc: 0.0270
top acc: 0.0459 ::: bot acc: 0.1640
top acc: 0.0412 ::: bot acc: 0.1330
top acc: 0.0212 ::: bot acc: 0.0832
top acc: 0.0397 ::: bot acc: 0.1136
current epoch: 9
train loss is 0.103190
average val loss: 0.084402, accuracy: 0.0888
average test loss: 0.073039, accuracy: 0.0810
case acc: 0.0981177
case acc: 0.07806223
case acc: 0.10342261
case acc: 0.08637587
case acc: 0.044363774
case acc: 0.07552812
top acc: 0.0540 ::: bot acc: 0.1578
top acc: 0.1251 ::: bot acc: 0.0246
top acc: 0.0424 ::: bot acc: 0.1599
top acc: 0.0375 ::: bot acc: 0.1291
top acc: 0.0172 ::: bot acc: 0.0727
top acc: 0.0369 ::: bot acc: 0.1103
current epoch: 10
train loss is 0.100628
average val loss: 0.085355, accuracy: 0.0895
average test loss: 0.068873, accuracy: 0.0766
case acc: 0.09083497
case acc: 0.07507772
case acc: 0.099530324
case acc: 0.08293762
case acc: 0.03826778
case acc: 0.07315088
top acc: 0.0468 ::: bot acc: 0.1506
top acc: 0.1218 ::: bot acc: 0.0225
top acc: 0.0390 ::: bot acc: 0.1558
top acc: 0.0341 ::: bot acc: 0.1255
top acc: 0.0169 ::: bot acc: 0.0637
top acc: 0.0349 ::: bot acc: 0.1078
current epoch: 11
train loss is 0.099761
average val loss: 0.085998, accuracy: 0.0899
average test loss: 0.065847, accuracy: 0.0734
case acc: 0.08506652
case acc: 0.0715295
case acc: 0.096514665
case acc: 0.08072712
case acc: 0.034054972
case acc: 0.07228448
top acc: 0.0410 ::: bot acc: 0.1448
top acc: 0.1178 ::: bot acc: 0.0200
top acc: 0.0366 ::: bot acc: 0.1525
top acc: 0.0320 ::: bot acc: 0.1232
top acc: 0.0177 ::: bot acc: 0.0571
top acc: 0.0344 ::: bot acc: 0.1068
current epoch: 12
train loss is 0.098149
average val loss: 0.085977, accuracy: 0.0897
average test loss: 0.063997, accuracy: 0.0712
case acc: 0.08121968
case acc: 0.067287825
case acc: 0.09473292
case acc: 0.079771936
case acc: 0.031730276
case acc: 0.07273735
top acc: 0.0372 ::: bot acc: 0.1410
top acc: 0.1127 ::: bot acc: 0.0174
top acc: 0.0351 ::: bot acc: 0.1505
top acc: 0.0311 ::: bot acc: 0.1223
top acc: 0.0189 ::: bot acc: 0.0531
top acc: 0.0350 ::: bot acc: 0.1072
current epoch: 13
train loss is 0.096731
average val loss: 0.083819, accuracy: 0.0874
average test loss: 0.064825, accuracy: 0.0718
case acc: 0.081989504
case acc: 0.060755733
case acc: 0.09652905
case acc: 0.082484476
case acc: 0.032386705
case acc: 0.07667588
top acc: 0.0380 ::: bot acc: 0.1417
top acc: 0.1043 ::: bot acc: 0.0147
top acc: 0.0364 ::: bot acc: 0.1526
top acc: 0.0334 ::: bot acc: 0.1253
top acc: 0.0185 ::: bot acc: 0.0542
top acc: 0.0386 ::: bot acc: 0.1113
current epoch: 14
train loss is 0.095794
average val loss: 0.083728, accuracy: 0.0872
average test loss: 0.063560, accuracy: 0.0703
case acc: 0.079209134
case acc: 0.05802933
case acc: 0.09479916
case acc: 0.08146885
case acc: 0.03132682
case acc: 0.076883405
top acc: 0.0352 ::: bot acc: 0.1390
top acc: 0.1001 ::: bot acc: 0.0148
top acc: 0.0350 ::: bot acc: 0.1507
top acc: 0.0323 ::: bot acc: 0.1244
top acc: 0.0192 ::: bot acc: 0.0524
top acc: 0.0387 ::: bot acc: 0.1116
current epoch: 15
train loss is 0.094799
average val loss: 0.083576, accuracy: 0.0869
average test loss: 0.062684, accuracy: 0.0692
case acc: 0.077022366
case acc: 0.055991225
case acc: 0.09310471
case acc: 0.0807033
case acc: 0.030860346
case acc: 0.07746733
top acc: 0.0330 ::: bot acc: 0.1368
top acc: 0.0965 ::: bot acc: 0.0158
top acc: 0.0337 ::: bot acc: 0.1489
top acc: 0.0316 ::: bot acc: 0.1236
top acc: 0.0196 ::: bot acc: 0.0515
top acc: 0.0391 ::: bot acc: 0.1123
current epoch: 16
train loss is 0.093492
average val loss: 0.083398, accuracy: 0.0867
average test loss: 0.062122, accuracy: 0.0684
case acc: 0.07517936
case acc: 0.05446281
case acc: 0.09129522
case acc: 0.08017528
case acc: 0.030861693
case acc: 0.078557186
top acc: 0.0312 ::: bot acc: 0.1350
top acc: 0.0936 ::: bot acc: 0.0171
top acc: 0.0323 ::: bot acc: 0.1469
top acc: 0.0311 ::: bot acc: 0.1232
top acc: 0.0196 ::: bot acc: 0.0514
top acc: 0.0402 ::: bot acc: 0.1134
current epoch: 17
train loss is 0.093013
average val loss: 0.086368, accuracy: 0.0894
average test loss: 0.058495, accuracy: 0.0645
case acc: 0.0682929
case acc: 0.056110967
case acc: 0.08469222
case acc: 0.074680075
case acc: 0.02856661
case acc: 0.07451442
top acc: 0.0245 ::: bot acc: 0.1280
top acc: 0.0967 ::: bot acc: 0.0158
top acc: 0.0278 ::: bot acc: 0.1393
top acc: 0.0265 ::: bot acc: 0.1172
top acc: 0.0220 ::: bot acc: 0.0468
top acc: 0.0366 ::: bot acc: 0.1092
current epoch: 18
train loss is 0.093335
average val loss: 0.088801, accuracy: 0.0917
average test loss: 0.056001, accuracy: 0.0616
case acc: 0.06294247
case acc: 0.05735824
case acc: 0.079439595
case acc: 0.07074021
case acc: 0.027339198
case acc: 0.07192681
top acc: 0.0194 ::: bot acc: 0.1226
top acc: 0.0989 ::: bot acc: 0.0151
top acc: 0.0247 ::: bot acc: 0.1330
top acc: 0.0237 ::: bot acc: 0.1127
top acc: 0.0242 ::: bot acc: 0.0439
top acc: 0.0343 ::: bot acc: 0.1064
current epoch: 19
train loss is 0.092811
average val loss: 0.090547, accuracy: 0.0934
average test loss: 0.054374, accuracy: 0.0597
case acc: 0.05912364
case acc: 0.05802958
case acc: 0.07547201
case acc: 0.06805168
case acc: 0.026687317
case acc: 0.070544004
top acc: 0.0158 ::: bot acc: 0.1187
top acc: 0.1000 ::: bot acc: 0.0148
top acc: 0.0226 ::: bot acc: 0.1281
top acc: 0.0219 ::: bot acc: 0.1095
top acc: 0.0258 ::: bot acc: 0.0421
top acc: 0.0330 ::: bot acc: 0.1050
current epoch: 20
train loss is 0.092750
average val loss: 0.091669, accuracy: 0.0944
average test loss: 0.053306, accuracy: 0.0583
case acc: 0.056596994
case acc: 0.058055725
case acc: 0.07259039
case acc: 0.0662358
case acc: 0.026271567
case acc: 0.06992275
top acc: 0.0136 ::: bot acc: 0.1160
top acc: 0.1001 ::: bot acc: 0.0148
top acc: 0.0212 ::: bot acc: 0.1245
top acc: 0.0207 ::: bot acc: 0.1074
top acc: 0.0270 ::: bot acc: 0.0409
top acc: 0.0325 ::: bot acc: 0.1043
current epoch: 21
train loss is 0.092883
average val loss: 0.092044, accuracy: 0.0948
average test loss: 0.052758, accuracy: 0.0575
case acc: 0.055370115
case acc: 0.057339393
case acc: 0.07100181
case acc: 0.065326415
case acc: 0.026116718
case acc: 0.0699216
top acc: 0.0127 ::: bot acc: 0.1146
top acc: 0.0989 ::: bot acc: 0.0151
top acc: 0.0205 ::: bot acc: 0.1225
top acc: 0.0202 ::: bot acc: 0.1064
top acc: 0.0275 ::: bot acc: 0.0404
top acc: 0.0326 ::: bot acc: 0.1043
current epoch: 22
train loss is 0.092532
average val loss: 0.091095, accuracy: 0.0938
average test loss: 0.053037, accuracy: 0.0577
case acc: 0.056072466
case acc: 0.055426907
case acc: 0.071295075
case acc: 0.06582979
case acc: 0.026503237
case acc: 0.07107967
top acc: 0.0132 ::: bot acc: 0.1154
top acc: 0.0954 ::: bot acc: 0.0163
top acc: 0.0206 ::: bot acc: 0.1229
top acc: 0.0204 ::: bot acc: 0.1070
top acc: 0.0265 ::: bot acc: 0.0415
top acc: 0.0336 ::: bot acc: 0.1055
current epoch: 23
train loss is 0.092324
average val loss: 0.089501, accuracy: 0.0923
average test loss: 0.053769, accuracy: 0.0584
case acc: 0.057880998
case acc: 0.05312831
case acc: 0.07267272
case acc: 0.067004755
case acc: 0.027152553
case acc: 0.07260676
top acc: 0.0147 ::: bot acc: 0.1174
top acc: 0.0908 ::: bot acc: 0.0186
top acc: 0.0212 ::: bot acc: 0.1247
top acc: 0.0211 ::: bot acc: 0.1085
top acc: 0.0248 ::: bot acc: 0.0434
top acc: 0.0351 ::: bot acc: 0.1071
current epoch: 24
train loss is 0.091465
average val loss: 0.087290, accuracy: 0.0901
average test loss: 0.055122, accuracy: 0.0598
case acc: 0.060811196
case acc: 0.05068968
case acc: 0.07497994
case acc: 0.0690557
case acc: 0.02826754
case acc: 0.07495055
top acc: 0.0173 ::: bot acc: 0.1205
top acc: 0.0853 ::: bot acc: 0.0223
top acc: 0.0223 ::: bot acc: 0.1277
top acc: 0.0224 ::: bot acc: 0.1109
top acc: 0.0227 ::: bot acc: 0.0461
top acc: 0.0372 ::: bot acc: 0.1096
current epoch: 25
train loss is 0.090713
average val loss: 0.085766, accuracy: 0.0887
average test loss: 0.056091, accuracy: 0.0607
case acc: 0.06302352
case acc: 0.049023923
case acc: 0.07645088
case acc: 0.07027984
case acc: 0.02922961
case acc: 0.07640173
top acc: 0.0194 ::: bot acc: 0.1228
top acc: 0.0812 ::: bot acc: 0.0255
top acc: 0.0230 ::: bot acc: 0.1295
top acc: 0.0232 ::: bot acc: 0.1124
top acc: 0.0214 ::: bot acc: 0.0482
top acc: 0.0384 ::: bot acc: 0.1111
current epoch: 26
train loss is 0.089736
average val loss: 0.085268, accuracy: 0.0882
average test loss: 0.056332, accuracy: 0.0609
case acc: 0.063653424
case acc: 0.0482536
case acc: 0.07641303
case acc: 0.07036996
case acc: 0.029724488
case acc: 0.07685995
top acc: 0.0200 ::: bot acc: 0.1234
top acc: 0.0792 ::: bot acc: 0.0271
top acc: 0.0230 ::: bot acc: 0.1295
top acc: 0.0233 ::: bot acc: 0.1125
top acc: 0.0208 ::: bot acc: 0.0492
top acc: 0.0388 ::: bot acc: 0.1116
current epoch: 27
train loss is 0.089832
average val loss: 0.086698, accuracy: 0.0896
average test loss: 0.054981, accuracy: 0.0593
case acc: 0.061210882
case acc: 0.04890015
case acc: 0.07362783
case acc: 0.068102226
case acc: 0.029035565
case acc: 0.07501688
top acc: 0.0177 ::: bot acc: 0.1209
top acc: 0.0808 ::: bot acc: 0.0258
top acc: 0.0215 ::: bot acc: 0.1261
top acc: 0.0218 ::: bot acc: 0.1099
top acc: 0.0217 ::: bot acc: 0.0477
top acc: 0.0371 ::: bot acc: 0.1097
current epoch: 28
train loss is 0.089839
average val loss: 0.089699, accuracy: 0.0925
average test loss: 0.052679, accuracy: 0.0567
case acc: 0.05676265
case acc: 0.050692666
case acc: 0.069061235
case acc: 0.064209945
case acc: 0.027521415
case acc: 0.07166219
top acc: 0.0137 ::: bot acc: 0.1162
top acc: 0.0852 ::: bot acc: 0.0224
top acc: 0.0199 ::: bot acc: 0.1201
top acc: 0.0194 ::: bot acc: 0.1052
top acc: 0.0242 ::: bot acc: 0.0442
top acc: 0.0342 ::: bot acc: 0.1062
current epoch: 29
train loss is 0.090017
average val loss: 0.090087, accuracy: 0.0930
average test loss: 0.052593, accuracy: 0.0564
case acc: 0.05611152
case acc: 0.050959215
case acc: 0.067528516
case acc: 0.06382791
case acc: 0.027699977
case acc: 0.07250525
top acc: 0.0132 ::: bot acc: 0.1156
top acc: 0.0859 ::: bot acc: 0.0219
top acc: 0.0197 ::: bot acc: 0.1179
top acc: 0.0191 ::: bot acc: 0.1048
top acc: 0.0238 ::: bot acc: 0.0447
top acc: 0.0350 ::: bot acc: 0.1070
current epoch: 30
train loss is 0.090082
average val loss: 0.092226, accuracy: 0.0950
average test loss: 0.051329, accuracy: 0.0549
case acc: 0.053355154
case acc: 0.052337777
case acc: 0.064507425
case acc: 0.061486147
case acc: 0.026802197
case acc: 0.07086302
top acc: 0.0111 ::: bot acc: 0.1125
top acc: 0.0890 ::: bot acc: 0.0197
top acc: 0.0198 ::: bot acc: 0.1133
top acc: 0.0179 ::: bot acc: 0.1019
top acc: 0.0259 ::: bot acc: 0.0423
top acc: 0.0335 ::: bot acc: 0.1053
current epoch: 31
train loss is 0.090173
average val loss: 0.094171, accuracy: 0.0969
average test loss: 0.050227, accuracy: 0.0535
case acc: 0.051163387
case acc: 0.053496487
case acc: 0.06220108
case acc: 0.059431534
case acc: 0.025952747
case acc: 0.06899354
top acc: 0.0097 ::: bot acc: 0.1099
top acc: 0.0915 ::: bot acc: 0.0183
top acc: 0.0204 ::: bot acc: 0.1095
top acc: 0.0171 ::: bot acc: 0.0992
top acc: 0.0285 ::: bot acc: 0.0398
top acc: 0.0319 ::: bot acc: 0.1033
current epoch: 32
train loss is 0.090538
average val loss: 0.093995, accuracy: 0.0968
average test loss: 0.050241, accuracy: 0.0535
case acc: 0.051654674
case acc: 0.052987203
case acc: 0.062116195
case acc: 0.059420902
case acc: 0.025864905
case acc: 0.06914144
top acc: 0.0100 ::: bot acc: 0.1105
top acc: 0.0904 ::: bot acc: 0.0189
top acc: 0.0205 ::: bot acc: 0.1094
top acc: 0.0171 ::: bot acc: 0.0992
top acc: 0.0288 ::: bot acc: 0.0395
top acc: 0.0320 ::: bot acc: 0.1035
current epoch: 33
train loss is 0.090559
average val loss: 0.093190, accuracy: 0.0961
average test loss: 0.050454, accuracy: 0.0538
case acc: 0.053063944
case acc: 0.05188582
case acc: 0.06289035
case acc: 0.059838265
case acc: 0.02586845
case acc: 0.06923479
top acc: 0.0109 ::: bot acc: 0.1122
top acc: 0.0879 ::: bot acc: 0.0205
top acc: 0.0202 ::: bot acc: 0.1107
top acc: 0.0172 ::: bot acc: 0.0998
top acc: 0.0288 ::: bot acc: 0.0395
top acc: 0.0321 ::: bot acc: 0.1036
current epoch: 34
train loss is 0.090302
average val loss: 0.088176, accuracy: 0.0914
average test loss: 0.053510, accuracy: 0.0573
case acc: 0.06034838
case acc: 0.048191372
case acc: 0.0681489
case acc: 0.06482309
case acc: 0.027959608
case acc: 0.07415909
top acc: 0.0168 ::: bot acc: 0.1201
top acc: 0.0789 ::: bot acc: 0.0274
top acc: 0.0198 ::: bot acc: 0.1188
top acc: 0.0197 ::: bot acc: 0.1061
top acc: 0.0235 ::: bot acc: 0.0454
top acc: 0.0365 ::: bot acc: 0.1088
current epoch: 35
train loss is 0.089297
average val loss: 0.084634, accuracy: 0.0880
average test loss: 0.056067, accuracy: 0.0600
case acc: 0.066446446
case acc: 0.04558346
case acc: 0.072898574
case acc: 0.068462774
case acc: 0.029808914
case acc: 0.07703748
top acc: 0.0227 ::: bot acc: 0.1263
top acc: 0.0716 ::: bot acc: 0.0343
top acc: 0.0211 ::: bot acc: 0.1253
top acc: 0.0219 ::: bot acc: 0.1104
top acc: 0.0209 ::: bot acc: 0.0494
top acc: 0.0391 ::: bot acc: 0.1118
current epoch: 36
train loss is 0.088473
average val loss: 0.081620, accuracy: 0.0851
average test loss: 0.058752, accuracy: 0.0628
case acc: 0.07196383
case acc: 0.043690156
case acc: 0.07736221
case acc: 0.07181453
case acc: 0.032112777
case acc: 0.07984655
top acc: 0.0281 ::: bot acc: 0.1320
top acc: 0.0652 ::: bot acc: 0.0407
top acc: 0.0233 ::: bot acc: 0.1309
top acc: 0.0239 ::: bot acc: 0.1144
top acc: 0.0194 ::: bot acc: 0.0537
top acc: 0.0417 ::: bot acc: 0.1147
current epoch: 37
train loss is 0.087963
average val loss: 0.079442, accuracy: 0.0832
average test loss: 0.061129, accuracy: 0.0651
case acc: 0.076148264
case acc: 0.042578675
case acc: 0.08028162
case acc: 0.074424684
case acc: 0.034575358
case acc: 0.08252135
top acc: 0.0322 ::: bot acc: 0.1362
top acc: 0.0607 ::: bot acc: 0.0452
top acc: 0.0248 ::: bot acc: 0.1345
top acc: 0.0255 ::: bot acc: 0.1175
top acc: 0.0182 ::: bot acc: 0.0580
top acc: 0.0441 ::: bot acc: 0.1175
current epoch: 38
train loss is 0.087956
average val loss: 0.078923, accuracy: 0.0828
average test loss: 0.061930, accuracy: 0.0658
case acc: 0.07710729
case acc: 0.042423375
case acc: 0.08001508
case acc: 0.07506034
case acc: 0.036102332
case acc: 0.084111534
top acc: 0.0331 ::: bot acc: 0.1372
top acc: 0.0599 ::: bot acc: 0.0460
top acc: 0.0247 ::: bot acc: 0.1342
top acc: 0.0259 ::: bot acc: 0.1182
top acc: 0.0176 ::: bot acc: 0.0605
top acc: 0.0455 ::: bot acc: 0.1192
current epoch: 39
train loss is 0.088072
average val loss: 0.082404, accuracy: 0.0861
average test loss: 0.058281, accuracy: 0.0621
case acc: 0.07061966
case acc: 0.044153243
case acc: 0.07334661
case acc: 0.07010358
case acc: 0.03382794
case acc: 0.08039506
top acc: 0.0268 ::: bot acc: 0.1307
top acc: 0.0668 ::: bot acc: 0.0390
top acc: 0.0213 ::: bot acc: 0.1259
top acc: 0.0226 ::: bot acc: 0.1124
top acc: 0.0185 ::: bot acc: 0.0567
top acc: 0.0422 ::: bot acc: 0.1153
current epoch: 40
train loss is 0.088430
average val loss: 0.089016, accuracy: 0.0924
average test loss: 0.052978, accuracy: 0.0562
case acc: 0.05991595
case acc: 0.04808406
case acc: 0.06421769
case acc: 0.06209258
case acc: 0.02935951
case acc: 0.07340292
top acc: 0.0164 ::: bot acc: 0.1198
top acc: 0.0785 ::: bot acc: 0.0277
top acc: 0.0199 ::: bot acc: 0.1129
top acc: 0.0179 ::: bot acc: 0.1027
top acc: 0.0215 ::: bot acc: 0.0485
top acc: 0.0359 ::: bot acc: 0.1079
current epoch: 41
train loss is 0.088934
average val loss: 0.095241, accuracy: 0.0984
average test loss: 0.049536, accuracy: 0.0521
case acc: 0.051640444
case acc: 0.052193306
case acc: 0.058132544
case acc: 0.0561953
case acc: 0.026304882
case acc: 0.06800007
top acc: 0.0099 ::: bot acc: 0.1106
top acc: 0.0885 ::: bot acc: 0.0201
top acc: 0.0241 ::: bot acc: 0.1017
top acc: 0.0160 ::: bot acc: 0.0949
top acc: 0.0273 ::: bot acc: 0.0409
top acc: 0.0311 ::: bot acc: 0.1022
current epoch: 42
train loss is 0.089770
average val loss: 0.098855, accuracy: 0.1018
average test loss: 0.048071, accuracy: 0.0503
case acc: 0.04790359
case acc: 0.054749187
case acc: 0.055397738
case acc: 0.053606603
case acc: 0.024973411
case acc: 0.06543098
top acc: 0.0083 ::: bot acc: 0.1058
top acc: 0.0939 ::: bot acc: 0.0171
top acc: 0.0284 ::: bot acc: 0.0955
top acc: 0.0164 ::: bot acc: 0.0908
top acc: 0.0325 ::: bot acc: 0.0358
top acc: 0.0288 ::: bot acc: 0.0995
current epoch: 43
train loss is 0.091099
average val loss: 0.100014, accuracy: 0.1030
average test loss: 0.047388, accuracy: 0.0496
case acc: 0.04729292
case acc: 0.054980457
case acc: 0.054913033
case acc: 0.052644156
case acc: 0.024475379
case acc: 0.06356508
top acc: 0.0081 ::: bot acc: 0.1050
top acc: 0.0943 ::: bot acc: 0.0168
top acc: 0.0293 ::: bot acc: 0.0943
top acc: 0.0167 ::: bot acc: 0.0893
top acc: 0.0360 ::: bot acc: 0.0323
top acc: 0.0272 ::: bot acc: 0.0976
current epoch: 44
train loss is 0.092494
average val loss: 0.094034, accuracy: 0.0975
average test loss: 0.049601, accuracy: 0.0524
case acc: 0.054699462
case acc: 0.049972773
case acc: 0.059539188
case acc: 0.05702328
case acc: 0.025205385
case acc: 0.06776805
top acc: 0.0120 ::: bot acc: 0.1141
top acc: 0.0833 ::: bot acc: 0.0239
top acc: 0.0226 ::: bot acc: 0.1046
top acc: 0.0163 ::: bot acc: 0.0961
top acc: 0.0314 ::: bot acc: 0.0369
top acc: 0.0309 ::: bot acc: 0.1020
current epoch: 45
train loss is 0.091524
average val loss: 0.082815, accuracy: 0.0867
average test loss: 0.057100, accuracy: 0.0607
case acc: 0.07222505
case acc: 0.04294754
case acc: 0.07266361
case acc: 0.06839464
case acc: 0.029856442
case acc: 0.07785761
top acc: 0.0283 ::: bot acc: 0.1323
top acc: 0.0622 ::: bot acc: 0.0437
top acc: 0.0211 ::: bot acc: 0.1251
top acc: 0.0217 ::: bot acc: 0.1105
top acc: 0.0209 ::: bot acc: 0.0495
top acc: 0.0400 ::: bot acc: 0.1126
current epoch: 46
train loss is 0.088369
average val loss: 0.073604, accuracy: 0.0777
average test loss: 0.068652, accuracy: 0.0715
case acc: 0.09107896
case acc: 0.039913345
case acc: 0.0901684
case acc: 0.08147654
case acc: 0.038012482
case acc: 0.08827804
top acc: 0.0471 ::: bot acc: 0.1511
top acc: 0.0407 ::: bot acc: 0.0652
top acc: 0.0310 ::: bot acc: 0.1464
top acc: 0.0312 ::: bot acc: 0.1253
top acc: 0.0174 ::: bot acc: 0.0635
top acc: 0.0495 ::: bot acc: 0.1235
current epoch: 47
train loss is 0.087113
average val loss: 0.068410, accuracy: 0.0728
average test loss: 0.080973, accuracy: 0.0825
case acc: 0.107132666
case acc: 0.04125015
case acc: 0.10564038
case acc: 0.09410351
case acc: 0.04826866
case acc: 0.09864628
top acc: 0.0632 ::: bot acc: 0.1672
top acc: 0.0234 ::: bot acc: 0.0825
top acc: 0.0428 ::: bot acc: 0.1637
top acc: 0.0429 ::: bot acc: 0.1383
top acc: 0.0196 ::: bot acc: 0.0778
top acc: 0.0596 ::: bot acc: 0.1340
current epoch: 48
train loss is 0.089140
average val loss: 0.067211, accuracy: 0.0721
average test loss: 0.086177, accuracy: 0.0873
case acc: 0.11279085
case acc: 0.04287487
case acc: 0.11035387
case acc: 0.09886298
case acc: 0.055264525
case acc: 0.10352695
top acc: 0.0688 ::: bot acc: 0.1729
top acc: 0.0180 ::: bot acc: 0.0880
top acc: 0.0469 ::: bot acc: 0.1687
top acc: 0.0476 ::: bot acc: 0.1431
top acc: 0.0232 ::: bot acc: 0.0865
top acc: 0.0643 ::: bot acc: 0.1389
current epoch: 49
train loss is 0.091897
average val loss: 0.070171, accuracy: 0.0747
average test loss: 0.076240, accuracy: 0.0783
case acc: 0.09993997
case acc: 0.03970892
case acc: 0.09611643
case acc: 0.087827355
case acc: 0.05044031
case acc: 0.09566531
top acc: 0.0560 ::: bot acc: 0.1600
top acc: 0.0324 ::: bot acc: 0.0735
top acc: 0.0352 ::: bot acc: 0.1532
top acc: 0.0370 ::: bot acc: 0.1319
top acc: 0.0207 ::: bot acc: 0.0805
top acc: 0.0567 ::: bot acc: 0.1309
current epoch: 50
train loss is 0.091213
average val loss: 0.081457, accuracy: 0.0856
average test loss: 0.058750, accuracy: 0.0619
case acc: 0.07374457
case acc: 0.04255523
case acc: 0.071144275
case acc: 0.06749182
case acc: 0.03729601
case acc: 0.07893803
top acc: 0.0298 ::: bot acc: 0.1338
top acc: 0.0607 ::: bot acc: 0.0451
top acc: 0.0205 ::: bot acc: 0.1231
top acc: 0.0210 ::: bot acc: 0.1093
top acc: 0.0174 ::: bot acc: 0.0624
top acc: 0.0410 ::: bot acc: 0.1136
LME_Co_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5136 5136 5136
1.7082474 -0.6288155 0.48738334 -0.33910522
Validation: 576 576 576
Testing: 744 744 744
pre-processing time: 0.00032401084899902344
the split date is 2011-01-01
net initializing with time: 0.0030617713928222656
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.221345
average val loss: 0.099322, accuracy: 0.1259
average test loss: 0.112874, accuracy: 0.1288
case acc: 0.18357827
case acc: 0.07435581
case acc: 0.21263741
case acc: 0.10780769
case acc: 0.10213123
case acc: 0.09206917
top acc: 0.3015 ::: bot acc: 0.0721
top acc: 0.1193 ::: bot acc: 0.0272
top acc: 0.1347 ::: bot acc: 0.2954
top acc: 0.0217 ::: bot acc: 0.1786
top acc: 0.1548 ::: bot acc: 0.1048
top acc: 0.1083 ::: bot acc: 0.1261
current epoch: 2
train loss is 0.208538
average val loss: 0.116871, accuracy: 0.1358
average test loss: 0.119948, accuracy: 0.1364
case acc: 0.25263816
case acc: 0.14089641
case acc: 0.13096192
case acc: 0.07496408
case acc: 0.12071744
case acc: 0.098068126
top acc: 0.3697 ::: bot acc: 0.1418
top acc: 0.1942 ::: bot acc: 0.0797
top acc: 0.0523 ::: bot acc: 0.2143
top acc: 0.0698 ::: bot acc: 0.1047
top acc: 0.2264 ::: bot acc: 0.0339
top acc: 0.1816 ::: bot acc: 0.0533
current epoch: 3
train loss is 0.142007
average val loss: 0.112342, accuracy: 0.1268
average test loss: 0.134733, accuracy: 0.1407
case acc: 0.10501171
case acc: 0.044478394
case acc: 0.28522855
case acc: 0.18657763
case acc: 0.10797559
case acc: 0.11503287
top acc: 0.1997 ::: bot acc: 0.0390
top acc: 0.0300 ::: bot acc: 0.0852
top acc: 0.2064 ::: bot acc: 0.3688
top acc: 0.0887 ::: bot acc: 0.2639
top acc: 0.0651 ::: bot acc: 0.1941
top acc: 0.0313 ::: bot acc: 0.2127
current epoch: 4
train loss is 0.131315
average val loss: 0.096164, accuracy: 0.1199
average test loss: 0.106492, accuracy: 0.1227
case acc: 0.19927606
case acc: 0.09617441
case acc: 0.15502484
case acc: 0.08650651
case acc: 0.10861466
case acc: 0.09060529
top acc: 0.3152 ::: bot acc: 0.0890
top acc: 0.1493 ::: bot acc: 0.0380
top acc: 0.0758 ::: bot acc: 0.2389
top acc: 0.0363 ::: bot acc: 0.1405
top acc: 0.1867 ::: bot acc: 0.0721
top acc: 0.1465 ::: bot acc: 0.0894
current epoch: 5
train loss is 0.135629
average val loss: 0.087128, accuracy: 0.1117
average test loss: 0.103925, accuracy: 0.1165
case acc: 0.14842463
case acc: 0.059631318
case acc: 0.18948695
case acc: 0.10894453
case acc: 0.09998668
case acc: 0.09246729
top acc: 0.2638 ::: bot acc: 0.0389
top acc: 0.1019 ::: bot acc: 0.0242
top acc: 0.1101 ::: bot acc: 0.2736
top acc: 0.0207 ::: bot acc: 0.1819
top acc: 0.1431 ::: bot acc: 0.1156
top acc: 0.1049 ::: bot acc: 0.1312
current epoch: 6
train loss is 0.118483
average val loss: 0.083886, accuracy: 0.1072
average test loss: 0.102452, accuracy: 0.1135
case acc: 0.13430232
case acc: 0.051828124
case acc: 0.18875624
case acc: 0.1135819
case acc: 0.09934024
case acc: 0.093278676
top acc: 0.2470 ::: bot acc: 0.0298
top acc: 0.0890 ::: bot acc: 0.0274
top acc: 0.1092 ::: bot acc: 0.2730
top acc: 0.0225 ::: bot acc: 0.1881
top acc: 0.1349 ::: bot acc: 0.1237
top acc: 0.0990 ::: bot acc: 0.1374
current epoch: 7
train loss is 0.120205
average val loss: 0.081483, accuracy: 0.1050
average test loss: 0.097578, accuracy: 0.1096
case acc: 0.14805888
case acc: 0.06342773
case acc: 0.1565655
case acc: 0.09720869
case acc: 0.101954915
case acc: 0.09047605
top acc: 0.2626 ::: bot acc: 0.0395
top acc: 0.1090 ::: bot acc: 0.0228
top acc: 0.0769 ::: bot acc: 0.2409
top acc: 0.0243 ::: bot acc: 0.1627
top acc: 0.1581 ::: bot acc: 0.1005
top acc: 0.1244 ::: bot acc: 0.1123
current epoch: 8
train loss is 0.116971
average val loss: 0.077500, accuracy: 0.0999
average test loss: 0.096518, accuracy: 0.1068
case acc: 0.12922277
case acc: 0.05214654
case acc: 0.16267256
case acc: 0.10474287
case acc: 0.100039005
case acc: 0.09181952
top acc: 0.2401 ::: bot acc: 0.0282
top acc: 0.0903 ::: bot acc: 0.0267
top acc: 0.0830 ::: bot acc: 0.2470
top acc: 0.0219 ::: bot acc: 0.1753
top acc: 0.1435 ::: bot acc: 0.1152
top acc: 0.1118 ::: bot acc: 0.1250
current epoch: 9
train loss is 0.112633
average val loss: 0.074967, accuracy: 0.0967
average test loss: 0.093806, accuracy: 0.1035
case acc: 0.12818292
case acc: 0.05340437
case acc: 0.14838585
case acc: 0.09973155
case acc: 0.10073449
case acc: 0.09084939
top acc: 0.2387 ::: bot acc: 0.0279
top acc: 0.0928 ::: bot acc: 0.0256
top acc: 0.0687 ::: bot acc: 0.2329
top acc: 0.0233 ::: bot acc: 0.1672
top acc: 0.1496 ::: bot acc: 0.1090
top acc: 0.1197 ::: bot acc: 0.1169
current epoch: 10
train loss is 0.110687
average val loss: 0.072173, accuracy: 0.0929
average test loss: 0.092453, accuracy: 0.1012
case acc: 0.12015952
case acc: 0.049256925
case acc: 0.14508802
case acc: 0.10128854
case acc: 0.100158185
case acc: 0.09118418
top acc: 0.2267 ::: bot acc: 0.0277
top acc: 0.0850 ::: bot acc: 0.0289
top acc: 0.0654 ::: bot acc: 0.2296
top acc: 0.0229 ::: bot acc: 0.1697
top acc: 0.1448 ::: bot acc: 0.1136
top acc: 0.1170 ::: bot acc: 0.1198
current epoch: 11
train loss is 0.106403
average val loss: 0.069728, accuracy: 0.0894
average test loss: 0.091191, accuracy: 0.0990
case acc: 0.114252046
case acc: 0.046607524
case acc: 0.14034495
case acc: 0.101793274
case acc: 0.09981698
case acc: 0.091250576
top acc: 0.2169 ::: bot acc: 0.0296
top acc: 0.0793 ::: bot acc: 0.0327
top acc: 0.0605 ::: bot acc: 0.2249
top acc: 0.0230 ::: bot acc: 0.1705
top acc: 0.1419 ::: bot acc: 0.1163
top acc: 0.1159 ::: bot acc: 0.1209
current epoch: 12
train loss is 0.104279
average val loss: 0.067587, accuracy: 0.0863
average test loss: 0.089790, accuracy: 0.0968
case acc: 0.11020515
case acc: 0.045393877
case acc: 0.1333926
case acc: 0.10081717
case acc: 0.099787794
case acc: 0.09114141
top acc: 0.2100 ::: bot acc: 0.0313
top acc: 0.0765 ::: bot acc: 0.0348
top acc: 0.0536 ::: bot acc: 0.2179
top acc: 0.0233 ::: bot acc: 0.1689
top acc: 0.1415 ::: bot acc: 0.1167
top acc: 0.1174 ::: bot acc: 0.1196
current epoch: 13
train loss is 0.102047
average val loss: 0.065595, accuracy: 0.0829
average test loss: 0.089667, accuracy: 0.0957
case acc: 0.10334168
case acc: 0.042891946
case acc: 0.13325489
case acc: 0.10391949
case acc: 0.09932619
case acc: 0.0917621
top acc: 0.1967 ::: bot acc: 0.0372
top acc: 0.0674 ::: bot acc: 0.0437
top acc: 0.0534 ::: bot acc: 0.2177
top acc: 0.0228 ::: bot acc: 0.1738
top acc: 0.1346 ::: bot acc: 0.1237
top acc: 0.1123 ::: bot acc: 0.1248
current epoch: 14
train loss is 0.098576
average val loss: 0.064303, accuracy: 0.0800
average test loss: 0.090284, accuracy: 0.0955
case acc: 0.0975991
case acc: 0.041370116
case acc: 0.13449255
case acc: 0.10772156
case acc: 0.099045575
case acc: 0.0925617
top acc: 0.1831 ::: bot acc: 0.0475
top acc: 0.0577 ::: bot acc: 0.0532
top acc: 0.0546 ::: bot acc: 0.2190
top acc: 0.0224 ::: bot acc: 0.1796
top acc: 0.1266 ::: bot acc: 0.1316
top acc: 0.1062 ::: bot acc: 0.1310
current epoch: 15
train loss is 0.097182
average val loss: 0.062665, accuracy: 0.0776
average test loss: 0.089086, accuracy: 0.0936
case acc: 0.09586494
case acc: 0.041362163
case acc: 0.12717567
case acc: 0.105821
case acc: 0.099103056
case acc: 0.092285946
top acc: 0.1789 ::: bot acc: 0.0508
top acc: 0.0575 ::: bot acc: 0.0535
top acc: 0.0474 ::: bot acc: 0.2116
top acc: 0.0224 ::: bot acc: 0.1767
top acc: 0.1275 ::: bot acc: 0.1309
top acc: 0.1087 ::: bot acc: 0.1285
current epoch: 16
train loss is 0.095425
average val loss: 0.061870, accuracy: 0.0753
average test loss: 0.089416, accuracy: 0.0931
case acc: 0.091962315
case acc: 0.04113407
case acc: 0.12596396
case acc: 0.10772628
case acc: 0.09907175
case acc: 0.09271163
top acc: 0.1694 ::: bot acc: 0.0584
top acc: 0.0517 ::: bot acc: 0.0593
top acc: 0.0463 ::: bot acc: 0.2104
top acc: 0.0223 ::: bot acc: 0.1796
top acc: 0.1226 ::: bot acc: 0.1359
top acc: 0.1055 ::: bot acc: 0.1317
current epoch: 17
train loss is 0.093045
average val loss: 0.061100, accuracy: 0.0733
average test loss: 0.089305, accuracy: 0.0922
case acc: 0.089252755
case acc: 0.041264784
case acc: 0.12282557
case acc: 0.10814546
case acc: 0.099040054
case acc: 0.09286933
top acc: 0.1625 ::: bot acc: 0.0640
top acc: 0.0487 ::: bot acc: 0.0625
top acc: 0.0433 ::: bot acc: 0.2072
top acc: 0.0223 ::: bot acc: 0.1801
top acc: 0.1201 ::: bot acc: 0.1383
top acc: 0.1045 ::: bot acc: 0.1328
current epoch: 18
train loss is 0.091882
average val loss: 0.060165, accuracy: 0.0715
average test loss: 0.088740, accuracy: 0.0910
case acc: 0.087798074
case acc: 0.0412824
case acc: 0.118063815
case acc: 0.10725605
case acc: 0.09904558
case acc: 0.092773296
top acc: 0.1581 ::: bot acc: 0.0679
top acc: 0.0480 ::: bot acc: 0.0633
top acc: 0.0387 ::: bot acc: 0.2024
top acc: 0.0223 ::: bot acc: 0.1788
top acc: 0.1195 ::: bot acc: 0.1388
top acc: 0.1054 ::: bot acc: 0.1319
current epoch: 19
train loss is 0.090531
average val loss: 0.059784, accuracy: 0.0699
average test loss: 0.088972, accuracy: 0.0906
case acc: 0.085903354
case acc: 0.041433107
case acc: 0.116240196
case acc: 0.108068146
case acc: 0.09910222
case acc: 0.093011856
top acc: 0.1515 ::: bot acc: 0.0738
top acc: 0.0449 ::: bot acc: 0.0663
top acc: 0.0370 ::: bot acc: 0.2005
top acc: 0.0224 ::: bot acc: 0.1800
top acc: 0.1165 ::: bot acc: 0.1418
top acc: 0.1038 ::: bot acc: 0.1336
current epoch: 20
train loss is 0.089552
average val loss: 0.059034, accuracy: 0.0685
average test loss: 0.088479, accuracy: 0.0896
case acc: 0.08498608
case acc: 0.041467253
case acc: 0.11219807
case acc: 0.107024685
case acc: 0.09911675
case acc: 0.092858635
top acc: 0.1482 ::: bot acc: 0.0768
top acc: 0.0450 ::: bot acc: 0.0664
top acc: 0.0335 ::: bot acc: 0.1963
top acc: 0.0223 ::: bot acc: 0.1785
top acc: 0.1162 ::: bot acc: 0.1420
top acc: 0.1047 ::: bot acc: 0.1325
current epoch: 21
train loss is 0.088963
average val loss: 0.058479, accuracy: 0.0674
average test loss: 0.088180, accuracy: 0.0888
case acc: 0.08415675
case acc: 0.04146258
case acc: 0.10896272
case acc: 0.10631037
case acc: 0.09908812
case acc: 0.092763096
top acc: 0.1448 ::: bot acc: 0.0800
top acc: 0.0447 ::: bot acc: 0.0667
top acc: 0.0308 ::: bot acc: 0.1927
top acc: 0.0221 ::: bot acc: 0.1775
top acc: 0.1155 ::: bot acc: 0.1426
top acc: 0.1050 ::: bot acc: 0.1322
current epoch: 22
train loss is 0.087685
average val loss: 0.058134, accuracy: 0.0664
average test loss: 0.088129, accuracy: 0.0882
case acc: 0.08333832
case acc: 0.041501615
case acc: 0.106655635
case acc: 0.106116794
case acc: 0.099106416
case acc: 0.09274534
top acc: 0.1410 ::: bot acc: 0.0834
top acc: 0.0439 ::: bot acc: 0.0675
top acc: 0.0290 ::: bot acc: 0.1902
top acc: 0.0220 ::: bot acc: 0.1772
top acc: 0.1141 ::: bot acc: 0.1440
top acc: 0.1046 ::: bot acc: 0.1325
current epoch: 23
train loss is 0.087327
average val loss: 0.057905, accuracy: 0.0656
average test loss: 0.088179, accuracy: 0.0879
case acc: 0.082745746
case acc: 0.04161728
case acc: 0.104805335
case acc: 0.106105104
case acc: 0.09917819
case acc: 0.09281013
top acc: 0.1370 ::: bot acc: 0.0874
top acc: 0.0431 ::: bot acc: 0.0684
top acc: 0.0276 ::: bot acc: 0.1881
top acc: 0.0219 ::: bot acc: 0.1771
top acc: 0.1125 ::: bot acc: 0.1454
top acc: 0.1041 ::: bot acc: 0.1330
current epoch: 24
train loss is 0.086423
average val loss: 0.057909, accuracy: 0.0649
average test loss: 0.088464, accuracy: 0.0878
case acc: 0.0822052
case acc: 0.041703872
case acc: 0.103842095
case acc: 0.106484376
case acc: 0.099332385
case acc: 0.09297028
top acc: 0.1328 ::: bot acc: 0.0917
top acc: 0.0418 ::: bot acc: 0.0697
top acc: 0.0269 ::: bot acc: 0.1870
top acc: 0.0219 ::: bot acc: 0.1778
top acc: 0.1105 ::: bot acc: 0.1476
top acc: 0.1029 ::: bot acc: 0.1342
current epoch: 25
train loss is 0.085768
average val loss: 0.058057, accuracy: 0.0646
average test loss: 0.088839, accuracy: 0.0878
case acc: 0.081896596
case acc: 0.04187786
case acc: 0.103228346
case acc: 0.10706418
case acc: 0.099535495
case acc: 0.09326431
top acc: 0.1285 ::: bot acc: 0.0958
top acc: 0.0405 ::: bot acc: 0.0710
top acc: 0.0266 ::: bot acc: 0.1862
top acc: 0.0220 ::: bot acc: 0.1786
top acc: 0.1084 ::: bot acc: 0.1499
top acc: 0.1016 ::: bot acc: 0.1357
current epoch: 26
train loss is 0.085286
average val loss: 0.057573, accuracy: 0.0640
average test loss: 0.088335, accuracy: 0.0870
case acc: 0.08182745
case acc: 0.041708425
case acc: 0.10033676
case acc: 0.105782785
case acc: 0.099490546
case acc: 0.09308154
top acc: 0.1274 ::: bot acc: 0.0969
top acc: 0.0422 ::: bot acc: 0.0694
top acc: 0.0246 ::: bot acc: 0.1829
top acc: 0.0221 ::: bot acc: 0.1766
top acc: 0.1091 ::: bot acc: 0.1492
top acc: 0.1030 ::: bot acc: 0.1343
current epoch: 27
train loss is 0.084923
average val loss: 0.057261, accuracy: 0.0635
average test loss: 0.088023, accuracy: 0.0865
case acc: 0.08172009
case acc: 0.0415815
case acc: 0.098195456
case acc: 0.10488613
case acc: 0.099477895
case acc: 0.09302107
top acc: 0.1259 ::: bot acc: 0.0983
top acc: 0.0434 ::: bot acc: 0.0682
top acc: 0.0233 ::: bot acc: 0.1803
top acc: 0.0223 ::: bot acc: 0.1751
top acc: 0.1093 ::: bot acc: 0.1490
top acc: 0.1039 ::: bot acc: 0.1335
current epoch: 28
train loss is 0.084775
average val loss: 0.057485, accuracy: 0.0635
average test loss: 0.088407, accuracy: 0.0866
case acc: 0.081785746
case acc: 0.041685063
case acc: 0.0980614
case acc: 0.105374575
case acc: 0.099621765
case acc: 0.09326274
top acc: 0.1224 ::: bot acc: 0.1017
top acc: 0.0425 ::: bot acc: 0.0691
top acc: 0.0233 ::: bot acc: 0.1801
top acc: 0.0223 ::: bot acc: 0.1759
top acc: 0.1074 ::: bot acc: 0.1509
top acc: 0.1026 ::: bot acc: 0.1349
current epoch: 29
train loss is 0.084330
average val loss: 0.057659, accuracy: 0.0634
average test loss: 0.088680, accuracy: 0.0867
case acc: 0.082068354
case acc: 0.04170834
case acc: 0.097782545
case acc: 0.105601765
case acc: 0.09972044
case acc: 0.09338719
top acc: 0.1194 ::: bot acc: 0.1047
top acc: 0.0421 ::: bot acc: 0.0695
top acc: 0.0231 ::: bot acc: 0.1798
top acc: 0.0222 ::: bot acc: 0.1762
top acc: 0.1060 ::: bot acc: 0.1523
top acc: 0.1017 ::: bot acc: 0.1359
current epoch: 30
train loss is 0.084141
average val loss: 0.057981, accuracy: 0.0636
average test loss: 0.089100, accuracy: 0.0870
case acc: 0.0824194
case acc: 0.041786604
case acc: 0.09795998
case acc: 0.10610261
case acc: 0.09988424
case acc: 0.09360127
top acc: 0.1163 ::: bot acc: 0.1078
top acc: 0.0413 ::: bot acc: 0.0702
top acc: 0.0232 ::: bot acc: 0.1800
top acc: 0.0221 ::: bot acc: 0.1770
top acc: 0.1041 ::: bot acc: 0.1542
top acc: 0.1003 ::: bot acc: 0.1373
current epoch: 31
train loss is 0.083665
average val loss: 0.058108, accuracy: 0.0637
average test loss: 0.089248, accuracy: 0.0869
case acc: 0.08262415
case acc: 0.04175131
case acc: 0.09752554
case acc: 0.10609414
case acc: 0.09997298
case acc: 0.09370322
top acc: 0.1143 ::: bot acc: 0.1098
top acc: 0.0416 ::: bot acc: 0.0699
top acc: 0.0229 ::: bot acc: 0.1795
top acc: 0.0221 ::: bot acc: 0.1770
top acc: 0.1031 ::: bot acc: 0.1551
top acc: 0.0997 ::: bot acc: 0.1380
current epoch: 32
train loss is 0.083697
average val loss: 0.057863, accuracy: 0.0635
average test loss: 0.088936, accuracy: 0.0865
case acc: 0.08270251
case acc: 0.04159939
case acc: 0.09609043
case acc: 0.10517173
case acc: 0.0999315
case acc: 0.09358915
top acc: 0.1139 ::: bot acc: 0.1102
top acc: 0.0434 ::: bot acc: 0.0683
top acc: 0.0222 ::: bot acc: 0.1777
top acc: 0.0222 ::: bot acc: 0.1755
top acc: 0.1036 ::: bot acc: 0.1546
top acc: 0.1004 ::: bot acc: 0.1372
current epoch: 33
train loss is 0.083699
average val loss: 0.057571, accuracy: 0.0633
average test loss: 0.088569, accuracy: 0.0860
case acc: 0.082733296
case acc: 0.04148927
case acc: 0.09458873
case acc: 0.10413885
case acc: 0.09985663
case acc: 0.09346033
top acc: 0.1134 ::: bot acc: 0.1106
top acc: 0.0451 ::: bot acc: 0.0666
top acc: 0.0215 ::: bot acc: 0.1758
top acc: 0.0223 ::: bot acc: 0.1739
top acc: 0.1043 ::: bot acc: 0.1539
top acc: 0.1015 ::: bot acc: 0.1362
current epoch: 34
train loss is 0.083266
average val loss: 0.057879, accuracy: 0.0635
average test loss: 0.088965, accuracy: 0.0863
case acc: 0.08317925
case acc: 0.041559543
case acc: 0.09503674
case acc: 0.104474485
case acc: 0.09999465
case acc: 0.0935787
top acc: 0.1105 ::: bot acc: 0.1137
top acc: 0.0444 ::: bot acc: 0.0675
top acc: 0.0216 ::: bot acc: 0.1764
top acc: 0.0221 ::: bot acc: 0.1745
top acc: 0.1028 ::: bot acc: 0.1553
top acc: 0.1002 ::: bot acc: 0.1373
current epoch: 35
train loss is 0.083082
average val loss: 0.058417, accuracy: 0.0639
average test loss: 0.089637, accuracy: 0.0868
case acc: 0.083657056
case acc: 0.041718468
case acc: 0.09622741
case acc: 0.10530121
case acc: 0.100223854
case acc: 0.09380658
top acc: 0.1071 ::: bot acc: 0.1174
top acc: 0.0429 ::: bot acc: 0.0691
top acc: 0.0221 ::: bot acc: 0.1780
top acc: 0.0219 ::: bot acc: 0.1758
top acc: 0.1006 ::: bot acc: 0.1574
top acc: 0.0982 ::: bot acc: 0.1392
current epoch: 36
train loss is 0.082808
average val loss: 0.057782, accuracy: 0.0634
average test loss: 0.088868, accuracy: 0.0860
case acc: 0.083521195
case acc: 0.04149321
case acc: 0.09413293
case acc: 0.10351025
case acc: 0.09997769
case acc: 0.0935034
top acc: 0.1081 ::: bot acc: 0.1164
top acc: 0.0457 ::: bot acc: 0.0662
top acc: 0.0210 ::: bot acc: 0.1753
top acc: 0.0221 ::: bot acc: 0.1731
top acc: 0.1026 ::: bot acc: 0.1554
top acc: 0.1005 ::: bot acc: 0.1370
current epoch: 37
train loss is 0.082702
average val loss: 0.057943, accuracy: 0.0636
average test loss: 0.089042, accuracy: 0.0861
case acc: 0.08367188
case acc: 0.04141601
case acc: 0.09434531
case acc: 0.10352238
case acc: 0.10007911
case acc: 0.093558975
top acc: 0.1065 ::: bot acc: 0.1179
top acc: 0.0459 ::: bot acc: 0.0659
top acc: 0.0211 ::: bot acc: 0.1756
top acc: 0.0221 ::: bot acc: 0.1731
top acc: 0.1018 ::: bot acc: 0.1562
top acc: 0.0998 ::: bot acc: 0.1377
current epoch: 38
train loss is 0.082678
average val loss: 0.058054, accuracy: 0.0638
average test loss: 0.089144, accuracy: 0.0862
case acc: 0.08381559
case acc: 0.041395262
case acc: 0.09445553
case acc: 0.10347772
case acc: 0.10021324
case acc: 0.0937083
top acc: 0.1051 ::: bot acc: 0.1191
top acc: 0.0462 ::: bot acc: 0.0656
top acc: 0.0213 ::: bot acc: 0.1756
top acc: 0.0222 ::: bot acc: 0.1730
top acc: 0.1014 ::: bot acc: 0.1569
top acc: 0.0996 ::: bot acc: 0.1380
current epoch: 39
train loss is 0.082428
average val loss: 0.058370, accuracy: 0.0641
average test loss: 0.089515, accuracy: 0.0865
case acc: 0.083994046
case acc: 0.041437764
case acc: 0.09519075
case acc: 0.103947245
case acc: 0.10036662
case acc: 0.09395448
top acc: 0.1030 ::: bot acc: 0.1210
top acc: 0.0458 ::: bot acc: 0.0660
top acc: 0.0217 ::: bot acc: 0.1765
top acc: 0.0223 ::: bot acc: 0.1736
top acc: 0.1001 ::: bot acc: 0.1582
top acc: 0.0986 ::: bot acc: 0.1392
current epoch: 40
train loss is 0.082289
average val loss: 0.058701, accuracy: 0.0644
average test loss: 0.089888, accuracy: 0.0868
case acc: 0.08424527
case acc: 0.0414361
case acc: 0.09592684
case acc: 0.104326226
case acc: 0.10056567
case acc: 0.094123274
top acc: 0.1010 ::: bot acc: 0.1230
top acc: 0.0452 ::: bot acc: 0.0665
top acc: 0.0221 ::: bot acc: 0.1775
top acc: 0.0223 ::: bot acc: 0.1741
top acc: 0.0989 ::: bot acc: 0.1594
top acc: 0.0975 ::: bot acc: 0.1404
current epoch: 41
train loss is 0.082266
average val loss: 0.059328, accuracy: 0.0650
average test loss: 0.090590, accuracy: 0.0874
case acc: 0.084676675
case acc: 0.041535087
case acc: 0.097379014
case acc: 0.105262935
case acc: 0.10095846
case acc: 0.09441488
top acc: 0.0981 ::: bot acc: 0.1258
top acc: 0.0438 ::: bot acc: 0.0680
top acc: 0.0229 ::: bot acc: 0.1792
top acc: 0.0221 ::: bot acc: 0.1756
top acc: 0.0968 ::: bot acc: 0.1615
top acc: 0.0955 ::: bot acc: 0.1423
current epoch: 42
train loss is 0.081884
average val loss: 0.058757, accuracy: 0.0646
average test loss: 0.089925, accuracy: 0.0867
case acc: 0.08445271
case acc: 0.04140486
case acc: 0.09586535
case acc: 0.10383758
case acc: 0.10068798
case acc: 0.09414977
top acc: 0.0995 ::: bot acc: 0.1244
top acc: 0.0463 ::: bot acc: 0.0655
top acc: 0.0222 ::: bot acc: 0.1773
top acc: 0.0223 ::: bot acc: 0.1734
top acc: 0.0985 ::: bot acc: 0.1599
top acc: 0.0973 ::: bot acc: 0.1405
current epoch: 43
train loss is 0.082157
average val loss: 0.058558, accuracy: 0.0645
average test loss: 0.089667, accuracy: 0.0865
case acc: 0.08438713
case acc: 0.041384183
case acc: 0.09526343
case acc: 0.103135526
case acc: 0.100576706
case acc: 0.09412642
top acc: 0.0996 ::: bot acc: 0.1242
top acc: 0.0477 ::: bot acc: 0.0642
top acc: 0.0219 ::: bot acc: 0.1765
top acc: 0.0224 ::: bot acc: 0.1723
top acc: 0.0991 ::: bot acc: 0.1593
top acc: 0.0980 ::: bot acc: 0.1400
current epoch: 44
train loss is 0.081987
average val loss: 0.057905, accuracy: 0.0639
average test loss: 0.088846, accuracy: 0.0857
case acc: 0.08411935
case acc: 0.04125919
case acc: 0.093388304
case acc: 0.10132412
case acc: 0.10022907
case acc: 0.09370826
top acc: 0.1015 ::: bot acc: 0.1225
top acc: 0.0505 ::: bot acc: 0.0612
top acc: 0.0209 ::: bot acc: 0.1743
top acc: 0.0227 ::: bot acc: 0.1694
top acc: 0.1014 ::: bot acc: 0.1569
top acc: 0.1003 ::: bot acc: 0.1377
current epoch: 45
train loss is 0.081924
average val loss: 0.057851, accuracy: 0.0639
average test loss: 0.088761, accuracy: 0.0856
case acc: 0.08421556
case acc: 0.041266475
case acc: 0.0931872
case acc: 0.100932874
case acc: 0.100169934
case acc: 0.09367258
top acc: 0.1012 ::: bot acc: 0.1228
top acc: 0.0512 ::: bot acc: 0.0604
top acc: 0.0207 ::: bot acc: 0.1741
top acc: 0.0228 ::: bot acc: 0.1688
top acc: 0.1016 ::: bot acc: 0.1566
top acc: 0.1003 ::: bot acc: 0.1376
current epoch: 46
train loss is 0.081849
average val loss: 0.057683, accuracy: 0.0638
average test loss: 0.088510, accuracy: 0.0853
case acc: 0.08416756
case acc: 0.04125677
case acc: 0.092571735
case acc: 0.10025068
case acc: 0.10004967
case acc: 0.0935232
top acc: 0.1017 ::: bot acc: 0.1225
top acc: 0.0525 ::: bot acc: 0.0591
top acc: 0.0204 ::: bot acc: 0.1733
top acc: 0.0230 ::: bot acc: 0.1678
top acc: 0.1022 ::: bot acc: 0.1558
top acc: 0.1009 ::: bot acc: 0.1370
current epoch: 47
train loss is 0.081959
average val loss: 0.057494, accuracy: 0.0636
average test loss: 0.088222, accuracy: 0.0850
case acc: 0.084103435
case acc: 0.041340813
case acc: 0.09180141
case acc: 0.09953494
case acc: 0.099977955
case acc: 0.09347016
top acc: 0.1022 ::: bot acc: 0.1220
top acc: 0.0538 ::: bot acc: 0.0578
top acc: 0.0201 ::: bot acc: 0.1723
top acc: 0.0233 ::: bot acc: 0.1666
top acc: 0.1030 ::: bot acc: 0.1551
top acc: 0.1017 ::: bot acc: 0.1363
current epoch: 48
train loss is 0.081662
average val loss: 0.058194, accuracy: 0.0643
average test loss: 0.089108, accuracy: 0.0858
case acc: 0.08458495
case acc: 0.041265607
case acc: 0.09383209
case acc: 0.100931756
case acc: 0.10036758
case acc: 0.09395761
top acc: 0.0989 ::: bot acc: 0.1251
top acc: 0.0514 ::: bot acc: 0.0603
top acc: 0.0210 ::: bot acc: 0.1749
top acc: 0.0228 ::: bot acc: 0.1688
top acc: 0.1002 ::: bot acc: 0.1580
top acc: 0.0990 ::: bot acc: 0.1390
current epoch: 49
train loss is 0.081537
average val loss: 0.058550, accuracy: 0.0646
average test loss: 0.089543, accuracy: 0.0862
case acc: 0.08481675
case acc: 0.041298494
case acc: 0.09491705
case acc: 0.101497285
case acc: 0.100567885
case acc: 0.0941981
top acc: 0.0971 ::: bot acc: 0.1268
top acc: 0.0504 ::: bot acc: 0.0614
top acc: 0.0216 ::: bot acc: 0.1762
top acc: 0.0226 ::: bot acc: 0.1697
top acc: 0.0989 ::: bot acc: 0.1594
top acc: 0.0978 ::: bot acc: 0.1403
current epoch: 50
train loss is 0.081655
average val loss: 0.058185, accuracy: 0.0644
average test loss: 0.089100, accuracy: 0.0859
case acc: 0.084737375
case acc: 0.041461136
case acc: 0.09401681
case acc: 0.10052132
case acc: 0.100367144
case acc: 0.094149426
top acc: 0.0980 ::: bot acc: 0.1260
top acc: 0.0522 ::: bot acc: 0.0600
top acc: 0.0213 ::: bot acc: 0.1750
top acc: 0.0231 ::: bot acc: 0.1680
top acc: 0.1002 ::: bot acc: 0.1580
top acc: 0.0991 ::: bot acc: 0.1391
LME_Co_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5112 5112 5112
1.7082474 -0.6288155 0.48738334 -0.33910522
Validation: 570 570 570
Testing: 774 774 774
pre-processing time: 0.0005156993865966797
the split date is 2011-07-01
net initializing with time: 0.004001140594482422
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.288951
average val loss: 0.107504, accuracy: 0.0853
average test loss: 0.109594, accuracy: 0.1145
case acc: 0.075850785
case acc: 0.17266816
case acc: 0.055529676
case acc: 0.16615416
case acc: 0.12666154
case acc: 0.090244636
top acc: 0.0388 ::: bot acc: 0.1301
top acc: 0.1078 ::: bot acc: 0.2255
top acc: 0.1006 ::: bot acc: 0.0416
top acc: 0.2738 ::: bot acc: 0.0678
top acc: 0.1161 ::: bot acc: 0.1610
top acc: 0.1647 ::: bot acc: 0.0236
current epoch: 2
train loss is 0.211425
average val loss: 0.144262, accuracy: 0.1307
average test loss: 0.139638, accuracy: 0.1429
case acc: 0.15850541
case acc: 0.28572193
case acc: 0.08860934
case acc: 0.08682892
case acc: 0.17830555
case acc: 0.05943976
top acc: 0.0734 ::: bot acc: 0.2373
top acc: 0.2210 ::: bot acc: 0.3392
top acc: 0.0319 ::: bot acc: 0.1493
top acc: 0.1663 ::: bot acc: 0.0434
top acc: 0.0586 ::: bot acc: 0.2678
top acc: 0.0594 ::: bot acc: 0.0930
current epoch: 3
train loss is 0.167410
average val loss: 0.111139, accuracy: 0.0951
average test loss: 0.115810, accuracy: 0.1189
case acc: 0.061610058
case acc: 0.11837695
case acc: 0.07862638
case acc: 0.21069255
case acc: 0.10886781
case acc: 0.13514999
top acc: 0.0840 ::: bot acc: 0.0801
top acc: 0.0537 ::: bot acc: 0.1721
top acc: 0.1467 ::: bot acc: 0.0206
top acc: 0.3179 ::: bot acc: 0.1107
top acc: 0.1610 ::: bot acc: 0.1129
top acc: 0.2139 ::: bot acc: 0.0610
current epoch: 4
train loss is 0.133301
average val loss: 0.128480, accuracy: 0.1138
average test loss: 0.124503, accuracy: 0.1286
case acc: 0.135631
case acc: 0.23984438
case acc: 0.079940505
case acc: 0.091814056
case acc: 0.16647951
case acc: 0.057920545
top acc: 0.0537 ::: bot acc: 0.2137
top acc: 0.1750 ::: bot acc: 0.2940
top acc: 0.0307 ::: bot acc: 0.1367
top acc: 0.1751 ::: bot acc: 0.0388
top acc: 0.0628 ::: bot acc: 0.2487
top acc: 0.0751 ::: bot acc: 0.0780
current epoch: 5
train loss is 0.133474
average val loss: 0.097342, accuracy: 0.0767
average test loss: 0.099266, accuracy: 0.1036
case acc: 0.07212918
case acc: 0.1361577
case acc: 0.052652936
case acc: 0.14983176
case acc: 0.12727995
case acc: 0.08373646
top acc: 0.0415 ::: bot acc: 0.1247
top acc: 0.0712 ::: bot acc: 0.1905
top acc: 0.0866 ::: bot acc: 0.0565
top acc: 0.2534 ::: bot acc: 0.0560
top acc: 0.1158 ::: bot acc: 0.1636
top acc: 0.1579 ::: bot acc: 0.0196
current epoch: 6
train loss is 0.114222
average val loss: 0.106228, accuracy: 0.0903
average test loss: 0.103852, accuracy: 0.1085
case acc: 0.10175344
case acc: 0.17083822
case acc: 0.06716224
case acc: 0.10299209
case acc: 0.14917077
case acc: 0.058935292
top acc: 0.0343 ::: bot acc: 0.1727
top acc: 0.1059 ::: bot acc: 0.2252
top acc: 0.0355 ::: bot acc: 0.1150
top acc: 0.1927 ::: bot acc: 0.0366
top acc: 0.0774 ::: bot acc: 0.2158
top acc: 0.1023 ::: bot acc: 0.0512
current epoch: 7
train loss is 0.116965
average val loss: 0.093734, accuracy: 0.0773
average test loss: 0.093190, accuracy: 0.0972
case acc: 0.08046672
case acc: 0.12591974
case acc: 0.05785448
case acc: 0.11648751
case acc: 0.1374873
case acc: 0.06526189
top acc: 0.0338 ::: bot acc: 0.1411
top acc: 0.0609 ::: bot acc: 0.1802
top acc: 0.0485 ::: bot acc: 0.0945
top acc: 0.2113 ::: bot acc: 0.0398
top acc: 0.0959 ::: bot acc: 0.1891
top acc: 0.1264 ::: bot acc: 0.0272
current epoch: 8
train loss is 0.104537
average val loss: 0.091060, accuracy: 0.0764
average test loss: 0.090089, accuracy: 0.0936
case acc: 0.078657985
case acc: 0.11064176
case acc: 0.060875416
case acc: 0.10926823
case acc: 0.13812704
case acc: 0.06382889
top acc: 0.0342 ::: bot acc: 0.1381
top acc: 0.0467 ::: bot acc: 0.1645
top acc: 0.0423 ::: bot acc: 0.1021
top acc: 0.2016 ::: bot acc: 0.0375
top acc: 0.0948 ::: bot acc: 0.1907
top acc: 0.1218 ::: bot acc: 0.0319
current epoch: 9
train loss is 0.100940
average val loss: 0.090058, accuracy: 0.0773
average test loss: 0.088459, accuracy: 0.0914
case acc: 0.079177015
case acc: 0.10063959
case acc: 0.066272445
case acc: 0.10025815
case acc: 0.14032376
case acc: 0.061507612
top acc: 0.0339 ::: bot acc: 0.1391
top acc: 0.0389 ::: bot acc: 0.1534
top acc: 0.0361 ::: bot acc: 0.1132
top acc: 0.1885 ::: bot acc: 0.0365
top acc: 0.0912 ::: bot acc: 0.1959
top acc: 0.1136 ::: bot acc: 0.0401
current epoch: 10
train loss is 0.097009
average val loss: 0.085562, accuracy: 0.0730
average test loss: 0.084285, accuracy: 0.0866
case acc: 0.07288629
case acc: 0.0813468
case acc: 0.06504603
case acc: 0.10068246
case acc: 0.1368926
case acc: 0.06297602
top acc: 0.0397 ::: bot acc: 0.1268
top acc: 0.0286 ::: bot acc: 0.1297
top acc: 0.0372 ::: bot acc: 0.1107
top acc: 0.1891 ::: bot acc: 0.0365
top acc: 0.0972 ::: bot acc: 0.1878
top acc: 0.1190 ::: bot acc: 0.0348
current epoch: 11
train loss is 0.091851
average val loss: 0.082949, accuracy: 0.0708
average test loss: 0.081727, accuracy: 0.0836
case acc: 0.06999604
case acc: 0.0684948
case acc: 0.06593115
case acc: 0.09842314
case acc: 0.13540663
case acc: 0.06320427
top acc: 0.0455 ::: bot acc: 0.1195
top acc: 0.0263 ::: bot acc: 0.1116
top acc: 0.0363 ::: bot acc: 0.1124
top acc: 0.1855 ::: bot acc: 0.0366
top acc: 0.1000 ::: bot acc: 0.1842
top acc: 0.1197 ::: bot acc: 0.0342
current epoch: 12
train loss is 0.087781
average val loss: 0.081596, accuracy: 0.0703
average test loss: 0.080269, accuracy: 0.0817
case acc: 0.0687613
case acc: 0.061069924
case acc: 0.06840306
case acc: 0.09452857
case acc: 0.13523726
case acc: 0.06248812
top acc: 0.0487 ::: bot acc: 0.1160
top acc: 0.0301 ::: bot acc: 0.0985
top acc: 0.0344 ::: bot acc: 0.1171
top acc: 0.1793 ::: bot acc: 0.0374
top acc: 0.1004 ::: bot acc: 0.1838
top acc: 0.1171 ::: bot acc: 0.0368
current epoch: 13
train loss is 0.084453
average val loss: 0.079100, accuracy: 0.0680
average test loss: 0.078134, accuracy: 0.0795
case acc: 0.0661028
case acc: 0.0526645
case acc: 0.06728641
case acc: 0.09494531
case acc: 0.1324173
case acc: 0.06359675
top acc: 0.0577 ::: bot acc: 0.1069
top acc: 0.0399 ::: bot acc: 0.0810
top acc: 0.0353 ::: bot acc: 0.1149
top acc: 0.1800 ::: bot acc: 0.0373
top acc: 0.1056 ::: bot acc: 0.1770
top acc: 0.1208 ::: bot acc: 0.0332
current epoch: 14
train loss is 0.081931
average val loss: 0.077634, accuracy: 0.0672
average test loss: 0.076880, accuracy: 0.0781
case acc: 0.06464568
case acc: 0.047846723
case acc: 0.067200795
case acc: 0.094281025
case acc: 0.13064116
case acc: 0.063956015
top acc: 0.0635 ::: bot acc: 0.1011
top acc: 0.0513 ::: bot acc: 0.0681
top acc: 0.0355 ::: bot acc: 0.1147
top acc: 0.1788 ::: bot acc: 0.0376
top acc: 0.1092 ::: bot acc: 0.1725
top acc: 0.1220 ::: bot acc: 0.0320
current epoch: 15
train loss is 0.080066
average val loss: 0.076427, accuracy: 0.0666
average test loss: 0.075967, accuracy: 0.0771
case acc: 0.06368943
case acc: 0.045176644
case acc: 0.066499665
case acc: 0.09441658
case acc: 0.12863068
case acc: 0.06447854
top acc: 0.0690 ::: bot acc: 0.0957
top acc: 0.0622 ::: bot acc: 0.0572
top acc: 0.0361 ::: bot acc: 0.1132
top acc: 0.1790 ::: bot acc: 0.0375
top acc: 0.1134 ::: bot acc: 0.1674
top acc: 0.1238 ::: bot acc: 0.0303
current epoch: 16
train loss is 0.079322
average val loss: 0.076024, accuracy: 0.0669
average test loss: 0.075587, accuracy: 0.0767
case acc: 0.0634852
case acc: 0.044418775
case acc: 0.06712671
case acc: 0.09311655
case acc: 0.1278135
case acc: 0.06405451
top acc: 0.0706 ::: bot acc: 0.0941
top acc: 0.0674 ::: bot acc: 0.0521
top acc: 0.0356 ::: bot acc: 0.1144
top acc: 0.1768 ::: bot acc: 0.0381
top acc: 0.1151 ::: bot acc: 0.1653
top acc: 0.1224 ::: bot acc: 0.0316
current epoch: 17
train loss is 0.078878
average val loss: 0.075396, accuracy: 0.0667
average test loss: 0.075153, accuracy: 0.0762
case acc: 0.06311471
case acc: 0.043958426
case acc: 0.06635465
case acc: 0.09347523
case acc: 0.12606308
case acc: 0.064362265
top acc: 0.0738 ::: bot acc: 0.0909
top acc: 0.0728 ::: bot acc: 0.0466
top acc: 0.0364 ::: bot acc: 0.1129
top acc: 0.1774 ::: bot acc: 0.0379
top acc: 0.1188 ::: bot acc: 0.1609
top acc: 0.1235 ::: bot acc: 0.0307
current epoch: 18
train loss is 0.078818
average val loss: 0.074935, accuracy: 0.0666
average test loss: 0.074846, accuracy: 0.0759
case acc: 0.06285177
case acc: 0.043871105
case acc: 0.06563509
case acc: 0.093832426
case acc: 0.12446704
case acc: 0.06455792
top acc: 0.0761 ::: bot acc: 0.0886
top acc: 0.0764 ::: bot acc: 0.0431
top acc: 0.0372 ::: bot acc: 0.1114
top acc: 0.1780 ::: bot acc: 0.0377
top acc: 0.1221 ::: bot acc: 0.1568
top acc: 0.1241 ::: bot acc: 0.0300
current epoch: 19
train loss is 0.078680
average val loss: 0.074973, accuracy: 0.0669
average test loss: 0.074761, accuracy: 0.0756
case acc: 0.06298812
case acc: 0.04387634
case acc: 0.06640804
case acc: 0.092597686
case acc: 0.1241196
case acc: 0.06379811
top acc: 0.0747 ::: bot acc: 0.0899
top acc: 0.0754 ::: bot acc: 0.0441
top acc: 0.0366 ::: bot acc: 0.1128
top acc: 0.1758 ::: bot acc: 0.0384
top acc: 0.1228 ::: bot acc: 0.1559
top acc: 0.1217 ::: bot acc: 0.0325
current epoch: 20
train loss is 0.078609
average val loss: 0.075122, accuracy: 0.0673
average test loss: 0.074749, accuracy: 0.0754
case acc: 0.06318987
case acc: 0.0439479
case acc: 0.067290515
case acc: 0.09132548
case acc: 0.12394504
case acc: 0.06298823
top acc: 0.0727 ::: bot acc: 0.0919
top acc: 0.0732 ::: bot acc: 0.0464
top acc: 0.0358 ::: bot acc: 0.1145
top acc: 0.1733 ::: bot acc: 0.0394
top acc: 0.1232 ::: bot acc: 0.1554
top acc: 0.1190 ::: bot acc: 0.0352
current epoch: 21
train loss is 0.078315
average val loss: 0.075036, accuracy: 0.0674
average test loss: 0.074635, accuracy: 0.0752
case acc: 0.063194074
case acc: 0.043971483
case acc: 0.06734034
case acc: 0.09096474
case acc: 0.123120874
case acc: 0.06269119
top acc: 0.0726 ::: bot acc: 0.0920
top acc: 0.0724 ::: bot acc: 0.0471
top acc: 0.0358 ::: bot acc: 0.1146
top acc: 0.1726 ::: bot acc: 0.0397
top acc: 0.1250 ::: bot acc: 0.1532
top acc: 0.1180 ::: bot acc: 0.0361
current epoch: 22
train loss is 0.078193
average val loss: 0.075420, accuracy: 0.0680
average test loss: 0.074772, accuracy: 0.0752
case acc: 0.06357661
case acc: 0.044216815
case acc: 0.06877836
case acc: 0.08931978
case acc: 0.12336459
case acc: 0.061657324
top acc: 0.0699 ::: bot acc: 0.0947
top acc: 0.0688 ::: bot acc: 0.0507
top acc: 0.0347 ::: bot acc: 0.1173
top acc: 0.1692 ::: bot acc: 0.0414
top acc: 0.1245 ::: bot acc: 0.1538
top acc: 0.1146 ::: bot acc: 0.0395
current epoch: 23
train loss is 0.078128
average val loss: 0.075693, accuracy: 0.0686
average test loss: 0.074877, accuracy: 0.0751
case acc: 0.06384712
case acc: 0.04454788
case acc: 0.06978038
case acc: 0.0881794
case acc: 0.12328115
case acc: 0.06096491
top acc: 0.0683 ::: bot acc: 0.0963
top acc: 0.0661 ::: bot acc: 0.0535
top acc: 0.0340 ::: bot acc: 0.1192
top acc: 0.1669 ::: bot acc: 0.0428
top acc: 0.1247 ::: bot acc: 0.1536
top acc: 0.1122 ::: bot acc: 0.0418
current epoch: 24
train loss is 0.077786
average val loss: 0.076110, accuracy: 0.0694
average test loss: 0.075089, accuracy: 0.0752
case acc: 0.06420801
case acc: 0.04502922
case acc: 0.07119708
case acc: 0.086788446
case acc: 0.123467855
case acc: 0.060249113
top acc: 0.0663 ::: bot acc: 0.0983
top acc: 0.0631 ::: bot acc: 0.0564
top acc: 0.0330 ::: bot acc: 0.1218
top acc: 0.1638 ::: bot acc: 0.0447
top acc: 0.1243 ::: bot acc: 0.1540
top acc: 0.1092 ::: bot acc: 0.0448
current epoch: 25
train loss is 0.077607
average val loss: 0.076466, accuracy: 0.0701
average test loss: 0.075284, accuracy: 0.0753
case acc: 0.06448101
case acc: 0.045480855
case acc: 0.07258177
case acc: 0.08568253
case acc: 0.12356814
case acc: 0.05971441
top acc: 0.0649 ::: bot acc: 0.0998
top acc: 0.0609 ::: bot acc: 0.0586
top acc: 0.0325 ::: bot acc: 0.1241
top acc: 0.1611 ::: bot acc: 0.0469
top acc: 0.1241 ::: bot acc: 0.1542
top acc: 0.1069 ::: bot acc: 0.0471
current epoch: 26
train loss is 0.077369
average val loss: 0.076436, accuracy: 0.0704
average test loss: 0.075235, accuracy: 0.0751
case acc: 0.06440494
case acc: 0.04556565
case acc: 0.072986916
case acc: 0.08527529
case acc: 0.123044744
case acc: 0.05958018
top acc: 0.0652 ::: bot acc: 0.0994
top acc: 0.0603 ::: bot acc: 0.0591
top acc: 0.0323 ::: bot acc: 0.1248
top acc: 0.1600 ::: bot acc: 0.0478
top acc: 0.1253 ::: bot acc: 0.1529
top acc: 0.1064 ::: bot acc: 0.0476
current epoch: 27
train loss is 0.077324
average val loss: 0.076595, accuracy: 0.0709
average test loss: 0.075317, accuracy: 0.0751
case acc: 0.06448059
case acc: 0.045754477
case acc: 0.07392098
case acc: 0.084522225
case acc: 0.12290664
case acc: 0.059297066
top acc: 0.0648 ::: bot acc: 0.0999
top acc: 0.0593 ::: bot acc: 0.0602
top acc: 0.0320 ::: bot acc: 0.1263
top acc: 0.1580 ::: bot acc: 0.0494
top acc: 0.1256 ::: bot acc: 0.1526
top acc: 0.1052 ::: bot acc: 0.0488
current epoch: 28
train loss is 0.077204
average val loss: 0.077069, accuracy: 0.0718
average test loss: 0.075617, accuracy: 0.0753
case acc: 0.06476665
case acc: 0.046168372
case acc: 0.07564647
case acc: 0.083312154
case acc: 0.12330121
case acc: 0.058852497
top acc: 0.0633 ::: bot acc: 0.1014
top acc: 0.0574 ::: bot acc: 0.0621
top acc: 0.0314 ::: bot acc: 0.1292
top acc: 0.1548 ::: bot acc: 0.0521
top acc: 0.1248 ::: bot acc: 0.1535
top acc: 0.1027 ::: bot acc: 0.0512
current epoch: 29
train loss is 0.077019
average val loss: 0.077235, accuracy: 0.0723
average test loss: 0.075708, accuracy: 0.0754
case acc: 0.06483141
case acc: 0.046299823
case acc: 0.076599956
case acc: 0.08264952
case acc: 0.1232404
case acc: 0.058713567
top acc: 0.0632 ::: bot acc: 0.1016
top acc: 0.0569 ::: bot acc: 0.0626
top acc: 0.0311 ::: bot acc: 0.1308
top acc: 0.1531 ::: bot acc: 0.0538
top acc: 0.1249 ::: bot acc: 0.1534
top acc: 0.1017 ::: bot acc: 0.0522
current epoch: 30
train loss is 0.076735
average val loss: 0.077744, accuracy: 0.0732
average test loss: 0.076050, accuracy: 0.0757
case acc: 0.0651619
case acc: 0.046643846
case acc: 0.07835201
case acc: 0.081618056
case acc: 0.12372716
case acc: 0.058421955
top acc: 0.0617 ::: bot acc: 0.1031
top acc: 0.0555 ::: bot acc: 0.0640
top acc: 0.0307 ::: bot acc: 0.1337
top acc: 0.1500 ::: bot acc: 0.0567
top acc: 0.1239 ::: bot acc: 0.1546
top acc: 0.0995 ::: bot acc: 0.0544
current epoch: 31
train loss is 0.076720
average val loss: 0.077576, accuracy: 0.0733
average test loss: 0.075912, accuracy: 0.0755
case acc: 0.064928114
case acc: 0.04625366
case acc: 0.07865221
case acc: 0.08141985
case acc: 0.123312846
case acc: 0.058422178
top acc: 0.0627 ::: bot acc: 0.1021
top acc: 0.0571 ::: bot acc: 0.0624
top acc: 0.0306 ::: bot acc: 0.1341
top acc: 0.1492 ::: bot acc: 0.0575
top acc: 0.1248 ::: bot acc: 0.1536
top acc: 0.0997 ::: bot acc: 0.0542
current epoch: 32
train loss is 0.076479
average val loss: 0.077346, accuracy: 0.0734
average test loss: 0.075732, accuracy: 0.0753
case acc: 0.06471704
case acc: 0.045885272
case acc: 0.078738816
case acc: 0.081332095
case acc: 0.12282063
case acc: 0.05847635
top acc: 0.0639 ::: bot acc: 0.1009
top acc: 0.0587 ::: bot acc: 0.0608
top acc: 0.0306 ::: bot acc: 0.1343
top acc: 0.1489 ::: bot acc: 0.0578
top acc: 0.1258 ::: bot acc: 0.1523
top acc: 0.1002 ::: bot acc: 0.0536
current epoch: 33
train loss is 0.076385
average val loss: 0.076945, accuracy: 0.0732
average test loss: 0.075434, accuracy: 0.0750
case acc: 0.064436965
case acc: 0.045362376
case acc: 0.07832604
case acc: 0.08144734
case acc: 0.122081965
case acc: 0.058611188
top acc: 0.0656 ::: bot acc: 0.0993
top acc: 0.0612 ::: bot acc: 0.0583
top acc: 0.0307 ::: bot acc: 0.1336
top acc: 0.1493 ::: bot acc: 0.0574
top acc: 0.1274 ::: bot acc: 0.1504
top acc: 0.1013 ::: bot acc: 0.0525
current epoch: 34
train loss is 0.076297
average val loss: 0.076843, accuracy: 0.0733
average test loss: 0.075351, accuracy: 0.0749
case acc: 0.06437971
case acc: 0.04513437
case acc: 0.07842213
case acc: 0.081343286
case acc: 0.12177807
case acc: 0.058600727
top acc: 0.0659 ::: bot acc: 0.0990
top acc: 0.0624 ::: bot acc: 0.0571
top acc: 0.0306 ::: bot acc: 0.1337
top acc: 0.1489 ::: bot acc: 0.0578
top acc: 0.1281 ::: bot acc: 0.1496
top acc: 0.1014 ::: bot acc: 0.0525
current epoch: 35
train loss is 0.076237
average val loss: 0.076681, accuracy: 0.0732
average test loss: 0.075230, accuracy: 0.0748
case acc: 0.06430914
case acc: 0.044881582
case acc: 0.078215964
case acc: 0.0813601
case acc: 0.12137243
case acc: 0.058622483
top acc: 0.0662 ::: bot acc: 0.0987
top acc: 0.0637 ::: bot acc: 0.0559
top acc: 0.0307 ::: bot acc: 0.1334
top acc: 0.1489 ::: bot acc: 0.0577
top acc: 0.1289 ::: bot acc: 0.1486
top acc: 0.1016 ::: bot acc: 0.0522
current epoch: 36
train loss is 0.076164
average val loss: 0.076498, accuracy: 0.0730
average test loss: 0.075097, accuracy: 0.0746
case acc: 0.06425686
case acc: 0.04462958
case acc: 0.077870235
case acc: 0.08144067
case acc: 0.12096166
case acc: 0.05866045
top acc: 0.0665 ::: bot acc: 0.0984
top acc: 0.0652 ::: bot acc: 0.0544
top acc: 0.0307 ::: bot acc: 0.1329
top acc: 0.1491 ::: bot acc: 0.0575
top acc: 0.1298 ::: bot acc: 0.1475
top acc: 0.1019 ::: bot acc: 0.0519
current epoch: 37
train loss is 0.076112
average val loss: 0.076473, accuracy: 0.0730
average test loss: 0.075072, accuracy: 0.0746
case acc: 0.064328074
case acc: 0.04449548
case acc: 0.077883594
case acc: 0.081367716
case acc: 0.12082907
case acc: 0.05862054
top acc: 0.0661 ::: bot acc: 0.0988
top acc: 0.0662 ::: bot acc: 0.0534
top acc: 0.0307 ::: bot acc: 0.1329
top acc: 0.1489 ::: bot acc: 0.0577
top acc: 0.1301 ::: bot acc: 0.1471
top acc: 0.1015 ::: bot acc: 0.0523
current epoch: 38
train loss is 0.076080
average val loss: 0.076366, accuracy: 0.0729
average test loss: 0.074993, accuracy: 0.0745
case acc: 0.064339414
case acc: 0.04436524
case acc: 0.077665456
case acc: 0.08139488
case acc: 0.1205835
case acc: 0.05862755
top acc: 0.0661 ::: bot acc: 0.0988
top acc: 0.0673 ::: bot acc: 0.0522
top acc: 0.0308 ::: bot acc: 0.1325
top acc: 0.1490 ::: bot acc: 0.0576
top acc: 0.1307 ::: bot acc: 0.1465
top acc: 0.1016 ::: bot acc: 0.0522
current epoch: 39
train loss is 0.076081
average val loss: 0.076271, accuracy: 0.0728
average test loss: 0.074924, accuracy: 0.0744
case acc: 0.06437431
case acc: 0.044274203
case acc: 0.0773724
case acc: 0.08145351
case acc: 0.1203441
case acc: 0.05863614
top acc: 0.0659 ::: bot acc: 0.0990
top acc: 0.0682 ::: bot acc: 0.0513
top acc: 0.0309 ::: bot acc: 0.1320
top acc: 0.1493 ::: bot acc: 0.0573
top acc: 0.1312 ::: bot acc: 0.1458
top acc: 0.1016 ::: bot acc: 0.0522
current epoch: 40
train loss is 0.076115
average val loss: 0.075906, accuracy: 0.0723
average test loss: 0.074679, accuracy: 0.0742
case acc: 0.0642157
case acc: 0.04413348
case acc: 0.07628675
case acc: 0.08188808
case acc: 0.11966369
case acc: 0.058824178
top acc: 0.0668 ::: bot acc: 0.0981
top acc: 0.0701 ::: bot acc: 0.0494
top acc: 0.0312 ::: bot acc: 0.1303
top acc: 0.1509 ::: bot acc: 0.0557
top acc: 0.1328 ::: bot acc: 0.1440
top acc: 0.1028 ::: bot acc: 0.0510
current epoch: 41
train loss is 0.076041
average val loss: 0.075511, accuracy: 0.0717
average test loss: 0.074426, accuracy: 0.0739
case acc: 0.06404391
case acc: 0.044042002
case acc: 0.0749749
case acc: 0.08257391
case acc: 0.11886443
case acc: 0.059087407
top acc: 0.0677 ::: bot acc: 0.0971
top acc: 0.0719 ::: bot acc: 0.0476
top acc: 0.0316 ::: bot acc: 0.1281
top acc: 0.1529 ::: bot acc: 0.0539
top acc: 0.1345 ::: bot acc: 0.1420
top acc: 0.1043 ::: bot acc: 0.0495
current epoch: 42
train loss is 0.076089
average val loss: 0.074833, accuracy: 0.0708
average test loss: 0.074017, accuracy: 0.0736
case acc: 0.063694455
case acc: 0.043922137
case acc: 0.07263921
case acc: 0.0839313
case acc: 0.11742368
case acc: 0.05969004
top acc: 0.0700 ::: bot acc: 0.0948
top acc: 0.0748 ::: bot acc: 0.0448
top acc: 0.0324 ::: bot acc: 0.1242
top acc: 0.1565 ::: bot acc: 0.0507
top acc: 0.1378 ::: bot acc: 0.1382
top acc: 0.1073 ::: bot acc: 0.0466
current epoch: 43
train loss is 0.076127
average val loss: 0.074682, accuracy: 0.0704
average test loss: 0.073931, accuracy: 0.0734
case acc: 0.06373641
case acc: 0.04390673
case acc: 0.071652375
case acc: 0.08448587
case acc: 0.11684579
case acc: 0.059792988
top acc: 0.0696 ::: bot acc: 0.0952
top acc: 0.0748 ::: bot acc: 0.0448
top acc: 0.0328 ::: bot acc: 0.1225
top acc: 0.1578 ::: bot acc: 0.0496
top acc: 0.1390 ::: bot acc: 0.1367
top acc: 0.1077 ::: bot acc: 0.0461
current epoch: 44
train loss is 0.076270
average val loss: 0.074256, accuracy: 0.0697
average test loss: 0.073702, accuracy: 0.0731
case acc: 0.063515924
case acc: 0.043865412
case acc: 0.069771595
case acc: 0.08574952
case acc: 0.11560999
case acc: 0.06030504
top acc: 0.0709 ::: bot acc: 0.0939
top acc: 0.0758 ::: bot acc: 0.0437
top acc: 0.0340 ::: bot acc: 0.1190
top acc: 0.1609 ::: bot acc: 0.0470
top acc: 0.1420 ::: bot acc: 0.1334
top acc: 0.1099 ::: bot acc: 0.0441
current epoch: 45
train loss is 0.076437
average val loss: 0.073872, accuracy: 0.0689
average test loss: 0.073528, accuracy: 0.0729
case acc: 0.063311376
case acc: 0.04385322
case acc: 0.06778306
case acc: 0.08717569
case acc: 0.11429441
case acc: 0.060881667
top acc: 0.0722 ::: bot acc: 0.0927
top acc: 0.0762 ::: bot acc: 0.0433
top acc: 0.0355 ::: bot acc: 0.1153
top acc: 0.1644 ::: bot acc: 0.0443
top acc: 0.1452 ::: bot acc: 0.1298
top acc: 0.1121 ::: bot acc: 0.0418
current epoch: 46
train loss is 0.076558
average val loss: 0.073837, accuracy: 0.0685
average test loss: 0.073533, accuracy: 0.0728
case acc: 0.06342996
case acc: 0.04389386
case acc: 0.066775925
case acc: 0.087941624
case acc: 0.11365199
case acc: 0.060971253
top acc: 0.0714 ::: bot acc: 0.0934
top acc: 0.0740 ::: bot acc: 0.0456
top acc: 0.0363 ::: bot acc: 0.1134
top acc: 0.1662 ::: bot acc: 0.0433
top acc: 0.1469 ::: bot acc: 0.1280
top acc: 0.1125 ::: bot acc: 0.0415
current epoch: 47
train loss is 0.076721
average val loss: 0.074101, accuracy: 0.0684
average test loss: 0.073677, accuracy: 0.0728
case acc: 0.06380283
case acc: 0.044127956
case acc: 0.06670346
case acc: 0.08791148
case acc: 0.113593444
case acc: 0.06064735
top acc: 0.0689 ::: bot acc: 0.0959
top acc: 0.0695 ::: bot acc: 0.0500
top acc: 0.0364 ::: bot acc: 0.1133
top acc: 0.1661 ::: bot acc: 0.0433
top acc: 0.1470 ::: bot acc: 0.1278
top acc: 0.1111 ::: bot acc: 0.0429
current epoch: 48
train loss is 0.077124
average val loss: 0.074339, accuracy: 0.0684
average test loss: 0.073832, accuracy: 0.0729
case acc: 0.06409194
case acc: 0.044663392
case acc: 0.06649166
case acc: 0.08805065
case acc: 0.11334787
case acc: 0.060475133
top acc: 0.0671 ::: bot acc: 0.0977
top acc: 0.0649 ::: bot acc: 0.0547
top acc: 0.0366 ::: bot acc: 0.1129
top acc: 0.1664 ::: bot acc: 0.0431
top acc: 0.1478 ::: bot acc: 0.1271
top acc: 0.1104 ::: bot acc: 0.0436
current epoch: 49
train loss is 0.077812
average val loss: 0.074715, accuracy: 0.0687
average test loss: 0.074083, accuracy: 0.0731
case acc: 0.064418405
case acc: 0.04589205
case acc: 0.066620804
case acc: 0.087947905
case acc: 0.113214925
case acc: 0.060285646
top acc: 0.0654 ::: bot acc: 0.0995
top acc: 0.0587 ::: bot acc: 0.0608
top acc: 0.0365 ::: bot acc: 0.1131
top acc: 0.1662 ::: bot acc: 0.0432
top acc: 0.1482 ::: bot acc: 0.1267
top acc: 0.1095 ::: bot acc: 0.0445
current epoch: 50
train loss is 0.078510
average val loss: 0.076235, accuracy: 0.0705
average test loss: 0.075053, accuracy: 0.0741
case acc: 0.06581655
case acc: 0.04958712
case acc: 0.06963009
case acc: 0.085656084
case acc: 0.11487856
case acc: 0.059120614
top acc: 0.0590 ::: bot acc: 0.1059
top acc: 0.0469 ::: bot acc: 0.0729
top acc: 0.0342 ::: bot acc: 0.1188
top acc: 0.1608 ::: bot acc: 0.0472
top acc: 0.1439 ::: bot acc: 0.1313
top acc: 0.1040 ::: bot acc: 0.0500
LME_Co_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5142 5142 5142
1.7082474 -0.6288155 0.48738334 -0.25570297
Validation: 576 576 576
Testing: 750 750 750
pre-processing time: 0.0004839897155761719
the split date is 2012-01-01
net initializing with time: 0.0037817955017089844
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.304478
average val loss: 0.157194, accuracy: 0.1543
average test loss: 0.299543, accuracy: 0.2995
case acc: 0.29288647
case acc: 0.21161656
case acc: 0.42121902
case acc: 0.2951225
case acc: 0.382695
case acc: 0.19371648
top acc: 0.3479 ::: bot acc: 0.2379
top acc: 0.2621 ::: bot acc: 0.1568
top acc: 0.5108 ::: bot acc: 0.3307
top acc: 0.3899 ::: bot acc: 0.2101
top acc: 0.4456 ::: bot acc: 0.3187
top acc: 0.2644 ::: bot acc: 0.1184
current epoch: 2
train loss is 0.244934
average val loss: 0.232521, accuracy: 0.2322
average test loss: 0.109904, accuracy: 0.1143
case acc: 0.09988389
case acc: 0.17568003
case acc: 0.07072097
case acc: 0.10862864
case acc: 0.04561348
case acc: 0.1850304
top acc: 0.0457 ::: bot acc: 0.1543
top acc: 0.1250 ::: bot acc: 0.2313
top acc: 0.1194 ::: bot acc: 0.0616
top acc: 0.0418 ::: bot acc: 0.1802
top acc: 0.0677 ::: bot acc: 0.0593
top acc: 0.1138 ::: bot acc: 0.2609
current epoch: 3
train loss is 0.116216
average val loss: 0.073897, accuracy: 0.0534
average test loss: 0.175334, accuracy: 0.1752
case acc: 0.16310753
case acc: 0.08618476
case acc: 0.2898779
case acc: 0.16175717
case acc: 0.26356724
case acc: 0.08684897
top acc: 0.2178 ::: bot acc: 0.1083
top acc: 0.1370 ::: bot acc: 0.0303
top acc: 0.3802 ::: bot acc: 0.1991
top acc: 0.2548 ::: bot acc: 0.0775
top acc: 0.3274 ::: bot acc: 0.2000
top acc: 0.1528 ::: bot acc: 0.0218
current epoch: 4
train loss is 0.113842
average val loss: 0.088859, accuracy: 0.0757
average test loss: 0.099811, accuracy: 0.1017
case acc: 0.0734528
case acc: 0.040475335
case acc: 0.19147167
case acc: 0.07907433
case acc: 0.17225002
case acc: 0.053701937
top acc: 0.1211 ::: bot acc: 0.0327
top acc: 0.0449 ::: bot acc: 0.0624
top acc: 0.2817 ::: bot acc: 0.1012
top acc: 0.1587 ::: bot acc: 0.0209
top acc: 0.2363 ::: bot acc: 0.1088
top acc: 0.0626 ::: bot acc: 0.0847
current epoch: 5
train loss is 0.096007
average val loss: 0.094974, accuracy: 0.0839
average test loss: 0.092314, accuracy: 0.0945
case acc: 0.061443165
case acc: 0.04133867
case acc: 0.17368168
case acc: 0.072674684
case acc: 0.16357867
case acc: 0.054178957
top acc: 0.1051 ::: bot acc: 0.0288
top acc: 0.0338 ::: bot acc: 0.0737
top acc: 0.2634 ::: bot acc: 0.0847
top acc: 0.1432 ::: bot acc: 0.0323
top acc: 0.2279 ::: bot acc: 0.1002
top acc: 0.0562 ::: bot acc: 0.0911
current epoch: 6
train loss is 0.086320
average val loss: 0.077698, accuracy: 0.0630
average test loss: 0.109886, accuracy: 0.1109
case acc: 0.08361941
case acc: 0.043386675
case acc: 0.19812241
case acc: 0.086016916
case acc: 0.19630508
case acc: 0.058146313
top acc: 0.1336 ::: bot acc: 0.0378
top acc: 0.0669 ::: bot acc: 0.0406
top acc: 0.2887 ::: bot acc: 0.1076
top acc: 0.1710 ::: bot acc: 0.0160
top acc: 0.2607 ::: bot acc: 0.1330
top acc: 0.0926 ::: bot acc: 0.0549
current epoch: 7
train loss is 0.086208
average val loss: 0.074806, accuracy: 0.0599
average test loss: 0.112453, accuracy: 0.1131
case acc: 0.085220546
case acc: 0.0451364
case acc: 0.19700663
case acc: 0.08784196
case acc: 0.2027439
case acc: 0.060389988
top acc: 0.1355 ::: bot acc: 0.0387
top acc: 0.0738 ::: bot acc: 0.0338
top acc: 0.2877 ::: bot acc: 0.1065
top acc: 0.1737 ::: bot acc: 0.0161
top acc: 0.2670 ::: bot acc: 0.1397
top acc: 0.1016 ::: bot acc: 0.0456
current epoch: 8
train loss is 0.086441
average val loss: 0.078754, accuracy: 0.0658
average test loss: 0.104126, accuracy: 0.1050
case acc: 0.07376698
case acc: 0.04276034
case acc: 0.18074243
case acc: 0.08018544
case acc: 0.19406645
case acc: 0.058485497
top acc: 0.1213 ::: bot acc: 0.0328
top acc: 0.0646 ::: bot acc: 0.0427
top acc: 0.2710 ::: bot acc: 0.0913
top acc: 0.1609 ::: bot acc: 0.0188
top acc: 0.2582 ::: bot acc: 0.1313
top acc: 0.0950 ::: bot acc: 0.0520
current epoch: 9
train loss is 0.084340
average val loss: 0.080691, accuracy: 0.0690
average test loss: 0.100142, accuracy: 0.1010
case acc: 0.06763764
case acc: 0.042006854
case acc: 0.17046243
case acc: 0.07693952
case acc: 0.19092706
case acc: 0.058203768
top acc: 0.1133 ::: bot acc: 0.0304
top acc: 0.0612 ::: bot acc: 0.0458
top acc: 0.2605 ::: bot acc: 0.0816
top acc: 0.1541 ::: bot acc: 0.0231
top acc: 0.2548 ::: bot acc: 0.1283
top acc: 0.0942 ::: bot acc: 0.0525
current epoch: 10
train loss is 0.082420
average val loss: 0.078964, accuracy: 0.0674
average test loss: 0.101120, accuracy: 0.1017
case acc: 0.06776546
case acc: 0.042805776
case acc: 0.16795157
case acc: 0.077265725
case acc: 0.19439149
case acc: 0.05979088
top acc: 0.1135 ::: bot acc: 0.0303
top acc: 0.0654 ::: bot acc: 0.0413
top acc: 0.2581 ::: bot acc: 0.0791
top acc: 0.1550 ::: bot acc: 0.0228
top acc: 0.2581 ::: bot acc: 0.1319
top acc: 0.1004 ::: bot acc: 0.0460
current epoch: 11
train loss is 0.081366
average val loss: 0.078115, accuracy: 0.0669
average test loss: 0.101183, accuracy: 0.1015
case acc: 0.067081
case acc: 0.04347592
case acc: 0.16448784
case acc: 0.07691955
case acc: 0.19569957
case acc: 0.06111278
top acc: 0.1126 ::: bot acc: 0.0300
top acc: 0.0683 ::: bot acc: 0.0384
top acc: 0.2546 ::: bot acc: 0.0760
top acc: 0.1542 ::: bot acc: 0.0232
top acc: 0.2593 ::: bot acc: 0.1333
top acc: 0.1047 ::: bot acc: 0.0417
current epoch: 12
train loss is 0.080783
average val loss: 0.079463, accuracy: 0.0692
average test loss: 0.098690, accuracy: 0.0989
case acc: 0.06360902
case acc: 0.043256357
case acc: 0.15762134
case acc: 0.07505536
case acc: 0.19274943
case acc: 0.061110675
top acc: 0.1080 ::: bot acc: 0.0288
top acc: 0.0672 ::: bot acc: 0.0396
top acc: 0.2474 ::: bot acc: 0.0699
top acc: 0.1499 ::: bot acc: 0.0265
top acc: 0.2564 ::: bot acc: 0.1304
top acc: 0.1047 ::: bot acc: 0.0417
current epoch: 13
train loss is 0.080415
average val loss: 0.079617, accuracy: 0.0699
average test loss: 0.097829, accuracy: 0.0978
case acc: 0.06206546
case acc: 0.043455485
case acc: 0.15340678
case acc: 0.074475504
case acc: 0.19190945
case acc: 0.061746508
top acc: 0.1060 ::: bot acc: 0.0281
top acc: 0.0683 ::: bot acc: 0.0382
top acc: 0.2430 ::: bot acc: 0.0660
top acc: 0.1484 ::: bot acc: 0.0280
top acc: 0.2553 ::: bot acc: 0.1297
top acc: 0.1066 ::: bot acc: 0.0397
current epoch: 14
train loss is 0.079895
average val loss: 0.080795, accuracy: 0.0719
average test loss: 0.095948, accuracy: 0.0958
case acc: 0.05944353
case acc: 0.04313034
case acc: 0.14794154
case acc: 0.07354942
case acc: 0.18908268
case acc: 0.061544403
top acc: 0.1026 ::: bot acc: 0.0271
top acc: 0.0678 ::: bot acc: 0.0382
top acc: 0.2374 ::: bot acc: 0.0610
top acc: 0.1457 ::: bot acc: 0.0311
top acc: 0.2521 ::: bot acc: 0.1272
top acc: 0.1064 ::: bot acc: 0.0394
current epoch: 15
train loss is 0.079625
average val loss: 0.078465, accuracy: 0.0694
average test loss: 0.098012, accuracy: 0.0975
case acc: 0.061700925
case acc: 0.044679895
case acc: 0.14869195
case acc: 0.074647486
case acc: 0.19171922
case acc: 0.06368762
top acc: 0.1056 ::: bot acc: 0.0277
top acc: 0.0734 ::: bot acc: 0.0328
top acc: 0.2383 ::: bot acc: 0.0617
top acc: 0.1488 ::: bot acc: 0.0281
top acc: 0.2547 ::: bot acc: 0.1300
top acc: 0.1119 ::: bot acc: 0.0345
current epoch: 16
train loss is 0.079247
average val loss: 0.077590, accuracy: 0.0687
average test loss: 0.098545, accuracy: 0.0979
case acc: 0.062246323
case acc: 0.045644637
case acc: 0.14749207
case acc: 0.07488395
case acc: 0.19180842
case acc: 0.06507847
top acc: 0.1063 ::: bot acc: 0.0280
top acc: 0.0764 ::: bot acc: 0.0299
top acc: 0.2370 ::: bot acc: 0.0606
top acc: 0.1496 ::: bot acc: 0.0271
top acc: 0.2549 ::: bot acc: 0.1300
top acc: 0.1149 ::: bot acc: 0.0327
current epoch: 17
train loss is 0.078702
average val loss: 0.077453, accuracy: 0.0689
average test loss: 0.098238, accuracy: 0.0974
case acc: 0.061871752
case acc: 0.04624832
case acc: 0.14534448
case acc: 0.074779145
case acc: 0.19054534
case acc: 0.065699056
top acc: 0.1058 ::: bot acc: 0.0279
top acc: 0.0780 ::: bot acc: 0.0285
top acc: 0.2348 ::: bot acc: 0.0587
top acc: 0.1493 ::: bot acc: 0.0271
top acc: 0.2538 ::: bot acc: 0.1287
top acc: 0.1163 ::: bot acc: 0.0318
current epoch: 18
train loss is 0.078606
average val loss: 0.077250, accuracy: 0.0690
average test loss: 0.098053, accuracy: 0.0971
case acc: 0.061684463
case acc: 0.04688312
case acc: 0.14358293
case acc: 0.074854866
case acc: 0.18925518
case acc: 0.066295534
top acc: 0.1055 ::: bot acc: 0.0279
top acc: 0.0797 ::: bot acc: 0.0270
top acc: 0.2329 ::: bot acc: 0.0572
top acc: 0.1495 ::: bot acc: 0.0269
top acc: 0.2525 ::: bot acc: 0.1275
top acc: 0.1175 ::: bot acc: 0.0311
current epoch: 19
train loss is 0.078426
average val loss: 0.074474, accuracy: 0.0658
average test loss: 0.101104, accuracy: 0.0999
case acc: 0.06508695
case acc: 0.049310997
case acc: 0.14657314
case acc: 0.07692102
case acc: 0.19234128
case acc: 0.06901497
top acc: 0.1099 ::: bot acc: 0.0290
top acc: 0.0860 ::: bot acc: 0.0216
top acc: 0.2361 ::: bot acc: 0.0598
top acc: 0.1545 ::: bot acc: 0.0231
top acc: 0.2555 ::: bot acc: 0.1307
top acc: 0.1231 ::: bot acc: 0.0279
current epoch: 20
train loss is 0.078136
average val loss: 0.074007, accuracy: 0.0656
average test loss: 0.101322, accuracy: 0.1000
case acc: 0.06538794
case acc: 0.050137665
case acc: 0.14591455
case acc: 0.077391066
case acc: 0.19124874
case acc: 0.06966517
top acc: 0.1103 ::: bot acc: 0.0290
top acc: 0.0879 ::: bot acc: 0.0201
top acc: 0.2356 ::: bot acc: 0.0592
top acc: 0.1554 ::: bot acc: 0.0225
top acc: 0.2543 ::: bot acc: 0.1297
top acc: 0.1243 ::: bot acc: 0.0273
current epoch: 21
train loss is 0.077937
average val loss: 0.073821, accuracy: 0.0656
average test loss: 0.101229, accuracy: 0.0997
case acc: 0.065261774
case acc: 0.050662346
case acc: 0.14505333
case acc: 0.07774872
case acc: 0.18978132
case acc: 0.0699769
top acc: 0.1101 ::: bot acc: 0.0289
top acc: 0.0893 ::: bot acc: 0.0190
top acc: 0.2348 ::: bot acc: 0.0584
top acc: 0.1561 ::: bot acc: 0.0221
top acc: 0.2527 ::: bot acc: 0.1284
top acc: 0.1250 ::: bot acc: 0.0269
current epoch: 22
train loss is 0.077648
average val loss: 0.073191, accuracy: 0.0651
average test loss: 0.101752, accuracy: 0.1001
case acc: 0.06576217
case acc: 0.051547978
case acc: 0.14506142
case acc: 0.078476176
case acc: 0.18930553
case acc: 0.07072615
top acc: 0.1108 ::: bot acc: 0.0291
top acc: 0.0913 ::: bot acc: 0.0176
top acc: 0.2348 ::: bot acc: 0.0584
top acc: 0.1576 ::: bot acc: 0.0212
top acc: 0.2522 ::: bot acc: 0.1280
top acc: 0.1264 ::: bot acc: 0.0263
current epoch: 23
train loss is 0.077590
average val loss: 0.071238, accuracy: 0.0629
average test loss: 0.104197, accuracy: 0.1024
case acc: 0.068160556
case acc: 0.05361358
case acc: 0.14759669
case acc: 0.08049344
case acc: 0.19159418
case acc: 0.07309079
top acc: 0.1139 ::: bot acc: 0.0300
top acc: 0.0957 ::: bot acc: 0.0149
top acc: 0.2374 ::: bot acc: 0.0607
top acc: 0.1617 ::: bot acc: 0.0189
top acc: 0.2545 ::: bot acc: 0.1303
top acc: 0.1305 ::: bot acc: 0.0251
current epoch: 24
train loss is 0.077389
average val loss: 0.071318, accuracy: 0.0633
average test loss: 0.103811, accuracy: 0.1020
case acc: 0.06753971
case acc: 0.05380957
case acc: 0.14657736
case acc: 0.08059065
case acc: 0.19014667
case acc: 0.073266976
top acc: 0.1131 ::: bot acc: 0.0297
top acc: 0.0961 ::: bot acc: 0.0147
top acc: 0.2363 ::: bot acc: 0.0598
top acc: 0.1619 ::: bot acc: 0.0187
top acc: 0.2530 ::: bot acc: 0.1289
top acc: 0.1308 ::: bot acc: 0.0250
current epoch: 25
train loss is 0.077105
average val loss: 0.071266, accuracy: 0.0635
average test loss: 0.103631, accuracy: 0.1018
case acc: 0.06707306
case acc: 0.054172862
case acc: 0.14584485
case acc: 0.08079944
case acc: 0.18906485
case acc: 0.0735769
top acc: 0.1124 ::: bot acc: 0.0296
top acc: 0.0967 ::: bot acc: 0.0145
top acc: 0.2356 ::: bot acc: 0.0592
top acc: 0.1624 ::: bot acc: 0.0184
top acc: 0.2519 ::: bot acc: 0.1279
top acc: 0.1312 ::: bot acc: 0.0249
current epoch: 26
train loss is 0.077021
average val loss: 0.071187, accuracy: 0.0636
average test loss: 0.103544, accuracy: 0.1016
case acc: 0.06668516
case acc: 0.05447596
case acc: 0.14532137
case acc: 0.081191964
case acc: 0.1880761
case acc: 0.07390666
top acc: 0.1119 ::: bot acc: 0.0294
top acc: 0.0973 ::: bot acc: 0.0143
top acc: 0.2351 ::: bot acc: 0.0586
top acc: 0.1632 ::: bot acc: 0.0181
top acc: 0.2508 ::: bot acc: 0.1270
top acc: 0.1318 ::: bot acc: 0.0249
current epoch: 27
train loss is 0.076807
average val loss: 0.071462, accuracy: 0.0641
average test loss: 0.102954, accuracy: 0.1010
case acc: 0.06579158
case acc: 0.054429725
case acc: 0.14410526
case acc: 0.081184074
case acc: 0.18643065
case acc: 0.07381834
top acc: 0.1108 ::: bot acc: 0.0290
top acc: 0.0972 ::: bot acc: 0.0142
top acc: 0.2338 ::: bot acc: 0.0575
top acc: 0.1632 ::: bot acc: 0.0182
top acc: 0.2490 ::: bot acc: 0.1254
top acc: 0.1316 ::: bot acc: 0.0249
current epoch: 28
train loss is 0.076800
average val loss: 0.070919, accuracy: 0.0637
average test loss: 0.103532, accuracy: 0.1015
case acc: 0.06614951
case acc: 0.055307463
case acc: 0.14446923
case acc: 0.081977695
case acc: 0.18628952
case acc: 0.074657224
top acc: 0.1113 ::: bot acc: 0.0290
top acc: 0.0987 ::: bot acc: 0.0138
top acc: 0.2343 ::: bot acc: 0.0577
top acc: 0.1649 ::: bot acc: 0.0174
top acc: 0.2489 ::: bot acc: 0.1253
top acc: 0.1330 ::: bot acc: 0.0246
current epoch: 29
train loss is 0.076533
average val loss: 0.070161, accuracy: 0.0630
average test loss: 0.104457, accuracy: 0.1023
case acc: 0.06686524
case acc: 0.056454495
case acc: 0.14535293
case acc: 0.08289252
case acc: 0.18666959
case acc: 0.07576098
top acc: 0.1122 ::: bot acc: 0.0293
top acc: 0.1006 ::: bot acc: 0.0135
top acc: 0.2352 ::: bot acc: 0.0585
top acc: 0.1666 ::: bot acc: 0.0164
top acc: 0.2492 ::: bot acc: 0.1258
top acc: 0.1348 ::: bot acc: 0.0242
current epoch: 30
train loss is 0.076534
average val loss: 0.070912, accuracy: 0.0641
average test loss: 0.103205, accuracy: 0.1011
case acc: 0.06532871
case acc: 0.05576371
case acc: 0.14356893
case acc: 0.08237657
case acc: 0.18422559
case acc: 0.07511262
top acc: 0.1102 ::: bot acc: 0.0287
top acc: 0.0994 ::: bot acc: 0.0137
top acc: 0.2334 ::: bot acc: 0.0569
top acc: 0.1657 ::: bot acc: 0.0169
top acc: 0.2468 ::: bot acc: 0.1234
top acc: 0.1337 ::: bot acc: 0.0245
current epoch: 31
train loss is 0.076224
average val loss: 0.069686, accuracy: 0.0628
average test loss: 0.104859, accuracy: 0.1027
case acc: 0.066721596
case acc: 0.057454165
case acc: 0.1454113
case acc: 0.08398511
case acc: 0.18564662
case acc: 0.07677689
top acc: 0.1120 ::: bot acc: 0.0292
top acc: 0.1021 ::: bot acc: 0.0134
top acc: 0.2353 ::: bot acc: 0.0585
top acc: 0.1687 ::: bot acc: 0.0158
top acc: 0.2481 ::: bot acc: 0.1248
top acc: 0.1363 ::: bot acc: 0.0240
current epoch: 32
train loss is 0.076296
average val loss: 0.070576, accuracy: 0.0640
average test loss: 0.103417, accuracy: 0.1012
case acc: 0.06492397
case acc: 0.05652462
case acc: 0.14340818
case acc: 0.08323991
case acc: 0.18307978
case acc: 0.07597933
top acc: 0.1097 ::: bot acc: 0.0285
top acc: 0.1006 ::: bot acc: 0.0136
top acc: 0.2332 ::: bot acc: 0.0567
top acc: 0.1674 ::: bot acc: 0.0163
top acc: 0.2455 ::: bot acc: 0.1222
top acc: 0.1350 ::: bot acc: 0.0243
current epoch: 33
train loss is 0.075894
average val loss: 0.070619, accuracy: 0.0642
average test loss: 0.103236, accuracy: 0.1010
case acc: 0.064453594
case acc: 0.05662606
case acc: 0.14299779
case acc: 0.08330413
case acc: 0.1821518
case acc: 0.076212995
top acc: 0.1090 ::: bot acc: 0.0284
top acc: 0.1007 ::: bot acc: 0.0135
top acc: 0.2328 ::: bot acc: 0.0564
top acc: 0.1674 ::: bot acc: 0.0161
top acc: 0.2446 ::: bot acc: 0.1214
top acc: 0.1354 ::: bot acc: 0.0241
current epoch: 34
train loss is 0.075978
average val loss: 0.069366, accuracy: 0.0630
average test loss: 0.104982, accuracy: 0.1026
case acc: 0.065895446
case acc: 0.058274016
case acc: 0.14507246
case acc: 0.084902376
case acc: 0.18377581
case acc: 0.07794425
top acc: 0.1109 ::: bot acc: 0.0288
top acc: 0.1033 ::: bot acc: 0.0132
top acc: 0.2350 ::: bot acc: 0.0583
top acc: 0.1701 ::: bot acc: 0.0154
top acc: 0.2462 ::: bot acc: 0.1231
top acc: 0.1383 ::: bot acc: 0.0234
current epoch: 35
train loss is 0.075843
average val loss: 0.068557, accuracy: 0.0622
average test loss: 0.106126, accuracy: 0.1038
case acc: 0.06670835
case acc: 0.059470527
case acc: 0.14631416
case acc: 0.0860855
case acc: 0.18467304
case acc: 0.079250544
top acc: 0.1119 ::: bot acc: 0.0292
top acc: 0.1052 ::: bot acc: 0.0131
top acc: 0.2363 ::: bot acc: 0.0594
top acc: 0.1719 ::: bot acc: 0.0152
top acc: 0.2470 ::: bot acc: 0.1240
top acc: 0.1404 ::: bot acc: 0.0231
current epoch: 36
train loss is 0.075602
average val loss: 0.068321, accuracy: 0.0621
average test loss: 0.106420, accuracy: 0.1040
case acc: 0.066622585
case acc: 0.05992436
case acc: 0.14653538
case acc: 0.086620584
case acc: 0.18451925
case acc: 0.07990859
top acc: 0.1118 ::: bot acc: 0.0291
top acc: 0.1058 ::: bot acc: 0.0132
top acc: 0.2366 ::: bot acc: 0.0596
top acc: 0.1728 ::: bot acc: 0.0151
top acc: 0.2468 ::: bot acc: 0.1239
top acc: 0.1413 ::: bot acc: 0.0231
current epoch: 37
train loss is 0.075683
average val loss: 0.069532, accuracy: 0.0636
average test loss: 0.104455, accuracy: 0.1020
case acc: 0.064323395
case acc: 0.058480155
case acc: 0.1439313
case acc: 0.085218355
case acc: 0.18147653
case acc: 0.07874442
top acc: 0.1087 ::: bot acc: 0.0283
top acc: 0.1035 ::: bot acc: 0.0134
top acc: 0.2338 ::: bot acc: 0.0572
top acc: 0.1706 ::: bot acc: 0.0153
top acc: 0.2438 ::: bot acc: 0.1209
top acc: 0.1394 ::: bot acc: 0.0234
current epoch: 38
train loss is 0.075440
average val loss: 0.070222, accuracy: 0.0644
average test loss: 0.103357, accuracy: 0.1009
case acc: 0.06297249
case acc: 0.0577658
case acc: 0.14243414
case acc: 0.084458105
case acc: 0.17939416
case acc: 0.07821698
top acc: 0.1069 ::: bot acc: 0.0279
top acc: 0.1024 ::: bot acc: 0.0135
top acc: 0.2323 ::: bot acc: 0.0559
top acc: 0.1693 ::: bot acc: 0.0155
top acc: 0.2417 ::: bot acc: 0.1189
top acc: 0.1386 ::: bot acc: 0.0235
current epoch: 39
train loss is 0.075538
average val loss: 0.069518, accuracy: 0.0638
average test loss: 0.104348, accuracy: 0.1018
case acc: 0.06381813
case acc: 0.05876658
case acc: 0.14367151
case acc: 0.08535037
case acc: 0.17989677
case acc: 0.07938761
top acc: 0.1079 ::: bot acc: 0.0282
top acc: 0.1040 ::: bot acc: 0.0133
top acc: 0.2336 ::: bot acc: 0.0571
top acc: 0.1706 ::: bot acc: 0.0152
top acc: 0.2422 ::: bot acc: 0.1195
top acc: 0.1405 ::: bot acc: 0.0231
current epoch: 40
train loss is 0.075594
average val loss: 0.068331, accuracy: 0.0626
average test loss: 0.106125, accuracy: 0.1036
case acc: 0.065391056
case acc: 0.06058694
case acc: 0.14581908
case acc: 0.087019846
case acc: 0.1813981
case acc: 0.08117872
top acc: 0.1100 ::: bot acc: 0.0288
top acc: 0.1066 ::: bot acc: 0.0135
top acc: 0.2358 ::: bot acc: 0.0591
top acc: 0.1730 ::: bot acc: 0.0151
top acc: 0.2437 ::: bot acc: 0.1211
top acc: 0.1433 ::: bot acc: 0.0228
current epoch: 41
train loss is 0.075500
average val loss: 0.067342, accuracy: 0.0616
average test loss: 0.107689, accuracy: 0.1051
case acc: 0.06670335
case acc: 0.062249947
case acc: 0.14774916
case acc: 0.08871283
case acc: 0.18265563
case acc: 0.08277203
top acc: 0.1116 ::: bot acc: 0.0292
top acc: 0.1089 ::: bot acc: 0.0140
top acc: 0.2379 ::: bot acc: 0.0608
top acc: 0.1754 ::: bot acc: 0.0154
top acc: 0.2450 ::: bot acc: 0.1224
top acc: 0.1457 ::: bot acc: 0.0228
current epoch: 42
train loss is 0.075556
average val loss: 0.068453, accuracy: 0.0629
average test loss: 0.105812, accuracy: 0.1032
case acc: 0.064580895
case acc: 0.060612738
case acc: 0.1455087
case acc: 0.08733815
case acc: 0.17963424
case acc: 0.081351556
top acc: 0.1088 ::: bot acc: 0.0284
top acc: 0.1066 ::: bot acc: 0.0135
top acc: 0.2356 ::: bot acc: 0.0588
top acc: 0.1734 ::: bot acc: 0.0152
top acc: 0.2419 ::: bot acc: 0.1195
top acc: 0.1436 ::: bot acc: 0.0227
current epoch: 43
train loss is 0.075311
average val loss: 0.069865, accuracy: 0.0646
average test loss: 0.103620, accuracy: 0.1009
case acc: 0.062184755
case acc: 0.058825046
case acc: 0.14277677
case acc: 0.08570303
case acc: 0.17613634
case acc: 0.07976534
top acc: 0.1056 ::: bot acc: 0.0276
top acc: 0.1039 ::: bot acc: 0.0135
top acc: 0.2327 ::: bot acc: 0.0563
top acc: 0.1709 ::: bot acc: 0.0152
top acc: 0.2384 ::: bot acc: 0.1160
top acc: 0.1411 ::: bot acc: 0.0230
current epoch: 44
train loss is 0.075087
average val loss: 0.068532, accuracy: 0.0632
average test loss: 0.105627, accuracy: 0.1029
case acc: 0.064006954
case acc: 0.060762126
case acc: 0.14529918
case acc: 0.0876802
case acc: 0.17801361
case acc: 0.08173371
top acc: 0.1081 ::: bot acc: 0.0281
top acc: 0.1067 ::: bot acc: 0.0136
top acc: 0.2354 ::: bot acc: 0.0586
top acc: 0.1739 ::: bot acc: 0.0152
top acc: 0.2402 ::: bot acc: 0.1179
top acc: 0.1440 ::: bot acc: 0.0229
current epoch: 45
train loss is 0.075330
average val loss: 0.067944, accuracy: 0.0627
average test loss: 0.106531, accuracy: 0.1038
case acc: 0.064607196
case acc: 0.06174411
case acc: 0.1464152
case acc: 0.08868779
case acc: 0.17866002
case acc: 0.08284983
top acc: 0.1088 ::: bot acc: 0.0283
top acc: 0.1081 ::: bot acc: 0.0139
top acc: 0.2366 ::: bot acc: 0.0596
top acc: 0.1752 ::: bot acc: 0.0154
top acc: 0.2408 ::: bot acc: 0.1187
top acc: 0.1456 ::: bot acc: 0.0230
current epoch: 46
train loss is 0.075270
average val loss: 0.067643, accuracy: 0.0625
average test loss: 0.106991, accuracy: 0.1043
case acc: 0.064709105
case acc: 0.062289383
case acc: 0.14698167
case acc: 0.08938556
case acc: 0.17888759
case acc: 0.083580784
top acc: 0.1088 ::: bot acc: 0.0284
top acc: 0.1087 ::: bot acc: 0.0142
top acc: 0.2372 ::: bot acc: 0.0601
top acc: 0.1761 ::: bot acc: 0.0156
top acc: 0.2410 ::: bot acc: 0.1190
top acc: 0.1466 ::: bot acc: 0.0232
current epoch: 47
train loss is 0.075151
average val loss: 0.068227, accuracy: 0.0632
average test loss: 0.105984, accuracy: 0.1032
case acc: 0.0632919
case acc: 0.061363466
case acc: 0.14568207
case acc: 0.08868844
case acc: 0.17741846
case acc: 0.08299699
top acc: 0.1069 ::: bot acc: 0.0280
top acc: 0.1074 ::: bot acc: 0.0140
top acc: 0.2358 ::: bot acc: 0.0589
top acc: 0.1751 ::: bot acc: 0.0154
top acc: 0.2395 ::: bot acc: 0.1175
top acc: 0.1457 ::: bot acc: 0.0232
current epoch: 48
train loss is 0.075052
average val loss: 0.068854, accuracy: 0.0640
average test loss: 0.104945, accuracy: 0.1021
case acc: 0.061729804
case acc: 0.06044268
case acc: 0.1442014
case acc: 0.08799942
case acc: 0.17605087
case acc: 0.082421206
top acc: 0.1049 ::: bot acc: 0.0274
top acc: 0.1061 ::: bot acc: 0.0138
top acc: 0.2342 ::: bot acc: 0.0576
top acc: 0.1742 ::: bot acc: 0.0153
top acc: 0.2381 ::: bot acc: 0.1162
top acc: 0.1448 ::: bot acc: 0.0231
current epoch: 49
train loss is 0.075137
average val loss: 0.069137, accuracy: 0.0644
average test loss: 0.104448, accuracy: 0.1016
case acc: 0.06073012
case acc: 0.059994824
case acc: 0.14337717
case acc: 0.08774993
case acc: 0.17544821
case acc: 0.0823973
top acc: 0.1035 ::: bot acc: 0.0271
top acc: 0.1056 ::: bot acc: 0.0137
top acc: 0.2334 ::: bot acc: 0.0569
top acc: 0.1738 ::: bot acc: 0.0152
top acc: 0.2375 ::: bot acc: 0.1157
top acc: 0.1448 ::: bot acc: 0.0231
current epoch: 50
train loss is 0.074825
average val loss: 0.071358, accuracy: 0.0668
average test loss: 0.101235, accuracy: 0.0983
case acc: 0.05705577
case acc: 0.05729113
case acc: 0.13910311
case acc: 0.0850598
case acc: 0.17114334
case acc: 0.07989002
top acc: 0.0986 ::: bot acc: 0.0259
top acc: 0.1014 ::: bot acc: 0.0140
top acc: 0.2289 ::: bot acc: 0.0531
top acc: 0.1697 ::: bot acc: 0.0153
top acc: 0.2331 ::: bot acc: 0.1115
top acc: 0.1412 ::: bot acc: 0.0229
LME_Co_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5130 5130 5130
1.7082474 -0.6288155 0.48738334 -0.27422604
Validation: 570 570 570
Testing: 768 768 768
pre-processing time: 0.00029158592224121094
the split date is 2012-07-01
net initializing with time: 0.0036356449127197266
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.215200
average val loss: 0.239056, accuracy: 0.2391
average test loss: 0.249903, accuracy: 0.2499
case acc: 0.36516482
case acc: 0.17016381
case acc: 0.25087434
case acc: 0.19061083
case acc: 0.2825433
case acc: 0.23981386
top acc: 0.4834 ::: bot acc: 0.2526
top acc: 0.2155 ::: bot acc: 0.1285
top acc: 0.3456 ::: bot acc: 0.1769
top acc: 0.2818 ::: bot acc: 0.0765
top acc: 0.3599 ::: bot acc: 0.2058
top acc: 0.3197 ::: bot acc: 0.1436
current epoch: 2
train loss is 0.120605
average val loss: 0.142187, accuracy: 0.1419
average test loss: 0.154784, accuracy: 0.1524
case acc: 0.2562099
case acc: 0.07052751
case acc: 0.15288155
case acc: 0.10935068
case acc: 0.18671587
case acc: 0.13887072
top acc: 0.3754 ::: bot acc: 0.1430
top acc: 0.1162 ::: bot acc: 0.0282
top acc: 0.2482 ::: bot acc: 0.0794
top acc: 0.1823 ::: bot acc: 0.0317
top acc: 0.2641 ::: bot acc: 0.1100
top acc: 0.2158 ::: bot acc: 0.0489
current epoch: 3
train loss is 0.093863
average val loss: 0.107618, accuracy: 0.1079
average test loss: 0.120443, accuracy: 0.1159
case acc: 0.20531577
case acc: 0.038044438
case acc: 0.108438924
case acc: 0.09250889
case acc: 0.14544675
case acc: 0.10586445
top acc: 0.3250 ::: bot acc: 0.0918
top acc: 0.0732 ::: bot acc: 0.0173
top acc: 0.2034 ::: bot acc: 0.0363
top acc: 0.1392 ::: bot acc: 0.0667
top acc: 0.2230 ::: bot acc: 0.0686
top acc: 0.1700 ::: bot acc: 0.0421
current epoch: 4
train loss is 0.091739
average val loss: 0.118665, accuracy: 0.1190
average test loss: 0.131585, accuracy: 0.1279
case acc: 0.21738322
case acc: 0.048885867
case acc: 0.12439565
case acc: 0.09809141
case acc: 0.16269837
case acc: 0.115981445
top acc: 0.3375 ::: bot acc: 0.1037
top acc: 0.0914 ::: bot acc: 0.0131
top acc: 0.2202 ::: bot acc: 0.0511
top acc: 0.1567 ::: bot acc: 0.0495
top acc: 0.2403 ::: bot acc: 0.0858
top acc: 0.1854 ::: bot acc: 0.0414
current epoch: 5
train loss is 0.093542
average val loss: 0.136004, accuracy: 0.1362
average test loss: 0.148739, accuracy: 0.1462
case acc: 0.23565377
case acc: 0.069645286
case acc: 0.14819583
case acc: 0.10856062
case acc: 0.18489574
case acc: 0.13045573
top acc: 0.3561 ::: bot acc: 0.1218
top acc: 0.1156 ::: bot acc: 0.0270
top acc: 0.2444 ::: bot acc: 0.0747
top acc: 0.1800 ::: bot acc: 0.0339
top acc: 0.2626 ::: bot acc: 0.1078
top acc: 0.2061 ::: bot acc: 0.0433
current epoch: 6
train loss is 0.091128
average val loss: 0.129909, accuracy: 0.1303
average test loss: 0.142785, accuracy: 0.1400
case acc: 0.22411156
case acc: 0.06475881
case acc: 0.14290455
case acc: 0.1058971
case acc: 0.17824173
case acc: 0.12421371
top acc: 0.3447 ::: bot acc: 0.1102
top acc: 0.1106 ::: bot acc: 0.0224
top acc: 0.2393 ::: bot acc: 0.0693
top acc: 0.1740 ::: bot acc: 0.0379
top acc: 0.2561 ::: bot acc: 0.1011
top acc: 0.1976 ::: bot acc: 0.0419
current epoch: 7
train loss is 0.088417
average val loss: 0.122062, accuracy: 0.1228
average test loss: 0.135023, accuracy: 0.1318
case acc: 0.21008343
case acc: 0.05805331
case acc: 0.13426092
case acc: 0.102137536
case acc: 0.16934223
case acc: 0.11686349
top acc: 0.3307 ::: bot acc: 0.0962
top acc: 0.1032 ::: bot acc: 0.0171
top acc: 0.2307 ::: bot acc: 0.0606
top acc: 0.1657 ::: bot acc: 0.0434
top acc: 0.2472 ::: bot acc: 0.0922
top acc: 0.1869 ::: bot acc: 0.0414
current epoch: 8
train loss is 0.086772
average val loss: 0.118593, accuracy: 0.1195
average test loss: 0.131573, accuracy: 0.1281
case acc: 0.20192581
case acc: 0.05633018
case acc: 0.13038561
case acc: 0.10074512
case acc: 0.16571215
case acc: 0.1135228
top acc: 0.3224 ::: bot acc: 0.0882
top acc: 0.1011 ::: bot acc: 0.0159
top acc: 0.2269 ::: bot acc: 0.0565
top acc: 0.1628 ::: bot acc: 0.0451
top acc: 0.2436 ::: bot acc: 0.0885
top acc: 0.1819 ::: bot acc: 0.0415
current epoch: 9
train loss is 0.085484
average val loss: 0.114036, accuracy: 0.1154
average test loss: 0.127050, accuracy: 0.1232
case acc: 0.19272283
case acc: 0.053444237
case acc: 0.124649085
case acc: 0.09867287
case acc: 0.16021755
case acc: 0.10943529
top acc: 0.3131 ::: bot acc: 0.0790
top acc: 0.0976 ::: bot acc: 0.0144
top acc: 0.2212 ::: bot acc: 0.0509
top acc: 0.1582 ::: bot acc: 0.0480
top acc: 0.2381 ::: bot acc: 0.0830
top acc: 0.1756 ::: bot acc: 0.0420
current epoch: 10
train loss is 0.084669
average val loss: 0.112165, accuracy: 0.1138
average test loss: 0.125204, accuracy: 0.1212
case acc: 0.187577
case acc: 0.05340085
case acc: 0.12223481
case acc: 0.098184854
case acc: 0.15793273
case acc: 0.107669264
top acc: 0.3078 ::: bot acc: 0.0738
top acc: 0.0974 ::: bot acc: 0.0145
top acc: 0.2186 ::: bot acc: 0.0486
top acc: 0.1571 ::: bot acc: 0.0487
top acc: 0.2357 ::: bot acc: 0.0808
top acc: 0.1730 ::: bot acc: 0.0421
current epoch: 11
train loss is 0.084053
average val loss: 0.110003, accuracy: 0.1119
average test loss: 0.123069, accuracy: 0.1188
case acc: 0.18207464
case acc: 0.05304987
case acc: 0.11946131
case acc: 0.09766835
case acc: 0.15509361
case acc: 0.10561653
top acc: 0.3023 ::: bot acc: 0.0684
top acc: 0.0970 ::: bot acc: 0.0143
top acc: 0.2158 ::: bot acc: 0.0459
top acc: 0.1554 ::: bot acc: 0.0501
top acc: 0.2327 ::: bot acc: 0.0780
top acc: 0.1698 ::: bot acc: 0.0424
current epoch: 12
train loss is 0.082720
average val loss: 0.109860, accuracy: 0.1118
average test loss: 0.122990, accuracy: 0.1188
case acc: 0.17965066
case acc: 0.055024426
case acc: 0.119074374
case acc: 0.09827756
case acc: 0.1552288
case acc: 0.1055277
top acc: 0.3000 ::: bot acc: 0.0657
top acc: 0.0994 ::: bot acc: 0.0153
top acc: 0.2155 ::: bot acc: 0.0456
top acc: 0.1568 ::: bot acc: 0.0491
top acc: 0.2329 ::: bot acc: 0.0781
top acc: 0.1696 ::: bot acc: 0.0424
current epoch: 13
train loss is 0.082605
average val loss: 0.107874, accuracy: 0.1100
average test loss: 0.121068, accuracy: 0.1167
case acc: 0.1747715
case acc: 0.054893196
case acc: 0.1159884
case acc: 0.09775217
case acc: 0.15288198
case acc: 0.10391801
top acc: 0.2954 ::: bot acc: 0.0607
top acc: 0.0992 ::: bot acc: 0.0152
top acc: 0.2127 ::: bot acc: 0.0426
top acc: 0.1555 ::: bot acc: 0.0501
top acc: 0.2308 ::: bot acc: 0.0757
top acc: 0.1670 ::: bot acc: 0.0429
current epoch: 14
train loss is 0.081482
average val loss: 0.106117, accuracy: 0.1084
average test loss: 0.119359, accuracy: 0.1148
case acc: 0.1706644
case acc: 0.055031464
case acc: 0.1127356
case acc: 0.09741187
case acc: 0.15054864
case acc: 0.10267776
top acc: 0.2913 ::: bot acc: 0.0566
top acc: 0.0994 ::: bot acc: 0.0153
top acc: 0.2093 ::: bot acc: 0.0396
top acc: 0.1546 ::: bot acc: 0.0508
top acc: 0.2285 ::: bot acc: 0.0733
top acc: 0.1650 ::: bot acc: 0.0433
current epoch: 15
train loss is 0.081076
average val loss: 0.105540, accuracy: 0.1079
average test loss: 0.118824, accuracy: 0.1143
case acc: 0.16844723
case acc: 0.05640667
case acc: 0.11111775
case acc: 0.09762056
case acc: 0.14972663
case acc: 0.102376424
top acc: 0.2891 ::: bot acc: 0.0543
top acc: 0.1011 ::: bot acc: 0.0160
top acc: 0.2077 ::: bot acc: 0.0382
top acc: 0.1551 ::: bot acc: 0.0502
top acc: 0.2277 ::: bot acc: 0.0725
top acc: 0.1646 ::: bot acc: 0.0433
current epoch: 16
train loss is 0.080365
average val loss: 0.102946, accuracy: 0.1055
average test loss: 0.116230, accuracy: 0.1113
case acc: 0.16341424
case acc: 0.055120282
case acc: 0.1069438
case acc: 0.09665455
case acc: 0.14588378
case acc: 0.10006188
top acc: 0.2840 ::: bot acc: 0.0494
top acc: 0.0995 ::: bot acc: 0.0152
top acc: 0.2032 ::: bot acc: 0.0345
top acc: 0.1525 ::: bot acc: 0.0525
top acc: 0.2238 ::: bot acc: 0.0687
top acc: 0.1609 ::: bot acc: 0.0438
current epoch: 17
train loss is 0.079970
average val loss: 0.102339, accuracy: 0.1050
average test loss: 0.115686, accuracy: 0.1108
case acc: 0.16116318
case acc: 0.056352884
case acc: 0.10569911
case acc: 0.09681289
case acc: 0.14497982
case acc: 0.09969637
top acc: 0.2817 ::: bot acc: 0.0473
top acc: 0.1009 ::: bot acc: 0.0160
top acc: 0.2018 ::: bot acc: 0.0335
top acc: 0.1528 ::: bot acc: 0.0522
top acc: 0.2229 ::: bot acc: 0.0679
top acc: 0.1604 ::: bot acc: 0.0438
current epoch: 18
train loss is 0.079326
average val loss: 0.101431, accuracy: 0.1042
average test loss: 0.114819, accuracy: 0.1099
case acc: 0.15847425
case acc: 0.057150334
case acc: 0.10414033
case acc: 0.09679724
case acc: 0.14363986
case acc: 0.09900095
top acc: 0.2787 ::: bot acc: 0.0450
top acc: 0.1018 ::: bot acc: 0.0165
top acc: 0.2000 ::: bot acc: 0.0324
top acc: 0.1526 ::: bot acc: 0.0523
top acc: 0.2215 ::: bot acc: 0.0666
top acc: 0.1593 ::: bot acc: 0.0440
current epoch: 19
train loss is 0.079127
average val loss: 0.100654, accuracy: 0.1035
average test loss: 0.114082, accuracy: 0.1091
case acc: 0.15607491
case acc: 0.05820209
case acc: 0.102601975
case acc: 0.096861586
case acc: 0.14256907
case acc: 0.09849346
top acc: 0.2760 ::: bot acc: 0.0432
top acc: 0.1031 ::: bot acc: 0.0172
top acc: 0.1984 ::: bot acc: 0.0313
top acc: 0.1526 ::: bot acc: 0.0523
top acc: 0.2204 ::: bot acc: 0.0654
top acc: 0.1584 ::: bot acc: 0.0443
current epoch: 20
train loss is 0.078421
average val loss: 0.098338, accuracy: 0.1013
average test loss: 0.111803, accuracy: 0.1066
case acc: 0.15189064
case acc: 0.057126805
case acc: 0.09899685
case acc: 0.09612924
case acc: 0.1391071
case acc: 0.096609876
top acc: 0.2715 ::: bot acc: 0.0400
top acc: 0.1018 ::: bot acc: 0.0165
top acc: 0.1943 ::: bot acc: 0.0290
top acc: 0.1502 ::: bot acc: 0.0546
top acc: 0.2170 ::: bot acc: 0.0619
top acc: 0.1553 ::: bot acc: 0.0450
current epoch: 21
train loss is 0.078236
average val loss: 0.097711, accuracy: 0.1007
average test loss: 0.111226, accuracy: 0.1061
case acc: 0.15025756
case acc: 0.05815538
case acc: 0.09752086
case acc: 0.09618288
case acc: 0.13808545
case acc: 0.0962372
top acc: 0.2700 ::: bot acc: 0.0387
top acc: 0.1030 ::: bot acc: 0.0171
top acc: 0.1927 ::: bot acc: 0.0281
top acc: 0.1503 ::: bot acc: 0.0546
top acc: 0.2161 ::: bot acc: 0.0608
top acc: 0.1547 ::: bot acc: 0.0452
current epoch: 22
train loss is 0.078131
average val loss: 0.096900, accuracy: 0.0999
average test loss: 0.110463, accuracy: 0.1053
case acc: 0.14839727
case acc: 0.05887205
case acc: 0.09597923
case acc: 0.096122086
case acc: 0.13669106
case acc: 0.0956685
top acc: 0.2679 ::: bot acc: 0.0374
top acc: 0.1038 ::: bot acc: 0.0176
top acc: 0.1911 ::: bot acc: 0.0272
top acc: 0.1500 ::: bot acc: 0.0549
top acc: 0.2148 ::: bot acc: 0.0594
top acc: 0.1538 ::: bot acc: 0.0455
current epoch: 23
train loss is 0.077571
average val loss: 0.096493, accuracy: 0.0996
average test loss: 0.110084, accuracy: 0.1049
case acc: 0.14706288
case acc: 0.059926093
case acc: 0.09512297
case acc: 0.09614269
case acc: 0.1358642
case acc: 0.095354855
top acc: 0.2665 ::: bot acc: 0.0364
top acc: 0.1051 ::: bot acc: 0.0183
top acc: 0.1902 ::: bot acc: 0.0267
top acc: 0.1501 ::: bot acc: 0.0547
top acc: 0.2140 ::: bot acc: 0.0585
top acc: 0.1532 ::: bot acc: 0.0456
current epoch: 24
train loss is 0.077074
average val loss: 0.094810, accuracy: 0.0980
average test loss: 0.108439, accuracy: 0.1031
case acc: 0.14403662
case acc: 0.059303753
case acc: 0.09257215
case acc: 0.095608935
case acc: 0.13311756
case acc: 0.09396349
top acc: 0.2629 ::: bot acc: 0.0344
top acc: 0.1043 ::: bot acc: 0.0179
top acc: 0.1869 ::: bot acc: 0.0255
top acc: 0.1483 ::: bot acc: 0.0564
top acc: 0.2112 ::: bot acc: 0.0560
top acc: 0.1510 ::: bot acc: 0.0461
current epoch: 25
train loss is 0.076973
average val loss: 0.094536, accuracy: 0.0977
average test loss: 0.108202, accuracy: 0.1029
case acc: 0.14292645
case acc: 0.06055219
case acc: 0.0918828
case acc: 0.09573412
case acc: 0.13252613
case acc: 0.09383267
top acc: 0.2615 ::: bot acc: 0.0336
top acc: 0.1057 ::: bot acc: 0.0188
top acc: 0.1861 ::: bot acc: 0.0251
top acc: 0.1486 ::: bot acc: 0.0560
top acc: 0.2106 ::: bot acc: 0.0553
top acc: 0.1507 ::: bot acc: 0.0463
current epoch: 26
train loss is 0.076881
average val loss: 0.096790, accuracy: 0.1000
average test loss: 0.110480, accuracy: 0.1056
case acc: 0.14520809
case acc: 0.06533109
case acc: 0.09440598
case acc: 0.09708808
case acc: 0.13571602
case acc: 0.09590685
top acc: 0.2641 ::: bot acc: 0.0351
top acc: 0.1108 ::: bot acc: 0.0229
top acc: 0.1894 ::: bot acc: 0.0263
top acc: 0.1527 ::: bot acc: 0.0519
top acc: 0.2140 ::: bot acc: 0.0584
top acc: 0.1542 ::: bot acc: 0.0453
current epoch: 27
train loss is 0.076319
average val loss: 0.095396, accuracy: 0.0987
average test loss: 0.109117, accuracy: 0.1041
case acc: 0.142513
case acc: 0.06489755
case acc: 0.09246681
case acc: 0.096523374
case acc: 0.13353635
case acc: 0.09471848
top acc: 0.2608 ::: bot acc: 0.0335
top acc: 0.1102 ::: bot acc: 0.0225
top acc: 0.1868 ::: bot acc: 0.0255
top acc: 0.1511 ::: bot acc: 0.0532
top acc: 0.2116 ::: bot acc: 0.0564
top acc: 0.1523 ::: bot acc: 0.0458
current epoch: 28
train loss is 0.076343
average val loss: 0.093529, accuracy: 0.0969
average test loss: 0.107277, accuracy: 0.1020
case acc: 0.1393176
case acc: 0.06365189
case acc: 0.0898592
case acc: 0.09572922
case acc: 0.13051498
case acc: 0.09307916
top acc: 0.2569 ::: bot acc: 0.0315
top acc: 0.1088 ::: bot acc: 0.0215
top acc: 0.1836 ::: bot acc: 0.0242
top acc: 0.1488 ::: bot acc: 0.0553
top acc: 0.2086 ::: bot acc: 0.0535
top acc: 0.1495 ::: bot acc: 0.0465
current epoch: 29
train loss is 0.075909
average val loss: 0.094796, accuracy: 0.0982
average test loss: 0.108579, accuracy: 0.1036
case acc: 0.14031893
case acc: 0.06702846
case acc: 0.0911652
case acc: 0.09654799
case acc: 0.13226143
case acc: 0.09424772
top acc: 0.2580 ::: bot acc: 0.0321
top acc: 0.1123 ::: bot acc: 0.0247
top acc: 0.1853 ::: bot acc: 0.0247
top acc: 0.1513 ::: bot acc: 0.0528
top acc: 0.2104 ::: bot acc: 0.0552
top acc: 0.1516 ::: bot acc: 0.0459
current epoch: 30
train loss is 0.075712
average val loss: 0.094728, accuracy: 0.0982
average test loss: 0.108536, accuracy: 0.1036
case acc: 0.13954951
case acc: 0.06834748
case acc: 0.09091081
case acc: 0.096710816
case acc: 0.13194142
case acc: 0.09414253
top acc: 0.2572 ::: bot acc: 0.0317
top acc: 0.1136 ::: bot acc: 0.0259
top acc: 0.1851 ::: bot acc: 0.0245
top acc: 0.1516 ::: bot acc: 0.0524
top acc: 0.2102 ::: bot acc: 0.0548
top acc: 0.1514 ::: bot acc: 0.0460
current epoch: 31
train loss is 0.075533
average val loss: 0.095216, accuracy: 0.0988
average test loss: 0.109040, accuracy: 0.1042
case acc: 0.139456
case acc: 0.070472084
case acc: 0.091367066
case acc: 0.09709796
case acc: 0.13251948
case acc: 0.09457128
top acc: 0.2570 ::: bot acc: 0.0317
top acc: 0.1158 ::: bot acc: 0.0281
top acc: 0.1858 ::: bot acc: 0.0247
top acc: 0.1527 ::: bot acc: 0.0511
top acc: 0.2107 ::: bot acc: 0.0553
top acc: 0.1522 ::: bot acc: 0.0458
current epoch: 32
train loss is 0.075197
average val loss: 0.095000, accuracy: 0.0986
average test loss: 0.108848, accuracy: 0.1041
case acc: 0.13851584
case acc: 0.071493015
case acc: 0.09088555
case acc: 0.09714545
case acc: 0.13200769
case acc: 0.09434011
top acc: 0.2558 ::: bot acc: 0.0312
top acc: 0.1167 ::: bot acc: 0.0292
top acc: 0.1850 ::: bot acc: 0.0245
top acc: 0.1528 ::: bot acc: 0.0508
top acc: 0.2101 ::: bot acc: 0.0550
top acc: 0.1520 ::: bot acc: 0.0458
current epoch: 33
train loss is 0.075126
average val loss: 0.095420, accuracy: 0.0991
average test loss: 0.109283, accuracy: 0.1046
case acc: 0.13838007
case acc: 0.073442765
case acc: 0.0910763
case acc: 0.097576834
case acc: 0.1325913
case acc: 0.094726875
top acc: 0.2557 ::: bot acc: 0.0311
top acc: 0.1187 ::: bot acc: 0.0312
top acc: 0.1854 ::: bot acc: 0.0246
top acc: 0.1539 ::: bot acc: 0.0497
top acc: 0.2108 ::: bot acc: 0.0555
top acc: 0.1526 ::: bot acc: 0.0457
current epoch: 34
train loss is 0.075056
average val loss: 0.094215, accuracy: 0.0980
average test loss: 0.108099, accuracy: 0.1033
case acc: 0.13615966
case acc: 0.07288261
case acc: 0.089338005
case acc: 0.097048745
case acc: 0.13074316
case acc: 0.09366742
top acc: 0.2527 ::: bot acc: 0.0300
top acc: 0.1181 ::: bot acc: 0.0307
top acc: 0.1831 ::: bot acc: 0.0238
top acc: 0.1525 ::: bot acc: 0.0509
top acc: 0.2088 ::: bot acc: 0.0537
top acc: 0.1509 ::: bot acc: 0.0460
current epoch: 35
train loss is 0.075048
average val loss: 0.096557, accuracy: 0.1004
average test loss: 0.110431, accuracy: 0.1061
case acc: 0.13847023
case acc: 0.077649236
case acc: 0.09174417
case acc: 0.09859394
case acc: 0.13424936
case acc: 0.095828116
top acc: 0.2553 ::: bot acc: 0.0314
top acc: 0.1228 ::: bot acc: 0.0354
top acc: 0.1861 ::: bot acc: 0.0250
top acc: 0.1564 ::: bot acc: 0.0473
top acc: 0.2123 ::: bot acc: 0.0572
top acc: 0.1546 ::: bot acc: 0.0451
current epoch: 36
train loss is 0.074529
average val loss: 0.095957, accuracy: 0.0999
average test loss: 0.109847, accuracy: 0.1055
case acc: 0.13720411
case acc: 0.07790759
case acc: 0.09058894
case acc: 0.09840552
case acc: 0.13333155
case acc: 0.09533776
top acc: 0.2537 ::: bot acc: 0.0307
top acc: 0.1230 ::: bot acc: 0.0357
top acc: 0.1847 ::: bot acc: 0.0244
top acc: 0.1559 ::: bot acc: 0.0475
top acc: 0.2113 ::: bot acc: 0.0563
top acc: 0.1538 ::: bot acc: 0.0452
current epoch: 37
train loss is 0.074651
average val loss: 0.096349, accuracy: 0.1003
average test loss: 0.110237, accuracy: 0.1060
case acc: 0.13722245
case acc: 0.07958098
case acc: 0.09063089
case acc: 0.09876664
case acc: 0.13379954
case acc: 0.09573277
top acc: 0.2539 ::: bot acc: 0.0306
top acc: 0.1247 ::: bot acc: 0.0375
top acc: 0.1848 ::: bot acc: 0.0244
top acc: 0.1569 ::: bot acc: 0.0466
top acc: 0.2118 ::: bot acc: 0.0567
top acc: 0.1545 ::: bot acc: 0.0451
current epoch: 38
train loss is 0.074818
average val loss: 0.095542, accuracy: 0.0995
average test loss: 0.109457, accuracy: 0.1051
case acc: 0.13580833
case acc: 0.0793823
case acc: 0.0893524
case acc: 0.09839557
case acc: 0.13253736
case acc: 0.09503952
top acc: 0.2522 ::: bot acc: 0.0299
top acc: 0.1244 ::: bot acc: 0.0374
top acc: 0.1833 ::: bot acc: 0.0237
top acc: 0.1560 ::: bot acc: 0.0471
top acc: 0.2107 ::: bot acc: 0.0554
top acc: 0.1535 ::: bot acc: 0.0453
current epoch: 39
train loss is 0.074576
average val loss: 0.094554, accuracy: 0.0986
average test loss: 0.108502, accuracy: 0.1040
case acc: 0.13433973
case acc: 0.078837916
case acc: 0.08786863
case acc: 0.09796477
case acc: 0.13092752
case acc: 0.09419957
top acc: 0.2503 ::: bot acc: 0.0292
top acc: 0.1239 ::: bot acc: 0.0367
top acc: 0.1816 ::: bot acc: 0.0231
top acc: 0.1547 ::: bot acc: 0.0481
top acc: 0.2090 ::: bot acc: 0.0539
top acc: 0.1521 ::: bot acc: 0.0456
current epoch: 40
train loss is 0.074402
average val loss: 0.094132, accuracy: 0.0982
average test loss: 0.108133, accuracy: 0.1036
case acc: 0.13354982
case acc: 0.079080485
case acc: 0.08724333
case acc: 0.09783934
case acc: 0.13026914
case acc: 0.09385598
top acc: 0.2491 ::: bot acc: 0.0291
top acc: 0.1241 ::: bot acc: 0.0369
top acc: 0.1809 ::: bot acc: 0.0229
top acc: 0.1541 ::: bot acc: 0.0484
top acc: 0.2083 ::: bot acc: 0.0532
top acc: 0.1516 ::: bot acc: 0.0458
current epoch: 41
train loss is 0.074260
average val loss: 0.095864, accuracy: 0.0999
average test loss: 0.109883, accuracy: 0.1057
case acc: 0.1354293
case acc: 0.08257188
case acc: 0.08918123
case acc: 0.09904553
case acc: 0.13265213
case acc: 0.09545268
top acc: 0.2514 ::: bot acc: 0.0299
top acc: 0.1276 ::: bot acc: 0.0404
top acc: 0.1835 ::: bot acc: 0.0238
top acc: 0.1568 ::: bot acc: 0.0460
top acc: 0.2108 ::: bot acc: 0.0555
top acc: 0.1542 ::: bot acc: 0.0453
current epoch: 42
train loss is 0.074300
average val loss: 0.096739, accuracy: 0.1008
average test loss: 0.110762, accuracy: 0.1068
case acc: 0.13614129
case acc: 0.08468206
case acc: 0.09017421
case acc: 0.099697046
case acc: 0.13369699
case acc: 0.09618707
top acc: 0.2523 ::: bot acc: 0.0303
top acc: 0.1297 ::: bot acc: 0.0424
top acc: 0.1848 ::: bot acc: 0.0243
top acc: 0.1581 ::: bot acc: 0.0449
top acc: 0.2119 ::: bot acc: 0.0564
top acc: 0.1555 ::: bot acc: 0.0451
current epoch: 43
train loss is 0.074305
average val loss: 0.098210, accuracy: 0.1023
average test loss: 0.112218, accuracy: 0.1084
case acc: 0.13755268
case acc: 0.08751796
case acc: 0.0918918
case acc: 0.10067906
case acc: 0.13556248
case acc: 0.0974133
top acc: 0.2539 ::: bot acc: 0.0311
top acc: 0.1325 ::: bot acc: 0.0453
top acc: 0.1870 ::: bot acc: 0.0250
top acc: 0.1603 ::: bot acc: 0.0433
top acc: 0.2138 ::: bot acc: 0.0583
top acc: 0.1575 ::: bot acc: 0.0447
current epoch: 44
train loss is 0.074414
average val loss: 0.099371, accuracy: 0.1034
average test loss: 0.113390, accuracy: 0.1098
case acc: 0.1385165
case acc: 0.089879364
case acc: 0.0932691
case acc: 0.10155541
case acc: 0.13708016
case acc: 0.09835898
top acc: 0.2551 ::: bot acc: 0.0317
top acc: 0.1349 ::: bot acc: 0.0476
top acc: 0.1889 ::: bot acc: 0.0256
top acc: 0.1620 ::: bot acc: 0.0421
top acc: 0.2154 ::: bot acc: 0.0596
top acc: 0.1591 ::: bot acc: 0.0445
current epoch: 45
train loss is 0.074447
average val loss: 0.098054, accuracy: 0.1022
average test loss: 0.112115, accuracy: 0.1083
case acc: 0.13658708
case acc: 0.088655114
case acc: 0.091771744
case acc: 0.10079177
case acc: 0.13514334
case acc: 0.09714133
top acc: 0.2526 ::: bot acc: 0.0307
top acc: 0.1336 ::: bot acc: 0.0464
top acc: 0.1872 ::: bot acc: 0.0249
top acc: 0.1601 ::: bot acc: 0.0433
top acc: 0.2135 ::: bot acc: 0.0577
top acc: 0.1572 ::: bot acc: 0.0448
current epoch: 46
train loss is 0.074493
average val loss: 0.098643, accuracy: 0.1028
average test loss: 0.112705, accuracy: 0.1090
case acc: 0.13685371
case acc: 0.09011182
case acc: 0.09232819
case acc: 0.101252854
case acc: 0.13595945
case acc: 0.097572275
top acc: 0.2529 ::: bot acc: 0.0309
top acc: 0.1351 ::: bot acc: 0.0479
top acc: 0.1880 ::: bot acc: 0.0251
top acc: 0.1611 ::: bot acc: 0.0425
top acc: 0.2143 ::: bot acc: 0.0585
top acc: 0.1580 ::: bot acc: 0.0445
current epoch: 47
train loss is 0.074401
average val loss: 0.097766, accuracy: 0.1020
average test loss: 0.111868, accuracy: 0.1081
case acc: 0.13548094
case acc: 0.08945094
case acc: 0.09114076
case acc: 0.10072631
case acc: 0.13481422
case acc: 0.09674838
top acc: 0.2512 ::: bot acc: 0.0301
top acc: 0.1342 ::: bot acc: 0.0474
top acc: 0.1865 ::: bot acc: 0.0245
top acc: 0.1600 ::: bot acc: 0.0430
top acc: 0.2132 ::: bot acc: 0.0575
top acc: 0.1568 ::: bot acc: 0.0446
current epoch: 48
train loss is 0.074481
average val loss: 0.095398, accuracy: 0.0998
average test loss: 0.109542, accuracy: 0.1054
case acc: 0.13248721
case acc: 0.086535394
case acc: 0.08811156
case acc: 0.09922442
case acc: 0.13140836
case acc: 0.09461744
top acc: 0.2472 ::: bot acc: 0.0290
top acc: 0.1312 ::: bot acc: 0.0446
top acc: 0.1827 ::: bot acc: 0.0231
top acc: 0.1567 ::: bot acc: 0.0450
top acc: 0.2097 ::: bot acc: 0.0542
top acc: 0.1535 ::: bot acc: 0.0450
current epoch: 49
train loss is 0.074256
average val loss: 0.095296, accuracy: 0.0997
average test loss: 0.109449, accuracy: 0.1053
case acc: 0.13225034
case acc: 0.08694273
case acc: 0.08767942
case acc: 0.09923788
case acc: 0.13117968
case acc: 0.09451474
top acc: 0.2468 ::: bot acc: 0.0289
top acc: 0.1316 ::: bot acc: 0.0450
top acc: 0.1823 ::: bot acc: 0.0228
top acc: 0.1566 ::: bot acc: 0.0449
top acc: 0.2095 ::: bot acc: 0.0540
top acc: 0.1534 ::: bot acc: 0.0449
current epoch: 50
train loss is 0.074274
average val loss: 0.095639, accuracy: 0.1000
average test loss: 0.109839, accuracy: 0.1058
case acc: 0.13266394
case acc: 0.08798718
case acc: 0.08785394
case acc: 0.09961866
case acc: 0.13158965
case acc: 0.094830886
top acc: 0.2476 ::: bot acc: 0.0290
top acc: 0.1327 ::: bot acc: 0.0461
top acc: 0.1829 ::: bot acc: 0.0226
top acc: 0.1574 ::: bot acc: 0.0445
top acc: 0.2101 ::: bot acc: 0.0543
top acc: 0.1541 ::: bot acc: 0.0447
