
		{"drop_out": 0.6, "drop_out_mc": 0.05, "repeat_mc": 50, "hidden": 30, "embedding_size": 5, "batch": 512, "lag": 4}
LME_Co_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6792 6792 6792
1.8562728 -0.6288155 0.09756618 -0.123651974
Validation: 756 756 756
Testing: 774 774 774
pre-processing time: 0.00026726722717285156
the split date is 2009-07-01
train dropout: 0.6 test dropout: 0.05
net initializing with time: 0.003704071044921875
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012622
average val loss: 0.005397, accuracy: 0.0993
average test loss: 0.005515, accuracy: 0.1006
case acc: 0.14267488
case acc: 0.08068113
case acc: 0.09858809
case acc: 0.093811475
case acc: 0.12826198
case acc: 0.05973785
top acc: 0.1326 ::: bot acc: 0.1525
top acc: 0.0928 ::: bot acc: 0.0689
top acc: 0.0818 ::: bot acc: 0.1162
top acc: 0.0853 ::: bot acc: 0.1035
top acc: 0.1161 ::: bot acc: 0.1406
top acc: 0.0484 ::: bot acc: 0.0708
current epoch: 2
train loss is 0.008014
average val loss: 0.002708, accuracy: 0.0491
average test loss: 0.002695, accuracy: 0.0489
case acc: 0.040047687
case acc: 0.16655259
case acc: 0.013246624
case acc: 0.0075029293
case acc: 0.0398833
case acc: 0.025967363
top acc: 0.0301 ::: bot acc: 0.0495
top acc: 0.1783 ::: bot acc: 0.1552
top acc: 0.0202 ::: bot acc: 0.0140
top acc: 0.0098 ::: bot acc: 0.0091
top acc: 0.0281 ::: bot acc: 0.0513
top acc: 0.0368 ::: bot acc: 0.0155
current epoch: 3
train loss is 0.008330
average val loss: 0.007868, accuracy: 0.1063
average test loss: 0.007782, accuracy: 0.1052
case acc: 0.05413337
case acc: 0.2464155
case acc: 0.0972275
case acc: 0.08681321
case acc: 0.043692462
case acc: 0.10315236
top acc: 0.0644 ::: bot acc: 0.0445
top acc: 0.2578 ::: bot acc: 0.2350
top acc: 0.1136 ::: bot acc: 0.0801
top acc: 0.0959 ::: bot acc: 0.0767
top acc: 0.0561 ::: bot acc: 0.0315
top acc: 0.1145 ::: bot acc: 0.0919
current epoch: 4
train loss is 0.009915
average val loss: 0.016872, accuracy: 0.1730
average test loss: 0.016658, accuracy: 0.1717
case acc: 0.12803578
case acc: 0.3064512
case acc: 0.16969325
case acc: 0.15233798
case acc: 0.112130895
case acc: 0.16144714
top acc: 0.1383 ::: bot acc: 0.1184
top acc: 0.3180 ::: bot acc: 0.2948
top acc: 0.1863 ::: bot acc: 0.1527
top acc: 0.1614 ::: bot acc: 0.1420
top acc: 0.1242 ::: bot acc: 0.1001
top acc: 0.1723 ::: bot acc: 0.1503
current epoch: 5
train loss is 0.012926
average val loss: 0.013155, accuracy: 0.1516
average test loss: 0.012979, accuracy: 0.1504
case acc: 0.1119508
case acc: 0.27524033
case acc: 0.15104643
case acc: 0.13018051
case acc: 0.09857768
case acc: 0.13541986
top acc: 0.1220 ::: bot acc: 0.1023
top acc: 0.2868 ::: bot acc: 0.2636
top acc: 0.1676 ::: bot acc: 0.1341
top acc: 0.1393 ::: bot acc: 0.1201
top acc: 0.1107 ::: bot acc: 0.0864
top acc: 0.1465 ::: bot acc: 0.1244
current epoch: 6
train loss is 0.012521
average val loss: 0.003033, accuracy: 0.0578
average test loss: 0.002966, accuracy: 0.0566
case acc: 0.022212949
case acc: 0.16781408
case acc: 0.05524487
case acc: 0.035457052
case acc: 0.017667113
case acc: 0.04117097
top acc: 0.0319 ::: bot acc: 0.0130
top acc: 0.1792 ::: bot acc: 0.1564
top acc: 0.0718 ::: bot acc: 0.0387
top acc: 0.0451 ::: bot acc: 0.0250
top acc: 0.0283 ::: bot acc: 0.0080
top acc: 0.0525 ::: bot acc: 0.0297
current epoch: 7
train loss is 0.006944
average val loss: 0.001051, accuracy: 0.0338
average test loss: 0.001062, accuracy: 0.0345
case acc: 0.03213999
case acc: 0.09729843
case acc: 0.01334909
case acc: 0.021510841
case acc: 0.02836502
case acc: 0.014075611
top acc: 0.0221 ::: bot acc: 0.0418
top acc: 0.1091 ::: bot acc: 0.0856
top acc: 0.0101 ::: bot acc: 0.0231
top acc: 0.0134 ::: bot acc: 0.0309
top acc: 0.0164 ::: bot acc: 0.0404
top acc: 0.0056 ::: bot acc: 0.0239
current epoch: 8
train loss is 0.003130
average val loss: 0.000892, accuracy: 0.0261
average test loss: 0.000894, accuracy: 0.0267
case acc: 0.01969042
case acc: 0.09689083
case acc: 0.012177046
case acc: 0.011030646
case acc: 0.011910941
case acc: 0.008313512
top acc: 0.0105 ::: bot acc: 0.0288
top acc: 0.1086 ::: bot acc: 0.0851
top acc: 0.0166 ::: bot acc: 0.0165
top acc: 0.0054 ::: bot acc: 0.0192
top acc: 0.0053 ::: bot acc: 0.0212
top acc: 0.0119 ::: bot acc: 0.0102
current epoch: 9
train loss is 0.002262
average val loss: 0.001027, accuracy: 0.0286
average test loss: 0.001012, accuracy: 0.0281
case acc: 0.007469897
case acc: 0.1034294
case acc: 0.016521571
case acc: 0.009470451
case acc: 0.013979413
case acc: 0.018028283
top acc: 0.0085 ::: bot acc: 0.0112
top acc: 0.1149 ::: bot acc: 0.0920
top acc: 0.0294 ::: bot acc: 0.0075
top acc: 0.0158 ::: bot acc: 0.0059
top acc: 0.0240 ::: bot acc: 0.0065
top acc: 0.0287 ::: bot acc: 0.0079
current epoch: 10
train loss is 0.002151
average val loss: 0.000908, accuracy: 0.0279
average test loss: 0.000896, accuracy: 0.0274
case acc: 0.007229696
case acc: 0.09592316
case acc: 0.015573853
case acc: 0.009809839
case acc: 0.016782228
case acc: 0.018905595
top acc: 0.0099 ::: bot acc: 0.0094
top acc: 0.1073 ::: bot acc: 0.0844
top acc: 0.0279 ::: bot acc: 0.0075
top acc: 0.0162 ::: bot acc: 0.0057
top acc: 0.0276 ::: bot acc: 0.0072
top acc: 0.0297 ::: bot acc: 0.0085
current epoch: 11
train loss is 0.001970
average val loss: 0.000773, accuracy: 0.0263
average test loss: 0.000766, accuracy: 0.0259
case acc: 0.0074515915
case acc: 0.0874139
case acc: 0.014128735
case acc: 0.009233754
case acc: 0.01843916
case acc: 0.018698575
top acc: 0.0111 ::: bot acc: 0.0086
top acc: 0.0991 ::: bot acc: 0.0757
top acc: 0.0251 ::: bot acc: 0.0087
top acc: 0.0154 ::: bot acc: 0.0056
top acc: 0.0296 ::: bot acc: 0.0086
top acc: 0.0296 ::: bot acc: 0.0081
current epoch: 12
train loss is 0.001677
average val loss: 0.000718, accuracy: 0.0267
average test loss: 0.000709, accuracy: 0.0261
case acc: 0.00787185
case acc: 0.08183741
case acc: 0.014246422
case acc: 0.010381122
case acc: 0.021692686
case acc: 0.020785498
top acc: 0.0139 ::: bot acc: 0.0060
top acc: 0.0936 ::: bot acc: 0.0704
top acc: 0.0251 ::: bot acc: 0.0087
top acc: 0.0174 ::: bot acc: 0.0052
top acc: 0.0332 ::: bot acc: 0.0110
top acc: 0.0316 ::: bot acc: 0.0104
current epoch: 13
train loss is 0.001457
average val loss: 0.000625, accuracy: 0.0255
average test loss: 0.000613, accuracy: 0.0249
case acc: 0.008228791
case acc: 0.07446972
case acc: 0.013468029
case acc: 0.010195806
case acc: 0.022498945
case acc: 0.020312335
top acc: 0.0146 ::: bot acc: 0.0055
top acc: 0.0861 ::: bot acc: 0.0629
top acc: 0.0236 ::: bot acc: 0.0099
top acc: 0.0169 ::: bot acc: 0.0052
top acc: 0.0340 ::: bot acc: 0.0115
top acc: 0.0310 ::: bot acc: 0.0098
current epoch: 14
train loss is 0.001262
average val loss: 0.000542, accuracy: 0.0243
average test loss: 0.000535, accuracy: 0.0237
case acc: 0.0086109415
case acc: 0.06790894
case acc: 0.012930578
case acc: 0.01012111
case acc: 0.022693856
case acc: 0.019997235
top acc: 0.0155 ::: bot acc: 0.0051
top acc: 0.0796 ::: bot acc: 0.0564
top acc: 0.0222 ::: bot acc: 0.0108
top acc: 0.0170 ::: bot acc: 0.0053
top acc: 0.0342 ::: bot acc: 0.0116
top acc: 0.0310 ::: bot acc: 0.0094
current epoch: 15
train loss is 0.001085
average val loss: 0.000491, accuracy: 0.0237
average test loss: 0.000483, accuracy: 0.0231
case acc: 0.009159938
case acc: 0.06281936
case acc: 0.0130326925
case acc: 0.010451944
case acc: 0.023253964
case acc: 0.020026354
top acc: 0.0168 ::: bot acc: 0.0040
top acc: 0.0743 ::: bot acc: 0.0513
top acc: 0.0221 ::: bot acc: 0.0114
top acc: 0.0173 ::: bot acc: 0.0053
top acc: 0.0348 ::: bot acc: 0.0124
top acc: 0.0309 ::: bot acc: 0.0095
current epoch: 16
train loss is 0.001006
average val loss: 0.000489, accuracy: 0.0246
average test loss: 0.000481, accuracy: 0.0240
case acc: 0.011266805
case acc: 0.0602352
case acc: 0.013635475
case acc: 0.01213881
case acc: 0.025040688
case acc: 0.021865174
top acc: 0.0200 ::: bot acc: 0.0043
top acc: 0.0720 ::: bot acc: 0.0485
top acc: 0.0242 ::: bot acc: 0.0095
top acc: 0.0198 ::: bot acc: 0.0056
top acc: 0.0367 ::: bot acc: 0.0139
top acc: 0.0327 ::: bot acc: 0.0114
current epoch: 17
train loss is 0.000931
average val loss: 0.000504, accuracy: 0.0260
average test loss: 0.000491, accuracy: 0.0252
case acc: 0.013808318
case acc: 0.058421556
case acc: 0.014652414
case acc: 0.014025954
case acc: 0.026676647
case acc: 0.023607614
top acc: 0.0231 ::: bot acc: 0.0056
top acc: 0.0703 ::: bot acc: 0.0466
top acc: 0.0263 ::: bot acc: 0.0079
top acc: 0.0220 ::: bot acc: 0.0065
top acc: 0.0384 ::: bot acc: 0.0150
top acc: 0.0346 ::: bot acc: 0.0128
current epoch: 18
train loss is 0.000863
average val loss: 0.000476, accuracy: 0.0257
average test loss: 0.000463, accuracy: 0.0250
case acc: 0.0151699595
case acc: 0.055059828
case acc: 0.015201991
case acc: 0.014814348
case acc: 0.026014302
case acc: 0.023552591
top acc: 0.0245 ::: bot acc: 0.0068
top acc: 0.0666 ::: bot acc: 0.0434
top acc: 0.0272 ::: bot acc: 0.0077
top acc: 0.0231 ::: bot acc: 0.0071
top acc: 0.0378 ::: bot acc: 0.0147
top acc: 0.0344 ::: bot acc: 0.0128
current epoch: 19
train loss is 0.000822
average val loss: 0.000433, accuracy: 0.0247
average test loss: 0.000421, accuracy: 0.0240
case acc: 0.015484346
case acc: 0.05137246
case acc: 0.015172004
case acc: 0.01465247
case acc: 0.024751874
case acc: 0.02239879
top acc: 0.0248 ::: bot acc: 0.0070
top acc: 0.0631 ::: bot acc: 0.0398
top acc: 0.0272 ::: bot acc: 0.0077
top acc: 0.0226 ::: bot acc: 0.0072
top acc: 0.0368 ::: bot acc: 0.0132
top acc: 0.0334 ::: bot acc: 0.0117
current epoch: 20
train loss is 0.000748
average val loss: 0.000450, accuracy: 0.0258
average test loss: 0.000435, accuracy: 0.0250
case acc: 0.01801282
case acc: 0.05025789
case acc: 0.016673328
case acc: 0.016381204
case acc: 0.025390899
case acc: 0.023334108
top acc: 0.0275 ::: bot acc: 0.0092
top acc: 0.0621 ::: bot acc: 0.0386
top acc: 0.0299 ::: bot acc: 0.0070
top acc: 0.0248 ::: bot acc: 0.0081
top acc: 0.0371 ::: bot acc: 0.0140
top acc: 0.0343 ::: bot acc: 0.0124
current epoch: 21
train loss is 0.000749
average val loss: 0.000472, accuracy: 0.0270
average test loss: 0.000450, accuracy: 0.0260
case acc: 0.020143272
case acc: 0.049344648
case acc: 0.018396033
case acc: 0.018129941
case acc: 0.025727246
case acc: 0.024205204
top acc: 0.0296 ::: bot acc: 0.0114
top acc: 0.0611 ::: bot acc: 0.0376
top acc: 0.0324 ::: bot acc: 0.0070
top acc: 0.0268 ::: bot acc: 0.0095
top acc: 0.0377 ::: bot acc: 0.0142
top acc: 0.0352 ::: bot acc: 0.0134
current epoch: 22
train loss is 0.000712
average val loss: 0.000514, accuracy: 0.0288
average test loss: 0.000493, accuracy: 0.0278
case acc: 0.023098554
case acc: 0.049622383
case acc: 0.020750267
case acc: 0.020546837
case acc: 0.026956074
case acc: 0.025560824
top acc: 0.0328 ::: bot acc: 0.0139
top acc: 0.0614 ::: bot acc: 0.0380
top acc: 0.0357 ::: bot acc: 0.0077
top acc: 0.0294 ::: bot acc: 0.0114
top acc: 0.0388 ::: bot acc: 0.0153
top acc: 0.0366 ::: bot acc: 0.0146
current epoch: 23
train loss is 0.000707
average val loss: 0.000541, accuracy: 0.0299
average test loss: 0.000514, accuracy: 0.0287
case acc: 0.02523718
case acc: 0.04900289
case acc: 0.022645717
case acc: 0.021723665
case acc: 0.027762745
case acc: 0.02580863
top acc: 0.0348 ::: bot acc: 0.0159
top acc: 0.0607 ::: bot acc: 0.0372
top acc: 0.0382 ::: bot acc: 0.0084
top acc: 0.0309 ::: bot acc: 0.0121
top acc: 0.0396 ::: bot acc: 0.0162
top acc: 0.0369 ::: bot acc: 0.0150
current epoch: 24
train loss is 0.000658
average val loss: 0.000480, accuracy: 0.0280
average test loss: 0.000459, accuracy: 0.0270
case acc: 0.024332456
case acc: 0.045914028
case acc: 0.022283247
case acc: 0.020528223
case acc: 0.025535956
case acc: 0.023360021
top acc: 0.0339 ::: bot acc: 0.0150
top acc: 0.0576 ::: bot acc: 0.0342
top acc: 0.0377 ::: bot acc: 0.0085
top acc: 0.0294 ::: bot acc: 0.0115
top acc: 0.0375 ::: bot acc: 0.0142
top acc: 0.0346 ::: bot acc: 0.0125
current epoch: 25
train loss is 0.000629
average val loss: 0.000551, accuracy: 0.0305
average test loss: 0.000524, accuracy: 0.0293
case acc: 0.027520435
case acc: 0.047321763
case acc: 0.0255197
case acc: 0.023047328
case acc: 0.027534204
case acc: 0.024883755
top acc: 0.0374 ::: bot acc: 0.0180
top acc: 0.0589 ::: bot acc: 0.0358
top acc: 0.0414 ::: bot acc: 0.0105
top acc: 0.0322 ::: bot acc: 0.0136
top acc: 0.0395 ::: bot acc: 0.0160
top acc: 0.0361 ::: bot acc: 0.0140
current epoch: 26
train loss is 0.000629
average val loss: 0.000547, accuracy: 0.0304
average test loss: 0.000522, accuracy: 0.0293
case acc: 0.028231572
case acc: 0.046420295
case acc: 0.026703794
case acc: 0.023495272
case acc: 0.026926126
case acc: 0.024301756
top acc: 0.0379 ::: bot acc: 0.0189
top acc: 0.0581 ::: bot acc: 0.0349
top acc: 0.0427 ::: bot acc: 0.0116
top acc: 0.0326 ::: bot acc: 0.0140
top acc: 0.0389 ::: bot acc: 0.0154
top acc: 0.0353 ::: bot acc: 0.0135
current epoch: 27
train loss is 0.000629
average val loss: 0.000597, accuracy: 0.0320
average test loss: 0.000567, accuracy: 0.0309
case acc: 0.030284556
case acc: 0.04711318
case acc: 0.02927493
case acc: 0.025419306
case acc: 0.028014911
case acc: 0.025167044
top acc: 0.0400 ::: bot acc: 0.0208
top acc: 0.0588 ::: bot acc: 0.0354
top acc: 0.0455 ::: bot acc: 0.0136
top acc: 0.0345 ::: bot acc: 0.0156
top acc: 0.0399 ::: bot acc: 0.0163
top acc: 0.0363 ::: bot acc: 0.0144
current epoch: 28
train loss is 0.000639
average val loss: 0.000670, accuracy: 0.0343
average test loss: 0.000637, accuracy: 0.0331
case acc: 0.033085573
case acc: 0.048451617
case acc: 0.032597132
case acc: 0.027798448
case acc: 0.029794563
case acc: 0.026762225
top acc: 0.0428 ::: bot acc: 0.0237
top acc: 0.0602 ::: bot acc: 0.0366
top acc: 0.0491 ::: bot acc: 0.0164
top acc: 0.0370 ::: bot acc: 0.0178
top acc: 0.0418 ::: bot acc: 0.0180
top acc: 0.0379 ::: bot acc: 0.0157
current epoch: 29
train loss is 0.000640
average val loss: 0.000772, accuracy: 0.0371
average test loss: 0.000733, accuracy: 0.0359
case acc: 0.036467537
case acc: 0.050316516
case acc: 0.036134463
case acc: 0.030826075
case acc: 0.032743514
case acc: 0.028856758
top acc: 0.0461 ::: bot acc: 0.0271
top acc: 0.0620 ::: bot acc: 0.0386
top acc: 0.0527 ::: bot acc: 0.0198
top acc: 0.0400 ::: bot acc: 0.0210
top acc: 0.0448 ::: bot acc: 0.0208
top acc: 0.0398 ::: bot acc: 0.0179
current epoch: 30
train loss is 0.000665
average val loss: 0.000801, accuracy: 0.0379
average test loss: 0.000763, accuracy: 0.0368
case acc: 0.037543904
case acc: 0.05021354
case acc: 0.03779671
case acc: 0.031906288
case acc: 0.03392475
case acc: 0.029140515
top acc: 0.0472 ::: bot acc: 0.0281
top acc: 0.0621 ::: bot acc: 0.0384
top acc: 0.0545 ::: bot acc: 0.0213
top acc: 0.0410 ::: bot acc: 0.0220
top acc: 0.0460 ::: bot acc: 0.0220
top acc: 0.0401 ::: bot acc: 0.0181
current epoch: 31
train loss is 0.000653
average val loss: 0.000833, accuracy: 0.0388
average test loss: 0.000795, accuracy: 0.0376
case acc: 0.03859535
case acc: 0.05018386
case acc: 0.039494846
case acc: 0.032856993
case acc: 0.0350895
case acc: 0.029559946
top acc: 0.0482 ::: bot acc: 0.0292
top acc: 0.0621 ::: bot acc: 0.0383
top acc: 0.0564 ::: bot acc: 0.0229
top acc: 0.0423 ::: bot acc: 0.0228
top acc: 0.0471 ::: bot acc: 0.0231
top acc: 0.0407 ::: bot acc: 0.0185
current epoch: 32
train loss is 0.000666
average val loss: 0.000850, accuracy: 0.0393
average test loss: 0.000808, accuracy: 0.0380
case acc: 0.038899355
case acc: 0.049697343
case acc: 0.040492598
case acc: 0.03363269
case acc: 0.03574227
case acc: 0.029563561
top acc: 0.0487 ::: bot acc: 0.0294
top acc: 0.0614 ::: bot acc: 0.0381
top acc: 0.0571 ::: bot acc: 0.0240
top acc: 0.0428 ::: bot acc: 0.0237
top acc: 0.0479 ::: bot acc: 0.0237
top acc: 0.0407 ::: bot acc: 0.0186
current epoch: 33
train loss is 0.000649
average val loss: 0.000898, accuracy: 0.0405
average test loss: 0.000854, accuracy: 0.0392
case acc: 0.040128995
case acc: 0.049829174
case acc: 0.042263854
case acc: 0.035210144
case acc: 0.03736753
case acc: 0.03064112
top acc: 0.0499 ::: bot acc: 0.0307
top acc: 0.0616 ::: bot acc: 0.0382
top acc: 0.0589 ::: bot acc: 0.0257
top acc: 0.0444 ::: bot acc: 0.0254
top acc: 0.0496 ::: bot acc: 0.0252
top acc: 0.0419 ::: bot acc: 0.0195
current epoch: 34
train loss is 0.000646
average val loss: 0.000861, accuracy: 0.0396
average test loss: 0.000820, accuracy: 0.0384
case acc: 0.039261166
case acc: 0.047935802
case acc: 0.041713994
case acc: 0.03467859
case acc: 0.037182476
case acc: 0.029641103
top acc: 0.0491 ::: bot acc: 0.0299
top acc: 0.0598 ::: bot acc: 0.0360
top acc: 0.0585 ::: bot acc: 0.0251
top acc: 0.0442 ::: bot acc: 0.0247
top acc: 0.0493 ::: bot acc: 0.0252
top acc: 0.0408 ::: bot acc: 0.0185
current epoch: 35
train loss is 0.000624
average val loss: 0.000790, accuracy: 0.0378
average test loss: 0.000749, accuracy: 0.0366
case acc: 0.037211377
case acc: 0.044965725
case acc: 0.040230583
case acc: 0.03316738
case acc: 0.035893153
case acc: 0.028007112
top acc: 0.0469 ::: bot acc: 0.0278
top acc: 0.0567 ::: bot acc: 0.0332
top acc: 0.0570 ::: bot acc: 0.0235
top acc: 0.0427 ::: bot acc: 0.0231
top acc: 0.0480 ::: bot acc: 0.0240
top acc: 0.0392 ::: bot acc: 0.0169
current epoch: 36
train loss is 0.000588
average val loss: 0.000771, accuracy: 0.0374
average test loss: 0.000730, accuracy: 0.0361
case acc: 0.036314093
case acc: 0.043473013
case acc: 0.03993007
case acc: 0.033225812
case acc: 0.035798967
case acc: 0.02790047
top acc: 0.0461 ::: bot acc: 0.0268
top acc: 0.0553 ::: bot acc: 0.0318
top acc: 0.0567 ::: bot acc: 0.0234
top acc: 0.0426 ::: bot acc: 0.0233
top acc: 0.0479 ::: bot acc: 0.0239
top acc: 0.0391 ::: bot acc: 0.0168
current epoch: 37
train loss is 0.000555
average val loss: 0.000634, accuracy: 0.0336
average test loss: 0.000599, accuracy: 0.0324
case acc: 0.03212404
case acc: 0.03866963
case acc: 0.036268394
case acc: 0.02998167
case acc: 0.03259438
case acc: 0.02479896
top acc: 0.0418 ::: bot acc: 0.0227
top acc: 0.0505 ::: bot acc: 0.0269
top acc: 0.0528 ::: bot acc: 0.0199
top acc: 0.0393 ::: bot acc: 0.0202
top acc: 0.0447 ::: bot acc: 0.0207
top acc: 0.0361 ::: bot acc: 0.0138
current epoch: 38
train loss is 0.000506
average val loss: 0.000521, accuracy: 0.0301
average test loss: 0.000490, accuracy: 0.0289
case acc: 0.028319798
case acc: 0.033953972
case acc: 0.032627713
case acc: 0.026967714
case acc: 0.029908044
case acc: 0.021801222
top acc: 0.0379 ::: bot acc: 0.0190
top acc: 0.0458 ::: bot acc: 0.0223
top acc: 0.0491 ::: bot acc: 0.0166
top acc: 0.0363 ::: bot acc: 0.0171
top acc: 0.0419 ::: bot acc: 0.0181
top acc: 0.0327 ::: bot acc: 0.0112
current epoch: 39
train loss is 0.000466
average val loss: 0.000436, accuracy: 0.0272
average test loss: 0.000407, accuracy: 0.0261
case acc: 0.025050279
case acc: 0.0298889
case acc: 0.029606493
case acc: 0.024435366
case acc: 0.027903646
case acc: 0.019436514
top acc: 0.0347 ::: bot acc: 0.0158
top acc: 0.0417 ::: bot acc: 0.0181
top acc: 0.0459 ::: bot acc: 0.0138
top acc: 0.0336 ::: bot acc: 0.0148
top acc: 0.0397 ::: bot acc: 0.0163
top acc: 0.0304 ::: bot acc: 0.0090
current epoch: 40
train loss is 0.000432
average val loss: 0.000394, accuracy: 0.0257
average test loss: 0.000367, accuracy: 0.0245
case acc: 0.023134986
case acc: 0.027499387
case acc: 0.027903967
case acc: 0.023281105
case acc: 0.02702817
case acc: 0.018420523
top acc: 0.0327 ::: bot acc: 0.0141
top acc: 0.0393 ::: bot acc: 0.0159
top acc: 0.0441 ::: bot acc: 0.0123
top acc: 0.0324 ::: bot acc: 0.0138
top acc: 0.0390 ::: bot acc: 0.0154
top acc: 0.0292 ::: bot acc: 0.0080
current epoch: 41
train loss is 0.000406
average val loss: 0.000315, accuracy: 0.0225
average test loss: 0.000293, accuracy: 0.0215
case acc: 0.019717015
case acc: 0.023335848
case acc: 0.024686603
case acc: 0.020775387
case acc: 0.024360815
case acc: 0.016151676
top acc: 0.0292 ::: bot acc: 0.0108
top acc: 0.0351 ::: bot acc: 0.0118
top acc: 0.0407 ::: bot acc: 0.0098
top acc: 0.0297 ::: bot acc: 0.0117
top acc: 0.0361 ::: bot acc: 0.0130
top acc: 0.0268 ::: bot acc: 0.0064
current epoch: 42
train loss is 0.000374
average val loss: 0.000221, accuracy: 0.0182
average test loss: 0.000204, accuracy: 0.0173
case acc: 0.014735512
case acc: 0.017867643
case acc: 0.020406613
case acc: 0.017020494
case acc: 0.020480989
case acc: 0.013308756
top acc: 0.0238 ::: bot acc: 0.0065
top acc: 0.0293 ::: bot acc: 0.0067
top acc: 0.0355 ::: bot acc: 0.0073
top acc: 0.0256 ::: bot acc: 0.0087
top acc: 0.0320 ::: bot acc: 0.0097
top acc: 0.0232 ::: bot acc: 0.0049
current epoch: 43
train loss is 0.000346
average val loss: 0.000153, accuracy: 0.0146
average test loss: 0.000143, accuracy: 0.0139
case acc: 0.0107534975
case acc: 0.01351032
case acc: 0.017056813
case acc: 0.0137656275
case acc: 0.017246375
case acc: 0.011119816
top acc: 0.0192 ::: bot acc: 0.0038
top acc: 0.0244 ::: bot acc: 0.0039
top acc: 0.0305 ::: bot acc: 0.0069
top acc: 0.0218 ::: bot acc: 0.0065
top acc: 0.0283 ::: bot acc: 0.0073
top acc: 0.0197 ::: bot acc: 0.0053
current epoch: 44
train loss is 0.000331
average val loss: 0.000126, accuracy: 0.0129
average test loss: 0.000118, accuracy: 0.0123
case acc: 0.009109275
case acc: 0.011255475
case acc: 0.015297156
case acc: 0.0120886145
case acc: 0.016053114
case acc: 0.010294753
top acc: 0.0167 ::: bot acc: 0.0040
top acc: 0.0209 ::: bot acc: 0.0039
top acc: 0.0278 ::: bot acc: 0.0075
top acc: 0.0197 ::: bot acc: 0.0056
top acc: 0.0266 ::: bot acc: 0.0070
top acc: 0.0182 ::: bot acc: 0.0057
current epoch: 45
train loss is 0.000326
average val loss: 0.000099, accuracy: 0.0112
average test loss: 0.000096, accuracy: 0.0109
case acc: 0.0077508558
case acc: 0.009764006
case acc: 0.01358321
case acc: 0.010447876
case acc: 0.014386427
case acc: 0.009613884
top acc: 0.0135 ::: bot acc: 0.0060
top acc: 0.0175 ::: bot acc: 0.0063
top acc: 0.0241 ::: bot acc: 0.0095
top acc: 0.0175 ::: bot acc: 0.0052
top acc: 0.0246 ::: bot acc: 0.0062
top acc: 0.0167 ::: bot acc: 0.0066
current epoch: 46
train loss is 0.000316
average val loss: 0.000070, accuracy: 0.0090
average test loss: 0.000073, accuracy: 0.0093
case acc: 0.0073033785
case acc: 0.008830884
case acc: 0.012151213
case acc: 0.007777057
case acc: 0.011014443
case acc: 0.008450202
top acc: 0.0077 ::: bot acc: 0.0115
top acc: 0.0109 ::: bot acc: 0.0128
top acc: 0.0178 ::: bot acc: 0.0157
top acc: 0.0126 ::: bot acc: 0.0070
top acc: 0.0197 ::: bot acc: 0.0059
top acc: 0.0127 ::: bot acc: 0.0100
current epoch: 47
train loss is 0.000321
average val loss: 0.000067, accuracy: 0.0088
average test loss: 0.000073, accuracy: 0.0093
case acc: 0.008205502
case acc: 0.009521514
case acc: 0.012520324
case acc: 0.007235435
case acc: 0.009870047
case acc: 0.0082411505
top acc: 0.0048 ::: bot acc: 0.0145
top acc: 0.0068 ::: bot acc: 0.0166
top acc: 0.0138 ::: bot acc: 0.0195
top acc: 0.0101 ::: bot acc: 0.0095
top acc: 0.0175 ::: bot acc: 0.0068
top acc: 0.0107 ::: bot acc: 0.0115
current epoch: 48
train loss is 0.000332
average val loss: 0.000075, accuracy: 0.0094
average test loss: 0.000086, accuracy: 0.0101
case acc: 0.0101639265
case acc: 0.011699573
case acc: 0.0137233
case acc: 0.007332454
case acc: 0.0092352135
case acc: 0.008516241
top acc: 0.0039 ::: bot acc: 0.0178
top acc: 0.0041 ::: bot acc: 0.0214
top acc: 0.0094 ::: bot acc: 0.0241
top acc: 0.0073 ::: bot acc: 0.0123
top acc: 0.0156 ::: bot acc: 0.0087
top acc: 0.0092 ::: bot acc: 0.0133
current epoch: 49
train loss is 0.000347
average val loss: 0.000107, accuracy: 0.0115
average test loss: 0.000126, accuracy: 0.0126
case acc: 0.014183654
case acc: 0.016912982
case acc: 0.017302243
case acc: 0.009217281
case acc: 0.008611647
case acc: 0.009387543
top acc: 0.0056 ::: bot acc: 0.0230
top acc: 0.0069 ::: bot acc: 0.0278
top acc: 0.0068 ::: bot acc: 0.0307
top acc: 0.0046 ::: bot acc: 0.0169
top acc: 0.0119 ::: bot acc: 0.0123
top acc: 0.0064 ::: bot acc: 0.0163
current epoch: 50
train loss is 0.000380
average val loss: 0.000190, accuracy: 0.0161
average test loss: 0.000217, accuracy: 0.0175
case acc: 0.020651396
case acc: 0.024371399
case acc: 0.023985907
case acc: 0.014054822
case acc: 0.009980536
case acc: 0.011824046
top acc: 0.0111 ::: bot acc: 0.0299
top acc: 0.0128 ::: bot acc: 0.0360
top acc: 0.0099 ::: bot acc: 0.0392
top acc: 0.0066 ::: bot acc: 0.0232
top acc: 0.0072 ::: bot acc: 0.0174
top acc: 0.0046 ::: bot acc: 0.0210
LME_Co_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6810 6810 6810
1.8562728 -0.6288155 0.08104724 -0.1112376
Validation: 762 762 762
Testing: 744 744 744
pre-processing time: 0.000179290771484375
the split date is 2010-01-01
train dropout: 0.6 test dropout: 0.05
net initializing with time: 0.004261493682861328
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012624
average val loss: 0.005153, accuracy: 0.0972
average test loss: 0.005248, accuracy: 0.0976
case acc: 0.13782223
case acc: 0.08533205
case acc: 0.09415719
case acc: 0.090600036
case acc: 0.12427677
case acc: 0.053702563
top acc: 0.1233 ::: bot acc: 0.1526
top acc: 0.0996 ::: bot acc: 0.0725
top acc: 0.0737 ::: bot acc: 0.1149
top acc: 0.0742 ::: bot acc: 0.1065
top acc: 0.1104 ::: bot acc: 0.1385
top acc: 0.0368 ::: bot acc: 0.0703
current epoch: 2
train loss is 0.007461
average val loss: 0.002884, accuracy: 0.0498
average test loss: 0.002913, accuracy: 0.0506
case acc: 0.03254122
case acc: 0.17399377
case acc: 0.017572759
case acc: 0.012797431
case acc: 0.03217826
case acc: 0.03454256
top acc: 0.0179 ::: bot acc: 0.0478
top acc: 0.1878 ::: bot acc: 0.1617
top acc: 0.0314 ::: bot acc: 0.0106
top acc: 0.0227 ::: bot acc: 0.0093
top acc: 0.0181 ::: bot acc: 0.0459
top acc: 0.0516 ::: bot acc: 0.0178
current epoch: 3
train loss is 0.007934
average val loss: 0.007773, accuracy: 0.1055
average test loss: 0.007802, accuracy: 0.1052
case acc: 0.0543665
case acc: 0.24589264
case acc: 0.096818276
case acc: 0.08501861
case acc: 0.045086853
case acc: 0.104169
top acc: 0.0683 ::: bot acc: 0.0399
top acc: 0.2595 ::: bot acc: 0.2340
top acc: 0.1179 ::: bot acc: 0.0765
top acc: 0.1009 ::: bot acc: 0.0692
top acc: 0.0595 ::: bot acc: 0.0311
top acc: 0.1213 ::: bot acc: 0.0873
current epoch: 4
train loss is 0.010232
average val loss: 0.016196, accuracy: 0.1692
average test loss: 0.016168, accuracy: 0.1687
case acc: 0.12525684
case acc: 0.30260718
case acc: 0.16597223
case acc: 0.14778237
case acc: 0.110804245
case acc: 0.15970185
top acc: 0.1396 ::: bot acc: 0.1107
top acc: 0.3161 ::: bot acc: 0.2905
top acc: 0.1871 ::: bot acc: 0.1450
top acc: 0.1637 ::: bot acc: 0.1319
top acc: 0.1255 ::: bot acc: 0.0969
top acc: 0.1764 ::: bot acc: 0.1430
current epoch: 5
train loss is 0.012601
average val loss: 0.013994, accuracy: 0.1572
average test loss: 0.013977, accuracy: 0.1569
case acc: 0.11888878
case acc: 0.2809381
case acc: 0.1571798
case acc: 0.1348285
case acc: 0.10651588
case acc: 0.1428601
top acc: 0.1328 ::: bot acc: 0.1047
top acc: 0.2944 ::: bot acc: 0.2692
top acc: 0.1784 ::: bot acc: 0.1361
top acc: 0.1503 ::: bot acc: 0.1192
top acc: 0.1208 ::: bot acc: 0.0930
top acc: 0.1596 ::: bot acc: 0.1266
current epoch: 6
train loss is 0.012908
average val loss: 0.002963, accuracy: 0.0568
average test loss: 0.002984, accuracy: 0.0566
case acc: 0.023007562
case acc: 0.16690253
case acc: 0.054847084
case acc: 0.034262534
case acc: 0.018352337
case acc: 0.042195294
top acc: 0.0363 ::: bot acc: 0.0100
top acc: 0.1807 ::: bot acc: 0.1549
top acc: 0.0757 ::: bot acc: 0.0341
top acc: 0.0495 ::: bot acc: 0.0191
top acc: 0.0322 ::: bot acc: 0.0066
top acc: 0.0588 ::: bot acc: 0.0259
current epoch: 7
train loss is 0.007137
average val loss: 0.001056, accuracy: 0.0315
average test loss: 0.001094, accuracy: 0.0331
case acc: 0.026465015
case acc: 0.10156737
case acc: 0.015384296
case acc: 0.019777952
case acc: 0.021812197
case acc: 0.013556443
top acc: 0.0131 ::: bot acc: 0.0407
top acc: 0.1151 ::: bot acc: 0.0895
top acc: 0.0194 ::: bot acc: 0.0225
top acc: 0.0097 ::: bot acc: 0.0328
top acc: 0.0081 ::: bot acc: 0.0347
top acc: 0.0103 ::: bot acc: 0.0232
current epoch: 8
train loss is 0.003083
average val loss: 0.000886, accuracy: 0.0261
average test loss: 0.000930, accuracy: 0.0284
case acc: 0.019001536
case acc: 0.09668508
case acc: 0.0153109655
case acc: 0.014855679
case acc: 0.011867884
case acc: 0.0124336975
top acc: 0.0073 ::: bot acc: 0.0324
top acc: 0.1101 ::: bot acc: 0.0849
top acc: 0.0217 ::: bot acc: 0.0203
top acc: 0.0083 ::: bot acc: 0.0264
top acc: 0.0077 ::: bot acc: 0.0203
top acc: 0.0192 ::: bot acc: 0.0144
current epoch: 9
train loss is 0.002187
average val loss: 0.001027, accuracy: 0.0288
average test loss: 0.001063, accuracy: 0.0301
case acc: 0.0102817165
case acc: 0.10332558
case acc: 0.018714797
case acc: 0.012492467
case acc: 0.015304924
case acc: 0.020310715
top acc: 0.0142 ::: bot acc: 0.0147
top acc: 0.1169 ::: bot acc: 0.0913
top acc: 0.0344 ::: bot acc: 0.0082
top acc: 0.0211 ::: bot acc: 0.0105
top acc: 0.0280 ::: bot acc: 0.0050
top acc: 0.0360 ::: bot acc: 0.0063
current epoch: 10
train loss is 0.002124
average val loss: 0.000959, accuracy: 0.0294
average test loss: 0.000999, accuracy: 0.0307
case acc: 0.011008847
case acc: 0.0976109
case acc: 0.019089483
case acc: 0.013559668
case acc: 0.019793773
case acc: 0.02293182
top acc: 0.0178 ::: bot acc: 0.0110
top acc: 0.1110 ::: bot acc: 0.0858
top acc: 0.0349 ::: bot acc: 0.0083
top acc: 0.0238 ::: bot acc: 0.0087
top acc: 0.0334 ::: bot acc: 0.0075
top acc: 0.0390 ::: bot acc: 0.0083
current epoch: 11
train loss is 0.001885
average val loss: 0.000772, accuracy: 0.0263
average test loss: 0.000814, accuracy: 0.0279
case acc: 0.010797495
case acc: 0.08710316
case acc: 0.016932538
case acc: 0.012315828
case acc: 0.019309578
case acc: 0.020847572
top acc: 0.0163 ::: bot acc: 0.0128
top acc: 0.1008 ::: bot acc: 0.0751
top acc: 0.0297 ::: bot acc: 0.0119
top acc: 0.0209 ::: bot acc: 0.0107
top acc: 0.0331 ::: bot acc: 0.0071
top acc: 0.0367 ::: bot acc: 0.0065
current epoch: 12
train loss is 0.001594
average val loss: 0.000618, accuracy: 0.0235
average test loss: 0.000660, accuracy: 0.0254
case acc: 0.010224205
case acc: 0.07702965
case acc: 0.016014563
case acc: 0.011741681
case acc: 0.018441422
case acc: 0.019041881
top acc: 0.0144 ::: bot acc: 0.0142
top acc: 0.0907 ::: bot acc: 0.0650
top acc: 0.0256 ::: bot acc: 0.0165
top acc: 0.0184 ::: bot acc: 0.0133
top acc: 0.0321 ::: bot acc: 0.0067
top acc: 0.0342 ::: bot acc: 0.0058
current epoch: 13
train loss is 0.001329
average val loss: 0.000589, accuracy: 0.0241
average test loss: 0.000627, accuracy: 0.0257
case acc: 0.011213339
case acc: 0.07274706
case acc: 0.015960647
case acc: 0.012374284
case acc: 0.021298625
case acc: 0.020704089
top acc: 0.0182 ::: bot acc: 0.0109
top acc: 0.0863 ::: bot acc: 0.0605
top acc: 0.0263 ::: bot acc: 0.0152
top acc: 0.0208 ::: bot acc: 0.0107
top acc: 0.0353 ::: bot acc: 0.0085
top acc: 0.0363 ::: bot acc: 0.0065
current epoch: 14
train loss is 0.001201
average val loss: 0.000554, accuracy: 0.0244
average test loss: 0.000597, accuracy: 0.0259
case acc: 0.012154563
case acc: 0.06853951
case acc: 0.016304953
case acc: 0.013079646
case acc: 0.023292916
case acc: 0.022120068
top acc: 0.0210 ::: bot acc: 0.0083
top acc: 0.0823 ::: bot acc: 0.0564
top acc: 0.0275 ::: bot acc: 0.0144
top acc: 0.0227 ::: bot acc: 0.0094
top acc: 0.0374 ::: bot acc: 0.0103
top acc: 0.0380 ::: bot acc: 0.0075
current epoch: 15
train loss is 0.001066
average val loss: 0.000484, accuracy: 0.0231
average test loss: 0.000526, accuracy: 0.0246
case acc: 0.012351843
case acc: 0.06258518
case acc: 0.015945721
case acc: 0.012847526
case acc: 0.022703882
case acc: 0.0213858
top acc: 0.0216 ::: bot acc: 0.0080
top acc: 0.0763 ::: bot acc: 0.0504
top acc: 0.0265 ::: bot acc: 0.0154
top acc: 0.0223 ::: bot acc: 0.0095
top acc: 0.0370 ::: bot acc: 0.0096
top acc: 0.0374 ::: bot acc: 0.0066
current epoch: 16
train loss is 0.001019
average val loss: 0.000496, accuracy: 0.0246
average test loss: 0.000539, accuracy: 0.0259
case acc: 0.014350529
case acc: 0.060692053
case acc: 0.016756928
case acc: 0.014544922
case acc: 0.025182297
case acc: 0.023669567
top acc: 0.0252 ::: bot acc: 0.0064
top acc: 0.0745 ::: bot acc: 0.0484
top acc: 0.0292 ::: bot acc: 0.0127
top acc: 0.0255 ::: bot acc: 0.0081
top acc: 0.0394 ::: bot acc: 0.0120
top acc: 0.0399 ::: bot acc: 0.0086
current epoch: 17
train loss is 0.000916
average val loss: 0.000479, accuracy: 0.0248
average test loss: 0.000521, accuracy: 0.0259
case acc: 0.015444471
case acc: 0.057624724
case acc: 0.01706373
case acc: 0.015319955
case acc: 0.025779892
case acc: 0.024262425
top acc: 0.0270 ::: bot acc: 0.0062
top acc: 0.0713 ::: bot acc: 0.0454
top acc: 0.0302 ::: bot acc: 0.0115
top acc: 0.0270 ::: bot acc: 0.0076
top acc: 0.0399 ::: bot acc: 0.0126
top acc: 0.0405 ::: bot acc: 0.0089
current epoch: 18
train loss is 0.000870
average val loss: 0.000451, accuracy: 0.0245
average test loss: 0.000494, accuracy: 0.0256
case acc: 0.016580466
case acc: 0.05420015
case acc: 0.017585047
case acc: 0.015825743
case acc: 0.025211396
case acc: 0.02412258
top acc: 0.0286 ::: bot acc: 0.0065
top acc: 0.0680 ::: bot acc: 0.0420
top acc: 0.0312 ::: bot acc: 0.0111
top acc: 0.0278 ::: bot acc: 0.0078
top acc: 0.0393 ::: bot acc: 0.0120
top acc: 0.0404 ::: bot acc: 0.0087
current epoch: 19
train loss is 0.000796
average val loss: 0.000444, accuracy: 0.0248
average test loss: 0.000484, accuracy: 0.0257
case acc: 0.017797433
case acc: 0.05198273
case acc: 0.018002821
case acc: 0.016684974
case acc: 0.025225023
case acc: 0.024303151
top acc: 0.0305 ::: bot acc: 0.0067
top acc: 0.0656 ::: bot acc: 0.0400
top acc: 0.0327 ::: bot acc: 0.0095
top acc: 0.0291 ::: bot acc: 0.0077
top acc: 0.0394 ::: bot acc: 0.0122
top acc: 0.0406 ::: bot acc: 0.0088
current epoch: 20
train loss is 0.000733
average val loss: 0.000416, accuracy: 0.0243
average test loss: 0.000458, accuracy: 0.0251
case acc: 0.018453859
case acc: 0.04922662
case acc: 0.018295266
case acc: 0.016830958
case acc: 0.02431863
case acc: 0.023669725
top acc: 0.0314 ::: bot acc: 0.0067
top acc: 0.0628 ::: bot acc: 0.0371
top acc: 0.0333 ::: bot acc: 0.0087
top acc: 0.0292 ::: bot acc: 0.0077
top acc: 0.0384 ::: bot acc: 0.0112
top acc: 0.0399 ::: bot acc: 0.0085
current epoch: 21
train loss is 0.000688
average val loss: 0.000416, accuracy: 0.0247
average test loss: 0.000459, accuracy: 0.0254
case acc: 0.020085316
case acc: 0.047434755
case acc: 0.019073274
case acc: 0.017890286
case acc: 0.024090193
case acc: 0.023874741
top acc: 0.0334 ::: bot acc: 0.0078
top acc: 0.0613 ::: bot acc: 0.0352
top acc: 0.0352 ::: bot acc: 0.0077
top acc: 0.0307 ::: bot acc: 0.0079
top acc: 0.0383 ::: bot acc: 0.0107
top acc: 0.0403 ::: bot acc: 0.0083
current epoch: 22
train loss is 0.000678
average val loss: 0.000437, accuracy: 0.0258
average test loss: 0.000481, accuracy: 0.0264
case acc: 0.022141498
case acc: 0.046973255
case acc: 0.0204101
case acc: 0.019325042
case acc: 0.02483027
case acc: 0.02489334
top acc: 0.0358 ::: bot acc: 0.0094
top acc: 0.0606 ::: bot acc: 0.0348
top acc: 0.0376 ::: bot acc: 0.0067
top acc: 0.0326 ::: bot acc: 0.0083
top acc: 0.0389 ::: bot acc: 0.0115
top acc: 0.0413 ::: bot acc: 0.0093
current epoch: 23
train loss is 0.000655
average val loss: 0.000407, accuracy: 0.0250
average test loss: 0.000452, accuracy: 0.0256
case acc: 0.022225866
case acc: 0.04434887
case acc: 0.020707257
case acc: 0.01919565
case acc: 0.023455316
case acc: 0.023821918
top acc: 0.0359 ::: bot acc: 0.0095
top acc: 0.0580 ::: bot acc: 0.0320
top acc: 0.0381 ::: bot acc: 0.0068
top acc: 0.0326 ::: bot acc: 0.0083
top acc: 0.0375 ::: bot acc: 0.0103
top acc: 0.0403 ::: bot acc: 0.0085
current epoch: 24
train loss is 0.000604
average val loss: 0.000381, accuracy: 0.0242
average test loss: 0.000426, accuracy: 0.0248
case acc: 0.02215613
case acc: 0.042097367
case acc: 0.020964522
case acc: 0.018901354
case acc: 0.022266058
case acc: 0.022689974
top acc: 0.0357 ::: bot acc: 0.0093
top acc: 0.0557 ::: bot acc: 0.0300
top acc: 0.0384 ::: bot acc: 0.0066
top acc: 0.0323 ::: bot acc: 0.0081
top acc: 0.0363 ::: bot acc: 0.0092
top acc: 0.0391 ::: bot acc: 0.0076
current epoch: 25
train loss is 0.000599
average val loss: 0.000458, accuracy: 0.0272
average test loss: 0.000503, accuracy: 0.0276
case acc: 0.02587267
case acc: 0.044050235
case acc: 0.023731135
case acc: 0.021879828
case acc: 0.024760757
case acc: 0.025258074
top acc: 0.0400 ::: bot acc: 0.0122
top acc: 0.0578 ::: bot acc: 0.0319
top acc: 0.0428 ::: bot acc: 0.0066
top acc: 0.0362 ::: bot acc: 0.0092
top acc: 0.0389 ::: bot acc: 0.0116
top acc: 0.0418 ::: bot acc: 0.0095
current epoch: 26
train loss is 0.000618
average val loss: 0.000506, accuracy: 0.0290
average test loss: 0.000550, accuracy: 0.0293
case acc: 0.028176676
case acc: 0.044714727
case acc: 0.026211934
case acc: 0.023806067
case acc: 0.02602506
case acc: 0.02665353
top acc: 0.0422 ::: bot acc: 0.0143
top acc: 0.0582 ::: bot acc: 0.0326
top acc: 0.0457 ::: bot acc: 0.0081
top acc: 0.0383 ::: bot acc: 0.0104
top acc: 0.0403 ::: bot acc: 0.0127
top acc: 0.0434 ::: bot acc: 0.0108
current epoch: 27
train loss is 0.000601
average val loss: 0.000475, accuracy: 0.0280
average test loss: 0.000519, accuracy: 0.0283
case acc: 0.027796395
case acc: 0.042643793
case acc: 0.026296914
case acc: 0.023156237
case acc: 0.024989279
case acc: 0.024943171
top acc: 0.0419 ::: bot acc: 0.0140
top acc: 0.0564 ::: bot acc: 0.0305
top acc: 0.0458 ::: bot acc: 0.0080
top acc: 0.0377 ::: bot acc: 0.0098
top acc: 0.0390 ::: bot acc: 0.0118
top acc: 0.0415 ::: bot acc: 0.0092
current epoch: 28
train loss is 0.000570
average val loss: 0.000462, accuracy: 0.0276
average test loss: 0.000509, accuracy: 0.0281
case acc: 0.027736759
case acc: 0.041455787
case acc: 0.027007246
case acc: 0.023191644
case acc: 0.024841882
case acc: 0.024071194
top acc: 0.0418 ::: bot acc: 0.0138
top acc: 0.0552 ::: bot acc: 0.0292
top acc: 0.0465 ::: bot acc: 0.0088
top acc: 0.0378 ::: bot acc: 0.0101
top acc: 0.0389 ::: bot acc: 0.0116
top acc: 0.0405 ::: bot acc: 0.0086
current epoch: 29
train loss is 0.000573
average val loss: 0.000523, accuracy: 0.0298
average test loss: 0.000568, accuracy: 0.0300
case acc: 0.030311223
case acc: 0.04271071
case acc: 0.029470654
case acc: 0.025002185
case acc: 0.02703858
case acc: 0.0254225
top acc: 0.0445 ::: bot acc: 0.0163
top acc: 0.0564 ::: bot acc: 0.0306
top acc: 0.0495 ::: bot acc: 0.0101
top acc: 0.0398 ::: bot acc: 0.0112
top acc: 0.0413 ::: bot acc: 0.0136
top acc: 0.0419 ::: bot acc: 0.0097
current epoch: 30
train loss is 0.000557
average val loss: 0.000568, accuracy: 0.0312
average test loss: 0.000613, accuracy: 0.0315
case acc: 0.03215597
case acc: 0.043442104
case acc: 0.031739563
case acc: 0.026436822
case acc: 0.028684063
case acc: 0.026249195
top acc: 0.0463 ::: bot acc: 0.0180
top acc: 0.0569 ::: bot acc: 0.0314
top acc: 0.0520 ::: bot acc: 0.0120
top acc: 0.0415 ::: bot acc: 0.0122
top acc: 0.0428 ::: bot acc: 0.0154
top acc: 0.0429 ::: bot acc: 0.0104
current epoch: 31
train loss is 0.000550
average val loss: 0.000552, accuracy: 0.0308
average test loss: 0.000599, accuracy: 0.0311
case acc: 0.03180545
case acc: 0.042148996
case acc: 0.032115333
case acc: 0.026480444
case acc: 0.02840931
case acc: 0.025488472
top acc: 0.0461 ::: bot acc: 0.0177
top acc: 0.0558 ::: bot acc: 0.0300
top acc: 0.0525 ::: bot acc: 0.0124
top acc: 0.0414 ::: bot acc: 0.0124
top acc: 0.0426 ::: bot acc: 0.0152
top acc: 0.0421 ::: bot acc: 0.0097
current epoch: 32
train loss is 0.000544
average val loss: 0.000572, accuracy: 0.0314
average test loss: 0.000617, accuracy: 0.0317
case acc: 0.03266713
case acc: 0.041873816
case acc: 0.033232123
case acc: 0.02719981
case acc: 0.02923716
case acc: 0.025724603
top acc: 0.0470 ::: bot acc: 0.0184
top acc: 0.0555 ::: bot acc: 0.0297
top acc: 0.0537 ::: bot acc: 0.0132
top acc: 0.0423 ::: bot acc: 0.0127
top acc: 0.0435 ::: bot acc: 0.0159
top acc: 0.0421 ::: bot acc: 0.0100
current epoch: 33
train loss is 0.000554
average val loss: 0.000607, accuracy: 0.0326
average test loss: 0.000652, accuracy: 0.0327
case acc: 0.033879224
case acc: 0.042324413
case acc: 0.03493949
case acc: 0.028458137
case acc: 0.030161409
case acc: 0.02636632
top acc: 0.0481 ::: bot acc: 0.0197
top acc: 0.0561 ::: bot acc: 0.0301
top acc: 0.0556 ::: bot acc: 0.0146
top acc: 0.0437 ::: bot acc: 0.0137
top acc: 0.0444 ::: bot acc: 0.0167
top acc: 0.0429 ::: bot acc: 0.0105
current epoch: 34
train loss is 0.000539
average val loss: 0.000607, accuracy: 0.0326
average test loss: 0.000651, accuracy: 0.0327
case acc: 0.03365005
case acc: 0.0416733
case acc: 0.035763852
case acc: 0.028779205
case acc: 0.03007453
case acc: 0.026265813
top acc: 0.0480 ::: bot acc: 0.0193
top acc: 0.0555 ::: bot acc: 0.0294
top acc: 0.0563 ::: bot acc: 0.0156
top acc: 0.0441 ::: bot acc: 0.0141
top acc: 0.0442 ::: bot acc: 0.0168
top acc: 0.0428 ::: bot acc: 0.0105
current epoch: 35
train loss is 0.000548
average val loss: 0.000645, accuracy: 0.0337
average test loss: 0.000690, accuracy: 0.0339
case acc: 0.03484015
case acc: 0.042310968
case acc: 0.037552197
case acc: 0.030122252
case acc: 0.031180525
case acc: 0.027168948
top acc: 0.0491 ::: bot acc: 0.0206
top acc: 0.0559 ::: bot acc: 0.0301
top acc: 0.0582 ::: bot acc: 0.0172
top acc: 0.0456 ::: bot acc: 0.0152
top acc: 0.0453 ::: bot acc: 0.0179
top acc: 0.0437 ::: bot acc: 0.0112
current epoch: 36
train loss is 0.000547
average val loss: 0.000665, accuracy: 0.0343
average test loss: 0.000712, accuracy: 0.0345
case acc: 0.03520093
case acc: 0.042288817
case acc: 0.038593665
case acc: 0.031100783
case acc: 0.032154173
case acc: 0.027593505
top acc: 0.0495 ::: bot acc: 0.0210
top acc: 0.0560 ::: bot acc: 0.0300
top acc: 0.0593 ::: bot acc: 0.0180
top acc: 0.0467 ::: bot acc: 0.0158
top acc: 0.0463 ::: bot acc: 0.0187
top acc: 0.0443 ::: bot acc: 0.0116
current epoch: 37
train loss is 0.000530
average val loss: 0.000663, accuracy: 0.0343
average test loss: 0.000710, accuracy: 0.0345
case acc: 0.035139564
case acc: 0.041318115
case acc: 0.038776852
case acc: 0.03130337
case acc: 0.03277266
case acc: 0.027510796
top acc: 0.0495 ::: bot acc: 0.0208
top acc: 0.0550 ::: bot acc: 0.0293
top acc: 0.0594 ::: bot acc: 0.0183
top acc: 0.0468 ::: bot acc: 0.0160
top acc: 0.0470 ::: bot acc: 0.0193
top acc: 0.0440 ::: bot acc: 0.0116
current epoch: 38
train loss is 0.000524
average val loss: 0.000640, accuracy: 0.0337
average test loss: 0.000686, accuracy: 0.0338
case acc: 0.034340795
case acc: 0.039586995
case acc: 0.038271323
case acc: 0.030867834
case acc: 0.032873467
case acc: 0.026957078
top acc: 0.0487 ::: bot acc: 0.0201
top acc: 0.0531 ::: bot acc: 0.0275
top acc: 0.0589 ::: bot acc: 0.0178
top acc: 0.0465 ::: bot acc: 0.0157
top acc: 0.0471 ::: bot acc: 0.0194
top acc: 0.0435 ::: bot acc: 0.0110
current epoch: 39
train loss is 0.000510
average val loss: 0.000584, accuracy: 0.0320
average test loss: 0.000630, accuracy: 0.0322
case acc: 0.03228984
case acc: 0.037000343
case acc: 0.03681402
case acc: 0.029693482
case acc: 0.03188919
case acc: 0.025517264
top acc: 0.0466 ::: bot acc: 0.0182
top acc: 0.0507 ::: bot acc: 0.0248
top acc: 0.0573 ::: bot acc: 0.0165
top acc: 0.0451 ::: bot acc: 0.0148
top acc: 0.0460 ::: bot acc: 0.0186
top acc: 0.0420 ::: bot acc: 0.0096
current epoch: 40
train loss is 0.000467
average val loss: 0.000483, accuracy: 0.0287
average test loss: 0.000529, accuracy: 0.0290
case acc: 0.028679494
case acc: 0.033058733
case acc: 0.033708442
case acc: 0.027015487
case acc: 0.028888166
case acc: 0.02281814
top acc: 0.0429 ::: bot acc: 0.0147
top acc: 0.0466 ::: bot acc: 0.0209
top acc: 0.0541 ::: bot acc: 0.0137
top acc: 0.0423 ::: bot acc: 0.0127
top acc: 0.0430 ::: bot acc: 0.0156
top acc: 0.0390 ::: bot acc: 0.0076
current epoch: 41
train loss is 0.000424
average val loss: 0.000432, accuracy: 0.0270
average test loss: 0.000476, accuracy: 0.0273
case acc: 0.026498038
case acc: 0.030426273
case acc: 0.031972427
case acc: 0.025778525
case acc: 0.027504979
case acc: 0.02144551
top acc: 0.0405 ::: bot acc: 0.0128
top acc: 0.0440 ::: bot acc: 0.0184
top acc: 0.0521 ::: bot acc: 0.0124
top acc: 0.0409 ::: bot acc: 0.0116
top acc: 0.0418 ::: bot acc: 0.0140
top acc: 0.0375 ::: bot acc: 0.0067
current epoch: 42
train loss is 0.000406
average val loss: 0.000343, accuracy: 0.0236
average test loss: 0.000390, accuracy: 0.0241
case acc: 0.023101363
case acc: 0.026122678
case acc: 0.028708894
case acc: 0.023124702
case acc: 0.024707427
case acc: 0.018992795
top acc: 0.0370 ::: bot acc: 0.0100
top acc: 0.0396 ::: bot acc: 0.0142
top acc: 0.0487 ::: bot acc: 0.0097
top acc: 0.0375 ::: bot acc: 0.0101
top acc: 0.0388 ::: bot acc: 0.0113
top acc: 0.0346 ::: bot acc: 0.0052
current epoch: 43
train loss is 0.000380
average val loss: 0.000330, accuracy: 0.0231
average test loss: 0.000375, accuracy: 0.0236
case acc: 0.022016168
case acc: 0.024574023
case acc: 0.027919663
case acc: 0.023011008
case acc: 0.02479191
case acc: 0.019184085
top acc: 0.0356 ::: bot acc: 0.0092
top acc: 0.0380 ::: bot acc: 0.0126
top acc: 0.0479 ::: bot acc: 0.0090
top acc: 0.0375 ::: bot acc: 0.0099
top acc: 0.0388 ::: bot acc: 0.0116
top acc: 0.0348 ::: bot acc: 0.0053
current epoch: 44
train loss is 0.000362
average val loss: 0.000262, accuracy: 0.0202
average test loss: 0.000309, accuracy: 0.0209
case acc: 0.018947331
case acc: 0.020512301
case acc: 0.02468739
case acc: 0.020939777
case acc: 0.022657244
case acc: 0.017702164
top acc: 0.0321 ::: bot acc: 0.0071
top acc: 0.0339 ::: bot acc: 0.0090
top acc: 0.0439 ::: bot acc: 0.0070
top acc: 0.0349 ::: bot acc: 0.0090
top acc: 0.0367 ::: bot acc: 0.0095
top acc: 0.0326 ::: bot acc: 0.0053
current epoch: 45
train loss is 0.000337
average val loss: 0.000200, accuracy: 0.0171
average test loss: 0.000247, accuracy: 0.0182
case acc: 0.016097588
case acc: 0.016440574
case acc: 0.02168006
case acc: 0.01862702
case acc: 0.02036031
case acc: 0.016188815
top acc: 0.0282 ::: bot acc: 0.0062
top acc: 0.0292 ::: bot acc: 0.0061
top acc: 0.0399 ::: bot acc: 0.0062
top acc: 0.0318 ::: bot acc: 0.0081
top acc: 0.0341 ::: bot acc: 0.0078
top acc: 0.0302 ::: bot acc: 0.0053
current epoch: 46
train loss is 0.000312
average val loss: 0.000123, accuracy: 0.0127
average test loss: 0.000170, accuracy: 0.0145
case acc: 0.012398641
case acc: 0.011417548
case acc: 0.01822586
case acc: 0.014977406
case acc: 0.016043708
case acc: 0.013967266
top acc: 0.0220 ::: bot acc: 0.0078
top acc: 0.0224 ::: bot acc: 0.0047
top acc: 0.0335 ::: bot acc: 0.0088
top acc: 0.0265 ::: bot acc: 0.0077
top acc: 0.0290 ::: bot acc: 0.0051
top acc: 0.0254 ::: bot acc: 0.0086
current epoch: 47
train loss is 0.000294
average val loss: 0.000097, accuracy: 0.0111
average test loss: 0.000143, accuracy: 0.0132
case acc: 0.011141914
case acc: 0.009680331
case acc: 0.01682716
case acc: 0.013540653
case acc: 0.014531321
case acc: 0.013208173
top acc: 0.0185 ::: bot acc: 0.0107
top acc: 0.0183 ::: bot acc: 0.0075
top acc: 0.0294 ::: bot acc: 0.0125
top acc: 0.0239 ::: bot acc: 0.0088
top acc: 0.0271 ::: bot acc: 0.0047
top acc: 0.0234 ::: bot acc: 0.0102
current epoch: 48
train loss is 0.000296
average val loss: 0.000080, accuracy: 0.0099
average test loss: 0.000126, accuracy: 0.0123
case acc: 0.010352278
case acc: 0.0094139585
case acc: 0.015645038
case acc: 0.0123596145
case acc: 0.013081807
case acc: 0.012713635
top acc: 0.0147 ::: bot acc: 0.0146
top acc: 0.0134 ::: bot acc: 0.0122
top acc: 0.0248 ::: bot acc: 0.0172
top acc: 0.0208 ::: bot acc: 0.0113
top acc: 0.0247 ::: bot acc: 0.0047
top acc: 0.0213 ::: bot acc: 0.0123
current epoch: 49
train loss is 0.000301
average val loss: 0.000077, accuracy: 0.0096
average test loss: 0.000123, accuracy: 0.0122
case acc: 0.011458735
case acc: 0.011476536
case acc: 0.015241479
case acc: 0.011481947
case acc: 0.011231722
case acc: 0.012320159
top acc: 0.0096 ::: bot acc: 0.0199
top acc: 0.0079 ::: bot acc: 0.0186
top acc: 0.0184 ::: bot acc: 0.0236
top acc: 0.0162 ::: bot acc: 0.0158
top acc: 0.0212 ::: bot acc: 0.0066
top acc: 0.0179 ::: bot acc: 0.0157
current epoch: 50
train loss is 0.000318
average val loss: 0.000103, accuracy: 0.0113
average test loss: 0.000149, accuracy: 0.0138
case acc: 0.014554857
case acc: 0.01610327
case acc: 0.017297853
case acc: 0.01228703
case acc: 0.010244709
case acc: 0.012469511
top acc: 0.0073 ::: bot acc: 0.0257
top acc: 0.0072 ::: bot acc: 0.0259
top acc: 0.0111 ::: bot acc: 0.0311
top acc: 0.0112 ::: bot acc: 0.0209
top acc: 0.0176 ::: bot acc: 0.0100
top acc: 0.0145 ::: bot acc: 0.0191
LME_Co_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6792 6792 6792
1.7082474 -0.6288155 0.08104724 -0.08406281
Validation: 756 756 756
Testing: 774 774 774
pre-processing time: 0.0003294944763183594
the split date is 2010-07-01
train dropout: 0.6 test dropout: 0.05
net initializing with time: 0.002992391586303711
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012512
average val loss: 0.005465, accuracy: 0.0997
average test loss: 0.005547, accuracy: 0.0999
case acc: 0.14233157
case acc: 0.081733175
case acc: 0.099136636
case acc: 0.09114934
case acc: 0.1291602
case acc: 0.05606079
top acc: 0.1259 ::: bot acc: 0.1588
top acc: 0.0957 ::: bot acc: 0.0683
top acc: 0.0764 ::: bot acc: 0.1229
top acc: 0.0736 ::: bot acc: 0.1072
top acc: 0.1096 ::: bot acc: 0.1478
top acc: 0.0390 ::: bot acc: 0.0726
current epoch: 2
train loss is 0.007901
average val loss: 0.002732, accuracy: 0.0501
average test loss: 0.002781, accuracy: 0.0513
case acc: 0.041304715
case acc: 0.16559272
case acc: 0.017531313
case acc: 0.012409386
case acc: 0.043358915
case acc: 0.027758421
top acc: 0.0258 ::: bot acc: 0.0571
top acc: 0.1794 ::: bot acc: 0.1523
top acc: 0.0246 ::: bot acc: 0.0229
top acc: 0.0193 ::: bot acc: 0.0141
top acc: 0.0252 ::: bot acc: 0.0608
top acc: 0.0448 ::: bot acc: 0.0118
current epoch: 3
train loss is 0.008272
average val loss: 0.007586, accuracy: 0.1031
average test loss: 0.007647, accuracy: 0.1031
case acc: 0.051536664
case acc: 0.24370995
case acc: 0.09410661
case acc: 0.08656417
case acc: 0.03968666
case acc: 0.10322184
top acc: 0.0672 ::: bot acc: 0.0360
top acc: 0.2575 ::: bot acc: 0.2309
top acc: 0.1172 ::: bot acc: 0.0704
top acc: 0.1043 ::: bot acc: 0.0705
top acc: 0.0594 ::: bot acc: 0.0215
top acc: 0.1206 ::: bot acc: 0.0871
current epoch: 4
train loss is 0.009819
average val loss: 0.016493, accuracy: 0.1705
average test loss: 0.016513, accuracy: 0.1703
case acc: 0.12622242
case acc: 0.30447918
case acc: 0.1675884
case acc: 0.152995
case acc: 0.10868251
case acc: 0.16213134
top acc: 0.1422 ::: bot acc: 0.1104
top acc: 0.3185 ::: bot acc: 0.2912
top acc: 0.1905 ::: bot acc: 0.1442
top acc: 0.1702 ::: bot acc: 0.1370
top acc: 0.1288 ::: bot acc: 0.0896
top acc: 0.1791 ::: bot acc: 0.1461
current epoch: 5
train loss is 0.012816
average val loss: 0.013073, accuracy: 0.1508
average test loss: 0.013115, accuracy: 0.1508
case acc: 0.11192862
case acc: 0.27512464
case acc: 0.15073232
case acc: 0.13235006
case acc: 0.096976936
case acc: 0.13766353
top acc: 0.1279 ::: bot acc: 0.0961
top acc: 0.2888 ::: bot acc: 0.2626
top acc: 0.1733 ::: bot acc: 0.1273
top acc: 0.1497 ::: bot acc: 0.1164
top acc: 0.1171 ::: bot acc: 0.0784
top acc: 0.1548 ::: bot acc: 0.1217
current epoch: 6
train loss is 0.012546
average val loss: 0.002965, accuracy: 0.0561
average test loss: 0.002997, accuracy: 0.0566
case acc: 0.021941844
case acc: 0.16644426
case acc: 0.05453633
case acc: 0.036956735
case acc: 0.017710565
case acc: 0.042028036
top acc: 0.0366 ::: bot acc: 0.0087
top acc: 0.1800 ::: bot acc: 0.1534
top acc: 0.0770 ::: bot acc: 0.0322
top acc: 0.0540 ::: bot acc: 0.0214
top acc: 0.0334 ::: bot acc: 0.0072
top acc: 0.0592 ::: bot acc: 0.0260
current epoch: 7
train loss is 0.006966
average val loss: 0.001099, accuracy: 0.0354
average test loss: 0.001136, accuracy: 0.0362
case acc: 0.03281302
case acc: 0.09690179
case acc: 0.01774351
case acc: 0.022227831
case acc: 0.031557407
case acc: 0.01587028
top acc: 0.0177 ::: bot acc: 0.0480
top acc: 0.1105 ::: bot acc: 0.0839
top acc: 0.0159 ::: bot acc: 0.0306
top acc: 0.0112 ::: bot acc: 0.0351
top acc: 0.0149 ::: bot acc: 0.0484
top acc: 0.0082 ::: bot acc: 0.0272
current epoch: 8
train loss is 0.003088
average val loss: 0.000930, accuracy: 0.0286
average test loss: 0.000967, accuracy: 0.0300
case acc: 0.022057578
case acc: 0.09617019
case acc: 0.016991237
case acc: 0.014861408
case acc: 0.017805558
case acc: 0.0122852605
top acc: 0.0103 ::: bot acc: 0.0354
top acc: 0.1098 ::: bot acc: 0.0833
top acc: 0.0222 ::: bot acc: 0.0243
top acc: 0.0111 ::: bot acc: 0.0241
top acc: 0.0109 ::: bot acc: 0.0300
top acc: 0.0196 ::: bot acc: 0.0138
current epoch: 9
train loss is 0.002196
average val loss: 0.001029, accuracy: 0.0292
average test loss: 0.001066, accuracy: 0.0303
case acc: 0.011825905
case acc: 0.102047555
case acc: 0.019559154
case acc: 0.013272246
case acc: 0.015567471
case acc: 0.019770231
top acc: 0.0135 ::: bot acc: 0.0183
top acc: 0.1158 ::: bot acc: 0.0892
top acc: 0.0344 ::: bot acc: 0.0125
top acc: 0.0250 ::: bot acc: 0.0092
top acc: 0.0295 ::: bot acc: 0.0096
top acc: 0.0359 ::: bot acc: 0.0061
current epoch: 10
train loss is 0.002078
average val loss: 0.000939, accuracy: 0.0291
average test loss: 0.000979, accuracy: 0.0300
case acc: 0.011503898
case acc: 0.09564066
case acc: 0.019464348
case acc: 0.013824676
case acc: 0.01794937
case acc: 0.021558667
top acc: 0.0163 ::: bot acc: 0.0154
top acc: 0.1096 ::: bot acc: 0.0826
top acc: 0.0342 ::: bot acc: 0.0128
top acc: 0.0266 ::: bot acc: 0.0078
top acc: 0.0343 ::: bot acc: 0.0071
top acc: 0.0377 ::: bot acc: 0.0073
current epoch: 11
train loss is 0.001899
average val loss: 0.000800, accuracy: 0.0274
average test loss: 0.000839, accuracy: 0.0283
case acc: 0.011518353
case acc: 0.08679081
case acc: 0.018571325
case acc: 0.0133348415
case acc: 0.018770004
case acc: 0.020875972
top acc: 0.0168 ::: bot acc: 0.0150
top acc: 0.1005 ::: bot acc: 0.0738
top acc: 0.0308 ::: bot acc: 0.0157
top acc: 0.0256 ::: bot acc: 0.0086
top acc: 0.0358 ::: bot acc: 0.0068
top acc: 0.0373 ::: bot acc: 0.0066
current epoch: 12
train loss is 0.001627
average val loss: 0.000743, accuracy: 0.0275
average test loss: 0.000782, accuracy: 0.0283
case acc: 0.011891663
case acc: 0.08110074
case acc: 0.018570542
case acc: 0.014212453
case acc: 0.021428578
case acc: 0.022654327
top acc: 0.0194 ::: bot acc: 0.0125
top acc: 0.0947 ::: bot acc: 0.0681
top acc: 0.0309 ::: bot acc: 0.0159
top acc: 0.0273 ::: bot acc: 0.0074
top acc: 0.0397 ::: bot acc: 0.0070
top acc: 0.0392 ::: bot acc: 0.0078
current epoch: 13
train loss is 0.001400
average val loss: 0.000649, accuracy: 0.0263
average test loss: 0.000682, accuracy: 0.0269
case acc: 0.011917223
case acc: 0.073540874
case acc: 0.017998623
case acc: 0.013983699
case acc: 0.022009699
case acc: 0.022190217
top acc: 0.0200 ::: bot acc: 0.0118
top acc: 0.0872 ::: bot acc: 0.0604
top acc: 0.0290 ::: bot acc: 0.0176
top acc: 0.0269 ::: bot acc: 0.0074
top acc: 0.0402 ::: bot acc: 0.0073
top acc: 0.0388 ::: bot acc: 0.0074
current epoch: 14
train loss is 0.001198
average val loss: 0.000563, accuracy: 0.0250
average test loss: 0.000598, accuracy: 0.0257
case acc: 0.01200734
case acc: 0.06667359
case acc: 0.017727278
case acc: 0.013843477
case acc: 0.022025466
case acc: 0.021708004
top acc: 0.0205 ::: bot acc: 0.0110
top acc: 0.0804 ::: bot acc: 0.0537
top acc: 0.0276 ::: bot acc: 0.0193
top acc: 0.0263 ::: bot acc: 0.0079
top acc: 0.0402 ::: bot acc: 0.0075
top acc: 0.0380 ::: bot acc: 0.0071
current epoch: 15
train loss is 0.001026
average val loss: 0.000512, accuracy: 0.0243
average test loss: 0.000547, accuracy: 0.0249
case acc: 0.012503517
case acc: 0.06155425
case acc: 0.017510835
case acc: 0.014037421
case acc: 0.022304587
case acc: 0.021759672
top acc: 0.0221 ::: bot acc: 0.0099
top acc: 0.0752 ::: bot acc: 0.0485
top acc: 0.0270 ::: bot acc: 0.0196
top acc: 0.0270 ::: bot acc: 0.0077
top acc: 0.0406 ::: bot acc: 0.0076
top acc: 0.0383 ::: bot acc: 0.0068
current epoch: 16
train loss is 0.000943
average val loss: 0.000528, accuracy: 0.0255
average test loss: 0.000566, accuracy: 0.0262
case acc: 0.01421132
case acc: 0.05993229
case acc: 0.01823496
case acc: 0.015714372
case acc: 0.024608098
case acc: 0.024212377
top acc: 0.0259 ::: bot acc: 0.0072
top acc: 0.0738 ::: bot acc: 0.0468
top acc: 0.0300 ::: bot acc: 0.0166
top acc: 0.0302 ::: bot acc: 0.0061
top acc: 0.0435 ::: bot acc: 0.0088
top acc: 0.0414 ::: bot acc: 0.0089
current epoch: 17
train loss is 0.000883
average val loss: 0.000539, accuracy: 0.0266
average test loss: 0.000574, accuracy: 0.0270
case acc: 0.015958095
case acc: 0.05798184
case acc: 0.018799571
case acc: 0.017254252
case acc: 0.026090715
case acc: 0.025734931
top acc: 0.0291 ::: bot acc: 0.0063
top acc: 0.0718 ::: bot acc: 0.0451
top acc: 0.0322 ::: bot acc: 0.0143
top acc: 0.0325 ::: bot acc: 0.0062
top acc: 0.0452 ::: bot acc: 0.0097
top acc: 0.0427 ::: bot acc: 0.0101
current epoch: 18
train loss is 0.000803
average val loss: 0.000508, accuracy: 0.0261
average test loss: 0.000542, accuracy: 0.0265
case acc: 0.016882734
case acc: 0.05454485
case acc: 0.018957604
case acc: 0.017650694
case acc: 0.02541778
case acc: 0.02547038
top acc: 0.0303 ::: bot acc: 0.0065
top acc: 0.0685 ::: bot acc: 0.0416
top acc: 0.0329 ::: bot acc: 0.0135
top acc: 0.0333 ::: bot acc: 0.0058
top acc: 0.0442 ::: bot acc: 0.0096
top acc: 0.0425 ::: bot acc: 0.0099
current epoch: 19
train loss is 0.000765
average val loss: 0.000473, accuracy: 0.0254
average test loss: 0.000510, accuracy: 0.0259
case acc: 0.017345576
case acc: 0.051186945
case acc: 0.019373612
case acc: 0.01786243
case acc: 0.024564702
case acc: 0.02481422
top acc: 0.0311 ::: bot acc: 0.0064
top acc: 0.0648 ::: bot acc: 0.0383
top acc: 0.0338 ::: bot acc: 0.0136
top acc: 0.0336 ::: bot acc: 0.0060
top acc: 0.0434 ::: bot acc: 0.0089
top acc: 0.0415 ::: bot acc: 0.0093
current epoch: 20
train loss is 0.000699
average val loss: 0.000468, accuracy: 0.0255
average test loss: 0.000504, accuracy: 0.0260
case acc: 0.018603923
case acc: 0.049254056
case acc: 0.019815061
case acc: 0.01882053
case acc: 0.024343159
case acc: 0.025005514
top acc: 0.0329 ::: bot acc: 0.0066
top acc: 0.0629 ::: bot acc: 0.0363
top acc: 0.0353 ::: bot acc: 0.0119
top acc: 0.0345 ::: bot acc: 0.0066
top acc: 0.0432 ::: bot acc: 0.0088
top acc: 0.0419 ::: bot acc: 0.0096
current epoch: 21
train loss is 0.000689
average val loss: 0.000465, accuracy: 0.0257
average test loss: 0.000500, accuracy: 0.0261
case acc: 0.019799085
case acc: 0.047483068
case acc: 0.020612897
case acc: 0.019770067
case acc: 0.024037605
case acc: 0.025100397
top acc: 0.0343 ::: bot acc: 0.0073
top acc: 0.0613 ::: bot acc: 0.0344
top acc: 0.0369 ::: bot acc: 0.0110
top acc: 0.0358 ::: bot acc: 0.0072
top acc: 0.0428 ::: bot acc: 0.0087
top acc: 0.0422 ::: bot acc: 0.0095
current epoch: 22
train loss is 0.000650
average val loss: 0.000522, accuracy: 0.0278
average test loss: 0.000557, accuracy: 0.0282
case acc: 0.02285643
case acc: 0.048427615
case acc: 0.02270232
case acc: 0.02245059
case acc: 0.025681775
case acc: 0.027006796
top acc: 0.0380 ::: bot acc: 0.0090
top acc: 0.0622 ::: bot acc: 0.0354
top acc: 0.0407 ::: bot acc: 0.0097
top acc: 0.0390 ::: bot acc: 0.0088
top acc: 0.0449 ::: bot acc: 0.0094
top acc: 0.0441 ::: bot acc: 0.0110
current epoch: 23
train loss is 0.000647
average val loss: 0.000548, accuracy: 0.0288
average test loss: 0.000582, accuracy: 0.0292
case acc: 0.024984067
case acc: 0.047841076
case acc: 0.024433982
case acc: 0.023840796
case acc: 0.02645597
case acc: 0.027502006
top acc: 0.0405 ::: bot acc: 0.0105
top acc: 0.0617 ::: bot acc: 0.0347
top acc: 0.0433 ::: bot acc: 0.0097
top acc: 0.0406 ::: bot acc: 0.0100
top acc: 0.0459 ::: bot acc: 0.0099
top acc: 0.0446 ::: bot acc: 0.0116
current epoch: 24
train loss is 0.000603
average val loss: 0.000488, accuracy: 0.0270
average test loss: 0.000525, accuracy: 0.0275
case acc: 0.024096288
case acc: 0.04469599
case acc: 0.02402741
case acc: 0.022459792
case acc: 0.024418417
case acc: 0.025072824
top acc: 0.0395 ::: bot acc: 0.0098
top acc: 0.0584 ::: bot acc: 0.0317
top acc: 0.0427 ::: bot acc: 0.0098
top acc: 0.0390 ::: bot acc: 0.0087
top acc: 0.0433 ::: bot acc: 0.0087
top acc: 0.0421 ::: bot acc: 0.0096
current epoch: 25
train loss is 0.000572
average val loss: 0.000540, accuracy: 0.0288
average test loss: 0.000573, accuracy: 0.0291
case acc: 0.026549185
case acc: 0.045610208
case acc: 0.026200034
case acc: 0.02458352
case acc: 0.025692387
case acc: 0.02608587
top acc: 0.0422 ::: bot acc: 0.0119
top acc: 0.0594 ::: bot acc: 0.0326
top acc: 0.0459 ::: bot acc: 0.0097
top acc: 0.0412 ::: bot acc: 0.0105
top acc: 0.0449 ::: bot acc: 0.0095
top acc: 0.0432 ::: bot acc: 0.0103
current epoch: 26
train loss is 0.000566
average val loss: 0.000542, accuracy: 0.0290
average test loss: 0.000580, accuracy: 0.0294
case acc: 0.02745934
case acc: 0.04502605
case acc: 0.027506802
case acc: 0.02532945
case acc: 0.025320165
case acc: 0.025788538
top acc: 0.0432 ::: bot acc: 0.0126
top acc: 0.0585 ::: bot acc: 0.0321
top acc: 0.0476 ::: bot acc: 0.0103
top acc: 0.0421 ::: bot acc: 0.0112
top acc: 0.0445 ::: bot acc: 0.0092
top acc: 0.0428 ::: bot acc: 0.0101
current epoch: 27
train loss is 0.000566
average val loss: 0.000602, accuracy: 0.0309
average test loss: 0.000636, accuracy: 0.0312
case acc: 0.029848099
case acc: 0.046129987
case acc: 0.030037945
case acc: 0.027632665
case acc: 0.026550695
case acc: 0.027013835
top acc: 0.0456 ::: bot acc: 0.0147
top acc: 0.0599 ::: bot acc: 0.0330
top acc: 0.0509 ::: bot acc: 0.0116
top acc: 0.0446 ::: bot acc: 0.0132
top acc: 0.0458 ::: bot acc: 0.0101
top acc: 0.0440 ::: bot acc: 0.0112
current epoch: 28
train loss is 0.000585
average val loss: 0.000684, accuracy: 0.0334
average test loss: 0.000719, accuracy: 0.0337
case acc: 0.03291113
case acc: 0.047932647
case acc: 0.0334613
case acc: 0.030365335
case acc: 0.028508313
case acc: 0.029035574
top acc: 0.0488 ::: bot acc: 0.0176
top acc: 0.0616 ::: bot acc: 0.0350
top acc: 0.0547 ::: bot acc: 0.0140
top acc: 0.0475 ::: bot acc: 0.0156
top acc: 0.0481 ::: bot acc: 0.0116
top acc: 0.0463 ::: bot acc: 0.0130
current epoch: 29
train loss is 0.000579
average val loss: 0.000783, accuracy: 0.0362
average test loss: 0.000816, accuracy: 0.0364
case acc: 0.03618773
case acc: 0.049886
case acc: 0.03693546
case acc: 0.03328072
case acc: 0.031273566
case acc: 0.031027475
top acc: 0.0522 ::: bot acc: 0.0207
top acc: 0.0637 ::: bot acc: 0.0367
top acc: 0.0586 ::: bot acc: 0.0167
top acc: 0.0506 ::: bot acc: 0.0183
top acc: 0.0510 ::: bot acc: 0.0138
top acc: 0.0483 ::: bot acc: 0.0148
current epoch: 30
train loss is 0.000608
average val loss: 0.000811, accuracy: 0.0370
average test loss: 0.000845, accuracy: 0.0372
case acc: 0.037228104
case acc: 0.04982894
case acc: 0.038482945
case acc: 0.034229428
case acc: 0.032271076
case acc: 0.03127245
top acc: 0.0534 ::: bot acc: 0.0215
top acc: 0.0636 ::: bot acc: 0.0367
top acc: 0.0603 ::: bot acc: 0.0179
top acc: 0.0514 ::: bot acc: 0.0193
top acc: 0.0521 ::: bot acc: 0.0146
top acc: 0.0484 ::: bot acc: 0.0151
current epoch: 31
train loss is 0.000601
average val loss: 0.000833, accuracy: 0.0376
average test loss: 0.000869, accuracy: 0.0379
case acc: 0.038075402
case acc: 0.049669016
case acc: 0.039935574
case acc: 0.034961965
case acc: 0.033140667
case acc: 0.03144698
top acc: 0.0542 ::: bot acc: 0.0225
top acc: 0.0633 ::: bot acc: 0.0366
top acc: 0.0617 ::: bot acc: 0.0193
top acc: 0.0523 ::: bot acc: 0.0198
top acc: 0.0531 ::: bot acc: 0.0155
top acc: 0.0486 ::: bot acc: 0.0153
current epoch: 32
train loss is 0.000610
average val loss: 0.000872, accuracy: 0.0386
average test loss: 0.000905, accuracy: 0.0388
case acc: 0.03900175
case acc: 0.0498229
case acc: 0.04152608
case acc: 0.03623225
case acc: 0.034335256
case acc: 0.031864706
top acc: 0.0552 ::: bot acc: 0.0233
top acc: 0.0635 ::: bot acc: 0.0368
top acc: 0.0634 ::: bot acc: 0.0206
top acc: 0.0536 ::: bot acc: 0.0210
top acc: 0.0544 ::: bot acc: 0.0163
top acc: 0.0491 ::: bot acc: 0.0156
current epoch: 33
train loss is 0.000599
average val loss: 0.000905, accuracy: 0.0395
average test loss: 0.000938, accuracy: 0.0397
case acc: 0.03991923
case acc: 0.049666572
case acc: 0.042905364
case acc: 0.03738511
case acc: 0.03555813
case acc: 0.032503594
top acc: 0.0561 ::: bot acc: 0.0243
top acc: 0.0635 ::: bot acc: 0.0367
top acc: 0.0649 ::: bot acc: 0.0218
top acc: 0.0547 ::: bot acc: 0.0222
top acc: 0.0557 ::: bot acc: 0.0173
top acc: 0.0498 ::: bot acc: 0.0162
current epoch: 34
train loss is 0.000601
average val loss: 0.000892, accuracy: 0.0393
average test loss: 0.000926, accuracy: 0.0394
case acc: 0.03957702
case acc: 0.04837561
case acc: 0.042980187
case acc: 0.037425514
case acc: 0.035920016
case acc: 0.03205458
top acc: 0.0558 ::: bot acc: 0.0238
top acc: 0.0620 ::: bot acc: 0.0353
top acc: 0.0650 ::: bot acc: 0.0219
top acc: 0.0549 ::: bot acc: 0.0222
top acc: 0.0562 ::: bot acc: 0.0176
top acc: 0.0494 ::: bot acc: 0.0159
current epoch: 35
train loss is 0.000585
average val loss: 0.000838, accuracy: 0.0379
average test loss: 0.000873, accuracy: 0.0381
case acc: 0.03807665
case acc: 0.045946535
case acc: 0.042045034
case acc: 0.036453456
case acc: 0.03510127
case acc: 0.030869704
top acc: 0.0543 ::: bot acc: 0.0223
top acc: 0.0595 ::: bot acc: 0.0329
top acc: 0.0640 ::: bot acc: 0.0210
top acc: 0.0538 ::: bot acc: 0.0212
top acc: 0.0553 ::: bot acc: 0.0169
top acc: 0.0481 ::: bot acc: 0.0146
current epoch: 36
train loss is 0.000556
average val loss: 0.000813, accuracy: 0.0373
average test loss: 0.000845, accuracy: 0.0374
case acc: 0.036947384
case acc: 0.04421796
case acc: 0.041558616
case acc: 0.03627766
case acc: 0.034914568
case acc: 0.03056745
top acc: 0.0530 ::: bot acc: 0.0213
top acc: 0.0579 ::: bot acc: 0.0312
top acc: 0.0635 ::: bot acc: 0.0206
top acc: 0.0536 ::: bot acc: 0.0211
top acc: 0.0551 ::: bot acc: 0.0167
top acc: 0.0478 ::: bot acc: 0.0144
current epoch: 37
train loss is 0.000518
average val loss: 0.000670, accuracy: 0.0334
average test loss: 0.000705, accuracy: 0.0336
case acc: 0.03269297
case acc: 0.039134033
case acc: 0.03785725
case acc: 0.032908406
case acc: 0.03177872
case acc: 0.027258303
top acc: 0.0487 ::: bot acc: 0.0173
top acc: 0.0529 ::: bot acc: 0.0260
top acc: 0.0596 ::: bot acc: 0.0175
top acc: 0.0502 ::: bot acc: 0.0179
top acc: 0.0516 ::: bot acc: 0.0142
top acc: 0.0444 ::: bot acc: 0.0113
current epoch: 38
train loss is 0.000463
average val loss: 0.000546, accuracy: 0.0296
average test loss: 0.000581, accuracy: 0.0299
case acc: 0.028729841
case acc: 0.03402315
case acc: 0.03405313
case acc: 0.029543111
case acc: 0.02903556
case acc: 0.024031557
top acc: 0.0445 ::: bot acc: 0.0137
top acc: 0.0477 ::: bot acc: 0.0211
top acc: 0.0555 ::: bot acc: 0.0143
top acc: 0.0468 ::: bot acc: 0.0149
top acc: 0.0486 ::: bot acc: 0.0119
top acc: 0.0408 ::: bot acc: 0.0086
current epoch: 39
train loss is 0.000418
average val loss: 0.000462, accuracy: 0.0268
average test loss: 0.000496, accuracy: 0.0271
case acc: 0.02554654
case acc: 0.029803952
case acc: 0.031244108
case acc: 0.027003573
case acc: 0.027180213
case acc: 0.021792168
top acc: 0.0412 ::: bot acc: 0.0109
top acc: 0.0435 ::: bot acc: 0.0168
top acc: 0.0521 ::: bot acc: 0.0125
top acc: 0.0439 ::: bot acc: 0.0128
top acc: 0.0465 ::: bot acc: 0.0105
top acc: 0.0385 ::: bot acc: 0.0071
current epoch: 40
train loss is 0.000386
average val loss: 0.000433, accuracy: 0.0258
average test loss: 0.000466, accuracy: 0.0261
case acc: 0.024215516
case acc: 0.027841693
case acc: 0.030020155
case acc: 0.026312126
case acc: 0.026790697
case acc: 0.021335863
top acc: 0.0397 ::: bot acc: 0.0099
top acc: 0.0415 ::: bot acc: 0.0149
top acc: 0.0508 ::: bot acc: 0.0116
top acc: 0.0432 ::: bot acc: 0.0122
top acc: 0.0461 ::: bot acc: 0.0103
top acc: 0.0377 ::: bot acc: 0.0068
current epoch: 41
train loss is 0.000361
average val loss: 0.000350, accuracy: 0.0227
average test loss: 0.000385, accuracy: 0.0231
case acc: 0.020879276
case acc: 0.023411386
case acc: 0.027007628
case acc: 0.023575442
case acc: 0.024342425
case acc: 0.019182205
top acc: 0.0358 ::: bot acc: 0.0076
top acc: 0.0370 ::: bot acc: 0.0106
top acc: 0.0470 ::: bot acc: 0.0100
top acc: 0.0402 ::: bot acc: 0.0098
top acc: 0.0431 ::: bot acc: 0.0088
top acc: 0.0350 ::: bot acc: 0.0056
current epoch: 42
train loss is 0.000325
average val loss: 0.000257, accuracy: 0.0187
average test loss: 0.000292, accuracy: 0.0192
case acc: 0.016782694
case acc: 0.017924681
case acc: 0.023374919
case acc: 0.019867765
case acc: 0.020840446
case acc: 0.016501687
top acc: 0.0303 ::: bot acc: 0.0063
top acc: 0.0311 ::: bot acc: 0.0060
top acc: 0.0417 ::: bot acc: 0.0097
top acc: 0.0360 ::: bot acc: 0.0072
top acc: 0.0388 ::: bot acc: 0.0070
top acc: 0.0314 ::: bot acc: 0.0051
current epoch: 43
train loss is 0.000298
average val loss: 0.000193, accuracy: 0.0157
average test loss: 0.000230, accuracy: 0.0165
case acc: 0.0140134785
case acc: 0.014049859
case acc: 0.020599363
case acc: 0.0170072
case acc: 0.018397756
case acc: 0.014715487
top acc: 0.0258 ::: bot acc: 0.0072
top acc: 0.0260 ::: bot acc: 0.0046
top acc: 0.0369 ::: bot acc: 0.0111
top acc: 0.0324 ::: bot acc: 0.0059
top acc: 0.0351 ::: bot acc: 0.0069
top acc: 0.0280 ::: bot acc: 0.0063
current epoch: 44
train loss is 0.000282
average val loss: 0.000164, accuracy: 0.0142
average test loss: 0.000200, accuracy: 0.0150
case acc: 0.012798185
case acc: 0.011776802
case acc: 0.019096404
case acc: 0.015357912
case acc: 0.01723522
case acc: 0.0139435455
top acc: 0.0228 ::: bot acc: 0.0096
top acc: 0.0222 ::: bot acc: 0.0054
top acc: 0.0334 ::: bot acc: 0.0134
top acc: 0.0299 ::: bot acc: 0.0060
top acc: 0.0333 ::: bot acc: 0.0072
top acc: 0.0262 ::: bot acc: 0.0076
current epoch: 45
train loss is 0.000273
average val loss: 0.000136, accuracy: 0.0128
average test loss: 0.000173, accuracy: 0.0138
case acc: 0.011727027
case acc: 0.010274857
case acc: 0.017998278
case acc: 0.013760016
case acc: 0.01589923
case acc: 0.013036081
top acc: 0.0189 ::: bot acc: 0.0133
top acc: 0.0179 ::: bot acc: 0.0090
top acc: 0.0291 ::: bot acc: 0.0179
top acc: 0.0268 ::: bot acc: 0.0073
top acc: 0.0304 ::: bot acc: 0.0089
top acc: 0.0238 ::: bot acc: 0.0095
current epoch: 46
train loss is 0.000266
average val loss: 0.000118, accuracy: 0.0118
average test loss: 0.000155, accuracy: 0.0130
case acc: 0.012078854
case acc: 0.010214495
case acc: 0.016964687
case acc: 0.012257711
case acc: 0.014408988
case acc: 0.01222263
top acc: 0.0132 ::: bot acc: 0.0192
top acc: 0.0111 ::: bot acc: 0.0157
top acc: 0.0227 ::: bot acc: 0.0242
top acc: 0.0220 ::: bot acc: 0.0116
top acc: 0.0255 ::: bot acc: 0.0135
top acc: 0.0198 ::: bot acc: 0.0136
current epoch: 47
train loss is 0.000273
average val loss: 0.000121, accuracy: 0.0121
average test loss: 0.000159, accuracy: 0.0134
case acc: 0.013282181
case acc: 0.011494475
case acc: 0.017277917
case acc: 0.01214003
case acc: 0.013890237
case acc: 0.012150916
top acc: 0.0106 ::: bot acc: 0.0223
top acc: 0.0073 ::: bot acc: 0.0196
top acc: 0.0185 ::: bot acc: 0.0282
top acc: 0.0195 ::: bot acc: 0.0141
top acc: 0.0233 ::: bot acc: 0.0153
top acc: 0.0181 ::: bot acc: 0.0154
current epoch: 48
train loss is 0.000287
average val loss: 0.000138, accuracy: 0.0132
average test loss: 0.000177, accuracy: 0.0145
case acc: 0.01540444
case acc: 0.01420435
case acc: 0.018836342
case acc: 0.012416651
case acc: 0.013845437
case acc: 0.0122222975
top acc: 0.0093 ::: bot acc: 0.0262
top acc: 0.0053 ::: bot acc: 0.0247
top acc: 0.0136 ::: bot acc: 0.0333
top acc: 0.0162 ::: bot acc: 0.0171
top acc: 0.0210 ::: bot acc: 0.0178
top acc: 0.0161 ::: bot acc: 0.0172
current epoch: 49
train loss is 0.000308
average val loss: 0.000188, accuracy: 0.0156
average test loss: 0.000228, accuracy: 0.0171
case acc: 0.01919537
case acc: 0.019601954
case acc: 0.022296147
case acc: 0.014089014
case acc: 0.014600808
case acc: 0.012930451
top acc: 0.0097 ::: bot acc: 0.0318
top acc: 0.0080 ::: bot acc: 0.0314
top acc: 0.0096 ::: bot acc: 0.0406
top acc: 0.0117 ::: bot acc: 0.0222
top acc: 0.0173 ::: bot acc: 0.0217
top acc: 0.0130 ::: bot acc: 0.0205
current epoch: 50
train loss is 0.000344
average val loss: 0.000302, accuracy: 0.0205
average test loss: 0.000343, accuracy: 0.0219
case acc: 0.025012434
case acc: 0.02769943
case acc: 0.029171513
case acc: 0.017992813
case acc: 0.016572682
case acc: 0.015030909
top acc: 0.0120 ::: bot acc: 0.0392
top acc: 0.0144 ::: bot acc: 0.0405
top acc: 0.0117 ::: bot acc: 0.0498
top acc: 0.0098 ::: bot acc: 0.0289
top acc: 0.0122 ::: bot acc: 0.0273
top acc: 0.0091 ::: bot acc: 0.0257
LME_Co_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6810 6810 6810
1.7082474 -0.6288155 0.08104724 -0.08406281
Validation: 762 762 762
Testing: 750 750 750
pre-processing time: 0.0001971721649169922
the split date is 2011-01-01
train dropout: 0.6 test dropout: 0.05
net initializing with time: 0.0023016929626464844
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012565
average val loss: 0.005287, accuracy: 0.0974
average test loss: 0.005013, accuracy: 0.0952
case acc: 0.13562655
case acc: 0.08721649
case acc: 0.09011238
case acc: 0.08570775
case acc: 0.12052531
case acc: 0.052117858
top acc: 0.1213 ::: bot acc: 0.1495
top acc: 0.1003 ::: bot acc: 0.0738
top acc: 0.0693 ::: bot acc: 0.1108
top acc: 0.0727 ::: bot acc: 0.0980
top acc: 0.1004 ::: bot acc: 0.1410
top acc: 0.0343 ::: bot acc: 0.0696
current epoch: 2
train loss is 0.007716
average val loss: 0.002932, accuracy: 0.0518
average test loss: 0.002927, accuracy: 0.0508
case acc: 0.031730738
case acc: 0.17416304
case acc: 0.019383466
case acc: 0.013646267
case acc: 0.030987028
case acc: 0.034854855
top acc: 0.0175 ::: bot acc: 0.0457
top acc: 0.1863 ::: bot acc: 0.1611
top acc: 0.0340 ::: bot acc: 0.0111
top acc: 0.0236 ::: bot acc: 0.0076
top acc: 0.0125 ::: bot acc: 0.0508
top acc: 0.0524 ::: bot acc: 0.0180
current epoch: 3
train loss is 0.008159
average val loss: 0.008228, accuracy: 0.1089
average test loss: 0.008480, accuracy: 0.1117
case acc: 0.060103454
case acc: 0.2509419
case acc: 0.104537815
case acc: 0.0938444
case acc: 0.051042106
case acc: 0.109738566
top acc: 0.0750 ::: bot acc: 0.0453
top acc: 0.2632 ::: bot acc: 0.2380
top acc: 0.1246 ::: bot acc: 0.0833
top acc: 0.1075 ::: bot acc: 0.0805
top acc: 0.0712 ::: bot acc: 0.0303
top acc: 0.1274 ::: bot acc: 0.0923
current epoch: 4
train loss is 0.010053
average val loss: 0.016909, accuracy: 0.1728
average test loss: 0.017281, accuracy: 0.1753
case acc: 0.13089722
case acc: 0.30798164
case acc: 0.17407963
case acc: 0.15694617
case acc: 0.116044044
case acc: 0.16566159
top acc: 0.1454 ::: bot acc: 0.1166
top acc: 0.3200 ::: bot acc: 0.2952
top acc: 0.1954 ::: bot acc: 0.1525
top acc: 0.1705 ::: bot acc: 0.1438
top acc: 0.1360 ::: bot acc: 0.0955
top acc: 0.1831 ::: bot acc: 0.1486
current epoch: 5
train loss is 0.012772
average val loss: 0.013264, accuracy: 0.1519
average test loss: 0.013589, accuracy: 0.1544
case acc: 0.11524643
case acc: 0.27717525
case acc: 0.15583609
case acc: 0.13499227
case acc: 0.1028025
case acc: 0.14006993
top acc: 0.1296 ::: bot acc: 0.1009
top acc: 0.2897 ::: bot acc: 0.2641
top acc: 0.1764 ::: bot acc: 0.1344
top acc: 0.1495 ::: bot acc: 0.1206
top acc: 0.1228 ::: bot acc: 0.0823
top acc: 0.1579 ::: bot acc: 0.1227
current epoch: 6
train loss is 0.012431
average val loss: 0.003196, accuracy: 0.0600
average test loss: 0.003299, accuracy: 0.0621
case acc: 0.02664294
case acc: 0.17113715
case acc: 0.06174129
case acc: 0.041948743
case acc: 0.023965774
case acc: 0.047197297
top acc: 0.0409 ::: bot acc: 0.0126
top acc: 0.1834 ::: bot acc: 0.1586
top acc: 0.0824 ::: bot acc: 0.0408
top acc: 0.0554 ::: bot acc: 0.0293
top acc: 0.0424 ::: bot acc: 0.0068
top acc: 0.0645 ::: bot acc: 0.0302
current epoch: 7
train loss is 0.007197
average val loss: 0.001126, accuracy: 0.0360
average test loss: 0.001080, accuracy: 0.0338
case acc: 0.029997785
case acc: 0.098006085
case acc: 0.015190156
case acc: 0.018410025
case acc: 0.026320904
case acc: 0.014690237
top acc: 0.0157 ::: bot acc: 0.0442
top acc: 0.1104 ::: bot acc: 0.0852
top acc: 0.0175 ::: bot acc: 0.0242
top acc: 0.0078 ::: bot acc: 0.0301
top acc: 0.0096 ::: bot acc: 0.0454
top acc: 0.0084 ::: bot acc: 0.0262
current epoch: 8
train loss is 0.003061
average val loss: 0.000951, accuracy: 0.0299
average test loss: 0.000940, accuracy: 0.0286
case acc: 0.018907534
case acc: 0.09684023
case acc: 0.015765721
case acc: 0.011498265
case acc: 0.015427462
case acc: 0.01310337
top acc: 0.0068 ::: bot acc: 0.0320
top acc: 0.1091 ::: bot acc: 0.0839
top acc: 0.0239 ::: bot acc: 0.0189
top acc: 0.0080 ::: bot acc: 0.0196
top acc: 0.0140 ::: bot acc: 0.0266
top acc: 0.0206 ::: bot acc: 0.0139
current epoch: 9
train loss is 0.002181
average val loss: 0.001156, accuracy: 0.0324
average test loss: 0.001189, accuracy: 0.0336
case acc: 0.010875653
case acc: 0.106758416
case acc: 0.022974586
case acc: 0.015181834
case acc: 0.021192033
case acc: 0.024351902
top acc: 0.0181 ::: bot acc: 0.0110
top acc: 0.1190 ::: bot acc: 0.0939
top acc: 0.0395 ::: bot acc: 0.0099
top acc: 0.0264 ::: bot acc: 0.0073
top acc: 0.0382 ::: bot acc: 0.0066
top acc: 0.0407 ::: bot acc: 0.0090
current epoch: 10
train loss is 0.002060
average val loss: 0.001054, accuracy: 0.0319
average test loss: 0.001097, accuracy: 0.0333
case acc: 0.011580987
case acc: 0.10005626
case acc: 0.022490531
case acc: 0.015887415
case acc: 0.024064634
case acc: 0.025574807
top acc: 0.0203 ::: bot acc: 0.0089
top acc: 0.1124 ::: bot acc: 0.0870
top acc: 0.0390 ::: bot acc: 0.0099
top acc: 0.0275 ::: bot acc: 0.0071
top acc: 0.0425 ::: bot acc: 0.0063
top acc: 0.0423 ::: bot acc: 0.0098
current epoch: 11
train loss is 0.001855
average val loss: 0.000862, accuracy: 0.0291
average test loss: 0.000895, accuracy: 0.0301
case acc: 0.011107002
case acc: 0.08928111
case acc: 0.019572759
case acc: 0.013682247
case acc: 0.023709033
case acc: 0.023072837
top acc: 0.0189 ::: bot acc: 0.0100
top acc: 0.1017 ::: bot acc: 0.0762
top acc: 0.0339 ::: bot acc: 0.0109
top acc: 0.0243 ::: bot acc: 0.0068
top acc: 0.0422 ::: bot acc: 0.0062
top acc: 0.0394 ::: bot acc: 0.0082
current epoch: 12
train loss is 0.001582
average val loss: 0.000838, accuracy: 0.0298
average test loss: 0.000875, accuracy: 0.0312
case acc: 0.012769813
case acc: 0.08481589
case acc: 0.020258542
case acc: 0.01572515
case acc: 0.027640816
case acc: 0.025893047
top acc: 0.0229 ::: bot acc: 0.0075
top acc: 0.0972 ::: bot acc: 0.0718
top acc: 0.0352 ::: bot acc: 0.0106
top acc: 0.0271 ::: bot acc: 0.0071
top acc: 0.0471 ::: bot acc: 0.0081
top acc: 0.0427 ::: bot acc: 0.0101
current epoch: 13
train loss is 0.001378
average val loss: 0.000791, accuracy: 0.0298
average test loss: 0.000825, accuracy: 0.0312
case acc: 0.01426416
case acc: 0.07947476
case acc: 0.020117791
case acc: 0.016858095
case acc: 0.029729532
case acc: 0.02691503
top acc: 0.0254 ::: bot acc: 0.0065
top acc: 0.0918 ::: bot acc: 0.0665
top acc: 0.0351 ::: bot acc: 0.0105
top acc: 0.0286 ::: bot acc: 0.0076
top acc: 0.0497 ::: bot acc: 0.0096
top acc: 0.0435 ::: bot acc: 0.0111
current epoch: 14
train loss is 0.001203
average val loss: 0.000721, accuracy: 0.0291
average test loss: 0.000758, accuracy: 0.0305
case acc: 0.015023519
case acc: 0.073621385
case acc: 0.019811805
case acc: 0.017162245
case acc: 0.030366067
case acc: 0.02709311
top acc: 0.0268 ::: bot acc: 0.0063
top acc: 0.0857 ::: bot acc: 0.0606
top acc: 0.0346 ::: bot acc: 0.0107
top acc: 0.0291 ::: bot acc: 0.0077
top acc: 0.0501 ::: bot acc: 0.0104
top acc: 0.0441 ::: bot acc: 0.0107
current epoch: 15
train loss is 0.001056
average val loss: 0.000661, accuracy: 0.0283
average test loss: 0.000698, accuracy: 0.0297
case acc: 0.015625924
case acc: 0.06854575
case acc: 0.019419191
case acc: 0.017535796
case acc: 0.030254137
case acc: 0.026961423
top acc: 0.0279 ::: bot acc: 0.0060
top acc: 0.0809 ::: bot acc: 0.0554
top acc: 0.0340 ::: bot acc: 0.0108
top acc: 0.0294 ::: bot acc: 0.0081
top acc: 0.0499 ::: bot acc: 0.0100
top acc: 0.0438 ::: bot acc: 0.0108
current epoch: 16
train loss is 0.000960
average val loss: 0.000645, accuracy: 0.0285
average test loss: 0.000684, accuracy: 0.0302
case acc: 0.017210348
case acc: 0.06521685
case acc: 0.020499108
case acc: 0.018993558
case acc: 0.03097141
case acc: 0.028099947
top acc: 0.0299 ::: bot acc: 0.0062
top acc: 0.0775 ::: bot acc: 0.0521
top acc: 0.0356 ::: bot acc: 0.0106
top acc: 0.0312 ::: bot acc: 0.0088
top acc: 0.0507 ::: bot acc: 0.0108
top acc: 0.0451 ::: bot acc: 0.0117
current epoch: 17
train loss is 0.000879
average val loss: 0.000658, accuracy: 0.0295
average test loss: 0.000697, accuracy: 0.0311
case acc: 0.019295191
case acc: 0.06312694
case acc: 0.02178165
case acc: 0.020754835
case acc: 0.03238308
case acc: 0.02949968
top acc: 0.0330 ::: bot acc: 0.0067
top acc: 0.0754 ::: bot acc: 0.0501
top acc: 0.0377 ::: bot acc: 0.0103
top acc: 0.0333 ::: bot acc: 0.0096
top acc: 0.0522 ::: bot acc: 0.0119
top acc: 0.0466 ::: bot acc: 0.0130
current epoch: 18
train loss is 0.000824
average val loss: 0.000583, accuracy: 0.0277
average test loss: 0.000618, accuracy: 0.0293
case acc: 0.019044328
case acc: 0.05809303
case acc: 0.021150637
case acc: 0.019870145
case acc: 0.030053336
case acc: 0.027836418
top acc: 0.0324 ::: bot acc: 0.0068
top acc: 0.0706 ::: bot acc: 0.0449
top acc: 0.0367 ::: bot acc: 0.0103
top acc: 0.0323 ::: bot acc: 0.0091
top acc: 0.0496 ::: bot acc: 0.0098
top acc: 0.0450 ::: bot acc: 0.0114
current epoch: 19
train loss is 0.000742
average val loss: 0.000570, accuracy: 0.0277
average test loss: 0.000606, accuracy: 0.0294
case acc: 0.020340096
case acc: 0.055687156
case acc: 0.022039
case acc: 0.020762185
case acc: 0.02992327
case acc: 0.027706549
top acc: 0.0340 ::: bot acc: 0.0077
top acc: 0.0682 ::: bot acc: 0.0423
top acc: 0.0379 ::: bot acc: 0.0104
top acc: 0.0333 ::: bot acc: 0.0097
top acc: 0.0494 ::: bot acc: 0.0099
top acc: 0.0447 ::: bot acc: 0.0115
current epoch: 20
train loss is 0.000706
average val loss: 0.000606, accuracy: 0.0292
average test loss: 0.000648, accuracy: 0.0310
case acc: 0.023162924
case acc: 0.055265155
case acc: 0.02420955
case acc: 0.022929553
case acc: 0.031352557
case acc: 0.029144926
top acc: 0.0370 ::: bot acc: 0.0095
top acc: 0.0675 ::: bot acc: 0.0420
top acc: 0.0414 ::: bot acc: 0.0103
top acc: 0.0358 ::: bot acc: 0.0113
top acc: 0.0509 ::: bot acc: 0.0110
top acc: 0.0462 ::: bot acc: 0.0127
current epoch: 21
train loss is 0.000683
average val loss: 0.000578, accuracy: 0.0286
average test loss: 0.000616, accuracy: 0.0304
case acc: 0.02391387
case acc: 0.052337203
case acc: 0.024571802
case acc: 0.023199556
case acc: 0.030215992
case acc: 0.028179863
top acc: 0.0380 ::: bot acc: 0.0103
top acc: 0.0646 ::: bot acc: 0.0393
top acc: 0.0419 ::: bot acc: 0.0103
top acc: 0.0362 ::: bot acc: 0.0113
top acc: 0.0496 ::: bot acc: 0.0102
top acc: 0.0451 ::: bot acc: 0.0118
current epoch: 22
train loss is 0.000649
average val loss: 0.000564, accuracy: 0.0284
average test loss: 0.000605, accuracy: 0.0303
case acc: 0.024839254
case acc: 0.050578307
case acc: 0.025402127
case acc: 0.023552664
case acc: 0.029592345
case acc: 0.027719514
top acc: 0.0390 ::: bot acc: 0.0108
top acc: 0.0629 ::: bot acc: 0.0374
top acc: 0.0430 ::: bot acc: 0.0106
top acc: 0.0364 ::: bot acc: 0.0118
top acc: 0.0492 ::: bot acc: 0.0095
top acc: 0.0447 ::: bot acc: 0.0114
current epoch: 23
train loss is 0.000618
average val loss: 0.000645, accuracy: 0.0311
average test loss: 0.000695, accuracy: 0.0331
case acc: 0.028667057
case acc: 0.052224696
case acc: 0.028921712
case acc: 0.026823498
case acc: 0.03223295
case acc: 0.02997413
top acc: 0.0429 ::: bot acc: 0.0146
top acc: 0.0645 ::: bot acc: 0.0392
top acc: 0.0474 ::: bot acc: 0.0121
top acc: 0.0401 ::: bot acc: 0.0145
top acc: 0.0518 ::: bot acc: 0.0119
top acc: 0.0471 ::: bot acc: 0.0134
current epoch: 24
train loss is 0.000600
average val loss: 0.000632, accuracy: 0.0308
average test loss: 0.000683, accuracy: 0.0329
case acc: 0.02939186
case acc: 0.05057357
case acc: 0.029840745
case acc: 0.027076304
case acc: 0.031508215
case acc: 0.029143602
top acc: 0.0439 ::: bot acc: 0.0150
top acc: 0.0628 ::: bot acc: 0.0375
top acc: 0.0485 ::: bot acc: 0.0127
top acc: 0.0404 ::: bot acc: 0.0147
top acc: 0.0511 ::: bot acc: 0.0111
top acc: 0.0463 ::: bot acc: 0.0125
current epoch: 25
train loss is 0.000600
average val loss: 0.000662, accuracy: 0.0319
average test loss: 0.000714, accuracy: 0.0340
case acc: 0.031272817
case acc: 0.050241098
case acc: 0.031787187
case acc: 0.028662592
case acc: 0.032064192
case acc: 0.029748125
top acc: 0.0459 ::: bot acc: 0.0168
top acc: 0.0626 ::: bot acc: 0.0371
top acc: 0.0508 ::: bot acc: 0.0141
top acc: 0.0420 ::: bot acc: 0.0161
top acc: 0.0518 ::: bot acc: 0.0116
top acc: 0.0469 ::: bot acc: 0.0130
current epoch: 26
train loss is 0.000592
average val loss: 0.000678, accuracy: 0.0324
average test loss: 0.000733, accuracy: 0.0346
case acc: 0.03259803
case acc: 0.0496069
case acc: 0.033338003
case acc: 0.029624876
case acc: 0.032395545
case acc: 0.029954143
top acc: 0.0471 ::: bot acc: 0.0184
top acc: 0.0619 ::: bot acc: 0.0364
top acc: 0.0523 ::: bot acc: 0.0153
top acc: 0.0431 ::: bot acc: 0.0169
top acc: 0.0522 ::: bot acc: 0.0116
top acc: 0.0473 ::: bot acc: 0.0132
current epoch: 27
train loss is 0.000575
average val loss: 0.000767, accuracy: 0.0350
average test loss: 0.000827, accuracy: 0.0372
case acc: 0.035887476
case acc: 0.051231828
case acc: 0.036861893
case acc: 0.03255356
case acc: 0.03494695
case acc: 0.031939127
top acc: 0.0505 ::: bot acc: 0.0213
top acc: 0.0636 ::: bot acc: 0.0380
top acc: 0.0563 ::: bot acc: 0.0182
top acc: 0.0462 ::: bot acc: 0.0196
top acc: 0.0545 ::: bot acc: 0.0143
top acc: 0.0492 ::: bot acc: 0.0150
current epoch: 28
train loss is 0.000588
average val loss: 0.000698, accuracy: 0.0331
average test loss: 0.000758, accuracy: 0.0354
case acc: 0.03435985
case acc: 0.04851488
case acc: 0.036207166
case acc: 0.031096479
case acc: 0.032736517
case acc: 0.029566608
top acc: 0.0489 ::: bot acc: 0.0200
top acc: 0.0609 ::: bot acc: 0.0354
top acc: 0.0555 ::: bot acc: 0.0177
top acc: 0.0445 ::: bot acc: 0.0185
top acc: 0.0524 ::: bot acc: 0.0124
top acc: 0.0468 ::: bot acc: 0.0128
current epoch: 29
train loss is 0.000571
average val loss: 0.000797, accuracy: 0.0359
average test loss: 0.000860, accuracy: 0.0382
case acc: 0.03776771
case acc: 0.050435495
case acc: 0.03979276
case acc: 0.034123134
case acc: 0.03522171
case acc: 0.031665705
top acc: 0.0523 ::: bot acc: 0.0233
top acc: 0.0627 ::: bot acc: 0.0374
top acc: 0.0594 ::: bot acc: 0.0209
top acc: 0.0478 ::: bot acc: 0.0210
top acc: 0.0548 ::: bot acc: 0.0147
top acc: 0.0488 ::: bot acc: 0.0149
current epoch: 30
train loss is 0.000596
average val loss: 0.000871, accuracy: 0.0379
average test loss: 0.000942, accuracy: 0.0402
case acc: 0.040130083
case acc: 0.051782645
case acc: 0.04275756
case acc: 0.036465634
case acc: 0.03710024
case acc: 0.033149887
top acc: 0.0546 ::: bot acc: 0.0259
top acc: 0.0640 ::: bot acc: 0.0388
top acc: 0.0627 ::: bot acc: 0.0233
top acc: 0.0501 ::: bot acc: 0.0232
top acc: 0.0568 ::: bot acc: 0.0165
top acc: 0.0504 ::: bot acc: 0.0161
current epoch: 31
train loss is 0.000590
average val loss: 0.000938, accuracy: 0.0396
average test loss: 0.001017, accuracy: 0.0420
case acc: 0.042031106
case acc: 0.052946076
case acc: 0.045504026
case acc: 0.038527593
case acc: 0.038772155
case acc: 0.0343227
top acc: 0.0565 ::: bot acc: 0.0277
top acc: 0.0654 ::: bot acc: 0.0398
top acc: 0.0655 ::: bot acc: 0.0256
top acc: 0.0523 ::: bot acc: 0.0252
top acc: 0.0586 ::: bot acc: 0.0180
top acc: 0.0516 ::: bot acc: 0.0173
current epoch: 32
train loss is 0.000588
average val loss: 0.001000, accuracy: 0.0411
average test loss: 0.001082, accuracy: 0.0435
case acc: 0.043762602
case acc: 0.05370137
case acc: 0.04777727
case acc: 0.040330518
case acc: 0.04029775
case acc: 0.03518745
top acc: 0.0583 ::: bot acc: 0.0293
top acc: 0.0661 ::: bot acc: 0.0405
top acc: 0.0680 ::: bot acc: 0.0277
top acc: 0.0541 ::: bot acc: 0.0269
top acc: 0.0601 ::: bot acc: 0.0196
top acc: 0.0526 ::: bot acc: 0.0180
current epoch: 33
train loss is 0.000608
average val loss: 0.001083, accuracy: 0.0431
average test loss: 0.001166, accuracy: 0.0454
case acc: 0.045830112
case acc: 0.0547646
case acc: 0.050288364
case acc: 0.042385653
case acc: 0.042674407
case acc: 0.03658267
top acc: 0.0602 ::: bot acc: 0.0315
top acc: 0.0670 ::: bot acc: 0.0417
top acc: 0.0705 ::: bot acc: 0.0302
top acc: 0.0562 ::: bot acc: 0.0289
top acc: 0.0624 ::: bot acc: 0.0219
top acc: 0.0540 ::: bot acc: 0.0192
current epoch: 34
train loss is 0.000615
average val loss: 0.001063, accuracy: 0.0426
average test loss: 0.001147, accuracy: 0.0450
case acc: 0.045278728
case acc: 0.053539958
case acc: 0.050521728
case acc: 0.042168673
case acc: 0.042745866
case acc: 0.035813406
top acc: 0.0598 ::: bot acc: 0.0310
top acc: 0.0660 ::: bot acc: 0.0402
top acc: 0.0708 ::: bot acc: 0.0304
top acc: 0.0559 ::: bot acc: 0.0288
top acc: 0.0625 ::: bot acc: 0.0221
top acc: 0.0531 ::: bot acc: 0.0186
current epoch: 35
train loss is 0.000577
average val loss: 0.001045, accuracy: 0.0422
average test loss: 0.001130, accuracy: 0.0446
case acc: 0.04480398
case acc: 0.052448653
case acc: 0.050694432
case acc: 0.041797657
case acc: 0.043038193
case acc: 0.03504893
top acc: 0.0594 ::: bot acc: 0.0304
top acc: 0.0647 ::: bot acc: 0.0393
top acc: 0.0709 ::: bot acc: 0.0304
top acc: 0.0555 ::: bot acc: 0.0286
top acc: 0.0627 ::: bot acc: 0.0224
top acc: 0.0523 ::: bot acc: 0.0180
current epoch: 36
train loss is 0.000572
average val loss: 0.001007, accuracy: 0.0413
average test loss: 0.001091, accuracy: 0.0438
case acc: 0.043685004
case acc: 0.050583217
case acc: 0.050141074
case acc: 0.04133045
case acc: 0.042838212
case acc: 0.034049746
top acc: 0.0583 ::: bot acc: 0.0293
top acc: 0.0628 ::: bot acc: 0.0375
top acc: 0.0706 ::: bot acc: 0.0299
top acc: 0.0551 ::: bot acc: 0.0280
top acc: 0.0627 ::: bot acc: 0.0221
top acc: 0.0514 ::: bot acc: 0.0169
current epoch: 37
train loss is 0.000562
average val loss: 0.000978, accuracy: 0.0407
average test loss: 0.001060, accuracy: 0.0431
case acc: 0.042791866
case acc: 0.048668724
case acc: 0.049480047
case acc: 0.041058093
case acc: 0.04286047
case acc: 0.03370241
top acc: 0.0572 ::: bot acc: 0.0285
top acc: 0.0611 ::: bot acc: 0.0355
top acc: 0.0699 ::: bot acc: 0.0293
top acc: 0.0549 ::: bot acc: 0.0276
top acc: 0.0627 ::: bot acc: 0.0221
top acc: 0.0510 ::: bot acc: 0.0168
current epoch: 38
train loss is 0.000523
average val loss: 0.000876, accuracy: 0.0382
average test loss: 0.000951, accuracy: 0.0405
case acc: 0.039888274
case acc: 0.04496778
case acc: 0.04698358
case acc: 0.038750213
case acc: 0.041046273
case acc: 0.03156008
top acc: 0.0543 ::: bot acc: 0.0254
top acc: 0.0574 ::: bot acc: 0.0318
top acc: 0.0672 ::: bot acc: 0.0269
top acc: 0.0526 ::: bot acc: 0.0252
top acc: 0.0609 ::: bot acc: 0.0202
top acc: 0.0490 ::: bot acc: 0.0146
current epoch: 39
train loss is 0.000482
average val loss: 0.000816, accuracy: 0.0367
average test loss: 0.000890, accuracy: 0.0391
case acc: 0.03788705
case acc: 0.042261582
case acc: 0.045531534
case acc: 0.037748493
case acc: 0.0405566
case acc: 0.030403087
top acc: 0.0524 ::: bot acc: 0.0235
top acc: 0.0547 ::: bot acc: 0.0292
top acc: 0.0658 ::: bot acc: 0.0256
top acc: 0.0515 ::: bot acc: 0.0245
top acc: 0.0602 ::: bot acc: 0.0199
top acc: 0.0477 ::: bot acc: 0.0136
current epoch: 40
train loss is 0.000443
average val loss: 0.000668, accuracy: 0.0325
average test loss: 0.000731, accuracy: 0.0349
case acc: 0.0331612
case acc: 0.036926523
case acc: 0.04135922
case acc: 0.03391548
case acc: 0.03697043
case acc: 0.026994968
top acc: 0.0476 ::: bot acc: 0.0189
top acc: 0.0493 ::: bot acc: 0.0237
top acc: 0.0613 ::: bot acc: 0.0220
top acc: 0.0474 ::: bot acc: 0.0208
top acc: 0.0566 ::: bot acc: 0.0164
top acc: 0.0439 ::: bot acc: 0.0106
current epoch: 41
train loss is 0.000401
average val loss: 0.000580, accuracy: 0.0298
average test loss: 0.000633, accuracy: 0.0320
case acc: 0.029858496
case acc: 0.032699816
case acc: 0.038266517
case acc: 0.031715136
case acc: 0.0348122
case acc: 0.024935246
top acc: 0.0444 ::: bot acc: 0.0155
top acc: 0.0450 ::: bot acc: 0.0197
top acc: 0.0579 ::: bot acc: 0.0193
top acc: 0.0453 ::: bot acc: 0.0188
top acc: 0.0545 ::: bot acc: 0.0142
top acc: 0.0418 ::: bot acc: 0.0090
current epoch: 42
train loss is 0.000364
average val loss: 0.000438, accuracy: 0.0250
average test loss: 0.000477, accuracy: 0.0270
case acc: 0.02414111
case acc: 0.02638146
case acc: 0.03262298
case acc: 0.027304422
case acc: 0.030377995
case acc: 0.021453703
top acc: 0.0382 ::: bot acc: 0.0103
top acc: 0.0385 ::: bot acc: 0.0135
top acc: 0.0517 ::: bot acc: 0.0146
top acc: 0.0408 ::: bot acc: 0.0147
top acc: 0.0500 ::: bot acc: 0.0101
top acc: 0.0376 ::: bot acc: 0.0070
current epoch: 43
train loss is 0.000322
average val loss: 0.000340, accuracy: 0.0211
average test loss: 0.000368, accuracy: 0.0230
case acc: 0.019764893
case acc: 0.02103204
case acc: 0.028106991
case acc: 0.023456555
case acc: 0.026977966
case acc: 0.018882371
top acc: 0.0332 ::: bot acc: 0.0074
top acc: 0.0328 ::: bot acc: 0.0090
top acc: 0.0468 ::: bot acc: 0.0115
top acc: 0.0367 ::: bot acc: 0.0114
top acc: 0.0461 ::: bot acc: 0.0075
top acc: 0.0342 ::: bot acc: 0.0061
current epoch: 44
train loss is 0.000291
average val loss: 0.000256, accuracy: 0.0176
average test loss: 0.000271, accuracy: 0.0191
case acc: 0.015760567
case acc: 0.015840333
case acc: 0.023794029
case acc: 0.019495213
case acc: 0.023636049
case acc: 0.016298953
top acc: 0.0278 ::: bot acc: 0.0062
top acc: 0.0266 ::: bot acc: 0.0059
top acc: 0.0409 ::: bot acc: 0.0100
top acc: 0.0321 ::: bot acc: 0.0085
top acc: 0.0417 ::: bot acc: 0.0061
top acc: 0.0302 ::: bot acc: 0.0062
current epoch: 45
train loss is 0.000273
average val loss: 0.000192, accuracy: 0.0147
average test loss: 0.000193, accuracy: 0.0156
case acc: 0.012264744
case acc: 0.011308244
case acc: 0.019745959
case acc: 0.015509081
case acc: 0.02058385
case acc: 0.014384726
top acc: 0.0219 ::: bot acc: 0.0078
top acc: 0.0198 ::: bot acc: 0.0060
top acc: 0.0345 ::: bot acc: 0.0110
top acc: 0.0271 ::: bot acc: 0.0067
top acc: 0.0369 ::: bot acc: 0.0067
top acc: 0.0259 ::: bot acc: 0.0089
current epoch: 46
train loss is 0.000269
average val loss: 0.000168, accuracy: 0.0135
average test loss: 0.000157, accuracy: 0.0138
case acc: 0.01065668
case acc: 0.009536946
case acc: 0.017066361
case acc: 0.013152408
case acc: 0.01875581
case acc: 0.013580716
top acc: 0.0172 ::: bot acc: 0.0116
top acc: 0.0147 ::: bot acc: 0.0108
top acc: 0.0292 ::: bot acc: 0.0136
top acc: 0.0234 ::: bot acc: 0.0067
top acc: 0.0336 ::: bot acc: 0.0080
top acc: 0.0233 ::: bot acc: 0.0114
current epoch: 47
train loss is 0.000267
average val loss: 0.000160, accuracy: 0.0133
average test loss: 0.000139, accuracy: 0.0128
case acc: 0.010600123
case acc: 0.009482925
case acc: 0.015458503
case acc: 0.011020547
case acc: 0.01739613
case acc: 0.0131328935
top acc: 0.0126 ::: bot acc: 0.0164
top acc: 0.0092 ::: bot acc: 0.0163
top acc: 0.0236 ::: bot acc: 0.0189
top acc: 0.0196 ::: bot acc: 0.0084
top acc: 0.0306 ::: bot acc: 0.0103
top acc: 0.0206 ::: bot acc: 0.0143
current epoch: 48
train loss is 0.000285
average val loss: 0.000182, accuracy: 0.0147
average test loss: 0.000145, accuracy: 0.0133
case acc: 0.012689488
case acc: 0.012628963
case acc: 0.015706407
case acc: 0.009969004
case acc: 0.01581301
case acc: 0.012864247
top acc: 0.0069 ::: bot acc: 0.0229
top acc: 0.0042 ::: bot acc: 0.0239
top acc: 0.0157 ::: bot acc: 0.0269
top acc: 0.0139 ::: bot acc: 0.0137
top acc: 0.0256 ::: bot acc: 0.0149
top acc: 0.0162 ::: bot acc: 0.0185
current epoch: 49
train loss is 0.000321
average val loss: 0.000222, accuracy: 0.0168
average test loss: 0.000174, accuracy: 0.0149
case acc: 0.015337269
case acc: 0.017008616
case acc: 0.01815593
case acc: 0.010574817
case acc: 0.0152133275
case acc: 0.012971203
top acc: 0.0057 ::: bot acc: 0.0275
top acc: 0.0057 ::: bot acc: 0.0297
top acc: 0.0111 ::: bot acc: 0.0328
top acc: 0.0103 ::: bot acc: 0.0174
top acc: 0.0228 ::: bot acc: 0.0176
top acc: 0.0141 ::: bot acc: 0.0206
current epoch: 50
train loss is 0.000361
average val loss: 0.000349, accuracy: 0.0221
average test loss: 0.000279, accuracy: 0.0195
case acc: 0.022017509
case acc: 0.02663235
case acc: 0.024575498
case acc: 0.014432938
case acc: 0.014723392
case acc: 0.01468766
top acc: 0.0089 ::: bot acc: 0.0358
top acc: 0.0144 ::: bot acc: 0.0398
top acc: 0.0095 ::: bot acc: 0.0432
top acc: 0.0062 ::: bot acc: 0.0252
top acc: 0.0163 ::: bot acc: 0.0239
top acc: 0.0085 ::: bot acc: 0.0265
LME_Co_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6798 6798 6798
1.7082474 -0.6288155 0.10460696 -0.09017589
Validation: 756 756 756
Testing: 768 768 768
pre-processing time: 0.00017881393432617188
the split date is 2011-07-01
train dropout: 0.6 test dropout: 0.05
net initializing with time: 0.002284526824951172
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012618
average val loss: 0.005239, accuracy: 0.0974
average test loss: 0.005293, accuracy: 0.0981
case acc: 0.1394729
case acc: 0.083673924
case acc: 0.095408686
case acc: 0.088562295
case acc: 0.125238
case acc: 0.056092754
top acc: 0.1270 ::: bot acc: 0.1534
top acc: 0.0951 ::: bot acc: 0.0724
top acc: 0.0760 ::: bot acc: 0.1148
top acc: 0.0783 ::: bot acc: 0.0995
top acc: 0.1089 ::: bot acc: 0.1431
top acc: 0.0398 ::: bot acc: 0.0732
current epoch: 2
train loss is 0.007512
average val loss: 0.002836, accuracy: 0.0500
average test loss: 0.002810, accuracy: 0.0499
case acc: 0.036385715
case acc: 0.16998322
case acc: 0.01563036
case acc: 0.010600481
case acc: 0.035636388
case acc: 0.031071937
top acc: 0.0242 ::: bot acc: 0.0500
top acc: 0.1813 ::: bot acc: 0.1585
top acc: 0.0268 ::: bot acc: 0.0132
top acc: 0.0166 ::: bot acc: 0.0075
top acc: 0.0195 ::: bot acc: 0.0530
top acc: 0.0467 ::: bot acc: 0.0158
current epoch: 3
train loss is 0.007851
average val loss: 0.007612, accuracy: 0.1037
average test loss: 0.007564, accuracy: 0.1033
case acc: 0.051823393
case acc: 0.24293108
case acc: 0.09456371
case acc: 0.085674845
case acc: 0.043534666
case acc: 0.10101388
top acc: 0.0644 ::: bot acc: 0.0385
top acc: 0.2545 ::: bot acc: 0.2322
top acc: 0.1142 ::: bot acc: 0.0755
top acc: 0.0958 ::: bot acc: 0.0744
top acc: 0.0594 ::: bot acc: 0.0268
top acc: 0.1176 ::: bot acc: 0.0834
current epoch: 4
train loss is 0.009773
average val loss: 0.016386, accuracy: 0.1701
average test loss: 0.016258, accuracy: 0.1693
case acc: 0.12548462
case acc: 0.3025888
case acc: 0.16658592
case acc: 0.1508804
case acc: 0.11154388
case acc: 0.158923
top acc: 0.1386 ::: bot acc: 0.1126
top acc: 0.3140 ::: bot acc: 0.2917
top acc: 0.1864 ::: bot acc: 0.1472
top acc: 0.1611 ::: bot acc: 0.1393
top acc: 0.1281 ::: bot acc: 0.0938
top acc: 0.1758 ::: bot acc: 0.1409
current epoch: 5
train loss is 0.012554
average val loss: 0.014729, accuracy: 0.1616
average test loss: 0.014602, accuracy: 0.1609
case acc: 0.12248123
case acc: 0.28448883
case acc: 0.16122195
case acc: 0.14130029
case acc: 0.110591196
case acc: 0.14551044
top acc: 0.1351 ::: bot acc: 0.1096
top acc: 0.2957 ::: bot acc: 0.2733
top acc: 0.1806 ::: bot acc: 0.1417
top acc: 0.1514 ::: bot acc: 0.1299
top acc: 0.1270 ::: bot acc: 0.0928
top acc: 0.1622 ::: bot acc: 0.1276
current epoch: 6
train loss is 0.012814
average val loss: 0.004376, accuracy: 0.0771
average test loss: 0.004309, accuracy: 0.0766
case acc: 0.042125415
case acc: 0.1867318
case acc: 0.07551202
case acc: 0.056281544
case acc: 0.03818885
case acc: 0.06075745
top acc: 0.0548 ::: bot acc: 0.0292
top acc: 0.1982 ::: bot acc: 0.1758
top acc: 0.0951 ::: bot acc: 0.0561
top acc: 0.0660 ::: bot acc: 0.0454
top acc: 0.0541 ::: bot acc: 0.0218
top acc: 0.0777 ::: bot acc: 0.0430
current epoch: 7
train loss is 0.007735
average val loss: 0.001158, accuracy: 0.0320
average test loss: 0.001131, accuracy: 0.0312
case acc: 0.022094108
case acc: 0.10693234
case acc: 0.014882738
case acc: 0.011355832
case acc: 0.019350931
case acc: 0.012671715
top acc: 0.0111 ::: bot acc: 0.0339
top acc: 0.1180 ::: bot acc: 0.0962
top acc: 0.0245 ::: bot acc: 0.0153
top acc: 0.0050 ::: bot acc: 0.0206
top acc: 0.0079 ::: bot acc: 0.0348
top acc: 0.0142 ::: bot acc: 0.0204
current epoch: 8
train loss is 0.003351
average val loss: 0.000915, accuracy: 0.0290
average test loss: 0.000889, accuracy: 0.0281
case acc: 0.021847958
case acc: 0.09345321
case acc: 0.013953798
case acc: 0.012050808
case acc: 0.014593834
case acc: 0.012522711
top acc: 0.0112 ::: bot acc: 0.0335
top acc: 0.1042 ::: bot acc: 0.0829
top acc: 0.0182 ::: bot acc: 0.0211
top acc: 0.0052 ::: bot acc: 0.0217
top acc: 0.0082 ::: bot acc: 0.0277
top acc: 0.0154 ::: bot acc: 0.0193
current epoch: 9
train loss is 0.002193
average val loss: 0.001054, accuracy: 0.0305
average test loss: 0.001019, accuracy: 0.0294
case acc: 0.009574605
case acc: 0.101337835
case acc: 0.018003363
case acc: 0.010207355
case acc: 0.01732838
case acc: 0.019689236
top acc: 0.0118 ::: bot acc: 0.0142
top acc: 0.1120 ::: bot acc: 0.0907
top acc: 0.0322 ::: bot acc: 0.0095
top acc: 0.0169 ::: bot acc: 0.0062
top acc: 0.0297 ::: bot acc: 0.0085
top acc: 0.0339 ::: bot acc: 0.0068
current epoch: 10
train loss is 0.002048
average val loss: 0.001008, accuracy: 0.0311
average test loss: 0.000975, accuracy: 0.0302
case acc: 0.009686556
case acc: 0.096554086
case acc: 0.018672924
case acc: 0.012370817
case acc: 0.021461358
case acc: 0.022671787
top acc: 0.0157 ::: bot acc: 0.0101
top acc: 0.1073 ::: bot acc: 0.0858
top acc: 0.0334 ::: bot acc: 0.0093
top acc: 0.0202 ::: bot acc: 0.0059
top acc: 0.0356 ::: bot acc: 0.0087
top acc: 0.0380 ::: bot acc: 0.0083
current epoch: 11
train loss is 0.001834
average val loss: 0.000847, accuracy: 0.0288
average test loss: 0.000817, accuracy: 0.0279
case acc: 0.009523271
case acc: 0.08704582
case acc: 0.016788622
case acc: 0.011147547
case acc: 0.021734
case acc: 0.021436753
top acc: 0.0153 ::: bot acc: 0.0103
top acc: 0.0979 ::: bot acc: 0.0765
top acc: 0.0296 ::: bot acc: 0.0109
top acc: 0.0184 ::: bot acc: 0.0058
top acc: 0.0359 ::: bot acc: 0.0086
top acc: 0.0366 ::: bot acc: 0.0075
current epoch: 12
train loss is 0.001559
average val loss: 0.000769, accuracy: 0.0282
average test loss: 0.000737, accuracy: 0.0274
case acc: 0.009845071
case acc: 0.08053659
case acc: 0.016188331
case acc: 0.011608046
case acc: 0.023723423
case acc: 0.022249795
top acc: 0.0171 ::: bot acc: 0.0086
top acc: 0.0914 ::: bot acc: 0.0700
top acc: 0.0286 ::: bot acc: 0.0112
top acc: 0.0190 ::: bot acc: 0.0058
top acc: 0.0383 ::: bot acc: 0.0099
top acc: 0.0373 ::: bot acc: 0.0080
current epoch: 13
train loss is 0.001424
average val loss: 0.000719, accuracy: 0.0281
average test loss: 0.000683, accuracy: 0.0273
case acc: 0.010907774
case acc: 0.07489075
case acc: 0.01625273
case acc: 0.012697505
case acc: 0.025698924
case acc: 0.023453517
top acc: 0.0196 ::: bot acc: 0.0067
top acc: 0.0857 ::: bot acc: 0.0642
top acc: 0.0286 ::: bot acc: 0.0114
top acc: 0.0208 ::: bot acc: 0.0057
top acc: 0.0405 ::: bot acc: 0.0111
top acc: 0.0387 ::: bot acc: 0.0087
current epoch: 14
train loss is 0.001181
average val loss: 0.000679, accuracy: 0.0282
average test loss: 0.000646, accuracy: 0.0274
case acc: 0.012113681
case acc: 0.07015442
case acc: 0.016420271
case acc: 0.013965284
case acc: 0.027254298
case acc: 0.024668375
top acc: 0.0221 ::: bot acc: 0.0055
top acc: 0.0809 ::: bot acc: 0.0595
top acc: 0.0291 ::: bot acc: 0.0109
top acc: 0.0223 ::: bot acc: 0.0064
top acc: 0.0427 ::: bot acc: 0.0121
top acc: 0.0401 ::: bot acc: 0.0098
current epoch: 15
train loss is 0.001076
average val loss: 0.000623, accuracy: 0.0274
average test loss: 0.000594, accuracy: 0.0268
case acc: 0.012809908
case acc: 0.06525132
case acc: 0.01650066
case acc: 0.014135981
case acc: 0.027649123
case acc: 0.024490181
top acc: 0.0232 ::: bot acc: 0.0049
top acc: 0.0760 ::: bot acc: 0.0549
top acc: 0.0294 ::: bot acc: 0.0113
top acc: 0.0227 ::: bot acc: 0.0064
top acc: 0.0429 ::: bot acc: 0.0124
top acc: 0.0401 ::: bot acc: 0.0094
current epoch: 16
train loss is 0.000949
average val loss: 0.000588, accuracy: 0.0271
average test loss: 0.000558, accuracy: 0.0265
case acc: 0.013789672
case acc: 0.061193496
case acc: 0.016511029
case acc: 0.014869592
case acc: 0.027810346
case acc: 0.024791986
top acc: 0.0248 ::: bot acc: 0.0050
top acc: 0.0719 ::: bot acc: 0.0506
top acc: 0.0297 ::: bot acc: 0.0104
top acc: 0.0236 ::: bot acc: 0.0066
top acc: 0.0431 ::: bot acc: 0.0128
top acc: 0.0405 ::: bot acc: 0.0098
current epoch: 17
train loss is 0.000883
average val loss: 0.000566, accuracy: 0.0271
average test loss: 0.000534, accuracy: 0.0264
case acc: 0.014922217
case acc: 0.05775799
case acc: 0.016984053
case acc: 0.015678097
case acc: 0.027947772
case acc: 0.025154995
top acc: 0.0261 ::: bot acc: 0.0054
top acc: 0.0685 ::: bot acc: 0.0472
top acc: 0.0303 ::: bot acc: 0.0104
top acc: 0.0246 ::: bot acc: 0.0069
top acc: 0.0433 ::: bot acc: 0.0128
top acc: 0.0406 ::: bot acc: 0.0101
current epoch: 18
train loss is 0.000834
average val loss: 0.000506, accuracy: 0.0258
average test loss: 0.000475, accuracy: 0.0251
case acc: 0.015027491
case acc: 0.052952956
case acc: 0.016786039
case acc: 0.015324101
case acc: 0.026124459
case acc: 0.024189467
top acc: 0.0264 ::: bot acc: 0.0053
top acc: 0.0638 ::: bot acc: 0.0423
top acc: 0.0300 ::: bot acc: 0.0105
top acc: 0.0242 ::: bot acc: 0.0068
top acc: 0.0411 ::: bot acc: 0.0115
top acc: 0.0395 ::: bot acc: 0.0093
current epoch: 19
train loss is 0.000741
average val loss: 0.000529, accuracy: 0.0270
average test loss: 0.000498, accuracy: 0.0263
case acc: 0.017313626
case acc: 0.051925804
case acc: 0.01817603
case acc: 0.017592937
case acc: 0.027058853
case acc: 0.025863688
top acc: 0.0294 ::: bot acc: 0.0062
top acc: 0.0627 ::: bot acc: 0.0412
top acc: 0.0326 ::: bot acc: 0.0094
top acc: 0.0270 ::: bot acc: 0.0081
top acc: 0.0423 ::: bot acc: 0.0120
top acc: 0.0415 ::: bot acc: 0.0104
current epoch: 20
train loss is 0.000716
average val loss: 0.000568, accuracy: 0.0286
average test loss: 0.000536, accuracy: 0.0280
case acc: 0.020581003
case acc: 0.051624347
case acc: 0.020266341
case acc: 0.020109203
case acc: 0.028351583
case acc: 0.027363818
top acc: 0.0329 ::: bot acc: 0.0089
top acc: 0.0624 ::: bot acc: 0.0410
top acc: 0.0359 ::: bot acc: 0.0091
top acc: 0.0299 ::: bot acc: 0.0097
top acc: 0.0437 ::: bot acc: 0.0131
top acc: 0.0430 ::: bot acc: 0.0117
current epoch: 21
train loss is 0.000673
average val loss: 0.000527, accuracy: 0.0276
average test loss: 0.000495, accuracy: 0.0270
case acc: 0.020777743
case acc: 0.048259307
case acc: 0.020491267
case acc: 0.019838091
case acc: 0.02685881
case acc: 0.026015643
top acc: 0.0331 ::: bot acc: 0.0091
top acc: 0.0591 ::: bot acc: 0.0378
top acc: 0.0362 ::: bot acc: 0.0092
top acc: 0.0297 ::: bot acc: 0.0096
top acc: 0.0420 ::: bot acc: 0.0117
top acc: 0.0417 ::: bot acc: 0.0107
current epoch: 22
train loss is 0.000650
average val loss: 0.000524, accuracy: 0.0277
average test loss: 0.000492, accuracy: 0.0272
case acc: 0.022131525
case acc: 0.046808552
case acc: 0.021477072
case acc: 0.020547224
case acc: 0.02651349
case acc: 0.025608126
top acc: 0.0345 ::: bot acc: 0.0102
top acc: 0.0578 ::: bot acc: 0.0360
top acc: 0.0375 ::: bot acc: 0.0091
top acc: 0.0304 ::: bot acc: 0.0102
top acc: 0.0416 ::: bot acc: 0.0115
top acc: 0.0412 ::: bot acc: 0.0103
current epoch: 23
train loss is 0.000614
average val loss: 0.000604, accuracy: 0.0304
average test loss: 0.000569, accuracy: 0.0299
case acc: 0.026033329
case acc: 0.04847658
case acc: 0.024688222
case acc: 0.023532238
case acc: 0.02890138
case acc: 0.027587553
top acc: 0.0387 ::: bot acc: 0.0135
top acc: 0.0592 ::: bot acc: 0.0382
top acc: 0.0422 ::: bot acc: 0.0096
top acc: 0.0337 ::: bot acc: 0.0126
top acc: 0.0444 ::: bot acc: 0.0136
top acc: 0.0434 ::: bot acc: 0.0120
current epoch: 24
train loss is 0.000612
average val loss: 0.000629, accuracy: 0.0313
average test loss: 0.000596, accuracy: 0.0309
case acc: 0.02782594
case acc: 0.0483594
case acc: 0.026548058
case acc: 0.025088519
case acc: 0.029309765
case acc: 0.028021015
top acc: 0.0405 ::: bot acc: 0.0153
top acc: 0.0590 ::: bot acc: 0.0379
top acc: 0.0445 ::: bot acc: 0.0104
top acc: 0.0351 ::: bot acc: 0.0142
top acc: 0.0448 ::: bot acc: 0.0137
top acc: 0.0439 ::: bot acc: 0.0123
current epoch: 25
train loss is 0.000606
average val loss: 0.000677, accuracy: 0.0329
average test loss: 0.000642, accuracy: 0.0323
case acc: 0.03017757
case acc: 0.048733506
case acc: 0.028832013
case acc: 0.027000066
case acc: 0.030430654
case acc: 0.028709192
top acc: 0.0428 ::: bot acc: 0.0177
top acc: 0.0595 ::: bot acc: 0.0383
top acc: 0.0476 ::: bot acc: 0.0113
top acc: 0.0370 ::: bot acc: 0.0157
top acc: 0.0461 ::: bot acc: 0.0145
top acc: 0.0448 ::: bot acc: 0.0127
current epoch: 26
train loss is 0.000600
average val loss: 0.000687, accuracy: 0.0332
average test loss: 0.000653, accuracy: 0.0328
case acc: 0.031266868
case acc: 0.047991578
case acc: 0.030352559
case acc: 0.027751103
case acc: 0.030364834
case acc: 0.028778747
top acc: 0.0441 ::: bot acc: 0.0186
top acc: 0.0587 ::: bot acc: 0.0374
top acc: 0.0492 ::: bot acc: 0.0127
top acc: 0.0379 ::: bot acc: 0.0165
top acc: 0.0459 ::: bot acc: 0.0145
top acc: 0.0446 ::: bot acc: 0.0132
current epoch: 27
train loss is 0.000593
average val loss: 0.000756, accuracy: 0.0352
average test loss: 0.000719, accuracy: 0.0347
case acc: 0.033873826
case acc: 0.048870705
case acc: 0.033094466
case acc: 0.030158583
case acc: 0.032307774
case acc: 0.030021973
top acc: 0.0465 ::: bot acc: 0.0213
top acc: 0.0597 ::: bot acc: 0.0384
top acc: 0.0523 ::: bot acc: 0.0146
top acc: 0.0404 ::: bot acc: 0.0190
top acc: 0.0482 ::: bot acc: 0.0161
top acc: 0.0460 ::: bot acc: 0.0140
current epoch: 28
train loss is 0.000611
average val loss: 0.000854, accuracy: 0.0379
average test loss: 0.000816, accuracy: 0.0374
case acc: 0.037122063
case acc: 0.050467025
case acc: 0.036660306
case acc: 0.033212107
case acc: 0.03492291
case acc: 0.03217692
top acc: 0.0499 ::: bot acc: 0.0245
top acc: 0.0615 ::: bot acc: 0.0398
top acc: 0.0560 ::: bot acc: 0.0177
top acc: 0.0435 ::: bot acc: 0.0219
top acc: 0.0508 ::: bot acc: 0.0183
top acc: 0.0483 ::: bot acc: 0.0158
current epoch: 29
train loss is 0.000588
average val loss: 0.000814, accuracy: 0.0369
average test loss: 0.000777, accuracy: 0.0364
case acc: 0.036468394
case acc: 0.04860434
case acc: 0.03646157
case acc: 0.03241179
case acc: 0.03410621
case acc: 0.030579232
top acc: 0.0492 ::: bot acc: 0.0238
top acc: 0.0595 ::: bot acc: 0.0379
top acc: 0.0558 ::: bot acc: 0.0175
top acc: 0.0427 ::: bot acc: 0.0212
top acc: 0.0500 ::: bot acc: 0.0177
top acc: 0.0466 ::: bot acc: 0.0146
current epoch: 30
train loss is 0.000581
average val loss: 0.000864, accuracy: 0.0382
average test loss: 0.000827, accuracy: 0.0378
case acc: 0.038148224
case acc: 0.04906466
case acc: 0.038611613
case acc: 0.03397791
case acc: 0.035470083
case acc: 0.031327
top acc: 0.0510 ::: bot acc: 0.0254
top acc: 0.0598 ::: bot acc: 0.0385
top acc: 0.0581 ::: bot acc: 0.0194
top acc: 0.0442 ::: bot acc: 0.0228
top acc: 0.0515 ::: bot acc: 0.0188
top acc: 0.0475 ::: bot acc: 0.0151
current epoch: 31
train loss is 0.000587
average val loss: 0.000869, accuracy: 0.0384
average test loss: 0.000835, accuracy: 0.0380
case acc: 0.03837946
case acc: 0.048485715
case acc: 0.039655544
case acc: 0.034654155
case acc: 0.035548914
case acc: 0.031384666
top acc: 0.0511 ::: bot acc: 0.0256
top acc: 0.0592 ::: bot acc: 0.0380
top acc: 0.0593 ::: bot acc: 0.0203
top acc: 0.0451 ::: bot acc: 0.0233
top acc: 0.0516 ::: bot acc: 0.0189
top acc: 0.0474 ::: bot acc: 0.0152
current epoch: 32
train loss is 0.000559
average val loss: 0.000927, accuracy: 0.0399
average test loss: 0.000889, accuracy: 0.0394
case acc: 0.039774098
case acc: 0.049219154
case acc: 0.04169587
case acc: 0.03648883
case acc: 0.036978357
case acc: 0.032292623
top acc: 0.0525 ::: bot acc: 0.0270
top acc: 0.0600 ::: bot acc: 0.0388
top acc: 0.0615 ::: bot acc: 0.0223
top acc: 0.0467 ::: bot acc: 0.0253
top acc: 0.0531 ::: bot acc: 0.0202
top acc: 0.0485 ::: bot acc: 0.0161
current epoch: 33
train loss is 0.000547
average val loss: 0.000970, accuracy: 0.0409
average test loss: 0.000930, accuracy: 0.0404
case acc: 0.040842712
case acc: 0.04945417
case acc: 0.043208145
case acc: 0.037739664
case acc: 0.038262308
case acc: 0.03317593
top acc: 0.0536 ::: bot acc: 0.0282
top acc: 0.0603 ::: bot acc: 0.0389
top acc: 0.0630 ::: bot acc: 0.0237
top acc: 0.0480 ::: bot acc: 0.0265
top acc: 0.0543 ::: bot acc: 0.0215
top acc: 0.0493 ::: bot acc: 0.0169
current epoch: 34
train loss is 0.000561
average val loss: 0.000925, accuracy: 0.0398
average test loss: 0.000885, accuracy: 0.0394
case acc: 0.039632723
case acc: 0.04743886
case acc: 0.042454787
case acc: 0.037037224
case acc: 0.03761622
case acc: 0.032106988
top acc: 0.0525 ::: bot acc: 0.0268
top acc: 0.0583 ::: bot acc: 0.0369
top acc: 0.0622 ::: bot acc: 0.0229
top acc: 0.0472 ::: bot acc: 0.0258
top acc: 0.0537 ::: bot acc: 0.0209
top acc: 0.0481 ::: bot acc: 0.0160
current epoch: 35
train loss is 0.000565
average val loss: 0.000950, accuracy: 0.0405
average test loss: 0.000911, accuracy: 0.0400
case acc: 0.040125623
case acc: 0.04696964
case acc: 0.0434367
case acc: 0.037984386
case acc: 0.03883842
case acc: 0.032918375
top acc: 0.0531 ::: bot acc: 0.0272
top acc: 0.0579 ::: bot acc: 0.0365
top acc: 0.0632 ::: bot acc: 0.0239
top acc: 0.0483 ::: bot acc: 0.0267
top acc: 0.0550 ::: bot acc: 0.0219
top acc: 0.0492 ::: bot acc: 0.0167
current epoch: 36
train loss is 0.000508
average val loss: 0.000846, accuracy: 0.0379
average test loss: 0.000806, accuracy: 0.0374
case acc: 0.037245177
case acc: 0.043277502
case acc: 0.04107306
case acc: 0.035718687
case acc: 0.036793664
case acc: 0.03036518
top acc: 0.0499 ::: bot acc: 0.0246
top acc: 0.0540 ::: bot acc: 0.0328
top acc: 0.0607 ::: bot acc: 0.0217
top acc: 0.0459 ::: bot acc: 0.0244
top acc: 0.0527 ::: bot acc: 0.0202
top acc: 0.0464 ::: bot acc: 0.0144
current epoch: 37
train loss is 0.000478
average val loss: 0.000769, accuracy: 0.0358
average test loss: 0.000731, accuracy: 0.0354
case acc: 0.0348714
case acc: 0.04032271
case acc: 0.03925037
case acc: 0.03400937
case acc: 0.03527993
case acc: 0.02859719
top acc: 0.0477 ::: bot acc: 0.0220
top acc: 0.0511 ::: bot acc: 0.0300
top acc: 0.0589 ::: bot acc: 0.0199
top acc: 0.0442 ::: bot acc: 0.0226
top acc: 0.0513 ::: bot acc: 0.0186
top acc: 0.0445 ::: bot acc: 0.0127
current epoch: 38
train loss is 0.000442
average val loss: 0.000700, accuracy: 0.0340
average test loss: 0.000663, accuracy: 0.0335
case acc: 0.032724045
case acc: 0.03744921
case acc: 0.03740126
case acc: 0.032372616
case acc: 0.033941884
case acc: 0.026961956
top acc: 0.0455 ::: bot acc: 0.0201
top acc: 0.0483 ::: bot acc: 0.0270
top acc: 0.0569 ::: bot acc: 0.0183
top acc: 0.0427 ::: bot acc: 0.0211
top acc: 0.0499 ::: bot acc: 0.0174
top acc: 0.0428 ::: bot acc: 0.0113
current epoch: 39
train loss is 0.000424
average val loss: 0.000621, accuracy: 0.0317
average test loss: 0.000587, accuracy: 0.0312
case acc: 0.030123364
case acc: 0.03420031
case acc: 0.0352405
case acc: 0.03036733
case acc: 0.032144167
case acc: 0.025162574
top acc: 0.0429 ::: bot acc: 0.0175
top acc: 0.0451 ::: bot acc: 0.0236
top acc: 0.0545 ::: bot acc: 0.0165
top acc: 0.0406 ::: bot acc: 0.0191
top acc: 0.0480 ::: bot acc: 0.0159
top acc: 0.0409 ::: bot acc: 0.0099
current epoch: 40
train loss is 0.000384
average val loss: 0.000554, accuracy: 0.0296
average test loss: 0.000522, accuracy: 0.0292
case acc: 0.02765983
case acc: 0.031330086
case acc: 0.03328528
case acc: 0.028654259
case acc: 0.030450467
case acc: 0.023599088
top acc: 0.0405 ::: bot acc: 0.0150
top acc: 0.0421 ::: bot acc: 0.0208
top acc: 0.0525 ::: bot acc: 0.0150
top acc: 0.0389 ::: bot acc: 0.0174
top acc: 0.0460 ::: bot acc: 0.0146
top acc: 0.0390 ::: bot acc: 0.0088
current epoch: 41
train loss is 0.000374
average val loss: 0.000563, accuracy: 0.0299
average test loss: 0.000530, accuracy: 0.0294
case acc: 0.027676154
case acc: 0.030734003
case acc: 0.033429097
case acc: 0.02933175
case acc: 0.03111362
case acc: 0.02430411
top acc: 0.0403 ::: bot acc: 0.0152
top acc: 0.0415 ::: bot acc: 0.0202
top acc: 0.0527 ::: bot acc: 0.0149
top acc: 0.0398 ::: bot acc: 0.0180
top acc: 0.0467 ::: bot acc: 0.0152
top acc: 0.0398 ::: bot acc: 0.0093
current epoch: 42
train loss is 0.000357
average val loss: 0.000497, accuracy: 0.0277
average test loss: 0.000464, accuracy: 0.0272
case acc: 0.024984187
case acc: 0.027478583
case acc: 0.031050578
case acc: 0.027432762
case acc: 0.029448567
case acc: 0.022798222
top acc: 0.0377 ::: bot acc: 0.0125
top acc: 0.0384 ::: bot acc: 0.0170
top acc: 0.0501 ::: bot acc: 0.0131
top acc: 0.0378 ::: bot acc: 0.0160
top acc: 0.0450 ::: bot acc: 0.0138
top acc: 0.0380 ::: bot acc: 0.0086
current epoch: 43
train loss is 0.000320
average val loss: 0.000436, accuracy: 0.0256
average test loss: 0.000404, accuracy: 0.0250
case acc: 0.022541652
case acc: 0.02435612
case acc: 0.028810732
case acc: 0.02552067
case acc: 0.027693154
case acc: 0.02113342
top acc: 0.0350 ::: bot acc: 0.0105
top acc: 0.0351 ::: bot acc: 0.0139
top acc: 0.0476 ::: bot acc: 0.0113
top acc: 0.0357 ::: bot acc: 0.0145
top acc: 0.0430 ::: bot acc: 0.0124
top acc: 0.0361 ::: bot acc: 0.0075
current epoch: 44
train loss is 0.000306
average val loss: 0.000344, accuracy: 0.0222
average test loss: 0.000315, accuracy: 0.0214
case acc: 0.018403267
case acc: 0.019495573
case acc: 0.02540633
case acc: 0.021990985
case acc: 0.024688093
case acc: 0.018699404
top acc: 0.0305 ::: bot acc: 0.0070
top acc: 0.0302 ::: bot acc: 0.0091
top acc: 0.0432 ::: bot acc: 0.0099
top acc: 0.0320 ::: bot acc: 0.0114
top acc: 0.0396 ::: bot acc: 0.0102
top acc: 0.0326 ::: bot acc: 0.0068
current epoch: 45
train loss is 0.000288
average val loss: 0.000306, accuracy: 0.0206
average test loss: 0.000278, accuracy: 0.0198
case acc: 0.016522748
case acc: 0.016882285
case acc: 0.02361122
case acc: 0.020424046
case acc: 0.023680698
case acc: 0.017796261
top acc: 0.0283 ::: bot acc: 0.0058
top acc: 0.0275 ::: bot acc: 0.0068
top acc: 0.0408 ::: bot acc: 0.0094
top acc: 0.0304 ::: bot acc: 0.0100
top acc: 0.0384 ::: bot acc: 0.0096
top acc: 0.0314 ::: bot acc: 0.0070
current epoch: 46
train loss is 0.000266
average val loss: 0.000248, accuracy: 0.0182
average test loss: 0.000221, accuracy: 0.0171
case acc: 0.013612254
case acc: 0.013048297
case acc: 0.020778267
case acc: 0.017739851
case acc: 0.021270093
case acc: 0.016282927
top acc: 0.0245 ::: bot acc: 0.0047
top acc: 0.0233 ::: bot acc: 0.0037
top acc: 0.0366 ::: bot acc: 0.0090
top acc: 0.0274 ::: bot acc: 0.0078
top acc: 0.0356 ::: bot acc: 0.0083
top acc: 0.0287 ::: bot acc: 0.0076
current epoch: 47
train loss is 0.000260
average val loss: 0.000187, accuracy: 0.0154
average test loss: 0.000161, accuracy: 0.0140
case acc: 0.010661006
case acc: 0.009277267
case acc: 0.017506959
case acc: 0.014214748
case acc: 0.018082377
case acc: 0.014542961
top acc: 0.0192 ::: bot acc: 0.0066
top acc: 0.0176 ::: bot acc: 0.0038
top acc: 0.0312 ::: bot acc: 0.0101
top acc: 0.0230 ::: bot acc: 0.0061
top acc: 0.0309 ::: bot acc: 0.0079
top acc: 0.0249 ::: bot acc: 0.0100
current epoch: 48
train loss is 0.000249
average val loss: 0.000148, accuracy: 0.0134
average test loss: 0.000126, accuracy: 0.0120
case acc: 0.009055814
case acc: 0.0078031383
case acc: 0.014990988
case acc: 0.011136585
case acc: 0.01571142
case acc: 0.01324451
top acc: 0.0140 ::: bot acc: 0.0112
top acc: 0.0119 ::: bot acc: 0.0092
top acc: 0.0255 ::: bot acc: 0.0141
top acc: 0.0186 ::: bot acc: 0.0059
top acc: 0.0268 ::: bot acc: 0.0090
top acc: 0.0210 ::: bot acc: 0.0134
current epoch: 49
train loss is 0.000262
average val loss: 0.000144, accuracy: 0.0132
average test loss: 0.000122, accuracy: 0.0118
case acc: 0.009054337
case acc: 0.0077955024
case acc: 0.014460057
case acc: 0.010393171
case acc: 0.015616229
case acc: 0.013251314
top acc: 0.0126 ::: bot acc: 0.0127
top acc: 0.0098 ::: bot acc: 0.0113
top acc: 0.0233 ::: bot acc: 0.0166
top acc: 0.0175 ::: bot acc: 0.0058
top acc: 0.0266 ::: bot acc: 0.0092
top acc: 0.0211 ::: bot acc: 0.0134
current epoch: 50
train loss is 0.000267
average val loss: 0.000139, accuracy: 0.0129
average test loss: 0.000119, accuracy: 0.0116
case acc: 0.009477867
case acc: 0.008744449
case acc: 0.014114981
case acc: 0.009203469
case acc: 0.015006041
case acc: 0.012969763
top acc: 0.0097 ::: bot acc: 0.0156
top acc: 0.0064 ::: bot acc: 0.0150
top acc: 0.0191 ::: bot acc: 0.0206
top acc: 0.0152 ::: bot acc: 0.0067
top acc: 0.0252 ::: bot acc: 0.0100
top acc: 0.0200 ::: bot acc: 0.0143

		{"drop_out": 0.6, "drop_out_mc": 0.1, "repeat_mc": 50, "hidden": 30, "embedding_size": 5, "batch": 512, "lag": 4}
LME_Co_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6792 6792 6792
1.8562728 -0.6288155 0.09756618 -0.123651974
Validation: 756 756 756
Testing: 774 774 774
pre-processing time: 0.00032401084899902344
the split date is 2009-07-01
train dropout: 0.6 test dropout: 0.1
net initializing with time: 0.004115104675292969
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012622
average val loss: 0.005343, accuracy: 0.0988
average test loss: 0.005460, accuracy: 0.1001
case acc: 0.14161943
case acc: 0.081785634
case acc: 0.09788291
case acc: 0.092924945
case acc: 0.12763695
case acc: 0.05886864
top acc: 0.1316 ::: bot acc: 0.1514
top acc: 0.0940 ::: bot acc: 0.0698
top acc: 0.0809 ::: bot acc: 0.1155
top acc: 0.0841 ::: bot acc: 0.1023
top acc: 0.1150 ::: bot acc: 0.1402
top acc: 0.0471 ::: bot acc: 0.0705
current epoch: 2
train loss is 0.008014
average val loss: 0.002707, accuracy: 0.0491
average test loss: 0.002698, accuracy: 0.0488
case acc: 0.040045533
case acc: 0.16671461
case acc: 0.013155639
case acc: 0.007513455
case acc: 0.039771404
case acc: 0.025632564
top acc: 0.0302 ::: bot acc: 0.0498
top acc: 0.1783 ::: bot acc: 0.1550
top acc: 0.0203 ::: bot acc: 0.0136
top acc: 0.0095 ::: bot acc: 0.0091
top acc: 0.0274 ::: bot acc: 0.0513
top acc: 0.0365 ::: bot acc: 0.0148
current epoch: 3
train loss is 0.008330
average val loss: 0.007815, accuracy: 0.1058
average test loss: 0.007735, accuracy: 0.1048
case acc: 0.05369352
case acc: 0.24602483
case acc: 0.096653834
case acc: 0.086381175
case acc: 0.04320605
case acc: 0.10266044
top acc: 0.0640 ::: bot acc: 0.0442
top acc: 0.2578 ::: bot acc: 0.2348
top acc: 0.1129 ::: bot acc: 0.0799
top acc: 0.0954 ::: bot acc: 0.0763
top acc: 0.0554 ::: bot acc: 0.0314
top acc: 0.1143 ::: bot acc: 0.0912
current epoch: 4
train loss is 0.009915
average val loss: 0.016810, accuracy: 0.1727
average test loss: 0.016605, accuracy: 0.1714
case acc: 0.1276825
case acc: 0.30608782
case acc: 0.169498
case acc: 0.1518788
case acc: 0.11195715
case acc: 0.16112038
top acc: 0.1383 ::: bot acc: 0.1178
top acc: 0.3180 ::: bot acc: 0.2946
top acc: 0.1861 ::: bot acc: 0.1525
top acc: 0.1612 ::: bot acc: 0.1412
top acc: 0.1238 ::: bot acc: 0.1002
top acc: 0.1725 ::: bot acc: 0.1498
current epoch: 5
train loss is 0.012926
average val loss: 0.013102, accuracy: 0.1513
average test loss: 0.012939, accuracy: 0.1501
case acc: 0.11166757
case acc: 0.27489755
case acc: 0.15088266
case acc: 0.12993962
case acc: 0.09817317
case acc: 0.13521284
top acc: 0.1216 ::: bot acc: 0.1021
top acc: 0.2866 ::: bot acc: 0.2631
top acc: 0.1678 ::: bot acc: 0.1337
top acc: 0.1389 ::: bot acc: 0.1197
top acc: 0.1106 ::: bot acc: 0.0855
top acc: 0.1465 ::: bot acc: 0.1241
current epoch: 6
train loss is 0.012521
average val loss: 0.003004, accuracy: 0.0573
average test loss: 0.002943, accuracy: 0.0561
case acc: 0.021811208
case acc: 0.16745546
case acc: 0.054789644
case acc: 0.03508579
case acc: 0.016918937
case acc: 0.040557623
top acc: 0.0315 ::: bot acc: 0.0127
top acc: 0.1791 ::: bot acc: 0.1560
top acc: 0.0718 ::: bot acc: 0.0381
top acc: 0.0447 ::: bot acc: 0.0245
top acc: 0.0278 ::: bot acc: 0.0074
top acc: 0.0518 ::: bot acc: 0.0290
current epoch: 7
train loss is 0.006944
average val loss: 0.001047, accuracy: 0.0340
average test loss: 0.001061, accuracy: 0.0348
case acc: 0.032686412
case acc: 0.09654963
case acc: 0.013658481
case acc: 0.022180527
case acc: 0.029071447
case acc: 0.014715549
top acc: 0.0229 ::: bot acc: 0.0420
top acc: 0.1082 ::: bot acc: 0.0849
top acc: 0.0097 ::: bot acc: 0.0236
top acc: 0.0144 ::: bot acc: 0.0316
top acc: 0.0172 ::: bot acc: 0.0411
top acc: 0.0060 ::: bot acc: 0.0244
current epoch: 8
train loss is 0.003130
average val loss: 0.000886, accuracy: 0.0263
average test loss: 0.000889, accuracy: 0.0269
case acc: 0.020424243
case acc: 0.09623663
case acc: 0.01210263
case acc: 0.011641471
case acc: 0.012303365
case acc: 0.008424897
top acc: 0.0110 ::: bot acc: 0.0296
top acc: 0.1080 ::: bot acc: 0.0845
top acc: 0.0160 ::: bot acc: 0.0171
top acc: 0.0056 ::: bot acc: 0.0199
top acc: 0.0052 ::: bot acc: 0.0220
top acc: 0.0113 ::: bot acc: 0.0110
current epoch: 9
train loss is 0.002262
average val loss: 0.001007, accuracy: 0.0282
average test loss: 0.000997, accuracy: 0.0278
case acc: 0.0077273874
case acc: 0.10280997
case acc: 0.01614086
case acc: 0.009042879
case acc: 0.013503073
case acc: 0.017508827
top acc: 0.0080 ::: bot acc: 0.0117
top acc: 0.1141 ::: bot acc: 0.0916
top acc: 0.0289 ::: bot acc: 0.0077
top acc: 0.0149 ::: bot acc: 0.0061
top acc: 0.0235 ::: bot acc: 0.0063
top acc: 0.0282 ::: bot acc: 0.0073
current epoch: 10
train loss is 0.002151
average val loss: 0.000886, accuracy: 0.0274
average test loss: 0.000879, accuracy: 0.0270
case acc: 0.0073622763
case acc: 0.09518601
case acc: 0.015310142
case acc: 0.009289725
case acc: 0.016396737
case acc: 0.018164193
top acc: 0.0095 ::: bot acc: 0.0101
top acc: 0.1067 ::: bot acc: 0.0835
top acc: 0.0272 ::: bot acc: 0.0080
top acc: 0.0151 ::: bot acc: 0.0060
top acc: 0.0271 ::: bot acc: 0.0071
top acc: 0.0291 ::: bot acc: 0.0076
current epoch: 11
train loss is 0.001970
average val loss: 0.000753, accuracy: 0.0257
average test loss: 0.000746, accuracy: 0.0254
case acc: 0.0075837974
case acc: 0.08650883
case acc: 0.013897057
case acc: 0.008740332
case acc: 0.01756895
case acc: 0.018006241
top acc: 0.0104 ::: bot acc: 0.0096
top acc: 0.0981 ::: bot acc: 0.0752
top acc: 0.0247 ::: bot acc: 0.0090
top acc: 0.0146 ::: bot acc: 0.0057
top acc: 0.0290 ::: bot acc: 0.0079
top acc: 0.0290 ::: bot acc: 0.0074
current epoch: 12
train loss is 0.001677
average val loss: 0.000693, accuracy: 0.0259
average test loss: 0.000685, accuracy: 0.0254
case acc: 0.0075710234
case acc: 0.080839716
case acc: 0.013828691
case acc: 0.0097498065
case acc: 0.020803075
case acc: 0.019670025
top acc: 0.0129 ::: bot acc: 0.0067
top acc: 0.0924 ::: bot acc: 0.0694
top acc: 0.0241 ::: bot acc: 0.0094
top acc: 0.0163 ::: bot acc: 0.0054
top acc: 0.0324 ::: bot acc: 0.0102
top acc: 0.0306 ::: bot acc: 0.0094
current epoch: 13
train loss is 0.001457
average val loss: 0.000602, accuracy: 0.0247
average test loss: 0.000592, accuracy: 0.0241
case acc: 0.0077343155
case acc: 0.073571876
case acc: 0.013282393
case acc: 0.009449576
case acc: 0.021604821
case acc: 0.019235548
top acc: 0.0134 ::: bot acc: 0.0062
top acc: 0.0851 ::: bot acc: 0.0619
top acc: 0.0228 ::: bot acc: 0.0109
top acc: 0.0157 ::: bot acc: 0.0053
top acc: 0.0332 ::: bot acc: 0.0107
top acc: 0.0300 ::: bot acc: 0.0088
current epoch: 14
train loss is 0.001262
average val loss: 0.000520, accuracy: 0.0235
average test loss: 0.000513, accuracy: 0.0230
case acc: 0.008187655
case acc: 0.06682207
case acc: 0.012652181
case acc: 0.009429945
case acc: 0.021663632
case acc: 0.019133098
top acc: 0.0146 ::: bot acc: 0.0054
top acc: 0.0783 ::: bot acc: 0.0555
top acc: 0.0213 ::: bot acc: 0.0118
top acc: 0.0160 ::: bot acc: 0.0055
top acc: 0.0335 ::: bot acc: 0.0105
top acc: 0.0300 ::: bot acc: 0.0088
current epoch: 15
train loss is 0.001085
average val loss: 0.000470, accuracy: 0.0229
average test loss: 0.000462, accuracy: 0.0224
case acc: 0.008947684
case acc: 0.06182627
case acc: 0.012624846
case acc: 0.009890502
case acc: 0.02221619
case acc: 0.01909063
top acc: 0.0161 ::: bot acc: 0.0047
top acc: 0.0733 ::: bot acc: 0.0502
top acc: 0.0208 ::: bot acc: 0.0124
top acc: 0.0163 ::: bot acc: 0.0055
top acc: 0.0337 ::: bot acc: 0.0115
top acc: 0.0301 ::: bot acc: 0.0086
current epoch: 16
train loss is 0.001006
average val loss: 0.000465, accuracy: 0.0237
average test loss: 0.000459, accuracy: 0.0232
case acc: 0.01058102
case acc: 0.059259605
case acc: 0.013255157
case acc: 0.011358597
case acc: 0.02395688
case acc: 0.020906504
top acc: 0.0188 ::: bot acc: 0.0041
top acc: 0.0711 ::: bot acc: 0.0473
top acc: 0.0230 ::: bot acc: 0.0106
top acc: 0.0188 ::: bot acc: 0.0052
top acc: 0.0356 ::: bot acc: 0.0130
top acc: 0.0318 ::: bot acc: 0.0104
current epoch: 17
train loss is 0.000931
average val loss: 0.000475, accuracy: 0.0250
average test loss: 0.000467, accuracy: 0.0243
case acc: 0.012950339
case acc: 0.05738625
case acc: 0.014142144
case acc: 0.013324383
case acc: 0.025707895
case acc: 0.022519438
top acc: 0.0219 ::: bot acc: 0.0049
top acc: 0.0691 ::: bot acc: 0.0458
top acc: 0.0253 ::: bot acc: 0.0085
top acc: 0.0212 ::: bot acc: 0.0060
top acc: 0.0375 ::: bot acc: 0.0142
top acc: 0.0335 ::: bot acc: 0.0118
current epoch: 18
train loss is 0.000863
average val loss: 0.000452, accuracy: 0.0248
average test loss: 0.000435, accuracy: 0.0240
case acc: 0.014192774
case acc: 0.053928137
case acc: 0.014708516
case acc: 0.013851102
case acc: 0.024827432
case acc: 0.022322886
top acc: 0.0233 ::: bot acc: 0.0060
top acc: 0.0655 ::: bot acc: 0.0423
top acc: 0.0261 ::: bot acc: 0.0085
top acc: 0.0218 ::: bot acc: 0.0065
top acc: 0.0366 ::: bot acc: 0.0135
top acc: 0.0332 ::: bot acc: 0.0116
current epoch: 19
train loss is 0.000822
average val loss: 0.000409, accuracy: 0.0238
average test loss: 0.000399, accuracy: 0.0231
case acc: 0.014606299
case acc: 0.050396938
case acc: 0.014791573
case acc: 0.013945728
case acc: 0.02376847
case acc: 0.021353373
top acc: 0.0236 ::: bot acc: 0.0063
top acc: 0.0623 ::: bot acc: 0.0388
top acc: 0.0262 ::: bot acc: 0.0082
top acc: 0.0216 ::: bot acc: 0.0068
top acc: 0.0357 ::: bot acc: 0.0121
top acc: 0.0322 ::: bot acc: 0.0107
current epoch: 20
train loss is 0.000748
average val loss: 0.000426, accuracy: 0.0249
average test loss: 0.000411, accuracy: 0.0241
case acc: 0.017029691
case acc: 0.04927761
case acc: 0.016151879
case acc: 0.015493191
case acc: 0.024390794
case acc: 0.022226378
top acc: 0.0265 ::: bot acc: 0.0082
top acc: 0.0611 ::: bot acc: 0.0376
top acc: 0.0290 ::: bot acc: 0.0072
top acc: 0.0237 ::: bot acc: 0.0076
top acc: 0.0363 ::: bot acc: 0.0131
top acc: 0.0332 ::: bot acc: 0.0112
current epoch: 21
train loss is 0.000749
average val loss: 0.000445, accuracy: 0.0260
average test loss: 0.000425, accuracy: 0.0250
case acc: 0.019235915
case acc: 0.048320975
case acc: 0.017575635
case acc: 0.017308261
case acc: 0.024716273
case acc: 0.023131063
top acc: 0.0288 ::: bot acc: 0.0104
top acc: 0.0599 ::: bot acc: 0.0366
top acc: 0.0312 ::: bot acc: 0.0071
top acc: 0.0261 ::: bot acc: 0.0087
top acc: 0.0367 ::: bot acc: 0.0135
top acc: 0.0340 ::: bot acc: 0.0124
current epoch: 22
train loss is 0.000712
average val loss: 0.000486, accuracy: 0.0278
average test loss: 0.000466, accuracy: 0.0268
case acc: 0.022159332
case acc: 0.048628446
case acc: 0.019806925
case acc: 0.019709136
case acc: 0.026012849
case acc: 0.024620522
top acc: 0.0318 ::: bot acc: 0.0129
top acc: 0.0606 ::: bot acc: 0.0369
top acc: 0.0343 ::: bot acc: 0.0073
top acc: 0.0284 ::: bot acc: 0.0107
top acc: 0.0378 ::: bot acc: 0.0145
top acc: 0.0356 ::: bot acc: 0.0138
current epoch: 23
train loss is 0.000707
average val loss: 0.000512, accuracy: 0.0289
average test loss: 0.000487, accuracy: 0.0278
case acc: 0.024310378
case acc: 0.048068933
case acc: 0.021783205
case acc: 0.020901397
case acc: 0.02677835
case acc: 0.024790054
top acc: 0.0337 ::: bot acc: 0.0152
top acc: 0.0597 ::: bot acc: 0.0365
top acc: 0.0373 ::: bot acc: 0.0079
top acc: 0.0299 ::: bot acc: 0.0116
top acc: 0.0386 ::: bot acc: 0.0152
top acc: 0.0359 ::: bot acc: 0.0140
current epoch: 24
train loss is 0.000658
average val loss: 0.000454, accuracy: 0.0271
average test loss: 0.000433, accuracy: 0.0261
case acc: 0.023344152
case acc: 0.044915363
case acc: 0.021511404
case acc: 0.019743841
case acc: 0.024456633
case acc: 0.022486566
top acc: 0.0330 ::: bot acc: 0.0141
top acc: 0.0564 ::: bot acc: 0.0335
top acc: 0.0366 ::: bot acc: 0.0082
top acc: 0.0284 ::: bot acc: 0.0107
top acc: 0.0364 ::: bot acc: 0.0130
top acc: 0.0336 ::: bot acc: 0.0118
current epoch: 25
train loss is 0.000629
average val loss: 0.000526, accuracy: 0.0296
average test loss: 0.000497, accuracy: 0.0284
case acc: 0.02670725
case acc: 0.04632351
case acc: 0.024751013
case acc: 0.02219161
case acc: 0.026610738
case acc: 0.023892425
top acc: 0.0366 ::: bot acc: 0.0171
top acc: 0.0580 ::: bot acc: 0.0348
top acc: 0.0405 ::: bot acc: 0.0101
top acc: 0.0313 ::: bot acc: 0.0127
top acc: 0.0385 ::: bot acc: 0.0151
top acc: 0.0350 ::: bot acc: 0.0129
current epoch: 26
train loss is 0.000629
average val loss: 0.000522, accuracy: 0.0296
average test loss: 0.000497, accuracy: 0.0285
case acc: 0.027347857
case acc: 0.045586947
case acc: 0.025836412
case acc: 0.022723602
case acc: 0.026108678
case acc: 0.023474094
top acc: 0.0371 ::: bot acc: 0.0181
top acc: 0.0574 ::: bot acc: 0.0338
top acc: 0.0420 ::: bot acc: 0.0107
top acc: 0.0319 ::: bot acc: 0.0133
top acc: 0.0380 ::: bot acc: 0.0146
top acc: 0.0345 ::: bot acc: 0.0126
current epoch: 27
train loss is 0.000629
average val loss: 0.000573, accuracy: 0.0313
average test loss: 0.000542, accuracy: 0.0301
case acc: 0.029516572
case acc: 0.046215914
case acc: 0.02852558
case acc: 0.024666255
case acc: 0.027211526
case acc: 0.024309
top acc: 0.0393 ::: bot acc: 0.0200
top acc: 0.0581 ::: bot acc: 0.0345
top acc: 0.0447 ::: bot acc: 0.0129
top acc: 0.0336 ::: bot acc: 0.0149
top acc: 0.0391 ::: bot acc: 0.0155
top acc: 0.0355 ::: bot acc: 0.0136
current epoch: 28
train loss is 0.000639
average val loss: 0.000643, accuracy: 0.0335
average test loss: 0.000611, accuracy: 0.0323
case acc: 0.03238454
case acc: 0.047533177
case acc: 0.031815417
case acc: 0.026984943
case acc: 0.02906718
case acc: 0.025994016
top acc: 0.0421 ::: bot acc: 0.0230
top acc: 0.0594 ::: bot acc: 0.0359
top acc: 0.0484 ::: bot acc: 0.0157
top acc: 0.0362 ::: bot acc: 0.0169
top acc: 0.0410 ::: bot acc: 0.0174
top acc: 0.0373 ::: bot acc: 0.0147
current epoch: 29
train loss is 0.000640
average val loss: 0.000741, accuracy: 0.0363
average test loss: 0.000705, accuracy: 0.0351
case acc: 0.03566535
case acc: 0.049472164
case acc: 0.03534676
case acc: 0.030084865
case acc: 0.031960644
case acc: 0.028139004
top acc: 0.0453 ::: bot acc: 0.0262
top acc: 0.0614 ::: bot acc: 0.0377
top acc: 0.0518 ::: bot acc: 0.0191
top acc: 0.0391 ::: bot acc: 0.0203
top acc: 0.0439 ::: bot acc: 0.0201
top acc: 0.0391 ::: bot acc: 0.0171
current epoch: 30
train loss is 0.000665
average val loss: 0.000774, accuracy: 0.0372
average test loss: 0.000735, accuracy: 0.0360
case acc: 0.036748502
case acc: 0.04947105
case acc: 0.03702129
case acc: 0.031125482
case acc: 0.03324777
case acc: 0.02830084
top acc: 0.0465 ::: bot acc: 0.0272
top acc: 0.0614 ::: bot acc: 0.0376
top acc: 0.0536 ::: bot acc: 0.0207
top acc: 0.0403 ::: bot acc: 0.0211
top acc: 0.0451 ::: bot acc: 0.0214
top acc: 0.0394 ::: bot acc: 0.0171
current epoch: 31
train loss is 0.000653
average val loss: 0.000806, accuracy: 0.0381
average test loss: 0.000769, accuracy: 0.0369
case acc: 0.037904073
case acc: 0.049496427
case acc: 0.038764328
case acc: 0.032198478
case acc: 0.03447535
case acc: 0.028855678
top acc: 0.0477 ::: bot acc: 0.0283
top acc: 0.0614 ::: bot acc: 0.0377
top acc: 0.0556 ::: bot acc: 0.0223
top acc: 0.0417 ::: bot acc: 0.0221
top acc: 0.0464 ::: bot acc: 0.0227
top acc: 0.0401 ::: bot acc: 0.0177
current epoch: 32
train loss is 0.000666
average val loss: 0.000822, accuracy: 0.0385
average test loss: 0.000778, accuracy: 0.0372
case acc: 0.03816094
case acc: 0.048870493
case acc: 0.039761823
case acc: 0.03282071
case acc: 0.03498012
case acc: 0.028816294
top acc: 0.0480 ::: bot acc: 0.0287
top acc: 0.0606 ::: bot acc: 0.0372
top acc: 0.0563 ::: bot acc: 0.0233
top acc: 0.0420 ::: bot acc: 0.0227
top acc: 0.0471 ::: bot acc: 0.0229
top acc: 0.0400 ::: bot acc: 0.0177
current epoch: 33
train loss is 0.000649
average val loss: 0.000870, accuracy: 0.0398
average test loss: 0.000827, accuracy: 0.0386
case acc: 0.039397128
case acc: 0.04906432
case acc: 0.041569542
case acc: 0.03464367
case acc: 0.036716297
case acc: 0.029980682
top acc: 0.0491 ::: bot acc: 0.0299
top acc: 0.0609 ::: bot acc: 0.0375
top acc: 0.0581 ::: bot acc: 0.0251
top acc: 0.0438 ::: bot acc: 0.0248
top acc: 0.0489 ::: bot acc: 0.0246
top acc: 0.0413 ::: bot acc: 0.0188
current epoch: 34
train loss is 0.000646
average val loss: 0.000836, accuracy: 0.0390
average test loss: 0.000796, accuracy: 0.0378
case acc: 0.038604427
case acc: 0.04725581
case acc: 0.04111905
case acc: 0.034098122
case acc: 0.036659848
case acc: 0.0290401
top acc: 0.0485 ::: bot acc: 0.0292
top acc: 0.0590 ::: bot acc: 0.0353
top acc: 0.0579 ::: bot acc: 0.0246
top acc: 0.0437 ::: bot acc: 0.0240
top acc: 0.0489 ::: bot acc: 0.0246
top acc: 0.0402 ::: bot acc: 0.0180
current epoch: 35
train loss is 0.000624
average val loss: 0.000767, accuracy: 0.0372
average test loss: 0.000727, accuracy: 0.0360
case acc: 0.036599115
case acc: 0.04430337
case acc: 0.03971578
case acc: 0.032610282
case acc: 0.03530658
case acc: 0.027415296
top acc: 0.0462 ::: bot acc: 0.0273
top acc: 0.0560 ::: bot acc: 0.0327
top acc: 0.0565 ::: bot acc: 0.0231
top acc: 0.0420 ::: bot acc: 0.0225
top acc: 0.0475 ::: bot acc: 0.0232
top acc: 0.0386 ::: bot acc: 0.0162
current epoch: 36
train loss is 0.000588
average val loss: 0.000750, accuracy: 0.0368
average test loss: 0.000710, accuracy: 0.0356
case acc: 0.035725802
case acc: 0.04292561
case acc: 0.039338317
case acc: 0.03279069
case acc: 0.035208613
case acc: 0.027399424
top acc: 0.0454 ::: bot acc: 0.0262
top acc: 0.0547 ::: bot acc: 0.0311
top acc: 0.0560 ::: bot acc: 0.0228
top acc: 0.0423 ::: bot acc: 0.0228
top acc: 0.0473 ::: bot acc: 0.0232
top acc: 0.0386 ::: bot acc: 0.0163
current epoch: 37
train loss is 0.000555
average val loss: 0.000620, accuracy: 0.0332
average test loss: 0.000585, accuracy: 0.0319
case acc: 0.031528473
case acc: 0.038231105
case acc: 0.035908766
case acc: 0.02947312
case acc: 0.03220145
case acc: 0.024323564
top acc: 0.0413 ::: bot acc: 0.0221
top acc: 0.0501 ::: bot acc: 0.0264
top acc: 0.0525 ::: bot acc: 0.0197
top acc: 0.0387 ::: bot acc: 0.0196
top acc: 0.0444 ::: bot acc: 0.0203
top acc: 0.0356 ::: bot acc: 0.0132
current epoch: 38
train loss is 0.000506
average val loss: 0.000509, accuracy: 0.0297
average test loss: 0.000478, accuracy: 0.0285
case acc: 0.027967414
case acc: 0.033542633
case acc: 0.032183386
case acc: 0.026643423
case acc: 0.029564489
case acc: 0.021371232
top acc: 0.0375 ::: bot acc: 0.0187
top acc: 0.0454 ::: bot acc: 0.0219
top acc: 0.0485 ::: bot acc: 0.0163
top acc: 0.0359 ::: bot acc: 0.0167
top acc: 0.0415 ::: bot acc: 0.0178
top acc: 0.0322 ::: bot acc: 0.0106
current epoch: 39
train loss is 0.000466
average val loss: 0.000426, accuracy: 0.0269
average test loss: 0.000397, accuracy: 0.0257
case acc: 0.024640316
case acc: 0.029430801
case acc: 0.02924255
case acc: 0.024109326
case acc: 0.027507516
case acc: 0.019076657
top acc: 0.0343 ::: bot acc: 0.0154
top acc: 0.0414 ::: bot acc: 0.0176
top acc: 0.0455 ::: bot acc: 0.0136
top acc: 0.0333 ::: bot acc: 0.0146
top acc: 0.0393 ::: bot acc: 0.0157
top acc: 0.0300 ::: bot acc: 0.0087
current epoch: 40
train loss is 0.000432
average val loss: 0.000383, accuracy: 0.0253
average test loss: 0.000358, accuracy: 0.0242
case acc: 0.022784606
case acc: 0.027114036
case acc: 0.027576713
case acc: 0.022902872
case acc: 0.026643654
case acc: 0.018062621
top acc: 0.0324 ::: bot acc: 0.0137
top acc: 0.0388 ::: bot acc: 0.0156
top acc: 0.0439 ::: bot acc: 0.0121
top acc: 0.0320 ::: bot acc: 0.0134
top acc: 0.0385 ::: bot acc: 0.0150
top acc: 0.0289 ::: bot acc: 0.0076
current epoch: 41
train loss is 0.000406
average val loss: 0.000306, accuracy: 0.0222
average test loss: 0.000285, accuracy: 0.0212
case acc: 0.01934074
case acc: 0.022854596
case acc: 0.02433518
case acc: 0.020541653
case acc: 0.023971926
case acc: 0.015895592
top acc: 0.0288 ::: bot acc: 0.0104
top acc: 0.0347 ::: bot acc: 0.0113
top acc: 0.0402 ::: bot acc: 0.0096
top acc: 0.0294 ::: bot acc: 0.0114
top acc: 0.0357 ::: bot acc: 0.0125
top acc: 0.0265 ::: bot acc: 0.0063
current epoch: 42
train loss is 0.000374
average val loss: 0.000216, accuracy: 0.0179
average test loss: 0.000200, accuracy: 0.0170
case acc: 0.01446282
case acc: 0.017627317
case acc: 0.020176116
case acc: 0.016691566
case acc: 0.020273682
case acc: 0.013065845
top acc: 0.0236 ::: bot acc: 0.0062
top acc: 0.0291 ::: bot acc: 0.0066
top acc: 0.0353 ::: bot acc: 0.0071
top acc: 0.0253 ::: bot acc: 0.0083
top acc: 0.0318 ::: bot acc: 0.0095
top acc: 0.0229 ::: bot acc: 0.0049
current epoch: 43
train loss is 0.000346
average val loss: 0.000149, accuracy: 0.0143
average test loss: 0.000139, accuracy: 0.0137
case acc: 0.010534895
case acc: 0.013229704
case acc: 0.016769515
case acc: 0.013547794
case acc: 0.017030789
case acc: 0.010959951
top acc: 0.0189 ::: bot acc: 0.0036
top acc: 0.0240 ::: bot acc: 0.0039
top acc: 0.0302 ::: bot acc: 0.0068
top acc: 0.0215 ::: bot acc: 0.0064
top acc: 0.0280 ::: bot acc: 0.0072
top acc: 0.0194 ::: bot acc: 0.0053
current epoch: 44
train loss is 0.000331
average val loss: 0.000123, accuracy: 0.0127
average test loss: 0.000115, accuracy: 0.0122
case acc: 0.008966216
case acc: 0.011164066
case acc: 0.015143723
case acc: 0.011944666
case acc: 0.015842823
case acc: 0.010212512
top acc: 0.0163 ::: bot acc: 0.0042
top acc: 0.0206 ::: bot acc: 0.0042
top acc: 0.0277 ::: bot acc: 0.0076
top acc: 0.0195 ::: bot acc: 0.0056
top acc: 0.0264 ::: bot acc: 0.0068
top acc: 0.0181 ::: bot acc: 0.0059
current epoch: 45
train loss is 0.000326
average val loss: 0.000097, accuracy: 0.0111
average test loss: 0.000094, accuracy: 0.0109
case acc: 0.0077406927
case acc: 0.00956401
case acc: 0.013563261
case acc: 0.010427359
case acc: 0.014240834
case acc: 0.009578727
top acc: 0.0133 ::: bot acc: 0.0063
top acc: 0.0171 ::: bot acc: 0.0064
top acc: 0.0241 ::: bot acc: 0.0097
top acc: 0.0175 ::: bot acc: 0.0051
top acc: 0.0245 ::: bot acc: 0.0061
top acc: 0.0166 ::: bot acc: 0.0067
current epoch: 46
train loss is 0.000316
average val loss: 0.000069, accuracy: 0.0090
average test loss: 0.000072, accuracy: 0.0092
case acc: 0.0073098815
case acc: 0.00885823
case acc: 0.012187456
case acc: 0.0077644605
case acc: 0.010914471
case acc: 0.008423867
top acc: 0.0077 ::: bot acc: 0.0115
top acc: 0.0106 ::: bot acc: 0.0130
top acc: 0.0177 ::: bot acc: 0.0158
top acc: 0.0124 ::: bot acc: 0.0073
top acc: 0.0195 ::: bot acc: 0.0059
top acc: 0.0126 ::: bot acc: 0.0100
current epoch: 47
train loss is 0.000321
average val loss: 0.000067, accuracy: 0.0088
average test loss: 0.000074, accuracy: 0.0093
case acc: 0.008279087
case acc: 0.009667522
case acc: 0.012711383
case acc: 0.0071823765
case acc: 0.009771077
case acc: 0.0082606785
top acc: 0.0046 ::: bot acc: 0.0147
top acc: 0.0068 ::: bot acc: 0.0169
top acc: 0.0137 ::: bot acc: 0.0200
top acc: 0.0098 ::: bot acc: 0.0097
top acc: 0.0172 ::: bot acc: 0.0070
top acc: 0.0105 ::: bot acc: 0.0118
current epoch: 48
train loss is 0.000332
average val loss: 0.000075, accuracy: 0.0094
average test loss: 0.000087, accuracy: 0.0102
case acc: 0.010368106
case acc: 0.011875758
case acc: 0.013834482
case acc: 0.0073595317
case acc: 0.009091678
case acc: 0.008569904
top acc: 0.0040 ::: bot acc: 0.0181
top acc: 0.0041 ::: bot acc: 0.0217
top acc: 0.0092 ::: bot acc: 0.0244
top acc: 0.0071 ::: bot acc: 0.0127
top acc: 0.0153 ::: bot acc: 0.0088
top acc: 0.0089 ::: bot acc: 0.0136
current epoch: 49
train loss is 0.000347
average val loss: 0.000109, accuracy: 0.0116
average test loss: 0.000128, accuracy: 0.0127
case acc: 0.014429238
case acc: 0.017126981
case acc: 0.01744494
case acc: 0.009324965
case acc: 0.008643305
case acc: 0.009429292
top acc: 0.0058 ::: bot acc: 0.0233
top acc: 0.0069 ::: bot acc: 0.0282
top acc: 0.0068 ::: bot acc: 0.0309
top acc: 0.0045 ::: bot acc: 0.0171
top acc: 0.0118 ::: bot acc: 0.0125
top acc: 0.0062 ::: bot acc: 0.0165
current epoch: 50
train loss is 0.000380
average val loss: 0.000192, accuracy: 0.0162
average test loss: 0.000221, accuracy: 0.0176
case acc: 0.02087938
case acc: 0.024577618
case acc: 0.02418648
case acc: 0.014214739
case acc: 0.01002518
case acc: 0.01194313
top acc: 0.0113 ::: bot acc: 0.0302
top acc: 0.0131 ::: bot acc: 0.0362
top acc: 0.0099 ::: bot acc: 0.0395
top acc: 0.0067 ::: bot acc: 0.0234
top acc: 0.0070 ::: bot acc: 0.0176
top acc: 0.0047 ::: bot acc: 0.0212
LME_Co_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6810 6810 6810
1.8562728 -0.6288155 0.08104724 -0.1112376
Validation: 762 762 762
Testing: 744 744 744
pre-processing time: 0.0003120899200439453
the split date is 2010-01-01
train dropout: 0.6 test dropout: 0.1
net initializing with time: 0.0037415027618408203
preparing training and testing date with time: 4.76837158203125e-07
current epoch: 1
train loss is 0.012624
average val loss: 0.005101, accuracy: 0.0967
average test loss: 0.005199, accuracy: 0.0972
case acc: 0.13686657
case acc: 0.086465366
case acc: 0.09340933
case acc: 0.08979391
case acc: 0.12365534
case acc: 0.052805003
top acc: 0.1224 ::: bot acc: 0.1514
top acc: 0.1004 ::: bot acc: 0.0738
top acc: 0.0734 ::: bot acc: 0.1140
top acc: 0.0737 ::: bot acc: 0.1053
top acc: 0.1097 ::: bot acc: 0.1376
top acc: 0.0357 ::: bot acc: 0.0697
current epoch: 2
train loss is 0.007461
average val loss: 0.002879, accuracy: 0.0499
average test loss: 0.002915, accuracy: 0.0506
case acc: 0.032707173
case acc: 0.17406161
case acc: 0.017691365
case acc: 0.0127398325
case acc: 0.03218507
case acc: 0.034088094
top acc: 0.0182 ::: bot acc: 0.0479
top acc: 0.1875 ::: bot acc: 0.1623
top acc: 0.0315 ::: bot acc: 0.0108
top acc: 0.0221 ::: bot acc: 0.0098
top acc: 0.0180 ::: bot acc: 0.0463
top acc: 0.0511 ::: bot acc: 0.0173
current epoch: 3
train loss is 0.007934
average val loss: 0.007704, accuracy: 0.1049
average test loss: 0.007742, accuracy: 0.1046
case acc: 0.05375959
case acc: 0.24538136
case acc: 0.09602669
case acc: 0.08454918
case acc: 0.04443417
case acc: 0.103497654
top acc: 0.0680 ::: bot acc: 0.0395
top acc: 0.2590 ::: bot acc: 0.2333
top acc: 0.1170 ::: bot acc: 0.0759
top acc: 0.1008 ::: bot acc: 0.0683
top acc: 0.0586 ::: bot acc: 0.0305
top acc: 0.1200 ::: bot acc: 0.0869
current epoch: 4
train loss is 0.010232
average val loss: 0.016160, accuracy: 0.1690
average test loss: 0.016140, accuracy: 0.1685
case acc: 0.12498064
case acc: 0.30244336
case acc: 0.16577704
case acc: 0.14750105
case acc: 0.11081313
case acc: 0.15961164
top acc: 0.1396 ::: bot acc: 0.1103
top acc: 0.3158 ::: bot acc: 0.2903
top acc: 0.1865 ::: bot acc: 0.1451
top acc: 0.1635 ::: bot acc: 0.1317
top acc: 0.1253 ::: bot acc: 0.0971
top acc: 0.1764 ::: bot acc: 0.1429
current epoch: 5
train loss is 0.012601
average val loss: 0.013871, accuracy: 0.1565
average test loss: 0.013865, accuracy: 0.1561
case acc: 0.11798501
case acc: 0.2801964
case acc: 0.1566896
case acc: 0.13411722
case acc: 0.10584018
case acc: 0.14202683
top acc: 0.1317 ::: bot acc: 0.1040
top acc: 0.2937 ::: bot acc: 0.2686
top acc: 0.1779 ::: bot acc: 0.1356
top acc: 0.1497 ::: bot acc: 0.1183
top acc: 0.1201 ::: bot acc: 0.0926
top acc: 0.1585 ::: bot acc: 0.1257
current epoch: 6
train loss is 0.012908
average val loss: 0.002934, accuracy: 0.0564
average test loss: 0.002960, accuracy: 0.0561
case acc: 0.022556828
case acc: 0.16655637
case acc: 0.054348145
case acc: 0.03391688
case acc: 0.017651971
case acc: 0.04164977
top acc: 0.0358 ::: bot acc: 0.0096
top acc: 0.1804 ::: bot acc: 0.1546
top acc: 0.0752 ::: bot acc: 0.0337
top acc: 0.0492 ::: bot acc: 0.0187
top acc: 0.0311 ::: bot acc: 0.0063
top acc: 0.0583 ::: bot acc: 0.0253
current epoch: 7
train loss is 0.007137
average val loss: 0.001050, accuracy: 0.0316
average test loss: 0.001096, accuracy: 0.0333
case acc: 0.026824037
case acc: 0.10128972
case acc: 0.015197235
case acc: 0.020157216
case acc: 0.022359202
case acc: 0.013879415
top acc: 0.0135 ::: bot acc: 0.0408
top acc: 0.1147 ::: bot acc: 0.0891
top acc: 0.0188 ::: bot acc: 0.0229
top acc: 0.0099 ::: bot acc: 0.0332
top acc: 0.0086 ::: bot acc: 0.0353
top acc: 0.0102 ::: bot acc: 0.0242
current epoch: 8
train loss is 0.003083
average val loss: 0.000882, accuracy: 0.0261
average test loss: 0.000927, accuracy: 0.0285
case acc: 0.019437006
case acc: 0.09637747
case acc: 0.015280524
case acc: 0.01519667
case acc: 0.012090138
case acc: 0.012408329
top acc: 0.0077 ::: bot acc: 0.0328
top acc: 0.1096 ::: bot acc: 0.0846
top acc: 0.0215 ::: bot acc: 0.0203
top acc: 0.0083 ::: bot acc: 0.0268
top acc: 0.0076 ::: bot acc: 0.0206
top acc: 0.0188 ::: bot acc: 0.0150
current epoch: 9
train loss is 0.002187
average val loss: 0.001012, accuracy: 0.0285
average test loss: 0.001051, accuracy: 0.0298
case acc: 0.01036696
case acc: 0.10284969
case acc: 0.018430473
case acc: 0.012354981
case acc: 0.014990648
case acc: 0.019901043
top acc: 0.0135 ::: bot acc: 0.0154
top acc: 0.1168 ::: bot acc: 0.0907
top acc: 0.0335 ::: bot acc: 0.0085
top acc: 0.0206 ::: bot acc: 0.0108
top acc: 0.0274 ::: bot acc: 0.0052
top acc: 0.0355 ::: bot acc: 0.0061
current epoch: 10
train loss is 0.002124
average val loss: 0.000944, accuracy: 0.0290
average test loss: 0.000985, accuracy: 0.0303
case acc: 0.010915627
case acc: 0.09712089
case acc: 0.018667646
case acc: 0.013409324
case acc: 0.019424293
case acc: 0.022357738
top acc: 0.0172 ::: bot acc: 0.0116
top acc: 0.1106 ::: bot acc: 0.0851
top acc: 0.0341 ::: bot acc: 0.0084
top acc: 0.0233 ::: bot acc: 0.0094
top acc: 0.0329 ::: bot acc: 0.0075
top acc: 0.0384 ::: bot acc: 0.0081
current epoch: 11
train loss is 0.001885
average val loss: 0.000759, accuracy: 0.0259
average test loss: 0.000800, accuracy: 0.0275
case acc: 0.010774814
case acc: 0.08648149
case acc: 0.016679808
case acc: 0.012222301
case acc: 0.018659508
case acc: 0.020376341
top acc: 0.0156 ::: bot acc: 0.0135
top acc: 0.1001 ::: bot acc: 0.0744
top acc: 0.0292 ::: bot acc: 0.0120
top acc: 0.0206 ::: bot acc: 0.0111
top acc: 0.0321 ::: bot acc: 0.0067
top acc: 0.0359 ::: bot acc: 0.0063
current epoch: 12
train loss is 0.001594
average val loss: 0.000606, accuracy: 0.0232
average test loss: 0.000649, accuracy: 0.0251
case acc: 0.010107458
case acc: 0.07641567
case acc: 0.016086696
case acc: 0.0117984945
case acc: 0.017987574
case acc: 0.018490775
top acc: 0.0136 ::: bot acc: 0.0150
top acc: 0.0901 ::: bot acc: 0.0642
top acc: 0.0255 ::: bot acc: 0.0171
top acc: 0.0179 ::: bot acc: 0.0140
top acc: 0.0314 ::: bot acc: 0.0063
top acc: 0.0334 ::: bot acc: 0.0058
current epoch: 13
train loss is 0.001329
average val loss: 0.000577, accuracy: 0.0237
average test loss: 0.000616, accuracy: 0.0254
case acc: 0.01105099
case acc: 0.07224908
case acc: 0.01600552
case acc: 0.012346233
case acc: 0.02062692
case acc: 0.020032236
top acc: 0.0175 ::: bot acc: 0.0119
top acc: 0.0859 ::: bot acc: 0.0598
top acc: 0.0258 ::: bot acc: 0.0161
top acc: 0.0203 ::: bot acc: 0.0117
top acc: 0.0345 ::: bot acc: 0.0080
top acc: 0.0355 ::: bot acc: 0.0064
current epoch: 14
train loss is 0.001201
average val loss: 0.000539, accuracy: 0.0239
average test loss: 0.000583, accuracy: 0.0255
case acc: 0.011924968
case acc: 0.06770405
case acc: 0.016141975
case acc: 0.012926826
case acc: 0.02276411
case acc: 0.021736544
top acc: 0.0205 ::: bot acc: 0.0088
top acc: 0.0814 ::: bot acc: 0.0557
top acc: 0.0269 ::: bot acc: 0.0149
top acc: 0.0223 ::: bot acc: 0.0101
top acc: 0.0370 ::: bot acc: 0.0098
top acc: 0.0377 ::: bot acc: 0.0072
current epoch: 15
train loss is 0.001066
average val loss: 0.000471, accuracy: 0.0226
average test loss: 0.000513, accuracy: 0.0242
case acc: 0.012106673
case acc: 0.061980788
case acc: 0.015790148
case acc: 0.012666158
case acc: 0.021961914
case acc: 0.020754863
top acc: 0.0208 ::: bot acc: 0.0086
top acc: 0.0754 ::: bot acc: 0.0498
top acc: 0.0259 ::: bot acc: 0.0161
top acc: 0.0216 ::: bot acc: 0.0102
top acc: 0.0361 ::: bot acc: 0.0090
top acc: 0.0367 ::: bot acc: 0.0063
current epoch: 16
train loss is 0.001019
average val loss: 0.000477, accuracy: 0.0239
average test loss: 0.000522, accuracy: 0.0253
case acc: 0.013948489
case acc: 0.060005795
case acc: 0.016476864
case acc: 0.01405704
case acc: 0.024241138
case acc: 0.023029918
top acc: 0.0243 ::: bot acc: 0.0066
top acc: 0.0738 ::: bot acc: 0.0478
top acc: 0.0284 ::: bot acc: 0.0137
top acc: 0.0247 ::: bot acc: 0.0083
top acc: 0.0383 ::: bot acc: 0.0114
top acc: 0.0390 ::: bot acc: 0.0084
current epoch: 17
train loss is 0.000916
average val loss: 0.000460, accuracy: 0.0241
average test loss: 0.000503, accuracy: 0.0253
case acc: 0.014967111
case acc: 0.05688352
case acc: 0.016884172
case acc: 0.014850998
case acc: 0.025059586
case acc: 0.023424309
top acc: 0.0261 ::: bot acc: 0.0065
top acc: 0.0705 ::: bot acc: 0.0448
top acc: 0.0295 ::: bot acc: 0.0124
top acc: 0.0262 ::: bot acc: 0.0077
top acc: 0.0392 ::: bot acc: 0.0118
top acc: 0.0396 ::: bot acc: 0.0082
current epoch: 18
train loss is 0.000870
average val loss: 0.000433, accuracy: 0.0239
average test loss: 0.000473, accuracy: 0.0249
case acc: 0.01592236
case acc: 0.053284246
case acc: 0.017361818
case acc: 0.0154057285
case acc: 0.02426081
case acc: 0.023234377
top acc: 0.0275 ::: bot acc: 0.0063
top acc: 0.0668 ::: bot acc: 0.0412
top acc: 0.0305 ::: bot acc: 0.0118
top acc: 0.0270 ::: bot acc: 0.0081
top acc: 0.0384 ::: bot acc: 0.0110
top acc: 0.0394 ::: bot acc: 0.0081
current epoch: 19
train loss is 0.000796
average val loss: 0.000422, accuracy: 0.0240
average test loss: 0.000465, accuracy: 0.0250
case acc: 0.017207654
case acc: 0.05119426
case acc: 0.017634496
case acc: 0.01604293
case acc: 0.02439334
case acc: 0.023476176
top acc: 0.0295 ::: bot acc: 0.0065
top acc: 0.0646 ::: bot acc: 0.0393
top acc: 0.0317 ::: bot acc: 0.0102
top acc: 0.0283 ::: bot acc: 0.0075
top acc: 0.0385 ::: bot acc: 0.0113
top acc: 0.0398 ::: bot acc: 0.0081
current epoch: 20
train loss is 0.000733
average val loss: 0.000395, accuracy: 0.0235
average test loss: 0.000439, accuracy: 0.0244
case acc: 0.017627796
case acc: 0.04844677
case acc: 0.01797203
case acc: 0.016153177
case acc: 0.023442999
case acc: 0.0230396
top acc: 0.0303 ::: bot acc: 0.0063
top acc: 0.0621 ::: bot acc: 0.0364
top acc: 0.0325 ::: bot acc: 0.0095
top acc: 0.0282 ::: bot acc: 0.0075
top acc: 0.0374 ::: bot acc: 0.0103
top acc: 0.0393 ::: bot acc: 0.0081
current epoch: 21
train loss is 0.000688
average val loss: 0.000396, accuracy: 0.0240
average test loss: 0.000439, accuracy: 0.0247
case acc: 0.019323876
case acc: 0.046635263
case acc: 0.018699491
case acc: 0.017233448
case acc: 0.023376133
case acc: 0.023214817
top acc: 0.0324 ::: bot acc: 0.0074
top acc: 0.0605 ::: bot acc: 0.0344
top acc: 0.0343 ::: bot acc: 0.0085
top acc: 0.0298 ::: bot acc: 0.0076
top acc: 0.0374 ::: bot acc: 0.0101
top acc: 0.0396 ::: bot acc: 0.0080
current epoch: 22
train loss is 0.000678
average val loss: 0.000416, accuracy: 0.0251
average test loss: 0.000461, accuracy: 0.0258
case acc: 0.021429354
case acc: 0.046194836
case acc: 0.02002974
case acc: 0.018718097
case acc: 0.024021506
case acc: 0.024241261
top acc: 0.0349 ::: bot acc: 0.0090
top acc: 0.0599 ::: bot acc: 0.0340
top acc: 0.0369 ::: bot acc: 0.0070
top acc: 0.0318 ::: bot acc: 0.0081
top acc: 0.0380 ::: bot acc: 0.0108
top acc: 0.0404 ::: bot acc: 0.0089
current epoch: 23
train loss is 0.000655
average val loss: 0.000391, accuracy: 0.0244
average test loss: 0.000433, accuracy: 0.0250
case acc: 0.02151311
case acc: 0.043595716
case acc: 0.02026947
case acc: 0.018625695
case acc: 0.022806315
case acc: 0.023088159
top acc: 0.0348 ::: bot acc: 0.0090
top acc: 0.0571 ::: bot acc: 0.0314
top acc: 0.0373 ::: bot acc: 0.0071
top acc: 0.0318 ::: bot acc: 0.0083
top acc: 0.0370 ::: bot acc: 0.0096
top acc: 0.0394 ::: bot acc: 0.0079
current epoch: 24
train loss is 0.000604
average val loss: 0.000366, accuracy: 0.0236
average test loss: 0.000411, accuracy: 0.0243
case acc: 0.021596583
case acc: 0.041360036
case acc: 0.02052307
case acc: 0.018442743
case acc: 0.021531919
case acc: 0.022152027
top acc: 0.0352 ::: bot acc: 0.0089
top acc: 0.0550 ::: bot acc: 0.0293
top acc: 0.0378 ::: bot acc: 0.0068
top acc: 0.0319 ::: bot acc: 0.0078
top acc: 0.0355 ::: bot acc: 0.0087
top acc: 0.0385 ::: bot acc: 0.0072
current epoch: 25
train loss is 0.000599
average val loss: 0.000441, accuracy: 0.0266
average test loss: 0.000487, accuracy: 0.0270
case acc: 0.025321346
case acc: 0.04342363
case acc: 0.023248896
case acc: 0.021287976
case acc: 0.024218006
case acc: 0.02472439
top acc: 0.0394 ::: bot acc: 0.0117
top acc: 0.0573 ::: bot acc: 0.0311
top acc: 0.0423 ::: bot acc: 0.0064
top acc: 0.0355 ::: bot acc: 0.0090
top acc: 0.0383 ::: bot acc: 0.0111
top acc: 0.0413 ::: bot acc: 0.0091
current epoch: 26
train loss is 0.000618
average val loss: 0.000490, accuracy: 0.0284
average test loss: 0.000534, accuracy: 0.0287
case acc: 0.027526729
case acc: 0.044189047
case acc: 0.025714865
case acc: 0.023351867
case acc: 0.025422476
case acc: 0.026210126
top acc: 0.0415 ::: bot acc: 0.0138
top acc: 0.0577 ::: bot acc: 0.0321
top acc: 0.0451 ::: bot acc: 0.0078
top acc: 0.0378 ::: bot acc: 0.0103
top acc: 0.0396 ::: bot acc: 0.0122
top acc: 0.0429 ::: bot acc: 0.0104
current epoch: 27
train loss is 0.000601
average val loss: 0.000460, accuracy: 0.0275
average test loss: 0.000504, accuracy: 0.0278
case acc: 0.027230613
case acc: 0.04218282
case acc: 0.025867894
case acc: 0.022731435
case acc: 0.024383206
case acc: 0.024377175
top acc: 0.0411 ::: bot acc: 0.0137
top acc: 0.0560 ::: bot acc: 0.0300
top acc: 0.0451 ::: bot acc: 0.0078
top acc: 0.0371 ::: bot acc: 0.0095
top acc: 0.0383 ::: bot acc: 0.0113
top acc: 0.0411 ::: bot acc: 0.0086
current epoch: 28
train loss is 0.000570
average val loss: 0.000448, accuracy: 0.0272
average test loss: 0.000495, accuracy: 0.0276
case acc: 0.027340565
case acc: 0.04098253
case acc: 0.026461406
case acc: 0.022706201
case acc: 0.024451707
case acc: 0.02361716
top acc: 0.0412 ::: bot acc: 0.0135
top acc: 0.0547 ::: bot acc: 0.0288
top acc: 0.0458 ::: bot acc: 0.0083
top acc: 0.0371 ::: bot acc: 0.0097
top acc: 0.0386 ::: bot acc: 0.0111
top acc: 0.0400 ::: bot acc: 0.0083
current epoch: 29
train loss is 0.000573
average val loss: 0.000508, accuracy: 0.0293
average test loss: 0.000551, accuracy: 0.0295
case acc: 0.029758438
case acc: 0.042205546
case acc: 0.029041775
case acc: 0.02456661
case acc: 0.02640857
case acc: 0.024867296
top acc: 0.0438 ::: bot acc: 0.0158
top acc: 0.0557 ::: bot acc: 0.0301
top acc: 0.0490 ::: bot acc: 0.0100
top acc: 0.0392 ::: bot acc: 0.0110
top acc: 0.0407 ::: bot acc: 0.0130
top acc: 0.0414 ::: bot acc: 0.0091
current epoch: 30
train loss is 0.000557
average val loss: 0.000550, accuracy: 0.0307
average test loss: 0.000596, accuracy: 0.0309
case acc: 0.031621948
case acc: 0.042946905
case acc: 0.03115591
case acc: 0.025895255
case acc: 0.028166052
case acc: 0.02574635
top acc: 0.0457 ::: bot acc: 0.0176
top acc: 0.0564 ::: bot acc: 0.0308
top acc: 0.0514 ::: bot acc: 0.0116
top acc: 0.0409 ::: bot acc: 0.0118
top acc: 0.0424 ::: bot acc: 0.0147
top acc: 0.0423 ::: bot acc: 0.0099
current epoch: 31
train loss is 0.000550
average val loss: 0.000535, accuracy: 0.0302
average test loss: 0.000582, accuracy: 0.0305
case acc: 0.031124871
case acc: 0.041707102
case acc: 0.031686235
case acc: 0.02580635
case acc: 0.02792078
case acc: 0.02496147
top acc: 0.0451 ::: bot acc: 0.0171
top acc: 0.0553 ::: bot acc: 0.0296
top acc: 0.0519 ::: bot acc: 0.0122
top acc: 0.0406 ::: bot acc: 0.0118
top acc: 0.0422 ::: bot acc: 0.0147
top acc: 0.0416 ::: bot acc: 0.0091
current epoch: 32
train loss is 0.000544
average val loss: 0.000555, accuracy: 0.0309
average test loss: 0.000599, accuracy: 0.0311
case acc: 0.032103583
case acc: 0.041409977
case acc: 0.032685764
case acc: 0.026689075
case acc: 0.028673813
case acc: 0.025213597
top acc: 0.0463 ::: bot acc: 0.0179
top acc: 0.0549 ::: bot acc: 0.0294
top acc: 0.0531 ::: bot acc: 0.0127
top acc: 0.0417 ::: bot acc: 0.0123
top acc: 0.0429 ::: bot acc: 0.0153
top acc: 0.0415 ::: bot acc: 0.0095
current epoch: 33
train loss is 0.000554
average val loss: 0.000590, accuracy: 0.0320
average test loss: 0.000637, accuracy: 0.0323
case acc: 0.033335958
case acc: 0.042007003
case acc: 0.034478772
case acc: 0.028048042
case acc: 0.029728655
case acc: 0.025916206
top acc: 0.0475 ::: bot acc: 0.0191
top acc: 0.0556 ::: bot acc: 0.0297
top acc: 0.0551 ::: bot acc: 0.0143
top acc: 0.0432 ::: bot acc: 0.0135
top acc: 0.0441 ::: bot acc: 0.0163
top acc: 0.0424 ::: bot acc: 0.0103
current epoch: 34
train loss is 0.000539
average val loss: 0.000594, accuracy: 0.0322
average test loss: 0.000636, accuracy: 0.0323
case acc: 0.033233713
case acc: 0.0412096
case acc: 0.035305217
case acc: 0.028422317
case acc: 0.029634794
case acc: 0.025849093
top acc: 0.0474 ::: bot acc: 0.0190
top acc: 0.0550 ::: bot acc: 0.0290
top acc: 0.0558 ::: bot acc: 0.0152
top acc: 0.0437 ::: bot acc: 0.0138
top acc: 0.0436 ::: bot acc: 0.0164
top acc: 0.0425 ::: bot acc: 0.0102
current epoch: 35
train loss is 0.000548
average val loss: 0.000634, accuracy: 0.0334
average test loss: 0.000677, accuracy: 0.0335
case acc: 0.034408554
case acc: 0.04195775
case acc: 0.037224248
case acc: 0.029724276
case acc: 0.030860135
case acc: 0.02668323
top acc: 0.0485 ::: bot acc: 0.0203
top acc: 0.0554 ::: bot acc: 0.0298
top acc: 0.0578 ::: bot acc: 0.0170
top acc: 0.0452 ::: bot acc: 0.0147
top acc: 0.0450 ::: bot acc: 0.0176
top acc: 0.0433 ::: bot acc: 0.0107
current epoch: 36
train loss is 0.000547
average val loss: 0.000654, accuracy: 0.0340
average test loss: 0.000703, accuracy: 0.0342
case acc: 0.03485609
case acc: 0.042083595
case acc: 0.038438264
case acc: 0.030899398
case acc: 0.031740706
case acc: 0.027291631
top acc: 0.0491 ::: bot acc: 0.0206
top acc: 0.0556 ::: bot acc: 0.0300
top acc: 0.0592 ::: bot acc: 0.0179
top acc: 0.0464 ::: bot acc: 0.0156
top acc: 0.0458 ::: bot acc: 0.0184
top acc: 0.0439 ::: bot acc: 0.0114
current epoch: 37
train loss is 0.000530
average val loss: 0.000655, accuracy: 0.0341
average test loss: 0.000704, accuracy: 0.0343
case acc: 0.03502191
case acc: 0.041210275
case acc: 0.038565256
case acc: 0.031117821
case acc: 0.032483116
case acc: 0.0273274
top acc: 0.0495 ::: bot acc: 0.0205
top acc: 0.0548 ::: bot acc: 0.0291
top acc: 0.0592 ::: bot acc: 0.0181
top acc: 0.0468 ::: bot acc: 0.0158
top acc: 0.0467 ::: bot acc: 0.0190
top acc: 0.0439 ::: bot acc: 0.0115
current epoch: 38
train loss is 0.000524
average val loss: 0.000634, accuracy: 0.0335
average test loss: 0.000681, accuracy: 0.0337
case acc: 0.03413856
case acc: 0.039487515
case acc: 0.038055375
case acc: 0.030663656
case acc: 0.0327706
case acc: 0.026833747
top acc: 0.0484 ::: bot acc: 0.0199
top acc: 0.0532 ::: bot acc: 0.0273
top acc: 0.0588 ::: bot acc: 0.0176
top acc: 0.0463 ::: bot acc: 0.0154
top acc: 0.0470 ::: bot acc: 0.0194
top acc: 0.0435 ::: bot acc: 0.0109
current epoch: 39
train loss is 0.000510
average val loss: 0.000581, accuracy: 0.0319
average test loss: 0.000629, accuracy: 0.0321
case acc: 0.032166515
case acc: 0.036981482
case acc: 0.036788434
case acc: 0.029655311
case acc: 0.031861648
case acc: 0.025413044
top acc: 0.0465 ::: bot acc: 0.0180
top acc: 0.0506 ::: bot acc: 0.0249
top acc: 0.0573 ::: bot acc: 0.0165
top acc: 0.0451 ::: bot acc: 0.0148
top acc: 0.0461 ::: bot acc: 0.0187
top acc: 0.0420 ::: bot acc: 0.0094
current epoch: 40
train loss is 0.000467
average val loss: 0.000480, accuracy: 0.0287
average test loss: 0.000525, accuracy: 0.0289
case acc: 0.028504213
case acc: 0.03290603
case acc: 0.033616535
case acc: 0.02695836
case acc: 0.028781567
case acc: 0.022691177
top acc: 0.0425 ::: bot acc: 0.0145
top acc: 0.0464 ::: bot acc: 0.0209
top acc: 0.0540 ::: bot acc: 0.0137
top acc: 0.0422 ::: bot acc: 0.0126
top acc: 0.0430 ::: bot acc: 0.0155
top acc: 0.0389 ::: bot acc: 0.0076
current epoch: 41
train loss is 0.000424
average val loss: 0.000430, accuracy: 0.0269
average test loss: 0.000474, accuracy: 0.0272
case acc: 0.026384398
case acc: 0.030425694
case acc: 0.031846337
case acc: 0.025654469
case acc: 0.027551156
case acc: 0.02140188
top acc: 0.0401 ::: bot acc: 0.0129
top acc: 0.0439 ::: bot acc: 0.0185
top acc: 0.0520 ::: bot acc: 0.0122
top acc: 0.0407 ::: bot acc: 0.0114
top acc: 0.0418 ::: bot acc: 0.0141
top acc: 0.0375 ::: bot acc: 0.0066
current epoch: 42
train loss is 0.000406
average val loss: 0.000342, accuracy: 0.0236
average test loss: 0.000388, accuracy: 0.0241
case acc: 0.023003653
case acc: 0.02613509
case acc: 0.028669512
case acc: 0.02312459
case acc: 0.024697063
case acc: 0.018923413
top acc: 0.0367 ::: bot acc: 0.0099
top acc: 0.0395 ::: bot acc: 0.0143
top acc: 0.0485 ::: bot acc: 0.0097
top acc: 0.0375 ::: bot acc: 0.0100
top acc: 0.0388 ::: bot acc: 0.0115
top acc: 0.0344 ::: bot acc: 0.0053
current epoch: 43
train loss is 0.000380
average val loss: 0.000328, accuracy: 0.0231
average test loss: 0.000373, accuracy: 0.0235
case acc: 0.02199509
case acc: 0.02461497
case acc: 0.027849624
case acc: 0.022924444
case acc: 0.024737354
case acc: 0.01904074
top acc: 0.0355 ::: bot acc: 0.0093
top acc: 0.0382 ::: bot acc: 0.0125
top acc: 0.0477 ::: bot acc: 0.0090
top acc: 0.0375 ::: bot acc: 0.0099
top acc: 0.0388 ::: bot acc: 0.0116
top acc: 0.0346 ::: bot acc: 0.0052
current epoch: 44
train loss is 0.000362
average val loss: 0.000260, accuracy: 0.0201
average test loss: 0.000307, accuracy: 0.0208
case acc: 0.018796563
case acc: 0.020511623
case acc: 0.02461422
case acc: 0.020855047
case acc: 0.02262732
case acc: 0.017628014
top acc: 0.0318 ::: bot acc: 0.0070
top acc: 0.0339 ::: bot acc: 0.0090
top acc: 0.0439 ::: bot acc: 0.0070
top acc: 0.0349 ::: bot acc: 0.0089
top acc: 0.0366 ::: bot acc: 0.0095
top acc: 0.0327 ::: bot acc: 0.0051
current epoch: 45
train loss is 0.000337
average val loss: 0.000199, accuracy: 0.0170
average test loss: 0.000245, accuracy: 0.0182
case acc: 0.01603171
case acc: 0.016416857
case acc: 0.021519689
case acc: 0.01856238
case acc: 0.020346943
case acc: 0.016166288
top acc: 0.0281 ::: bot acc: 0.0062
top acc: 0.0292 ::: bot acc: 0.0061
top acc: 0.0397 ::: bot acc: 0.0061
top acc: 0.0317 ::: bot acc: 0.0079
top acc: 0.0341 ::: bot acc: 0.0078
top acc: 0.0301 ::: bot acc: 0.0054
current epoch: 46
train loss is 0.000312
average val loss: 0.000123, accuracy: 0.0127
average test loss: 0.000169, accuracy: 0.0145
case acc: 0.012400239
case acc: 0.011455138
case acc: 0.018204758
case acc: 0.015104337
case acc: 0.016088618
case acc: 0.013953679
top acc: 0.0219 ::: bot acc: 0.0079
top acc: 0.0224 ::: bot acc: 0.0048
top acc: 0.0335 ::: bot acc: 0.0088
top acc: 0.0266 ::: bot acc: 0.0079
top acc: 0.0291 ::: bot acc: 0.0053
top acc: 0.0253 ::: bot acc: 0.0086
current epoch: 47
train loss is 0.000294
average val loss: 0.000097, accuracy: 0.0111
average test loss: 0.000143, accuracy: 0.0132
case acc: 0.011114272
case acc: 0.009755233
case acc: 0.01676619
case acc: 0.013557625
case acc: 0.014555064
case acc: 0.013229736
top acc: 0.0185 ::: bot acc: 0.0107
top acc: 0.0184 ::: bot acc: 0.0074
top acc: 0.0293 ::: bot acc: 0.0124
top acc: 0.0239 ::: bot acc: 0.0088
top acc: 0.0271 ::: bot acc: 0.0047
top acc: 0.0234 ::: bot acc: 0.0102
current epoch: 48
train loss is 0.000296
average val loss: 0.000079, accuracy: 0.0099
average test loss: 0.000126, accuracy: 0.0123
case acc: 0.010369697
case acc: 0.00942826
case acc: 0.01566799
case acc: 0.012316234
case acc: 0.013054176
case acc: 0.01273836
top acc: 0.0147 ::: bot acc: 0.0148
top acc: 0.0135 ::: bot acc: 0.0122
top acc: 0.0248 ::: bot acc: 0.0173
top acc: 0.0208 ::: bot acc: 0.0112
top acc: 0.0248 ::: bot acc: 0.0048
top acc: 0.0214 ::: bot acc: 0.0123
current epoch: 49
train loss is 0.000301
average val loss: 0.000077, accuracy: 0.0096
average test loss: 0.000122, accuracy: 0.0122
case acc: 0.011457433
case acc: 0.011418036
case acc: 0.015259109
case acc: 0.01141104
case acc: 0.011292166
case acc: 0.012371379
top acc: 0.0097 ::: bot acc: 0.0199
top acc: 0.0080 ::: bot acc: 0.0186
top acc: 0.0183 ::: bot acc: 0.0236
top acc: 0.0161 ::: bot acc: 0.0156
top acc: 0.0213 ::: bot acc: 0.0065
top acc: 0.0180 ::: bot acc: 0.0157
current epoch: 50
train loss is 0.000318
average val loss: 0.000104, accuracy: 0.0113
average test loss: 0.000149, accuracy: 0.0138
case acc: 0.014517338
case acc: 0.01597193
case acc: 0.017310554
case acc: 0.012256427
case acc: 0.010197645
case acc: 0.012416785
top acc: 0.0073 ::: bot acc: 0.0257
top acc: 0.0071 ::: bot acc: 0.0256
top acc: 0.0111 ::: bot acc: 0.0312
top acc: 0.0112 ::: bot acc: 0.0208
top acc: 0.0176 ::: bot acc: 0.0099
top acc: 0.0145 ::: bot acc: 0.0190
LME_Co_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6792 6792 6792
1.7082474 -0.6288155 0.08104724 -0.08406281
Validation: 756 756 756
Testing: 774 774 774
pre-processing time: 0.00020265579223632812
the split date is 2010-07-01
train dropout: 0.6 test dropout: 0.1
net initializing with time: 0.0026383399963378906
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012512
average val loss: 0.005411, accuracy: 0.0992
average test loss: 0.005495, accuracy: 0.0994
case acc: 0.14140779
case acc: 0.08280322
case acc: 0.098385036
case acc: 0.090196006
case acc: 0.1285741
case acc: 0.05520956
top acc: 0.1250 ::: bot acc: 0.1580
top acc: 0.0962 ::: bot acc: 0.0697
top acc: 0.0750 ::: bot acc: 0.1220
top acc: 0.0730 ::: bot acc: 0.1066
top acc: 0.1087 ::: bot acc: 0.1470
top acc: 0.0379 ::: bot acc: 0.0714
current epoch: 2
train loss is 0.007901
average val loss: 0.002734, accuracy: 0.0503
average test loss: 0.002782, accuracy: 0.0512
case acc: 0.041306067
case acc: 0.16574499
case acc: 0.01722687
case acc: 0.012135112
case acc: 0.043232422
case acc: 0.027302042
top acc: 0.0257 ::: bot acc: 0.0566
top acc: 0.1797 ::: bot acc: 0.1523
top acc: 0.0247 ::: bot acc: 0.0223
top acc: 0.0190 ::: bot acc: 0.0139
top acc: 0.0244 ::: bot acc: 0.0609
top acc: 0.0439 ::: bot acc: 0.0118
current epoch: 3
train loss is 0.008272
average val loss: 0.007535, accuracy: 0.1026
average test loss: 0.007601, accuracy: 0.1026
case acc: 0.051030084
case acc: 0.24336468
case acc: 0.09350744
case acc: 0.08604668
case acc: 0.039224382
case acc: 0.10265658
top acc: 0.0669 ::: bot acc: 0.0356
top acc: 0.2571 ::: bot acc: 0.2306
top acc: 0.1170 ::: bot acc: 0.0693
top acc: 0.1033 ::: bot acc: 0.0704
top acc: 0.0587 ::: bot acc: 0.0211
top acc: 0.1202 ::: bot acc: 0.0864
current epoch: 4
train loss is 0.009819
average val loss: 0.016411, accuracy: 0.1700
average test loss: 0.016436, accuracy: 0.1699
case acc: 0.12574106
case acc: 0.30392703
case acc: 0.16729529
case acc: 0.15237631
case acc: 0.10840113
case acc: 0.16166022
top acc: 0.1421 ::: bot acc: 0.1096
top acc: 0.3177 ::: bot acc: 0.2907
top acc: 0.1900 ::: bot acc: 0.1439
top acc: 0.1691 ::: bot acc: 0.1365
top acc: 0.1286 ::: bot acc: 0.0896
top acc: 0.1789 ::: bot acc: 0.1454
current epoch: 5
train loss is 0.012816
average val loss: 0.013009, accuracy: 0.1504
average test loss: 0.013056, accuracy: 0.1504
case acc: 0.11157426
case acc: 0.27468273
case acc: 0.15049356
case acc: 0.13195573
case acc: 0.09641093
case acc: 0.1372964
top acc: 0.1275 ::: bot acc: 0.0961
top acc: 0.2883 ::: bot acc: 0.2618
top acc: 0.1728 ::: bot acc: 0.1271
top acc: 0.1494 ::: bot acc: 0.1160
top acc: 0.1165 ::: bot acc: 0.0781
top acc: 0.1541 ::: bot acc: 0.1216
current epoch: 6
train loss is 0.012546
average val loss: 0.002934, accuracy: 0.0555
average test loss: 0.002973, accuracy: 0.0562
case acc: 0.02155548
case acc: 0.16605248
case acc: 0.05400503
case acc: 0.036553703
case acc: 0.017326437
case acc: 0.041419435
top acc: 0.0361 ::: bot acc: 0.0086
top acc: 0.1793 ::: bot acc: 0.1532
top acc: 0.0766 ::: bot acc: 0.0316
top acc: 0.0536 ::: bot acc: 0.0211
top acc: 0.0327 ::: bot acc: 0.0075
top acc: 0.0586 ::: bot acc: 0.0255
current epoch: 7
train loss is 0.006966
average val loss: 0.001098, accuracy: 0.0357
average test loss: 0.001137, accuracy: 0.0365
case acc: 0.033409413
case acc: 0.09614733
case acc: 0.018023184
case acc: 0.022813441
case acc: 0.032208916
case acc: 0.016282914
top acc: 0.0185 ::: bot acc: 0.0486
top acc: 0.1095 ::: bot acc: 0.0833
top acc: 0.0154 ::: bot acc: 0.0315
top acc: 0.0113 ::: bot acc: 0.0358
top acc: 0.0155 ::: bot acc: 0.0492
top acc: 0.0077 ::: bot acc: 0.0282
current epoch: 8
train loss is 0.003088
average val loss: 0.000926, accuracy: 0.0287
average test loss: 0.000963, accuracy: 0.0302
case acc: 0.02267146
case acc: 0.095501065
case acc: 0.01712354
case acc: 0.015302763
case acc: 0.018061053
case acc: 0.012499715
top acc: 0.0105 ::: bot acc: 0.0359
top acc: 0.1092 ::: bot acc: 0.0826
top acc: 0.0217 ::: bot acc: 0.0249
top acc: 0.0109 ::: bot acc: 0.0248
top acc: 0.0105 ::: bot acc: 0.0307
top acc: 0.0191 ::: bot acc: 0.0147
current epoch: 9
train loss is 0.002196
average val loss: 0.001012, accuracy: 0.0289
average test loss: 0.001052, accuracy: 0.0301
case acc: 0.01225567
case acc: 0.10138306
case acc: 0.019112358
case acc: 0.0131672835
case acc: 0.015234596
case acc: 0.019379593
top acc: 0.0131 ::: bot acc: 0.0195
top acc: 0.1149 ::: bot acc: 0.0886
top acc: 0.0337 ::: bot acc: 0.0128
top acc: 0.0243 ::: bot acc: 0.0100
top acc: 0.0287 ::: bot acc: 0.0101
top acc: 0.0357 ::: bot acc: 0.0058
current epoch: 10
train loss is 0.002078
average val loss: 0.000918, accuracy: 0.0286
average test loss: 0.000962, accuracy: 0.0296
case acc: 0.011655079
case acc: 0.094890945
case acc: 0.019354029
case acc: 0.013545183
case acc: 0.017382521
case acc: 0.0209764
top acc: 0.0159 ::: bot acc: 0.0162
top acc: 0.1091 ::: bot acc: 0.0819
top acc: 0.0335 ::: bot acc: 0.0138
top acc: 0.0258 ::: bot acc: 0.0087
top acc: 0.0333 ::: bot acc: 0.0070
top acc: 0.0372 ::: bot acc: 0.0069
current epoch: 11
train loss is 0.001899
average val loss: 0.000781, accuracy: 0.0270
average test loss: 0.000818, accuracy: 0.0279
case acc: 0.011665605
case acc: 0.08582057
case acc: 0.018345138
case acc: 0.012861399
case acc: 0.018221475
case acc: 0.020227237
top acc: 0.0163 ::: bot acc: 0.0159
top acc: 0.0992 ::: bot acc: 0.0733
top acc: 0.0301 ::: bot acc: 0.0163
top acc: 0.0247 ::: bot acc: 0.0090
top acc: 0.0349 ::: bot acc: 0.0071
top acc: 0.0364 ::: bot acc: 0.0061
current epoch: 12
train loss is 0.001627
average val loss: 0.000718, accuracy: 0.0269
average test loss: 0.000759, accuracy: 0.0277
case acc: 0.011732599
case acc: 0.08009924
case acc: 0.018281527
case acc: 0.013664636
case acc: 0.020554034
case acc: 0.021746151
top acc: 0.0184 ::: bot acc: 0.0135
top acc: 0.0938 ::: bot acc: 0.0667
top acc: 0.0300 ::: bot acc: 0.0168
top acc: 0.0262 ::: bot acc: 0.0081
top acc: 0.0385 ::: bot acc: 0.0068
top acc: 0.0383 ::: bot acc: 0.0070
current epoch: 13
train loss is 0.001400
average val loss: 0.000628, accuracy: 0.0257
average test loss: 0.000660, accuracy: 0.0263
case acc: 0.011812917
case acc: 0.072627164
case acc: 0.017767062
case acc: 0.013280463
case acc: 0.021135008
case acc: 0.021180168
top acc: 0.0191 ::: bot acc: 0.0130
top acc: 0.0862 ::: bot acc: 0.0596
top acc: 0.0280 ::: bot acc: 0.0186
top acc: 0.0256 ::: bot acc: 0.0082
top acc: 0.0393 ::: bot acc: 0.0066
top acc: 0.0376 ::: bot acc: 0.0067
current epoch: 14
train loss is 0.001198
average val loss: 0.000541, accuracy: 0.0243
average test loss: 0.000578, accuracy: 0.0251
case acc: 0.011840097
case acc: 0.06554817
case acc: 0.017471857
case acc: 0.013349783
case acc: 0.02123205
case acc: 0.021048704
top acc: 0.0197 ::: bot acc: 0.0121
top acc: 0.0794 ::: bot acc: 0.0523
top acc: 0.0268 ::: bot acc: 0.0205
top acc: 0.0253 ::: bot acc: 0.0089
top acc: 0.0392 ::: bot acc: 0.0071
top acc: 0.0371 ::: bot acc: 0.0068
current epoch: 15
train loss is 0.001026
average val loss: 0.000491, accuracy: 0.0236
average test loss: 0.000526, accuracy: 0.0243
case acc: 0.012182726
case acc: 0.060525134
case acc: 0.017343318
case acc: 0.013668716
case acc: 0.02138763
case acc: 0.020988915
top acc: 0.0210 ::: bot acc: 0.0108
top acc: 0.0741 ::: bot acc: 0.0475
top acc: 0.0259 ::: bot acc: 0.0207
top acc: 0.0258 ::: bot acc: 0.0086
top acc: 0.0393 ::: bot acc: 0.0074
top acc: 0.0372 ::: bot acc: 0.0065
current epoch: 16
train loss is 0.000943
average val loss: 0.000502, accuracy: 0.0247
average test loss: 0.000543, accuracy: 0.0255
case acc: 0.013683418
case acc: 0.058880717
case acc: 0.018004801
case acc: 0.015118048
case acc: 0.023742769
case acc: 0.02334978
top acc: 0.0251 ::: bot acc: 0.0077
top acc: 0.0727 ::: bot acc: 0.0455
top acc: 0.0290 ::: bot acc: 0.0179
top acc: 0.0291 ::: bot acc: 0.0065
top acc: 0.0425 ::: bot acc: 0.0084
top acc: 0.0402 ::: bot acc: 0.0083
current epoch: 17
train loss is 0.000883
average val loss: 0.000512, accuracy: 0.0257
average test loss: 0.000548, accuracy: 0.0262
case acc: 0.0153257055
case acc: 0.056886088
case acc: 0.01851847
case acc: 0.016564105
case acc: 0.025168989
case acc: 0.024762154
top acc: 0.0280 ::: bot acc: 0.0067
top acc: 0.0706 ::: bot acc: 0.0438
top acc: 0.0310 ::: bot acc: 0.0155
top acc: 0.0313 ::: bot acc: 0.0062
top acc: 0.0437 ::: bot acc: 0.0090
top acc: 0.0416 ::: bot acc: 0.0092
current epoch: 18
train loss is 0.000803
average val loss: 0.000483, accuracy: 0.0253
average test loss: 0.000515, accuracy: 0.0256
case acc: 0.016142977
case acc: 0.053340867
case acc: 0.01865949
case acc: 0.016855408
case acc: 0.02433891
case acc: 0.024332184
top acc: 0.0292 ::: bot acc: 0.0065
top acc: 0.0673 ::: bot acc: 0.0406
top acc: 0.0319 ::: bot acc: 0.0146
top acc: 0.0324 ::: bot acc: 0.0057
top acc: 0.0429 ::: bot acc: 0.0089
top acc: 0.0413 ::: bot acc: 0.0088
current epoch: 19
train loss is 0.000765
average val loss: 0.000449, accuracy: 0.0245
average test loss: 0.000487, accuracy: 0.0251
case acc: 0.016659424
case acc: 0.050180465
case acc: 0.019206114
case acc: 0.017199239
case acc: 0.023659281
case acc: 0.023828683
top acc: 0.0302 ::: bot acc: 0.0063
top acc: 0.0639 ::: bot acc: 0.0374
top acc: 0.0328 ::: bot acc: 0.0146
top acc: 0.0326 ::: bot acc: 0.0061
top acc: 0.0425 ::: bot acc: 0.0084
top acc: 0.0404 ::: bot acc: 0.0083
current epoch: 20
train loss is 0.000699
average val loss: 0.000446, accuracy: 0.0247
average test loss: 0.000481, accuracy: 0.0252
case acc: 0.017957332
case acc: 0.048314646
case acc: 0.01945629
case acc: 0.018059185
case acc: 0.023508305
case acc: 0.023933103
top acc: 0.0320 ::: bot acc: 0.0065
top acc: 0.0620 ::: bot acc: 0.0354
top acc: 0.0342 ::: bot acc: 0.0126
top acc: 0.0335 ::: bot acc: 0.0063
top acc: 0.0424 ::: bot acc: 0.0083
top acc: 0.0406 ::: bot acc: 0.0089
current epoch: 21
train loss is 0.000689
average val loss: 0.000443, accuracy: 0.0249
average test loss: 0.000476, accuracy: 0.0253
case acc: 0.019043844
case acc: 0.046499778
case acc: 0.020000245
case acc: 0.019041996
case acc: 0.023105163
case acc: 0.024076402
top acc: 0.0333 ::: bot acc: 0.0069
top acc: 0.0603 ::: bot acc: 0.0336
top acc: 0.0356 ::: bot acc: 0.0114
top acc: 0.0348 ::: bot acc: 0.0067
top acc: 0.0417 ::: bot acc: 0.0080
top acc: 0.0411 ::: bot acc: 0.0086
current epoch: 22
train loss is 0.000650
average val loss: 0.000496, accuracy: 0.0269
average test loss: 0.000532, accuracy: 0.0274
case acc: 0.022081379
case acc: 0.047457445
case acc: 0.021940956
case acc: 0.021729818
case acc: 0.024855085
case acc: 0.02614884
top acc: 0.0371 ::: bot acc: 0.0086
top acc: 0.0613 ::: bot acc: 0.0345
top acc: 0.0395 ::: bot acc: 0.0097
top acc: 0.0381 ::: bot acc: 0.0084
top acc: 0.0438 ::: bot acc: 0.0087
top acc: 0.0431 ::: bot acc: 0.0102
current epoch: 23
train loss is 0.000647
average val loss: 0.000521, accuracy: 0.0280
average test loss: 0.000554, accuracy: 0.0283
case acc: 0.02410298
case acc: 0.046952106
case acc: 0.023664393
case acc: 0.023032784
case acc: 0.025566109
case acc: 0.026512027
top acc: 0.0395 ::: bot acc: 0.0097
top acc: 0.0606 ::: bot acc: 0.0338
top acc: 0.0421 ::: bot acc: 0.0096
top acc: 0.0397 ::: bot acc: 0.0093
top acc: 0.0447 ::: bot acc: 0.0092
top acc: 0.0433 ::: bot acc: 0.0110
current epoch: 24
train loss is 0.000603
average val loss: 0.000465, accuracy: 0.0262
average test loss: 0.000501, accuracy: 0.0266
case acc: 0.023197176
case acc: 0.043736827
case acc: 0.023360327
case acc: 0.021643963
case acc: 0.023583531
case acc: 0.024267774
top acc: 0.0387 ::: bot acc: 0.0089
top acc: 0.0574 ::: bot acc: 0.0307
top acc: 0.0417 ::: bot acc: 0.0096
top acc: 0.0381 ::: bot acc: 0.0081
top acc: 0.0423 ::: bot acc: 0.0081
top acc: 0.0412 ::: bot acc: 0.0089
current epoch: 25
train loss is 0.000572
average val loss: 0.000515, accuracy: 0.0280
average test loss: 0.000547, accuracy: 0.0283
case acc: 0.025798386
case acc: 0.04462452
case acc: 0.025570601
case acc: 0.023725683
case acc: 0.024872003
case acc: 0.02513732
top acc: 0.0415 ::: bot acc: 0.0112
top acc: 0.0582 ::: bot acc: 0.0317
top acc: 0.0449 ::: bot acc: 0.0096
top acc: 0.0403 ::: bot acc: 0.0099
top acc: 0.0437 ::: bot acc: 0.0089
top acc: 0.0422 ::: bot acc: 0.0094
current epoch: 26
train loss is 0.000566
average val loss: 0.000519, accuracy: 0.0282
average test loss: 0.000555, accuracy: 0.0286
case acc: 0.026654473
case acc: 0.04417695
case acc: 0.026791306
case acc: 0.024521548
case acc: 0.024598576
case acc: 0.024988782
top acc: 0.0422 ::: bot acc: 0.0119
top acc: 0.0577 ::: bot acc: 0.0312
top acc: 0.0466 ::: bot acc: 0.0101
top acc: 0.0412 ::: bot acc: 0.0105
top acc: 0.0437 ::: bot acc: 0.0086
top acc: 0.0419 ::: bot acc: 0.0095
current epoch: 27
train loss is 0.000566
average val loss: 0.000578, accuracy: 0.0301
average test loss: 0.000610, accuracy: 0.0304
case acc: 0.029093927
case acc: 0.045239538
case acc: 0.0293757
case acc: 0.026896698
case acc: 0.025795797
case acc: 0.02615959
top acc: 0.0450 ::: bot acc: 0.0140
top acc: 0.0589 ::: bot acc: 0.0321
top acc: 0.0501 ::: bot acc: 0.0111
top acc: 0.0438 ::: bot acc: 0.0125
top acc: 0.0448 ::: bot acc: 0.0094
top acc: 0.0430 ::: bot acc: 0.0105
current epoch: 28
train loss is 0.000585
average val loss: 0.000658, accuracy: 0.0326
average test loss: 0.000691, accuracy: 0.0329
case acc: 0.032210674
case acc: 0.04697777
case acc: 0.032831207
case acc: 0.029504184
case acc: 0.02778509
case acc: 0.028179867
top acc: 0.0479 ::: bot acc: 0.0171
top acc: 0.0607 ::: bot acc: 0.0340
top acc: 0.0539 ::: bot acc: 0.0136
top acc: 0.0464 ::: bot acc: 0.0148
top acc: 0.0471 ::: bot acc: 0.0111
top acc: 0.0454 ::: bot acc: 0.0121
current epoch: 29
train loss is 0.000579
average val loss: 0.000751, accuracy: 0.0354
average test loss: 0.000786, accuracy: 0.0356
case acc: 0.035341308
case acc: 0.04899853
case acc: 0.036153078
case acc: 0.03248327
case acc: 0.030430768
case acc: 0.03025854
top acc: 0.0513 ::: bot acc: 0.0199
top acc: 0.0627 ::: bot acc: 0.0360
top acc: 0.0577 ::: bot acc: 0.0159
top acc: 0.0498 ::: bot acc: 0.0174
top acc: 0.0500 ::: bot acc: 0.0130
top acc: 0.0475 ::: bot acc: 0.0141
current epoch: 30
train loss is 0.000608
average val loss: 0.000783, accuracy: 0.0362
average test loss: 0.000815, accuracy: 0.0364
case acc: 0.036416918
case acc: 0.04907014
case acc: 0.037679434
case acc: 0.03342298
case acc: 0.031561695
case acc: 0.030382948
top acc: 0.0527 ::: bot acc: 0.0208
top acc: 0.0630 ::: bot acc: 0.0360
top acc: 0.0594 ::: bot acc: 0.0172
top acc: 0.0504 ::: bot acc: 0.0186
top acc: 0.0511 ::: bot acc: 0.0139
top acc: 0.0474 ::: bot acc: 0.0143
current epoch: 31
train loss is 0.000601
average val loss: 0.000804, accuracy: 0.0369
average test loss: 0.000841, accuracy: 0.0371
case acc: 0.03729487
case acc: 0.04890654
case acc: 0.039239194
case acc: 0.034293566
case acc: 0.032478813
case acc: 0.030648015
top acc: 0.0535 ::: bot acc: 0.0216
top acc: 0.0625 ::: bot acc: 0.0361
top acc: 0.0610 ::: bot acc: 0.0187
top acc: 0.0517 ::: bot acc: 0.0191
top acc: 0.0524 ::: bot acc: 0.0148
top acc: 0.0478 ::: bot acc: 0.0143
current epoch: 32
train loss is 0.000610
average val loss: 0.000844, accuracy: 0.0379
average test loss: 0.000875, accuracy: 0.0380
case acc: 0.03824822
case acc: 0.048975617
case acc: 0.040845584
case acc: 0.035435453
case acc: 0.03364229
case acc: 0.031054726
top acc: 0.0544 ::: bot acc: 0.0225
top acc: 0.0625 ::: bot acc: 0.0361
top acc: 0.0626 ::: bot acc: 0.0200
top acc: 0.0526 ::: bot acc: 0.0204
top acc: 0.0538 ::: bot acc: 0.0156
top acc: 0.0482 ::: bot acc: 0.0148
current epoch: 33
train loss is 0.000599
average val loss: 0.000880, accuracy: 0.0389
average test loss: 0.000911, accuracy: 0.0390
case acc: 0.03920594
case acc: 0.048976302
case acc: 0.042236455
case acc: 0.03684641
case acc: 0.034963284
case acc: 0.031807713
top acc: 0.0554 ::: bot acc: 0.0235
top acc: 0.0628 ::: bot acc: 0.0359
top acc: 0.0642 ::: bot acc: 0.0213
top acc: 0.0542 ::: bot acc: 0.0217
top acc: 0.0549 ::: bot acc: 0.0170
top acc: 0.0489 ::: bot acc: 0.0156
current epoch: 34
train loss is 0.000601
average val loss: 0.000868, accuracy: 0.0386
average test loss: 0.000902, accuracy: 0.0388
case acc: 0.038993627
case acc: 0.047690302
case acc: 0.042427592
case acc: 0.03685301
case acc: 0.035347097
case acc: 0.031417858
top acc: 0.0552 ::: bot acc: 0.0232
top acc: 0.0613 ::: bot acc: 0.0347
top acc: 0.0644 ::: bot acc: 0.0217
top acc: 0.0543 ::: bot acc: 0.0216
top acc: 0.0556 ::: bot acc: 0.0170
top acc: 0.0487 ::: bot acc: 0.0153
current epoch: 35
train loss is 0.000585
average val loss: 0.000816, accuracy: 0.0373
average test loss: 0.000850, accuracy: 0.0375
case acc: 0.037452217
case acc: 0.04528854
case acc: 0.04157326
case acc: 0.035923176
case acc: 0.034534108
case acc: 0.030256977
top acc: 0.0537 ::: bot acc: 0.0217
top acc: 0.0588 ::: bot acc: 0.0324
top acc: 0.0634 ::: bot acc: 0.0207
top acc: 0.0532 ::: bot acc: 0.0207
top acc: 0.0546 ::: bot acc: 0.0163
top acc: 0.0474 ::: bot acc: 0.0139
current epoch: 36
train loss is 0.000556
average val loss: 0.000793, accuracy: 0.0368
average test loss: 0.000825, accuracy: 0.0369
case acc: 0.036371946
case acc: 0.043663222
case acc: 0.041026283
case acc: 0.035833318
case acc: 0.034368597
case acc: 0.030058488
top acc: 0.0524 ::: bot acc: 0.0208
top acc: 0.0573 ::: bot acc: 0.0307
top acc: 0.0630 ::: bot acc: 0.0202
top acc: 0.0532 ::: bot acc: 0.0208
top acc: 0.0544 ::: bot acc: 0.0162
top acc: 0.0472 ::: bot acc: 0.0138
current epoch: 37
train loss is 0.000518
average val loss: 0.000656, accuracy: 0.0330
average test loss: 0.000689, accuracy: 0.0332
case acc: 0.032135528
case acc: 0.03868596
case acc: 0.037513632
case acc: 0.032450557
case acc: 0.03136375
case acc: 0.026761321
top acc: 0.0482 ::: bot acc: 0.0168
top acc: 0.0525 ::: bot acc: 0.0256
top acc: 0.0591 ::: bot acc: 0.0172
top acc: 0.0497 ::: bot acc: 0.0176
top acc: 0.0510 ::: bot acc: 0.0138
top acc: 0.0437 ::: bot acc: 0.0110
current epoch: 38
train loss is 0.000463
average val loss: 0.000535, accuracy: 0.0293
average test loss: 0.000569, accuracy: 0.0295
case acc: 0.028383559
case acc: 0.033634
case acc: 0.033638902
case acc: 0.029207552
case acc: 0.028652253
case acc: 0.02367566
top acc: 0.0441 ::: bot acc: 0.0135
top acc: 0.0473 ::: bot acc: 0.0207
top acc: 0.0551 ::: bot acc: 0.0139
top acc: 0.0464 ::: bot acc: 0.0145
top acc: 0.0482 ::: bot acc: 0.0115
top acc: 0.0404 ::: bot acc: 0.0083
current epoch: 39
train loss is 0.000418
average val loss: 0.000454, accuracy: 0.0265
average test loss: 0.000486, accuracy: 0.0268
case acc: 0.025205601
case acc: 0.029398289
case acc: 0.030976443
case acc: 0.026752299
case acc: 0.02685295
case acc: 0.021466019
top acc: 0.0407 ::: bot acc: 0.0106
top acc: 0.0432 ::: bot acc: 0.0164
top acc: 0.0518 ::: bot acc: 0.0123
top acc: 0.0436 ::: bot acc: 0.0125
top acc: 0.0461 ::: bot acc: 0.0102
top acc: 0.0379 ::: bot acc: 0.0069
current epoch: 40
train loss is 0.000386
average val loss: 0.000424, accuracy: 0.0255
average test loss: 0.000457, accuracy: 0.0258
case acc: 0.02390771
case acc: 0.027469022
case acc: 0.02977734
case acc: 0.02601955
case acc: 0.0264604
case acc: 0.021059228
top acc: 0.0391 ::: bot acc: 0.0097
top acc: 0.0410 ::: bot acc: 0.0145
top acc: 0.0505 ::: bot acc: 0.0115
top acc: 0.0428 ::: bot acc: 0.0119
top acc: 0.0457 ::: bot acc: 0.0101
top acc: 0.0373 ::: bot acc: 0.0068
current epoch: 41
train loss is 0.000361
average val loss: 0.000342, accuracy: 0.0224
average test loss: 0.000377, accuracy: 0.0228
case acc: 0.020599073
case acc: 0.022938358
case acc: 0.02674515
case acc: 0.023335513
case acc: 0.024066614
case acc: 0.018913964
top acc: 0.0355 ::: bot acc: 0.0075
top acc: 0.0365 ::: bot acc: 0.0101
top acc: 0.0466 ::: bot acc: 0.0100
top acc: 0.0401 ::: bot acc: 0.0095
top acc: 0.0426 ::: bot acc: 0.0086
top acc: 0.0347 ::: bot acc: 0.0056
current epoch: 42
train loss is 0.000325
average val loss: 0.000251, accuracy: 0.0185
average test loss: 0.000287, accuracy: 0.0190
case acc: 0.016593462
case acc: 0.01769561
case acc: 0.02322133
case acc: 0.019607488
case acc: 0.020651445
case acc: 0.01633104
top acc: 0.0300 ::: bot acc: 0.0063
top acc: 0.0309 ::: bot acc: 0.0058
top acc: 0.0415 ::: bot acc: 0.0096
top acc: 0.0357 ::: bot acc: 0.0072
top acc: 0.0385 ::: bot acc: 0.0069
top acc: 0.0310 ::: bot acc: 0.0051
current epoch: 43
train loss is 0.000298
average val loss: 0.000190, accuracy: 0.0156
average test loss: 0.000227, accuracy: 0.0163
case acc: 0.013971528
case acc: 0.013791247
case acc: 0.02042581
case acc: 0.016914977
case acc: 0.018183988
case acc: 0.014597377
top acc: 0.0256 ::: bot acc: 0.0075
top acc: 0.0256 ::: bot acc: 0.0045
top acc: 0.0367 ::: bot acc: 0.0112
top acc: 0.0321 ::: bot acc: 0.0059
top acc: 0.0349 ::: bot acc: 0.0068
top acc: 0.0278 ::: bot acc: 0.0065
current epoch: 44
train loss is 0.000282
average val loss: 0.000162, accuracy: 0.0141
average test loss: 0.000197, accuracy: 0.0149
case acc: 0.0126231
case acc: 0.0116015
case acc: 0.018967994
case acc: 0.015290884
case acc: 0.01709476
case acc: 0.013801472
top acc: 0.0225 ::: bot acc: 0.0096
top acc: 0.0219 ::: bot acc: 0.0054
top acc: 0.0332 ::: bot acc: 0.0135
top acc: 0.0297 ::: bot acc: 0.0061
top acc: 0.0331 ::: bot acc: 0.0073
top acc: 0.0259 ::: bot acc: 0.0077
current epoch: 45
train loss is 0.000273
average val loss: 0.000134, accuracy: 0.0127
average test loss: 0.000172, accuracy: 0.0137
case acc: 0.011735147
case acc: 0.010182447
case acc: 0.017989233
case acc: 0.0137749165
case acc: 0.015828429
case acc: 0.0129837915
top acc: 0.0187 ::: bot acc: 0.0134
top acc: 0.0176 ::: bot acc: 0.0091
top acc: 0.0290 ::: bot acc: 0.0179
top acc: 0.0267 ::: bot acc: 0.0073
top acc: 0.0303 ::: bot acc: 0.0089
top acc: 0.0236 ::: bot acc: 0.0097
current epoch: 46
train loss is 0.000266
average val loss: 0.000117, accuracy: 0.0118
average test loss: 0.000154, accuracy: 0.0130
case acc: 0.011928151
case acc: 0.010271685
case acc: 0.016862594
case acc: 0.012236331
case acc: 0.014339238
case acc: 0.012270635
top acc: 0.0131 ::: bot acc: 0.0190
top acc: 0.0110 ::: bot acc: 0.0157
top acc: 0.0226 ::: bot acc: 0.0241
top acc: 0.0219 ::: bot acc: 0.0117
top acc: 0.0253 ::: bot acc: 0.0135
top acc: 0.0198 ::: bot acc: 0.0138
current epoch: 47
train loss is 0.000273
average val loss: 0.000121, accuracy: 0.0121
average test loss: 0.000158, accuracy: 0.0134
case acc: 0.013308105
case acc: 0.011508048
case acc: 0.017293284
case acc: 0.012095283
case acc: 0.013873383
case acc: 0.012103761
top acc: 0.0104 ::: bot acc: 0.0225
top acc: 0.0073 ::: bot acc: 0.0196
top acc: 0.0184 ::: bot acc: 0.0283
top acc: 0.0194 ::: bot acc: 0.0142
top acc: 0.0230 ::: bot acc: 0.0155
top acc: 0.0178 ::: bot acc: 0.0156
current epoch: 48
train loss is 0.000287
average val loss: 0.000138, accuracy: 0.0132
average test loss: 0.000177, accuracy: 0.0145
case acc: 0.015419057
case acc: 0.01430897
case acc: 0.018918233
case acc: 0.012447123
case acc: 0.01384924
case acc: 0.012214232
top acc: 0.0093 ::: bot acc: 0.0262
top acc: 0.0053 ::: bot acc: 0.0249
top acc: 0.0135 ::: bot acc: 0.0334
top acc: 0.0161 ::: bot acc: 0.0172
top acc: 0.0209 ::: bot acc: 0.0178
top acc: 0.0160 ::: bot acc: 0.0174
current epoch: 49
train loss is 0.000308
average val loss: 0.000189, accuracy: 0.0157
average test loss: 0.000229, accuracy: 0.0172
case acc: 0.019275902
case acc: 0.019726643
case acc: 0.022282315
case acc: 0.014199439
case acc: 0.0146393245
case acc: 0.012939421
top acc: 0.0097 ::: bot acc: 0.0320
top acc: 0.0081 ::: bot acc: 0.0316
top acc: 0.0094 ::: bot acc: 0.0406
top acc: 0.0118 ::: bot acc: 0.0223
top acc: 0.0172 ::: bot acc: 0.0218
top acc: 0.0128 ::: bot acc: 0.0206
current epoch: 50
train loss is 0.000344
average val loss: 0.000302, accuracy: 0.0205
average test loss: 0.000345, accuracy: 0.0220
case acc: 0.025125783
case acc: 0.02779421
case acc: 0.029264187
case acc: 0.018040292
case acc: 0.01657759
case acc: 0.015073774
top acc: 0.0121 ::: bot acc: 0.0393
top acc: 0.0145 ::: bot acc: 0.0407
top acc: 0.0117 ::: bot acc: 0.0500
top acc: 0.0098 ::: bot acc: 0.0290
top acc: 0.0121 ::: bot acc: 0.0273
top acc: 0.0091 ::: bot acc: 0.0258
LME_Co_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6810 6810 6810
1.7082474 -0.6288155 0.08104724 -0.08406281
Validation: 762 762 762
Testing: 750 750 750
pre-processing time: 0.00018215179443359375
the split date is 2011-01-01
train dropout: 0.6 test dropout: 0.1
net initializing with time: 0.0022416114807128906
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012565
average val loss: 0.005220, accuracy: 0.0968
average test loss: 0.004956, accuracy: 0.0946
case acc: 0.1345356
case acc: 0.088540465
case acc: 0.0891706
case acc: 0.08463771
case acc: 0.11972842
case acc: 0.051179245
top acc: 0.1203 ::: bot acc: 0.1484
top acc: 0.1013 ::: bot acc: 0.0750
top acc: 0.0691 ::: bot acc: 0.1096
top acc: 0.0715 ::: bot acc: 0.0972
top acc: 0.0996 ::: bot acc: 0.1407
top acc: 0.0329 ::: bot acc: 0.0687
current epoch: 2
train loss is 0.007716
average val loss: 0.002931, accuracy: 0.0518
average test loss: 0.002932, accuracy: 0.0508
case acc: 0.03186991
case acc: 0.17434187
case acc: 0.019440982
case acc: 0.013502335
case acc: 0.030956255
case acc: 0.034559436
top acc: 0.0183 ::: bot acc: 0.0456
top acc: 0.1865 ::: bot acc: 0.1611
top acc: 0.0335 ::: bot acc: 0.0111
top acc: 0.0237 ::: bot acc: 0.0074
top acc: 0.0124 ::: bot acc: 0.0509
top acc: 0.0525 ::: bot acc: 0.0180
current epoch: 3
train loss is 0.008159
average val loss: 0.008174, accuracy: 0.1084
average test loss: 0.008431, accuracy: 0.1112
case acc: 0.059590317
case acc: 0.25058392
case acc: 0.10396052
case acc: 0.09347095
case acc: 0.050575893
case acc: 0.10917483
top acc: 0.0740 ::: bot acc: 0.0452
top acc: 0.2628 ::: bot acc: 0.2378
top acc: 0.1242 ::: bot acc: 0.0825
top acc: 0.1069 ::: bot acc: 0.0805
top acc: 0.0706 ::: bot acc: 0.0297
top acc: 0.1269 ::: bot acc: 0.0916
current epoch: 4
train loss is 0.010053
average val loss: 0.016832, accuracy: 0.1724
average test loss: 0.017217, accuracy: 0.1749
case acc: 0.13043629
case acc: 0.30759546
case acc: 0.17375806
case acc: 0.15642056
case acc: 0.11572618
case acc: 0.16537727
top acc: 0.1447 ::: bot acc: 0.1161
top acc: 0.3199 ::: bot acc: 0.2945
top acc: 0.1948 ::: bot acc: 0.1524
top acc: 0.1701 ::: bot acc: 0.1433
top acc: 0.1359 ::: bot acc: 0.0952
top acc: 0.1827 ::: bot acc: 0.1485
current epoch: 5
train loss is 0.012772
average val loss: 0.013131, accuracy: 0.1510
average test loss: 0.013468, accuracy: 0.1535
case acc: 0.11432597
case acc: 0.2763809
case acc: 0.15516964
case acc: 0.13423145
case acc: 0.10198001
case acc: 0.13918568
top acc: 0.1289 ::: bot acc: 0.0996
top acc: 0.2891 ::: bot acc: 0.2635
top acc: 0.1756 ::: bot acc: 0.1336
top acc: 0.1489 ::: bot acc: 0.1198
top acc: 0.1218 ::: bot acc: 0.0818
top acc: 0.1570 ::: bot acc: 0.1218
current epoch: 6
train loss is 0.012431
average val loss: 0.003150, accuracy: 0.0592
average test loss: 0.003253, accuracy: 0.0614
case acc: 0.025934309
case acc: 0.17042613
case acc: 0.061011817
case acc: 0.04139223
case acc: 0.023071915
case acc: 0.04634398
top acc: 0.0401 ::: bot acc: 0.0121
top acc: 0.1830 ::: bot acc: 0.1578
top acc: 0.0817 ::: bot acc: 0.0402
top acc: 0.0550 ::: bot acc: 0.0286
top acc: 0.0411 ::: bot acc: 0.0064
top acc: 0.0633 ::: bot acc: 0.0294
current epoch: 7
train loss is 0.007197
average val loss: 0.001123, accuracy: 0.0362
average test loss: 0.001081, accuracy: 0.0341
case acc: 0.030636935
case acc: 0.0973418
case acc: 0.015248568
case acc: 0.019005822
case acc: 0.02703772
case acc: 0.015337508
top acc: 0.0164 ::: bot acc: 0.0446
top acc: 0.1096 ::: bot acc: 0.0845
top acc: 0.0167 ::: bot acc: 0.0251
top acc: 0.0084 ::: bot acc: 0.0308
top acc: 0.0099 ::: bot acc: 0.0461
top acc: 0.0086 ::: bot acc: 0.0275
current epoch: 8
train loss is 0.003061
average val loss: 0.000944, accuracy: 0.0300
average test loss: 0.000930, accuracy: 0.0287
case acc: 0.01950436
case acc: 0.09599441
case acc: 0.015735641
case acc: 0.011669552
case acc: 0.015725857
case acc: 0.013309807
top acc: 0.0074 ::: bot acc: 0.0328
top acc: 0.1082 ::: bot acc: 0.0830
top acc: 0.0234 ::: bot acc: 0.0195
top acc: 0.0073 ::: bot acc: 0.0201
top acc: 0.0134 ::: bot acc: 0.0275
top acc: 0.0201 ::: bot acc: 0.0150
current epoch: 9
train loss is 0.002181
average val loss: 0.001129, accuracy: 0.0319
average test loss: 0.001165, accuracy: 0.0330
case acc: 0.0107038645
case acc: 0.10589364
case acc: 0.02244301
case acc: 0.014550337
case acc: 0.020830745
case acc: 0.023667559
top acc: 0.0169 ::: bot acc: 0.0118
top acc: 0.1182 ::: bot acc: 0.0929
top acc: 0.0386 ::: bot acc: 0.0100
top acc: 0.0254 ::: bot acc: 0.0074
top acc: 0.0376 ::: bot acc: 0.0070
top acc: 0.0397 ::: bot acc: 0.0085
current epoch: 10
train loss is 0.002060
average val loss: 0.001024, accuracy: 0.0312
average test loss: 0.001064, accuracy: 0.0325
case acc: 0.011118302
case acc: 0.09901323
case acc: 0.021826353
case acc: 0.0148796
case acc: 0.023381343
case acc: 0.024735296
top acc: 0.0191 ::: bot acc: 0.0096
top acc: 0.1113 ::: bot acc: 0.0864
top acc: 0.0380 ::: bot acc: 0.0103
top acc: 0.0260 ::: bot acc: 0.0068
top acc: 0.0414 ::: bot acc: 0.0062
top acc: 0.0412 ::: bot acc: 0.0093
current epoch: 11
train loss is 0.001855
average val loss: 0.000837, accuracy: 0.0285
average test loss: 0.000865, accuracy: 0.0294
case acc: 0.01082877
case acc: 0.088111445
case acc: 0.019042797
case acc: 0.013088548
case acc: 0.022891842
case acc: 0.022206195
top acc: 0.0176 ::: bot acc: 0.0111
top acc: 0.1004 ::: bot acc: 0.0751
top acc: 0.0329 ::: bot acc: 0.0113
top acc: 0.0234 ::: bot acc: 0.0069
top acc: 0.0408 ::: bot acc: 0.0063
top acc: 0.0383 ::: bot acc: 0.0078
current epoch: 12
train loss is 0.001582
average val loss: 0.000807, accuracy: 0.0291
average test loss: 0.000840, accuracy: 0.0303
case acc: 0.012213049
case acc: 0.08357856
case acc: 0.019504927
case acc: 0.014814421
case acc: 0.026772073
case acc: 0.024677243
top acc: 0.0213 ::: bot acc: 0.0083
top acc: 0.0958 ::: bot acc: 0.0708
top acc: 0.0341 ::: bot acc: 0.0110
top acc: 0.0257 ::: bot acc: 0.0067
top acc: 0.0462 ::: bot acc: 0.0076
top acc: 0.0414 ::: bot acc: 0.0092
current epoch: 13
train loss is 0.001378
average val loss: 0.000759, accuracy: 0.0290
average test loss: 0.000789, accuracy: 0.0302
case acc: 0.013515997
case acc: 0.07822332
case acc: 0.019571658
case acc: 0.015696112
case acc: 0.028431721
case acc: 0.025707103
top acc: 0.0242 ::: bot acc: 0.0071
top acc: 0.0907 ::: bot acc: 0.0651
top acc: 0.0339 ::: bot acc: 0.0113
top acc: 0.0274 ::: bot acc: 0.0069
top acc: 0.0482 ::: bot acc: 0.0084
top acc: 0.0421 ::: bot acc: 0.0103
current epoch: 14
train loss is 0.001203
average val loss: 0.000688, accuracy: 0.0282
average test loss: 0.000718, accuracy: 0.0294
case acc: 0.014099027
case acc: 0.072137974
case acc: 0.018889513
case acc: 0.015926378
case acc: 0.029112952
case acc: 0.026014188
top acc: 0.0253 ::: bot acc: 0.0064
top acc: 0.0843 ::: bot acc: 0.0590
top acc: 0.0331 ::: bot acc: 0.0109
top acc: 0.0274 ::: bot acc: 0.0071
top acc: 0.0486 ::: bot acc: 0.0094
top acc: 0.0429 ::: bot acc: 0.0098
current epoch: 15
train loss is 0.001056
average val loss: 0.000628, accuracy: 0.0273
average test loss: 0.000660, accuracy: 0.0286
case acc: 0.014619757
case acc: 0.06723155
case acc: 0.018691413
case acc: 0.016371612
case acc: 0.028742444
case acc: 0.025731657
top acc: 0.0264 ::: bot acc: 0.0059
top acc: 0.0796 ::: bot acc: 0.0542
top acc: 0.0323 ::: bot acc: 0.0114
top acc: 0.0280 ::: bot acc: 0.0075
top acc: 0.0482 ::: bot acc: 0.0089
top acc: 0.0421 ::: bot acc: 0.0100
current epoch: 16
train loss is 0.000960
average val loss: 0.000609, accuracy: 0.0275
average test loss: 0.000645, accuracy: 0.0290
case acc: 0.016226653
case acc: 0.06381709
case acc: 0.019535162
case acc: 0.01794369
case acc: 0.029475963
case acc: 0.026880993
top acc: 0.0283 ::: bot acc: 0.0061
top acc: 0.0758 ::: bot acc: 0.0509
top acc: 0.0341 ::: bot acc: 0.0111
top acc: 0.0301 ::: bot acc: 0.0081
top acc: 0.0489 ::: bot acc: 0.0095
top acc: 0.0437 ::: bot acc: 0.0106
current epoch: 17
train loss is 0.000879
average val loss: 0.000618, accuracy: 0.0283
average test loss: 0.000656, accuracy: 0.0299
case acc: 0.018191136
case acc: 0.061694097
case acc: 0.021042326
case acc: 0.019552678
case acc: 0.03099572
case acc: 0.028123215
top acc: 0.0315 ::: bot acc: 0.0062
top acc: 0.0741 ::: bot acc: 0.0488
top acc: 0.0363 ::: bot acc: 0.0108
top acc: 0.0320 ::: bot acc: 0.0088
top acc: 0.0507 ::: bot acc: 0.0106
top acc: 0.0451 ::: bot acc: 0.0120
current epoch: 18
train loss is 0.000824
average val loss: 0.000549, accuracy: 0.0267
average test loss: 0.000578, accuracy: 0.0281
case acc: 0.017940877
case acc: 0.056584988
case acc: 0.020299066
case acc: 0.018773505
case acc: 0.028643046
case acc: 0.026385298
top acc: 0.0310 ::: bot acc: 0.0063
top acc: 0.0691 ::: bot acc: 0.0435
top acc: 0.0354 ::: bot acc: 0.0107
top acc: 0.0309 ::: bot acc: 0.0086
top acc: 0.0478 ::: bot acc: 0.0087
top acc: 0.0432 ::: bot acc: 0.0103
current epoch: 19
train loss is 0.000742
average val loss: 0.000534, accuracy: 0.0266
average test loss: 0.000570, accuracy: 0.0283
case acc: 0.019296193
case acc: 0.054478325
case acc: 0.021168873
case acc: 0.01968458
case acc: 0.028687637
case acc: 0.0264919
top acc: 0.0326 ::: bot acc: 0.0071
top acc: 0.0671 ::: bot acc: 0.0413
top acc: 0.0365 ::: bot acc: 0.0107
top acc: 0.0318 ::: bot acc: 0.0091
top acc: 0.0478 ::: bot acc: 0.0089
top acc: 0.0434 ::: bot acc: 0.0106
current epoch: 20
train loss is 0.000706
average val loss: 0.000569, accuracy: 0.0280
average test loss: 0.000608, accuracy: 0.0298
case acc: 0.02186492
case acc: 0.054002553
case acc: 0.023279142
case acc: 0.021744942
case acc: 0.029939108
case acc: 0.028032865
top acc: 0.0356 ::: bot acc: 0.0083
top acc: 0.0661 ::: bot acc: 0.0410
top acc: 0.0402 ::: bot acc: 0.0102
top acc: 0.0347 ::: bot acc: 0.0102
top acc: 0.0497 ::: bot acc: 0.0096
top acc: 0.0448 ::: bot acc: 0.0120
current epoch: 21
train loss is 0.000683
average val loss: 0.000544, accuracy: 0.0274
average test loss: 0.000577, accuracy: 0.0292
case acc: 0.022677084
case acc: 0.051062033
case acc: 0.02355934
case acc: 0.021963365
case acc: 0.028910393
case acc: 0.026951559
top acc: 0.0364 ::: bot acc: 0.0092
top acc: 0.0634 ::: bot acc: 0.0379
top acc: 0.0406 ::: bot acc: 0.0101
top acc: 0.0348 ::: bot acc: 0.0103
top acc: 0.0483 ::: bot acc: 0.0090
top acc: 0.0437 ::: bot acc: 0.0109
current epoch: 22
train loss is 0.000649
average val loss: 0.000528, accuracy: 0.0272
average test loss: 0.000568, accuracy: 0.0291
case acc: 0.023523798
case acc: 0.049329057
case acc: 0.024347074
case acc: 0.022332843
case acc: 0.02836519
case acc: 0.02665044
top acc: 0.0377 ::: bot acc: 0.0095
top acc: 0.0616 ::: bot acc: 0.0362
top acc: 0.0415 ::: bot acc: 0.0103
top acc: 0.0352 ::: bot acc: 0.0107
top acc: 0.0479 ::: bot acc: 0.0083
top acc: 0.0434 ::: bot acc: 0.0105
current epoch: 23
train loss is 0.000618
average val loss: 0.000607, accuracy: 0.0299
average test loss: 0.000654, accuracy: 0.0319
case acc: 0.027395412
case acc: 0.050995328
case acc: 0.027782725
case acc: 0.025571266
case acc: 0.030986099
case acc: 0.028664488
top acc: 0.0418 ::: bot acc: 0.0133
top acc: 0.0633 ::: bot acc: 0.0381
top acc: 0.0459 ::: bot acc: 0.0114
top acc: 0.0388 ::: bot acc: 0.0134
top acc: 0.0506 ::: bot acc: 0.0106
top acc: 0.0459 ::: bot acc: 0.0121
current epoch: 24
train loss is 0.000600
average val loss: 0.000597, accuracy: 0.0297
average test loss: 0.000644, accuracy: 0.0318
case acc: 0.02823239
case acc: 0.049298495
case acc: 0.02883717
case acc: 0.026010735
case acc: 0.030253068
case acc: 0.028052712
top acc: 0.0428 ::: bot acc: 0.0138
top acc: 0.0616 ::: bot acc: 0.0363
top acc: 0.0472 ::: bot acc: 0.0121
top acc: 0.0395 ::: bot acc: 0.0137
top acc: 0.0497 ::: bot acc: 0.0099
top acc: 0.0451 ::: bot acc: 0.0115
current epoch: 25
train loss is 0.000600
average val loss: 0.000627, accuracy: 0.0308
average test loss: 0.000676, accuracy: 0.0328
case acc: 0.030102493
case acc: 0.04902966
case acc: 0.030727694
case acc: 0.027553963
case acc: 0.030947557
case acc: 0.028670255
top acc: 0.0447 ::: bot acc: 0.0156
top acc: 0.0614 ::: bot acc: 0.0359
top acc: 0.0496 ::: bot acc: 0.0134
top acc: 0.0410 ::: bot acc: 0.0152
top acc: 0.0508 ::: bot acc: 0.0106
top acc: 0.0458 ::: bot acc: 0.0122
current epoch: 26
train loss is 0.000592
average val loss: 0.000645, accuracy: 0.0314
average test loss: 0.000695, accuracy: 0.0335
case acc: 0.03134983
case acc: 0.048469994
case acc: 0.032227296
case acc: 0.028604675
case acc: 0.031295292
case acc: 0.028998815
top acc: 0.0460 ::: bot acc: 0.0171
top acc: 0.0607 ::: bot acc: 0.0355
top acc: 0.0510 ::: bot acc: 0.0146
top acc: 0.0420 ::: bot acc: 0.0160
top acc: 0.0512 ::: bot acc: 0.0104
top acc: 0.0463 ::: bot acc: 0.0122
current epoch: 27
train loss is 0.000575
average val loss: 0.000729, accuracy: 0.0340
average test loss: 0.000789, accuracy: 0.0362
case acc: 0.034805443
case acc: 0.050256908
case acc: 0.035836764
case acc: 0.03154958
case acc: 0.03384373
case acc: 0.030839186
top acc: 0.0494 ::: bot acc: 0.0201
top acc: 0.0627 ::: bot acc: 0.0370
top acc: 0.0554 ::: bot acc: 0.0173
top acc: 0.0452 ::: bot acc: 0.0188
top acc: 0.0533 ::: bot acc: 0.0132
top acc: 0.0481 ::: bot acc: 0.0139
current epoch: 28
train loss is 0.000588
average val loss: 0.000665, accuracy: 0.0321
average test loss: 0.000720, accuracy: 0.0343
case acc: 0.033276524
case acc: 0.04743439
case acc: 0.03509126
case acc: 0.02994043
case acc: 0.031696554
case acc: 0.028625127
top acc: 0.0478 ::: bot acc: 0.0188
top acc: 0.0598 ::: bot acc: 0.0344
top acc: 0.0544 ::: bot acc: 0.0168
top acc: 0.0435 ::: bot acc: 0.0172
top acc: 0.0513 ::: bot acc: 0.0113
top acc: 0.0458 ::: bot acc: 0.0120
current epoch: 29
train loss is 0.000571
average val loss: 0.000760, accuracy: 0.0349
average test loss: 0.000820, accuracy: 0.0371
case acc: 0.036656216
case acc: 0.049368527
case acc: 0.038820654
case acc: 0.033156056
case acc: 0.034085423
case acc: 0.030642595
top acc: 0.0511 ::: bot acc: 0.0222
top acc: 0.0616 ::: bot acc: 0.0365
top acc: 0.0584 ::: bot acc: 0.0198
top acc: 0.0467 ::: bot acc: 0.0201
top acc: 0.0537 ::: bot acc: 0.0137
top acc: 0.0479 ::: bot acc: 0.0139
current epoch: 30
train loss is 0.000596
average val loss: 0.000832, accuracy: 0.0369
average test loss: 0.000900, accuracy: 0.0392
case acc: 0.03909399
case acc: 0.05081816
case acc: 0.041676465
case acc: 0.035449006
case acc: 0.036113314
case acc: 0.032159302
top acc: 0.0535 ::: bot acc: 0.0248
top acc: 0.0630 ::: bot acc: 0.0381
top acc: 0.0615 ::: bot acc: 0.0224
top acc: 0.0492 ::: bot acc: 0.0221
top acc: 0.0558 ::: bot acc: 0.0157
top acc: 0.0494 ::: bot acc: 0.0151
current epoch: 31
train loss is 0.000590
average val loss: 0.000895, accuracy: 0.0385
average test loss: 0.000970, accuracy: 0.0409
case acc: 0.040865082
case acc: 0.051859293
case acc: 0.044404365
case acc: 0.037392966
case acc: 0.037712988
case acc: 0.03321084
top acc: 0.0553 ::: bot acc: 0.0263
top acc: 0.0640 ::: bot acc: 0.0389
top acc: 0.0643 ::: bot acc: 0.0247
top acc: 0.0512 ::: bot acc: 0.0241
top acc: 0.0574 ::: bot acc: 0.0169
top acc: 0.0506 ::: bot acc: 0.0162
current epoch: 32
train loss is 0.000588
average val loss: 0.000953, accuracy: 0.0400
average test loss: 0.001030, accuracy: 0.0423
case acc: 0.042572487
case acc: 0.05248366
case acc: 0.046517
case acc: 0.03917966
case acc: 0.03912832
case acc: 0.034007996
top acc: 0.0571 ::: bot acc: 0.0281
top acc: 0.0649 ::: bot acc: 0.0393
top acc: 0.0667 ::: bot acc: 0.0266
top acc: 0.0532 ::: bot acc: 0.0257
top acc: 0.0589 ::: bot acc: 0.0184
top acc: 0.0512 ::: bot acc: 0.0169
current epoch: 33
train loss is 0.000608
average val loss: 0.001035, accuracy: 0.0420
average test loss: 0.001116, accuracy: 0.0443
case acc: 0.044621494
case acc: 0.053796977
case acc: 0.04915623
case acc: 0.041317023
case acc: 0.041601006
case acc: 0.03544713
top acc: 0.0591 ::: bot acc: 0.0301
top acc: 0.0660 ::: bot acc: 0.0409
top acc: 0.0694 ::: bot acc: 0.0290
top acc: 0.0550 ::: bot acc: 0.0278
top acc: 0.0614 ::: bot acc: 0.0208
top acc: 0.0527 ::: bot acc: 0.0183
current epoch: 34
train loss is 0.000615
average val loss: 0.001016, accuracy: 0.0415
average test loss: 0.001097, accuracy: 0.0439
case acc: 0.0441104
case acc: 0.052443452
case acc: 0.049430393
case acc: 0.04109643
case acc: 0.04160939
case acc: 0.034763593
top acc: 0.0587 ::: bot acc: 0.0297
top acc: 0.0648 ::: bot acc: 0.0392
top acc: 0.0695 ::: bot acc: 0.0294
top acc: 0.0550 ::: bot acc: 0.0276
top acc: 0.0613 ::: bot acc: 0.0210
top acc: 0.0519 ::: bot acc: 0.0177
current epoch: 35
train loss is 0.000577
average val loss: 0.001004, accuracy: 0.0413
average test loss: 0.001083, accuracy: 0.0436
case acc: 0.04379033
case acc: 0.051356755
case acc: 0.049695373
case acc: 0.04074411
case acc: 0.041967057
case acc: 0.034029383
top acc: 0.0583 ::: bot acc: 0.0293
top acc: 0.0637 ::: bot acc: 0.0382
top acc: 0.0698 ::: bot acc: 0.0294
top acc: 0.0545 ::: bot acc: 0.0274
top acc: 0.0617 ::: bot acc: 0.0213
top acc: 0.0513 ::: bot acc: 0.0171
current epoch: 36
train loss is 0.000572
average val loss: 0.000968, accuracy: 0.0404
average test loss: 0.001050, accuracy: 0.0428
case acc: 0.042696312
case acc: 0.04966063
case acc: 0.04925849
case acc: 0.040469866
case acc: 0.041824847
case acc: 0.033107884
top acc: 0.0572 ::: bot acc: 0.0283
top acc: 0.0617 ::: bot acc: 0.0367
top acc: 0.0696 ::: bot acc: 0.0291
top acc: 0.0542 ::: bot acc: 0.0271
top acc: 0.0615 ::: bot acc: 0.0212
top acc: 0.0503 ::: bot acc: 0.0160
current epoch: 37
train loss is 0.000562
average val loss: 0.000944, accuracy: 0.0399
average test loss: 0.001024, accuracy: 0.0423
case acc: 0.041917615
case acc: 0.04791833
case acc: 0.048632137
case acc: 0.040300723
case acc: 0.041937053
case acc: 0.032872625
top acc: 0.0563 ::: bot acc: 0.0275
top acc: 0.0603 ::: bot acc: 0.0348
top acc: 0.0691 ::: bot acc: 0.0284
top acc: 0.0541 ::: bot acc: 0.0269
top acc: 0.0617 ::: bot acc: 0.0213
top acc: 0.0500 ::: bot acc: 0.0160
current epoch: 38
train loss is 0.000523
average val loss: 0.000846, accuracy: 0.0374
average test loss: 0.000919, accuracy: 0.0398
case acc: 0.03907364
case acc: 0.04423106
case acc: 0.046242334
case acc: 0.037943106
case acc: 0.040249206
case acc: 0.030821223
top acc: 0.0535 ::: bot acc: 0.0246
top acc: 0.0565 ::: bot acc: 0.0312
top acc: 0.0664 ::: bot acc: 0.0263
top acc: 0.0518 ::: bot acc: 0.0243
top acc: 0.0602 ::: bot acc: 0.0194
top acc: 0.0481 ::: bot acc: 0.0141
current epoch: 39
train loss is 0.000482
average val loss: 0.000790, accuracy: 0.0360
average test loss: 0.000862, accuracy: 0.0383
case acc: 0.037123326
case acc: 0.0415689
case acc: 0.04482248
case acc: 0.03708448
case acc: 0.039777197
case acc: 0.029653467
top acc: 0.0515 ::: bot acc: 0.0228
top acc: 0.0541 ::: bot acc: 0.0285
top acc: 0.0650 ::: bot acc: 0.0250
top acc: 0.0510 ::: bot acc: 0.0237
top acc: 0.0594 ::: bot acc: 0.0191
top acc: 0.0471 ::: bot acc: 0.0127
current epoch: 40
train loss is 0.000443
average val loss: 0.000647, accuracy: 0.0319
average test loss: 0.000704, accuracy: 0.0341
case acc: 0.032397117
case acc: 0.03619577
case acc: 0.040561423
case acc: 0.03323939
case acc: 0.036218997
case acc: 0.026264869
top acc: 0.0469 ::: bot acc: 0.0179
top acc: 0.0485 ::: bot acc: 0.0231
top acc: 0.0603 ::: bot acc: 0.0215
top acc: 0.0467 ::: bot acc: 0.0202
top acc: 0.0559 ::: bot acc: 0.0157
top acc: 0.0431 ::: bot acc: 0.0100
current epoch: 41
train loss is 0.000401
average val loss: 0.000561, accuracy: 0.0292
average test loss: 0.000612, accuracy: 0.0314
case acc: 0.029168958
case acc: 0.03213015
case acc: 0.037662618
case acc: 0.031023486
case acc: 0.0341323
case acc: 0.024328588
top acc: 0.0435 ::: bot acc: 0.0149
top acc: 0.0444 ::: bot acc: 0.0192
top acc: 0.0573 ::: bot acc: 0.0188
top acc: 0.0446 ::: bot acc: 0.0180
top acc: 0.0539 ::: bot acc: 0.0134
top acc: 0.0412 ::: bot acc: 0.0086
current epoch: 42
train loss is 0.000364
average val loss: 0.000426, accuracy: 0.0245
average test loss: 0.000464, accuracy: 0.0266
case acc: 0.023629423
case acc: 0.02589191
case acc: 0.032134637
case acc: 0.026750786
case acc: 0.02988918
case acc: 0.021026466
top acc: 0.0377 ::: bot acc: 0.0097
top acc: 0.0380 ::: bot acc: 0.0131
top acc: 0.0512 ::: bot acc: 0.0143
top acc: 0.0402 ::: bot acc: 0.0143
top acc: 0.0495 ::: bot acc: 0.0096
top acc: 0.0372 ::: bot acc: 0.0066
current epoch: 43
train loss is 0.000322
average val loss: 0.000330, accuracy: 0.0207
average test loss: 0.000356, accuracy: 0.0226
case acc: 0.019369056
case acc: 0.020701226
case acc: 0.027620176
case acc: 0.022972334
case acc: 0.026512993
case acc: 0.018478166
top acc: 0.0327 ::: bot acc: 0.0072
top acc: 0.0323 ::: bot acc: 0.0089
top acc: 0.0461 ::: bot acc: 0.0112
top acc: 0.0360 ::: bot acc: 0.0110
top acc: 0.0456 ::: bot acc: 0.0071
top acc: 0.0335 ::: bot acc: 0.0061
current epoch: 44
train loss is 0.000291
average val loss: 0.000251, accuracy: 0.0173
average test loss: 0.000265, accuracy: 0.0189
case acc: 0.0154578015
case acc: 0.015581036
case acc: 0.023552382
case acc: 0.019217433
case acc: 0.023369808
case acc: 0.016177366
top acc: 0.0274 ::: bot acc: 0.0062
top acc: 0.0263 ::: bot acc: 0.0058
top acc: 0.0404 ::: bot acc: 0.0102
top acc: 0.0318 ::: bot acc: 0.0083
top acc: 0.0413 ::: bot acc: 0.0061
top acc: 0.0299 ::: bot acc: 0.0065
current epoch: 45
train loss is 0.000273
average val loss: 0.000190, accuracy: 0.0145
average test loss: 0.000189, accuracy: 0.0154
case acc: 0.0120852785
case acc: 0.011134536
case acc: 0.019589655
case acc: 0.015324127
case acc: 0.020341732
case acc: 0.014178776
top acc: 0.0216 ::: bot acc: 0.0080
top acc: 0.0194 ::: bot acc: 0.0062
top acc: 0.0343 ::: bot acc: 0.0111
top acc: 0.0268 ::: bot acc: 0.0067
top acc: 0.0364 ::: bot acc: 0.0069
top acc: 0.0254 ::: bot acc: 0.0091
current epoch: 46
train loss is 0.000269
average val loss: 0.000166, accuracy: 0.0135
average test loss: 0.000155, accuracy: 0.0137
case acc: 0.010549133
case acc: 0.00947882
case acc: 0.016932849
case acc: 0.012987874
case acc: 0.018607501
case acc: 0.013460758
top acc: 0.0168 ::: bot acc: 0.0118
top acc: 0.0144 ::: bot acc: 0.0111
top acc: 0.0290 ::: bot acc: 0.0138
top acc: 0.0230 ::: bot acc: 0.0069
top acc: 0.0333 ::: bot acc: 0.0080
top acc: 0.0229 ::: bot acc: 0.0117
current epoch: 47
train loss is 0.000267
average val loss: 0.000159, accuracy: 0.0133
average test loss: 0.000138, accuracy: 0.0128
case acc: 0.010564885
case acc: 0.009475349
case acc: 0.015457985
case acc: 0.010834162
case acc: 0.017310241
case acc: 0.013055256
top acc: 0.0122 ::: bot acc: 0.0168
top acc: 0.0088 ::: bot acc: 0.0165
top acc: 0.0233 ::: bot acc: 0.0193
top acc: 0.0193 ::: bot acc: 0.0085
top acc: 0.0305 ::: bot acc: 0.0106
top acc: 0.0203 ::: bot acc: 0.0145
current epoch: 48
train loss is 0.000285
average val loss: 0.000183, accuracy: 0.0148
average test loss: 0.000146, accuracy: 0.0133
case acc: 0.012779454
case acc: 0.012775922
case acc: 0.015800849
case acc: 0.00999339
case acc: 0.015723575
case acc: 0.012820012
top acc: 0.0066 ::: bot acc: 0.0231
top acc: 0.0041 ::: bot acc: 0.0241
top acc: 0.0154 ::: bot acc: 0.0271
top acc: 0.0136 ::: bot acc: 0.0141
top acc: 0.0253 ::: bot acc: 0.0152
top acc: 0.0158 ::: bot acc: 0.0188
current epoch: 49
train loss is 0.000321
average val loss: 0.000225, accuracy: 0.0169
average test loss: 0.000176, accuracy: 0.0150
case acc: 0.015587137
case acc: 0.017217837
case acc: 0.018348167
case acc: 0.010695757
case acc: 0.015201193
case acc: 0.01302327
top acc: 0.0058 ::: bot acc: 0.0277
top acc: 0.0058 ::: bot acc: 0.0298
top acc: 0.0109 ::: bot acc: 0.0332
top acc: 0.0099 ::: bot acc: 0.0179
top acc: 0.0225 ::: bot acc: 0.0179
top acc: 0.0138 ::: bot acc: 0.0209
current epoch: 50
train loss is 0.000361
average val loss: 0.000353, accuracy: 0.0223
average test loss: 0.000283, accuracy: 0.0197
case acc: 0.022219962
case acc: 0.026772248
case acc: 0.024814647
case acc: 0.014619197
case acc: 0.014824213
case acc: 0.014755115
top acc: 0.0090 ::: bot acc: 0.0361
top acc: 0.0146 ::: bot acc: 0.0399
top acc: 0.0096 ::: bot acc: 0.0436
top acc: 0.0063 ::: bot acc: 0.0255
top acc: 0.0160 ::: bot acc: 0.0243
top acc: 0.0084 ::: bot acc: 0.0266
LME_Co_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6798 6798 6798
1.7082474 -0.6288155 0.10460696 -0.09017589
Validation: 756 756 756
Testing: 768 768 768
pre-processing time: 0.00020933151245117188
the split date is 2011-07-01
train dropout: 0.6 test dropout: 0.1
net initializing with time: 0.0023658275604248047
preparing training and testing date with time: 0.0
current epoch: 1
train loss is 0.012618
average val loss: 0.005183, accuracy: 0.0968
average test loss: 0.005242, accuracy: 0.0976
case acc: 0.13846144
case acc: 0.08485182
case acc: 0.094667286
case acc: 0.08759174
case acc: 0.12467485
case acc: 0.05541979
top acc: 0.1261 ::: bot acc: 0.1521
top acc: 0.0965 ::: bot acc: 0.0732
top acc: 0.0753 ::: bot acc: 0.1138
top acc: 0.0782 ::: bot acc: 0.0982
top acc: 0.1079 ::: bot acc: 0.1425
top acc: 0.0396 ::: bot acc: 0.0723
current epoch: 2
train loss is 0.007512
average val loss: 0.002839, accuracy: 0.0503
average test loss: 0.002814, accuracy: 0.0499
case acc: 0.036408637
case acc: 0.1701645
case acc: 0.015780048
case acc: 0.010668563
case acc: 0.035599522
case acc: 0.030735586
top acc: 0.0240 ::: bot acc: 0.0500
top acc: 0.1816 ::: bot acc: 0.1587
top acc: 0.0270 ::: bot acc: 0.0129
top acc: 0.0168 ::: bot acc: 0.0079
top acc: 0.0194 ::: bot acc: 0.0531
top acc: 0.0463 ::: bot acc: 0.0152
current epoch: 3
train loss is 0.007851
average val loss: 0.007588, accuracy: 0.1034
average test loss: 0.007552, accuracy: 0.1031
case acc: 0.05165002
case acc: 0.24291024
case acc: 0.09430202
case acc: 0.08552453
case acc: 0.043395042
case acc: 0.10083981
top acc: 0.0643 ::: bot acc: 0.0381
top acc: 0.2542 ::: bot acc: 0.2324
top acc: 0.1141 ::: bot acc: 0.0749
top acc: 0.0954 ::: bot acc: 0.0749
top acc: 0.0589 ::: bot acc: 0.0264
top acc: 0.1179 ::: bot acc: 0.0829
current epoch: 4
train loss is 0.009773
average val loss: 0.016290, accuracy: 0.1695
average test loss: 0.016175, accuracy: 0.1688
case acc: 0.124962
case acc: 0.30205858
case acc: 0.16625631
case acc: 0.15019307
case acc: 0.11114289
case acc: 0.15838596
top acc: 0.1385 ::: bot acc: 0.1119
top acc: 0.3134 ::: bot acc: 0.2916
top acc: 0.1862 ::: bot acc: 0.1467
top acc: 0.1603 ::: bot acc: 0.1386
top acc: 0.1279 ::: bot acc: 0.0933
top acc: 0.1750 ::: bot acc: 0.1403
current epoch: 5
train loss is 0.012554
average val loss: 0.014636, accuracy: 0.1611
average test loss: 0.014526, accuracy: 0.1604
case acc: 0.12196781
case acc: 0.28401685
case acc: 0.16094258
case acc: 0.14080821
case acc: 0.10998577
case acc: 0.14494489
top acc: 0.1342 ::: bot acc: 0.1092
top acc: 0.2949 ::: bot acc: 0.2734
top acc: 0.1806 ::: bot acc: 0.1411
top acc: 0.1509 ::: bot acc: 0.1295
top acc: 0.1267 ::: bot acc: 0.0919
top acc: 0.1616 ::: bot acc: 0.1268
current epoch: 6
train loss is 0.012814
average val loss: 0.004374, accuracy: 0.0771
average test loss: 0.004314, accuracy: 0.0766
case acc: 0.04224547
case acc: 0.18695612
case acc: 0.0755661
case acc: 0.056388363
case acc: 0.03777024
case acc: 0.06064327
top acc: 0.0548 ::: bot acc: 0.0292
top acc: 0.1980 ::: bot acc: 0.1763
top acc: 0.0953 ::: bot acc: 0.0561
top acc: 0.0660 ::: bot acc: 0.0457
top acc: 0.0535 ::: bot acc: 0.0215
top acc: 0.0775 ::: bot acc: 0.0430
current epoch: 7
train loss is 0.007735
average val loss: 0.001153, accuracy: 0.0320
average test loss: 0.001127, accuracy: 0.0313
case acc: 0.022411158
case acc: 0.106547095
case acc: 0.014744356
case acc: 0.011664087
case acc: 0.01952887
case acc: 0.012871473
top acc: 0.0113 ::: bot acc: 0.0342
top acc: 0.1176 ::: bot acc: 0.0961
top acc: 0.0239 ::: bot acc: 0.0156
top acc: 0.0052 ::: bot acc: 0.0211
top acc: 0.0078 ::: bot acc: 0.0352
top acc: 0.0138 ::: bot acc: 0.0213
current epoch: 8
train loss is 0.003351
average val loss: 0.000913, accuracy: 0.0291
average test loss: 0.000886, accuracy: 0.0282
case acc: 0.022330895
case acc: 0.09301789
case acc: 0.014158592
case acc: 0.0124984905
case acc: 0.014828395
case acc: 0.012445942
top acc: 0.0118 ::: bot acc: 0.0340
top acc: 0.1036 ::: bot acc: 0.0828
top acc: 0.0182 ::: bot acc: 0.0216
top acc: 0.0055 ::: bot acc: 0.0223
top acc: 0.0080 ::: bot acc: 0.0281
top acc: 0.0147 ::: bot acc: 0.0200
current epoch: 9
train loss is 0.002193
average val loss: 0.001041, accuracy: 0.0303
average test loss: 0.001010, accuracy: 0.0291
case acc: 0.009439312
case acc: 0.10104525
case acc: 0.017693508
case acc: 0.009968788
case acc: 0.017073061
case acc: 0.019422436
top acc: 0.0111 ::: bot acc: 0.0144
top acc: 0.1117 ::: bot acc: 0.0908
top acc: 0.0317 ::: bot acc: 0.0097
top acc: 0.0164 ::: bot acc: 0.0062
top acc: 0.0292 ::: bot acc: 0.0087
top acc: 0.0336 ::: bot acc: 0.0068
current epoch: 10
train loss is 0.002048
average val loss: 0.000994, accuracy: 0.0309
average test loss: 0.000965, accuracy: 0.0300
case acc: 0.009657839
case acc: 0.096181035
case acc: 0.01842128
case acc: 0.012004394
case acc: 0.021419588
case acc: 0.022225019
top acc: 0.0151 ::: bot acc: 0.0106
top acc: 0.1068 ::: bot acc: 0.0854
top acc: 0.0329 ::: bot acc: 0.0095
top acc: 0.0198 ::: bot acc: 0.0058
top acc: 0.0353 ::: bot acc: 0.0090
top acc: 0.0375 ::: bot acc: 0.0078
current epoch: 11
train loss is 0.001834
average val loss: 0.000833, accuracy: 0.0285
average test loss: 0.000805, accuracy: 0.0277
case acc: 0.009428961
case acc: 0.086597905
case acc: 0.016709479
case acc: 0.0109389145
case acc: 0.021207351
case acc: 0.021096576
top acc: 0.0147 ::: bot acc: 0.0107
top acc: 0.0973 ::: bot acc: 0.0757
top acc: 0.0294 ::: bot acc: 0.0112
top acc: 0.0181 ::: bot acc: 0.0057
top acc: 0.0350 ::: bot acc: 0.0085
top acc: 0.0360 ::: bot acc: 0.0074
current epoch: 12
train loss is 0.001559
average val loss: 0.000753, accuracy: 0.0278
average test loss: 0.000722, accuracy: 0.0269
case acc: 0.009677229
case acc: 0.0798976
case acc: 0.01589547
case acc: 0.011220326
case acc: 0.023241129
case acc: 0.021558804
top acc: 0.0162 ::: bot acc: 0.0092
top acc: 0.0908 ::: bot acc: 0.0696
top acc: 0.0279 ::: bot acc: 0.0117
top acc: 0.0184 ::: bot acc: 0.0058
top acc: 0.0379 ::: bot acc: 0.0094
top acc: 0.0366 ::: bot acc: 0.0074
current epoch: 13
train loss is 0.001424
average val loss: 0.000703, accuracy: 0.0277
average test loss: 0.000667, accuracy: 0.0268
case acc: 0.010505526
case acc: 0.07438862
case acc: 0.015915664
case acc: 0.012065807
case acc: 0.025021631
case acc: 0.02273328
top acc: 0.0187 ::: bot acc: 0.0072
top acc: 0.0852 ::: bot acc: 0.0640
top acc: 0.0280 ::: bot acc: 0.0116
top acc: 0.0201 ::: bot acc: 0.0055
top acc: 0.0395 ::: bot acc: 0.0107
top acc: 0.0379 ::: bot acc: 0.0083
current epoch: 14
train loss is 0.001181
average val loss: 0.000661, accuracy: 0.0276
average test loss: 0.000626, accuracy: 0.0269
case acc: 0.011625532
case acc: 0.06927584
case acc: 0.016079534
case acc: 0.013303019
case acc: 0.026657451
case acc: 0.024192503
top acc: 0.0210 ::: bot acc: 0.0057
top acc: 0.0800 ::: bot acc: 0.0587
top acc: 0.0285 ::: bot acc: 0.0114
top acc: 0.0214 ::: bot acc: 0.0063
top acc: 0.0419 ::: bot acc: 0.0117
top acc: 0.0394 ::: bot acc: 0.0094
current epoch: 15
train loss is 0.001076
average val loss: 0.000604, accuracy: 0.0268
average test loss: 0.000574, accuracy: 0.0261
case acc: 0.0122870235
case acc: 0.064425305
case acc: 0.016073827
case acc: 0.013386448
case acc: 0.026826717
case acc: 0.023774838
top acc: 0.0225 ::: bot acc: 0.0050
top acc: 0.0751 ::: bot acc: 0.0542
top acc: 0.0285 ::: bot acc: 0.0118
top acc: 0.0218 ::: bot acc: 0.0059
top acc: 0.0420 ::: bot acc: 0.0119
top acc: 0.0393 ::: bot acc: 0.0088
current epoch: 16
train loss is 0.000949
average val loss: 0.000566, accuracy: 0.0265
average test loss: 0.000537, accuracy: 0.0258
case acc: 0.0133193135
case acc: 0.06034694
case acc: 0.016034087
case acc: 0.013991918
case acc: 0.026869837
case acc: 0.024104878
top acc: 0.0238 ::: bot acc: 0.0054
top acc: 0.0710 ::: bot acc: 0.0497
top acc: 0.0286 ::: bot acc: 0.0111
top acc: 0.0224 ::: bot acc: 0.0062
top acc: 0.0422 ::: bot acc: 0.0119
top acc: 0.0399 ::: bot acc: 0.0094
current epoch: 17
train loss is 0.000883
average val loss: 0.000542, accuracy: 0.0263
average test loss: 0.000511, accuracy: 0.0257
case acc: 0.01438617
case acc: 0.05687911
case acc: 0.016543161
case acc: 0.015090052
case acc: 0.02705891
case acc: 0.024203245
top acc: 0.0253 ::: bot acc: 0.0057
top acc: 0.0675 ::: bot acc: 0.0464
top acc: 0.0294 ::: bot acc: 0.0111
top acc: 0.0239 ::: bot acc: 0.0068
top acc: 0.0421 ::: bot acc: 0.0124
top acc: 0.0396 ::: bot acc: 0.0093
current epoch: 18
train loss is 0.000834
average val loss: 0.000486, accuracy: 0.0251
average test loss: 0.000450, accuracy: 0.0242
case acc: 0.014320362
case acc: 0.051862136
case acc: 0.016353074
case acc: 0.014452265
case acc: 0.025224153
case acc: 0.02312228
top acc: 0.0252 ::: bot acc: 0.0052
top acc: 0.0628 ::: bot acc: 0.0413
top acc: 0.0290 ::: bot acc: 0.0112
top acc: 0.0232 ::: bot acc: 0.0064
top acc: 0.0400 ::: bot acc: 0.0109
top acc: 0.0385 ::: bot acc: 0.0084
current epoch: 19
train loss is 0.000741
average val loss: 0.000503, accuracy: 0.0261
average test loss: 0.000476, accuracy: 0.0256
case acc: 0.016715836
case acc: 0.05106452
case acc: 0.01754125
case acc: 0.016924886
case acc: 0.026217755
case acc: 0.02494406
top acc: 0.0284 ::: bot acc: 0.0060
top acc: 0.0620 ::: bot acc: 0.0403
top acc: 0.0316 ::: bot acc: 0.0096
top acc: 0.0262 ::: bot acc: 0.0078
top acc: 0.0413 ::: bot acc: 0.0114
top acc: 0.0404 ::: bot acc: 0.0098
current epoch: 20
train loss is 0.000716
average val loss: 0.000540, accuracy: 0.0277
average test loss: 0.000509, accuracy: 0.0272
case acc: 0.019615814
case acc: 0.05062692
case acc: 0.019737408
case acc: 0.019195309
case acc: 0.0275076
case acc: 0.026314918
top acc: 0.0320 ::: bot acc: 0.0081
top acc: 0.0615 ::: bot acc: 0.0400
top acc: 0.0349 ::: bot acc: 0.0093
top acc: 0.0289 ::: bot acc: 0.0088
top acc: 0.0426 ::: bot acc: 0.0126
top acc: 0.0418 ::: bot acc: 0.0107
current epoch: 21
train loss is 0.000673
average val loss: 0.000500, accuracy: 0.0267
average test loss: 0.000468, accuracy: 0.0261
case acc: 0.019924562
case acc: 0.047265828
case acc: 0.019712312
case acc: 0.018931545
case acc: 0.025880456
case acc: 0.024999553
top acc: 0.0321 ::: bot acc: 0.0085
top acc: 0.0580 ::: bot acc: 0.0370
top acc: 0.0349 ::: bot acc: 0.0093
top acc: 0.0287 ::: bot acc: 0.0090
top acc: 0.0411 ::: bot acc: 0.0110
top acc: 0.0406 ::: bot acc: 0.0099
current epoch: 22
train loss is 0.000650
average val loss: 0.000497, accuracy: 0.0268
average test loss: 0.000465, accuracy: 0.0262
case acc: 0.02116429
case acc: 0.045828022
case acc: 0.020677298
case acc: 0.019583408
case acc: 0.025576979
case acc: 0.024644144
top acc: 0.0334 ::: bot acc: 0.0093
top acc: 0.0567 ::: bot acc: 0.0352
top acc: 0.0366 ::: bot acc: 0.0089
top acc: 0.0294 ::: bot acc: 0.0095
top acc: 0.0404 ::: bot acc: 0.0109
top acc: 0.0403 ::: bot acc: 0.0094
current epoch: 23
train loss is 0.000614
average val loss: 0.000576, accuracy: 0.0295
average test loss: 0.000540, accuracy: 0.0289
case acc: 0.025076937
case acc: 0.047526002
case acc: 0.023901079
case acc: 0.02258451
case acc: 0.027980383
case acc: 0.026612258
top acc: 0.0377 ::: bot acc: 0.0127
top acc: 0.0583 ::: bot acc: 0.0371
top acc: 0.0412 ::: bot acc: 0.0094
top acc: 0.0327 ::: bot acc: 0.0118
top acc: 0.0433 ::: bot acc: 0.0130
top acc: 0.0422 ::: bot acc: 0.0112
current epoch: 24
train loss is 0.000612
average val loss: 0.000601, accuracy: 0.0305
average test loss: 0.000567, accuracy: 0.0299
case acc: 0.02692605
case acc: 0.047344312
case acc: 0.025745843
case acc: 0.024141617
case acc: 0.028357174
case acc: 0.02717311
top acc: 0.0397 ::: bot acc: 0.0143
top acc: 0.0580 ::: bot acc: 0.0370
top acc: 0.0435 ::: bot acc: 0.0100
top acc: 0.0341 ::: bot acc: 0.0135
top acc: 0.0437 ::: bot acc: 0.0129
top acc: 0.0430 ::: bot acc: 0.0115
current epoch: 25
train loss is 0.000606
average val loss: 0.000647, accuracy: 0.0320
average test loss: 0.000612, accuracy: 0.0314
case acc: 0.029304672
case acc: 0.04770787
case acc: 0.028029818
case acc: 0.026060902
case acc: 0.02953037
case acc: 0.027931418
top acc: 0.0418 ::: bot acc: 0.0168
top acc: 0.0585 ::: bot acc: 0.0374
top acc: 0.0467 ::: bot acc: 0.0108
top acc: 0.0363 ::: bot acc: 0.0148
top acc: 0.0452 ::: bot acc: 0.0137
top acc: 0.0440 ::: bot acc: 0.0122
current epoch: 26
train loss is 0.000600
average val loss: 0.000660, accuracy: 0.0324
average test loss: 0.000622, accuracy: 0.0319
case acc: 0.03037365
case acc: 0.047101155
case acc: 0.02940604
case acc: 0.026943244
case acc: 0.029456716
case acc: 0.027924273
top acc: 0.0432 ::: bot acc: 0.0177
top acc: 0.0577 ::: bot acc: 0.0368
top acc: 0.0482 ::: bot acc: 0.0119
top acc: 0.0371 ::: bot acc: 0.0159
top acc: 0.0446 ::: bot acc: 0.0139
top acc: 0.0436 ::: bot acc: 0.0124
current epoch: 27
train loss is 0.000593
average val loss: 0.000725, accuracy: 0.0343
average test loss: 0.000688, accuracy: 0.0339
case acc: 0.03299745
case acc: 0.04795171
case acc: 0.032265052
case acc: 0.029305508
case acc: 0.03151775
case acc: 0.029180903
top acc: 0.0455 ::: bot acc: 0.0205
top acc: 0.0588 ::: bot acc: 0.0376
top acc: 0.0514 ::: bot acc: 0.0138
top acc: 0.0397 ::: bot acc: 0.0181
top acc: 0.0471 ::: bot acc: 0.0156
top acc: 0.0449 ::: bot acc: 0.0131
current epoch: 28
train loss is 0.000611
average val loss: 0.000818, accuracy: 0.0370
average test loss: 0.000782, accuracy: 0.0365
case acc: 0.03627641
case acc: 0.049588434
case acc: 0.035721518
case acc: 0.032201745
case acc: 0.034029372
case acc: 0.031331472
top acc: 0.0492 ::: bot acc: 0.0236
top acc: 0.0606 ::: bot acc: 0.0392
top acc: 0.0549 ::: bot acc: 0.0168
top acc: 0.0425 ::: bot acc: 0.0209
top acc: 0.0498 ::: bot acc: 0.0176
top acc: 0.0474 ::: bot acc: 0.0150
current epoch: 29
train loss is 0.000588
average val loss: 0.000779, accuracy: 0.0359
average test loss: 0.000743, accuracy: 0.0355
case acc: 0.035505533
case acc: 0.047635064
case acc: 0.03561742
case acc: 0.03146679
case acc: 0.03316226
case acc: 0.029697461
top acc: 0.0484 ::: bot acc: 0.0226
top acc: 0.0584 ::: bot acc: 0.0371
top acc: 0.0550 ::: bot acc: 0.0167
top acc: 0.0416 ::: bot acc: 0.0203
top acc: 0.0490 ::: bot acc: 0.0169
top acc: 0.0457 ::: bot acc: 0.0137
current epoch: 30
train loss is 0.000581
average val loss: 0.000829, accuracy: 0.0373
average test loss: 0.000792, accuracy: 0.0368
case acc: 0.037287433
case acc: 0.04820529
case acc: 0.037703604
case acc: 0.03297048
case acc: 0.03458289
case acc: 0.030343957
top acc: 0.0502 ::: bot acc: 0.0246
top acc: 0.0590 ::: bot acc: 0.0377
top acc: 0.0572 ::: bot acc: 0.0185
top acc: 0.0432 ::: bot acc: 0.0220
top acc: 0.0504 ::: bot acc: 0.0181
top acc: 0.0465 ::: bot acc: 0.0144
current epoch: 31
train loss is 0.000587
average val loss: 0.000833, accuracy: 0.0374
average test loss: 0.000799, accuracy: 0.0371
case acc: 0.03746805
case acc: 0.04759044
case acc: 0.038716353
case acc: 0.03367862
case acc: 0.034622647
case acc: 0.030443363
top acc: 0.0502 ::: bot acc: 0.0249
top acc: 0.0584 ::: bot acc: 0.0372
top acc: 0.0582 ::: bot acc: 0.0192
top acc: 0.0439 ::: bot acc: 0.0224
top acc: 0.0508 ::: bot acc: 0.0181
top acc: 0.0466 ::: bot acc: 0.0143
current epoch: 32
train loss is 0.000559
average val loss: 0.000890, accuracy: 0.0389
average test loss: 0.000852, accuracy: 0.0384
case acc: 0.03886491
case acc: 0.048189275
case acc: 0.040766187
case acc: 0.035555568
case acc: 0.03603924
case acc: 0.031260286
top acc: 0.0517 ::: bot acc: 0.0262
top acc: 0.0591 ::: bot acc: 0.0377
top acc: 0.0607 ::: bot acc: 0.0214
top acc: 0.0459 ::: bot acc: 0.0242
top acc: 0.0522 ::: bot acc: 0.0193
top acc: 0.0476 ::: bot acc: 0.0150
current epoch: 33
train loss is 0.000547
average val loss: 0.000932, accuracy: 0.0400
average test loss: 0.000893, accuracy: 0.0396
case acc: 0.03983753
case acc: 0.04854881
case acc: 0.04229635
case acc: 0.036983404
case acc: 0.037414145
case acc: 0.03225366
top acc: 0.0525 ::: bot acc: 0.0274
top acc: 0.0593 ::: bot acc: 0.0381
top acc: 0.0619 ::: bot acc: 0.0228
top acc: 0.0474 ::: bot acc: 0.0257
top acc: 0.0534 ::: bot acc: 0.0209
top acc: 0.0484 ::: bot acc: 0.0161
current epoch: 34
train loss is 0.000561
average val loss: 0.000890, accuracy: 0.0390
average test loss: 0.000852, accuracy: 0.0386
case acc: 0.038809232
case acc: 0.046494946
case acc: 0.041588042
case acc: 0.03628757
case acc: 0.036811978
case acc: 0.03134895
top acc: 0.0516 ::: bot acc: 0.0262
top acc: 0.0573 ::: bot acc: 0.0361
top acc: 0.0613 ::: bot acc: 0.0221
top acc: 0.0465 ::: bot acc: 0.0251
top acc: 0.0530 ::: bot acc: 0.0202
top acc: 0.0474 ::: bot acc: 0.0152
current epoch: 35
train loss is 0.000565
average val loss: 0.000914, accuracy: 0.0396
average test loss: 0.000874, accuracy: 0.0392
case acc: 0.039230015
case acc: 0.046012904
case acc: 0.042534396
case acc: 0.037109792
case acc: 0.037927248
case acc: 0.03208941
top acc: 0.0520 ::: bot acc: 0.0264
top acc: 0.0569 ::: bot acc: 0.0358
top acc: 0.0622 ::: bot acc: 0.0231
top acc: 0.0475 ::: bot acc: 0.0258
top acc: 0.0539 ::: bot acc: 0.0211
top acc: 0.0483 ::: bot acc: 0.0159
current epoch: 36
train loss is 0.000508
average val loss: 0.000815, accuracy: 0.0370
average test loss: 0.000774, accuracy: 0.0366
case acc: 0.036370665
case acc: 0.04240822
case acc: 0.040202435
case acc: 0.0349963
case acc: 0.03596738
case acc: 0.029511094
top acc: 0.0490 ::: bot acc: 0.0239
top acc: 0.0532 ::: bot acc: 0.0320
top acc: 0.0598 ::: bot acc: 0.0208
top acc: 0.0451 ::: bot acc: 0.0236
top acc: 0.0518 ::: bot acc: 0.0194
top acc: 0.0455 ::: bot acc: 0.0136
current epoch: 37
train loss is 0.000478
average val loss: 0.000742, accuracy: 0.0351
average test loss: 0.000702, accuracy: 0.0346
case acc: 0.034031924
case acc: 0.03954016
case acc: 0.038498394
case acc: 0.033155054
case acc: 0.034537476
case acc: 0.02780207
top acc: 0.0469 ::: bot acc: 0.0213
top acc: 0.0503 ::: bot acc: 0.0293
top acc: 0.0580 ::: bot acc: 0.0193
top acc: 0.0433 ::: bot acc: 0.0218
top acc: 0.0505 ::: bot acc: 0.0180
top acc: 0.0438 ::: bot acc: 0.0119
current epoch: 38
train loss is 0.000442
average val loss: 0.000676, accuracy: 0.0333
average test loss: 0.000638, accuracy: 0.0328
case acc: 0.032026805
case acc: 0.03670471
case acc: 0.03662817
case acc: 0.031718098
case acc: 0.033209495
case acc: 0.026246661
top acc: 0.0448 ::: bot acc: 0.0194
top acc: 0.0475 ::: bot acc: 0.0263
top acc: 0.0561 ::: bot acc: 0.0176
top acc: 0.0422 ::: bot acc: 0.0204
top acc: 0.0491 ::: bot acc: 0.0169
top acc: 0.0420 ::: bot acc: 0.0107
current epoch: 39
train loss is 0.000424
average val loss: 0.000600, accuracy: 0.0310
average test loss: 0.000565, accuracy: 0.0305
case acc: 0.029415224
case acc: 0.033476017
case acc: 0.034658987
case acc: 0.029708557
case acc: 0.03144855
case acc: 0.024557965
top acc: 0.0421 ::: bot acc: 0.0170
top acc: 0.0443 ::: bot acc: 0.0231
top acc: 0.0539 ::: bot acc: 0.0159
top acc: 0.0398 ::: bot acc: 0.0185
top acc: 0.0473 ::: bot acc: 0.0153
top acc: 0.0402 ::: bot acc: 0.0094
current epoch: 40
train loss is 0.000384
average val loss: 0.000532, accuracy: 0.0289
average test loss: 0.000502, accuracy: 0.0285
case acc: 0.026999228
case acc: 0.030611988
case acc: 0.032657098
case acc: 0.027993377
case acc: 0.029728137
case acc: 0.022929076
top acc: 0.0398 ::: bot acc: 0.0144
top acc: 0.0413 ::: bot acc: 0.0200
top acc: 0.0516 ::: bot acc: 0.0146
top acc: 0.0382 ::: bot acc: 0.0166
top acc: 0.0453 ::: bot acc: 0.0140
top acc: 0.0381 ::: bot acc: 0.0083
current epoch: 41
train loss is 0.000374
average val loss: 0.000540, accuracy: 0.0292
average test loss: 0.000508, accuracy: 0.0287
case acc: 0.026949916
case acc: 0.02991742
case acc: 0.03274299
case acc: 0.028602991
case acc: 0.030412512
case acc: 0.023643374
top acc: 0.0395 ::: bot acc: 0.0146
top acc: 0.0406 ::: bot acc: 0.0195
top acc: 0.0519 ::: bot acc: 0.0142
top acc: 0.0390 ::: bot acc: 0.0174
top acc: 0.0459 ::: bot acc: 0.0147
top acc: 0.0390 ::: bot acc: 0.0089
current epoch: 42
train loss is 0.000357
average val loss: 0.000476, accuracy: 0.0270
average test loss: 0.000445, accuracy: 0.0265
case acc: 0.024303056
case acc: 0.02674637
case acc: 0.030456562
case acc: 0.026601233
case acc: 0.028843468
case acc: 0.022096675
top acc: 0.0370 ::: bot acc: 0.0119
top acc: 0.0376 ::: bot acc: 0.0163
top acc: 0.0494 ::: bot acc: 0.0127
top acc: 0.0369 ::: bot acc: 0.0153
top acc: 0.0443 ::: bot acc: 0.0133
top acc: 0.0372 ::: bot acc: 0.0081
current epoch: 43
train loss is 0.000320
average val loss: 0.000418, accuracy: 0.0249
average test loss: 0.000388, accuracy: 0.0244
case acc: 0.021949377
case acc: 0.02370501
case acc: 0.028231397
case acc: 0.02485301
case acc: 0.027065068
case acc: 0.020621095
top acc: 0.0344 ::: bot acc: 0.0100
top acc: 0.0345 ::: bot acc: 0.0132
top acc: 0.0469 ::: bot acc: 0.0110
top acc: 0.0350 ::: bot acc: 0.0139
top acc: 0.0423 ::: bot acc: 0.0120
top acc: 0.0356 ::: bot acc: 0.0073
current epoch: 44
train loss is 0.000306
average val loss: 0.000330, accuracy: 0.0216
average test loss: 0.000301, accuracy: 0.0209
case acc: 0.017768582
case acc: 0.018804064
case acc: 0.025010977
case acc: 0.02138739
case acc: 0.024125852
case acc: 0.018231887
top acc: 0.0299 ::: bot acc: 0.0066
top acc: 0.0296 ::: bot acc: 0.0083
top acc: 0.0426 ::: bot acc: 0.0099
top acc: 0.0314 ::: bot acc: 0.0109
top acc: 0.0389 ::: bot acc: 0.0100
top acc: 0.0319 ::: bot acc: 0.0068
current epoch: 45
train loss is 0.000288
average val loss: 0.000294, accuracy: 0.0201
average test loss: 0.000266, accuracy: 0.0193
case acc: 0.01600181
case acc: 0.016186628
case acc: 0.023150258
case acc: 0.019872846
case acc: 0.023083491
case acc: 0.017385487
top acc: 0.0276 ::: bot acc: 0.0057
top acc: 0.0267 ::: bot acc: 0.0061
top acc: 0.0401 ::: bot acc: 0.0093
top acc: 0.0298 ::: bot acc: 0.0096
top acc: 0.0377 ::: bot acc: 0.0091
top acc: 0.0309 ::: bot acc: 0.0069
current epoch: 46
train loss is 0.000266
average val loss: 0.000238, accuracy: 0.0178
average test loss: 0.000212, accuracy: 0.0167
case acc: 0.013274791
case acc: 0.0124370195
case acc: 0.020442724
case acc: 0.017198656
case acc: 0.020859301
case acc: 0.0159702
top acc: 0.0241 ::: bot acc: 0.0047
top acc: 0.0226 ::: bot acc: 0.0032
top acc: 0.0361 ::: bot acc: 0.0091
top acc: 0.0268 ::: bot acc: 0.0074
top acc: 0.0350 ::: bot acc: 0.0082
top acc: 0.0281 ::: bot acc: 0.0078
current epoch: 47
train loss is 0.000260
average val loss: 0.000181, accuracy: 0.0151
average test loss: 0.000155, accuracy: 0.0137
case acc: 0.010371108
case acc: 0.009003825
case acc: 0.017243888
case acc: 0.013757221
case acc: 0.017730523
case acc: 0.014291521
top acc: 0.0185 ::: bot acc: 0.0069
top acc: 0.0171 ::: bot acc: 0.0042
top acc: 0.0306 ::: bot acc: 0.0104
top acc: 0.0223 ::: bot acc: 0.0059
top acc: 0.0304 ::: bot acc: 0.0079
top acc: 0.0243 ::: bot acc: 0.0103
current epoch: 48
train loss is 0.000249
average val loss: 0.000145, accuracy: 0.0133
average test loss: 0.000123, accuracy: 0.0118
case acc: 0.008970369
case acc: 0.0077097937
case acc: 0.014870566
case acc: 0.010826804
case acc: 0.015453973
case acc: 0.0131204575
top acc: 0.0136 ::: bot acc: 0.0116
top acc: 0.0114 ::: bot acc: 0.0098
top acc: 0.0251 ::: bot acc: 0.0146
top acc: 0.0181 ::: bot acc: 0.0059
top acc: 0.0262 ::: bot acc: 0.0094
top acc: 0.0204 ::: bot acc: 0.0139
current epoch: 49
train loss is 0.000262
average val loss: 0.000142, accuracy: 0.0130
average test loss: 0.000120, accuracy: 0.0116
case acc: 0.00905343
case acc: 0.007778156
case acc: 0.014360529
case acc: 0.010022906
case acc: 0.015390355
case acc: 0.013024578
top acc: 0.0120 ::: bot acc: 0.0132
top acc: 0.0092 ::: bot acc: 0.0117
top acc: 0.0227 ::: bot acc: 0.0170
top acc: 0.0169 ::: bot acc: 0.0057
top acc: 0.0262 ::: bot acc: 0.0094
top acc: 0.0206 ::: bot acc: 0.0138
current epoch: 50
train loss is 0.000267
average val loss: 0.000138, accuracy: 0.0128
average test loss: 0.000118, accuracy: 0.0116
case acc: 0.009652588
case acc: 0.008951098
case acc: 0.014083096
case acc: 0.009080124
case acc: 0.014761984
case acc: 0.012840825
top acc: 0.0094 ::: bot acc: 0.0160
top acc: 0.0061 ::: bot acc: 0.0155
top acc: 0.0188 ::: bot acc: 0.0209
top acc: 0.0149 ::: bot acc: 0.0070
top acc: 0.0249 ::: bot acc: 0.0102
top acc: 0.0196 ::: bot acc: 0.0147

		{"drop_out": 0.6, "drop_out_mc": 0.15, "repeat_mc": 50, "hidden": 30, "embedding_size": 5, "batch": 512, "lag": 4}
LME_Co_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6792 6792 6792
1.8562728 -0.6288155 0.09756618 -0.123651974
Validation: 756 756 756
Testing: 774 774 774
pre-processing time: 0.000431060791015625
the split date is 2009-07-01
train dropout: 0.6 test dropout: 0.15
net initializing with time: 0.0047168731689453125
preparing training and testing date with time: 4.76837158203125e-07
current epoch: 1
train loss is 0.012622
average val loss: 0.005300, accuracy: 0.0984
average test loss: 0.005414, accuracy: 0.0997
case acc: 0.14080869
case acc: 0.08231522
case acc: 0.09702119
case acc: 0.09235673
case acc: 0.12722838
case acc: 0.05829368
top acc: 0.1317 ::: bot acc: 0.1495
top acc: 0.0945 ::: bot acc: 0.0701
top acc: 0.0803 ::: bot acc: 0.1145
top acc: 0.0834 ::: bot acc: 0.1021
top acc: 0.1148 ::: bot acc: 0.1396
top acc: 0.0460 ::: bot acc: 0.0705
current epoch: 2
train loss is 0.008014
average val loss: 0.002714, accuracy: 0.0492
average test loss: 0.002706, accuracy: 0.0488
case acc: 0.039811596
case acc: 0.16702272
case acc: 0.013392051
case acc: 0.0077337734
case acc: 0.039668642
case acc: 0.025442755
top acc: 0.0305 ::: bot acc: 0.0494
top acc: 0.1786 ::: bot acc: 0.1553
top acc: 0.0204 ::: bot acc: 0.0142
top acc: 0.0098 ::: bot acc: 0.0091
top acc: 0.0269 ::: bot acc: 0.0517
top acc: 0.0363 ::: bot acc: 0.0147
current epoch: 3
train loss is 0.008330
average val loss: 0.007817, accuracy: 0.1058
average test loss: 0.007733, accuracy: 0.1047
case acc: 0.053657863
case acc: 0.2460881
case acc: 0.096497975
case acc: 0.08608837
case acc: 0.043261744
case acc: 0.10273726
top acc: 0.0643 ::: bot acc: 0.0438
top acc: 0.2576 ::: bot acc: 0.2350
top acc: 0.1127 ::: bot acc: 0.0797
top acc: 0.0954 ::: bot acc: 0.0756
top acc: 0.0554 ::: bot acc: 0.0316
top acc: 0.1145 ::: bot acc: 0.0910
current epoch: 4
train loss is 0.009915
average val loss: 0.016808, accuracy: 0.1727
average test loss: 0.016607, accuracy: 0.1714
case acc: 0.12763394
case acc: 0.30620068
case acc: 0.16931896
case acc: 0.15162791
case acc: 0.112293035
case acc: 0.1611444
top acc: 0.1388 ::: bot acc: 0.1176
top acc: 0.3182 ::: bot acc: 0.2948
top acc: 0.1861 ::: bot acc: 0.1523
top acc: 0.1607 ::: bot acc: 0.1409
top acc: 0.1245 ::: bot acc: 0.1005
top acc: 0.1725 ::: bot acc: 0.1497
current epoch: 5
train loss is 0.012926
average val loss: 0.013076, accuracy: 0.1511
average test loss: 0.012914, accuracy: 0.1500
case acc: 0.111456774
case acc: 0.27470067
case acc: 0.15071045
case acc: 0.13007115
case acc: 0.098003164
case acc: 0.13476826
top acc: 0.1215 ::: bot acc: 0.1017
top acc: 0.2866 ::: bot acc: 0.2629
top acc: 0.1675 ::: bot acc: 0.1333
top acc: 0.1392 ::: bot acc: 0.1196
top acc: 0.1106 ::: bot acc: 0.0851
top acc: 0.1464 ::: bot acc: 0.1236
current epoch: 6
train loss is 0.012521
average val loss: 0.002981, accuracy: 0.0569
average test loss: 0.002929, accuracy: 0.0559
case acc: 0.021585178
case acc: 0.16719747
case acc: 0.05460902
case acc: 0.03467918
case acc: 0.016726956
case acc: 0.04037968
top acc: 0.0309 ::: bot acc: 0.0126
top acc: 0.1790 ::: bot acc: 0.1557
top acc: 0.0717 ::: bot acc: 0.0381
top acc: 0.0443 ::: bot acc: 0.0242
top acc: 0.0278 ::: bot acc: 0.0072
top acc: 0.0515 ::: bot acc: 0.0287
current epoch: 7
train loss is 0.006944
average val loss: 0.001043, accuracy: 0.0342
average test loss: 0.001063, accuracy: 0.0351
case acc: 0.033150278
case acc: 0.09608245
case acc: 0.013986584
case acc: 0.022542946
case acc: 0.029603416
case acc: 0.015021796
top acc: 0.0230 ::: bot acc: 0.0427
top acc: 0.1076 ::: bot acc: 0.0846
top acc: 0.0098 ::: bot acc: 0.0241
top acc: 0.0145 ::: bot acc: 0.0321
top acc: 0.0175 ::: bot acc: 0.0419
top acc: 0.0063 ::: bot acc: 0.0250
current epoch: 8
train loss is 0.003130
average val loss: 0.000886, accuracy: 0.0265
average test loss: 0.000880, accuracy: 0.0269
case acc: 0.020847362
case acc: 0.09546993
case acc: 0.012312075
case acc: 0.011973891
case acc: 0.012654659
case acc: 0.008423881
top acc: 0.0115 ::: bot acc: 0.0297
top acc: 0.1075 ::: bot acc: 0.0836
top acc: 0.0157 ::: bot acc: 0.0177
top acc: 0.0061 ::: bot acc: 0.0201
top acc: 0.0053 ::: bot acc: 0.0225
top acc: 0.0107 ::: bot acc: 0.0116
current epoch: 9
train loss is 0.002262
average val loss: 0.000993, accuracy: 0.0279
average test loss: 0.000980, accuracy: 0.0275
case acc: 0.00795984
case acc: 0.10204266
case acc: 0.015835097
case acc: 0.008714709
case acc: 0.01314602
case acc: 0.017109038
top acc: 0.0078 ::: bot acc: 0.0124
top acc: 0.1131 ::: bot acc: 0.0908
top acc: 0.0283 ::: bot acc: 0.0075
top acc: 0.0142 ::: bot acc: 0.0064
top acc: 0.0229 ::: bot acc: 0.0063
top acc: 0.0277 ::: bot acc: 0.0071
current epoch: 10
train loss is 0.002151
average val loss: 0.000873, accuracy: 0.0270
average test loss: 0.000868, accuracy: 0.0267
case acc: 0.0076527903
case acc: 0.094726175
case acc: 0.014932055
case acc: 0.009236028
case acc: 0.015996933
case acc: 0.0175053
top acc: 0.0092 ::: bot acc: 0.0107
top acc: 0.1064 ::: bot acc: 0.0828
top acc: 0.0266 ::: bot acc: 0.0081
top acc: 0.0149 ::: bot acc: 0.0063
top acc: 0.0266 ::: bot acc: 0.0071
top acc: 0.0281 ::: bot acc: 0.0072
current epoch: 11
train loss is 0.001970
average val loss: 0.000734, accuracy: 0.0252
average test loss: 0.000731, accuracy: 0.0250
case acc: 0.0076031582
case acc: 0.08576739
case acc: 0.013613895
case acc: 0.008353384
case acc: 0.017186655
case acc: 0.017293712
top acc: 0.0097 ::: bot acc: 0.0101
top acc: 0.0976 ::: bot acc: 0.0743
top acc: 0.0238 ::: bot acc: 0.0097
top acc: 0.0140 ::: bot acc: 0.0058
top acc: 0.0284 ::: bot acc: 0.0077
top acc: 0.0280 ::: bot acc: 0.0069
current epoch: 12
train loss is 0.001677
average val loss: 0.000673, accuracy: 0.0253
average test loss: 0.000665, accuracy: 0.0249
case acc: 0.0074920245
case acc: 0.079834044
case acc: 0.0136226425
case acc: 0.009310906
case acc: 0.020115152
case acc: 0.018848257
top acc: 0.0124 ::: bot acc: 0.0074
top acc: 0.0914 ::: bot acc: 0.0685
top acc: 0.0237 ::: bot acc: 0.0101
top acc: 0.0158 ::: bot acc: 0.0054
top acc: 0.0316 ::: bot acc: 0.0098
top acc: 0.0296 ::: bot acc: 0.0086
current epoch: 13
train loss is 0.001457
average val loss: 0.000583, accuracy: 0.0241
average test loss: 0.000573, accuracy: 0.0236
case acc: 0.0075553777
case acc: 0.072685
case acc: 0.0129810525
case acc: 0.009070168
case acc: 0.020808505
case acc: 0.018341368
top acc: 0.0127 ::: bot acc: 0.0067
top acc: 0.0843 ::: bot acc: 0.0609
top acc: 0.0219 ::: bot acc: 0.0116
top acc: 0.0148 ::: bot acc: 0.0057
top acc: 0.0321 ::: bot acc: 0.0101
top acc: 0.0289 ::: bot acc: 0.0079
current epoch: 14
train loss is 0.001262
average val loss: 0.000504, accuracy: 0.0229
average test loss: 0.000498, accuracy: 0.0225
case acc: 0.007955219
case acc: 0.0660821
case acc: 0.012642442
case acc: 0.008978125
case acc: 0.021099571
case acc: 0.018434152
top acc: 0.0135 ::: bot acc: 0.0062
top acc: 0.0776 ::: bot acc: 0.0547
top acc: 0.0205 ::: bot acc: 0.0128
top acc: 0.0151 ::: bot acc: 0.0058
top acc: 0.0329 ::: bot acc: 0.0099
top acc: 0.0292 ::: bot acc: 0.0081
current epoch: 15
train loss is 0.001085
average val loss: 0.000455, accuracy: 0.0223
average test loss: 0.000447, accuracy: 0.0219
case acc: 0.008461988
case acc: 0.06097799
case acc: 0.012481535
case acc: 0.009438434
case acc: 0.02148083
case acc: 0.018356172
top acc: 0.0154 ::: bot acc: 0.0051
top acc: 0.0724 ::: bot acc: 0.0494
top acc: 0.0199 ::: bot acc: 0.0132
top acc: 0.0155 ::: bot acc: 0.0057
top acc: 0.0330 ::: bot acc: 0.0107
top acc: 0.0293 ::: bot acc: 0.0079
current epoch: 16
train loss is 0.001006
average val loss: 0.000447, accuracy: 0.0231
average test loss: 0.000441, accuracy: 0.0226
case acc: 0.010128085
case acc: 0.058315013
case acc: 0.013167413
case acc: 0.010966241
case acc: 0.023213888
case acc: 0.02004822
top acc: 0.0181 ::: bot acc: 0.0042
top acc: 0.0701 ::: bot acc: 0.0463
top acc: 0.0222 ::: bot acc: 0.0116
top acc: 0.0179 ::: bot acc: 0.0055
top acc: 0.0347 ::: bot acc: 0.0126
top acc: 0.0308 ::: bot acc: 0.0097
current epoch: 17
train loss is 0.000931
average val loss: 0.000454, accuracy: 0.0242
average test loss: 0.000448, accuracy: 0.0236
case acc: 0.012374376
case acc: 0.05655829
case acc: 0.013765474
case acc: 0.012462325
case acc: 0.0248421
case acc: 0.021774074
top acc: 0.0214 ::: bot acc: 0.0043
top acc: 0.0681 ::: bot acc: 0.0451
top acc: 0.0244 ::: bot acc: 0.0092
top acc: 0.0202 ::: bot acc: 0.0056
top acc: 0.0365 ::: bot acc: 0.0134
top acc: 0.0326 ::: bot acc: 0.0111
current epoch: 18
train loss is 0.000863
average val loss: 0.000430, accuracy: 0.0240
average test loss: 0.000419, accuracy: 0.0233
case acc: 0.013589975
case acc: 0.0531528
case acc: 0.014310892
case acc: 0.013256758
case acc: 0.024102591
case acc: 0.021550974
top acc: 0.0228 ::: bot acc: 0.0055
top acc: 0.0649 ::: bot acc: 0.0415
top acc: 0.0254 ::: bot acc: 0.0089
top acc: 0.0210 ::: bot acc: 0.0059
top acc: 0.0360 ::: bot acc: 0.0128
top acc: 0.0326 ::: bot acc: 0.0108
current epoch: 19
train loss is 0.000822
average val loss: 0.000391, accuracy: 0.0231
average test loss: 0.000381, accuracy: 0.0224
case acc: 0.013882014
case acc: 0.049621854
case acc: 0.014122927
case acc: 0.013275607
case acc: 0.022958761
case acc: 0.020473452
top acc: 0.0228 ::: bot acc: 0.0055
top acc: 0.0615 ::: bot acc: 0.0379
top acc: 0.0251 ::: bot acc: 0.0085
top acc: 0.0208 ::: bot acc: 0.0064
top acc: 0.0352 ::: bot acc: 0.0113
top acc: 0.0311 ::: bot acc: 0.0099
current epoch: 20
train loss is 0.000748
average val loss: 0.000407, accuracy: 0.0242
average test loss: 0.000394, accuracy: 0.0234
case acc: 0.016312484
case acc: 0.048472974
case acc: 0.015740387
case acc: 0.014773461
case acc: 0.023764543
case acc: 0.02149549
top acc: 0.0257 ::: bot acc: 0.0078
top acc: 0.0603 ::: bot acc: 0.0367
top acc: 0.0281 ::: bot acc: 0.0077
top acc: 0.0231 ::: bot acc: 0.0069
top acc: 0.0355 ::: bot acc: 0.0126
top acc: 0.0324 ::: bot acc: 0.0105
current epoch: 21
train loss is 0.000749
average val loss: 0.000425, accuracy: 0.0253
average test loss: 0.000407, accuracy: 0.0243
case acc: 0.01837847
case acc: 0.0477282
case acc: 0.017012578
case acc: 0.016571792
case acc: 0.024050757
case acc: 0.022352474
top acc: 0.0280 ::: bot acc: 0.0095
top acc: 0.0595 ::: bot acc: 0.0359
top acc: 0.0304 ::: bot acc: 0.0072
top acc: 0.0252 ::: bot acc: 0.0081
top acc: 0.0359 ::: bot acc: 0.0128
top acc: 0.0333 ::: bot acc: 0.0117
current epoch: 22
train loss is 0.000712
average val loss: 0.000464, accuracy: 0.0270
average test loss: 0.000445, accuracy: 0.0261
case acc: 0.02152629
case acc: 0.0477971
case acc: 0.019306002
case acc: 0.01901416
case acc: 0.025263704
case acc: 0.023577498
top acc: 0.0311 ::: bot acc: 0.0124
top acc: 0.0596 ::: bot acc: 0.0362
top acc: 0.0336 ::: bot acc: 0.0071
top acc: 0.0277 ::: bot acc: 0.0101
top acc: 0.0372 ::: bot acc: 0.0136
top acc: 0.0347 ::: bot acc: 0.0128
current epoch: 23
train loss is 0.000707
average val loss: 0.000489, accuracy: 0.0281
average test loss: 0.000464, accuracy: 0.0270
case acc: 0.023515712
case acc: 0.047165886
case acc: 0.02108515
case acc: 0.020219041
case acc: 0.025984662
case acc: 0.024082681
top acc: 0.0329 ::: bot acc: 0.0146
top acc: 0.0585 ::: bot acc: 0.0358
top acc: 0.0363 ::: bot acc: 0.0078
top acc: 0.0292 ::: bot acc: 0.0109
top acc: 0.0379 ::: bot acc: 0.0144
top acc: 0.0352 ::: bot acc: 0.0132
current epoch: 24
train loss is 0.000658
average val loss: 0.000433, accuracy: 0.0263
average test loss: 0.000414, accuracy: 0.0254
case acc: 0.022634957
case acc: 0.044105586
case acc: 0.020885656
case acc: 0.01912961
case acc: 0.023776742
case acc: 0.021609938
top acc: 0.0324 ::: bot acc: 0.0135
top acc: 0.0557 ::: bot acc: 0.0325
top acc: 0.0359 ::: bot acc: 0.0079
top acc: 0.0277 ::: bot acc: 0.0103
top acc: 0.0357 ::: bot acc: 0.0124
top acc: 0.0327 ::: bot acc: 0.0107
current epoch: 25
train loss is 0.000629
average val loss: 0.000506, accuracy: 0.0289
average test loss: 0.000477, accuracy: 0.0277
case acc: 0.026065454
case acc: 0.045541164
case acc: 0.024205966
case acc: 0.021480799
case acc: 0.025807133
case acc: 0.023216
top acc: 0.0361 ::: bot acc: 0.0163
top acc: 0.0572 ::: bot acc: 0.0340
top acc: 0.0399 ::: bot acc: 0.0099
top acc: 0.0307 ::: bot acc: 0.0120
top acc: 0.0376 ::: bot acc: 0.0143
top acc: 0.0343 ::: bot acc: 0.0124
current epoch: 26
train loss is 0.000629
average val loss: 0.000501, accuracy: 0.0289
average test loss: 0.000477, accuracy: 0.0278
case acc: 0.026686098
case acc: 0.04478212
case acc: 0.025178373
case acc: 0.022057744
case acc: 0.025377974
case acc: 0.022769932
top acc: 0.0363 ::: bot acc: 0.0174
top acc: 0.0568 ::: bot acc: 0.0330
top acc: 0.0410 ::: bot acc: 0.0102
top acc: 0.0315 ::: bot acc: 0.0123
top acc: 0.0372 ::: bot acc: 0.0139
top acc: 0.0337 ::: bot acc: 0.0120
current epoch: 27
train loss is 0.000629
average val loss: 0.000551, accuracy: 0.0306
average test loss: 0.000521, accuracy: 0.0294
case acc: 0.028851844
case acc: 0.045415524
case acc: 0.027818942
case acc: 0.024017544
case acc: 0.026529128
case acc: 0.023602035
top acc: 0.0388 ::: bot acc: 0.0192
top acc: 0.0574 ::: bot acc: 0.0337
top acc: 0.0439 ::: bot acc: 0.0125
top acc: 0.0330 ::: bot acc: 0.0141
top acc: 0.0383 ::: bot acc: 0.0150
top acc: 0.0349 ::: bot acc: 0.0128
current epoch: 28
train loss is 0.000639
average val loss: 0.000620, accuracy: 0.0328
average test loss: 0.000589, accuracy: 0.0316
case acc: 0.031675253
case acc: 0.04682559
case acc: 0.030984135
case acc: 0.02630637
case acc: 0.028432501
case acc: 0.025438484
top acc: 0.0414 ::: bot acc: 0.0221
top acc: 0.0586 ::: bot acc: 0.0352
top acc: 0.0477 ::: bot acc: 0.0150
top acc: 0.0355 ::: bot acc: 0.0163
top acc: 0.0404 ::: bot acc: 0.0168
top acc: 0.0366 ::: bot acc: 0.0142
current epoch: 29
train loss is 0.000640
average val loss: 0.000719, accuracy: 0.0357
average test loss: 0.000683, accuracy: 0.0345
case acc: 0.03511672
case acc: 0.04889073
case acc: 0.034671694
case acc: 0.029387083
case acc: 0.031280268
case acc: 0.027512465
top acc: 0.0448 ::: bot acc: 0.0256
top acc: 0.0609 ::: bot acc: 0.0370
top acc: 0.0510 ::: bot acc: 0.0186
top acc: 0.0383 ::: bot acc: 0.0197
top acc: 0.0433 ::: bot acc: 0.0194
top acc: 0.0386 ::: bot acc: 0.0165
current epoch: 30
train loss is 0.000665
average val loss: 0.000750, accuracy: 0.0366
average test loss: 0.000713, accuracy: 0.0354
case acc: 0.036176898
case acc: 0.04880937
case acc: 0.03649571
case acc: 0.030500634
case acc: 0.03267552
case acc: 0.027751466
top acc: 0.0459 ::: bot acc: 0.0266
top acc: 0.0607 ::: bot acc: 0.0370
top acc: 0.0529 ::: bot acc: 0.0203
top acc: 0.0397 ::: bot acc: 0.0205
top acc: 0.0445 ::: bot acc: 0.0209
top acc: 0.0388 ::: bot acc: 0.0166
current epoch: 31
train loss is 0.000653
average val loss: 0.000784, accuracy: 0.0375
average test loss: 0.000750, accuracy: 0.0364
case acc: 0.03742344
case acc: 0.04888076
case acc: 0.03827667
case acc: 0.031756733
case acc: 0.033953816
case acc: 0.028296055
top acc: 0.0472 ::: bot acc: 0.0277
top acc: 0.0610 ::: bot acc: 0.0370
top acc: 0.0551 ::: bot acc: 0.0219
top acc: 0.0412 ::: bot acc: 0.0215
top acc: 0.0459 ::: bot acc: 0.0220
top acc: 0.0395 ::: bot acc: 0.0170
current epoch: 32
train loss is 0.000666
average val loss: 0.000798, accuracy: 0.0379
average test loss: 0.000758, accuracy: 0.0367
case acc: 0.03768332
case acc: 0.048218418
case acc: 0.039212942
case acc: 0.03219912
case acc: 0.03451707
case acc: 0.028182417
top acc: 0.0477 ::: bot acc: 0.0281
top acc: 0.0600 ::: bot acc: 0.0365
top acc: 0.0558 ::: bot acc: 0.0228
top acc: 0.0413 ::: bot acc: 0.0221
top acc: 0.0467 ::: bot acc: 0.0224
top acc: 0.0394 ::: bot acc: 0.0170
current epoch: 33
train loss is 0.000649
average val loss: 0.000846, accuracy: 0.0392
average test loss: 0.000803, accuracy: 0.0380
case acc: 0.038815256
case acc: 0.048407286
case acc: 0.041037284
case acc: 0.034039646
case acc: 0.03599146
case acc: 0.02941353
top acc: 0.0486 ::: bot acc: 0.0293
top acc: 0.0602 ::: bot acc: 0.0367
top acc: 0.0575 ::: bot acc: 0.0246
top acc: 0.0434 ::: bot acc: 0.0241
top acc: 0.0482 ::: bot acc: 0.0237
top acc: 0.0406 ::: bot acc: 0.0182
current epoch: 34
train loss is 0.000646
average val loss: 0.000816, accuracy: 0.0384
average test loss: 0.000776, accuracy: 0.0373
case acc: 0.038175374
case acc: 0.04679683
case acc: 0.040513836
case acc: 0.033560403
case acc: 0.036053117
case acc: 0.028552793
top acc: 0.0480 ::: bot acc: 0.0288
top acc: 0.0584 ::: bot acc: 0.0349
top acc: 0.0574 ::: bot acc: 0.0240
top acc: 0.0430 ::: bot acc: 0.0236
top acc: 0.0480 ::: bot acc: 0.0241
top acc: 0.0397 ::: bot acc: 0.0174
current epoch: 35
train loss is 0.000624
average val loss: 0.000748, accuracy: 0.0367
average test loss: 0.000709, accuracy: 0.0355
case acc: 0.036075156
case acc: 0.043771382
case acc: 0.039373048
case acc: 0.032076906
case acc: 0.034741446
case acc: 0.026847923
top acc: 0.0456 ::: bot acc: 0.0267
top acc: 0.0554 ::: bot acc: 0.0321
top acc: 0.0560 ::: bot acc: 0.0229
top acc: 0.0416 ::: bot acc: 0.0218
top acc: 0.0471 ::: bot acc: 0.0226
top acc: 0.0379 ::: bot acc: 0.0156
current epoch: 36
train loss is 0.000588
average val loss: 0.000733, accuracy: 0.0364
average test loss: 0.000693, accuracy: 0.0351
case acc: 0.035252452
case acc: 0.042360898
case acc: 0.0389599
case acc: 0.032346025
case acc: 0.03482797
case acc: 0.026775923
top acc: 0.0449 ::: bot acc: 0.0257
top acc: 0.0543 ::: bot acc: 0.0305
top acc: 0.0556 ::: bot acc: 0.0225
top acc: 0.0418 ::: bot acc: 0.0222
top acc: 0.0469 ::: bot acc: 0.0229
top acc: 0.0378 ::: bot acc: 0.0158
current epoch: 37
train loss is 0.000555
average val loss: 0.000607, accuracy: 0.0328
average test loss: 0.000571, accuracy: 0.0315
case acc: 0.03119317
case acc: 0.037751995
case acc: 0.035408787
case acc: 0.02910527
case acc: 0.031735066
case acc: 0.024013886
top acc: 0.0410 ::: bot acc: 0.0217
top acc: 0.0497 ::: bot acc: 0.0260
top acc: 0.0518 ::: bot acc: 0.0192
top acc: 0.0383 ::: bot acc: 0.0193
top acc: 0.0440 ::: bot acc: 0.0197
top acc: 0.0354 ::: bot acc: 0.0127
current epoch: 38
train loss is 0.000506
average val loss: 0.000498, accuracy: 0.0294
average test loss: 0.000467, accuracy: 0.0282
case acc: 0.027624063
case acc: 0.033112384
case acc: 0.031837553
case acc: 0.026257925
case acc: 0.029156143
case acc: 0.021135248
top acc: 0.0372 ::: bot acc: 0.0184
top acc: 0.0451 ::: bot acc: 0.0214
top acc: 0.0481 ::: bot acc: 0.0160
top acc: 0.0358 ::: bot acc: 0.0162
top acc: 0.0412 ::: bot acc: 0.0174
top acc: 0.0318 ::: bot acc: 0.0106
current epoch: 39
train loss is 0.000466
average val loss: 0.000417, accuracy: 0.0265
average test loss: 0.000388, accuracy: 0.0253
case acc: 0.024250083
case acc: 0.029062977
case acc: 0.028852548
case acc: 0.023876032
case acc: 0.027251495
case acc: 0.018721148
top acc: 0.0339 ::: bot acc: 0.0149
top acc: 0.0409 ::: bot acc: 0.0173
top acc: 0.0451 ::: bot acc: 0.0132
top acc: 0.0331 ::: bot acc: 0.0143
top acc: 0.0392 ::: bot acc: 0.0154
top acc: 0.0296 ::: bot acc: 0.0085
current epoch: 40
train loss is 0.000432
average val loss: 0.000374, accuracy: 0.0249
average test loss: 0.000349, accuracy: 0.0239
case acc: 0.022392945
case acc: 0.026825309
case acc: 0.027214317
case acc: 0.022576684
case acc: 0.026394146
case acc: 0.017776353
top acc: 0.0320 ::: bot acc: 0.0134
top acc: 0.0385 ::: bot acc: 0.0153
top acc: 0.0435 ::: bot acc: 0.0117
top acc: 0.0315 ::: bot acc: 0.0132
top acc: 0.0382 ::: bot acc: 0.0148
top acc: 0.0287 ::: bot acc: 0.0073
current epoch: 41
train loss is 0.000406
average val loss: 0.000298, accuracy: 0.0218
average test loss: 0.000278, accuracy: 0.0208
case acc: 0.019058352
case acc: 0.022520376
case acc: 0.023997178
case acc: 0.02023466
case acc: 0.023590857
case acc: 0.015610509
top acc: 0.0284 ::: bot acc: 0.0100
top acc: 0.0343 ::: bot acc: 0.0108
top acc: 0.0397 ::: bot acc: 0.0093
top acc: 0.0290 ::: bot acc: 0.0112
top acc: 0.0353 ::: bot acc: 0.0122
top acc: 0.0263 ::: bot acc: 0.0059
current epoch: 42
train loss is 0.000374
average val loss: 0.000211, accuracy: 0.0177
average test loss: 0.000196, accuracy: 0.0169
case acc: 0.014286712
case acc: 0.017404983
case acc: 0.020087844
case acc: 0.016494483
case acc: 0.020072555
case acc: 0.012931384
top acc: 0.0233 ::: bot acc: 0.0061
top acc: 0.0288 ::: bot acc: 0.0063
top acc: 0.0351 ::: bot acc: 0.0072
top acc: 0.0251 ::: bot acc: 0.0081
top acc: 0.0316 ::: bot acc: 0.0094
top acc: 0.0226 ::: bot acc: 0.0049
current epoch: 43
train loss is 0.000346
average val loss: 0.000146, accuracy: 0.0141
average test loss: 0.000137, accuracy: 0.0136
case acc: 0.010423812
case acc: 0.013196161
case acc: 0.016713068
case acc: 0.013312671
case acc: 0.016904583
case acc: 0.0107508665
top acc: 0.0187 ::: bot acc: 0.0037
top acc: 0.0239 ::: bot acc: 0.0040
top acc: 0.0301 ::: bot acc: 0.0069
top acc: 0.0213 ::: bot acc: 0.0063
top acc: 0.0278 ::: bot acc: 0.0072
top acc: 0.0191 ::: bot acc: 0.0053
current epoch: 44
train loss is 0.000331
average val loss: 0.000120, accuracy: 0.0126
average test loss: 0.000113, accuracy: 0.0121
case acc: 0.0088386405
case acc: 0.01102916
case acc: 0.01513161
case acc: 0.011798547
case acc: 0.015644388
case acc: 0.010179337
top acc: 0.0161 ::: bot acc: 0.0043
top acc: 0.0203 ::: bot acc: 0.0043
top acc: 0.0276 ::: bot acc: 0.0078
top acc: 0.0192 ::: bot acc: 0.0056
top acc: 0.0261 ::: bot acc: 0.0067
top acc: 0.0181 ::: bot acc: 0.0058
current epoch: 45
train loss is 0.000326
average val loss: 0.000095, accuracy: 0.0109
average test loss: 0.000093, accuracy: 0.0108
case acc: 0.0076929806
case acc: 0.009571144
case acc: 0.013536878
case acc: 0.01028401
case acc: 0.014008661
case acc: 0.009496602
top acc: 0.0132 ::: bot acc: 0.0064
top acc: 0.0170 ::: bot acc: 0.0067
top acc: 0.0239 ::: bot acc: 0.0099
top acc: 0.0172 ::: bot acc: 0.0052
top acc: 0.0241 ::: bot acc: 0.0059
top acc: 0.0165 ::: bot acc: 0.0067
current epoch: 46
train loss is 0.000316
average val loss: 0.000069, accuracy: 0.0089
average test loss: 0.000072, accuracy: 0.0092
case acc: 0.0073182024
case acc: 0.008763163
case acc: 0.012247317
case acc: 0.0077526146
case acc: 0.010815038
case acc: 0.008441895
top acc: 0.0075 ::: bot acc: 0.0117
top acc: 0.0101 ::: bot acc: 0.0131
top acc: 0.0175 ::: bot acc: 0.0160
top acc: 0.0123 ::: bot acc: 0.0075
top acc: 0.0193 ::: bot acc: 0.0060
top acc: 0.0125 ::: bot acc: 0.0103
current epoch: 47
train loss is 0.000321
average val loss: 0.000067, accuracy: 0.0088
average test loss: 0.000074, accuracy: 0.0093
case acc: 0.008340332
case acc: 0.009745902
case acc: 0.012785782
case acc: 0.0072209076
case acc: 0.0097348085
case acc: 0.008235843
top acc: 0.0044 ::: bot acc: 0.0149
top acc: 0.0065 ::: bot acc: 0.0171
top acc: 0.0135 ::: bot acc: 0.0202
top acc: 0.0099 ::: bot acc: 0.0100
top acc: 0.0170 ::: bot acc: 0.0073
top acc: 0.0104 ::: bot acc: 0.0119
current epoch: 48
train loss is 0.000332
average val loss: 0.000076, accuracy: 0.0094
average test loss: 0.000088, accuracy: 0.0103
case acc: 0.01041015
case acc: 0.012076539
case acc: 0.013962638
case acc: 0.007468413
case acc: 0.009066499
case acc: 0.008628934
top acc: 0.0039 ::: bot acc: 0.0181
top acc: 0.0042 ::: bot acc: 0.0219
top acc: 0.0091 ::: bot acc: 0.0246
top acc: 0.0069 ::: bot acc: 0.0132
top acc: 0.0151 ::: bot acc: 0.0090
top acc: 0.0088 ::: bot acc: 0.0139
current epoch: 49
train loss is 0.000347
average val loss: 0.000111, accuracy: 0.0117
average test loss: 0.000130, accuracy: 0.0128
case acc: 0.01459848
case acc: 0.017271575
case acc: 0.017583769
case acc: 0.00940865
case acc: 0.0086934725
case acc: 0.009504628
top acc: 0.0058 ::: bot acc: 0.0235
top acc: 0.0071 ::: bot acc: 0.0283
top acc: 0.0069 ::: bot acc: 0.0310
top acc: 0.0043 ::: bot acc: 0.0174
top acc: 0.0117 ::: bot acc: 0.0127
top acc: 0.0060 ::: bot acc: 0.0168
current epoch: 50
train loss is 0.000380
average val loss: 0.000195, accuracy: 0.0164
average test loss: 0.000224, accuracy: 0.0178
case acc: 0.021012247
case acc: 0.024777383
case acc: 0.024344359
case acc: 0.014418299
case acc: 0.010079046
case acc: 0.012098816
top acc: 0.0114 ::: bot acc: 0.0305
top acc: 0.0132 ::: bot acc: 0.0365
top acc: 0.0101 ::: bot acc: 0.0397
top acc: 0.0068 ::: bot acc: 0.0236
top acc: 0.0069 ::: bot acc: 0.0177
top acc: 0.0048 ::: bot acc: 0.0214
LME_Co_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6810 6810 6810
1.8562728 -0.6288155 0.08104724 -0.1112376
Validation: 762 762 762
Testing: 744 744 744
pre-processing time: 0.00022339820861816406
the split date is 2010-01-01
train dropout: 0.6 test dropout: 0.15
net initializing with time: 0.0024826526641845703
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012624
average val loss: 0.005058, accuracy: 0.0962
average test loss: 0.005154, accuracy: 0.0967
case acc: 0.13606447
case acc: 0.08710063
case acc: 0.09249652
case acc: 0.08899484
case acc: 0.12325185
case acc: 0.052222654
top acc: 0.1216 ::: bot acc: 0.1503
top acc: 0.1010 ::: bot acc: 0.0743
top acc: 0.0730 ::: bot acc: 0.1132
top acc: 0.0721 ::: bot acc: 0.1056
top acc: 0.1094 ::: bot acc: 0.1371
top acc: 0.0355 ::: bot acc: 0.0684
current epoch: 2
train loss is 0.007461
average val loss: 0.002886, accuracy: 0.0500
average test loss: 0.002922, accuracy: 0.0506
case acc: 0.03252619
case acc: 0.17434992
case acc: 0.017777124
case acc: 0.013078591
case acc: 0.032094862
case acc: 0.03380539
top acc: 0.0180 ::: bot acc: 0.0477
top acc: 0.1882 ::: bot acc: 0.1622
top acc: 0.0314 ::: bot acc: 0.0112
top acc: 0.0226 ::: bot acc: 0.0105
top acc: 0.0176 ::: bot acc: 0.0459
top acc: 0.0506 ::: bot acc: 0.0172
current epoch: 3
train loss is 0.007934
average val loss: 0.007700, accuracy: 0.1048
average test loss: 0.007733, accuracy: 0.1045
case acc: 0.053736616
case acc: 0.24531889
case acc: 0.09581992
case acc: 0.08415624
case acc: 0.044448864
case acc: 0.10356712
top acc: 0.0679 ::: bot acc: 0.0397
top acc: 0.2593 ::: bot acc: 0.2331
top acc: 0.1167 ::: bot acc: 0.0759
top acc: 0.1000 ::: bot acc: 0.0683
top acc: 0.0584 ::: bot acc: 0.0306
top acc: 0.1202 ::: bot acc: 0.0866
current epoch: 4
train loss is 0.010232
average val loss: 0.016182, accuracy: 0.1691
average test loss: 0.016172, accuracy: 0.1687
case acc: 0.12509347
case acc: 0.3027487
case acc: 0.16590896
case acc: 0.14744048
case acc: 0.11121733
case acc: 0.15982023
top acc: 0.1399 ::: bot acc: 0.1099
top acc: 0.3163 ::: bot acc: 0.2905
top acc: 0.1870 ::: bot acc: 0.1452
top acc: 0.1628 ::: bot acc: 0.1316
top acc: 0.1254 ::: bot acc: 0.0978
top acc: 0.1765 ::: bot acc: 0.1430
current epoch: 5
train loss is 0.012601
average val loss: 0.013802, accuracy: 0.1561
average test loss: 0.013799, accuracy: 0.1557
case acc: 0.11751386
case acc: 0.27976552
case acc: 0.15619278
case acc: 0.13393979
case acc: 0.10546449
case acc: 0.14141352
top acc: 0.1312 ::: bot acc: 0.1034
top acc: 0.2936 ::: bot acc: 0.2680
top acc: 0.1773 ::: bot acc: 0.1352
top acc: 0.1498 ::: bot acc: 0.1178
top acc: 0.1194 ::: bot acc: 0.0924
top acc: 0.1579 ::: bot acc: 0.1253
current epoch: 6
train loss is 0.012908
average val loss: 0.002913, accuracy: 0.0559
average test loss: 0.002948, accuracy: 0.0559
case acc: 0.022256939
case acc: 0.16636439
case acc: 0.054091908
case acc: 0.03358915
case acc: 0.01738204
case acc: 0.041426364
top acc: 0.0355 ::: bot acc: 0.0096
top acc: 0.1802 ::: bot acc: 0.1545
top acc: 0.0750 ::: bot acc: 0.0331
top acc: 0.0489 ::: bot acc: 0.0184
top acc: 0.0308 ::: bot acc: 0.0061
top acc: 0.0579 ::: bot acc: 0.0250
current epoch: 7
train loss is 0.007137
average val loss: 0.001044, accuracy: 0.0317
average test loss: 0.001096, accuracy: 0.0335
case acc: 0.027273979
case acc: 0.10088001
case acc: 0.015337783
case acc: 0.020439724
case acc: 0.022807742
case acc: 0.014231272
top acc: 0.0139 ::: bot acc: 0.0416
top acc: 0.1143 ::: bot acc: 0.0887
top acc: 0.0186 ::: bot acc: 0.0236
top acc: 0.0099 ::: bot acc: 0.0337
top acc: 0.0092 ::: bot acc: 0.0359
top acc: 0.0101 ::: bot acc: 0.0244
current epoch: 8
train loss is 0.003083
average val loss: 0.000881, accuracy: 0.0262
average test loss: 0.000920, accuracy: 0.0285
case acc: 0.01980148
case acc: 0.095783055
case acc: 0.0152294
case acc: 0.01556981
case acc: 0.012094204
case acc: 0.012502948
top acc: 0.0079 ::: bot acc: 0.0333
top acc: 0.1090 ::: bot acc: 0.0840
top acc: 0.0210 ::: bot acc: 0.0206
top acc: 0.0085 ::: bot acc: 0.0269
top acc: 0.0074 ::: bot acc: 0.0208
top acc: 0.0184 ::: bot acc: 0.0155
current epoch: 9
train loss is 0.002187
average val loss: 0.001000, accuracy: 0.0282
average test loss: 0.001035, accuracy: 0.0295
case acc: 0.010133666
case acc: 0.102086134
case acc: 0.01832889
case acc: 0.012251443
case acc: 0.014701701
case acc: 0.019362157
top acc: 0.0130 ::: bot acc: 0.0153
top acc: 0.1160 ::: bot acc: 0.0899
top acc: 0.0332 ::: bot acc: 0.0089
top acc: 0.0202 ::: bot acc: 0.0116
top acc: 0.0272 ::: bot acc: 0.0052
top acc: 0.0350 ::: bot acc: 0.0058
current epoch: 10
train loss is 0.002124
average val loss: 0.000929, accuracy: 0.0286
average test loss: 0.000976, accuracy: 0.0301
case acc: 0.010862952
case acc: 0.096844465
case acc: 0.018654196
case acc: 0.013196699
case acc: 0.018903846
case acc: 0.02186369
top acc: 0.0167 ::: bot acc: 0.0122
top acc: 0.1106 ::: bot acc: 0.0848
top acc: 0.0339 ::: bot acc: 0.0088
top acc: 0.0227 ::: bot acc: 0.0096
top acc: 0.0323 ::: bot acc: 0.0073
top acc: 0.0379 ::: bot acc: 0.0077
current epoch: 11
train loss is 0.001885
average val loss: 0.000745, accuracy: 0.0255
average test loss: 0.000784, accuracy: 0.0271
case acc: 0.0105968295
case acc: 0.0857365
case acc: 0.01645079
case acc: 0.0121750925
case acc: 0.018202681
case acc: 0.019731853
top acc: 0.0150 ::: bot acc: 0.0141
top acc: 0.0993 ::: bot acc: 0.0736
top acc: 0.0286 ::: bot acc: 0.0127
top acc: 0.0199 ::: bot acc: 0.0118
top acc: 0.0315 ::: bot acc: 0.0065
top acc: 0.0352 ::: bot acc: 0.0060
current epoch: 12
train loss is 0.001594
average val loss: 0.000594, accuracy: 0.0229
average test loss: 0.000636, accuracy: 0.0249
case acc: 0.010240928
case acc: 0.07564905
case acc: 0.015978472
case acc: 0.011721905
case acc: 0.017564787
case acc: 0.018008355
top acc: 0.0131 ::: bot acc: 0.0159
top acc: 0.0893 ::: bot acc: 0.0635
top acc: 0.0248 ::: bot acc: 0.0181
top acc: 0.0173 ::: bot acc: 0.0144
top acc: 0.0306 ::: bot acc: 0.0062
top acc: 0.0328 ::: bot acc: 0.0053
current epoch: 13
train loss is 0.001329
average val loss: 0.000563, accuracy: 0.0233
average test loss: 0.000602, accuracy: 0.0250
case acc: 0.01077434
case acc: 0.07161619
case acc: 0.015702149
case acc: 0.012122928
case acc: 0.02005175
case acc: 0.019476272
top acc: 0.0170 ::: bot acc: 0.0122
top acc: 0.0854 ::: bot acc: 0.0591
top acc: 0.0249 ::: bot acc: 0.0164
top acc: 0.0195 ::: bot acc: 0.0123
top acc: 0.0335 ::: bot acc: 0.0078
top acc: 0.0349 ::: bot acc: 0.0060
current epoch: 14
train loss is 0.001201
average val loss: 0.000525, accuracy: 0.0234
average test loss: 0.000569, accuracy: 0.0251
case acc: 0.011633038
case acc: 0.067073844
case acc: 0.01609069
case acc: 0.012602475
case acc: 0.022172557
case acc: 0.021197045
top acc: 0.0199 ::: bot acc: 0.0094
top acc: 0.0805 ::: bot acc: 0.0551
top acc: 0.0264 ::: bot acc: 0.0157
top acc: 0.0214 ::: bot acc: 0.0107
top acc: 0.0362 ::: bot acc: 0.0092
top acc: 0.0372 ::: bot acc: 0.0068
current epoch: 15
train loss is 0.001066
average val loss: 0.000460, accuracy: 0.0222
average test loss: 0.000500, accuracy: 0.0238
case acc: 0.011797563
case acc: 0.06135984
case acc: 0.015538369
case acc: 0.0124386875
case acc: 0.021427358
case acc: 0.020350814
top acc: 0.0201 ::: bot acc: 0.0092
top acc: 0.0746 ::: bot acc: 0.0492
top acc: 0.0252 ::: bot acc: 0.0165
top acc: 0.0207 ::: bot acc: 0.0110
top acc: 0.0355 ::: bot acc: 0.0086
top acc: 0.0364 ::: bot acc: 0.0058
current epoch: 16
train loss is 0.001019
average val loss: 0.000464, accuracy: 0.0234
average test loss: 0.000507, accuracy: 0.0249
case acc: 0.013526198
case acc: 0.059336558
case acc: 0.01633271
case acc: 0.013896785
case acc: 0.02360401
case acc: 0.022434352
top acc: 0.0234 ::: bot acc: 0.0065
top acc: 0.0731 ::: bot acc: 0.0471
top acc: 0.0277 ::: bot acc: 0.0145
top acc: 0.0242 ::: bot acc: 0.0088
top acc: 0.0374 ::: bot acc: 0.0109
top acc: 0.0383 ::: bot acc: 0.0078
current epoch: 17
train loss is 0.000916
average val loss: 0.000446, accuracy: 0.0236
average test loss: 0.000489, accuracy: 0.0249
case acc: 0.014517144
case acc: 0.056214005
case acc: 0.01654691
case acc: 0.01450828
case acc: 0.024467517
case acc: 0.022848189
top acc: 0.0252 ::: bot acc: 0.0067
top acc: 0.0699 ::: bot acc: 0.0444
top acc: 0.0289 ::: bot acc: 0.0129
top acc: 0.0256 ::: bot acc: 0.0079
top acc: 0.0384 ::: bot acc: 0.0114
top acc: 0.0390 ::: bot acc: 0.0078
current epoch: 18
train loss is 0.000870
average val loss: 0.000418, accuracy: 0.0233
average test loss: 0.000459, accuracy: 0.0244
case acc: 0.015517457
case acc: 0.052615285
case acc: 0.017106015
case acc: 0.015027123
case acc: 0.023572363
case acc: 0.022751097
top acc: 0.0270 ::: bot acc: 0.0062
top acc: 0.0662 ::: bot acc: 0.0406
top acc: 0.0298 ::: bot acc: 0.0126
top acc: 0.0263 ::: bot acc: 0.0083
top acc: 0.0376 ::: bot acc: 0.0105
top acc: 0.0388 ::: bot acc: 0.0078
current epoch: 19
train loss is 0.000796
average val loss: 0.000409, accuracy: 0.0235
average test loss: 0.000451, accuracy: 0.0245
case acc: 0.01667123
case acc: 0.05064562
case acc: 0.017232066
case acc: 0.015657239
case acc: 0.023879787
case acc: 0.02284262
top acc: 0.0288 ::: bot acc: 0.0064
top acc: 0.0642 ::: bot acc: 0.0386
top acc: 0.0308 ::: bot acc: 0.0110
top acc: 0.0276 ::: bot acc: 0.0077
top acc: 0.0378 ::: bot acc: 0.0109
top acc: 0.0389 ::: bot acc: 0.0077
current epoch: 20
train loss is 0.000733
average val loss: 0.000379, accuracy: 0.0229
average test loss: 0.000425, accuracy: 0.0240
case acc: 0.01705008
case acc: 0.047836084
case acc: 0.017736869
case acc: 0.015725167
case acc: 0.022879308
case acc: 0.02248187
top acc: 0.0296 ::: bot acc: 0.0061
top acc: 0.0616 ::: bot acc: 0.0360
top acc: 0.0318 ::: bot acc: 0.0100
top acc: 0.0275 ::: bot acc: 0.0077
top acc: 0.0367 ::: bot acc: 0.0099
top acc: 0.0387 ::: bot acc: 0.0077
current epoch: 21
train loss is 0.000688
average val loss: 0.000382, accuracy: 0.0234
average test loss: 0.000426, accuracy: 0.0243
case acc: 0.01880822
case acc: 0.046241578
case acc: 0.01841171
case acc: 0.016675413
case acc: 0.022738954
case acc: 0.022751266
top acc: 0.0316 ::: bot acc: 0.0073
top acc: 0.0600 ::: bot acc: 0.0340
top acc: 0.0339 ::: bot acc: 0.0088
top acc: 0.0288 ::: bot acc: 0.0076
top acc: 0.0366 ::: bot acc: 0.0096
top acc: 0.0389 ::: bot acc: 0.0081
current epoch: 22
train loss is 0.000678
average val loss: 0.000403, accuracy: 0.0246
average test loss: 0.000448, accuracy: 0.0253
case acc: 0.020781513
case acc: 0.04572396
case acc: 0.019801134
case acc: 0.018256493
case acc: 0.02354538
case acc: 0.023714148
top acc: 0.0340 ::: bot acc: 0.0086
top acc: 0.0594 ::: bot acc: 0.0336
top acc: 0.0364 ::: bot acc: 0.0075
top acc: 0.0311 ::: bot acc: 0.0080
top acc: 0.0377 ::: bot acc: 0.0104
top acc: 0.0397 ::: bot acc: 0.0085
current epoch: 23
train loss is 0.000655
average val loss: 0.000378, accuracy: 0.0239
average test loss: 0.000422, accuracy: 0.0246
case acc: 0.021040386
case acc: 0.043054037
case acc: 0.020046594
case acc: 0.018370088
case acc: 0.022278417
case acc: 0.022717394
top acc: 0.0342 ::: bot acc: 0.0087
top acc: 0.0566 ::: bot acc: 0.0308
top acc: 0.0369 ::: bot acc: 0.0072
top acc: 0.0313 ::: bot acc: 0.0084
top acc: 0.0365 ::: bot acc: 0.0090
top acc: 0.0390 ::: bot acc: 0.0076
current epoch: 24
train loss is 0.000604
average val loss: 0.000352, accuracy: 0.0231
average test loss: 0.000399, accuracy: 0.0238
case acc: 0.021150757
case acc: 0.04085755
case acc: 0.020104209
case acc: 0.018121993
case acc: 0.021114483
case acc: 0.021734368
top acc: 0.0345 ::: bot acc: 0.0087
top acc: 0.0544 ::: bot acc: 0.0287
top acc: 0.0373 ::: bot acc: 0.0067
top acc: 0.0312 ::: bot acc: 0.0077
top acc: 0.0351 ::: bot acc: 0.0085
top acc: 0.0380 ::: bot acc: 0.0070
current epoch: 25
train loss is 0.000599
average val loss: 0.000428, accuracy: 0.0261
average test loss: 0.000476, accuracy: 0.0267
case acc: 0.024738556
case acc: 0.04303623
case acc: 0.02306806
case acc: 0.021050675
case acc: 0.023755627
case acc: 0.024368944
top acc: 0.0388 ::: bot acc: 0.0111
top acc: 0.0569 ::: bot acc: 0.0308
top acc: 0.0419 ::: bot acc: 0.0063
top acc: 0.0352 ::: bot acc: 0.0089
top acc: 0.0377 ::: bot acc: 0.0110
top acc: 0.0408 ::: bot acc: 0.0089
current epoch: 26
train loss is 0.000618
average val loss: 0.000477, accuracy: 0.0280
average test loss: 0.000522, accuracy: 0.0284
case acc: 0.027163278
case acc: 0.043852046
case acc: 0.025484681
case acc: 0.022951135
case acc: 0.02500954
case acc: 0.025678033
top acc: 0.0411 ::: bot acc: 0.0136
top acc: 0.0573 ::: bot acc: 0.0318
top acc: 0.0448 ::: bot acc: 0.0077
top acc: 0.0373 ::: bot acc: 0.0100
top acc: 0.0391 ::: bot acc: 0.0118
top acc: 0.0424 ::: bot acc: 0.0099
current epoch: 27
train loss is 0.000601
average val loss: 0.000448, accuracy: 0.0270
average test loss: 0.000492, accuracy: 0.0274
case acc: 0.026867436
case acc: 0.041686192
case acc: 0.02547917
case acc: 0.02229707
case acc: 0.024063425
case acc: 0.02394308
top acc: 0.0408 ::: bot acc: 0.0131
top acc: 0.0555 ::: bot acc: 0.0296
top acc: 0.0445 ::: bot acc: 0.0077
top acc: 0.0365 ::: bot acc: 0.0094
top acc: 0.0380 ::: bot acc: 0.0112
top acc: 0.0406 ::: bot acc: 0.0083
current epoch: 28
train loss is 0.000570
average val loss: 0.000437, accuracy: 0.0268
average test loss: 0.000482, accuracy: 0.0272
case acc: 0.02696962
case acc: 0.040496156
case acc: 0.02617239
case acc: 0.022340234
case acc: 0.023982016
case acc: 0.023131885
top acc: 0.0407 ::: bot acc: 0.0135
top acc: 0.0542 ::: bot acc: 0.0284
top acc: 0.0456 ::: bot acc: 0.0082
top acc: 0.0367 ::: bot acc: 0.0095
top acc: 0.0381 ::: bot acc: 0.0109
top acc: 0.0395 ::: bot acc: 0.0078
current epoch: 29
train loss is 0.000573
average val loss: 0.000496, accuracy: 0.0288
average test loss: 0.000539, accuracy: 0.0291
case acc: 0.029399501
case acc: 0.041807186
case acc: 0.028672019
case acc: 0.024171056
case acc: 0.025927948
case acc: 0.024523681
top acc: 0.0434 ::: bot acc: 0.0155
top acc: 0.0553 ::: bot acc: 0.0297
top acc: 0.0487 ::: bot acc: 0.0097
top acc: 0.0386 ::: bot acc: 0.0108
top acc: 0.0402 ::: bot acc: 0.0125
top acc: 0.0410 ::: bot acc: 0.0089
current epoch: 30
train loss is 0.000557
average val loss: 0.000537, accuracy: 0.0303
average test loss: 0.000581, accuracy: 0.0305
case acc: 0.031132333
case acc: 0.04243918
case acc: 0.030761318
case acc: 0.025527244
case acc: 0.027620686
case acc: 0.025265807
top acc: 0.0451 ::: bot acc: 0.0170
top acc: 0.0560 ::: bot acc: 0.0303
top acc: 0.0510 ::: bot acc: 0.0114
top acc: 0.0404 ::: bot acc: 0.0115
top acc: 0.0419 ::: bot acc: 0.0143
top acc: 0.0420 ::: bot acc: 0.0096
current epoch: 31
train loss is 0.000550
average val loss: 0.000521, accuracy: 0.0298
average test loss: 0.000567, accuracy: 0.0301
case acc: 0.030652538
case acc: 0.04125596
case acc: 0.031207519
case acc: 0.025290618
case acc: 0.027309336
case acc: 0.024598714
top acc: 0.0443 ::: bot acc: 0.0168
top acc: 0.0549 ::: bot acc: 0.0293
top acc: 0.0514 ::: bot acc: 0.0118
top acc: 0.0401 ::: bot acc: 0.0114
top acc: 0.0415 ::: bot acc: 0.0142
top acc: 0.0413 ::: bot acc: 0.0088
current epoch: 32
train loss is 0.000544
average val loss: 0.000542, accuracy: 0.0305
average test loss: 0.000585, accuracy: 0.0307
case acc: 0.031672347
case acc: 0.0408604
case acc: 0.032324128
case acc: 0.026313176
case acc: 0.028275894
case acc: 0.024778582
top acc: 0.0459 ::: bot acc: 0.0176
top acc: 0.0542 ::: bot acc: 0.0288
top acc: 0.0528 ::: bot acc: 0.0123
top acc: 0.0413 ::: bot acc: 0.0120
top acc: 0.0425 ::: bot acc: 0.0149
top acc: 0.0410 ::: bot acc: 0.0093
current epoch: 33
train loss is 0.000554
average val loss: 0.000577, accuracy: 0.0316
average test loss: 0.000622, accuracy: 0.0318
case acc: 0.032894272
case acc: 0.041623373
case acc: 0.03400554
case acc: 0.027634075
case acc: 0.029344043
case acc: 0.025480969
top acc: 0.0470 ::: bot acc: 0.0189
top acc: 0.0550 ::: bot acc: 0.0295
top acc: 0.0545 ::: bot acc: 0.0138
top acc: 0.0428 ::: bot acc: 0.0131
top acc: 0.0435 ::: bot acc: 0.0161
top acc: 0.0420 ::: bot acc: 0.0098
current epoch: 34
train loss is 0.000539
average val loss: 0.000580, accuracy: 0.0317
average test loss: 0.000623, accuracy: 0.0319
case acc: 0.032832973
case acc: 0.0409158
case acc: 0.03494722
case acc: 0.0280005
case acc: 0.029172393
case acc: 0.025464017
top acc: 0.0469 ::: bot acc: 0.0187
top acc: 0.0546 ::: bot acc: 0.0289
top acc: 0.0556 ::: bot acc: 0.0147
top acc: 0.0431 ::: bot acc: 0.0135
top acc: 0.0432 ::: bot acc: 0.0160
top acc: 0.0420 ::: bot acc: 0.0100
current epoch: 35
train loss is 0.000548
average val loss: 0.000623, accuracy: 0.0330
average test loss: 0.000665, accuracy: 0.0331
case acc: 0.034013234
case acc: 0.041681834
case acc: 0.03676382
case acc: 0.029349858
case acc: 0.030565515
case acc: 0.026263433
top acc: 0.0482 ::: bot acc: 0.0199
top acc: 0.0552 ::: bot acc: 0.0294
top acc: 0.0574 ::: bot acc: 0.0165
top acc: 0.0448 ::: bot acc: 0.0144
top acc: 0.0445 ::: bot acc: 0.0177
top acc: 0.0428 ::: bot acc: 0.0103
current epoch: 36
train loss is 0.000547
average val loss: 0.000643, accuracy: 0.0337
average test loss: 0.000690, accuracy: 0.0339
case acc: 0.034523044
case acc: 0.041595854
case acc: 0.038143553
case acc: 0.030604223
case acc: 0.03136612
case acc: 0.026985228
top acc: 0.0487 ::: bot acc: 0.0204
top acc: 0.0552 ::: bot acc: 0.0295
top acc: 0.0588 ::: bot acc: 0.0177
top acc: 0.0461 ::: bot acc: 0.0153
top acc: 0.0454 ::: bot acc: 0.0181
top acc: 0.0437 ::: bot acc: 0.0111
current epoch: 37
train loss is 0.000530
average val loss: 0.000644, accuracy: 0.0338
average test loss: 0.000693, accuracy: 0.0340
case acc: 0.034699727
case acc: 0.04088272
case acc: 0.03824758
case acc: 0.03092789
case acc: 0.032188084
case acc: 0.027024908
top acc: 0.0490 ::: bot acc: 0.0203
top acc: 0.0544 ::: bot acc: 0.0289
top acc: 0.0589 ::: bot acc: 0.0177
top acc: 0.0465 ::: bot acc: 0.0156
top acc: 0.0465 ::: bot acc: 0.0188
top acc: 0.0435 ::: bot acc: 0.0112
current epoch: 38
train loss is 0.000524
average val loss: 0.000625, accuracy: 0.0332
average test loss: 0.000670, accuracy: 0.0333
case acc: 0.033733252
case acc: 0.039179426
case acc: 0.03788287
case acc: 0.030313734
case acc: 0.03247119
case acc: 0.026513226
top acc: 0.0481 ::: bot acc: 0.0195
top acc: 0.0529 ::: bot acc: 0.0269
top acc: 0.0585 ::: bot acc: 0.0175
top acc: 0.0460 ::: bot acc: 0.0151
top acc: 0.0467 ::: bot acc: 0.0191
top acc: 0.0431 ::: bot acc: 0.0106
current epoch: 39
train loss is 0.000510
average val loss: 0.000574, accuracy: 0.0317
average test loss: 0.000621, accuracy: 0.0319
case acc: 0.03190588
case acc: 0.03674642
case acc: 0.036570262
case acc: 0.02939167
case acc: 0.031724364
case acc: 0.025177412
top acc: 0.0462 ::: bot acc: 0.0176
top acc: 0.0503 ::: bot acc: 0.0248
top acc: 0.0569 ::: bot acc: 0.0164
top acc: 0.0449 ::: bot acc: 0.0145
top acc: 0.0460 ::: bot acc: 0.0185
top acc: 0.0417 ::: bot acc: 0.0093
current epoch: 40
train loss is 0.000467
average val loss: 0.000474, accuracy: 0.0284
average test loss: 0.000519, accuracy: 0.0287
case acc: 0.028249884
case acc: 0.032735363
case acc: 0.033341803
case acc: 0.026862347
case acc: 0.028544266
case acc: 0.022454591
top acc: 0.0423 ::: bot acc: 0.0142
top acc: 0.0463 ::: bot acc: 0.0207
top acc: 0.0537 ::: bot acc: 0.0135
top acc: 0.0420 ::: bot acc: 0.0125
top acc: 0.0427 ::: bot acc: 0.0152
top acc: 0.0387 ::: bot acc: 0.0073
current epoch: 41
train loss is 0.000424
average val loss: 0.000425, accuracy: 0.0267
average test loss: 0.000469, accuracy: 0.0270
case acc: 0.026114604
case acc: 0.03023338
case acc: 0.031645685
case acc: 0.025489004
case acc: 0.027354384
case acc: 0.021329071
top acc: 0.0399 ::: bot acc: 0.0126
top acc: 0.0437 ::: bot acc: 0.0183
top acc: 0.0518 ::: bot acc: 0.0121
top acc: 0.0406 ::: bot acc: 0.0114
top acc: 0.0415 ::: bot acc: 0.0138
top acc: 0.0373 ::: bot acc: 0.0068
current epoch: 42
train loss is 0.000406
average val loss: 0.000338, accuracy: 0.0234
average test loss: 0.000383, accuracy: 0.0239
case acc: 0.022770425
case acc: 0.025928997
case acc: 0.028502807
case acc: 0.022920566
case acc: 0.024538914
case acc: 0.01885412
top acc: 0.0365 ::: bot acc: 0.0097
top acc: 0.0393 ::: bot acc: 0.0142
top acc: 0.0482 ::: bot acc: 0.0096
top acc: 0.0372 ::: bot acc: 0.0100
top acc: 0.0387 ::: bot acc: 0.0114
top acc: 0.0344 ::: bot acc: 0.0052
current epoch: 43
train loss is 0.000380
average val loss: 0.000325, accuracy: 0.0229
average test loss: 0.000369, accuracy: 0.0234
case acc: 0.02187619
case acc: 0.024431786
case acc: 0.027754594
case acc: 0.02278072
case acc: 0.024558526
case acc: 0.018881312
top acc: 0.0354 ::: bot acc: 0.0093
top acc: 0.0380 ::: bot acc: 0.0125
top acc: 0.0475 ::: bot acc: 0.0090
top acc: 0.0374 ::: bot acc: 0.0097
top acc: 0.0384 ::: bot acc: 0.0116
top acc: 0.0343 ::: bot acc: 0.0052
current epoch: 44
train loss is 0.000362
average val loss: 0.000257, accuracy: 0.0199
average test loss: 0.000303, accuracy: 0.0207
case acc: 0.018714659
case acc: 0.020354383
case acc: 0.02444334
case acc: 0.020799685
case acc: 0.02241267
case acc: 0.0174255
top acc: 0.0316 ::: bot acc: 0.0070
top acc: 0.0336 ::: bot acc: 0.0090
top acc: 0.0436 ::: bot acc: 0.0070
top acc: 0.0347 ::: bot acc: 0.0089
top acc: 0.0363 ::: bot acc: 0.0093
top acc: 0.0323 ::: bot acc: 0.0051
current epoch: 45
train loss is 0.000337
average val loss: 0.000197, accuracy: 0.0169
average test loss: 0.000243, accuracy: 0.0181
case acc: 0.015909515
case acc: 0.016369501
case acc: 0.021473225
case acc: 0.018534757
case acc: 0.020155875
case acc: 0.016154883
top acc: 0.0280 ::: bot acc: 0.0061
top acc: 0.0290 ::: bot acc: 0.0063
top acc: 0.0396 ::: bot acc: 0.0062
top acc: 0.0316 ::: bot acc: 0.0080
top acc: 0.0339 ::: bot acc: 0.0077
top acc: 0.0300 ::: bot acc: 0.0055
current epoch: 46
train loss is 0.000312
average val loss: 0.000122, accuracy: 0.0126
average test loss: 0.000168, accuracy: 0.0145
case acc: 0.012285017
case acc: 0.011296171
case acc: 0.018176632
case acc: 0.015032923
case acc: 0.016112292
case acc: 0.01390346
top acc: 0.0216 ::: bot acc: 0.0080
top acc: 0.0221 ::: bot acc: 0.0047
top acc: 0.0333 ::: bot acc: 0.0089
top acc: 0.0265 ::: bot acc: 0.0080
top acc: 0.0290 ::: bot acc: 0.0053
top acc: 0.0253 ::: bot acc: 0.0086
current epoch: 47
train loss is 0.000294
average val loss: 0.000096, accuracy: 0.0110
average test loss: 0.000142, accuracy: 0.0131
case acc: 0.011055198
case acc: 0.00972453
case acc: 0.016798725
case acc: 0.013534794
case acc: 0.014484781
case acc: 0.013130245
top acc: 0.0184 ::: bot acc: 0.0107
top acc: 0.0183 ::: bot acc: 0.0075
top acc: 0.0292 ::: bot acc: 0.0126
top acc: 0.0238 ::: bot acc: 0.0090
top acc: 0.0270 ::: bot acc: 0.0047
top acc: 0.0233 ::: bot acc: 0.0102
current epoch: 48
train loss is 0.000296
average val loss: 0.000079, accuracy: 0.0098
average test loss: 0.000126, accuracy: 0.0122
case acc: 0.010308111
case acc: 0.009407309
case acc: 0.015685674
case acc: 0.01237771
case acc: 0.012973901
case acc: 0.012682703
top acc: 0.0146 ::: bot acc: 0.0148
top acc: 0.0132 ::: bot acc: 0.0124
top acc: 0.0248 ::: bot acc: 0.0174
top acc: 0.0208 ::: bot acc: 0.0114
top acc: 0.0247 ::: bot acc: 0.0049
top acc: 0.0213 ::: bot acc: 0.0123
current epoch: 49
train loss is 0.000301
average val loss: 0.000077, accuracy: 0.0097
average test loss: 0.000122, accuracy: 0.0122
case acc: 0.011497673
case acc: 0.011376118
case acc: 0.015243837
case acc: 0.0114338705
case acc: 0.011225953
case acc: 0.012344982
top acc: 0.0096 ::: bot acc: 0.0200
top acc: 0.0078 ::: bot acc: 0.0185
top acc: 0.0183 ::: bot acc: 0.0237
top acc: 0.0160 ::: bot acc: 0.0157
top acc: 0.0210 ::: bot acc: 0.0066
top acc: 0.0179 ::: bot acc: 0.0158
current epoch: 50
train loss is 0.000318
average val loss: 0.000104, accuracy: 0.0114
average test loss: 0.000149, accuracy: 0.0138
case acc: 0.014599563
case acc: 0.01598785
case acc: 0.017377786
case acc: 0.012149557
case acc: 0.010129276
case acc: 0.012441377
top acc: 0.0073 ::: bot acc: 0.0258
top acc: 0.0072 ::: bot acc: 0.0257
top acc: 0.0111 ::: bot acc: 0.0313
top acc: 0.0111 ::: bot acc: 0.0208
top acc: 0.0176 ::: bot acc: 0.0098
top acc: 0.0144 ::: bot acc: 0.0192
LME_Co_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6792 6792 6792
1.7082474 -0.6288155 0.08104724 -0.08406281
Validation: 756 756 756
Testing: 774 774 774
pre-processing time: 0.0002028942108154297
the split date is 2010-07-01
train dropout: 0.6 test dropout: 0.15
net initializing with time: 0.002279043197631836
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012512
average val loss: 0.005367, accuracy: 0.0987
average test loss: 0.005447, accuracy: 0.0989
case acc: 0.14062397
case acc: 0.08330007
case acc: 0.09737719
case acc: 0.08942369
case acc: 0.12824152
case acc: 0.054615907
top acc: 0.1245 ::: bot acc: 0.1570
top acc: 0.0968 ::: bot acc: 0.0705
top acc: 0.0740 ::: bot acc: 0.1212
top acc: 0.0723 ::: bot acc: 0.1059
top acc: 0.1084 ::: bot acc: 0.1463
top acc: 0.0366 ::: bot acc: 0.0709
current epoch: 2
train loss is 0.007901
average val loss: 0.002740, accuracy: 0.0505
average test loss: 0.002789, accuracy: 0.0511
case acc: 0.041035023
case acc: 0.16605008
case acc: 0.01693436
case acc: 0.01229707
case acc: 0.04317016
case acc: 0.027230578
top acc: 0.0254 ::: bot acc: 0.0564
top acc: 0.1798 ::: bot acc: 0.1527
top acc: 0.0239 ::: bot acc: 0.0223
top acc: 0.0187 ::: bot acc: 0.0141
top acc: 0.0245 ::: bot acc: 0.0605
top acc: 0.0443 ::: bot acc: 0.0111
current epoch: 3
train loss is 0.008272
average val loss: 0.007544, accuracy: 0.1027
average test loss: 0.007603, accuracy: 0.1026
case acc: 0.05102018
case acc: 0.24344783
case acc: 0.09341938
case acc: 0.08580505
case acc: 0.0392653
case acc: 0.10272155
top acc: 0.0672 ::: bot acc: 0.0356
top acc: 0.2570 ::: bot acc: 0.2308
top acc: 0.1175 ::: bot acc: 0.0689
top acc: 0.1032 ::: bot acc: 0.0700
top acc: 0.0587 ::: bot acc: 0.0211
top acc: 0.1199 ::: bot acc: 0.0865
current epoch: 4
train loss is 0.009819
average val loss: 0.016408, accuracy: 0.1700
average test loss: 0.016435, accuracy: 0.1699
case acc: 0.12576868
case acc: 0.30402082
case acc: 0.16710378
case acc: 0.15212259
case acc: 0.10864648
case acc: 0.1616637
top acc: 0.1419 ::: bot acc: 0.1100
top acc: 0.3179 ::: bot acc: 0.2908
top acc: 0.1899 ::: bot acc: 0.1440
top acc: 0.1691 ::: bot acc: 0.1360
top acc: 0.1291 ::: bot acc: 0.0899
top acc: 0.1787 ::: bot acc: 0.1454
current epoch: 5
train loss is 0.012816
average val loss: 0.012981, accuracy: 0.1502
average test loss: 0.013026, accuracy: 0.1502
case acc: 0.11126145
case acc: 0.27445015
case acc: 0.15033542
case acc: 0.13212927
case acc: 0.09622428
case acc: 0.13681331
top acc: 0.1273 ::: bot acc: 0.0956
top acc: 0.2880 ::: bot acc: 0.2615
top acc: 0.1724 ::: bot acc: 0.1270
top acc: 0.1494 ::: bot acc: 0.1165
top acc: 0.1161 ::: bot acc: 0.0779
top acc: 0.1537 ::: bot acc: 0.1210
current epoch: 6
train loss is 0.012546
average val loss: 0.002913, accuracy: 0.0551
average test loss: 0.002962, accuracy: 0.0560
case acc: 0.021293962
case acc: 0.16581883
case acc: 0.053869348
case acc: 0.036187876
case acc: 0.017218512
case acc: 0.04134397
top acc: 0.0359 ::: bot acc: 0.0082
top acc: 0.1792 ::: bot acc: 0.1530
top acc: 0.0766 ::: bot acc: 0.0318
top acc: 0.0533 ::: bot acc: 0.0208
top acc: 0.0325 ::: bot acc: 0.0077
top acc: 0.0586 ::: bot acc: 0.0252
current epoch: 7
train loss is 0.006966
average val loss: 0.001093, accuracy: 0.0358
average test loss: 0.001138, accuracy: 0.0367
case acc: 0.034070328
case acc: 0.09559227
case acc: 0.018396325
case acc: 0.023134775
case acc: 0.03275241
case acc: 0.016532259
top acc: 0.0189 ::: bot acc: 0.0494
top acc: 0.1086 ::: bot acc: 0.0832
top acc: 0.0153 ::: bot acc: 0.0323
top acc: 0.0115 ::: bot acc: 0.0364
top acc: 0.0160 ::: bot acc: 0.0498
top acc: 0.0078 ::: bot acc: 0.0285
current epoch: 8
train loss is 0.003088
average val loss: 0.000925, accuracy: 0.0289
average test loss: 0.000953, accuracy: 0.0302
case acc: 0.022961741
case acc: 0.09473047
case acc: 0.017076112
case acc: 0.015618638
case acc: 0.018397719
case acc: 0.0123130595
top acc: 0.0105 ::: bot acc: 0.0364
top acc: 0.1083 ::: bot acc: 0.0820
top acc: 0.0213 ::: bot acc: 0.0255
top acc: 0.0109 ::: bot acc: 0.0253
top acc: 0.0106 ::: bot acc: 0.0310
top acc: 0.0183 ::: bot acc: 0.0149
current epoch: 9
train loss is 0.002196
average val loss: 0.000998, accuracy: 0.0287
average test loss: 0.001035, accuracy: 0.0298
case acc: 0.012192902
case acc: 0.10059465
case acc: 0.01911766
case acc: 0.012927673
case acc: 0.015052005
case acc: 0.019173112
top acc: 0.0125 ::: bot acc: 0.0198
top acc: 0.1141 ::: bot acc: 0.0878
top acc: 0.0332 ::: bot acc: 0.0134
top acc: 0.0235 ::: bot acc: 0.0105
top acc: 0.0281 ::: bot acc: 0.0106
top acc: 0.0349 ::: bot acc: 0.0063
current epoch: 10
train loss is 0.002078
average val loss: 0.000903, accuracy: 0.0282
average test loss: 0.000949, accuracy: 0.0294
case acc: 0.01175072
case acc: 0.09435523
case acc: 0.019246021
case acc: 0.013545283
case acc: 0.017126912
case acc: 0.020413248
top acc: 0.0155 ::: bot acc: 0.0167
top acc: 0.1087 ::: bot acc: 0.0813
top acc: 0.0328 ::: bot acc: 0.0145
top acc: 0.0254 ::: bot acc: 0.0092
top acc: 0.0327 ::: bot acc: 0.0073
top acc: 0.0365 ::: bot acc: 0.0066
current epoch: 11
train loss is 0.001899
average val loss: 0.000765, accuracy: 0.0266
average test loss: 0.000803, accuracy: 0.0275
case acc: 0.011680464
case acc: 0.085069925
case acc: 0.018289773
case acc: 0.012761829
case acc: 0.017948601
case acc: 0.019525265
top acc: 0.0155 ::: bot acc: 0.0165
top acc: 0.0986 ::: bot acc: 0.0724
top acc: 0.0296 ::: bot acc: 0.0173
top acc: 0.0241 ::: bot acc: 0.0095
top acc: 0.0344 ::: bot acc: 0.0070
top acc: 0.0354 ::: bot acc: 0.0057
current epoch: 12
train loss is 0.001627
average val loss: 0.000699, accuracy: 0.0264
average test loss: 0.000737, accuracy: 0.0272
case acc: 0.011823586
case acc: 0.07907099
case acc: 0.017981222
case acc: 0.013309663
case acc: 0.02011886
case acc: 0.021067446
top acc: 0.0177 ::: bot acc: 0.0146
top acc: 0.0928 ::: bot acc: 0.0657
top acc: 0.0292 ::: bot acc: 0.0175
top acc: 0.0254 ::: bot acc: 0.0085
top acc: 0.0377 ::: bot acc: 0.0069
top acc: 0.0373 ::: bot acc: 0.0069
current epoch: 13
train loss is 0.001400
average val loss: 0.000610, accuracy: 0.0252
average test loss: 0.000641, accuracy: 0.0258
case acc: 0.011799957
case acc: 0.071689874
case acc: 0.01753605
case acc: 0.013108455
case acc: 0.020647436
case acc: 0.020311873
top acc: 0.0184 ::: bot acc: 0.0138
top acc: 0.0854 ::: bot acc: 0.0589
top acc: 0.0271 ::: bot acc: 0.0194
top acc: 0.0248 ::: bot acc: 0.0091
top acc: 0.0385 ::: bot acc: 0.0067
top acc: 0.0365 ::: bot acc: 0.0062
current epoch: 14
train loss is 0.001198
average val loss: 0.000525, accuracy: 0.0239
average test loss: 0.000562, accuracy: 0.0246
case acc: 0.011529996
case acc: 0.06476694
case acc: 0.017303532
case acc: 0.013124984
case acc: 0.020796396
case acc: 0.020283012
top acc: 0.0186 ::: bot acc: 0.0128
top acc: 0.0789 ::: bot acc: 0.0513
top acc: 0.0260 ::: bot acc: 0.0212
top acc: 0.0245 ::: bot acc: 0.0097
top acc: 0.0387 ::: bot acc: 0.0070
top acc: 0.0360 ::: bot acc: 0.0063
current epoch: 15
train loss is 0.001026
average val loss: 0.000477, accuracy: 0.0232
average test loss: 0.000509, accuracy: 0.0238
case acc: 0.0119562335
case acc: 0.05963377
case acc: 0.017102607
case acc: 0.013329337
case acc: 0.020757334
case acc: 0.0201956
top acc: 0.0202 ::: bot acc: 0.0118
top acc: 0.0730 ::: bot acc: 0.0468
top acc: 0.0251 ::: bot acc: 0.0215
top acc: 0.0250 ::: bot acc: 0.0091
top acc: 0.0385 ::: bot acc: 0.0070
top acc: 0.0361 ::: bot acc: 0.0061
current epoch: 16
train loss is 0.000943
average val loss: 0.000485, accuracy: 0.0242
average test loss: 0.000522, accuracy: 0.0248
case acc: 0.01326231
case acc: 0.057886913
case acc: 0.017611176
case acc: 0.014550396
case acc: 0.02304515
case acc: 0.022555832
top acc: 0.0244 ::: bot acc: 0.0081
top acc: 0.0716 ::: bot acc: 0.0448
top acc: 0.0279 ::: bot acc: 0.0187
top acc: 0.0283 ::: bot acc: 0.0066
top acc: 0.0414 ::: bot acc: 0.0081
top acc: 0.0392 ::: bot acc: 0.0077
current epoch: 17
train loss is 0.000883
average val loss: 0.000491, accuracy: 0.0250
average test loss: 0.000527, accuracy: 0.0255
case acc: 0.01500037
case acc: 0.056017153
case acc: 0.018072417
case acc: 0.015720628
case acc: 0.024477005
case acc: 0.023975672
top acc: 0.0272 ::: bot acc: 0.0070
top acc: 0.0698 ::: bot acc: 0.0431
top acc: 0.0299 ::: bot acc: 0.0162
top acc: 0.0301 ::: bot acc: 0.0060
top acc: 0.0427 ::: bot acc: 0.0087
top acc: 0.0408 ::: bot acc: 0.0085
current epoch: 18
train loss is 0.000803
average val loss: 0.000461, accuracy: 0.0245
average test loss: 0.000498, accuracy: 0.0250
case acc: 0.015541703
case acc: 0.052539643
case acc: 0.018403232
case acc: 0.016383396
case acc: 0.023692064
case acc: 0.023610005
top acc: 0.0285 ::: bot acc: 0.0064
top acc: 0.0666 ::: bot acc: 0.0397
top acc: 0.0310 ::: bot acc: 0.0156
top acc: 0.0315 ::: bot acc: 0.0057
top acc: 0.0421 ::: bot acc: 0.0085
top acc: 0.0407 ::: bot acc: 0.0082
current epoch: 19
train loss is 0.000765
average val loss: 0.000431, accuracy: 0.0239
average test loss: 0.000468, accuracy: 0.0245
case acc: 0.016096609
case acc: 0.049362656
case acc: 0.018868746
case acc: 0.016627032
case acc: 0.02294715
case acc: 0.023052542
top acc: 0.0293 ::: bot acc: 0.0063
top acc: 0.0630 ::: bot acc: 0.0366
top acc: 0.0318 ::: bot acc: 0.0156
top acc: 0.0317 ::: bot acc: 0.0063
top acc: 0.0415 ::: bot acc: 0.0081
top acc: 0.0397 ::: bot acc: 0.0079
current epoch: 20
train loss is 0.000699
average val loss: 0.000428, accuracy: 0.0241
average test loss: 0.000462, accuracy: 0.0246
case acc: 0.017375754
case acc: 0.04750863
case acc: 0.019106207
case acc: 0.017368648
case acc: 0.02287171
case acc: 0.02324127
top acc: 0.0311 ::: bot acc: 0.0065
top acc: 0.0612 ::: bot acc: 0.0347
top acc: 0.0334 ::: bot acc: 0.0132
top acc: 0.0326 ::: bot acc: 0.0061
top acc: 0.0416 ::: bot acc: 0.0080
top acc: 0.0398 ::: bot acc: 0.0084
current epoch: 21
train loss is 0.000689
average val loss: 0.000425, accuracy: 0.0243
average test loss: 0.000460, accuracy: 0.0247
case acc: 0.018321
case acc: 0.04589758
case acc: 0.019648049
case acc: 0.018275088
case acc: 0.02255821
case acc: 0.023310507
top acc: 0.0324 ::: bot acc: 0.0064
top acc: 0.0599 ::: bot acc: 0.0328
top acc: 0.0349 ::: bot acc: 0.0120
top acc: 0.0339 ::: bot acc: 0.0064
top acc: 0.0410 ::: bot acc: 0.0075
top acc: 0.0403 ::: bot acc: 0.0079
current epoch: 22
train loss is 0.000650
average val loss: 0.000477, accuracy: 0.0263
average test loss: 0.000512, accuracy: 0.0268
case acc: 0.02163159
case acc: 0.046630364
case acc: 0.021641882
case acc: 0.021066068
case acc: 0.024255652
case acc: 0.025288116
top acc: 0.0362 ::: bot acc: 0.0085
top acc: 0.0603 ::: bot acc: 0.0337
top acc: 0.0390 ::: bot acc: 0.0100
top acc: 0.0372 ::: bot acc: 0.0081
top acc: 0.0431 ::: bot acc: 0.0082
top acc: 0.0421 ::: bot acc: 0.0096
current epoch: 23
train loss is 0.000647
average val loss: 0.000500, accuracy: 0.0273
average test loss: 0.000532, accuracy: 0.0276
case acc: 0.023307443
case acc: 0.046028994
case acc: 0.023113882
case acc: 0.022395533
case acc: 0.024883728
case acc: 0.025838057
top acc: 0.0386 ::: bot acc: 0.0090
top acc: 0.0594 ::: bot acc: 0.0331
top acc: 0.0413 ::: bot acc: 0.0096
top acc: 0.0391 ::: bot acc: 0.0088
top acc: 0.0438 ::: bot acc: 0.0088
top acc: 0.0425 ::: bot acc: 0.0104
current epoch: 24
train loss is 0.000603
average val loss: 0.000445, accuracy: 0.0255
average test loss: 0.000482, accuracy: 0.0260
case acc: 0.022571275
case acc: 0.04290185
case acc: 0.022812277
case acc: 0.02102279
case acc: 0.022966316
case acc: 0.023528734
top acc: 0.0379 ::: bot acc: 0.0086
top acc: 0.0568 ::: bot acc: 0.0298
top acc: 0.0410 ::: bot acc: 0.0096
top acc: 0.0375 ::: bot acc: 0.0076
top acc: 0.0417 ::: bot acc: 0.0078
top acc: 0.0403 ::: bot acc: 0.0084
current epoch: 25
train loss is 0.000572
average val loss: 0.000496, accuracy: 0.0273
average test loss: 0.000526, accuracy: 0.0276
case acc: 0.02519681
case acc: 0.043838084
case acc: 0.025135307
case acc: 0.022984978
case acc: 0.024065442
case acc: 0.024402017
top acc: 0.0407 ::: bot acc: 0.0108
top acc: 0.0574 ::: bot acc: 0.0310
top acc: 0.0443 ::: bot acc: 0.0095
top acc: 0.0396 ::: bot acc: 0.0092
top acc: 0.0427 ::: bot acc: 0.0084
top acc: 0.0414 ::: bot acc: 0.0088
current epoch: 26
train loss is 0.000566
average val loss: 0.000500, accuracy: 0.0276
average test loss: 0.000535, accuracy: 0.0280
case acc: 0.026082387
case acc: 0.043349065
case acc: 0.026273115
case acc: 0.023881396
case acc: 0.023952905
case acc: 0.024205014
top acc: 0.0416 ::: bot acc: 0.0115
top acc: 0.0568 ::: bot acc: 0.0304
top acc: 0.0460 ::: bot acc: 0.0098
top acc: 0.0406 ::: bot acc: 0.0100
top acc: 0.0427 ::: bot acc: 0.0083
top acc: 0.0412 ::: bot acc: 0.0087
current epoch: 27
train loss is 0.000566
average val loss: 0.000557, accuracy: 0.0295
average test loss: 0.000588, accuracy: 0.0297
case acc: 0.028448747
case acc: 0.044388194
case acc: 0.028804287
case acc: 0.026281156
case acc: 0.025158713
case acc: 0.02538378
top acc: 0.0444 ::: bot acc: 0.0133
top acc: 0.0582 ::: bot acc: 0.0313
top acc: 0.0494 ::: bot acc: 0.0108
top acc: 0.0430 ::: bot acc: 0.0121
top acc: 0.0441 ::: bot acc: 0.0090
top acc: 0.0420 ::: bot acc: 0.0099
current epoch: 28
train loss is 0.000585
average val loss: 0.000634, accuracy: 0.0320
average test loss: 0.000666, accuracy: 0.0322
case acc: 0.031507898
case acc: 0.046224255
case acc: 0.032060847
case acc: 0.02887012
case acc: 0.02716514
case acc: 0.02754941
top acc: 0.0472 ::: bot acc: 0.0165
top acc: 0.0600 ::: bot acc: 0.0333
top acc: 0.0528 ::: bot acc: 0.0131
top acc: 0.0457 ::: bot acc: 0.0143
top acc: 0.0463 ::: bot acc: 0.0107
top acc: 0.0447 ::: bot acc: 0.0115
current epoch: 29
train loss is 0.000579
average val loss: 0.000729, accuracy: 0.0348
average test loss: 0.000762, accuracy: 0.0349
case acc: 0.0347564
case acc: 0.048354793
case acc: 0.0354316
case acc: 0.031736184
case acc: 0.029731732
case acc: 0.029575448
top acc: 0.0508 ::: bot acc: 0.0193
top acc: 0.0622 ::: bot acc: 0.0353
top acc: 0.0569 ::: bot acc: 0.0152
top acc: 0.0491 ::: bot acc: 0.0165
top acc: 0.0493 ::: bot acc: 0.0125
top acc: 0.0468 ::: bot acc: 0.0134
current epoch: 30
train loss is 0.000608
average val loss: 0.000759, accuracy: 0.0356
average test loss: 0.000791, accuracy: 0.0358
case acc: 0.035816673
case acc: 0.048360657
case acc: 0.037091207
case acc: 0.032737754
case acc: 0.03087765
case acc: 0.029754333
top acc: 0.0520 ::: bot acc: 0.0202
top acc: 0.0623 ::: bot acc: 0.0355
top acc: 0.0588 ::: bot acc: 0.0166
top acc: 0.0498 ::: bot acc: 0.0179
top acc: 0.0504 ::: bot acc: 0.0133
top acc: 0.0468 ::: bot acc: 0.0137
current epoch: 31
train loss is 0.000601
average val loss: 0.000781, accuracy: 0.0363
average test loss: 0.000820, accuracy: 0.0366
case acc: 0.03679361
case acc: 0.048251066
case acc: 0.038740896
case acc: 0.03379838
case acc: 0.031967297
case acc: 0.03001221
top acc: 0.0532 ::: bot acc: 0.0212
top acc: 0.0617 ::: bot acc: 0.0354
top acc: 0.0605 ::: bot acc: 0.0182
top acc: 0.0512 ::: bot acc: 0.0187
top acc: 0.0518 ::: bot acc: 0.0143
top acc: 0.0471 ::: bot acc: 0.0137
current epoch: 32
train loss is 0.000610
average val loss: 0.000821, accuracy: 0.0373
average test loss: 0.000851, accuracy: 0.0374
case acc: 0.037725758
case acc: 0.04828321
case acc: 0.04030841
case acc: 0.034848236
case acc: 0.033085056
case acc: 0.030331245
top acc: 0.0538 ::: bot acc: 0.0222
top acc: 0.0618 ::: bot acc: 0.0354
top acc: 0.0620 ::: bot acc: 0.0195
top acc: 0.0522 ::: bot acc: 0.0196
top acc: 0.0530 ::: bot acc: 0.0151
top acc: 0.0473 ::: bot acc: 0.0142
current epoch: 33
train loss is 0.000599
average val loss: 0.000858, accuracy: 0.0383
average test loss: 0.000888, accuracy: 0.0384
case acc: 0.038655326
case acc: 0.048269812
case acc: 0.041756645
case acc: 0.036281902
case acc: 0.03428709
case acc: 0.031259947
top acc: 0.0547 ::: bot acc: 0.0230
top acc: 0.0621 ::: bot acc: 0.0352
top acc: 0.0637 ::: bot acc: 0.0210
top acc: 0.0537 ::: bot acc: 0.0211
top acc: 0.0541 ::: bot acc: 0.0163
top acc: 0.0483 ::: bot acc: 0.0151
current epoch: 34
train loss is 0.000601
average val loss: 0.000847, accuracy: 0.0381
average test loss: 0.000881, accuracy: 0.0383
case acc: 0.0385523
case acc: 0.047212616
case acc: 0.041905914
case acc: 0.036295757
case acc: 0.034777943
case acc: 0.030915983
top acc: 0.0545 ::: bot acc: 0.0230
top acc: 0.0607 ::: bot acc: 0.0344
top acc: 0.0637 ::: bot acc: 0.0213
top acc: 0.0538 ::: bot acc: 0.0209
top acc: 0.0549 ::: bot acc: 0.0165
top acc: 0.0482 ::: bot acc: 0.0148
current epoch: 35
train loss is 0.000585
average val loss: 0.000798, accuracy: 0.0368
average test loss: 0.000831, accuracy: 0.0370
case acc: 0.03697407
case acc: 0.04472846
case acc: 0.041261997
case acc: 0.03538443
case acc: 0.033998955
case acc: 0.02969773
top acc: 0.0532 ::: bot acc: 0.0214
top acc: 0.0582 ::: bot acc: 0.0319
top acc: 0.0632 ::: bot acc: 0.0204
top acc: 0.0527 ::: bot acc: 0.0201
top acc: 0.0540 ::: bot acc: 0.0159
top acc: 0.0468 ::: bot acc: 0.0136
current epoch: 36
train loss is 0.000556
average val loss: 0.000775, accuracy: 0.0363
average test loss: 0.000808, accuracy: 0.0364
case acc: 0.035903346
case acc: 0.04308334
case acc: 0.0406442
case acc: 0.03542294
case acc: 0.033923052
case acc: 0.029423548
top acc: 0.0518 ::: bot acc: 0.0204
top acc: 0.0567 ::: bot acc: 0.0302
top acc: 0.0626 ::: bot acc: 0.0197
top acc: 0.0528 ::: bot acc: 0.0202
top acc: 0.0540 ::: bot acc: 0.0156
top acc: 0.0464 ::: bot acc: 0.0133
current epoch: 37
train loss is 0.000518
average val loss: 0.000642, accuracy: 0.0326
average test loss: 0.000674, accuracy: 0.0327
case acc: 0.0317872
case acc: 0.038192455
case acc: 0.03698676
case acc: 0.03210189
case acc: 0.030919375
case acc: 0.026464326
top acc: 0.0476 ::: bot acc: 0.0164
top acc: 0.0518 ::: bot acc: 0.0251
top acc: 0.0585 ::: bot acc: 0.0167
top acc: 0.0494 ::: bot acc: 0.0173
top acc: 0.0506 ::: bot acc: 0.0134
top acc: 0.0434 ::: bot acc: 0.0107
current epoch: 38
train loss is 0.000463
average val loss: 0.000525, accuracy: 0.0289
average test loss: 0.000559, accuracy: 0.0292
case acc: 0.028041169
case acc: 0.0331928
case acc: 0.033367246
case acc: 0.028821059
case acc: 0.028267877
case acc: 0.02343993
top acc: 0.0439 ::: bot acc: 0.0132
top acc: 0.0470 ::: bot acc: 0.0201
top acc: 0.0549 ::: bot acc: 0.0139
top acc: 0.0460 ::: bot acc: 0.0142
top acc: 0.0479 ::: bot acc: 0.0111
top acc: 0.0401 ::: bot acc: 0.0081
current epoch: 39
train loss is 0.000418
average val loss: 0.000445, accuracy: 0.0262
average test loss: 0.000477, accuracy: 0.0265
case acc: 0.024796335
case acc: 0.029027658
case acc: 0.030662362
case acc: 0.026557362
case acc: 0.026549228
case acc: 0.021128928
top acc: 0.0404 ::: bot acc: 0.0102
top acc: 0.0428 ::: bot acc: 0.0160
top acc: 0.0514 ::: bot acc: 0.0121
top acc: 0.0434 ::: bot acc: 0.0122
top acc: 0.0458 ::: bot acc: 0.0099
top acc: 0.0374 ::: bot acc: 0.0065
current epoch: 40
train loss is 0.000386
average val loss: 0.000416, accuracy: 0.0252
average test loss: 0.000449, accuracy: 0.0255
case acc: 0.023556352
case acc: 0.027176274
case acc: 0.029509595
case acc: 0.025683407
case acc: 0.026262032
case acc: 0.02083458
top acc: 0.0387 ::: bot acc: 0.0095
top acc: 0.0408 ::: bot acc: 0.0143
top acc: 0.0502 ::: bot acc: 0.0114
top acc: 0.0423 ::: bot acc: 0.0116
top acc: 0.0455 ::: bot acc: 0.0099
top acc: 0.0371 ::: bot acc: 0.0066
current epoch: 41
train loss is 0.000361
average val loss: 0.000335, accuracy: 0.0221
average test loss: 0.000369, accuracy: 0.0225
case acc: 0.020405946
case acc: 0.022567727
case acc: 0.026419062
case acc: 0.023031756
case acc: 0.023679068
case acc: 0.018633455
top acc: 0.0351 ::: bot acc: 0.0074
top acc: 0.0361 ::: bot acc: 0.0098
top acc: 0.0463 ::: bot acc: 0.0098
top acc: 0.0398 ::: bot acc: 0.0093
top acc: 0.0421 ::: bot acc: 0.0083
top acc: 0.0343 ::: bot acc: 0.0054
current epoch: 42
train loss is 0.000325
average val loss: 0.000247, accuracy: 0.0183
average test loss: 0.000284, accuracy: 0.0189
case acc: 0.016482612
case acc: 0.017490495
case acc: 0.023035849
case acc: 0.019415919
case acc: 0.02045984
case acc: 0.016239453
top acc: 0.0297 ::: bot acc: 0.0064
top acc: 0.0307 ::: bot acc: 0.0056
top acc: 0.0414 ::: bot acc: 0.0094
top acc: 0.0354 ::: bot acc: 0.0070
top acc: 0.0383 ::: bot acc: 0.0069
top acc: 0.0307 ::: bot acc: 0.0052
current epoch: 43
train loss is 0.000298
average val loss: 0.000187, accuracy: 0.0155
average test loss: 0.000224, accuracy: 0.0162
case acc: 0.013855142
case acc: 0.013698759
case acc: 0.020349115
case acc: 0.016705591
case acc: 0.018023081
case acc: 0.01444144
top acc: 0.0254 ::: bot acc: 0.0075
top acc: 0.0254 ::: bot acc: 0.0045
top acc: 0.0366 ::: bot acc: 0.0113
top acc: 0.0317 ::: bot acc: 0.0059
top acc: 0.0347 ::: bot acc: 0.0068
top acc: 0.0274 ::: bot acc: 0.0067
current epoch: 44
train loss is 0.000282
average val loss: 0.000159, accuracy: 0.0140
average test loss: 0.000194, accuracy: 0.0148
case acc: 0.0125145335
case acc: 0.011392113
case acc: 0.018829942
case acc: 0.01518717
case acc: 0.0169514
case acc: 0.0137570035
top acc: 0.0223 ::: bot acc: 0.0098
top acc: 0.0215 ::: bot acc: 0.0054
top acc: 0.0330 ::: bot acc: 0.0134
top acc: 0.0295 ::: bot acc: 0.0062
top acc: 0.0328 ::: bot acc: 0.0073
top acc: 0.0259 ::: bot acc: 0.0078
current epoch: 45
train loss is 0.000273
average val loss: 0.000133, accuracy: 0.0126
average test loss: 0.000171, accuracy: 0.0137
case acc: 0.011681107
case acc: 0.010098256
case acc: 0.017942395
case acc: 0.0137621155
case acc: 0.015764369
case acc: 0.012927802
top acc: 0.0185 ::: bot acc: 0.0136
top acc: 0.0174 ::: bot acc: 0.0092
top acc: 0.0289 ::: bot acc: 0.0181
top acc: 0.0265 ::: bot acc: 0.0076
top acc: 0.0302 ::: bot acc: 0.0091
top acc: 0.0235 ::: bot acc: 0.0098
current epoch: 46
train loss is 0.000266
average val loss: 0.000117, accuracy: 0.0118
average test loss: 0.000154, accuracy: 0.0130
case acc: 0.011896
case acc: 0.010307227
case acc: 0.016805686
case acc: 0.01224538
case acc: 0.014383103
case acc: 0.012196533
top acc: 0.0129 ::: bot acc: 0.0189
top acc: 0.0106 ::: bot acc: 0.0160
top acc: 0.0224 ::: bot acc: 0.0242
top acc: 0.0217 ::: bot acc: 0.0120
top acc: 0.0253 ::: bot acc: 0.0137
top acc: 0.0195 ::: bot acc: 0.0139
current epoch: 47
train loss is 0.000273
average val loss: 0.000121, accuracy: 0.0122
average test loss: 0.000158, accuracy: 0.0134
case acc: 0.013424605
case acc: 0.011620851
case acc: 0.017391922
case acc: 0.0121292975
case acc: 0.013867528
case acc: 0.012129444
top acc: 0.0104 ::: bot acc: 0.0227
top acc: 0.0072 ::: bot acc: 0.0198
top acc: 0.0183 ::: bot acc: 0.0284
top acc: 0.0193 ::: bot acc: 0.0144
top acc: 0.0229 ::: bot acc: 0.0158
top acc: 0.0178 ::: bot acc: 0.0156
current epoch: 48
train loss is 0.000287
average val loss: 0.000139, accuracy: 0.0133
average test loss: 0.000178, accuracy: 0.0146
case acc: 0.015474064
case acc: 0.01440742
case acc: 0.018985467
case acc: 0.012616229
case acc: 0.01392772
case acc: 0.012145378
top acc: 0.0093 ::: bot acc: 0.0263
top acc: 0.0054 ::: bot acc: 0.0250
top acc: 0.0134 ::: bot acc: 0.0336
top acc: 0.0161 ::: bot acc: 0.0176
top acc: 0.0208 ::: bot acc: 0.0181
top acc: 0.0157 ::: bot acc: 0.0174
current epoch: 49
train loss is 0.000308
average val loss: 0.000189, accuracy: 0.0157
average test loss: 0.000231, accuracy: 0.0173
case acc: 0.019423231
case acc: 0.019880876
case acc: 0.022316948
case acc: 0.014241623
case acc: 0.014650212
case acc: 0.0130177615
top acc: 0.0098 ::: bot acc: 0.0321
top acc: 0.0084 ::: bot acc: 0.0318
top acc: 0.0094 ::: bot acc: 0.0406
top acc: 0.0116 ::: bot acc: 0.0224
top acc: 0.0170 ::: bot acc: 0.0220
top acc: 0.0127 ::: bot acc: 0.0209
current epoch: 50
train loss is 0.000344
average val loss: 0.000304, accuracy: 0.0206
average test loss: 0.000347, accuracy: 0.0221
case acc: 0.025198596
case acc: 0.027949525
case acc: 0.029355226
case acc: 0.018209677
case acc: 0.016609594
case acc: 0.015147732
top acc: 0.0121 ::: bot acc: 0.0394
top acc: 0.0147 ::: bot acc: 0.0408
top acc: 0.0118 ::: bot acc: 0.0501
top acc: 0.0099 ::: bot acc: 0.0293
top acc: 0.0120 ::: bot acc: 0.0276
top acc: 0.0091 ::: bot acc: 0.0260
LME_Co_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6810 6810 6810
1.7082474 -0.6288155 0.08104724 -0.08406281
Validation: 762 762 762
Testing: 750 750 750
pre-processing time: 0.0002636909484863281
the split date is 2011-01-01
train dropout: 0.6 test dropout: 0.15
net initializing with time: 0.002406597137451172
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012565
average val loss: 0.005172, accuracy: 0.0962
average test loss: 0.004909, accuracy: 0.0941
case acc: 0.1337583
case acc: 0.08910154
case acc: 0.08820506
case acc: 0.08380135
case acc: 0.11929496
case acc: 0.05043932
top acc: 0.1197 ::: bot acc: 0.1474
top acc: 0.1021 ::: bot acc: 0.0760
top acc: 0.0679 ::: bot acc: 0.1094
top acc: 0.0706 ::: bot acc: 0.0968
top acc: 0.0995 ::: bot acc: 0.1399
top acc: 0.0324 ::: bot acc: 0.0680
current epoch: 2
train loss is 0.007716
average val loss: 0.002937, accuracy: 0.0519
average test loss: 0.002938, accuracy: 0.0508
case acc: 0.031640362
case acc: 0.1746162
case acc: 0.019391173
case acc: 0.013853501
case acc: 0.030871402
case acc: 0.03436933
top acc: 0.0183 ::: bot acc: 0.0453
top acc: 0.1863 ::: bot acc: 0.1617
top acc: 0.0335 ::: bot acc: 0.0111
top acc: 0.0240 ::: bot acc: 0.0078
top acc: 0.0123 ::: bot acc: 0.0508
top acc: 0.0519 ::: bot acc: 0.0178
current epoch: 3
train loss is 0.008159
average val loss: 0.008180, accuracy: 0.1084
average test loss: 0.008432, accuracy: 0.1112
case acc: 0.059630394
case acc: 0.2506582
case acc: 0.1037793
case acc: 0.093252696
case acc: 0.050625633
case acc: 0.10924504
top acc: 0.0738 ::: bot acc: 0.0453
top acc: 0.2631 ::: bot acc: 0.2375
top acc: 0.1242 ::: bot acc: 0.0822
top acc: 0.1066 ::: bot acc: 0.0800
top acc: 0.0709 ::: bot acc: 0.0298
top acc: 0.1268 ::: bot acc: 0.0912
current epoch: 4
train loss is 0.010053
average val loss: 0.016804, accuracy: 0.1722
average test loss: 0.017201, accuracy: 0.1748
case acc: 0.13024488
case acc: 0.307631
case acc: 0.17352398
case acc: 0.15607873
case acc: 0.115817346
case acc: 0.16532633
top acc: 0.1449 ::: bot acc: 0.1155
top acc: 0.3196 ::: bot acc: 0.2948
top acc: 0.1945 ::: bot acc: 0.1523
top acc: 0.1693 ::: bot acc: 0.1431
top acc: 0.1363 ::: bot acc: 0.0951
top acc: 0.1826 ::: bot acc: 0.1483
current epoch: 5
train loss is 0.012772
average val loss: 0.013055, accuracy: 0.1505
average test loss: 0.013396, accuracy: 0.1531
case acc: 0.11383976
case acc: 0.27590847
case acc: 0.15456903
case acc: 0.1340085
case acc: 0.10152916
case acc: 0.13854131
top acc: 0.1283 ::: bot acc: 0.0992
top acc: 0.2887 ::: bot acc: 0.2631
top acc: 0.1750 ::: bot acc: 0.1332
top acc: 0.1487 ::: bot acc: 0.1199
top acc: 0.1212 ::: bot acc: 0.0809
top acc: 0.1562 ::: bot acc: 0.1211
current epoch: 6
train loss is 0.012431
average val loss: 0.003111, accuracy: 0.0586
average test loss: 0.003225, accuracy: 0.0609
case acc: 0.025531959
case acc: 0.16998695
case acc: 0.060543913
case acc: 0.04078144
case acc: 0.022742512
case acc: 0.045902945
top acc: 0.0392 ::: bot acc: 0.0121
top acc: 0.1824 ::: bot acc: 0.1572
top acc: 0.0810 ::: bot acc: 0.0399
top acc: 0.0545 ::: bot acc: 0.0281
top acc: 0.0405 ::: bot acc: 0.0066
top acc: 0.0628 ::: bot acc: 0.0289
current epoch: 7
train loss is 0.007197
average val loss: 0.001123, accuracy: 0.0364
average test loss: 0.001080, accuracy: 0.0343
case acc: 0.031130582
case acc: 0.096670784
case acc: 0.015467795
case acc: 0.019508842
case acc: 0.027656518
case acc: 0.015610284
top acc: 0.0169 ::: bot acc: 0.0449
top acc: 0.1088 ::: bot acc: 0.0838
top acc: 0.0162 ::: bot acc: 0.0258
top acc: 0.0089 ::: bot acc: 0.0312
top acc: 0.0103 ::: bot acc: 0.0470
top acc: 0.0082 ::: bot acc: 0.0279
current epoch: 8
train loss is 0.003061
average val loss: 0.000942, accuracy: 0.0302
average test loss: 0.000921, accuracy: 0.0287
case acc: 0.020181542
case acc: 0.09516734
case acc: 0.015529433
case acc: 0.012236678
case acc: 0.015848244
case acc: 0.01309396
top acc: 0.0080 ::: bot acc: 0.0336
top acc: 0.1073 ::: bot acc: 0.0823
top acc: 0.0227 ::: bot acc: 0.0201
top acc: 0.0073 ::: bot acc: 0.0209
top acc: 0.0129 ::: bot acc: 0.0278
top acc: 0.0190 ::: bot acc: 0.0155
current epoch: 9
train loss is 0.002181
average val loss: 0.001108, accuracy: 0.0315
average test loss: 0.001136, accuracy: 0.0325
case acc: 0.010634402
case acc: 0.10476695
case acc: 0.022023277
case acc: 0.013932624
case acc: 0.020564532
case acc: 0.022921136
top acc: 0.0163 ::: bot acc: 0.0125
top acc: 0.1170 ::: bot acc: 0.0921
top acc: 0.0382 ::: bot acc: 0.0102
top acc: 0.0244 ::: bot acc: 0.0074
top acc: 0.0369 ::: bot acc: 0.0077
top acc: 0.0389 ::: bot acc: 0.0082
current epoch: 10
train loss is 0.002060
average val loss: 0.000995, accuracy: 0.0306
average test loss: 0.001041, accuracy: 0.0320
case acc: 0.011089163
case acc: 0.09821416
case acc: 0.021382518
case acc: 0.014275447
case acc: 0.022802353
case acc: 0.023936968
top acc: 0.0184 ::: bot acc: 0.0107
top acc: 0.1107 ::: bot acc: 0.0853
top acc: 0.0370 ::: bot acc: 0.0106
top acc: 0.0250 ::: bot acc: 0.0069
top acc: 0.0405 ::: bot acc: 0.0064
top acc: 0.0401 ::: bot acc: 0.0087
current epoch: 11
train loss is 0.001855
average val loss: 0.000814, accuracy: 0.0280
average test loss: 0.000838, accuracy: 0.0287
case acc: 0.010557533
case acc: 0.08696252
case acc: 0.018578133
case acc: 0.012427731
case acc: 0.02241273
case acc: 0.021306207
top acc: 0.0164 ::: bot acc: 0.0121
top acc: 0.0993 ::: bot acc: 0.0739
top acc: 0.0318 ::: bot acc: 0.0122
top acc: 0.0224 ::: bot acc: 0.0067
top acc: 0.0399 ::: bot acc: 0.0066
top acc: 0.0372 ::: bot acc: 0.0073
current epoch: 12
train loss is 0.001582
average val loss: 0.000781, accuracy: 0.0284
average test loss: 0.000810, accuracy: 0.0295
case acc: 0.011769392
case acc: 0.082335494
case acc: 0.018928224
case acc: 0.014139812
case acc: 0.025839943
case acc: 0.023932487
top acc: 0.0202 ::: bot acc: 0.0092
top acc: 0.0947 ::: bot acc: 0.0695
top acc: 0.0329 ::: bot acc: 0.0115
top acc: 0.0248 ::: bot acc: 0.0065
top acc: 0.0451 ::: bot acc: 0.0071
top acc: 0.0406 ::: bot acc: 0.0087
current epoch: 13
train loss is 0.001378
average val loss: 0.000728, accuracy: 0.0282
average test loss: 0.000758, accuracy: 0.0293
case acc: 0.012992663
case acc: 0.07707444
case acc: 0.018851267
case acc: 0.014978654
case acc: 0.02733216
case acc: 0.02466849
top acc: 0.0230 ::: bot acc: 0.0077
top acc: 0.0895 ::: bot acc: 0.0642
top acc: 0.0328 ::: bot acc: 0.0114
top acc: 0.0264 ::: bot acc: 0.0070
top acc: 0.0473 ::: bot acc: 0.0076
top acc: 0.0410 ::: bot acc: 0.0094
current epoch: 14
train loss is 0.001203
average val loss: 0.000658, accuracy: 0.0273
average test loss: 0.000689, accuracy: 0.0285
case acc: 0.013514316
case acc: 0.071100086
case acc: 0.018353991
case acc: 0.015065515
case acc: 0.028059565
case acc: 0.02506538
top acc: 0.0241 ::: bot acc: 0.0070
top acc: 0.0833 ::: bot acc: 0.0580
top acc: 0.0320 ::: bot acc: 0.0116
top acc: 0.0261 ::: bot acc: 0.0069
top acc: 0.0474 ::: bot acc: 0.0088
top acc: 0.0419 ::: bot acc: 0.0089
current epoch: 15
train loss is 0.001056
average val loss: 0.000604, accuracy: 0.0266
average test loss: 0.000629, accuracy: 0.0276
case acc: 0.013799103
case acc: 0.065897815
case acc: 0.018043673
case acc: 0.015417708
case acc: 0.027815457
case acc: 0.024703713
top acc: 0.0252 ::: bot acc: 0.0062
top acc: 0.0783 ::: bot acc: 0.0528
top acc: 0.0312 ::: bot acc: 0.0117
top acc: 0.0271 ::: bot acc: 0.0071
top acc: 0.0472 ::: bot acc: 0.0084
top acc: 0.0414 ::: bot acc: 0.0090
current epoch: 16
train loss is 0.000960
average val loss: 0.000581, accuracy: 0.0267
average test loss: 0.000612, accuracy: 0.0280
case acc: 0.015554956
case acc: 0.0625872
case acc: 0.018815972
case acc: 0.017040795
case acc: 0.028329814
case acc: 0.025739871
top acc: 0.0273 ::: bot acc: 0.0063
top acc: 0.0745 ::: bot acc: 0.0497
top acc: 0.0329 ::: bot acc: 0.0117
top acc: 0.0289 ::: bot acc: 0.0076
top acc: 0.0475 ::: bot acc: 0.0087
top acc: 0.0424 ::: bot acc: 0.0097
current epoch: 17
train loss is 0.000879
average val loss: 0.000588, accuracy: 0.0274
average test loss: 0.000621, accuracy: 0.0289
case acc: 0.017263887
case acc: 0.06043044
case acc: 0.020204289
case acc: 0.01850677
case acc: 0.0299105
case acc: 0.027034821
top acc: 0.0303 ::: bot acc: 0.0058
top acc: 0.0727 ::: bot acc: 0.0477
top acc: 0.0350 ::: bot acc: 0.0110
top acc: 0.0308 ::: bot acc: 0.0083
top acc: 0.0492 ::: bot acc: 0.0099
top acc: 0.0439 ::: bot acc: 0.0112
current epoch: 18
train loss is 0.000824
average val loss: 0.000520, accuracy: 0.0257
average test loss: 0.000547, accuracy: 0.0271
case acc: 0.017183071
case acc: 0.0553539
case acc: 0.019469503
case acc: 0.017723875
case acc: 0.02754976
case acc: 0.025363369
top acc: 0.0300 ::: bot acc: 0.0064
top acc: 0.0681 ::: bot acc: 0.0422
top acc: 0.0341 ::: bot acc: 0.0109
top acc: 0.0299 ::: bot acc: 0.0078
top acc: 0.0465 ::: bot acc: 0.0079
top acc: 0.0421 ::: bot acc: 0.0095
current epoch: 19
train loss is 0.000742
average val loss: 0.000509, accuracy: 0.0258
average test loss: 0.000541, accuracy: 0.0274
case acc: 0.018370125
case acc: 0.05348895
case acc: 0.020285277
case acc: 0.0187567
case acc: 0.027784161
case acc: 0.025545659
top acc: 0.0315 ::: bot acc: 0.0068
top acc: 0.0658 ::: bot acc: 0.0406
top acc: 0.0352 ::: bot acc: 0.0108
top acc: 0.0307 ::: bot acc: 0.0085
top acc: 0.0467 ::: bot acc: 0.0082
top acc: 0.0419 ::: bot acc: 0.0100
current epoch: 20
train loss is 0.000706
average val loss: 0.000540, accuracy: 0.0271
average test loss: 0.000577, accuracy: 0.0289
case acc: 0.020911891
case acc: 0.052873548
case acc: 0.022526804
case acc: 0.020790078
case acc: 0.02900477
case acc: 0.027053565
top acc: 0.0344 ::: bot acc: 0.0078
top acc: 0.0651 ::: bot acc: 0.0399
top acc: 0.0388 ::: bot acc: 0.0103
top acc: 0.0334 ::: bot acc: 0.0096
top acc: 0.0487 ::: bot acc: 0.0089
top acc: 0.0436 ::: bot acc: 0.0114
current epoch: 21
train loss is 0.000683
average val loss: 0.000515, accuracy: 0.0265
average test loss: 0.000549, accuracy: 0.0283
case acc: 0.021678492
case acc: 0.050186884
case acc: 0.02277736
case acc: 0.021046963
case acc: 0.028052982
case acc: 0.026056461
top acc: 0.0354 ::: bot acc: 0.0085
top acc: 0.0625 ::: bot acc: 0.0372
top acc: 0.0396 ::: bot acc: 0.0101
top acc: 0.0339 ::: bot acc: 0.0095
top acc: 0.0473 ::: bot acc: 0.0085
top acc: 0.0427 ::: bot acc: 0.0102
current epoch: 22
train loss is 0.000649
average val loss: 0.000503, accuracy: 0.0264
average test loss: 0.000536, accuracy: 0.0281
case acc: 0.022606684
case acc: 0.048258435
case acc: 0.023593687
case acc: 0.021323835
case acc: 0.027403908
case acc: 0.025630629
top acc: 0.0364 ::: bot acc: 0.0090
top acc: 0.0602 ::: bot acc: 0.0353
top acc: 0.0402 ::: bot acc: 0.0104
top acc: 0.0341 ::: bot acc: 0.0099
top acc: 0.0468 ::: bot acc: 0.0077
top acc: 0.0424 ::: bot acc: 0.0099
current epoch: 23
train loss is 0.000618
average val loss: 0.000580, accuracy: 0.0290
average test loss: 0.000622, accuracy: 0.0310
case acc: 0.026434563
case acc: 0.04992186
case acc: 0.027029414
case acc: 0.02474103
case acc: 0.029866131
case acc: 0.027760556
top acc: 0.0408 ::: bot acc: 0.0124
top acc: 0.0624 ::: bot acc: 0.0371
top acc: 0.0451 ::: bot acc: 0.0109
top acc: 0.0377 ::: bot acc: 0.0128
top acc: 0.0494 ::: bot acc: 0.0094
top acc: 0.0448 ::: bot acc: 0.0113
current epoch: 24
train loss is 0.000600
average val loss: 0.000566, accuracy: 0.0288
average test loss: 0.000613, accuracy: 0.0309
case acc: 0.027223073
case acc: 0.04825806
case acc: 0.028056415
case acc: 0.02515797
case acc: 0.029319618
case acc: 0.027108742
top acc: 0.0417 ::: bot acc: 0.0130
top acc: 0.0604 ::: bot acc: 0.0353
top acc: 0.0461 ::: bot acc: 0.0119
top acc: 0.0385 ::: bot acc: 0.0130
top acc: 0.0486 ::: bot acc: 0.0094
top acc: 0.0440 ::: bot acc: 0.0107
current epoch: 25
train loss is 0.000600
average val loss: 0.000598, accuracy: 0.0299
average test loss: 0.000647, accuracy: 0.0320
case acc: 0.028970614
case acc: 0.048141714
case acc: 0.029985974
case acc: 0.026770977
case acc: 0.03012487
case acc: 0.027932175
top acc: 0.0435 ::: bot acc: 0.0146
top acc: 0.0604 ::: bot acc: 0.0353
top acc: 0.0488 ::: bot acc: 0.0128
top acc: 0.0402 ::: bot acc: 0.0144
top acc: 0.0500 ::: bot acc: 0.0100
top acc: 0.0450 ::: bot acc: 0.0116
current epoch: 26
train loss is 0.000592
average val loss: 0.000615, accuracy: 0.0306
average test loss: 0.000664, accuracy: 0.0326
case acc: 0.030466426
case acc: 0.047583323
case acc: 0.03137846
case acc: 0.027777849
case acc: 0.03040934
case acc: 0.028096225
top acc: 0.0450 ::: bot acc: 0.0161
top acc: 0.0596 ::: bot acc: 0.0350
top acc: 0.0499 ::: bot acc: 0.0141
top acc: 0.0412 ::: bot acc: 0.0151
top acc: 0.0502 ::: bot acc: 0.0098
top acc: 0.0454 ::: bot acc: 0.0114
current epoch: 27
train loss is 0.000575
average val loss: 0.000698, accuracy: 0.0331
average test loss: 0.000755, accuracy: 0.0353
case acc: 0.03394933
case acc: 0.04928036
case acc: 0.034928113
case acc: 0.030561892
case acc: 0.03302217
case acc: 0.029916912
top acc: 0.0486 ::: bot acc: 0.0193
top acc: 0.0616 ::: bot acc: 0.0362
top acc: 0.0543 ::: bot acc: 0.0167
top acc: 0.0441 ::: bot acc: 0.0178
top acc: 0.0525 ::: bot acc: 0.0125
top acc: 0.0470 ::: bot acc: 0.0132
current epoch: 28
train loss is 0.000588
average val loss: 0.000636, accuracy: 0.0313
average test loss: 0.000689, accuracy: 0.0334
case acc: 0.032381598
case acc: 0.04645169
case acc: 0.03421055
case acc: 0.029146573
case acc: 0.030723466
case acc: 0.027755197
top acc: 0.0470 ::: bot acc: 0.0178
top acc: 0.0588 ::: bot acc: 0.0334
top acc: 0.0533 ::: bot acc: 0.0161
top acc: 0.0429 ::: bot acc: 0.0164
top acc: 0.0504 ::: bot acc: 0.0101
top acc: 0.0446 ::: bot acc: 0.0113
current epoch: 29
train loss is 0.000571
average val loss: 0.000729, accuracy: 0.0341
average test loss: 0.000789, accuracy: 0.0363
case acc: 0.035880256
case acc: 0.0485853
case acc: 0.037998833
case acc: 0.032300398
case acc: 0.033174478
case acc: 0.029879794
top acc: 0.0505 ::: bot acc: 0.0213
top acc: 0.0608 ::: bot acc: 0.0356
top acc: 0.0574 ::: bot acc: 0.0193
top acc: 0.0459 ::: bot acc: 0.0192
top acc: 0.0529 ::: bot acc: 0.0128
top acc: 0.0471 ::: bot acc: 0.0132
current epoch: 30
train loss is 0.000596
average val loss: 0.000800, accuracy: 0.0360
average test loss: 0.000867, accuracy: 0.0384
case acc: 0.03822472
case acc: 0.049893122
case acc: 0.04094889
case acc: 0.034617603
case acc: 0.035194013
case acc: 0.031295273
top acc: 0.0527 ::: bot acc: 0.0239
top acc: 0.0620 ::: bot acc: 0.0372
top acc: 0.0609 ::: bot acc: 0.0216
top acc: 0.0482 ::: bot acc: 0.0214
top acc: 0.0550 ::: bot acc: 0.0147
top acc: 0.0486 ::: bot acc: 0.0143
current epoch: 31
train loss is 0.000590
average val loss: 0.000861, accuracy: 0.0376
average test loss: 0.000933, accuracy: 0.0400
case acc: 0.03996744
case acc: 0.050937817
case acc: 0.04352486
case acc: 0.036462612
case acc: 0.036620446
case acc: 0.032370858
top acc: 0.0543 ::: bot acc: 0.0255
top acc: 0.0631 ::: bot acc: 0.0379
top acc: 0.0635 ::: bot acc: 0.0239
top acc: 0.0503 ::: bot acc: 0.0231
top acc: 0.0563 ::: bot acc: 0.0159
top acc: 0.0498 ::: bot acc: 0.0154
current epoch: 32
train loss is 0.000588
average val loss: 0.000916, accuracy: 0.0391
average test loss: 0.000988, accuracy: 0.0413
case acc: 0.04166792
case acc: 0.051389642
case acc: 0.045600384
case acc: 0.03818213
case acc: 0.038230956
case acc: 0.03300552
top acc: 0.0563 ::: bot acc: 0.0272
top acc: 0.0636 ::: bot acc: 0.0385
top acc: 0.0658 ::: bot acc: 0.0258
top acc: 0.0522 ::: bot acc: 0.0248
top acc: 0.0581 ::: bot acc: 0.0175
top acc: 0.0502 ::: bot acc: 0.0161
current epoch: 33
train loss is 0.000608
average val loss: 0.000996, accuracy: 0.0410
average test loss: 0.001072, accuracy: 0.0433
case acc: 0.04359044
case acc: 0.052798014
case acc: 0.048120063
case acc: 0.04043447
case acc: 0.04064435
case acc: 0.034460332
top acc: 0.0580 ::: bot acc: 0.0290
top acc: 0.0650 ::: bot acc: 0.0400
top acc: 0.0682 ::: bot acc: 0.0281
top acc: 0.0541 ::: bot acc: 0.0269
top acc: 0.0604 ::: bot acc: 0.0199
top acc: 0.0515 ::: bot acc: 0.0177
current epoch: 34
train loss is 0.000615
average val loss: 0.000977, accuracy: 0.0406
average test loss: 0.001056, accuracy: 0.0430
case acc: 0.043203976
case acc: 0.051546887
case acc: 0.0484984
case acc: 0.040165786
case acc: 0.04055664
case acc: 0.033792254
top acc: 0.0576 ::: bot acc: 0.0289
top acc: 0.0639 ::: bot acc: 0.0383
top acc: 0.0687 ::: bot acc: 0.0283
top acc: 0.0541 ::: bot acc: 0.0267
top acc: 0.0602 ::: bot acc: 0.0200
top acc: 0.0510 ::: bot acc: 0.0169
current epoch: 35
train loss is 0.000577
average val loss: 0.000969, accuracy: 0.0404
average test loss: 0.001045, accuracy: 0.0427
case acc: 0.042858124
case acc: 0.05057299
case acc: 0.048794076
case acc: 0.03984704
case acc: 0.041134737
case acc: 0.03309095
top acc: 0.0573 ::: bot acc: 0.0283
top acc: 0.0631 ::: bot acc: 0.0375
top acc: 0.0689 ::: bot acc: 0.0286
top acc: 0.0534 ::: bot acc: 0.0266
top acc: 0.0607 ::: bot acc: 0.0206
top acc: 0.0505 ::: bot acc: 0.0162
current epoch: 36
train loss is 0.000572
average val loss: 0.000936, accuracy: 0.0396
average test loss: 0.001014, accuracy: 0.0420
case acc: 0.041823696
case acc: 0.048696406
case acc: 0.048446495
case acc: 0.0397737
case acc: 0.04101009
case acc: 0.03228321
top acc: 0.0564 ::: bot acc: 0.0273
top acc: 0.0608 ::: bot acc: 0.0359
top acc: 0.0687 ::: bot acc: 0.0285
top acc: 0.0535 ::: bot acc: 0.0264
top acc: 0.0607 ::: bot acc: 0.0203
top acc: 0.0494 ::: bot acc: 0.0152
current epoch: 37
train loss is 0.000562
average val loss: 0.000913, accuracy: 0.0391
average test loss: 0.000992, accuracy: 0.0415
case acc: 0.041180186
case acc: 0.047209643
case acc: 0.047843236
case acc: 0.039647065
case acc: 0.041169953
case acc: 0.032187264
top acc: 0.0553 ::: bot acc: 0.0268
top acc: 0.0596 ::: bot acc: 0.0342
top acc: 0.0684 ::: bot acc: 0.0276
top acc: 0.0536 ::: bot acc: 0.0262
top acc: 0.0608 ::: bot acc: 0.0207
top acc: 0.0493 ::: bot acc: 0.0155
current epoch: 38
train loss is 0.000523
average val loss: 0.000819, accuracy: 0.0367
average test loss: 0.000890, accuracy: 0.0391
case acc: 0.03826896
case acc: 0.04355135
case acc: 0.04564131
case acc: 0.03724058
case acc: 0.03953007
case acc: 0.030172238
top acc: 0.0525 ::: bot acc: 0.0240
top acc: 0.0556 ::: bot acc: 0.0307
top acc: 0.0656 ::: bot acc: 0.0258
top acc: 0.0510 ::: bot acc: 0.0237
top acc: 0.0594 ::: bot acc: 0.0187
top acc: 0.0475 ::: bot acc: 0.0134
current epoch: 39
train loss is 0.000482
average val loss: 0.000766, accuracy: 0.0353
average test loss: 0.000836, accuracy: 0.0377
case acc: 0.036322724
case acc: 0.04083581
case acc: 0.044134498
case acc: 0.036532175
case acc: 0.03913124
case acc: 0.029053535
top acc: 0.0506 ::: bot acc: 0.0220
top acc: 0.0534 ::: bot acc: 0.0279
top acc: 0.0642 ::: bot acc: 0.0245
top acc: 0.0504 ::: bot acc: 0.0232
top acc: 0.0589 ::: bot acc: 0.0183
top acc: 0.0464 ::: bot acc: 0.0123
current epoch: 40
train loss is 0.000443
average val loss: 0.000627, accuracy: 0.0313
average test loss: 0.000683, accuracy: 0.0335
case acc: 0.031619523
case acc: 0.03554703
case acc: 0.039950307
case acc: 0.03269643
case acc: 0.035595633
case acc: 0.025645494
top acc: 0.0461 ::: bot acc: 0.0170
top acc: 0.0478 ::: bot acc: 0.0224
top acc: 0.0598 ::: bot acc: 0.0208
top acc: 0.0462 ::: bot acc: 0.0197
top acc: 0.0553 ::: bot acc: 0.0149
top acc: 0.0424 ::: bot acc: 0.0094
current epoch: 41
train loss is 0.000401
average val loss: 0.000544, accuracy: 0.0287
average test loss: 0.000593, accuracy: 0.0308
case acc: 0.028477605
case acc: 0.031544562
case acc: 0.03700621
case acc: 0.030529926
case acc: 0.033542946
case acc: 0.023789046
top acc: 0.0427 ::: bot acc: 0.0143
top acc: 0.0438 ::: bot acc: 0.0186
top acc: 0.0565 ::: bot acc: 0.0182
top acc: 0.0441 ::: bot acc: 0.0176
top acc: 0.0533 ::: bot acc: 0.0128
top acc: 0.0405 ::: bot acc: 0.0082
current epoch: 42
train loss is 0.000364
average val loss: 0.000414, accuracy: 0.0241
average test loss: 0.000451, accuracy: 0.0261
case acc: 0.023168383
case acc: 0.025381865
case acc: 0.03171896
case acc: 0.026225656
case acc: 0.0294797
case acc: 0.020670615
top acc: 0.0371 ::: bot acc: 0.0094
top acc: 0.0374 ::: bot acc: 0.0126
top acc: 0.0506 ::: bot acc: 0.0141
top acc: 0.0395 ::: bot acc: 0.0138
top acc: 0.0490 ::: bot acc: 0.0093
top acc: 0.0367 ::: bot acc: 0.0065
current epoch: 43
train loss is 0.000322
average val loss: 0.000324, accuracy: 0.0204
average test loss: 0.000346, accuracy: 0.0222
case acc: 0.019111628
case acc: 0.020184161
case acc: 0.02725037
case acc: 0.022612184
case acc: 0.02611375
case acc: 0.01804603
top acc: 0.0323 ::: bot acc: 0.0073
top acc: 0.0320 ::: bot acc: 0.0084
top acc: 0.0458 ::: bot acc: 0.0111
top acc: 0.0356 ::: bot acc: 0.0107
top acc: 0.0451 ::: bot acc: 0.0069
top acc: 0.0330 ::: bot acc: 0.0059
current epoch: 44
train loss is 0.000291
average val loss: 0.000245, accuracy: 0.0170
average test loss: 0.000258, accuracy: 0.0186
case acc: 0.015289474
case acc: 0.015254407
case acc: 0.02322987
case acc: 0.018912857
case acc: 0.023061896
case acc: 0.015899032
top acc: 0.0270 ::: bot acc: 0.0063
top acc: 0.0258 ::: bot acc: 0.0057
top acc: 0.0399 ::: bot acc: 0.0103
top acc: 0.0316 ::: bot acc: 0.0080
top acc: 0.0408 ::: bot acc: 0.0060
top acc: 0.0294 ::: bot acc: 0.0066
current epoch: 45
train loss is 0.000273
average val loss: 0.000188, accuracy: 0.0145
average test loss: 0.000186, accuracy: 0.0153
case acc: 0.011944476
case acc: 0.010957714
case acc: 0.019409943
case acc: 0.015145464
case acc: 0.02015193
case acc: 0.014169282
top acc: 0.0212 ::: bot acc: 0.0083
top acc: 0.0190 ::: bot acc: 0.0064
top acc: 0.0339 ::: bot acc: 0.0113
top acc: 0.0266 ::: bot acc: 0.0067
top acc: 0.0359 ::: bot acc: 0.0071
top acc: 0.0252 ::: bot acc: 0.0094
current epoch: 46
train loss is 0.000269
average val loss: 0.000164, accuracy: 0.0134
average test loss: 0.000152, accuracy: 0.0135
case acc: 0.010519839
case acc: 0.009341085
case acc: 0.016834123
case acc: 0.012781608
case acc: 0.018371923
case acc: 0.013426648
top acc: 0.0165 ::: bot acc: 0.0120
top acc: 0.0140 ::: bot acc: 0.0114
top acc: 0.0285 ::: bot acc: 0.0141
top acc: 0.0227 ::: bot acc: 0.0070
top acc: 0.0329 ::: bot acc: 0.0080
top acc: 0.0226 ::: bot acc: 0.0121
current epoch: 47
train loss is 0.000267
average val loss: 0.000159, accuracy: 0.0133
average test loss: 0.000137, accuracy: 0.0127
case acc: 0.010518478
case acc: 0.009556276
case acc: 0.015361317
case acc: 0.010685381
case acc: 0.017254211
case acc: 0.012982329
top acc: 0.0119 ::: bot acc: 0.0170
top acc: 0.0086 ::: bot acc: 0.0168
top acc: 0.0229 ::: bot acc: 0.0197
top acc: 0.0188 ::: bot acc: 0.0088
top acc: 0.0301 ::: bot acc: 0.0109
top acc: 0.0200 ::: bot acc: 0.0147
current epoch: 48
train loss is 0.000285
average val loss: 0.000185, accuracy: 0.0150
average test loss: 0.000147, accuracy: 0.0134
case acc: 0.012943721
case acc: 0.013016048
case acc: 0.01592007
case acc: 0.009905547
case acc: 0.015670512
case acc: 0.01282285
top acc: 0.0065 ::: bot acc: 0.0233
top acc: 0.0042 ::: bot acc: 0.0245
top acc: 0.0151 ::: bot acc: 0.0275
top acc: 0.0132 ::: bot acc: 0.0142
top acc: 0.0251 ::: bot acc: 0.0156
top acc: 0.0155 ::: bot acc: 0.0190
current epoch: 49
train loss is 0.000321
average val loss: 0.000227, accuracy: 0.0171
average test loss: 0.000178, accuracy: 0.0151
case acc: 0.015751768
case acc: 0.01743074
case acc: 0.018456263
case acc: 0.010790535
case acc: 0.015031515
case acc: 0.013084661
top acc: 0.0058 ::: bot acc: 0.0280
top acc: 0.0060 ::: bot acc: 0.0300
top acc: 0.0106 ::: bot acc: 0.0335
top acc: 0.0097 ::: bot acc: 0.0182
top acc: 0.0221 ::: bot acc: 0.0180
top acc: 0.0135 ::: bot acc: 0.0212
current epoch: 50
train loss is 0.000361
average val loss: 0.000357, accuracy: 0.0224
average test loss: 0.000286, accuracy: 0.0198
case acc: 0.022453211
case acc: 0.026955536
case acc: 0.025076387
case acc: 0.014754047
case acc: 0.014809375
case acc: 0.01481125
top acc: 0.0093 ::: bot acc: 0.0363
top acc: 0.0147 ::: bot acc: 0.0401
top acc: 0.0098 ::: bot acc: 0.0439
top acc: 0.0062 ::: bot acc: 0.0258
top acc: 0.0157 ::: bot acc: 0.0246
top acc: 0.0082 ::: bot acc: 0.0268
LME_Co_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6798 6798 6798
1.7082474 -0.6288155 0.10460696 -0.09017589
Validation: 756 756 756
Testing: 768 768 768
pre-processing time: 0.00022459030151367188
the split date is 2011-07-01
train dropout: 0.6 test dropout: 0.15
net initializing with time: 0.0022966861724853516
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012618
average val loss: 0.005139, accuracy: 0.0964
average test loss: 0.005199, accuracy: 0.0972
case acc: 0.13766274
case acc: 0.08541141
case acc: 0.09376462
case acc: 0.08690877
case acc: 0.12433838
case acc: 0.054831482
top acc: 0.1255 ::: bot acc: 0.1514
top acc: 0.0971 ::: bot acc: 0.0740
top acc: 0.0744 ::: bot acc: 0.1128
top acc: 0.0776 ::: bot acc: 0.0980
top acc: 0.1082 ::: bot acc: 0.1416
top acc: 0.0387 ::: bot acc: 0.0722
current epoch: 2
train loss is 0.007512
average val loss: 0.002850, accuracy: 0.0504
average test loss: 0.002822, accuracy: 0.0499
case acc: 0.036043115
case acc: 0.1705057
case acc: 0.01598877
case acc: 0.010979895
case acc: 0.0354263
case acc: 0.030656343
top acc: 0.0235 ::: bot acc: 0.0495
top acc: 0.1819 ::: bot acc: 0.1590
top acc: 0.0271 ::: bot acc: 0.0136
top acc: 0.0172 ::: bot acc: 0.0082
top acc: 0.0191 ::: bot acc: 0.0525
top acc: 0.0463 ::: bot acc: 0.0156
current epoch: 3
train loss is 0.007851
average val loss: 0.007616, accuracy: 0.1037
average test loss: 0.007575, accuracy: 0.1033
case acc: 0.05189582
case acc: 0.24321783
case acc: 0.094352275
case acc: 0.085429974
case acc: 0.043757536
case acc: 0.10106312
top acc: 0.0645 ::: bot acc: 0.0384
top acc: 0.2546 ::: bot acc: 0.2327
top acc: 0.1143 ::: bot acc: 0.0747
top acc: 0.0950 ::: bot acc: 0.0750
top acc: 0.0593 ::: bot acc: 0.0272
top acc: 0.1180 ::: bot acc: 0.0831
current epoch: 4
train loss is 0.009773
average val loss: 0.016258, accuracy: 0.1693
average test loss: 0.016154, accuracy: 0.1687
case acc: 0.124772474
case acc: 0.30206868
case acc: 0.16591132
case acc: 0.14987163
case acc: 0.1112257
case acc: 0.15828177
top acc: 0.1383 ::: bot acc: 0.1120
top acc: 0.3133 ::: bot acc: 0.2915
top acc: 0.1860 ::: bot acc: 0.1462
top acc: 0.1600 ::: bot acc: 0.1380
top acc: 0.1283 ::: bot acc: 0.0931
top acc: 0.1747 ::: bot acc: 0.1401
current epoch: 5
train loss is 0.012554
average val loss: 0.014605, accuracy: 0.1609
average test loss: 0.014501, accuracy: 0.1603
case acc: 0.12177928
case acc: 0.2838672
case acc: 0.16074882
case acc: 0.14096618
case acc: 0.10978067
case acc: 0.14452887
top acc: 0.1338 ::: bot acc: 0.1091
top acc: 0.2946 ::: bot acc: 0.2735
top acc: 0.1806 ::: bot acc: 0.1407
top acc: 0.1514 ::: bot acc: 0.1294
top acc: 0.1265 ::: bot acc: 0.0919
top acc: 0.1612 ::: bot acc: 0.1265
current epoch: 6
train loss is 0.012814
average val loss: 0.004358, accuracy: 0.0768
average test loss: 0.004311, accuracy: 0.0765
case acc: 0.042223625
case acc: 0.18689762
case acc: 0.07549626
case acc: 0.056192715
case acc: 0.037836265
case acc: 0.060652852
top acc: 0.0550 ::: bot acc: 0.0288
top acc: 0.1980 ::: bot acc: 0.1765
top acc: 0.0954 ::: bot acc: 0.0559
top acc: 0.0663 ::: bot acc: 0.0452
top acc: 0.0533 ::: bot acc: 0.0218
top acc: 0.0777 ::: bot acc: 0.0428
current epoch: 7
train loss is 0.007735
average val loss: 0.001144, accuracy: 0.0320
average test loss: 0.001126, accuracy: 0.0314
case acc: 0.022708464
case acc: 0.10619991
case acc: 0.014740945
case acc: 0.011906713
case acc: 0.019976337
case acc: 0.012830301
top acc: 0.0114 ::: bot acc: 0.0344
top acc: 0.1170 ::: bot acc: 0.0960
top acc: 0.0237 ::: bot acc: 0.0162
top acc: 0.0051 ::: bot acc: 0.0216
top acc: 0.0080 ::: bot acc: 0.0357
top acc: 0.0133 ::: bot acc: 0.0215
current epoch: 8
train loss is 0.003351
average val loss: 0.000915, accuracy: 0.0293
average test loss: 0.000881, accuracy: 0.0283
case acc: 0.022585675
case acc: 0.09241526
case acc: 0.014416453
case acc: 0.01278187
case acc: 0.0150465965
case acc: 0.012447121
top acc: 0.0119 ::: bot acc: 0.0342
top acc: 0.1030 ::: bot acc: 0.0822
top acc: 0.0180 ::: bot acc: 0.0224
top acc: 0.0055 ::: bot acc: 0.0229
top acc: 0.0081 ::: bot acc: 0.0282
top acc: 0.0142 ::: bot acc: 0.0206
current epoch: 9
train loss is 0.002193
average val loss: 0.001033, accuracy: 0.0302
average test loss: 0.000995, accuracy: 0.0289
case acc: 0.009500278
case acc: 0.100303315
case acc: 0.017655605
case acc: 0.009694409
case acc: 0.016851645
case acc: 0.019221306
top acc: 0.0106 ::: bot acc: 0.0148
top acc: 0.1111 ::: bot acc: 0.0897
top acc: 0.0315 ::: bot acc: 0.0099
top acc: 0.0160 ::: bot acc: 0.0064
top acc: 0.0287 ::: bot acc: 0.0090
top acc: 0.0333 ::: bot acc: 0.0068
current epoch: 10
train loss is 0.002048
average val loss: 0.000982, accuracy: 0.0306
average test loss: 0.000956, accuracy: 0.0298
case acc: 0.009702164
case acc: 0.09584819
case acc: 0.018154662
case acc: 0.011808701
case acc: 0.02122204
case acc: 0.021788899
top acc: 0.0150 ::: bot acc: 0.0111
top acc: 0.1067 ::: bot acc: 0.0849
top acc: 0.0328 ::: bot acc: 0.0095
top acc: 0.0195 ::: bot acc: 0.0057
top acc: 0.0349 ::: bot acc: 0.0093
top acc: 0.0371 ::: bot acc: 0.0075
current epoch: 11
train loss is 0.001834
average val loss: 0.000821, accuracy: 0.0282
average test loss: 0.000793, accuracy: 0.0273
case acc: 0.009212911
case acc: 0.08603921
case acc: 0.016328964
case acc: 0.010750286
case acc: 0.021071654
case acc: 0.020511305
top acc: 0.0139 ::: bot acc: 0.0111
top acc: 0.0968 ::: bot acc: 0.0752
top acc: 0.0290 ::: bot acc: 0.0114
top acc: 0.0174 ::: bot acc: 0.0059
top acc: 0.0350 ::: bot acc: 0.0086
top acc: 0.0350 ::: bot acc: 0.0073
current epoch: 12
train loss is 0.001559
average val loss: 0.000740, accuracy: 0.0275
average test loss: 0.000707, accuracy: 0.0266
case acc: 0.009532536
case acc: 0.079176575
case acc: 0.015783947
case acc: 0.010968293
case acc: 0.022855457
case acc: 0.021085571
top acc: 0.0156 ::: bot acc: 0.0097
top acc: 0.0898 ::: bot acc: 0.0690
top acc: 0.0275 ::: bot acc: 0.0122
top acc: 0.0180 ::: bot acc: 0.0058
top acc: 0.0375 ::: bot acc: 0.0092
top acc: 0.0360 ::: bot acc: 0.0073
current epoch: 13
train loss is 0.001424
average val loss: 0.000687, accuracy: 0.0272
average test loss: 0.000653, accuracy: 0.0264
case acc: 0.010414882
case acc: 0.07373905
case acc: 0.015828924
case acc: 0.011708852
case acc: 0.024477884
case acc: 0.022251213
top acc: 0.0181 ::: bot acc: 0.0078
top acc: 0.0846 ::: bot acc: 0.0633
top acc: 0.0273 ::: bot acc: 0.0124
top acc: 0.0197 ::: bot acc: 0.0055
top acc: 0.0388 ::: bot acc: 0.0105
top acc: 0.0373 ::: bot acc: 0.0080
current epoch: 14
train loss is 0.001181
average val loss: 0.000643, accuracy: 0.0272
average test loss: 0.000613, accuracy: 0.0264
case acc: 0.011252364
case acc: 0.06872366
case acc: 0.015818944
case acc: 0.012842363
case acc: 0.026178159
case acc: 0.023778161
top acc: 0.0204 ::: bot acc: 0.0060
top acc: 0.0795 ::: bot acc: 0.0584
top acc: 0.0279 ::: bot acc: 0.0119
top acc: 0.0208 ::: bot acc: 0.0062
top acc: 0.0413 ::: bot acc: 0.0116
top acc: 0.0390 ::: bot acc: 0.0091
current epoch: 15
train loss is 0.001076
average val loss: 0.000590, accuracy: 0.0264
average test loss: 0.000559, accuracy: 0.0257
case acc: 0.011855384
case acc: 0.06373846
case acc: 0.016048536
case acc: 0.01281979
case acc: 0.02629242
case acc: 0.023291197
top acc: 0.0216 ::: bot acc: 0.0053
top acc: 0.0744 ::: bot acc: 0.0535
top acc: 0.0280 ::: bot acc: 0.0127
top acc: 0.0212 ::: bot acc: 0.0058
top acc: 0.0413 ::: bot acc: 0.0114
top acc: 0.0390 ::: bot acc: 0.0084
current epoch: 16
train loss is 0.000949
average val loss: 0.000550, accuracy: 0.0259
average test loss: 0.000519, accuracy: 0.0252
case acc: 0.012924215
case acc: 0.059520073
case acc: 0.015636735
case acc: 0.013520438
case acc: 0.026146427
case acc: 0.023416743
top acc: 0.0232 ::: bot acc: 0.0057
top acc: 0.0700 ::: bot acc: 0.0490
top acc: 0.0278 ::: bot acc: 0.0116
top acc: 0.0215 ::: bot acc: 0.0063
top acc: 0.0413 ::: bot acc: 0.0112
top acc: 0.0392 ::: bot acc: 0.0089
current epoch: 17
train loss is 0.000883
average val loss: 0.000523, accuracy: 0.0257
average test loss: 0.000493, accuracy: 0.0251
case acc: 0.013803937
case acc: 0.056109957
case acc: 0.016248379
case acc: 0.014286451
case acc: 0.026403269
case acc: 0.02353204
top acc: 0.0246 ::: bot acc: 0.0053
top acc: 0.0667 ::: bot acc: 0.0459
top acc: 0.0286 ::: bot acc: 0.0117
top acc: 0.0230 ::: bot acc: 0.0063
top acc: 0.0412 ::: bot acc: 0.0119
top acc: 0.0390 ::: bot acc: 0.0089
current epoch: 18
train loss is 0.000834
average val loss: 0.000467, accuracy: 0.0245
average test loss: 0.000436, accuracy: 0.0237
case acc: 0.01408867
case acc: 0.05111649
case acc: 0.016035736
case acc: 0.013812768
case acc: 0.024618957
case acc: 0.022571463
top acc: 0.0247 ::: bot acc: 0.0055
top acc: 0.0623 ::: bot acc: 0.0402
top acc: 0.0282 ::: bot acc: 0.0119
top acc: 0.0224 ::: bot acc: 0.0061
top acc: 0.0394 ::: bot acc: 0.0103
top acc: 0.0377 ::: bot acc: 0.0083
current epoch: 19
train loss is 0.000741
average val loss: 0.000487, accuracy: 0.0256
average test loss: 0.000457, accuracy: 0.0249
case acc: 0.016050365
case acc: 0.050364487
case acc: 0.017030101
case acc: 0.016279217
case acc: 0.025483295
case acc: 0.024095168
top acc: 0.0275 ::: bot acc: 0.0056
top acc: 0.0612 ::: bot acc: 0.0398
top acc: 0.0304 ::: bot acc: 0.0101
top acc: 0.0253 ::: bot acc: 0.0074
top acc: 0.0404 ::: bot acc: 0.0108
top acc: 0.0394 ::: bot acc: 0.0091
current epoch: 20
train loss is 0.000716
average val loss: 0.000521, accuracy: 0.0271
average test loss: 0.000490, accuracy: 0.0265
case acc: 0.018882701
case acc: 0.049876697
case acc: 0.019254953
case acc: 0.018432528
case acc: 0.02692206
case acc: 0.025521334
top acc: 0.0310 ::: bot acc: 0.0074
top acc: 0.0608 ::: bot acc: 0.0393
top acc: 0.0343 ::: bot acc: 0.0093
top acc: 0.0281 ::: bot acc: 0.0082
top acc: 0.0418 ::: bot acc: 0.0122
top acc: 0.0408 ::: bot acc: 0.0102
current epoch: 21
train loss is 0.000673
average val loss: 0.000479, accuracy: 0.0260
average test loss: 0.000449, accuracy: 0.0254
case acc: 0.01915925
case acc: 0.046620186
case acc: 0.019195193
case acc: 0.01814557
case acc: 0.025146645
case acc: 0.024342991
top acc: 0.0311 ::: bot acc: 0.0080
top acc: 0.0573 ::: bot acc: 0.0363
top acc: 0.0341 ::: bot acc: 0.0092
top acc: 0.0277 ::: bot acc: 0.0085
top acc: 0.0402 ::: bot acc: 0.0106
top acc: 0.0398 ::: bot acc: 0.0096
current epoch: 22
train loss is 0.000650
average val loss: 0.000478, accuracy: 0.0262
average test loss: 0.000446, accuracy: 0.0256
case acc: 0.020494675
case acc: 0.04499619
case acc: 0.020071758
case acc: 0.018984333
case acc: 0.02486439
case acc: 0.023900712
top acc: 0.0326 ::: bot acc: 0.0089
top acc: 0.0559 ::: bot acc: 0.0343
top acc: 0.0358 ::: bot acc: 0.0087
top acc: 0.0289 ::: bot acc: 0.0090
top acc: 0.0395 ::: bot acc: 0.0104
top acc: 0.0394 ::: bot acc: 0.0089
current epoch: 23
train loss is 0.000614
average val loss: 0.000553, accuracy: 0.0288
average test loss: 0.000519, accuracy: 0.0283
case acc: 0.024344772
case acc: 0.046699196
case acc: 0.023441918
case acc: 0.02193085
case acc: 0.027172808
case acc: 0.02599336
top acc: 0.0368 ::: bot acc: 0.0121
top acc: 0.0578 ::: bot acc: 0.0361
top acc: 0.0406 ::: bot acc: 0.0095
top acc: 0.0320 ::: bot acc: 0.0112
top acc: 0.0422 ::: bot acc: 0.0123
top acc: 0.0414 ::: bot acc: 0.0105
current epoch: 24
train loss is 0.000612
average val loss: 0.000578, accuracy: 0.0297
average test loss: 0.000545, accuracy: 0.0293
case acc: 0.026123023
case acc: 0.046651922
case acc: 0.025202895
case acc: 0.023431858
case acc: 0.02767724
case acc: 0.02644487
top acc: 0.0388 ::: bot acc: 0.0134
top acc: 0.0574 ::: bot acc: 0.0361
top acc: 0.0427 ::: bot acc: 0.0099
top acc: 0.0333 ::: bot acc: 0.0129
top acc: 0.0429 ::: bot acc: 0.0127
top acc: 0.0423 ::: bot acc: 0.0109
current epoch: 25
train loss is 0.000606
average val loss: 0.000625, accuracy: 0.0313
average test loss: 0.000589, accuracy: 0.0307
case acc: 0.028511494
case acc: 0.04698606
case acc: 0.027465882
case acc: 0.02532844
case acc: 0.0288107
case acc: 0.02729745
top acc: 0.0409 ::: bot acc: 0.0162
top acc: 0.0577 ::: bot acc: 0.0369
top acc: 0.0460 ::: bot acc: 0.0105
top acc: 0.0354 ::: bot acc: 0.0141
top acc: 0.0442 ::: bot acc: 0.0131
top acc: 0.0435 ::: bot acc: 0.0117
current epoch: 26
train loss is 0.000600
average val loss: 0.000638, accuracy: 0.0317
average test loss: 0.000601, accuracy: 0.0312
case acc: 0.029791083
case acc: 0.046365593
case acc: 0.02886074
case acc: 0.02622795
case acc: 0.028750526
case acc: 0.02722317
top acc: 0.0426 ::: bot acc: 0.0172
top acc: 0.0571 ::: bot acc: 0.0361
top acc: 0.0477 ::: bot acc: 0.0114
top acc: 0.0364 ::: bot acc: 0.0153
top acc: 0.0440 ::: bot acc: 0.0134
top acc: 0.0429 ::: bot acc: 0.0116
current epoch: 27
train loss is 0.000593
average val loss: 0.000702, accuracy: 0.0337
average test loss: 0.000666, accuracy: 0.0332
case acc: 0.03229558
case acc: 0.047188528
case acc: 0.03170137
case acc: 0.028724764
case acc: 0.030929837
case acc: 0.028432883
top acc: 0.0450 ::: bot acc: 0.0197
top acc: 0.0581 ::: bot acc: 0.0368
top acc: 0.0509 ::: bot acc: 0.0134
top acc: 0.0391 ::: bot acc: 0.0176
top acc: 0.0464 ::: bot acc: 0.0151
top acc: 0.0442 ::: bot acc: 0.0124
current epoch: 28
train loss is 0.000611
average val loss: 0.000790, accuracy: 0.0362
average test loss: 0.000755, accuracy: 0.0358
case acc: 0.035571635
case acc: 0.048815295
case acc: 0.035037205
case acc: 0.031538665
case acc: 0.033294283
case acc: 0.030677006
top acc: 0.0484 ::: bot acc: 0.0230
top acc: 0.0598 ::: bot acc: 0.0385
top acc: 0.0540 ::: bot acc: 0.0163
top acc: 0.0418 ::: bot acc: 0.0203
top acc: 0.0491 ::: bot acc: 0.0170
top acc: 0.0466 ::: bot acc: 0.0144
current epoch: 29
train loss is 0.000588
average val loss: 0.000753, accuracy: 0.0352
average test loss: 0.000717, accuracy: 0.0348
case acc: 0.034824513
case acc: 0.046961337
case acc: 0.034792855
case acc: 0.030716449
case acc: 0.03240674
case acc: 0.028960684
top acc: 0.0476 ::: bot acc: 0.0221
top acc: 0.0578 ::: bot acc: 0.0364
top acc: 0.0541 ::: bot acc: 0.0161
top acc: 0.0408 ::: bot acc: 0.0195
top acc: 0.0482 ::: bot acc: 0.0162
top acc: 0.0449 ::: bot acc: 0.0130
current epoch: 30
train loss is 0.000581
average val loss: 0.000801, accuracy: 0.0366
average test loss: 0.000767, accuracy: 0.0362
case acc: 0.036577318
case acc: 0.047454394
case acc: 0.037084967
case acc: 0.03229203
case acc: 0.033959836
case acc: 0.02969289
top acc: 0.0494 ::: bot acc: 0.0239
top acc: 0.0584 ::: bot acc: 0.0371
top acc: 0.0566 ::: bot acc: 0.0179
top acc: 0.0424 ::: bot acc: 0.0213
top acc: 0.0498 ::: bot acc: 0.0176
top acc: 0.0458 ::: bot acc: 0.0138
current epoch: 31
train loss is 0.000587
average val loss: 0.000805, accuracy: 0.0367
average test loss: 0.000773, accuracy: 0.0364
case acc: 0.036796365
case acc: 0.04694216
case acc: 0.037988767
case acc: 0.032961726
case acc: 0.033931416
case acc: 0.029816477
top acc: 0.0495 ::: bot acc: 0.0244
top acc: 0.0578 ::: bot acc: 0.0367
top acc: 0.0574 ::: bot acc: 0.0186
top acc: 0.0433 ::: bot acc: 0.0216
top acc: 0.0500 ::: bot acc: 0.0176
top acc: 0.0459 ::: bot acc: 0.0140
current epoch: 32
train loss is 0.000559
average val loss: 0.000860, accuracy: 0.0382
average test loss: 0.000824, accuracy: 0.0377
case acc: 0.03828779
case acc: 0.047302343
case acc: 0.040084627
case acc: 0.034780674
case acc: 0.035495847
case acc: 0.030515697
top acc: 0.0510 ::: bot acc: 0.0258
top acc: 0.0582 ::: bot acc: 0.0369
top acc: 0.0600 ::: bot acc: 0.0207
top acc: 0.0451 ::: bot acc: 0.0236
top acc: 0.0516 ::: bot acc: 0.0187
top acc: 0.0467 ::: bot acc: 0.0144
current epoch: 33
train loss is 0.000547
average val loss: 0.000903, accuracy: 0.0393
average test loss: 0.000863, accuracy: 0.0388
case acc: 0.03907511
case acc: 0.047755472
case acc: 0.041635938
case acc: 0.036316566
case acc: 0.036641
case acc: 0.031511307
top acc: 0.0517 ::: bot acc: 0.0268
top acc: 0.0586 ::: bot acc: 0.0372
top acc: 0.0612 ::: bot acc: 0.0223
top acc: 0.0466 ::: bot acc: 0.0253
top acc: 0.0525 ::: bot acc: 0.0202
top acc: 0.0477 ::: bot acc: 0.0155
current epoch: 34
train loss is 0.000561
average val loss: 0.000863, accuracy: 0.0383
average test loss: 0.000828, accuracy: 0.0379
case acc: 0.0382617
case acc: 0.045917828
case acc: 0.04099417
case acc: 0.035698984
case acc: 0.03607841
case acc: 0.030718049
top acc: 0.0510 ::: bot acc: 0.0259
top acc: 0.0568 ::: bot acc: 0.0356
top acc: 0.0607 ::: bot acc: 0.0216
top acc: 0.0459 ::: bot acc: 0.0246
top acc: 0.0521 ::: bot acc: 0.0195
top acc: 0.0468 ::: bot acc: 0.0145
current epoch: 35
train loss is 0.000565
average val loss: 0.000885, accuracy: 0.0389
average test loss: 0.000847, accuracy: 0.0385
case acc: 0.03851786
case acc: 0.04534466
case acc: 0.041896194
case acc: 0.036379524
case acc: 0.037244122
case acc: 0.03132172
top acc: 0.0516 ::: bot acc: 0.0257
top acc: 0.0563 ::: bot acc: 0.0349
top acc: 0.0615 ::: bot acc: 0.0225
top acc: 0.0467 ::: bot acc: 0.0251
top acc: 0.0531 ::: bot acc: 0.0205
top acc: 0.0474 ::: bot acc: 0.0152
current epoch: 36
train loss is 0.000508
average val loss: 0.000791, accuracy: 0.0364
average test loss: 0.000751, accuracy: 0.0360
case acc: 0.035773195
case acc: 0.041702762
case acc: 0.039641406
case acc: 0.034424264
case acc: 0.035439283
case acc: 0.02873465
top acc: 0.0484 ::: bot acc: 0.0233
top acc: 0.0522 ::: bot acc: 0.0317
top acc: 0.0593 ::: bot acc: 0.0204
top acc: 0.0445 ::: bot acc: 0.0230
top acc: 0.0513 ::: bot acc: 0.0189
top acc: 0.0448 ::: bot acc: 0.0128
current epoch: 37
train loss is 0.000478
average val loss: 0.000721, accuracy: 0.0345
average test loss: 0.000682, accuracy: 0.0340
case acc: 0.033578753
case acc: 0.038873415
case acc: 0.037796974
case acc: 0.032638382
case acc: 0.03394212
case acc: 0.027282875
top acc: 0.0463 ::: bot acc: 0.0209
top acc: 0.0498 ::: bot acc: 0.0286
top acc: 0.0574 ::: bot acc: 0.0185
top acc: 0.0428 ::: bot acc: 0.0214
top acc: 0.0499 ::: bot acc: 0.0174
top acc: 0.0433 ::: bot acc: 0.0115
current epoch: 38
train loss is 0.000442
average val loss: 0.000657, accuracy: 0.0327
average test loss: 0.000620, accuracy: 0.0322
case acc: 0.031470455
case acc: 0.036159292
case acc: 0.036142353
case acc: 0.031082764
case acc: 0.032655463
case acc: 0.025740657
top acc: 0.0444 ::: bot acc: 0.0190
top acc: 0.0469 ::: bot acc: 0.0257
top acc: 0.0556 ::: bot acc: 0.0170
top acc: 0.0415 ::: bot acc: 0.0198
top acc: 0.0483 ::: bot acc: 0.0166
top acc: 0.0416 ::: bot acc: 0.0101
current epoch: 39
train loss is 0.000424
average val loss: 0.000585, accuracy: 0.0305
average test loss: 0.000549, accuracy: 0.0300
case acc: 0.028835384
case acc: 0.032856114
case acc: 0.03416443
case acc: 0.029268552
case acc: 0.030966554
case acc: 0.024062317
top acc: 0.0414 ::: bot acc: 0.0164
top acc: 0.0436 ::: bot acc: 0.0225
top acc: 0.0533 ::: bot acc: 0.0155
top acc: 0.0393 ::: bot acc: 0.0182
top acc: 0.0468 ::: bot acc: 0.0147
top acc: 0.0395 ::: bot acc: 0.0092
current epoch: 40
train loss is 0.000384
average val loss: 0.000517, accuracy: 0.0284
average test loss: 0.000487, accuracy: 0.0280
case acc: 0.026374718
case acc: 0.030117795
case acc: 0.032099187
case acc: 0.027505267
case acc: 0.029308964
case acc: 0.022563428
top acc: 0.0391 ::: bot acc: 0.0139
top acc: 0.0408 ::: bot acc: 0.0196
top acc: 0.0512 ::: bot acc: 0.0139
top acc: 0.0377 ::: bot acc: 0.0163
top acc: 0.0449 ::: bot acc: 0.0137
top acc: 0.0378 ::: bot acc: 0.0082
current epoch: 41
train loss is 0.000374
average val loss: 0.000525, accuracy: 0.0286
average test loss: 0.000492, accuracy: 0.0282
case acc: 0.026414596
case acc: 0.029358957
case acc: 0.0321844
case acc: 0.028029516
case acc: 0.029875848
case acc: 0.023113135
top acc: 0.0391 ::: bot acc: 0.0141
top acc: 0.0401 ::: bot acc: 0.0191
top acc: 0.0513 ::: bot acc: 0.0138
top acc: 0.0385 ::: bot acc: 0.0168
top acc: 0.0453 ::: bot acc: 0.0141
top acc: 0.0383 ::: bot acc: 0.0086
current epoch: 42
train loss is 0.000357
average val loss: 0.000461, accuracy: 0.0265
average test loss: 0.000432, accuracy: 0.0260
case acc: 0.023888914
case acc: 0.026246242
case acc: 0.030010616
case acc: 0.026042879
case acc: 0.028361581
case acc: 0.021709414
top acc: 0.0367 ::: bot acc: 0.0115
top acc: 0.0370 ::: bot acc: 0.0160
top acc: 0.0489 ::: bot acc: 0.0123
top acc: 0.0364 ::: bot acc: 0.0147
top acc: 0.0438 ::: bot acc: 0.0129
top acc: 0.0367 ::: bot acc: 0.0078
current epoch: 43
train loss is 0.000320
average val loss: 0.000406, accuracy: 0.0245
average test loss: 0.000376, accuracy: 0.0240
case acc: 0.02153039
case acc: 0.023262626
case acc: 0.027822692
case acc: 0.02431944
case acc: 0.026662394
case acc: 0.020170374
top acc: 0.0341 ::: bot acc: 0.0097
top acc: 0.0340 ::: bot acc: 0.0128
top acc: 0.0463 ::: bot acc: 0.0109
top acc: 0.0344 ::: bot acc: 0.0134
top acc: 0.0419 ::: bot acc: 0.0117
top acc: 0.0350 ::: bot acc: 0.0072
current epoch: 44
train loss is 0.000306
average val loss: 0.000319, accuracy: 0.0212
average test loss: 0.000293, accuracy: 0.0205
case acc: 0.017334668
case acc: 0.018341716
case acc: 0.024628103
case acc: 0.020952508
case acc: 0.023826407
case acc: 0.017922828
top acc: 0.0295 ::: bot acc: 0.0062
top acc: 0.0292 ::: bot acc: 0.0079
top acc: 0.0420 ::: bot acc: 0.0098
top acc: 0.0309 ::: bot acc: 0.0105
top acc: 0.0386 ::: bot acc: 0.0099
top acc: 0.0315 ::: bot acc: 0.0069
current epoch: 45
train loss is 0.000288
average val loss: 0.000285, accuracy: 0.0198
average test loss: 0.000257, accuracy: 0.0189
case acc: 0.015614233
case acc: 0.015733937
case acc: 0.022866128
case acc: 0.019389484
case acc: 0.022691905
case acc: 0.017065894
top acc: 0.0272 ::: bot acc: 0.0055
top acc: 0.0262 ::: bot acc: 0.0057
top acc: 0.0396 ::: bot acc: 0.0093
top acc: 0.0293 ::: bot acc: 0.0093
top acc: 0.0371 ::: bot acc: 0.0090
top acc: 0.0302 ::: bot acc: 0.0070
current epoch: 46
train loss is 0.000266
average val loss: 0.000232, accuracy: 0.0175
average test loss: 0.000206, accuracy: 0.0164
case acc: 0.013038134
case acc: 0.011976127
case acc: 0.020113233
case acc: 0.016749684
case acc: 0.02056604
case acc: 0.015719444
top acc: 0.0237 ::: bot acc: 0.0047
top acc: 0.0220 ::: bot acc: 0.0030
top acc: 0.0357 ::: bot acc: 0.0091
top acc: 0.0262 ::: bot acc: 0.0071
top acc: 0.0346 ::: bot acc: 0.0082
top acc: 0.0276 ::: bot acc: 0.0081
current epoch: 47
train loss is 0.000260
average val loss: 0.000177, accuracy: 0.0149
average test loss: 0.000151, accuracy: 0.0135
case acc: 0.010124005
case acc: 0.008796925
case acc: 0.017113825
case acc: 0.013358586
case acc: 0.017516537
case acc: 0.014149876
top acc: 0.0180 ::: bot acc: 0.0072
top acc: 0.0167 ::: bot acc: 0.0045
top acc: 0.0303 ::: bot acc: 0.0107
top acc: 0.0218 ::: bot acc: 0.0057
top acc: 0.0299 ::: bot acc: 0.0081
top acc: 0.0240 ::: bot acc: 0.0106
current epoch: 48
train loss is 0.000249
average val loss: 0.000143, accuracy: 0.0132
average test loss: 0.000121, accuracy: 0.0117
case acc: 0.008844384
case acc: 0.0076783746
case acc: 0.014724914
case acc: 0.010558961
case acc: 0.015316918
case acc: 0.013046211
top acc: 0.0133 ::: bot acc: 0.0117
top acc: 0.0110 ::: bot acc: 0.0102
top acc: 0.0246 ::: bot acc: 0.0148
top acc: 0.0178 ::: bot acc: 0.0058
top acc: 0.0259 ::: bot acc: 0.0097
top acc: 0.0202 ::: bot acc: 0.0142
current epoch: 49
train loss is 0.000262
average val loss: 0.000140, accuracy: 0.0130
average test loss: 0.000118, accuracy: 0.0115
case acc: 0.009002695
case acc: 0.0077948933
case acc: 0.014289843
case acc: 0.009826381
case acc: 0.015185347
case acc: 0.012928655
top acc: 0.0116 ::: bot acc: 0.0133
top acc: 0.0088 ::: bot acc: 0.0120
top acc: 0.0222 ::: bot acc: 0.0174
top acc: 0.0165 ::: bot acc: 0.0059
top acc: 0.0258 ::: bot acc: 0.0096
top acc: 0.0202 ::: bot acc: 0.0142
current epoch: 50
train loss is 0.000267
average val loss: 0.000137, accuracy: 0.0128
average test loss: 0.000118, accuracy: 0.0116
case acc: 0.009715606
case acc: 0.0090633
case acc: 0.014128265
case acc: 0.009031075
case acc: 0.014625522
case acc: 0.012780445
top acc: 0.0091 ::: bot acc: 0.0162
top acc: 0.0059 ::: bot acc: 0.0158
top acc: 0.0185 ::: bot acc: 0.0214
top acc: 0.0147 ::: bot acc: 0.0073
top acc: 0.0246 ::: bot acc: 0.0105
top acc: 0.0193 ::: bot acc: 0.0149
