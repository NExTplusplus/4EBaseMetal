
		{"drop_out": 0.6, "drop_out_mc": 0.05, "repeat_mc": 50, "hidden": 30, "embedding_size": 5, "batch": 512, "lag": 4}
LME_Co_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6780 6780 6780
1.8562728 -0.6288155 0.15869391 -0.16256663
Validation: 756 756 756
Testing: 774 774 774
pre-processing time: 0.0002295970916748047
the split date is 2009-07-01
train dropout: 0.6 test dropout: 0.05
net initializing with time: 0.06296753883361816
preparing training and testing date with time: 0.0
current epoch: 1
train loss is 0.012803
average val loss: 0.005737, accuracy: 0.1012
average test loss: 0.006062, accuracy: 0.1046
case acc: 0.14840728
case acc: 0.076225
case acc: 0.10588032
case acc: 0.09939191
case acc: 0.13225453
case acc: 0.06573285
top acc: 0.1352 ::: bot acc: 0.1620
top acc: 0.0959 ::: bot acc: 0.0570
top acc: 0.0779 ::: bot acc: 0.1340
top acc: 0.0817 ::: bot acc: 0.1181
top acc: 0.1135 ::: bot acc: 0.1520
top acc: 0.0508 ::: bot acc: 0.0818
current epoch: 2
train loss is 0.008285
average val loss: 0.002808, accuracy: 0.0519
average test loss: 0.002726, accuracy: 0.0514
case acc: 0.04548007
case acc: 0.16206181
case acc: 0.020139342
case acc: 0.014402462
case acc: 0.04355707
case acc: 0.022574795
top acc: 0.0321 ::: bot acc: 0.0595
top acc: 0.1814 ::: bot acc: 0.1428
top acc: 0.0237 ::: bot acc: 0.0309
top acc: 0.0143 ::: bot acc: 0.0239
top acc: 0.0249 ::: bot acc: 0.0641
top acc: 0.0352 ::: bot acc: 0.0114
current epoch: 3
train loss is 0.008670
average val loss: 0.007296, accuracy: 0.0995
average test loss: 0.006902, accuracy: 0.0955
case acc: 0.044228107
case acc: 0.23706262
case acc: 0.08546241
case acc: 0.07681894
case acc: 0.036545083
case acc: 0.09278668
top acc: 0.0579 ::: bot acc: 0.0302
top acc: 0.2563 ::: bot acc: 0.2173
top acc: 0.1131 ::: bot acc: 0.0592
top acc: 0.0956 ::: bot acc: 0.0570
top acc: 0.0544 ::: bot acc: 0.0178
top acc: 0.1084 ::: bot acc: 0.0765
current epoch: 4
train loss is 0.010076
average val loss: 0.016548, accuracy: 0.1702
average test loss: 0.015812, accuracy: 0.1660
case acc: 0.12246772
case acc: 0.30144796
case acc: 0.16227935
case acc: 0.14639495
case acc: 0.10818791
case acc: 0.15498842
top acc: 0.1360 ::: bot acc: 0.1084
top acc: 0.3208 ::: bot acc: 0.2821
top acc: 0.1899 ::: bot acc: 0.1354
top acc: 0.1651 ::: bot acc: 0.1268
top acc: 0.1275 ::: bot acc: 0.0877
top acc: 0.1707 ::: bot acc: 0.1381
current epoch: 5
train loss is 0.013129
average val loss: 0.013774, accuracy: 0.1546
average test loss: 0.013102, accuracy: 0.1504
case acc: 0.11234071
case acc: 0.27637497
case acc: 0.14965613
case acc: 0.12966274
case acc: 0.1003563
case acc: 0.13426872
top acc: 0.1259 ::: bot acc: 0.0980
top acc: 0.2956 ::: bot acc: 0.2572
top acc: 0.1775 ::: bot acc: 0.1227
top acc: 0.1486 ::: bot acc: 0.1097
top acc: 0.1195 ::: bot acc: 0.0795
top acc: 0.1503 ::: bot acc: 0.1171
current epoch: 6
train loss is 0.013176
average val loss: 0.003545, accuracy: 0.0643
average test loss: 0.003249, accuracy: 0.0600
case acc: 0.025334325
case acc: 0.17169072
case acc: 0.05701593
case acc: 0.038515616
case acc: 0.02420154
case acc: 0.042958524
top acc: 0.0388 ::: bot acc: 0.0115
top acc: 0.1909 ::: bot acc: 0.1526
top acc: 0.0844 ::: bot acc: 0.0306
top acc: 0.0568 ::: bot acc: 0.0199
top acc: 0.0399 ::: bot acc: 0.0102
top acc: 0.0586 ::: bot acc: 0.0265
current epoch: 7
train loss is 0.007534
average val loss: 0.001211, accuracy: 0.0366
average test loss: 0.001186, accuracy: 0.0372
case acc: 0.033143383
case acc: 0.097004965
case acc: 0.021382013
case acc: 0.025383424
case acc: 0.028332263
case acc: 0.018044272
top acc: 0.0194 ::: bot acc: 0.0474
top acc: 0.1163 ::: bot acc: 0.0777
top acc: 0.0185 ::: bot acc: 0.0359
top acc: 0.0119 ::: bot acc: 0.0426
top acc: 0.0101 ::: bot acc: 0.0484
top acc: 0.0069 ::: bot acc: 0.0323
current epoch: 8
train loss is 0.003560
average val loss: 0.001034, accuracy: 0.0310
average test loss: 0.000982, accuracy: 0.0308
case acc: 0.023178492
case acc: 0.09383935
case acc: 0.020196319
case acc: 0.018707322
case acc: 0.01684381
case acc: 0.012273556
top acc: 0.0101 ::: bot acc: 0.0369
top acc: 0.1130 ::: bot acc: 0.0746
top acc: 0.0224 ::: bot acc: 0.0315
top acc: 0.0100 ::: bot acc: 0.0336
top acc: 0.0092 ::: bot acc: 0.0318
top acc: 0.0114 ::: bot acc: 0.0212
current epoch: 9
train loss is 0.002605
average val loss: 0.001133, accuracy: 0.0316
average test loss: 0.001028, accuracy: 0.0299
case acc: 0.01123793
case acc: 0.09963289
case acc: 0.020484326
case acc: 0.01425024
case acc: 0.017262425
case acc: 0.016601255
top acc: 0.0083 ::: bot acc: 0.0199
top acc: 0.1188 ::: bot acc: 0.0804
top acc: 0.0346 ::: bot acc: 0.0200
top acc: 0.0200 ::: bot acc: 0.0188
top acc: 0.0285 ::: bot acc: 0.0127
top acc: 0.0276 ::: bot acc: 0.0096
current epoch: 10
train loss is 0.002470
average val loss: 0.001078, accuracy: 0.0320
average test loss: 0.000973, accuracy: 0.0302
case acc: 0.010436974
case acc: 0.09480544
case acc: 0.020910902
case acc: 0.014873015
case acc: 0.02073549
case acc: 0.01916277
top acc: 0.0123 ::: bot acc: 0.0159
top acc: 0.1141 ::: bot acc: 0.0758
top acc: 0.0356 ::: bot acc: 0.0190
top acc: 0.0232 ::: bot acc: 0.0158
top acc: 0.0350 ::: bot acc: 0.0101
top acc: 0.0315 ::: bot acc: 0.0098
current epoch: 11
train loss is 0.002324
average val loss: 0.000902, accuracy: 0.0297
average test loss: 0.000811, accuracy: 0.0280
case acc: 0.010539386
case acc: 0.08463636
case acc: 0.019900152
case acc: 0.01417705
case acc: 0.020848837
case acc: 0.017752163
top acc: 0.0114 ::: bot acc: 0.0167
top acc: 0.1039 ::: bot acc: 0.0655
top acc: 0.0314 ::: bot acc: 0.0231
top acc: 0.0206 ::: bot acc: 0.0177
top acc: 0.0352 ::: bot acc: 0.0101
top acc: 0.0296 ::: bot acc: 0.0094
current epoch: 12
train loss is 0.002020
average val loss: 0.000844, accuracy: 0.0296
average test loss: 0.000752, accuracy: 0.0278
case acc: 0.010145028
case acc: 0.07904903
case acc: 0.020040665
case acc: 0.014890423
case acc: 0.023322016
case acc: 0.01947035
top acc: 0.0138 ::: bot acc: 0.0138
top acc: 0.0983 ::: bot acc: 0.0601
top acc: 0.0314 ::: bot acc: 0.0231
top acc: 0.0229 ::: bot acc: 0.0164
top acc: 0.0390 ::: bot acc: 0.0099
top acc: 0.0319 ::: bot acc: 0.0098
current epoch: 13
train loss is 0.001804
average val loss: 0.000748, accuracy: 0.0284
average test loss: 0.000657, accuracy: 0.0265
case acc: 0.010026941
case acc: 0.07148651
case acc: 0.019760454
case acc: 0.014374985
case acc: 0.02396449
case acc: 0.019217255
top acc: 0.0143 ::: bot acc: 0.0128
top acc: 0.0907 ::: bot acc: 0.0523
top acc: 0.0296 ::: bot acc: 0.0252
top acc: 0.0222 ::: bot acc: 0.0160
top acc: 0.0395 ::: bot acc: 0.0106
top acc: 0.0314 ::: bot acc: 0.0099
current epoch: 14
train loss is 0.001589
average val loss: 0.000686, accuracy: 0.0277
average test loss: 0.000603, accuracy: 0.0259
case acc: 0.010508025
case acc: 0.06598443
case acc: 0.019694246
case acc: 0.014701213
case acc: 0.024916423
case acc: 0.019548263
top acc: 0.0161 ::: bot acc: 0.0111
top acc: 0.0853 ::: bot acc: 0.0469
top acc: 0.0294 ::: bot acc: 0.0254
top acc: 0.0231 ::: bot acc: 0.0155
top acc: 0.0409 ::: bot acc: 0.0108
top acc: 0.0322 ::: bot acc: 0.0096
current epoch: 15
train loss is 0.001426
average val loss: 0.000627, accuracy: 0.0269
average test loss: 0.000543, accuracy: 0.0250
case acc: 0.010674024
case acc: 0.060474403
case acc: 0.019623717
case acc: 0.014865649
case acc: 0.024905663
case acc: 0.01923495
top acc: 0.0172 ::: bot acc: 0.0100
top acc: 0.0798 ::: bot acc: 0.0414
top acc: 0.0286 ::: bot acc: 0.0261
top acc: 0.0233 ::: bot acc: 0.0156
top acc: 0.0410 ::: bot acc: 0.0106
top acc: 0.0315 ::: bot acc: 0.0098
current epoch: 16
train loss is 0.001344
average val loss: 0.000645, accuracy: 0.0280
average test loss: 0.000553, accuracy: 0.0258
case acc: 0.012151103
case acc: 0.058851197
case acc: 0.020118225
case acc: 0.015925601
case acc: 0.026857845
case acc: 0.021189714
top acc: 0.0211 ::: bot acc: 0.0069
top acc: 0.0781 ::: bot acc: 0.0396
top acc: 0.0316 ::: bot acc: 0.0235
top acc: 0.0266 ::: bot acc: 0.0126
top acc: 0.0436 ::: bot acc: 0.0114
top acc: 0.0343 ::: bot acc: 0.0102
current epoch: 17
train loss is 0.001283
average val loss: 0.000633, accuracy: 0.0282
average test loss: 0.000537, accuracy: 0.0258
case acc: 0.013148793
case acc: 0.056002505
case acc: 0.020351416
case acc: 0.016358472
case acc: 0.027402105
case acc: 0.021802677
top acc: 0.0230 ::: bot acc: 0.0058
top acc: 0.0754 ::: bot acc: 0.0368
top acc: 0.0328 ::: bot acc: 0.0222
top acc: 0.0276 ::: bot acc: 0.0115
top acc: 0.0443 ::: bot acc: 0.0115
top acc: 0.0352 ::: bot acc: 0.0104
current epoch: 18
train loss is 0.001196
average val loss: 0.000625, accuracy: 0.0285
average test loss: 0.000523, accuracy: 0.0258
case acc: 0.014501996
case acc: 0.053476177
case acc: 0.020378519
case acc: 0.016952487
case acc: 0.027527181
case acc: 0.02218961
top acc: 0.0252 ::: bot acc: 0.0054
top acc: 0.0729 ::: bot acc: 0.0342
top acc: 0.0344 ::: bot acc: 0.0202
top acc: 0.0292 ::: bot acc: 0.0105
top acc: 0.0443 ::: bot acc: 0.0118
top acc: 0.0354 ::: bot acc: 0.0107
current epoch: 19
train loss is 0.001162
average val loss: 0.000604, accuracy: 0.0282
average test loss: 0.000501, accuracy: 0.0255
case acc: 0.015280842
case acc: 0.05080616
case acc: 0.020716147
case acc: 0.017283024
case acc: 0.027141197
case acc: 0.021783927
top acc: 0.0266 ::: bot acc: 0.0051
top acc: 0.0700 ::: bot acc: 0.0315
top acc: 0.0354 ::: bot acc: 0.0194
top acc: 0.0296 ::: bot acc: 0.0101
top acc: 0.0440 ::: bot acc: 0.0115
top acc: 0.0350 ::: bot acc: 0.0104
current epoch: 20
train loss is 0.001106
average val loss: 0.000594, accuracy: 0.0282
average test loss: 0.000489, accuracy: 0.0254
case acc: 0.016429309
case acc: 0.04861336
case acc: 0.021217778
case acc: 0.017726863
case acc: 0.026704526
case acc: 0.021739645
top acc: 0.0283 ::: bot acc: 0.0056
top acc: 0.0682 ::: bot acc: 0.0291
top acc: 0.0370 ::: bot acc: 0.0181
top acc: 0.0306 ::: bot acc: 0.0097
top acc: 0.0435 ::: bot acc: 0.0113
top acc: 0.0347 ::: bot acc: 0.0106
current epoch: 21
train loss is 0.001092
average val loss: 0.000619, accuracy: 0.0292
average test loss: 0.000503, accuracy: 0.0261
case acc: 0.01831756
case acc: 0.04789804
case acc: 0.022225969
case acc: 0.01886934
case acc: 0.027103402
case acc: 0.022282263
top acc: 0.0304 ::: bot acc: 0.0065
top acc: 0.0673 ::: bot acc: 0.0285
top acc: 0.0398 ::: bot acc: 0.0159
top acc: 0.0327 ::: bot acc: 0.0088
top acc: 0.0438 ::: bot acc: 0.0115
top acc: 0.0357 ::: bot acc: 0.0105
current epoch: 22
train loss is 0.001064
average val loss: 0.000676, accuracy: 0.0310
average test loss: 0.000549, accuracy: 0.0278
case acc: 0.02133033
case acc: 0.04868162
case acc: 0.024054509
case acc: 0.0205067
case acc: 0.028498176
case acc: 0.023636546
top acc: 0.0341 ::: bot acc: 0.0085
top acc: 0.0681 ::: bot acc: 0.0294
top acc: 0.0435 ::: bot acc: 0.0140
top acc: 0.0355 ::: bot acc: 0.0080
top acc: 0.0455 ::: bot acc: 0.0122
top acc: 0.0374 ::: bot acc: 0.0112
current epoch: 23
train loss is 0.001073
average val loss: 0.000729, accuracy: 0.0327
average test loss: 0.000590, accuracy: 0.0292
case acc: 0.024025965
case acc: 0.048885457
case acc: 0.025635103
case acc: 0.022072345
case acc: 0.02990191
case acc: 0.02459723
top acc: 0.0368 ::: bot acc: 0.0108
top acc: 0.0683 ::: bot acc: 0.0294
top acc: 0.0468 ::: bot acc: 0.0124
top acc: 0.0378 ::: bot acc: 0.0083
top acc: 0.0472 ::: bot acc: 0.0132
top acc: 0.0384 ::: bot acc: 0.0117
current epoch: 24
train loss is 0.001022
average val loss: 0.000667, accuracy: 0.0310
average test loss: 0.000537, accuracy: 0.0277
case acc: 0.023116427
case acc: 0.045752443
case acc: 0.025451731
case acc: 0.021099476
case acc: 0.028038202
case acc: 0.022721166
top acc: 0.0358 ::: bot acc: 0.0100
top acc: 0.0651 ::: bot acc: 0.0264
top acc: 0.0461 ::: bot acc: 0.0128
top acc: 0.0364 ::: bot acc: 0.0081
top acc: 0.0451 ::: bot acc: 0.0120
top acc: 0.0361 ::: bot acc: 0.0107
current epoch: 25
train loss is 0.000996
average val loss: 0.000708, accuracy: 0.0323
average test loss: 0.000569, accuracy: 0.0288
case acc: 0.025144054
case acc: 0.04594205
case acc: 0.026916372
case acc: 0.022441063
case acc: 0.029017696
case acc: 0.023234714
top acc: 0.0382 ::: bot acc: 0.0116
top acc: 0.0653 ::: bot acc: 0.0265
top acc: 0.0490 ::: bot acc: 0.0120
top acc: 0.0383 ::: bot acc: 0.0085
top acc: 0.0459 ::: bot acc: 0.0127
top acc: 0.0368 ::: bot acc: 0.0108
current epoch: 26
train loss is 0.000978
average val loss: 0.000695, accuracy: 0.0319
average test loss: 0.000558, accuracy: 0.0285
case acc: 0.025451556
case acc: 0.04464516
case acc: 0.027423618
case acc: 0.022531202
case acc: 0.028318094
case acc: 0.022546062
top acc: 0.0385 ::: bot acc: 0.0119
top acc: 0.0641 ::: bot acc: 0.0253
top acc: 0.0499 ::: bot acc: 0.0117
top acc: 0.0383 ::: bot acc: 0.0087
top acc: 0.0453 ::: bot acc: 0.0124
top acc: 0.0360 ::: bot acc: 0.0106
current epoch: 27
train loss is 0.000981
average val loss: 0.000738, accuracy: 0.0332
average test loss: 0.000589, accuracy: 0.0296
case acc: 0.027116325
case acc: 0.044965386
case acc: 0.029059695
case acc: 0.024007117
case acc: 0.029105337
case acc: 0.02314161
top acc: 0.0402 ::: bot acc: 0.0136
top acc: 0.0645 ::: bot acc: 0.0256
top acc: 0.0521 ::: bot acc: 0.0118
top acc: 0.0400 ::: bot acc: 0.0094
top acc: 0.0461 ::: bot acc: 0.0128
top acc: 0.0368 ::: bot acc: 0.0110
current epoch: 28
train loss is 0.000983
average val loss: 0.000813, accuracy: 0.0352
average test loss: 0.000654, accuracy: 0.0315
case acc: 0.029943427
case acc: 0.046295453
case acc: 0.031467084
case acc: 0.026081402
case acc: 0.030784182
case acc: 0.024633773
top acc: 0.0429 ::: bot acc: 0.0163
top acc: 0.0658 ::: bot acc: 0.0268
top acc: 0.0556 ::: bot acc: 0.0120
top acc: 0.0427 ::: bot acc: 0.0105
top acc: 0.0482 ::: bot acc: 0.0136
top acc: 0.0386 ::: bot acc: 0.0116
current epoch: 29
train loss is 0.000979
average val loss: 0.000889, accuracy: 0.0372
average test loss: 0.000717, accuracy: 0.0334
case acc: 0.03253251
case acc: 0.047347438
case acc: 0.033546023
case acc: 0.028110147
case acc: 0.032784477
case acc: 0.025934746
top acc: 0.0457 ::: bot acc: 0.0188
top acc: 0.0668 ::: bot acc: 0.0280
top acc: 0.0585 ::: bot acc: 0.0125
top acc: 0.0449 ::: bot acc: 0.0120
top acc: 0.0506 ::: bot acc: 0.0150
top acc: 0.0400 ::: bot acc: 0.0125
current epoch: 30
train loss is 0.000995
average val loss: 0.000910, accuracy: 0.0377
average test loss: 0.000738, accuracy: 0.0340
case acc: 0.03336429
case acc: 0.04698662
case acc: 0.03470101
case acc: 0.028956875
case acc: 0.033673983
case acc: 0.026210397
top acc: 0.0464 ::: bot acc: 0.0197
top acc: 0.0666 ::: bot acc: 0.0275
top acc: 0.0601 ::: bot acc: 0.0132
top acc: 0.0460 ::: bot acc: 0.0125
top acc: 0.0516 ::: bot acc: 0.0158
top acc: 0.0404 ::: bot acc: 0.0128
current epoch: 31
train loss is 0.000976
average val loss: 0.000933, accuracy: 0.0383
average test loss: 0.000758, accuracy: 0.0345
case acc: 0.034169152
case acc: 0.046728615
case acc: 0.035829924
case acc: 0.029662393
case acc: 0.034435034
case acc: 0.026426215
top acc: 0.0473 ::: bot acc: 0.0203
top acc: 0.0664 ::: bot acc: 0.0271
top acc: 0.0614 ::: bot acc: 0.0136
top acc: 0.0468 ::: bot acc: 0.0132
top acc: 0.0525 ::: bot acc: 0.0162
top acc: 0.0407 ::: bot acc: 0.0129
current epoch: 32
train loss is 0.000981
average val loss: 0.000968, accuracy: 0.0392
average test loss: 0.000786, accuracy: 0.0353
case acc: 0.034996748
case acc: 0.046807513
case acc: 0.037163552
case acc: 0.030811435
case acc: 0.035466775
case acc: 0.026819887
top acc: 0.0482 ::: bot acc: 0.0211
top acc: 0.0664 ::: bot acc: 0.0275
top acc: 0.0630 ::: bot acc: 0.0145
top acc: 0.0479 ::: bot acc: 0.0141
top acc: 0.0537 ::: bot acc: 0.0171
top acc: 0.0411 ::: bot acc: 0.0131
current epoch: 33
train loss is 0.000970
average val loss: 0.000985, accuracy: 0.0396
average test loss: 0.000799, accuracy: 0.0357
case acc: 0.03539391
case acc: 0.04617194
case acc: 0.037974875
case acc: 0.031463932
case acc: 0.0360196
case acc: 0.027147962
top acc: 0.0487 ::: bot acc: 0.0214
top acc: 0.0657 ::: bot acc: 0.0268
top acc: 0.0640 ::: bot acc: 0.0149
top acc: 0.0488 ::: bot acc: 0.0144
top acc: 0.0542 ::: bot acc: 0.0174
top acc: 0.0416 ::: bot acc: 0.0133
current epoch: 34
train loss is 0.000960
average val loss: 0.000935, accuracy: 0.0384
average test loss: 0.000755, accuracy: 0.0346
case acc: 0.034182224
case acc: 0.044000886
case acc: 0.037262596
case acc: 0.030685008
case acc: 0.035407074
case acc: 0.025902009
top acc: 0.0473 ::: bot acc: 0.0205
top acc: 0.0635 ::: bot acc: 0.0246
top acc: 0.0632 ::: bot acc: 0.0146
top acc: 0.0478 ::: bot acc: 0.0139
top acc: 0.0535 ::: bot acc: 0.0172
top acc: 0.0400 ::: bot acc: 0.0125
current epoch: 35
train loss is 0.000938
average val loss: 0.000896, accuracy: 0.0374
average test loss: 0.000719, accuracy: 0.0336
case acc: 0.0329747
case acc: 0.041998126
case acc: 0.036752705
case acc: 0.029965013
case acc: 0.03483906
case acc: 0.025137927
top acc: 0.0460 ::: bot acc: 0.0193
top acc: 0.0616 ::: bot acc: 0.0226
top acc: 0.0625 ::: bot acc: 0.0144
top acc: 0.0471 ::: bot acc: 0.0133
top acc: 0.0527 ::: bot acc: 0.0167
top acc: 0.0391 ::: bot acc: 0.0120
current epoch: 36
train loss is 0.000913
average val loss: 0.000901, accuracy: 0.0376
average test loss: 0.000722, accuracy: 0.0337
case acc: 0.03276823
case acc: 0.041318517
case acc: 0.03707469
case acc: 0.030562194
case acc: 0.035148147
case acc: 0.025385272
top acc: 0.0459 ::: bot acc: 0.0189
top acc: 0.0608 ::: bot acc: 0.0219
top acc: 0.0629 ::: bot acc: 0.0143
top acc: 0.0477 ::: bot acc: 0.0138
top acc: 0.0533 ::: bot acc: 0.0170
top acc: 0.0395 ::: bot acc: 0.0122
current epoch: 37
train loss is 0.000889
average val loss: 0.000789, accuracy: 0.0347
average test loss: 0.000623, accuracy: 0.0309
case acc: 0.029307809
case acc: 0.03738634
case acc: 0.03465156
case acc: 0.028253242
case acc: 0.032661054
case acc: 0.023215856
top acc: 0.0424 ::: bot acc: 0.0156
top acc: 0.0569 ::: bot acc: 0.0183
top acc: 0.0598 ::: bot acc: 0.0131
top acc: 0.0450 ::: bot acc: 0.0122
top acc: 0.0504 ::: bot acc: 0.0151
top acc: 0.0368 ::: bot acc: 0.0110
current epoch: 38
train loss is 0.000847
average val loss: 0.000702, accuracy: 0.0323
average test loss: 0.000548, accuracy: 0.0286
case acc: 0.026478617
case acc: 0.033871394
case acc: 0.03256025
case acc: 0.026284868
case acc: 0.030949734
case acc: 0.021355892
top acc: 0.0396 ::: bot acc: 0.0129
top acc: 0.0530 ::: bot acc: 0.0150
top acc: 0.0573 ::: bot acc: 0.0122
top acc: 0.0429 ::: bot acc: 0.0107
top acc: 0.0484 ::: bot acc: 0.0138
top acc: 0.0344 ::: bot acc: 0.0100
current epoch: 39
train loss is 0.000819
average val loss: 0.000625, accuracy: 0.0301
average test loss: 0.000480, accuracy: 0.0264
case acc: 0.0237281
case acc: 0.03056925
case acc: 0.03057275
case acc: 0.024351904
case acc: 0.029451976
case acc: 0.019667806
top acc: 0.0366 ::: bot acc: 0.0105
top acc: 0.0496 ::: bot acc: 0.0122
top acc: 0.0544 ::: bot acc: 0.0117
top acc: 0.0405 ::: bot acc: 0.0096
top acc: 0.0466 ::: bot acc: 0.0130
top acc: 0.0321 ::: bot acc: 0.0096
current epoch: 40
train loss is 0.000782
average val loss: 0.000600, accuracy: 0.0293
average test loss: 0.000459, accuracy: 0.0257
case acc: 0.022655608
case acc: 0.029154075
case acc: 0.029937927
case acc: 0.02379819
case acc: 0.029300444
case acc: 0.019073734
top acc: 0.0353 ::: bot acc: 0.0097
top acc: 0.0480 ::: bot acc: 0.0111
top acc: 0.0535 ::: bot acc: 0.0118
top acc: 0.0398 ::: bot acc: 0.0093
top acc: 0.0465 ::: bot acc: 0.0130
top acc: 0.0314 ::: bot acc: 0.0094
current epoch: 41
train loss is 0.000763
average val loss: 0.000525, accuracy: 0.0270
average test loss: 0.000396, accuracy: 0.0234
case acc: 0.019943714
case acc: 0.025938328
case acc: 0.028025853
case acc: 0.02192006
case acc: 0.027222827
case acc: 0.017337898
top acc: 0.0324 ::: bot acc: 0.0076
top acc: 0.0445 ::: bot acc: 0.0085
top acc: 0.0508 ::: bot acc: 0.0116
top acc: 0.0374 ::: bot acc: 0.0085
top acc: 0.0439 ::: bot acc: 0.0117
top acc: 0.0291 ::: bot acc: 0.0089
current epoch: 42
train loss is 0.000733
average val loss: 0.000440, accuracy: 0.0241
average test loss: 0.000327, accuracy: 0.0207
case acc: 0.016491706
case acc: 0.0223388
case acc: 0.025657116
case acc: 0.019760381
case acc: 0.02465325
case acc: 0.015470767
top acc: 0.0281 ::: bot acc: 0.0056
top acc: 0.0400 ::: bot acc: 0.0065
top acc: 0.0469 ::: bot acc: 0.0122
top acc: 0.0342 ::: bot acc: 0.0083
top acc: 0.0405 ::: bot acc: 0.0107
top acc: 0.0261 ::: bot acc: 0.0091
current epoch: 43
train loss is 0.000710
average val loss: 0.000382, accuracy: 0.0220
average test loss: 0.000283, accuracy: 0.0190
case acc: 0.014264166
case acc: 0.02003559
case acc: 0.024017585
case acc: 0.018347442
case acc: 0.02289609
case acc: 0.014355535
top acc: 0.0252 ::: bot acc: 0.0051
top acc: 0.0366 ::: bot acc: 0.0064
top acc: 0.0439 ::: bot acc: 0.0135
top acc: 0.0320 ::: bot acc: 0.0087
top acc: 0.0381 ::: bot acc: 0.0103
top acc: 0.0238 ::: bot acc: 0.0104
current epoch: 44
train loss is 0.000694
average val loss: 0.000363, accuracy: 0.0213
average test loss: 0.000268, accuracy: 0.0184
case acc: 0.013412001
case acc: 0.018829316
case acc: 0.02341283
case acc: 0.017856812
case acc: 0.022674661
case acc: 0.014069838
top acc: 0.0237 ::: bot acc: 0.0053
top acc: 0.0346 ::: bot acc: 0.0068
top acc: 0.0425 ::: bot acc: 0.0143
top acc: 0.0311 ::: bot acc: 0.0089
top acc: 0.0378 ::: bot acc: 0.0104
top acc: 0.0234 ::: bot acc: 0.0103
current epoch: 45
train loss is 0.000686
average val loss: 0.000334, accuracy: 0.0202
average test loss: 0.000247, accuracy: 0.0175
case acc: 0.012262631
case acc: 0.017491365
case acc: 0.022302639
case acc: 0.017176831
case acc: 0.021874148
case acc: 0.013809408
top acc: 0.0216 ::: bot acc: 0.0063
top acc: 0.0323 ::: bot acc: 0.0077
top acc: 0.0403 ::: bot acc: 0.0156
top acc: 0.0298 ::: bot acc: 0.0094
top acc: 0.0365 ::: bot acc: 0.0105
top acc: 0.0228 ::: bot acc: 0.0108
current epoch: 46
train loss is 0.000672
average val loss: 0.000280, accuracy: 0.0179
average test loss: 0.000209, accuracy: 0.0158
case acc: 0.010621875
case acc: 0.015591595
case acc: 0.020818662
case acc: 0.01563308
case acc: 0.019592647
case acc: 0.012749984
top acc: 0.0173 ::: bot acc: 0.0096
top acc: 0.0274 ::: bot acc: 0.0115
top acc: 0.0357 ::: bot acc: 0.0195
top acc: 0.0264 ::: bot acc: 0.0117
top acc: 0.0331 ::: bot acc: 0.0105
top acc: 0.0198 ::: bot acc: 0.0132
current epoch: 47
train loss is 0.000661
average val loss: 0.000251, accuracy: 0.0167
average test loss: 0.000192, accuracy: 0.0150
case acc: 0.009951682
case acc: 0.014674981
case acc: 0.020023808
case acc: 0.014669476
case acc: 0.018361509
case acc: 0.012207061
top acc: 0.0140 ::: bot acc: 0.0128
top acc: 0.0235 ::: bot acc: 0.0152
top acc: 0.0321 ::: bot acc: 0.0230
top acc: 0.0239 ::: bot acc: 0.0139
top acc: 0.0309 ::: bot acc: 0.0110
top acc: 0.0176 ::: bot acc: 0.0150
current epoch: 48
train loss is 0.000655
average val loss: 0.000229, accuracy: 0.0159
average test loss: 0.000185, accuracy: 0.0146
case acc: 0.009995629
case acc: 0.014380036
case acc: 0.019728454
case acc: 0.014106499
case acc: 0.017502123
case acc: 0.012143595
top acc: 0.0111 ::: bot acc: 0.0157
top acc: 0.0197 ::: bot acc: 0.0192
top acc: 0.0284 ::: bot acc: 0.0268
top acc: 0.0215 ::: bot acc: 0.0164
top acc: 0.0292 ::: bot acc: 0.0119
top acc: 0.0162 ::: bot acc: 0.0169
current epoch: 49
train loss is 0.000659
average val loss: 0.000214, accuracy: 0.0154
average test loss: 0.000186, accuracy: 0.0147
case acc: 0.010948529
case acc: 0.014753825
case acc: 0.020532267
case acc: 0.013427221
case acc: 0.016231319
case acc: 0.012107684
top acc: 0.0072 ::: bot acc: 0.0199
top acc: 0.0148 ::: bot acc: 0.0242
top acc: 0.0234 ::: bot acc: 0.0319
top acc: 0.0178 ::: bot acc: 0.0196
top acc: 0.0263 ::: bot acc: 0.0139
top acc: 0.0139 ::: bot acc: 0.0193
current epoch: 50
train loss is 0.000665
average val loss: 0.000216, accuracy: 0.0155
average test loss: 0.000214, accuracy: 0.0159
case acc: 0.014035379
case acc: 0.017271383
case acc: 0.022552706
case acc: 0.014131259
case acc: 0.014724774
case acc: 0.012678532
top acc: 0.0056 ::: bot acc: 0.0255
top acc: 0.0098 ::: bot acc: 0.0307
top acc: 0.0169 ::: bot acc: 0.0384
top acc: 0.0131 ::: bot acc: 0.0246
top acc: 0.0225 ::: bot acc: 0.0173
top acc: 0.0101 ::: bot acc: 0.0229
LME_Co_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6804 6804 6804
1.8562728 -0.6288155 0.12137239 -0.16228472
Validation: 762 762 762
Testing: 744 744 744
pre-processing time: 0.0001964569091796875
the split date is 2010-01-01
train dropout: 0.6 test dropout: 0.05
net initializing with time: 0.0021162033081054688
preparing training and testing date with time: 0.0
current epoch: 1
train loss is 0.012907
average val loss: 0.005494, accuracy: 0.0997
average test loss: 0.005703, accuracy: 0.1004
case acc: 0.14009307
case acc: 0.083294414
case acc: 0.09873534
case acc: 0.09665081
case acc: 0.12654176
case acc: 0.056809563
top acc: 0.1148 ::: bot acc: 0.1626
top acc: 0.1047 ::: bot acc: 0.0627
top acc: 0.0629 ::: bot acc: 0.1349
top acc: 0.0709 ::: bot acc: 0.1227
top acc: 0.1038 ::: bot acc: 0.1496
top acc: 0.0306 ::: bot acc: 0.0821
current epoch: 2
train loss is 0.007748
average val loss: 0.002966, accuracy: 0.0515
average test loss: 0.003084, accuracy: 0.0543
case acc: 0.035950884
case acc: 0.17289382
case acc: 0.027438235
case acc: 0.019043278
case acc: 0.033995096
case acc: 0.036202483
top acc: 0.0164 ::: bot acc: 0.0557
top acc: 0.1937 ::: bot acc: 0.1536
top acc: 0.0428 ::: bot acc: 0.0298
top acc: 0.0279 ::: bot acc: 0.0243
top acc: 0.0131 ::: bot acc: 0.0560
top acc: 0.0637 ::: bot acc: 0.0138
current epoch: 3
train loss is 0.008256
average val loss: 0.007746, accuracy: 0.1041
average test loss: 0.007848, accuracy: 0.1037
case acc: 0.05443992
case acc: 0.24520777
case acc: 0.09361272
case acc: 0.080272034
case acc: 0.044878732
case acc: 0.103881516
top acc: 0.0776 ::: bot acc: 0.0339
top acc: 0.2653 ::: bot acc: 0.2260
top acc: 0.1293 ::: bot acc: 0.0572
top acc: 0.1071 ::: bot acc: 0.0539
top acc: 0.0677 ::: bot acc: 0.0222
top acc: 0.1349 ::: bot acc: 0.0753
current epoch: 4
train loss is 0.010538
average val loss: 0.016026, accuracy: 0.1674
average test loss: 0.016064, accuracy: 0.1668
case acc: 0.12439256
case acc: 0.30174315
case acc: 0.16254829
case acc: 0.14271152
case acc: 0.11018301
case acc: 0.15906285
top acc: 0.1489 ::: bot acc: 0.1019
top acc: 0.3221 ::: bot acc: 0.2820
top acc: 0.1982 ::: bot acc: 0.1259
top acc: 0.1692 ::: bot acc: 0.1168
top acc: 0.1330 ::: bot acc: 0.0872
top acc: 0.1896 ::: bot acc: 0.1310
current epoch: 5
train loss is 0.012872
average val loss: 0.013741, accuracy: 0.1548
average test loss: 0.013794, accuracy: 0.1543
case acc: 0.11740829
case acc: 0.2793906
case acc: 0.15308982
case acc: 0.1290156
case acc: 0.10536494
case acc: 0.14148577
top acc: 0.1416 ::: bot acc: 0.0951
top acc: 0.2998 ::: bot acc: 0.2602
top acc: 0.1887 ::: bot acc: 0.1160
top acc: 0.1556 ::: bot acc: 0.1029
top acc: 0.1284 ::: bot acc: 0.0824
top acc: 0.1721 ::: bot acc: 0.1133
current epoch: 6
train loss is 0.013144
average val loss: 0.002881, accuracy: 0.0542
average test loss: 0.002988, accuracy: 0.0552
case acc: 0.023528012
case acc: 0.16403957
case acc: 0.050869223
case acc: 0.030908028
case acc: 0.020813135
case acc: 0.041075274
top acc: 0.0439 ::: bot acc: 0.0095
top acc: 0.1842 ::: bot acc: 0.1445
top acc: 0.0844 ::: bot acc: 0.0174
top acc: 0.0529 ::: bot acc: 0.0133
top acc: 0.0378 ::: bot acc: 0.0097
top acc: 0.0701 ::: bot acc: 0.0160
current epoch: 7
train loss is 0.007498
average val loss: 0.001171, accuracy: 0.0357
average test loss: 0.001305, accuracy: 0.0396
case acc: 0.03227847
case acc: 0.09856536
case acc: 0.026975267
case acc: 0.029291637
case acc: 0.026802724
case acc: 0.023591356
top acc: 0.0144 ::: bot acc: 0.0515
top acc: 0.1191 ::: bot acc: 0.0788
top acc: 0.0284 ::: bot acc: 0.0438
top acc: 0.0128 ::: bot acc: 0.0507
top acc: 0.0096 ::: bot acc: 0.0471
top acc: 0.0215 ::: bot acc: 0.0377
current epoch: 8
train loss is 0.003366
average val loss: 0.000986, accuracy: 0.0305
average test loss: 0.001121, accuracy: 0.0350
case acc: 0.025570488
case acc: 0.09424665
case acc: 0.026804594
case acc: 0.024046903
case acc: 0.018090453
case acc: 0.02111125
top acc: 0.0122 ::: bot acc: 0.0424
top acc: 0.1147 ::: bot acc: 0.0748
top acc: 0.0314 ::: bot acc: 0.0413
top acc: 0.0120 ::: bot acc: 0.0430
top acc: 0.0138 ::: bot acc: 0.0319
top acc: 0.0304 ::: bot acc: 0.0280
current epoch: 9
train loss is 0.002462
average val loss: 0.001085, accuracy: 0.0310
average test loss: 0.001212, accuracy: 0.0350
case acc: 0.017339533
case acc: 0.1012328
case acc: 0.02762327
case acc: 0.019388124
case acc: 0.019176064
case acc: 0.024993848
top acc: 0.0224 ::: bot acc: 0.0243
top acc: 0.1215 ::: bot acc: 0.0816
top acc: 0.0442 ::: bot acc: 0.0279
top acc: 0.0261 ::: bot acc: 0.0272
top acc: 0.0343 ::: bot acc: 0.0110
top acc: 0.0479 ::: bot acc: 0.0126
current epoch: 10
train loss is 0.002412
average val loss: 0.001006, accuracy: 0.0309
average test loss: 0.001136, accuracy: 0.0346
case acc: 0.017053101
case acc: 0.09530706
case acc: 0.0276413
case acc: 0.01925765
case acc: 0.021794291
case acc: 0.026419116
top acc: 0.0259 ::: bot acc: 0.0210
top acc: 0.1157 ::: bot acc: 0.0757
top acc: 0.0445 ::: bot acc: 0.0276
top acc: 0.0280 ::: bot acc: 0.0249
top acc: 0.0397 ::: bot acc: 0.0084
top acc: 0.0501 ::: bot acc: 0.0118
current epoch: 11
train loss is 0.002167
average val loss: 0.000865, accuracy: 0.0292
average test loss: 0.000996, accuracy: 0.0329
case acc: 0.016903102
case acc: 0.08637209
case acc: 0.026890201
case acc: 0.019214941
case acc: 0.022472154
case acc: 0.025762204
top acc: 0.0256 ::: bot acc: 0.0209
top acc: 0.1069 ::: bot acc: 0.0667
top acc: 0.0410 ::: bot acc: 0.0309
top acc: 0.0267 ::: bot acc: 0.0261
top acc: 0.0407 ::: bot acc: 0.0085
top acc: 0.0494 ::: bot acc: 0.0118
current epoch: 12
train loss is 0.001922
average val loss: 0.000710, accuracy: 0.0267
average test loss: 0.000843, accuracy: 0.0309
case acc: 0.017135408
case acc: 0.075990364
case acc: 0.026733225
case acc: 0.019308595
case acc: 0.021648632
case acc: 0.024466466
top acc: 0.0234 ::: bot acc: 0.0232
top acc: 0.0964 ::: bot acc: 0.0562
top acc: 0.0365 ::: bot acc: 0.0359
top acc: 0.0238 ::: bot acc: 0.0289
top acc: 0.0394 ::: bot acc: 0.0089
top acc: 0.0467 ::: bot acc: 0.0127
current epoch: 13
train loss is 0.001647
average val loss: 0.000665, accuracy: 0.0266
average test loss: 0.000792, accuracy: 0.0304
case acc: 0.017241219
case acc: 0.070931844
case acc: 0.02648828
case acc: 0.019161372
case acc: 0.023245174
case acc: 0.025268149
top acc: 0.0269 ::: bot acc: 0.0207
top acc: 0.0911 ::: bot acc: 0.0514
top acc: 0.0363 ::: bot acc: 0.0353
top acc: 0.0257 ::: bot acc: 0.0269
top acc: 0.0418 ::: bot acc: 0.0084
top acc: 0.0484 ::: bot acc: 0.0120
current epoch: 14
train loss is 0.001503
average val loss: 0.000610, accuracy: 0.0260
average test loss: 0.000743, accuracy: 0.0297
case acc: 0.017257078
case acc: 0.06574162
case acc: 0.026556408
case acc: 0.019089844
case acc: 0.024065759
case acc: 0.025695883
top acc: 0.0287 ::: bot acc: 0.0186
top acc: 0.0862 ::: bot acc: 0.0459
top acc: 0.0368 ::: bot acc: 0.0353
top acc: 0.0268 ::: bot acc: 0.0262
top acc: 0.0430 ::: bot acc: 0.0083
top acc: 0.0492 ::: bot acc: 0.0116
current epoch: 15
train loss is 0.001358
average val loss: 0.000559, accuracy: 0.0253
average test loss: 0.000693, accuracy: 0.0290
case acc: 0.01717777
case acc: 0.060740136
case acc: 0.026618017
case acc: 0.019103613
case acc: 0.024464073
case acc: 0.025806734
top acc: 0.0299 ::: bot acc: 0.0171
top acc: 0.0811 ::: bot acc: 0.0410
top acc: 0.0363 ::: bot acc: 0.0359
top acc: 0.0274 ::: bot acc: 0.0253
top acc: 0.0439 ::: bot acc: 0.0082
top acc: 0.0496 ::: bot acc: 0.0114
current epoch: 16
train loss is 0.001316
average val loss: 0.000561, accuracy: 0.0260
average test loss: 0.000696, accuracy: 0.0294
case acc: 0.0181364
case acc: 0.058523554
case acc: 0.026760845
case acc: 0.01924793
case acc: 0.02616127
case acc: 0.027401486
top acc: 0.0336 ::: bot acc: 0.0140
top acc: 0.0791 ::: bot acc: 0.0386
top acc: 0.0390 ::: bot acc: 0.0333
top acc: 0.0303 ::: bot acc: 0.0225
top acc: 0.0460 ::: bot acc: 0.0090
top acc: 0.0521 ::: bot acc: 0.0111
current epoch: 17
train loss is 0.001220
average val loss: 0.000536, accuracy: 0.0258
average test loss: 0.000668, accuracy: 0.0290
case acc: 0.018468965
case acc: 0.05512556
case acc: 0.026741788
case acc: 0.019457687
case acc: 0.026289599
case acc: 0.027659286
top acc: 0.0349 ::: bot acc: 0.0126
top acc: 0.0756 ::: bot acc: 0.0353
top acc: 0.0396 ::: bot acc: 0.0323
top acc: 0.0315 ::: bot acc: 0.0215
top acc: 0.0462 ::: bot acc: 0.0088
top acc: 0.0524 ::: bot acc: 0.0110
current epoch: 18
train loss is 0.001167
average val loss: 0.000502, accuracy: 0.0252
average test loss: 0.000638, accuracy: 0.0284
case acc: 0.01922551
case acc: 0.05144718
case acc: 0.026941145
case acc: 0.019620836
case acc: 0.025712958
case acc: 0.027481608
top acc: 0.0364 ::: bot acc: 0.0120
top acc: 0.0720 ::: bot acc: 0.0316
top acc: 0.0404 ::: bot acc: 0.0320
top acc: 0.0321 ::: bot acc: 0.0211
top acc: 0.0453 ::: bot acc: 0.0086
top acc: 0.0523 ::: bot acc: 0.0110
current epoch: 19
train loss is 0.001090
average val loss: 0.000479, accuracy: 0.0248
average test loss: 0.000612, accuracy: 0.0279
case acc: 0.019536685
case acc: 0.048555706
case acc: 0.027046897
case acc: 0.019802604
case acc: 0.025311435
case acc: 0.02721645
top acc: 0.0373 ::: bot acc: 0.0111
top acc: 0.0689 ::: bot acc: 0.0290
top acc: 0.0411 ::: bot acc: 0.0311
top acc: 0.0328 ::: bot acc: 0.0203
top acc: 0.0448 ::: bot acc: 0.0086
top acc: 0.0517 ::: bot acc: 0.0108
current epoch: 20
train loss is 0.001023
average val loss: 0.000465, accuracy: 0.0246
average test loss: 0.000598, accuracy: 0.0277
case acc: 0.020350164
case acc: 0.0465339
case acc: 0.02711635
case acc: 0.01997669
case acc: 0.025022645
case acc: 0.027139662
top acc: 0.0388 ::: bot acc: 0.0103
top acc: 0.0669 ::: bot acc: 0.0268
top acc: 0.0425 ::: bot acc: 0.0294
top acc: 0.0336 ::: bot acc: 0.0197
top acc: 0.0444 ::: bot acc: 0.0087
top acc: 0.0518 ::: bot acc: 0.0111
current epoch: 21
train loss is 0.000981
average val loss: 0.000460, accuracy: 0.0247
average test loss: 0.000596, accuracy: 0.0278
case acc: 0.021616641
case acc: 0.044847704
case acc: 0.027654434
case acc: 0.020535922
case acc: 0.02486557
case acc: 0.027135761
top acc: 0.0409 ::: bot acc: 0.0101
top acc: 0.0654 ::: bot acc: 0.0251
top acc: 0.0442 ::: bot acc: 0.0280
top acc: 0.0354 ::: bot acc: 0.0186
top acc: 0.0444 ::: bot acc: 0.0087
top acc: 0.0518 ::: bot acc: 0.0107
current epoch: 22
train loss is 0.000976
average val loss: 0.000476, accuracy: 0.0255
average test loss: 0.000613, accuracy: 0.0284
case acc: 0.023177462
case acc: 0.044444874
case acc: 0.0282581
case acc: 0.021256916
case acc: 0.025268141
case acc: 0.027895538
top acc: 0.0433 ::: bot acc: 0.0101
top acc: 0.0649 ::: bot acc: 0.0248
top acc: 0.0469 ::: bot acc: 0.0253
top acc: 0.0369 ::: bot acc: 0.0169
top acc: 0.0448 ::: bot acc: 0.0086
top acc: 0.0530 ::: bot acc: 0.0109
current epoch: 23
train loss is 0.000951
average val loss: 0.000439, accuracy: 0.0244
average test loss: 0.000577, accuracy: 0.0274
case acc: 0.022790527
case acc: 0.04159462
case acc: 0.028338969
case acc: 0.02104159
case acc: 0.023844572
case acc: 0.026751645
top acc: 0.0430 ::: bot acc: 0.0099
top acc: 0.0619 ::: bot acc: 0.0222
top acc: 0.0470 ::: bot acc: 0.0252
top acc: 0.0365 ::: bot acc: 0.0175
top acc: 0.0429 ::: bot acc: 0.0083
top acc: 0.0513 ::: bot acc: 0.0110
current epoch: 24
train loss is 0.000901
average val loss: 0.000435, accuracy: 0.0244
average test loss: 0.000573, accuracy: 0.0274
case acc: 0.02353314
case acc: 0.040410228
case acc: 0.028856197
case acc: 0.021306911
case acc: 0.023561016
case acc: 0.026538827
top acc: 0.0438 ::: bot acc: 0.0100
top acc: 0.0606 ::: bot acc: 0.0212
top acc: 0.0485 ::: bot acc: 0.0240
top acc: 0.0370 ::: bot acc: 0.0171
top acc: 0.0425 ::: bot acc: 0.0082
top acc: 0.0508 ::: bot acc: 0.0112
current epoch: 25
train loss is 0.000904
average val loss: 0.000505, accuracy: 0.0268
average test loss: 0.000642, accuracy: 0.0294
case acc: 0.026569458
case acc: 0.042505316
case acc: 0.030232247
case acc: 0.02310856
case acc: 0.025522944
case acc: 0.028315112
top acc: 0.0482 ::: bot acc: 0.0107
top acc: 0.0630 ::: bot acc: 0.0229
top acc: 0.0528 ::: bot acc: 0.0196
top acc: 0.0410 ::: bot acc: 0.0146
top acc: 0.0453 ::: bot acc: 0.0085
top acc: 0.0536 ::: bot acc: 0.0108
current epoch: 26
train loss is 0.000919
average val loss: 0.000562, accuracy: 0.0287
average test loss: 0.000698, accuracy: 0.0309
case acc: 0.028962607
case acc: 0.04384282
case acc: 0.03177917
case acc: 0.024626544
case acc: 0.026881773
case acc: 0.029489271
top acc: 0.0511 ::: bot acc: 0.0116
top acc: 0.0642 ::: bot acc: 0.0243
top acc: 0.0567 ::: bot acc: 0.0166
top acc: 0.0436 ::: bot acc: 0.0138
top acc: 0.0471 ::: bot acc: 0.0090
top acc: 0.0554 ::: bot acc: 0.0108
current epoch: 27
train loss is 0.000908
average val loss: 0.000546, accuracy: 0.0283
average test loss: 0.000681, accuracy: 0.0305
case acc: 0.029011508
case acc: 0.042467564
case acc: 0.031961773
case acc: 0.024408257
case acc: 0.026353441
case acc: 0.028540336
top acc: 0.0512 ::: bot acc: 0.0117
top acc: 0.0629 ::: bot acc: 0.0230
top acc: 0.0573 ::: bot acc: 0.0158
top acc: 0.0435 ::: bot acc: 0.0137
top acc: 0.0463 ::: bot acc: 0.0089
top acc: 0.0541 ::: bot acc: 0.0108
current epoch: 28
train loss is 0.000886
average val loss: 0.000561, accuracy: 0.0288
average test loss: 0.000698, accuracy: 0.0309
case acc: 0.029876277
case acc: 0.04238141
case acc: 0.03269197
case acc: 0.025058351
case acc: 0.027076863
case acc: 0.028524801
top acc: 0.0522 ::: bot acc: 0.0123
top acc: 0.0629 ::: bot acc: 0.0229
top acc: 0.0592 ::: bot acc: 0.0141
top acc: 0.0445 ::: bot acc: 0.0134
top acc: 0.0472 ::: bot acc: 0.0092
top acc: 0.0538 ::: bot acc: 0.0109
current epoch: 29
train loss is 0.000900
average val loss: 0.000647, accuracy: 0.0315
average test loss: 0.000782, accuracy: 0.0331
case acc: 0.032919828
case acc: 0.04454161
case acc: 0.03468224
case acc: 0.026920302
case acc: 0.029529413
case acc: 0.03022234
top acc: 0.0559 ::: bot acc: 0.0142
top acc: 0.0651 ::: bot acc: 0.0250
top acc: 0.0631 ::: bot acc: 0.0123
top acc: 0.0476 ::: bot acc: 0.0128
top acc: 0.0506 ::: bot acc: 0.0102
top acc: 0.0563 ::: bot acc: 0.0112
current epoch: 30
train loss is 0.000893
average val loss: 0.000683, accuracy: 0.0325
average test loss: 0.000819, accuracy: 0.0341
case acc: 0.034417056
case acc: 0.0449127
case acc: 0.03603014
case acc: 0.027708229
case acc: 0.030718105
case acc: 0.030680476
top acc: 0.0575 ::: bot acc: 0.0153
top acc: 0.0652 ::: bot acc: 0.0254
top acc: 0.0652 ::: bot acc: 0.0119
top acc: 0.0491 ::: bot acc: 0.0124
top acc: 0.0520 ::: bot acc: 0.0108
top acc: 0.0570 ::: bot acc: 0.0114
current epoch: 31
train loss is 0.000883
average val loss: 0.000668, accuracy: 0.0321
average test loss: 0.000806, accuracy: 0.0337
case acc: 0.03401392
case acc: 0.0435936
case acc: 0.0363247
case acc: 0.027712118
case acc: 0.030628858
case acc: 0.030079605
top acc: 0.0572 ::: bot acc: 0.0150
top acc: 0.0640 ::: bot acc: 0.0240
top acc: 0.0657 ::: bot acc: 0.0120
top acc: 0.0490 ::: bot acc: 0.0125
top acc: 0.0517 ::: bot acc: 0.0108
top acc: 0.0563 ::: bot acc: 0.0110
current epoch: 32
train loss is 0.000879
average val loss: 0.000683, accuracy: 0.0326
average test loss: 0.000818, accuracy: 0.0341
case acc: 0.03470175
case acc: 0.043161057
case acc: 0.036951743
case acc: 0.028138967
case acc: 0.03123655
case acc: 0.030210996
top acc: 0.0581 ::: bot acc: 0.0155
top acc: 0.0636 ::: bot acc: 0.0237
top acc: 0.0667 ::: bot acc: 0.0118
top acc: 0.0497 ::: bot acc: 0.0123
top acc: 0.0527 ::: bot acc: 0.0110
top acc: 0.0563 ::: bot acc: 0.0111
current epoch: 33
train loss is 0.000882
average val loss: 0.000699, accuracy: 0.0330
average test loss: 0.000834, accuracy: 0.0345
case acc: 0.03526343
case acc: 0.04298729
case acc: 0.037809614
case acc: 0.02885268
case acc: 0.0316114
case acc: 0.030249385
top acc: 0.0586 ::: bot acc: 0.0160
top acc: 0.0634 ::: bot acc: 0.0235
top acc: 0.0681 ::: bot acc: 0.0118
top acc: 0.0506 ::: bot acc: 0.0126
top acc: 0.0530 ::: bot acc: 0.0113
top acc: 0.0564 ::: bot acc: 0.0111
current epoch: 34
train loss is 0.000856
average val loss: 0.000704, accuracy: 0.0332
average test loss: 0.000837, accuracy: 0.0345
case acc: 0.035253454
case acc: 0.042433772
case acc: 0.038338266
case acc: 0.029061204
case acc: 0.031719312
case acc: 0.030220214
top acc: 0.0586 ::: bot acc: 0.0160
top acc: 0.0628 ::: bot acc: 0.0231
top acc: 0.0690 ::: bot acc: 0.0114
top acc: 0.0511 ::: bot acc: 0.0124
top acc: 0.0531 ::: bot acc: 0.0114
top acc: 0.0563 ::: bot acc: 0.0111
current epoch: 35
train loss is 0.000875
average val loss: 0.000705, accuracy: 0.0332
average test loss: 0.000839, accuracy: 0.0346
case acc: 0.035299197
case acc: 0.041919265
case acc: 0.038880266
case acc: 0.029360475
case acc: 0.03177147
case acc: 0.03013657
top acc: 0.0585 ::: bot acc: 0.0160
top acc: 0.0623 ::: bot acc: 0.0226
top acc: 0.0697 ::: bot acc: 0.0117
top acc: 0.0516 ::: bot acc: 0.0125
top acc: 0.0531 ::: bot acc: 0.0114
top acc: 0.0562 ::: bot acc: 0.0111
current epoch: 36
train loss is 0.000858
average val loss: 0.000768, accuracy: 0.0350
average test loss: 0.000905, accuracy: 0.0362
case acc: 0.036920227
case acc: 0.04327698
case acc: 0.04054847
case acc: 0.03113812
case acc: 0.033875152
case acc: 0.031406797
top acc: 0.0603 ::: bot acc: 0.0174
top acc: 0.0637 ::: bot acc: 0.0238
top acc: 0.0723 ::: bot acc: 0.0115
top acc: 0.0540 ::: bot acc: 0.0130
top acc: 0.0556 ::: bot acc: 0.0127
top acc: 0.0580 ::: bot acc: 0.0114
current epoch: 37
train loss is 0.000858
average val loss: 0.000760, accuracy: 0.0348
average test loss: 0.000895, accuracy: 0.0360
case acc: 0.03663389
case acc: 0.042058434
case acc: 0.04056195
case acc: 0.031172192
case acc: 0.03412942
case acc: 0.031315494
top acc: 0.0600 ::: bot acc: 0.0172
top acc: 0.0623 ::: bot acc: 0.0228
top acc: 0.0722 ::: bot acc: 0.0117
top acc: 0.0539 ::: bot acc: 0.0130
top acc: 0.0560 ::: bot acc: 0.0128
top acc: 0.0578 ::: bot acc: 0.0112
current epoch: 38
train loss is 0.000844
average val loss: 0.000737, accuracy: 0.0342
average test loss: 0.000873, accuracy: 0.0354
case acc: 0.035847712
case acc: 0.040384673
case acc: 0.040117405
case acc: 0.030896537
case acc: 0.034281526
case acc: 0.03082369
top acc: 0.0593 ::: bot acc: 0.0163
top acc: 0.0607 ::: bot acc: 0.0213
top acc: 0.0717 ::: bot acc: 0.0114
top acc: 0.0536 ::: bot acc: 0.0129
top acc: 0.0562 ::: bot acc: 0.0129
top acc: 0.0573 ::: bot acc: 0.0111
current epoch: 39
train loss is 0.000830
average val loss: 0.000674, accuracy: 0.0324
average test loss: 0.000810, accuracy: 0.0338
case acc: 0.0336525
case acc: 0.03760199
case acc: 0.038884323
case acc: 0.029801797
case acc: 0.03328597
case acc: 0.029734198
top acc: 0.0569 ::: bot acc: 0.0146
top acc: 0.0578 ::: bot acc: 0.0186
top acc: 0.0698 ::: bot acc: 0.0114
top acc: 0.0522 ::: bot acc: 0.0125
top acc: 0.0550 ::: bot acc: 0.0123
top acc: 0.0558 ::: bot acc: 0.0110
current epoch: 40
train loss is 0.000784
average val loss: 0.000536, accuracy: 0.0282
average test loss: 0.000673, accuracy: 0.0302
case acc: 0.029263875
case acc: 0.03251572
case acc: 0.035882395
case acc: 0.026879948
case acc: 0.029773438
case acc: 0.026836958
top acc: 0.0516 ::: bot acc: 0.0118
top acc: 0.0523 ::: bot acc: 0.0145
top acc: 0.0650 ::: bot acc: 0.0120
top acc: 0.0478 ::: bot acc: 0.0125
top acc: 0.0506 ::: bot acc: 0.0102
top acc: 0.0514 ::: bot acc: 0.0107
current epoch: 41
train loss is 0.000726
average val loss: 0.000485, accuracy: 0.0266
average test loss: 0.000621, accuracy: 0.0287
case acc: 0.027335122
case acc: 0.029786857
case acc: 0.03457379
case acc: 0.026080849
case acc: 0.028666781
case acc: 0.025882917
top acc: 0.0491 ::: bot acc: 0.0110
top acc: 0.0493 ::: bot acc: 0.0120
top acc: 0.0628 ::: bot acc: 0.0125
top acc: 0.0463 ::: bot acc: 0.0133
top acc: 0.0494 ::: bot acc: 0.0097
top acc: 0.0499 ::: bot acc: 0.0112
current epoch: 42
train loss is 0.000710
average val loss: 0.000405, accuracy: 0.0237
average test loss: 0.000542, accuracy: 0.0264
case acc: 0.024598341
case acc: 0.025941705
case acc: 0.032789323
case acc: 0.024139958
case acc: 0.026610443
case acc: 0.024305962
top acc: 0.0454 ::: bot acc: 0.0103
top acc: 0.0448 ::: bot acc: 0.0094
top acc: 0.0593 ::: bot acc: 0.0146
top acc: 0.0429 ::: bot acc: 0.0138
top acc: 0.0468 ::: bot acc: 0.0089
top acc: 0.0470 ::: bot acc: 0.0123
current epoch: 43
train loss is 0.000682
average val loss: 0.000369, accuracy: 0.0224
average test loss: 0.000506, accuracy: 0.0253
case acc: 0.023046425
case acc: 0.023619823
case acc: 0.03181606
case acc: 0.023532707
case acc: 0.025852641
case acc: 0.023961766
top acc: 0.0431 ::: bot acc: 0.0104
top acc: 0.0419 ::: bot acc: 0.0084
top acc: 0.0569 ::: bot acc: 0.0162
top acc: 0.0418 ::: bot acc: 0.0143
top acc: 0.0457 ::: bot acc: 0.0086
top acc: 0.0462 ::: bot acc: 0.0127
current epoch: 44
train loss is 0.000655
average val loss: 0.000291, accuracy: 0.0193
average test loss: 0.000430, accuracy: 0.0229
case acc: 0.020021008
case acc: 0.019665025
case acc: 0.029902546
case acc: 0.021679416
case acc: 0.023369301
case acc: 0.022544904
top acc: 0.0381 ::: bot acc: 0.0111
top acc: 0.0364 ::: bot acc: 0.0078
top acc: 0.0517 ::: bot acc: 0.0209
top acc: 0.0379 ::: bot acc: 0.0166
top acc: 0.0422 ::: bot acc: 0.0082
top acc: 0.0428 ::: bot acc: 0.0152
current epoch: 45
train loss is 0.000626
average val loss: 0.000244, accuracy: 0.0173
average test loss: 0.000384, accuracy: 0.0212
case acc: 0.018079376
case acc: 0.016910443
case acc: 0.028457182
case acc: 0.020267015
case acc: 0.021694474
case acc: 0.021761768
top acc: 0.0341 ::: bot acc: 0.0134
top acc: 0.0314 ::: bot acc: 0.0093
top acc: 0.0473 ::: bot acc: 0.0252
top acc: 0.0347 ::: bot acc: 0.0188
top acc: 0.0394 ::: bot acc: 0.0089
top acc: 0.0404 ::: bot acc: 0.0174
current epoch: 46
train loss is 0.000609
average val loss: 0.000198, accuracy: 0.0153
average test loss: 0.000338, accuracy: 0.0198
case acc: 0.017165588
case acc: 0.014861199
case acc: 0.026948296
case acc: 0.019367425
case acc: 0.01933187
case acc: 0.020886512
top acc: 0.0281 ::: bot acc: 0.0194
top acc: 0.0248 ::: bot acc: 0.0154
top acc: 0.0410 ::: bot acc: 0.0313
top acc: 0.0298 ::: bot acc: 0.0236
top acc: 0.0345 ::: bot acc: 0.0115
top acc: 0.0360 ::: bot acc: 0.0215
current epoch: 47
train loss is 0.000601
average val loss: 0.000188, accuracy: 0.0148
average test loss: 0.000329, accuracy: 0.0196
case acc: 0.017304897
case acc: 0.014529299
case acc: 0.026653228
case acc: 0.01930842
case acc: 0.018765794
case acc: 0.020841286
top acc: 0.0252 ::: bot acc: 0.0223
top acc: 0.0210 ::: bot acc: 0.0191
top acc: 0.0372 ::: bot acc: 0.0350
top acc: 0.0277 ::: bot acc: 0.0257
top acc: 0.0330 ::: bot acc: 0.0126
top acc: 0.0350 ::: bot acc: 0.0228
current epoch: 48
train loss is 0.000607
average val loss: 0.000187, accuracy: 0.0147
average test loss: 0.000329, accuracy: 0.0197
case acc: 0.018149674
case acc: 0.0153439315
case acc: 0.026811799
case acc: 0.019514909
case acc: 0.017763609
case acc: 0.0207723
top acc: 0.0206 ::: bot acc: 0.0269
top acc: 0.0153 ::: bot acc: 0.0249
top acc: 0.0316 ::: bot acc: 0.0410
top acc: 0.0238 ::: bot acc: 0.0295
top acc: 0.0302 ::: bot acc: 0.0153
top acc: 0.0322 ::: bot acc: 0.0254
current epoch: 49
train loss is 0.000623
average val loss: 0.000216, accuracy: 0.0161
average test loss: 0.000358, accuracy: 0.0209
case acc: 0.020403547
case acc: 0.018388502
case acc: 0.027689762
case acc: 0.02067106
case acc: 0.01688653
case acc: 0.021196092
top acc: 0.0155 ::: bot acc: 0.0330
top acc: 0.0097 ::: bot acc: 0.0322
top acc: 0.0241 ::: bot acc: 0.0482
top acc: 0.0184 ::: bot acc: 0.0350
top acc: 0.0260 ::: bot acc: 0.0194
top acc: 0.0284 ::: bot acc: 0.0294
current epoch: 50
train loss is 0.000656
average val loss: 0.000285, accuracy: 0.0188
average test loss: 0.000429, accuracy: 0.0232
case acc: 0.023769759
case acc: 0.02365976
case acc: 0.030315807
case acc: 0.022824112
case acc: 0.016695667
case acc: 0.022013113
top acc: 0.0127 ::: bot acc: 0.0395
top acc: 0.0096 ::: bot acc: 0.0403
top acc: 0.0162 ::: bot acc: 0.0566
top acc: 0.0136 ::: bot acc: 0.0407
top acc: 0.0220 ::: bot acc: 0.0235
top acc: 0.0246 ::: bot acc: 0.0333
LME_Co_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6780 6780 6780
1.7082474 -0.6288155 0.12137239 -0.1537469
Validation: 756 756 756
Testing: 774 774 774
pre-processing time: 0.00020599365234375
the split date is 2010-07-01
train dropout: 0.6 test dropout: 0.05
net initializing with time: 0.0022306442260742188
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012799
average val loss: 0.005889, accuracy: 0.1020
average test loss: 0.006060, accuracy: 0.1025
case acc: 0.14646043
case acc: 0.078956485
case acc: 0.10520657
case acc: 0.09235452
case acc: 0.13384624
case acc: 0.058018353
top acc: 0.1232 ::: bot acc: 0.1680
top acc: 0.1014 ::: bot acc: 0.0580
top acc: 0.0723 ::: bot acc: 0.1371
top acc: 0.0598 ::: bot acc: 0.1238
top acc: 0.1013 ::: bot acc: 0.1620
top acc: 0.0300 ::: bot acc: 0.0848
current epoch: 2
train loss is 0.008189
average val loss: 0.002944, accuracy: 0.0541
average test loss: 0.003042, accuracy: 0.0562
case acc: 0.04291791
case acc: 0.16553962
case acc: 0.024268996
case acc: 0.023931747
case acc: 0.047805216
case acc: 0.032731224
top acc: 0.0210 ::: bot acc: 0.0648
top acc: 0.1874 ::: bot acc: 0.1445
top acc: 0.0317 ::: bot acc: 0.0343
top acc: 0.0378 ::: bot acc: 0.0277
top acc: 0.0225 ::: bot acc: 0.0723
top acc: 0.0593 ::: bot acc: 0.0113
current epoch: 3
train loss is 0.008525
average val loss: 0.007531, accuracy: 0.1009
average test loss: 0.007597, accuracy: 0.1005
case acc: 0.047970563
case acc: 0.2409049
case acc: 0.08855604
case acc: 0.086009845
case acc: 0.036759187
case acc: 0.10267019
top acc: 0.0704 ::: bot acc: 0.0270
top acc: 0.2630 ::: bot acc: 0.2199
top acc: 0.1212 ::: bot acc: 0.0570
top acc: 0.1201 ::: bot acc: 0.0540
top acc: 0.0683 ::: bot acc: 0.0110
top acc: 0.1325 ::: bot acc: 0.0753
current epoch: 4
train loss is 0.010011
average val loss: 0.016185, accuracy: 0.1676
average test loss: 0.016174, accuracy: 0.1669
case acc: 0.12173172
case acc: 0.30115983
case acc: 0.16100359
case acc: 0.15182562
case acc: 0.1045691
case acc: 0.16110347
top acc: 0.1446 ::: bot acc: 0.0997
top acc: 0.3232 ::: bot acc: 0.2800
top acc: 0.1945 ::: bot acc: 0.1283
top acc: 0.1858 ::: bot acc: 0.1197
top acc: 0.1375 ::: bot acc: 0.0761
top acc: 0.1908 ::: bot acc: 0.1334
current epoch: 5
train loss is 0.012984
average val loss: 0.012766, accuracy: 0.1475
average test loss: 0.012771, accuracy: 0.1469
case acc: 0.10700878
case acc: 0.27133256
case acc: 0.14364456
case acc: 0.13075349
case acc: 0.09256534
case acc: 0.1361772
top acc: 0.1299 ::: bot acc: 0.0851
top acc: 0.2931 ::: bot acc: 0.2506
top acc: 0.1763 ::: bot acc: 0.1110
top acc: 0.1645 ::: bot acc: 0.0986
top acc: 0.1260 ::: bot acc: 0.0639
top acc: 0.1659 ::: bot acc: 0.1087
current epoch: 6
train loss is 0.012850
average val loss: 0.003159, accuracy: 0.0579
average test loss: 0.003204, accuracy: 0.0586
case acc: 0.023103252
case acc: 0.16603611
case acc: 0.05284037
case acc: 0.041284706
case acc: 0.023443684
case acc: 0.045048047
top acc: 0.0423 ::: bot acc: 0.0087
top acc: 0.1881 ::: bot acc: 0.1450
top acc: 0.0834 ::: bot acc: 0.0255
top acc: 0.0722 ::: bot acc: 0.0153
top acc: 0.0460 ::: bot acc: 0.0160
top acc: 0.0737 ::: bot acc: 0.0191
current epoch: 7
train loss is 0.007161
average val loss: 0.001306, accuracy: 0.0407
average test loss: 0.001380, accuracy: 0.0422
case acc: 0.037665848
case acc: 0.09367778
case acc: 0.02677214
case acc: 0.03139957
case acc: 0.039797626
case acc: 0.023985934
top acc: 0.0163 ::: bot acc: 0.0588
top acc: 0.1153 ::: bot acc: 0.0727
top acc: 0.0198 ::: bot acc: 0.0463
top acc: 0.0200 ::: bot acc: 0.0520
top acc: 0.0183 ::: bot acc: 0.0625
top acc: 0.0185 ::: bot acc: 0.0394
current epoch: 8
train loss is 0.003345
average val loss: 0.001108, accuracy: 0.0350
average test loss: 0.001174, accuracy: 0.0368
case acc: 0.027951773
case acc: 0.09201616
case acc: 0.025053201
case acc: 0.026336951
case acc: 0.02831954
case acc: 0.021401778
top acc: 0.0102 ::: bot acc: 0.0473
top acc: 0.1138 ::: bot acc: 0.0712
top acc: 0.0248 ::: bot acc: 0.0409
top acc: 0.0252 ::: bot acc: 0.0420
top acc: 0.0197 ::: bot acc: 0.0447
top acc: 0.0307 ::: bot acc: 0.0270
current epoch: 9
train loss is 0.002441
average val loss: 0.001147, accuracy: 0.0340
average test loss: 0.001208, accuracy: 0.0353
case acc: 0.018154977
case acc: 0.09751274
case acc: 0.024237651
case acc: 0.024128234
case acc: 0.022968184
case acc: 0.024606911
top acc: 0.0143 ::: bot acc: 0.0308
top acc: 0.1194 ::: bot acc: 0.0768
top acc: 0.0367 ::: bot acc: 0.0294
top acc: 0.0393 ::: bot acc: 0.0274
top acc: 0.0374 ::: bot acc: 0.0249
top acc: 0.0466 ::: bot acc: 0.0129
current epoch: 10
train loss is 0.002301
average val loss: 0.001077, accuracy: 0.0336
average test loss: 0.001137, accuracy: 0.0345
case acc: 0.017274639
case acc: 0.09210905
case acc: 0.024289083
case acc: 0.024473656
case acc: 0.022942493
case acc: 0.026081795
top acc: 0.0182 ::: bot acc: 0.0268
top acc: 0.1143 ::: bot acc: 0.0709
top acc: 0.0374 ::: bot acc: 0.0288
top acc: 0.0421 ::: bot acc: 0.0248
top acc: 0.0428 ::: bot acc: 0.0192
top acc: 0.0495 ::: bot acc: 0.0113
current epoch: 11
train loss is 0.002147
average val loss: 0.000924, accuracy: 0.0318
average test loss: 0.000982, accuracy: 0.0326
case acc: 0.017354343
case acc: 0.082165785
case acc: 0.023874031
case acc: 0.024048014
case acc: 0.022962172
case acc: 0.025236629
top acc: 0.0173 ::: bot acc: 0.0276
top acc: 0.1045 ::: bot acc: 0.0611
top acc: 0.0328 ::: bot acc: 0.0328
top acc: 0.0397 ::: bot acc: 0.0265
top acc: 0.0431 ::: bot acc: 0.0190
top acc: 0.0479 ::: bot acc: 0.0120
current epoch: 12
train loss is 0.001852
average val loss: 0.000869, accuracy: 0.0314
average test loss: 0.000930, accuracy: 0.0321
case acc: 0.016934725
case acc: 0.076802894
case acc: 0.02395973
case acc: 0.024478856
case acc: 0.023713266
case acc: 0.026416639
top acc: 0.0206 ::: bot acc: 0.0248
top acc: 0.0987 ::: bot acc: 0.0558
top acc: 0.0332 ::: bot acc: 0.0327
top acc: 0.0421 ::: bot acc: 0.0245
top acc: 0.0471 ::: bot acc: 0.0148
top acc: 0.0505 ::: bot acc: 0.0106
current epoch: 13
train loss is 0.001636
average val loss: 0.000765, accuracy: 0.0299
average test loss: 0.000819, accuracy: 0.0305
case acc: 0.01667975
case acc: 0.06816
case acc: 0.024105283
case acc: 0.024248442
case acc: 0.023765827
case acc: 0.025778892
top acc: 0.0200 ::: bot acc: 0.0248
top acc: 0.0902 ::: bot acc: 0.0471
top acc: 0.0302 ::: bot acc: 0.0357
top acc: 0.0406 ::: bot acc: 0.0261
top acc: 0.0467 ::: bot acc: 0.0156
top acc: 0.0489 ::: bot acc: 0.0116
current epoch: 14
train loss is 0.001421
average val loss: 0.000701, accuracy: 0.0289
average test loss: 0.000757, accuracy: 0.0295
case acc: 0.016663544
case acc: 0.062246263
case acc: 0.02431605
case acc: 0.024162807
case acc: 0.02404424
case acc: 0.025717078
top acc: 0.0217 ::: bot acc: 0.0235
top acc: 0.0841 ::: bot acc: 0.0412
top acc: 0.0298 ::: bot acc: 0.0363
top acc: 0.0407 ::: bot acc: 0.0255
top acc: 0.0476 ::: bot acc: 0.0149
top acc: 0.0490 ::: bot acc: 0.0112
current epoch: 15
train loss is 0.001269
average val loss: 0.000655, accuracy: 0.0282
average test loss: 0.000712, accuracy: 0.0288
case acc: 0.016678944
case acc: 0.057324205
case acc: 0.024143707
case acc: 0.02439127
case acc: 0.024091454
case acc: 0.026109163
top acc: 0.0233 ::: bot acc: 0.0222
top acc: 0.0792 ::: bot acc: 0.0363
top acc: 0.0293 ::: bot acc: 0.0365
top acc: 0.0414 ::: bot acc: 0.0251
top acc: 0.0480 ::: bot acc: 0.0142
top acc: 0.0494 ::: bot acc: 0.0118
current epoch: 16
train loss is 0.001183
average val loss: 0.000661, accuracy: 0.0285
average test loss: 0.000717, accuracy: 0.0290
case acc: 0.016458567
case acc: 0.05578251
case acc: 0.023999684
case acc: 0.025128255
case acc: 0.025223065
case acc: 0.027389623
top acc: 0.0268 ::: bot acc: 0.0181
top acc: 0.0779 ::: bot acc: 0.0349
top acc: 0.0324 ::: bot acc: 0.0335
top acc: 0.0449 ::: bot acc: 0.0216
top acc: 0.0509 ::: bot acc: 0.0121
top acc: 0.0520 ::: bot acc: 0.0103
current epoch: 17
train loss is 0.001121
average val loss: 0.000638, accuracy: 0.0282
average test loss: 0.000692, accuracy: 0.0286
case acc: 0.016934358
case acc: 0.05249909
case acc: 0.023970261
case acc: 0.025390027
case acc: 0.02515072
case acc: 0.02752503
top acc: 0.0289 ::: bot acc: 0.0165
top acc: 0.0745 ::: bot acc: 0.0313
top acc: 0.0334 ::: bot acc: 0.0325
top acc: 0.0457 ::: bot acc: 0.0207
top acc: 0.0509 ::: bot acc: 0.0116
top acc: 0.0522 ::: bot acc: 0.0102
current epoch: 18
train loss is 0.001037
average val loss: 0.000622, accuracy: 0.0280
average test loss: 0.000675, accuracy: 0.0283
case acc: 0.017428394
case acc: 0.04987323
case acc: 0.024002615
case acc: 0.025930215
case acc: 0.025071891
case acc: 0.027787006
top acc: 0.0308 ::: bot acc: 0.0143
top acc: 0.0718 ::: bot acc: 0.0288
top acc: 0.0350 ::: bot acc: 0.0309
top acc: 0.0471 ::: bot acc: 0.0196
top acc: 0.0508 ::: bot acc: 0.0118
top acc: 0.0525 ::: bot acc: 0.0105
current epoch: 19
train loss is 0.001003
average val loss: 0.000609, accuracy: 0.0278
average test loss: 0.000667, accuracy: 0.0282
case acc: 0.01821849
case acc: 0.047746915
case acc: 0.024268646
case acc: 0.026350792
case acc: 0.025037037
case acc: 0.027875334
top acc: 0.0328 ::: bot acc: 0.0128
top acc: 0.0698 ::: bot acc: 0.0267
top acc: 0.0367 ::: bot acc: 0.0296
top acc: 0.0484 ::: bot acc: 0.0185
top acc: 0.0507 ::: bot acc: 0.0119
top acc: 0.0525 ::: bot acc: 0.0105
current epoch: 20
train loss is 0.000952
average val loss: 0.000609, accuracy: 0.0280
average test loss: 0.000665, accuracy: 0.0283
case acc: 0.018918008
case acc: 0.046375636
case acc: 0.024437003
case acc: 0.026702821
case acc: 0.025203705
case acc: 0.028008679
top acc: 0.0350 ::: bot acc: 0.0107
top acc: 0.0683 ::: bot acc: 0.0254
top acc: 0.0387 ::: bot acc: 0.0271
top acc: 0.0499 ::: bot acc: 0.0168
top acc: 0.0508 ::: bot acc: 0.0121
top acc: 0.0531 ::: bot acc: 0.0100
current epoch: 21
train loss is 0.000939
average val loss: 0.000626, accuracy: 0.0287
average test loss: 0.000680, accuracy: 0.0287
case acc: 0.02023736
case acc: 0.045744807
case acc: 0.024914032
case acc: 0.0275733
case acc: 0.025292879
case acc: 0.028606055
top acc: 0.0376 ::: bot acc: 0.0093
top acc: 0.0677 ::: bot acc: 0.0248
top acc: 0.0416 ::: bot acc: 0.0243
top acc: 0.0519 ::: bot acc: 0.0153
top acc: 0.0512 ::: bot acc: 0.0112
top acc: 0.0539 ::: bot acc: 0.0100
current epoch: 22
train loss is 0.000912
average val loss: 0.000674, accuracy: 0.0301
average test loss: 0.000726, accuracy: 0.0300
case acc: 0.022330934
case acc: 0.046622876
case acc: 0.025771808
case acc: 0.029058663
case acc: 0.026288254
case acc: 0.029700495
top acc: 0.0414 ::: bot acc: 0.0084
top acc: 0.0686 ::: bot acc: 0.0256
top acc: 0.0452 ::: bot acc: 0.0205
top acc: 0.0548 ::: bot acc: 0.0137
top acc: 0.0533 ::: bot acc: 0.0105
top acc: 0.0557 ::: bot acc: 0.0099
current epoch: 23
train loss is 0.000912
average val loss: 0.000717, accuracy: 0.0313
average test loss: 0.000768, accuracy: 0.0312
case acc: 0.024640646
case acc: 0.04682524
case acc: 0.027181216
case acc: 0.030409142
case acc: 0.027336726
case acc: 0.030680394
top acc: 0.0444 ::: bot acc: 0.0090
top acc: 0.0687 ::: bot acc: 0.0258
top acc: 0.0488 ::: bot acc: 0.0179
top acc: 0.0571 ::: bot acc: 0.0132
top acc: 0.0552 ::: bot acc: 0.0098
top acc: 0.0571 ::: bot acc: 0.0102
current epoch: 24
train loss is 0.000878
average val loss: 0.000659, accuracy: 0.0298
average test loss: 0.000714, accuracy: 0.0298
case acc: 0.0239033
case acc: 0.043613765
case acc: 0.02697853
case acc: 0.029571993
case acc: 0.025988324
case acc: 0.028873166
top acc: 0.0437 ::: bot acc: 0.0088
top acc: 0.0655 ::: bot acc: 0.0227
top acc: 0.0480 ::: bot acc: 0.0187
top acc: 0.0557 ::: bot acc: 0.0134
top acc: 0.0528 ::: bot acc: 0.0106
top acc: 0.0545 ::: bot acc: 0.0098
current epoch: 25
train loss is 0.000836
average val loss: 0.000695, accuracy: 0.0308
average test loss: 0.000744, accuracy: 0.0307
case acc: 0.0255124
case acc: 0.04392033
case acc: 0.02803002
case acc: 0.03053579
case acc: 0.026762454
case acc: 0.02943704
top acc: 0.0456 ::: bot acc: 0.0092
top acc: 0.0659 ::: bot acc: 0.0229
top acc: 0.0508 ::: bot acc: 0.0165
top acc: 0.0576 ::: bot acc: 0.0127
top acc: 0.0542 ::: bot acc: 0.0102
top acc: 0.0551 ::: bot acc: 0.0100
current epoch: 26
train loss is 0.000826
average val loss: 0.000701, accuracy: 0.0310
average test loss: 0.000757, accuracy: 0.0311
case acc: 0.026568564
case acc: 0.04345731
case acc: 0.028945131
case acc: 0.031194147
case acc: 0.026744792
case acc: 0.029412605
top acc: 0.0470 ::: bot acc: 0.0098
top acc: 0.0652 ::: bot acc: 0.0227
top acc: 0.0526 ::: bot acc: 0.0155
top acc: 0.0586 ::: bot acc: 0.0127
top acc: 0.0543 ::: bot acc: 0.0101
top acc: 0.0553 ::: bot acc: 0.0099
current epoch: 27
train loss is 0.000827
average val loss: 0.000721, accuracy: 0.0315
average test loss: 0.000772, accuracy: 0.0315
case acc: 0.027477453
case acc: 0.043201588
case acc: 0.029951805
case acc: 0.0318954
case acc: 0.02689261
case acc: 0.029426066
top acc: 0.0480 ::: bot acc: 0.0101
top acc: 0.0650 ::: bot acc: 0.0224
top acc: 0.0544 ::: bot acc: 0.0150
top acc: 0.0598 ::: bot acc: 0.0124
top acc: 0.0546 ::: bot acc: 0.0099
top acc: 0.0554 ::: bot acc: 0.0097
current epoch: 28
train loss is 0.000832
average val loss: 0.000781, accuracy: 0.0331
average test loss: 0.000834, accuracy: 0.0331
case acc: 0.029759593
case acc: 0.04449565
case acc: 0.03212927
case acc: 0.033597317
case acc: 0.027996138
case acc: 0.030795693
top acc: 0.0509 ::: bot acc: 0.0115
top acc: 0.0664 ::: bot acc: 0.0235
top acc: 0.0578 ::: bot acc: 0.0147
top acc: 0.0626 ::: bot acc: 0.0122
top acc: 0.0565 ::: bot acc: 0.0092
top acc: 0.0573 ::: bot acc: 0.0101
current epoch: 29
train loss is 0.000828
average val loss: 0.000826, accuracy: 0.0342
average test loss: 0.000875, accuracy: 0.0342
case acc: 0.03138988
case acc: 0.044898715
case acc: 0.03370949
case acc: 0.03476622
case acc: 0.028918924
case acc: 0.031393774
top acc: 0.0530 ::: bot acc: 0.0123
top acc: 0.0666 ::: bot acc: 0.0240
top acc: 0.0600 ::: bot acc: 0.0146
top acc: 0.0641 ::: bot acc: 0.0123
top acc: 0.0582 ::: bot acc: 0.0088
top acc: 0.0581 ::: bot acc: 0.0101
current epoch: 30
train loss is 0.000828
average val loss: 0.000861, accuracy: 0.0351
average test loss: 0.000911, accuracy: 0.0351
case acc: 0.032593753
case acc: 0.045222376
case acc: 0.03517202
case acc: 0.035695083
case acc: 0.029862886
case acc: 0.03200274
top acc: 0.0543 ::: bot acc: 0.0132
top acc: 0.0671 ::: bot acc: 0.0241
top acc: 0.0622 ::: bot acc: 0.0150
top acc: 0.0655 ::: bot acc: 0.0124
top acc: 0.0598 ::: bot acc: 0.0085
top acc: 0.0588 ::: bot acc: 0.0105
current epoch: 31
train loss is 0.000827
average val loss: 0.000900, accuracy: 0.0361
average test loss: 0.000951, accuracy: 0.0361
case acc: 0.033878423
case acc: 0.045639962
case acc: 0.036631104
case acc: 0.036751926
case acc: 0.030863026
case acc: 0.03270887
top acc: 0.0558 ::: bot acc: 0.0141
top acc: 0.0675 ::: bot acc: 0.0246
top acc: 0.0642 ::: bot acc: 0.0154
top acc: 0.0670 ::: bot acc: 0.0126
top acc: 0.0613 ::: bot acc: 0.0083
top acc: 0.0598 ::: bot acc: 0.0106
current epoch: 32
train loss is 0.000836
average val loss: 0.000915, accuracy: 0.0365
average test loss: 0.000963, accuracy: 0.0364
case acc: 0.03420342
case acc: 0.045170296
case acc: 0.03737938
case acc: 0.03739776
case acc: 0.03130529
case acc: 0.032786425
top acc: 0.0561 ::: bot acc: 0.0144
top acc: 0.0671 ::: bot acc: 0.0242
top acc: 0.0652 ::: bot acc: 0.0156
top acc: 0.0677 ::: bot acc: 0.0129
top acc: 0.0619 ::: bot acc: 0.0084
top acc: 0.0600 ::: bot acc: 0.0106
current epoch: 33
train loss is 0.000824
average val loss: 0.000934, accuracy: 0.0370
average test loss: 0.000982, accuracy: 0.0368
case acc: 0.03476914
case acc: 0.044667844
case acc: 0.03821605
case acc: 0.038140148
case acc: 0.03194205
case acc: 0.033248816
top acc: 0.0570 ::: bot acc: 0.0147
top acc: 0.0665 ::: bot acc: 0.0236
top acc: 0.0662 ::: bot acc: 0.0159
top acc: 0.0688 ::: bot acc: 0.0132
top acc: 0.0629 ::: bot acc: 0.0084
top acc: 0.0604 ::: bot acc: 0.0111
current epoch: 34
train loss is 0.000817
average val loss: 0.000878, accuracy: 0.0356
average test loss: 0.000928, accuracy: 0.0355
case acc: 0.03335116
case acc: 0.042168226
case acc: 0.03724331
case acc: 0.037196837
case acc: 0.03131346
case acc: 0.03193562
top acc: 0.0551 ::: bot acc: 0.0138
top acc: 0.0640 ::: bot acc: 0.0214
top acc: 0.0650 ::: bot acc: 0.0155
top acc: 0.0677 ::: bot acc: 0.0128
top acc: 0.0620 ::: bot acc: 0.0083
top acc: 0.0589 ::: bot acc: 0.0104
current epoch: 35
train loss is 0.000788
average val loss: 0.000825, accuracy: 0.0343
average test loss: 0.000878, accuracy: 0.0343
case acc: 0.031873014
case acc: 0.039655883
case acc: 0.036478214
case acc: 0.03631269
case acc: 0.030428095
case acc: 0.030868258
top acc: 0.0536 ::: bot acc: 0.0127
top acc: 0.0614 ::: bot acc: 0.0190
top acc: 0.0639 ::: bot acc: 0.0154
top acc: 0.0664 ::: bot acc: 0.0126
top acc: 0.0607 ::: bot acc: 0.0084
top acc: 0.0574 ::: bot acc: 0.0101
current epoch: 36
train loss is 0.000755
average val loss: 0.000821, accuracy: 0.0342
average test loss: 0.000870, accuracy: 0.0341
case acc: 0.031410545
case acc: 0.03869164
case acc: 0.03648444
case acc: 0.03654811
case acc: 0.030469581
case acc: 0.03090686
top acc: 0.0529 ::: bot acc: 0.0125
top acc: 0.0604 ::: bot acc: 0.0180
top acc: 0.0640 ::: bot acc: 0.0153
top acc: 0.0666 ::: bot acc: 0.0127
top acc: 0.0608 ::: bot acc: 0.0084
top acc: 0.0574 ::: bot acc: 0.0102
current epoch: 37
train loss is 0.000731
average val loss: 0.000734, accuracy: 0.0319
average test loss: 0.000788, accuracy: 0.0320
case acc: 0.02878298
case acc: 0.0352425
case acc: 0.03459053
case acc: 0.03492285
case acc: 0.028932245
case acc: 0.029352397
top acc: 0.0498 ::: bot acc: 0.0109
top acc: 0.0568 ::: bot acc: 0.0150
top acc: 0.0614 ::: bot acc: 0.0148
top acc: 0.0646 ::: bot acc: 0.0123
top acc: 0.0582 ::: bot acc: 0.0087
top acc: 0.0552 ::: bot acc: 0.0099
current epoch: 38
train loss is 0.000692
average val loss: 0.000650, accuracy: 0.0296
average test loss: 0.000705, accuracy: 0.0297
case acc: 0.026135234
case acc: 0.031479083
case acc: 0.032340482
case acc: 0.0330515
case acc: 0.027697593
case acc: 0.027696239
top acc: 0.0466 ::: bot acc: 0.0096
top acc: 0.0526 ::: bot acc: 0.0122
top acc: 0.0582 ::: bot acc: 0.0147
top acc: 0.0618 ::: bot acc: 0.0121
top acc: 0.0561 ::: bot acc: 0.0095
top acc: 0.0524 ::: bot acc: 0.0103
current epoch: 39
train loss is 0.000663
average val loss: 0.000579, accuracy: 0.0276
average test loss: 0.000634, accuracy: 0.0277
case acc: 0.023734229
case acc: 0.027969548
case acc: 0.030316535
case acc: 0.031416085
case acc: 0.02661247
case acc: 0.026335496
top acc: 0.0433 ::: bot acc: 0.0088
top acc: 0.0485 ::: bot acc: 0.0097
top acc: 0.0550 ::: bot acc: 0.0149
top acc: 0.0591 ::: bot acc: 0.0125
top acc: 0.0540 ::: bot acc: 0.0105
top acc: 0.0499 ::: bot acc: 0.0112
current epoch: 40
train loss is 0.000626
average val loss: 0.000555, accuracy: 0.0268
average test loss: 0.000610, accuracy: 0.0270
case acc: 0.022665683
case acc: 0.026570134
case acc: 0.029668976
case acc: 0.030874567
case acc: 0.026369635
case acc: 0.025766782
top acc: 0.0418 ::: bot acc: 0.0084
top acc: 0.0467 ::: bot acc: 0.0091
top acc: 0.0538 ::: bot acc: 0.0153
top acc: 0.0582 ::: bot acc: 0.0127
top acc: 0.0536 ::: bot acc: 0.0103
top acc: 0.0492 ::: bot acc: 0.0113
current epoch: 41
train loss is 0.000605
average val loss: 0.000488, accuracy: 0.0248
average test loss: 0.000548, accuracy: 0.0251
case acc: 0.020570563
case acc: 0.023461226
case acc: 0.027868917
case acc: 0.029313032
case acc: 0.025115257
case acc: 0.024417786
top acc: 0.0384 ::: bot acc: 0.0091
top acc: 0.0426 ::: bot acc: 0.0078
top acc: 0.0505 ::: bot acc: 0.0166
top acc: 0.0554 ::: bot acc: 0.0135
top acc: 0.0509 ::: bot acc: 0.0122
top acc: 0.0465 ::: bot acc: 0.0127
current epoch: 42
train loss is 0.000572
average val loss: 0.000425, accuracy: 0.0227
average test loss: 0.000486, accuracy: 0.0233
case acc: 0.018618222
case acc: 0.020397319
case acc: 0.026221892
case acc: 0.027650585
case acc: 0.023835447
case acc: 0.023018781
top acc: 0.0342 ::: bot acc: 0.0119
top acc: 0.0380 ::: bot acc: 0.0078
top acc: 0.0465 ::: bot acc: 0.0198
top acc: 0.0521 ::: bot acc: 0.0152
top acc: 0.0473 ::: bot acc: 0.0153
top acc: 0.0433 ::: bot acc: 0.0146
current epoch: 43
train loss is 0.000550
average val loss: 0.000383, accuracy: 0.0212
average test loss: 0.000447, accuracy: 0.0221
case acc: 0.0173785
case acc: 0.018295698
case acc: 0.02521982
case acc: 0.026540022
case acc: 0.023173263
case acc: 0.022074012
top acc: 0.0304 ::: bot acc: 0.0151
top acc: 0.0341 ::: bot acc: 0.0094
top acc: 0.0429 ::: bot acc: 0.0234
top acc: 0.0494 ::: bot acc: 0.0173
top acc: 0.0445 ::: bot acc: 0.0179
top acc: 0.0408 ::: bot acc: 0.0170
current epoch: 44
train loss is 0.000538
average val loss: 0.000371, accuracy: 0.0208
average test loss: 0.000434, accuracy: 0.0218
case acc: 0.017080177
case acc: 0.017426083
case acc: 0.024772573
case acc: 0.026137333
case acc: 0.02306773
case acc: 0.02202755
top acc: 0.0291 ::: bot acc: 0.0167
top acc: 0.0321 ::: bot acc: 0.0108
top acc: 0.0412 ::: bot acc: 0.0248
top acc: 0.0485 ::: bot acc: 0.0180
top acc: 0.0440 ::: bot acc: 0.0183
top acc: 0.0404 ::: bot acc: 0.0173
current epoch: 45
train loss is 0.000528
average val loss: 0.000352, accuracy: 0.0202
average test loss: 0.000418, accuracy: 0.0213
case acc: 0.016701639
case acc: 0.016555892
case acc: 0.024373118
case acc: 0.02558435
case acc: 0.022985987
case acc: 0.021800412
top acc: 0.0267 ::: bot acc: 0.0192
top acc: 0.0290 ::: bot acc: 0.0139
top acc: 0.0382 ::: bot acc: 0.0278
top acc: 0.0468 ::: bot acc: 0.0197
top acc: 0.0428 ::: bot acc: 0.0199
top acc: 0.0395 ::: bot acc: 0.0184
current epoch: 46
train loss is 0.000519
average val loss: 0.000326, accuracy: 0.0195
average test loss: 0.000396, accuracy: 0.0210
case acc: 0.016936162
case acc: 0.015977217
case acc: 0.024120538
case acc: 0.024585735
case acc: 0.022905864
case acc: 0.021340292
top acc: 0.0217 ::: bot acc: 0.0240
top acc: 0.0236 ::: bot acc: 0.0196
top acc: 0.0330 ::: bot acc: 0.0331
top acc: 0.0430 ::: bot acc: 0.0237
top acc: 0.0389 ::: bot acc: 0.0237
top acc: 0.0360 ::: bot acc: 0.0217
current epoch: 47
train loss is 0.000516
average val loss: 0.000322, accuracy: 0.0194
average test loss: 0.000392, accuracy: 0.0211
case acc: 0.017575454
case acc: 0.016050098
case acc: 0.024462475
case acc: 0.024082985
case acc: 0.022926293
case acc: 0.02125644
top acc: 0.0183 ::: bot acc: 0.0276
top acc: 0.0192 ::: bot acc: 0.0239
top acc: 0.0287 ::: bot acc: 0.0374
top acc: 0.0399 ::: bot acc: 0.0265
top acc: 0.0365 ::: bot acc: 0.0260
top acc: 0.0339 ::: bot acc: 0.0239
current epoch: 48
train loss is 0.000518
average val loss: 0.000329, accuracy: 0.0198
average test loss: 0.000402, accuracy: 0.0215
case acc: 0.018475654
case acc: 0.017131524
case acc: 0.025307512
case acc: 0.023840962
case acc: 0.023118917
case acc: 0.021272251
top acc: 0.0148 ::: bot acc: 0.0311
top acc: 0.0147 ::: bot acc: 0.0283
top acc: 0.0243 ::: bot acc: 0.0418
top acc: 0.0371 ::: bot acc: 0.0293
top acc: 0.0345 ::: bot acc: 0.0280
top acc: 0.0320 ::: bot acc: 0.0256
current epoch: 49
train loss is 0.000529
average val loss: 0.000356, accuracy: 0.0208
average test loss: 0.000433, accuracy: 0.0227
case acc: 0.020456735
case acc: 0.01925474
case acc: 0.027550146
case acc: 0.023911031
case acc: 0.023637583
case acc: 0.021457927
top acc: 0.0114 ::: bot acc: 0.0357
top acc: 0.0094 ::: bot acc: 0.0341
top acc: 0.0190 ::: bot acc: 0.0479
top acc: 0.0328 ::: bot acc: 0.0334
top acc: 0.0313 ::: bot acc: 0.0313
top acc: 0.0296 ::: bot acc: 0.0282
current epoch: 50
train loss is 0.000550
average val loss: 0.000422, accuracy: 0.0230
average test loss: 0.000501, accuracy: 0.0251
case acc: 0.024004312
case acc: 0.023563541
case acc: 0.031217642
case acc: 0.025100598
case acc: 0.024657864
case acc: 0.022010526
top acc: 0.0099 ::: bot acc: 0.0418
top acc: 0.0078 ::: bot acc: 0.0415
top acc: 0.0151 ::: bot acc: 0.0554
top acc: 0.0278 ::: bot acc: 0.0385
top acc: 0.0271 ::: bot acc: 0.0353
top acc: 0.0256 ::: bot acc: 0.0321
LME_Co_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6804 6804 6804
1.7082474 -0.6288155 0.12137239 -0.15229516
Validation: 762 762 762
Testing: 750 750 750
pre-processing time: 0.0002048015594482422
the split date is 2011-01-01
train dropout: 0.6 test dropout: 0.05
net initializing with time: 0.0021524429321289062
preparing training and testing date with time: 0.0
current epoch: 1
train loss is 0.012884
average val loss: 0.005538, accuracy: 0.0976
average test loss: 0.004883, accuracy: 0.0916
case acc: 0.13229673
case acc: 0.09161153
case acc: 0.0851758
case acc: 0.07901629
case acc: 0.112642944
case acc: 0.048793204
top acc: 0.1081 ::: bot acc: 0.1581
top acc: 0.1135 ::: bot acc: 0.0684
top acc: 0.0490 ::: bot acc: 0.1208
top acc: 0.0534 ::: bot acc: 0.1053
top acc: 0.0772 ::: bot acc: 0.1482
top acc: 0.0200 ::: bot acc: 0.0777
current epoch: 2
train loss is 0.007976
average val loss: 0.003155, accuracy: 0.0563
average test loss: 0.003221, accuracy: 0.0553
case acc: 0.03158494
case acc: 0.17643428
case acc: 0.02902173
case acc: 0.022863239
case acc: 0.032581583
case acc: 0.039136726
top acc: 0.0096 ::: bot acc: 0.0566
top acc: 0.1975 ::: bot acc: 0.1536
top acc: 0.0529 ::: bot acc: 0.0195
top acc: 0.0413 ::: bot acc: 0.0127
top acc: 0.0142 ::: bot acc: 0.0600
top acc: 0.0667 ::: bot acc: 0.0148
current epoch: 3
train loss is 0.008389
average val loss: 0.008448, accuracy: 0.1087
average test loss: 0.009192, accuracy: 0.1162
case acc: 0.06263116
case acc: 0.25472772
case acc: 0.108946495
case acc: 0.09977622
case acc: 0.058159746
case acc: 0.11299341
top acc: 0.0872 ::: bot acc: 0.0365
top acc: 0.2763 ::: bot acc: 0.2325
top acc: 0.1454 ::: bot acc: 0.0732
top acc: 0.1264 ::: bot acc: 0.0726
top acc: 0.0926 ::: bot acc: 0.0232
top acc: 0.1437 ::: bot acc: 0.0826
current epoch: 4
train loss is 0.010298
average val loss: 0.017348, accuracy: 0.1740
average test loss: 0.018523, accuracy: 0.1812
case acc: 0.13490812
case acc: 0.31318504
case acc: 0.17988688
case acc: 0.16423382
case acc: 0.12463954
case acc: 0.17043331
top acc: 0.1593 ::: bot acc: 0.1093
top acc: 0.3339 ::: bot acc: 0.2914
top acc: 0.2169 ::: bot acc: 0.1439
top acc: 0.1906 ::: bot acc: 0.1370
top acc: 0.1595 ::: bot acc: 0.0897
top acc: 0.2007 ::: bot acc: 0.1405
current epoch: 5
train loss is 0.012955
average val loss: 0.013278, accuracy: 0.1504
average test loss: 0.014301, accuracy: 0.1577
case acc: 0.11663116
case acc: 0.27968752
case acc: 0.15893832
case acc: 0.13960484
case acc: 0.1089041
case acc: 0.14215602
top acc: 0.1415 ::: bot acc: 0.0906
top acc: 0.3011 ::: bot acc: 0.2572
top acc: 0.1955 ::: bot acc: 0.1228
top acc: 0.1669 ::: bot acc: 0.1115
top acc: 0.1437 ::: bot acc: 0.0739
top acc: 0.1723 ::: bot acc: 0.1120
current epoch: 6
train loss is 0.012510
average val loss: 0.003469, accuracy: 0.0624
average test loss: 0.003851, accuracy: 0.0685
case acc: 0.031829417
case acc: 0.17572095
case acc: 0.06723265
case acc: 0.048712805
case acc: 0.035108197
case acc: 0.05220449
top acc: 0.0543 ::: bot acc: 0.0098
top acc: 0.1967 ::: bot acc: 0.1536
top acc: 0.1033 ::: bot acc: 0.0325
top acc: 0.0752 ::: bot acc: 0.0216
top acc: 0.0653 ::: bot acc: 0.0100
top acc: 0.0814 ::: bot acc: 0.0244
current epoch: 7
train loss is 0.007302
average val loss: 0.001368, accuracy: 0.0414
average test loss: 0.001316, accuracy: 0.0385
case acc: 0.02969601
case acc: 0.100842565
case acc: 0.026152622
case acc: 0.022204012
case acc: 0.029779198
case acc: 0.022356646
top acc: 0.0081 ::: bot acc: 0.0539
top acc: 0.1219 ::: bot acc: 0.0788
top acc: 0.0373 ::: bot acc: 0.0356
top acc: 0.0156 ::: bot acc: 0.0396
top acc: 0.0177 ::: bot acc: 0.0537
top acc: 0.0229 ::: bot acc: 0.0367
current epoch: 8
train loss is 0.003288
average val loss: 0.001187, accuracy: 0.0363
average test loss: 0.001220, accuracy: 0.0359
case acc: 0.021553924
case acc: 0.10010511
case acc: 0.026905512
case acc: 0.019394973
case acc: 0.025436714
case acc: 0.022125134
top acc: 0.0092 ::: bot acc: 0.0413
top acc: 0.1211 ::: bot acc: 0.0779
top acc: 0.0438 ::: bot acc: 0.0296
top acc: 0.0256 ::: bot acc: 0.0285
top acc: 0.0352 ::: bot acc: 0.0347
top acc: 0.0360 ::: bot acc: 0.0239
current epoch: 9
train loss is 0.002435
average val loss: 0.001346, accuracy: 0.0370
average test loss: 0.001507, accuracy: 0.0409
case acc: 0.018980682
case acc: 0.108949415
case acc: 0.031478092
case acc: 0.024091233
case acc: 0.031556234
case acc: 0.030234557
top acc: 0.0289 ::: bot acc: 0.0215
top acc: 0.1299 ::: bot acc: 0.0869
top acc: 0.0584 ::: bot acc: 0.0154
top acc: 0.0436 ::: bot acc: 0.0117
top acc: 0.0580 ::: bot acc: 0.0132
top acc: 0.0553 ::: bot acc: 0.0111
current epoch: 10
train loss is 0.002286
average val loss: 0.001270, accuracy: 0.0365
average test loss: 0.001454, accuracy: 0.0411
case acc: 0.01971461
case acc: 0.103331715
case acc: 0.031738836
case acc: 0.025229221
case acc: 0.034394447
case acc: 0.03197471
top acc: 0.0324 ::: bot acc: 0.0177
top acc: 0.1243 ::: bot acc: 0.0813
top acc: 0.0587 ::: bot acc: 0.0152
top acc: 0.0457 ::: bot acc: 0.0107
top acc: 0.0637 ::: bot acc: 0.0106
top acc: 0.0577 ::: bot acc: 0.0113
current epoch: 11
train loss is 0.002103
average val loss: 0.001060, accuracy: 0.0338
average test loss: 0.001210, accuracy: 0.0377
case acc: 0.019027974
case acc: 0.09160245
case acc: 0.029337978
case acc: 0.023256522
case acc: 0.03363559
case acc: 0.02946754
top acc: 0.0299 ::: bot acc: 0.0199
top acc: 0.1131 ::: bot acc: 0.0693
top acc: 0.0531 ::: bot acc: 0.0200
top acc: 0.0419 ::: bot acc: 0.0127
top acc: 0.0624 ::: bot acc: 0.0113
top acc: 0.0540 ::: bot acc: 0.0113
current epoch: 12
train loss is 0.001806
average val loss: 0.001051, accuracy: 0.0342
average test loss: 0.001221, accuracy: 0.0387
case acc: 0.020346858
case acc: 0.08792574
case acc: 0.029976659
case acc: 0.025018463
case acc: 0.036859173
case acc: 0.03219164
top acc: 0.0345 ::: bot acc: 0.0154
top acc: 0.1089 ::: bot acc: 0.0660
top acc: 0.0549 ::: bot acc: 0.0180
top acc: 0.0453 ::: bot acc: 0.0108
top acc: 0.0681 ::: bot acc: 0.0094
top acc: 0.0579 ::: bot acc: 0.0112
current epoch: 13
train loss is 0.001622
average val loss: 0.001008, accuracy: 0.0339
average test loss: 0.001177, accuracy: 0.0386
case acc: 0.021278784
case acc: 0.08268564
case acc: 0.03007682
case acc: 0.025843851
case acc: 0.038499177
case acc: 0.03312506
top acc: 0.0374 ::: bot acc: 0.0127
top acc: 0.1037 ::: bot acc: 0.0607
top acc: 0.0552 ::: bot acc: 0.0180
top acc: 0.0470 ::: bot acc: 0.0100
top acc: 0.0705 ::: bot acc: 0.0095
top acc: 0.0591 ::: bot acc: 0.0117
current epoch: 14
train loss is 0.001451
average val loss: 0.000924, accuracy: 0.0328
average test loss: 0.001087, accuracy: 0.0374
case acc: 0.021665016
case acc: 0.07623759
case acc: 0.029469078
case acc: 0.025708338
case acc: 0.038607482
case acc: 0.032911547
top acc: 0.0382 ::: bot acc: 0.0122
top acc: 0.0974 ::: bot acc: 0.0544
top acc: 0.0538 ::: bot acc: 0.0186
top acc: 0.0467 ::: bot acc: 0.0104
top acc: 0.0704 ::: bot acc: 0.0099
top acc: 0.0589 ::: bot acc: 0.0117
current epoch: 15
train loss is 0.001300
average val loss: 0.000859, accuracy: 0.0318
average test loss: 0.001019, accuracy: 0.0365
case acc: 0.022041064
case acc: 0.07085681
case acc: 0.02922898
case acc: 0.025720824
case acc: 0.0383074
case acc: 0.03266227
top acc: 0.0390 ::: bot acc: 0.0118
top acc: 0.0919 ::: bot acc: 0.0489
top acc: 0.0534 ::: bot acc: 0.0193
top acc: 0.0468 ::: bot acc: 0.0099
top acc: 0.0703 ::: bot acc: 0.0096
top acc: 0.0583 ::: bot acc: 0.0118
current epoch: 16
train loss is 0.001201
average val loss: 0.000825, accuracy: 0.0314
average test loss: 0.000985, accuracy: 0.0361
case acc: 0.022493811
case acc: 0.066799164
case acc: 0.029522574
case acc: 0.02645502
case acc: 0.038372178
case acc: 0.03300563
top acc: 0.0405 ::: bot acc: 0.0101
top acc: 0.0880 ::: bot acc: 0.0448
top acc: 0.0540 ::: bot acc: 0.0186
top acc: 0.0481 ::: bot acc: 0.0098
top acc: 0.0702 ::: bot acc: 0.0096
top acc: 0.0591 ::: bot acc: 0.0115
current epoch: 17
train loss is 0.001107
average val loss: 0.000825, accuracy: 0.0317
average test loss: 0.000990, accuracy: 0.0365
case acc: 0.023964478
case acc: 0.06434199
case acc: 0.030079156
case acc: 0.027694894
case acc: 0.039192703
case acc: 0.033807397
top acc: 0.0429 ::: bot acc: 0.0093
top acc: 0.0852 ::: bot acc: 0.0426
top acc: 0.0556 ::: bot acc: 0.0169
top acc: 0.0499 ::: bot acc: 0.0097
top acc: 0.0711 ::: bot acc: 0.0099
top acc: 0.0604 ::: bot acc: 0.0117
current epoch: 18
train loss is 0.001049
average val loss: 0.000777, accuracy: 0.0308
average test loss: 0.000937, accuracy: 0.0357
case acc: 0.024285408
case acc: 0.060303114
case acc: 0.030289588
case acc: 0.027594278
case acc: 0.03817744
case acc: 0.03326658
top acc: 0.0436 ::: bot acc: 0.0092
top acc: 0.0813 ::: bot acc: 0.0385
top acc: 0.0559 ::: bot acc: 0.0171
top acc: 0.0499 ::: bot acc: 0.0095
top acc: 0.0697 ::: bot acc: 0.0098
top acc: 0.0593 ::: bot acc: 0.0117
current epoch: 19
train loss is 0.000977
average val loss: 0.000776, accuracy: 0.0309
average test loss: 0.000943, accuracy: 0.0359
case acc: 0.025465481
case acc: 0.058438104
case acc: 0.030991415
case acc: 0.028809885
case acc: 0.038287606
case acc: 0.033611502
top acc: 0.0457 ::: bot acc: 0.0085
top acc: 0.0796 ::: bot acc: 0.0364
top acc: 0.0577 ::: bot acc: 0.0157
top acc: 0.0516 ::: bot acc: 0.0099
top acc: 0.0701 ::: bot acc: 0.0095
top acc: 0.0599 ::: bot acc: 0.0118
current epoch: 20
train loss is 0.000943
average val loss: 0.000779, accuracy: 0.0312
average test loss: 0.000951, accuracy: 0.0363
case acc: 0.026733004
case acc: 0.056762792
case acc: 0.031967856
case acc: 0.029721588
case acc: 0.038690574
case acc: 0.033985518
top acc: 0.0477 ::: bot acc: 0.0081
top acc: 0.0779 ::: bot acc: 0.0350
top acc: 0.0596 ::: bot acc: 0.0144
top acc: 0.0528 ::: bot acc: 0.0097
top acc: 0.0703 ::: bot acc: 0.0098
top acc: 0.0601 ::: bot acc: 0.0121
current epoch: 21
train loss is 0.000908
average val loss: 0.000758, accuracy: 0.0307
average test loss: 0.000924, accuracy: 0.0358
case acc: 0.027471919
case acc: 0.053969577
case acc: 0.03233023
case acc: 0.030004516
case acc: 0.03789637
case acc: 0.03335956
top acc: 0.0489 ::: bot acc: 0.0083
top acc: 0.0750 ::: bot acc: 0.0321
top acc: 0.0604 ::: bot acc: 0.0142
top acc: 0.0534 ::: bot acc: 0.0098
top acc: 0.0695 ::: bot acc: 0.0098
top acc: 0.0593 ::: bot acc: 0.0119
current epoch: 22
train loss is 0.000882
average val loss: 0.000735, accuracy: 0.0302
average test loss: 0.000902, accuracy: 0.0354
case acc: 0.027992642
case acc: 0.051845007
case acc: 0.03273317
case acc: 0.029980078
case acc: 0.037200496
case acc: 0.03264734
top acc: 0.0495 ::: bot acc: 0.0085
top acc: 0.0729 ::: bot acc: 0.0299
top acc: 0.0611 ::: bot acc: 0.0138
top acc: 0.0532 ::: bot acc: 0.0098
top acc: 0.0683 ::: bot acc: 0.0097
top acc: 0.0587 ::: bot acc: 0.0115
current epoch: 23
train loss is 0.000845
average val loss: 0.000802, accuracy: 0.0321
average test loss: 0.000994, accuracy: 0.0376
case acc: 0.030977987
case acc: 0.053353433
case acc: 0.035382327
case acc: 0.03260648
case acc: 0.039051343
case acc: 0.034267694
top acc: 0.0531 ::: bot acc: 0.0098
top acc: 0.0743 ::: bot acc: 0.0316
top acc: 0.0656 ::: bot acc: 0.0127
top acc: 0.0567 ::: bot acc: 0.0107
top acc: 0.0709 ::: bot acc: 0.0100
top acc: 0.0608 ::: bot acc: 0.0118
current epoch: 24
train loss is 0.000824
average val loss: 0.000786, accuracy: 0.0318
average test loss: 0.000975, accuracy: 0.0373
case acc: 0.031487066
case acc: 0.051567838
case acc: 0.036021736
case acc: 0.03264422
case acc: 0.038355377
case acc: 0.033543646
top acc: 0.0539 ::: bot acc: 0.0098
top acc: 0.0724 ::: bot acc: 0.0300
top acc: 0.0664 ::: bot acc: 0.0128
top acc: 0.0568 ::: bot acc: 0.0107
top acc: 0.0700 ::: bot acc: 0.0096
top acc: 0.0597 ::: bot acc: 0.0119
current epoch: 25
train loss is 0.000819
average val loss: 0.000831, accuracy: 0.0330
average test loss: 0.001036, accuracy: 0.0387
case acc: 0.033743966
case acc: 0.052076153
case acc: 0.037972216
case acc: 0.03448972
case acc: 0.039291773
case acc: 0.034450203
top acc: 0.0568 ::: bot acc: 0.0113
top acc: 0.0730 ::: bot acc: 0.0302
top acc: 0.0695 ::: bot acc: 0.0126
top acc: 0.0590 ::: bot acc: 0.0116
top acc: 0.0714 ::: bot acc: 0.0099
top acc: 0.0611 ::: bot acc: 0.0122
current epoch: 26
train loss is 0.000822
average val loss: 0.000867, accuracy: 0.0339
average test loss: 0.001081, accuracy: 0.0397
case acc: 0.03551407
case acc: 0.05222863
case acc: 0.039700415
case acc: 0.035819843
case acc: 0.039993487
case acc: 0.034936674
top acc: 0.0586 ::: bot acc: 0.0125
top acc: 0.0733 ::: bot acc: 0.0304
top acc: 0.0720 ::: bot acc: 0.0126
top acc: 0.0608 ::: bot acc: 0.0122
top acc: 0.0722 ::: bot acc: 0.0102
top acc: 0.0617 ::: bot acc: 0.0122
current epoch: 27
train loss is 0.000810
average val loss: 0.000967, accuracy: 0.0364
average test loss: 0.001204, accuracy: 0.0423
case acc: 0.038700987
case acc: 0.05443415
case acc: 0.042838573
case acc: 0.038722355
case acc: 0.042246006
case acc: 0.03678172
top acc: 0.0624 ::: bot acc: 0.0146
top acc: 0.0753 ::: bot acc: 0.0325
top acc: 0.0763 ::: bot acc: 0.0134
top acc: 0.0642 ::: bot acc: 0.0139
top acc: 0.0753 ::: bot acc: 0.0109
top acc: 0.0642 ::: bot acc: 0.0130
current epoch: 28
train loss is 0.000834
average val loss: 0.000925, accuracy: 0.0354
average test loss: 0.001159, accuracy: 0.0414
case acc: 0.038110595
case acc: 0.05254846
case acc: 0.043049242
case acc: 0.038015008
case acc: 0.041002333
case acc: 0.03540622
top acc: 0.0616 ::: bot acc: 0.0142
top acc: 0.0737 ::: bot acc: 0.0305
top acc: 0.0765 ::: bot acc: 0.0136
top acc: 0.0634 ::: bot acc: 0.0136
top acc: 0.0737 ::: bot acc: 0.0102
top acc: 0.0623 ::: bot acc: 0.0124
current epoch: 29
train loss is 0.000827
average val loss: 0.001031, accuracy: 0.0380
average test loss: 0.001286, accuracy: 0.0441
case acc: 0.04149745
case acc: 0.05478332
case acc: 0.04647639
case acc: 0.0411781
case acc: 0.043231
case acc: 0.037259016
top acc: 0.0655 ::: bot acc: 0.0170
top acc: 0.0759 ::: bot acc: 0.0328
top acc: 0.0807 ::: bot acc: 0.0155
top acc: 0.0669 ::: bot acc: 0.0160
top acc: 0.0765 ::: bot acc: 0.0113
top acc: 0.0646 ::: bot acc: 0.0132
current epoch: 30
train loss is 0.000859
average val loss: 0.001104, accuracy: 0.0396
average test loss: 0.001376, accuracy: 0.0459
case acc: 0.043670073
case acc: 0.056049947
case acc: 0.049107805
case acc: 0.04322924
case acc: 0.044687457
case acc: 0.03846903
top acc: 0.0677 ::: bot acc: 0.0190
top acc: 0.0771 ::: bot acc: 0.0342
top acc: 0.0838 ::: bot acc: 0.0171
top acc: 0.0694 ::: bot acc: 0.0175
top acc: 0.0785 ::: bot acc: 0.0119
top acc: 0.0662 ::: bot acc: 0.0138
current epoch: 31
train loss is 0.000855
average val loss: 0.001175, accuracy: 0.0413
average test loss: 0.001466, accuracy: 0.0477
case acc: 0.045643512
case acc: 0.057264794
case acc: 0.05184635
case acc: 0.045313187
case acc: 0.046345253
case acc: 0.03959235
top acc: 0.0697 ::: bot acc: 0.0208
top acc: 0.0783 ::: bot acc: 0.0354
top acc: 0.0870 ::: bot acc: 0.0190
top acc: 0.0716 ::: bot acc: 0.0193
top acc: 0.0804 ::: bot acc: 0.0131
top acc: 0.0674 ::: bot acc: 0.0145
current epoch: 32
train loss is 0.000852
average val loss: 0.001239, accuracy: 0.0427
average test loss: 0.001543, accuracy: 0.0491
case acc: 0.047376707
case acc: 0.057993162
case acc: 0.05409299
case acc: 0.04705292
case acc: 0.047959235
case acc: 0.04032333
top acc: 0.0718 ::: bot acc: 0.0222
top acc: 0.0791 ::: bot acc: 0.0361
top acc: 0.0894 ::: bot acc: 0.0210
top acc: 0.0734 ::: bot acc: 0.0207
top acc: 0.0821 ::: bot acc: 0.0142
top acc: 0.0684 ::: bot acc: 0.0150
current epoch: 33
train loss is 0.000876
average val loss: 0.001309, accuracy: 0.0442
average test loss: 0.001622, accuracy: 0.0507
case acc: 0.049075186
case acc: 0.058676515
case acc: 0.056192603
case acc: 0.04875492
case acc: 0.050045762
case acc: 0.041428555
top acc: 0.0733 ::: bot acc: 0.0240
top acc: 0.0796 ::: bot acc: 0.0370
top acc: 0.0916 ::: bot acc: 0.0229
top acc: 0.0752 ::: bot acc: 0.0223
top acc: 0.0844 ::: bot acc: 0.0159
top acc: 0.0696 ::: bot acc: 0.0157
current epoch: 34
train loss is 0.000883
average val loss: 0.001266, accuracy: 0.0433
average test loss: 0.001575, accuracy: 0.0498
case acc: 0.048001446
case acc: 0.05672529
case acc: 0.055763956
case acc: 0.048100498
case acc: 0.04972592
case acc: 0.040390704
top acc: 0.0721 ::: bot acc: 0.0230
top acc: 0.0779 ::: bot acc: 0.0347
top acc: 0.0912 ::: bot acc: 0.0224
top acc: 0.0746 ::: bot acc: 0.0216
top acc: 0.0841 ::: bot acc: 0.0156
top acc: 0.0683 ::: bot acc: 0.0151
current epoch: 35
train loss is 0.000832
average val loss: 0.001223, accuracy: 0.0424
average test loss: 0.001528, accuracy: 0.0489
case acc: 0.046872426
case acc: 0.0548346
case acc: 0.055253748
case acc: 0.047172993
case acc: 0.04961336
case acc: 0.039441176
top acc: 0.0710 ::: bot acc: 0.0219
top acc: 0.0759 ::: bot acc: 0.0329
top acc: 0.0906 ::: bot acc: 0.0221
top acc: 0.0735 ::: bot acc: 0.0209
top acc: 0.0839 ::: bot acc: 0.0154
top acc: 0.0672 ::: bot acc: 0.0146
current epoch: 36
train loss is 0.000822
average val loss: 0.001137, accuracy: 0.0405
average test loss: 0.001428, accuracy: 0.0469
case acc: 0.044520024
case acc: 0.051493656
case acc: 0.05336193
case acc: 0.045662854
case acc: 0.04849625
case acc: 0.037718304
top acc: 0.0686 ::: bot acc: 0.0197
top acc: 0.0726 ::: bot acc: 0.0296
top acc: 0.0887 ::: bot acc: 0.0203
top acc: 0.0718 ::: bot acc: 0.0195
top acc: 0.0828 ::: bot acc: 0.0145
top acc: 0.0652 ::: bot acc: 0.0135
current epoch: 37
train loss is 0.000796
average val loss: 0.001075, accuracy: 0.0390
average test loss: 0.001353, accuracy: 0.0454
case acc: 0.0426637
case acc: 0.04849573
case acc: 0.051680863
case acc: 0.044626355
case acc: 0.047852047
case acc: 0.036838148
top acc: 0.0667 ::: bot acc: 0.0181
top acc: 0.0695 ::: bot acc: 0.0268
top acc: 0.0869 ::: bot acc: 0.0188
top acc: 0.0708 ::: bot acc: 0.0186
top acc: 0.0821 ::: bot acc: 0.0141
top acc: 0.0641 ::: bot acc: 0.0129
current epoch: 38
train loss is 0.000740
average val loss: 0.000940, accuracy: 0.0358
average test loss: 0.001192, accuracy: 0.0420
case acc: 0.038807444
case acc: 0.043587033
case acc: 0.048258815
case acc: 0.041470427
case acc: 0.04540375
case acc: 0.03443037
top acc: 0.0625 ::: bot acc: 0.0147
top acc: 0.0644 ::: bot acc: 0.0223
top acc: 0.0830 ::: bot acc: 0.0165
top acc: 0.0675 ::: bot acc: 0.0161
top acc: 0.0791 ::: bot acc: 0.0126
top acc: 0.0611 ::: bot acc: 0.0119
current epoch: 39
train loss is 0.000689
average val loss: 0.000838, accuracy: 0.0332
average test loss: 0.001070, accuracy: 0.0393
case acc: 0.035675485
case acc: 0.039325774
case acc: 0.04548087
case acc: 0.039215907
case acc: 0.043612756
case acc: 0.032598242
top acc: 0.0589 ::: bot acc: 0.0126
top acc: 0.0600 ::: bot acc: 0.0185
top acc: 0.0796 ::: bot acc: 0.0149
top acc: 0.0649 ::: bot acc: 0.0143
top acc: 0.0770 ::: bot acc: 0.0114
top acc: 0.0585 ::: bot acc: 0.0116
current epoch: 40
train loss is 0.000639
average val loss: 0.000690, accuracy: 0.0292
average test loss: 0.000883, accuracy: 0.0350
case acc: 0.030784853
case acc: 0.03347051
case acc: 0.04092485
case acc: 0.03510215
case acc: 0.04013089
case acc: 0.029514583
top acc: 0.0530 ::: bot acc: 0.0097
top acc: 0.0533 ::: bot acc: 0.0142
top acc: 0.0739 ::: bot acc: 0.0128
top acc: 0.0599 ::: bot acc: 0.0119
top acc: 0.0725 ::: bot acc: 0.0102
top acc: 0.0542 ::: bot acc: 0.0111
current epoch: 41
train loss is 0.000590
average val loss: 0.000611, accuracy: 0.0270
average test loss: 0.000778, accuracy: 0.0324
case acc: 0.027838213
case acc: 0.029390031
case acc: 0.038163956
case acc: 0.032875746
case acc: 0.038248762
case acc: 0.027972152
top acc: 0.0493 ::: bot acc: 0.0086
top acc: 0.0487 ::: bot acc: 0.0115
top acc: 0.0698 ::: bot acc: 0.0126
top acc: 0.0573 ::: bot acc: 0.0108
top acc: 0.0699 ::: bot acc: 0.0099
top acc: 0.0515 ::: bot acc: 0.0117
current epoch: 42
train loss is 0.000560
average val loss: 0.000504, accuracy: 0.0238
average test loss: 0.000626, accuracy: 0.0285
case acc: 0.023725515
case acc: 0.023854861
case acc: 0.03375467
case acc: 0.029101301
case acc: 0.034888197
case acc: 0.025704697
top acc: 0.0427 ::: bot acc: 0.0094
top acc: 0.0415 ::: bot acc: 0.0090
top acc: 0.0630 ::: bot acc: 0.0131
top acc: 0.0522 ::: bot acc: 0.0097
top acc: 0.0646 ::: bot acc: 0.0103
top acc: 0.0471 ::: bot acc: 0.0139
current epoch: 43
train loss is 0.000527
average val loss: 0.000437, accuracy: 0.0218
average test loss: 0.000519, accuracy: 0.0256
case acc: 0.021007514
case acc: 0.01957038
case acc: 0.030599177
case acc: 0.026018051
case acc: 0.03241067
case acc: 0.024008336
top acc: 0.0367 ::: bot acc: 0.0130
top acc: 0.0349 ::: bot acc: 0.0097
top acc: 0.0567 ::: bot acc: 0.0162
top acc: 0.0473 ::: bot acc: 0.0101
top acc: 0.0600 ::: bot acc: 0.0122
top acc: 0.0432 ::: bot acc: 0.0168
current epoch: 44
train loss is 0.000505
average val loss: 0.000396, accuracy: 0.0208
average test loss: 0.000434, accuracy: 0.0233
case acc: 0.019119537
case acc: 0.016605666
case acc: 0.028169256
case acc: 0.023205519
case acc: 0.030101547
case acc: 0.022485387
top acc: 0.0307 ::: bot acc: 0.0193
top acc: 0.0276 ::: bot acc: 0.0152
top acc: 0.0499 ::: bot acc: 0.0228
top acc: 0.0420 ::: bot acc: 0.0128
top acc: 0.0547 ::: bot acc: 0.0160
top acc: 0.0384 ::: bot acc: 0.0213
current epoch: 45
train loss is 0.000499
average val loss: 0.000391, accuracy: 0.0211
average test loss: 0.000386, accuracy: 0.0219
case acc: 0.018182259
case acc: 0.015511362
case acc: 0.026428787
case acc: 0.0212131
case acc: 0.028207375
case acc: 0.021782527
top acc: 0.0246 ::: bot acc: 0.0251
top acc: 0.0207 ::: bot acc: 0.0220
top acc: 0.0432 ::: bot acc: 0.0293
top acc: 0.0367 ::: bot acc: 0.0172
top acc: 0.0496 ::: bot acc: 0.0203
top acc: 0.0342 ::: bot acc: 0.0254
current epoch: 46
train loss is 0.000518
average val loss: 0.000408, accuracy: 0.0218
average test loss: 0.000375, accuracy: 0.0216
case acc: 0.01837301
case acc: 0.01599653
case acc: 0.026016189
case acc: 0.020301118
case acc: 0.027182275
case acc: 0.021540849
top acc: 0.0204 ::: bot acc: 0.0293
top acc: 0.0159 ::: bot acc: 0.0270
top acc: 0.0382 ::: bot acc: 0.0345
top acc: 0.0336 ::: bot acc: 0.0203
top acc: 0.0466 ::: bot acc: 0.0231
top acc: 0.0322 ::: bot acc: 0.0274
current epoch: 47
train loss is 0.000530
average val loss: 0.000452, accuracy: 0.0235
average test loss: 0.000383, accuracy: 0.0218
case acc: 0.019464735
case acc: 0.017856114
case acc: 0.026488237
case acc: 0.019434877
case acc: 0.026411112
case acc: 0.021407884
top acc: 0.0151 ::: bot acc: 0.0349
top acc: 0.0097 ::: bot acc: 0.0334
top acc: 0.0314 ::: bot acc: 0.0413
top acc: 0.0288 ::: bot acc: 0.0251
top acc: 0.0428 ::: bot acc: 0.0272
top acc: 0.0290 ::: bot acc: 0.0308
current epoch: 48
train loss is 0.000571
average val loss: 0.000549, accuracy: 0.0267
average test loss: 0.000430, accuracy: 0.0234
case acc: 0.021877456
case acc: 0.022204159
case acc: 0.028886942
case acc: 0.019577106
case acc: 0.025727559
case acc: 0.021939944
top acc: 0.0092 ::: bot acc: 0.0415
top acc: 0.0067 ::: bot acc: 0.0413
top acc: 0.0231 ::: bot acc: 0.0497
top acc: 0.0227 ::: bot acc: 0.0314
top acc: 0.0379 ::: bot acc: 0.0322
top acc: 0.0244 ::: bot acc: 0.0353
current epoch: 49
train loss is 0.000635
average val loss: 0.000649, accuracy: 0.0296
average test loss: 0.000495, accuracy: 0.0254
case acc: 0.024609882
case acc: 0.026996678
case acc: 0.031904344
case acc: 0.020575525
case acc: 0.025514316
case acc: 0.022598999
top acc: 0.0072 ::: bot acc: 0.0465
top acc: 0.0081 ::: bot acc: 0.0477
top acc: 0.0182 ::: bot acc: 0.0566
top acc: 0.0186 ::: bot acc: 0.0355
top acc: 0.0345 ::: bot acc: 0.0352
top acc: 0.0223 ::: bot acc: 0.0375
current epoch: 50
train loss is 0.000700
average val loss: 0.000915, accuracy: 0.0365
average test loss: 0.000688, accuracy: 0.0308
case acc: 0.03180104
case acc: 0.037597954
case acc: 0.0391966
case acc: 0.024748059
case acc: 0.026216196
case acc: 0.024947612
top acc: 0.0094 ::: bot acc: 0.0562
top acc: 0.0170 ::: bot acc: 0.0591
top acc: 0.0159 ::: bot acc: 0.0686
top acc: 0.0129 ::: bot acc: 0.0446
top acc: 0.0269 ::: bot acc: 0.0431
top acc: 0.0164 ::: bot acc: 0.0442
LME_Co_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6786 6786 6786
1.7082474 -0.6288155 0.12137239 -0.15229516
Validation: 756 756 756
Testing: 768 768 768
pre-processing time: 0.00019741058349609375
the split date is 2011-07-01
train dropout: 0.6 test dropout: 0.05
net initializing with time: 0.0021677017211914062
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012768
average val loss: 0.005040, accuracy: 0.0933
average test loss: 0.005039, accuracy: 0.0939
case acc: 0.13282424
case acc: 0.08852875
case acc: 0.090824045
case acc: 0.08155599
case acc: 0.1175251
case acc: 0.05232346
top acc: 0.1066 ::: bot acc: 0.1571
top acc: 0.1094 ::: bot acc: 0.0683
top acc: 0.0579 ::: bot acc: 0.1237
top acc: 0.0633 ::: bot acc: 0.1014
top acc: 0.0913 ::: bot acc: 0.1466
top acc: 0.0261 ::: bot acc: 0.0798
current epoch: 2
train loss is 0.007765
average val loss: 0.003085, accuracy: 0.0543
average test loss: 0.003028, accuracy: 0.0536
case acc: 0.03698016
case acc: 0.17069273
case acc: 0.02538462
case acc: 0.017063232
case acc: 0.03446941
case acc: 0.037050284
top acc: 0.0179 ::: bot acc: 0.0570
top acc: 0.1912 ::: bot acc: 0.1504
top acc: 0.0424 ::: bot acc: 0.0250
top acc: 0.0281 ::: bot acc: 0.0123
top acc: 0.0129 ::: bot acc: 0.0613
top acc: 0.0649 ::: bot acc: 0.0147
current epoch: 3
train loss is 0.008000
average val loss: 0.007993, accuracy: 0.1055
average test loss: 0.007937, accuracy: 0.1055
case acc: 0.05444126
case acc: 0.24327709
case acc: 0.0952331
case acc: 0.088620074
case acc: 0.047860783
case acc: 0.10330466
top acc: 0.0804 ::: bot acc: 0.0306
top acc: 0.2644 ::: bot acc: 0.2229
top acc: 0.1295 ::: bot acc: 0.0621
top acc: 0.1070 ::: bot acc: 0.0687
top acc: 0.0742 ::: bot acc: 0.0210
top acc: 0.1352 ::: bot acc: 0.0731
current epoch: 4
train loss is 0.009914
average val loss: 0.016846, accuracy: 0.1717
average test loss: 0.016736, accuracy: 0.1712
case acc: 0.12791216
case acc: 0.30273488
case acc: 0.16710629
case acc: 0.15371434
case acc: 0.114869945
case acc: 0.1610421
top acc: 0.1542 ::: bot acc: 0.1044
top acc: 0.3235 ::: bot acc: 0.2831
top acc: 0.2017 ::: bot acc: 0.1336
top acc: 0.1719 ::: bot acc: 0.1340
top acc: 0.1425 ::: bot acc: 0.0851
top acc: 0.1934 ::: bot acc: 0.1297
current epoch: 5
train loss is 0.012574
average val loss: 0.015894, accuracy: 0.1676
average test loss: 0.015777, accuracy: 0.1671
case acc: 0.12928233
case acc: 0.28914216
case acc: 0.1661798
case acc: 0.14837241
case acc: 0.118197866
case acc: 0.15171668
top acc: 0.1556 ::: bot acc: 0.1059
top acc: 0.3096 ::: bot acc: 0.2694
top acc: 0.2008 ::: bot acc: 0.1325
top acc: 0.1668 ::: bot acc: 0.1286
top acc: 0.1461 ::: bot acc: 0.0887
top acc: 0.1844 ::: bot acc: 0.1200
current epoch: 6
train loss is 0.013033
average val loss: 0.005241, accuracy: 0.0853
average test loss: 0.005161, accuracy: 0.0851
case acc: 0.051047537
case acc: 0.19367203
case acc: 0.082757376
case acc: 0.06549574
case acc: 0.048246726
case acc: 0.069187365
top acc: 0.0772 ::: bot acc: 0.0279
top acc: 0.2148 ::: bot acc: 0.1735
top acc: 0.1176 ::: bot acc: 0.0487
top acc: 0.0838 ::: bot acc: 0.0458
top acc: 0.0748 ::: bot acc: 0.0214
top acc: 0.1010 ::: bot acc: 0.0384
current epoch: 7
train loss is 0.008108
average val loss: 0.001461, accuracy: 0.0385
average test loss: 0.001402, accuracy: 0.0366
case acc: 0.023396598
case acc: 0.11170854
case acc: 0.026008788
case acc: 0.013866235
case acc: 0.02148423
case acc: 0.023320623
top acc: 0.0157 ::: bot acc: 0.0372
top acc: 0.1326 ::: bot acc: 0.0920
top acc: 0.0443 ::: bot acc: 0.0241
top acc: 0.0156 ::: bot acc: 0.0224
top acc: 0.0183 ::: bot acc: 0.0388
top acc: 0.0350 ::: bot acc: 0.0284
current epoch: 8
train loss is 0.003684
average val loss: 0.001156, accuracy: 0.0357
average test loss: 0.001101, accuracy: 0.0341
case acc: 0.025783846
case acc: 0.0938219
case acc: 0.025162658
case acc: 0.014932103
case acc: 0.021211032
case acc: 0.023429489
top acc: 0.0148 ::: bot acc: 0.0413
top acc: 0.1142 ::: bot acc: 0.0742
top acc: 0.0343 ::: bot acc: 0.0346
top acc: 0.0106 ::: bot acc: 0.0276
top acc: 0.0212 ::: bot acc: 0.0362
top acc: 0.0321 ::: bot acc: 0.0317
current epoch: 9
train loss is 0.002366
average val loss: 0.001394, accuracy: 0.0389
average test loss: 0.001333, accuracy: 0.0371
case acc: 0.01762116
case acc: 0.10428049
case acc: 0.027771961
case acc: 0.017663151
case acc: 0.026541311
case acc: 0.028827172
top acc: 0.0300 ::: bot acc: 0.0192
top acc: 0.1247 ::: bot acc: 0.0852
top acc: 0.0508 ::: bot acc: 0.0182
top acc: 0.0296 ::: bot acc: 0.0101
top acc: 0.0465 ::: bot acc: 0.0133
top acc: 0.0532 ::: bot acc: 0.0133
current epoch: 10
train loss is 0.002265
average val loss: 0.001362, accuracy: 0.0394
average test loss: 0.001308, accuracy: 0.0378
case acc: 0.01796638
case acc: 0.09965265
case acc: 0.028373709
case acc: 0.01963773
case acc: 0.030194271
case acc: 0.031172488
top acc: 0.0342 ::: bot acc: 0.0148
top acc: 0.1199 ::: bot acc: 0.0804
top acc: 0.0522 ::: bot acc: 0.0173
top acc: 0.0331 ::: bot acc: 0.0092
top acc: 0.0526 ::: bot acc: 0.0118
top acc: 0.0571 ::: bot acc: 0.0126
current epoch: 11
train loss is 0.002069
average val loss: 0.001178, accuracy: 0.0371
average test loss: 0.001126, accuracy: 0.0355
case acc: 0.017837707
case acc: 0.089700185
case acc: 0.026929704
case acc: 0.018260164
case acc: 0.030116392
case acc: 0.02995929
top acc: 0.0336 ::: bot acc: 0.0156
top acc: 0.1101 ::: bot acc: 0.0703
top acc: 0.0478 ::: bot acc: 0.0209
top acc: 0.0309 ::: bot acc: 0.0098
top acc: 0.0524 ::: bot acc: 0.0118
top acc: 0.0549 ::: bot acc: 0.0129
current epoch: 12
train loss is 0.001796
average val loss: 0.001098, accuracy: 0.0363
average test loss: 0.001044, accuracy: 0.0348
case acc: 0.01824374
case acc: 0.083115935
case acc: 0.026675206
case acc: 0.018593907
case acc: 0.031603962
case acc: 0.030379314
top acc: 0.0351 ::: bot acc: 0.0142
top acc: 0.1034 ::: bot acc: 0.0637
top acc: 0.0468 ::: bot acc: 0.0220
top acc: 0.0316 ::: bot acc: 0.0096
top acc: 0.0546 ::: bot acc: 0.0116
top acc: 0.0558 ::: bot acc: 0.0127
current epoch: 13
train loss is 0.001653
average val loss: 0.001019, accuracy: 0.0354
average test loss: 0.000964, accuracy: 0.0339
case acc: 0.018702114
case acc: 0.07644501
case acc: 0.026302412
case acc: 0.018971313
case acc: 0.032539226
case acc: 0.030427298
top acc: 0.0366 ::: bot acc: 0.0128
top acc: 0.0969 ::: bot acc: 0.0569
top acc: 0.0456 ::: bot acc: 0.0229
top acc: 0.0323 ::: bot acc: 0.0095
top acc: 0.0560 ::: bot acc: 0.0120
top acc: 0.0559 ::: bot acc: 0.0125
current epoch: 14
train loss is 0.001399
average val loss: 0.001037, accuracy: 0.0362
average test loss: 0.000984, accuracy: 0.0350
case acc: 0.02047925
case acc: 0.07366187
case acc: 0.026983509
case acc: 0.021229867
case acc: 0.035116006
case acc: 0.03266009
top acc: 0.0408 ::: bot acc: 0.0097
top acc: 0.0941 ::: bot acc: 0.0540
top acc: 0.0482 ::: bot acc: 0.0204
top acc: 0.0355 ::: bot acc: 0.0098
top acc: 0.0596 ::: bot acc: 0.0122
top acc: 0.0590 ::: bot acc: 0.0128
current epoch: 15
train loss is 0.001331
average val loss: 0.000949, accuracy: 0.0348
average test loss: 0.000904, accuracy: 0.0337
case acc: 0.020535253
case acc: 0.06777194
case acc: 0.026690898
case acc: 0.020724066
case acc: 0.034688015
case acc: 0.031919
top acc: 0.0411 ::: bot acc: 0.0091
top acc: 0.0881 ::: bot acc: 0.0486
top acc: 0.0472 ::: bot acc: 0.0217
top acc: 0.0350 ::: bot acc: 0.0092
top acc: 0.0592 ::: bot acc: 0.0120
top acc: 0.0577 ::: bot acc: 0.0127
current epoch: 16
train loss is 0.001188
average val loss: 0.000915, accuracy: 0.0344
average test loss: 0.000870, accuracy: 0.0334
case acc: 0.021345438
case acc: 0.06366125
case acc: 0.026905585
case acc: 0.021540705
case acc: 0.034816675
case acc: 0.032105148
top acc: 0.0427 ::: bot acc: 0.0086
top acc: 0.0839 ::: bot acc: 0.0444
top acc: 0.0477 ::: bot acc: 0.0211
top acc: 0.0360 ::: bot acc: 0.0095
top acc: 0.0591 ::: bot acc: 0.0122
top acc: 0.0585 ::: bot acc: 0.0125
current epoch: 17
train loss is 0.001118
average val loss: 0.000868, accuracy: 0.0337
average test loss: 0.000822, accuracy: 0.0326
case acc: 0.021782668
case acc: 0.059292566
case acc: 0.02680801
case acc: 0.021674132
case acc: 0.0343704
case acc: 0.031804245
top acc: 0.0434 ::: bot acc: 0.0087
top acc: 0.0797 ::: bot acc: 0.0398
top acc: 0.0475 ::: bot acc: 0.0214
top acc: 0.0361 ::: bot acc: 0.0095
top acc: 0.0586 ::: bot acc: 0.0120
top acc: 0.0581 ::: bot acc: 0.0123
current epoch: 18
train loss is 0.001054
average val loss: 0.000798, accuracy: 0.0324
average test loss: 0.000753, accuracy: 0.0312
case acc: 0.021622209
case acc: 0.05420053
case acc: 0.026580565
case acc: 0.02113865
case acc: 0.032830514
case acc: 0.030903151
top acc: 0.0432 ::: bot acc: 0.0085
top acc: 0.0746 ::: bot acc: 0.0346
top acc: 0.0466 ::: bot acc: 0.0220
top acc: 0.0357 ::: bot acc: 0.0093
top acc: 0.0565 ::: bot acc: 0.0121
top acc: 0.0566 ::: bot acc: 0.0121
current epoch: 19
train loss is 0.000961
average val loss: 0.000818, accuracy: 0.0330
average test loss: 0.000773, accuracy: 0.0320
case acc: 0.023364604
case acc: 0.05285926
case acc: 0.027218018
case acc: 0.022988394
case acc: 0.033282407
case acc: 0.032151777
top acc: 0.0459 ::: bot acc: 0.0084
top acc: 0.0732 ::: bot acc: 0.0335
top acc: 0.0490 ::: bot acc: 0.0196
top acc: 0.0380 ::: bot acc: 0.0097
top acc: 0.0571 ::: bot acc: 0.0118
top acc: 0.0585 ::: bot acc: 0.0126
current epoch: 20
train loss is 0.000930
average val loss: 0.000861, accuracy: 0.0341
average test loss: 0.000816, accuracy: 0.0333
case acc: 0.025701184
case acc: 0.052545123
case acc: 0.028471246
case acc: 0.025289392
case acc: 0.03420399
case acc: 0.03341806
top acc: 0.0492 ::: bot acc: 0.0085
top acc: 0.0729 ::: bot acc: 0.0333
top acc: 0.0523 ::: bot acc: 0.0169
top acc: 0.0409 ::: bot acc: 0.0108
top acc: 0.0585 ::: bot acc: 0.0119
top acc: 0.0602 ::: bot acc: 0.0127
current epoch: 21
train loss is 0.000892
average val loss: 0.000846, accuracy: 0.0339
average test loss: 0.000802, accuracy: 0.0331
case acc: 0.026615404
case acc: 0.050269768
case acc: 0.02907367
case acc: 0.025890296
case acc: 0.03374833
case acc: 0.03306421
top acc: 0.0504 ::: bot acc: 0.0086
top acc: 0.0706 ::: bot acc: 0.0309
top acc: 0.0536 ::: bot acc: 0.0159
top acc: 0.0416 ::: bot acc: 0.0112
top acc: 0.0579 ::: bot acc: 0.0119
top acc: 0.0597 ::: bot acc: 0.0127
current epoch: 22
train loss is 0.000870
average val loss: 0.000864, accuracy: 0.0344
average test loss: 0.000821, accuracy: 0.0337
case acc: 0.028302317
case acc: 0.049573068
case acc: 0.030268852
case acc: 0.027128823
case acc: 0.033888556
case acc: 0.033222295
top acc: 0.0526 ::: bot acc: 0.0095
top acc: 0.0701 ::: bot acc: 0.0300
top acc: 0.0557 ::: bot acc: 0.0150
top acc: 0.0433 ::: bot acc: 0.0118
top acc: 0.0579 ::: bot acc: 0.0119
top acc: 0.0597 ::: bot acc: 0.0127
current epoch: 23
train loss is 0.000841
average val loss: 0.000941, accuracy: 0.0362
average test loss: 0.000894, accuracy: 0.0356
case acc: 0.031251013
case acc: 0.050788507
case acc: 0.03248456
case acc: 0.029507536
case acc: 0.03535538
case acc: 0.03437469
top acc: 0.0563 ::: bot acc: 0.0109
top acc: 0.0710 ::: bot acc: 0.0316
top acc: 0.0599 ::: bot acc: 0.0133
top acc: 0.0458 ::: bot acc: 0.0135
top acc: 0.0600 ::: bot acc: 0.0124
top acc: 0.0613 ::: bot acc: 0.0130
current epoch: 24
train loss is 0.000846
average val loss: 0.000975, accuracy: 0.0371
average test loss: 0.000931, accuracy: 0.0366
case acc: 0.03290681
case acc: 0.050871387
case acc: 0.033955857
case acc: 0.031116098
case acc: 0.035877526
case acc: 0.034762587
top acc: 0.0584 ::: bot acc: 0.0118
top acc: 0.0712 ::: bot acc: 0.0316
top acc: 0.0625 ::: bot acc: 0.0126
top acc: 0.0475 ::: bot acc: 0.0149
top acc: 0.0605 ::: bot acc: 0.0128
top acc: 0.0619 ::: bot acc: 0.0132
current epoch: 25
train loss is 0.000834
average val loss: 0.001085, accuracy: 0.0396
average test loss: 0.001040, accuracy: 0.0391
case acc: 0.036544055
case acc: 0.052932546
case acc: 0.036650315
case acc: 0.03417316
case acc: 0.038054615
case acc: 0.036473572
top acc: 0.0622 ::: bot acc: 0.0147
top acc: 0.0733 ::: bot acc: 0.0336
top acc: 0.0672 ::: bot acc: 0.0112
top acc: 0.0510 ::: bot acc: 0.0170
top acc: 0.0633 ::: bot acc: 0.0136
top acc: 0.0642 ::: bot acc: 0.0140
current epoch: 26
train loss is 0.000850
average val loss: 0.001099, accuracy: 0.0399
average test loss: 0.001054, accuracy: 0.0395
case acc: 0.03760621
case acc: 0.05228275
case acc: 0.037863497
case acc: 0.034931123
case acc: 0.03798517
case acc: 0.036419746
top acc: 0.0635 ::: bot acc: 0.0156
top acc: 0.0725 ::: bot acc: 0.0329
top acc: 0.0690 ::: bot acc: 0.0114
top acc: 0.0520 ::: bot acc: 0.0177
top acc: 0.0632 ::: bot acc: 0.0136
top acc: 0.0637 ::: bot acc: 0.0142
current epoch: 27
train loss is 0.000842
average val loss: 0.001180, accuracy: 0.0417
average test loss: 0.001132, accuracy: 0.0413
case acc: 0.040186074
case acc: 0.05323667
case acc: 0.039962348
case acc: 0.037181165
case acc: 0.039824232
case acc: 0.037395343
top acc: 0.0660 ::: bot acc: 0.0181
top acc: 0.0735 ::: bot acc: 0.0340
top acc: 0.0719 ::: bot acc: 0.0115
top acc: 0.0541 ::: bot acc: 0.0198
top acc: 0.0655 ::: bot acc: 0.0146
top acc: 0.0654 ::: bot acc: 0.0143
current epoch: 28
train loss is 0.000865
average val loss: 0.001257, accuracy: 0.0433
average test loss: 0.001211, accuracy: 0.0430
case acc: 0.042467557
case acc: 0.053989574
case acc: 0.04218532
case acc: 0.039303154
case acc: 0.041520603
case acc: 0.038606238
top acc: 0.0686 ::: bot acc: 0.0201
top acc: 0.0742 ::: bot acc: 0.0346
top acc: 0.0749 ::: bot acc: 0.0119
top acc: 0.0567 ::: bot acc: 0.0215
top acc: 0.0675 ::: bot acc: 0.0156
top acc: 0.0668 ::: bot acc: 0.0150
current epoch: 29
train loss is 0.000839
average val loss: 0.001227, accuracy: 0.0427
average test loss: 0.001180, accuracy: 0.0424
case acc: 0.042180568
case acc: 0.05240722
case acc: 0.04233977
case acc: 0.03881251
case acc: 0.041009236
case acc: 0.037491973
top acc: 0.0682 ::: bot acc: 0.0197
top acc: 0.0727 ::: bot acc: 0.0331
top acc: 0.0752 ::: bot acc: 0.0121
top acc: 0.0560 ::: bot acc: 0.0211
top acc: 0.0669 ::: bot acc: 0.0153
top acc: 0.0654 ::: bot acc: 0.0146
current epoch: 30
train loss is 0.000836
average val loss: 0.001274, accuracy: 0.0437
average test loss: 0.001228, accuracy: 0.0434
case acc: 0.04354073
case acc: 0.05260516
case acc: 0.04380327
case acc: 0.040098403
case acc: 0.0422095
case acc: 0.037957434
top acc: 0.0698 ::: bot acc: 0.0210
top acc: 0.0728 ::: bot acc: 0.0332
top acc: 0.0772 ::: bot acc: 0.0124
top acc: 0.0575 ::: bot acc: 0.0221
top acc: 0.0684 ::: bot acc: 0.0160
top acc: 0.0660 ::: bot acc: 0.0149
current epoch: 31
train loss is 0.000839
average val loss: 0.001292, accuracy: 0.0441
average test loss: 0.001249, accuracy: 0.0439
case acc: 0.044031598
case acc: 0.05225014
case acc: 0.044911068
case acc: 0.040949166
case acc: 0.042726845
case acc: 0.038237244
top acc: 0.0702 ::: bot acc: 0.0214
top acc: 0.0725 ::: bot acc: 0.0330
top acc: 0.0784 ::: bot acc: 0.0130
top acc: 0.0583 ::: bot acc: 0.0230
top acc: 0.0688 ::: bot acc: 0.0166
top acc: 0.0662 ::: bot acc: 0.0151
current epoch: 32
train loss is 0.000809
average val loss: 0.001339, accuracy: 0.0450
average test loss: 0.001292, accuracy: 0.0448
case acc: 0.044988345
case acc: 0.05254729
case acc: 0.04635508
case acc: 0.04228057
case acc: 0.043742545
case acc: 0.038648263
top acc: 0.0710 ::: bot acc: 0.0225
top acc: 0.0728 ::: bot acc: 0.0332
top acc: 0.0803 ::: bot acc: 0.0140
top acc: 0.0596 ::: bot acc: 0.0242
top acc: 0.0702 ::: bot acc: 0.0172
top acc: 0.0667 ::: bot acc: 0.0153
current epoch: 33
train loss is 0.000798
average val loss: 0.001372, accuracy: 0.0457
average test loss: 0.001321, accuracy: 0.0454
case acc: 0.0456366
case acc: 0.05234821
case acc: 0.047323488
case acc: 0.043184906
case acc: 0.04466947
case acc: 0.039140992
top acc: 0.0716 ::: bot acc: 0.0232
top acc: 0.0727 ::: bot acc: 0.0330
top acc: 0.0813 ::: bot acc: 0.0147
top acc: 0.0605 ::: bot acc: 0.0251
top acc: 0.0711 ::: bot acc: 0.0180
top acc: 0.0673 ::: bot acc: 0.0156
current epoch: 34
train loss is 0.000803
average val loss: 0.001308, accuracy: 0.0444
average test loss: 0.001259, accuracy: 0.0441
case acc: 0.044110604
case acc: 0.049949624
case acc: 0.046364598
case acc: 0.04218706
case acc: 0.043887552
case acc: 0.037944745
top acc: 0.0701 ::: bot acc: 0.0216
top acc: 0.0702 ::: bot acc: 0.0306
top acc: 0.0802 ::: bot acc: 0.0139
top acc: 0.0596 ::: bot acc: 0.0241
top acc: 0.0702 ::: bot acc: 0.0173
top acc: 0.0660 ::: bot acc: 0.0148
current epoch: 35
train loss is 0.000799
average val loss: 0.001349, accuracy: 0.0452
average test loss: 0.001301, accuracy: 0.0450
case acc: 0.04488626
case acc: 0.049732186
case acc: 0.047520433
case acc: 0.043410778
case acc: 0.045411468
case acc: 0.03892003
top acc: 0.0711 ::: bot acc: 0.0221
top acc: 0.0700 ::: bot acc: 0.0304
top acc: 0.0815 ::: bot acc: 0.0150
top acc: 0.0608 ::: bot acc: 0.0252
top acc: 0.0719 ::: bot acc: 0.0185
top acc: 0.0671 ::: bot acc: 0.0154
current epoch: 36
train loss is 0.000746
average val loss: 0.001229, accuracy: 0.0427
average test loss: 0.001178, accuracy: 0.0423
case acc: 0.04189149
case acc: 0.0457213
case acc: 0.045149
case acc: 0.041191272
case acc: 0.04332296
case acc: 0.03671062
top acc: 0.0679 ::: bot acc: 0.0196
top acc: 0.0660 ::: bot acc: 0.0264
top acc: 0.0788 ::: bot acc: 0.0132
top acc: 0.0585 ::: bot acc: 0.0231
top acc: 0.0695 ::: bot acc: 0.0170
top acc: 0.0644 ::: bot acc: 0.0142
current epoch: 37
train loss is 0.000712
average val loss: 0.001137, accuracy: 0.0408
average test loss: 0.001088, accuracy: 0.0403
case acc: 0.039372317
case acc: 0.0424605
case acc: 0.043364838
case acc: 0.03945504
case acc: 0.041867565
case acc: 0.035261855
top acc: 0.0653 ::: bot acc: 0.0172
top acc: 0.0627 ::: bot acc: 0.0230
top acc: 0.0767 ::: bot acc: 0.0124
top acc: 0.0567 ::: bot acc: 0.0216
top acc: 0.0678 ::: bot acc: 0.0159
top acc: 0.0626 ::: bot acc: 0.0135
current epoch: 38
train loss is 0.000674
average val loss: 0.001028, accuracy: 0.0383
average test loss: 0.000978, accuracy: 0.0377
case acc: 0.036418658
case acc: 0.038459837
case acc: 0.041111924
case acc: 0.037124477
case acc: 0.039888546
case acc: 0.03338725
top acc: 0.0622 ::: bot acc: 0.0147
top acc: 0.0587 ::: bot acc: 0.0192
top acc: 0.0736 ::: bot acc: 0.0117
top acc: 0.0543 ::: bot acc: 0.0195
top acc: 0.0656 ::: bot acc: 0.0147
top acc: 0.0601 ::: bot acc: 0.0127
current epoch: 39
train loss is 0.000640
average val loss: 0.000922, accuracy: 0.0359
average test loss: 0.000874, accuracy: 0.0352
case acc: 0.033405703
case acc: 0.034575872
case acc: 0.0390008
case acc: 0.034676276
case acc: 0.037873156
case acc: 0.031435948
top acc: 0.0587 ::: bot acc: 0.0123
top acc: 0.0548 ::: bot acc: 0.0154
top acc: 0.0705 ::: bot acc: 0.0114
top acc: 0.0516 ::: bot acc: 0.0175
top acc: 0.0632 ::: bot acc: 0.0135
top acc: 0.0575 ::: bot acc: 0.0121
current epoch: 40
train loss is 0.000598
average val loss: 0.000838, accuracy: 0.0339
average test loss: 0.000792, accuracy: 0.0331
case acc: 0.030838927
case acc: 0.031303138
case acc: 0.037109233
case acc: 0.032874614
case acc: 0.036075287
case acc: 0.030129032
top acc: 0.0559 ::: bot acc: 0.0106
top acc: 0.0513 ::: bot acc: 0.0125
top acc: 0.0677 ::: bot acc: 0.0114
top acc: 0.0495 ::: bot acc: 0.0162
top acc: 0.0608 ::: bot acc: 0.0127
top acc: 0.0556 ::: bot acc: 0.0121
current epoch: 41
train loss is 0.000585
average val loss: 0.000800, accuracy: 0.0329
average test loss: 0.000756, accuracy: 0.0321
case acc: 0.029606014
case acc: 0.029310782
case acc: 0.036276888
case acc: 0.032182902
case acc: 0.03545714
case acc: 0.029873528
top acc: 0.0544 ::: bot acc: 0.0099
top acc: 0.0492 ::: bot acc: 0.0109
top acc: 0.0663 ::: bot acc: 0.0118
top acc: 0.0488 ::: bot acc: 0.0156
top acc: 0.0601 ::: bot acc: 0.0123
top acc: 0.0551 ::: bot acc: 0.0123
current epoch: 42
train loss is 0.000556
average val loss: 0.000704, accuracy: 0.0305
average test loss: 0.000659, accuracy: 0.0295
case acc: 0.02661087
case acc: 0.025384307
case acc: 0.033938777
case acc: 0.029392071
case acc: 0.03334038
case acc: 0.028064627
top acc: 0.0505 ::: bot acc: 0.0087
top acc: 0.0447 ::: bot acc: 0.0080
top acc: 0.0624 ::: bot acc: 0.0127
top acc: 0.0457 ::: bot acc: 0.0135
top acc: 0.0571 ::: bot acc: 0.0118
top acc: 0.0522 ::: bot acc: 0.0129
current epoch: 43
train loss is 0.000522
average val loss: 0.000639, accuracy: 0.0288
average test loss: 0.000595, accuracy: 0.0276
case acc: 0.024623139
case acc: 0.022587262
case acc: 0.03233327
case acc: 0.027378395
case acc: 0.031716328
case acc: 0.026939986
top acc: 0.0476 ::: bot acc: 0.0084
top acc: 0.0411 ::: bot acc: 0.0067
top acc: 0.0597 ::: bot acc: 0.0135
top acc: 0.0433 ::: bot acc: 0.0121
top acc: 0.0550 ::: bot acc: 0.0115
top acc: 0.0499 ::: bot acc: 0.0140
current epoch: 44
train loss is 0.000509
average val loss: 0.000567, accuracy: 0.0269
average test loss: 0.000523, accuracy: 0.0254
case acc: 0.022016052
case acc: 0.019628972
case acc: 0.030313052
case acc: 0.024881463
case acc: 0.029842192
case acc: 0.025771556
top acc: 0.0438 ::: bot acc: 0.0084
top acc: 0.0370 ::: bot acc: 0.0062
top acc: 0.0558 ::: bot acc: 0.0151
top acc: 0.0403 ::: bot acc: 0.0106
top acc: 0.0521 ::: bot acc: 0.0119
top acc: 0.0474 ::: bot acc: 0.0156
current epoch: 45
train loss is 0.000504
average val loss: 0.000523, accuracy: 0.0257
average test loss: 0.000479, accuracy: 0.0240
case acc: 0.020409819
case acc: 0.017844448
case acc: 0.028753605
case acc: 0.023215555
case acc: 0.028775865
case acc: 0.025259916
top acc: 0.0411 ::: bot acc: 0.0090
top acc: 0.0336 ::: bot acc: 0.0075
top acc: 0.0527 ::: bot acc: 0.0168
top acc: 0.0382 ::: bot acc: 0.0099
top acc: 0.0502 ::: bot acc: 0.0122
top acc: 0.0458 ::: bot acc: 0.0170
current epoch: 46
train loss is 0.000480
average val loss: 0.000459, accuracy: 0.0241
average test loss: 0.000416, accuracy: 0.0220
case acc: 0.018420476
case acc: 0.015686197
case acc: 0.026761848
case acc: 0.020522784
case acc: 0.026673373
case acc: 0.024160517
top acc: 0.0365 ::: bot acc: 0.0121
top acc: 0.0288 ::: bot acc: 0.0110
top acc: 0.0477 ::: bot acc: 0.0207
top acc: 0.0344 ::: bot acc: 0.0094
top acc: 0.0468 ::: bot acc: 0.0131
top acc: 0.0424 ::: bot acc: 0.0200
current epoch: 47
train loss is 0.000480
average val loss: 0.000415, accuracy: 0.0228
average test loss: 0.000373, accuracy: 0.0206
case acc: 0.017346956
case acc: 0.014472755
case acc: 0.02570704
case acc: 0.018185161
case acc: 0.02458363
case acc: 0.023590542
top acc: 0.0319 ::: bot acc: 0.0165
top acc: 0.0239 ::: bot acc: 0.0159
top acc: 0.0430 ::: bot acc: 0.0258
top acc: 0.0308 ::: bot acc: 0.0097
top acc: 0.0428 ::: bot acc: 0.0147
top acc: 0.0396 ::: bot acc: 0.0229
current epoch: 48
train loss is 0.000484
average val loss: 0.000384, accuracy: 0.0219
average test loss: 0.000344, accuracy: 0.0199
case acc: 0.017536435
case acc: 0.014401547
case acc: 0.02490548
case acc: 0.015972218
case acc: 0.023171749
case acc: 0.023133103
top acc: 0.0269 ::: bot acc: 0.0215
top acc: 0.0182 ::: bot acc: 0.0214
top acc: 0.0371 ::: bot acc: 0.0314
top acc: 0.0265 ::: bot acc: 0.0118
top acc: 0.0390 ::: bot acc: 0.0183
top acc: 0.0361 ::: bot acc: 0.0263
current epoch: 49
train loss is 0.000502
average val loss: 0.000383, accuracy: 0.0219
average test loss: 0.000343, accuracy: 0.0199
case acc: 0.01794488
case acc: 0.0151093835
case acc: 0.025011182
case acc: 0.015312858
case acc: 0.023034891
case acc: 0.02314109
top acc: 0.0251 ::: bot acc: 0.0236
top acc: 0.0155 ::: bot acc: 0.0243
top acc: 0.0342 ::: bot acc: 0.0347
top acc: 0.0249 ::: bot acc: 0.0129
top acc: 0.0385 ::: bot acc: 0.0187
top acc: 0.0359 ::: bot acc: 0.0266
current epoch: 50
train loss is 0.000509
average val loss: 0.000384, accuracy: 0.0220
average test loss: 0.000346, accuracy: 0.0202
case acc: 0.018889979
case acc: 0.01723887
case acc: 0.025606446
case acc: 0.014195584
case acc: 0.022310086
case acc: 0.023023916
top acc: 0.0209 ::: bot acc: 0.0276
top acc: 0.0120 ::: bot acc: 0.0292
top acc: 0.0287 ::: bot acc: 0.0401
top acc: 0.0215 ::: bot acc: 0.0160
top acc: 0.0363 ::: bot acc: 0.0210
top acc: 0.0339 ::: bot acc: 0.0286

		{"drop_out": 0.6, "drop_out_mc": 0.1, "repeat_mc": 50, "hidden": 30, "embedding_size": 5, "batch": 512, "lag": 4}
LME_Co_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6780 6780 6780
1.8562728 -0.6288155 0.15869391 -0.16256663
Validation: 756 756 756
Testing: 774 774 774
pre-processing time: 0.0002567768096923828
the split date is 2009-07-01
train dropout: 0.6 test dropout: 0.1
net initializing with time: 0.003863811492919922
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012803
average val loss: 0.005676, accuracy: 0.1007
average test loss: 0.005999, accuracy: 0.1041
case acc: 0.14731675
case acc: 0.07737055
case acc: 0.10513426
case acc: 0.09847981
case acc: 0.13158989
case acc: 0.06483184
top acc: 0.1345 ::: bot acc: 0.1605
top acc: 0.0968 ::: bot acc: 0.0583
top acc: 0.0774 ::: bot acc: 0.1332
top acc: 0.0808 ::: bot acc: 0.1173
top acc: 0.1127 ::: bot acc: 0.1514
top acc: 0.0493 ::: bot acc: 0.0810
current epoch: 2
train loss is 0.008285
average val loss: 0.002813, accuracy: 0.0520
average test loss: 0.002729, accuracy: 0.0513
case acc: 0.04536898
case acc: 0.16232061
case acc: 0.020305807
case acc: 0.014473808
case acc: 0.043374196
case acc: 0.022174332
top acc: 0.0319 ::: bot acc: 0.0593
top acc: 0.1819 ::: bot acc: 0.1432
top acc: 0.0243 ::: bot acc: 0.0308
top acc: 0.0144 ::: bot acc: 0.0235
top acc: 0.0253 ::: bot acc: 0.0635
top acc: 0.0353 ::: bot acc: 0.0107
current epoch: 3
train loss is 0.008670
average val loss: 0.007264, accuracy: 0.0992
average test loss: 0.006876, accuracy: 0.0952
case acc: 0.043976422
case acc: 0.23685023
case acc: 0.08504695
case acc: 0.07654842
case acc: 0.036174346
case acc: 0.09245933
top acc: 0.0576 ::: bot acc: 0.0303
top acc: 0.2558 ::: bot acc: 0.2176
top acc: 0.1130 ::: bot acc: 0.0587
top acc: 0.0955 ::: bot acc: 0.0567
top acc: 0.0540 ::: bot acc: 0.0176
top acc: 0.1081 ::: bot acc: 0.0759
current epoch: 4
train loss is 0.010076
average val loss: 0.016506, accuracy: 0.1700
average test loss: 0.015783, accuracy: 0.1658
case acc: 0.12226498
case acc: 0.3012198
case acc: 0.16221118
case acc: 0.14609432
case acc: 0.108152345
case acc: 0.1548008
top acc: 0.1362 ::: bot acc: 0.1079
top acc: 0.3203 ::: bot acc: 0.2819
top acc: 0.1901 ::: bot acc: 0.1354
top acc: 0.1646 ::: bot acc: 0.1265
top acc: 0.1273 ::: bot acc: 0.0877
top acc: 0.1707 ::: bot acc: 0.1381
current epoch: 5
train loss is 0.013129
average val loss: 0.013706, accuracy: 0.1542
average test loss: 0.013053, accuracy: 0.1501
case acc: 0.112031214
case acc: 0.27597213
case acc: 0.14944306
case acc: 0.12934072
case acc: 0.099902704
case acc: 0.13399503
top acc: 0.1255 ::: bot acc: 0.0979
top acc: 0.2955 ::: bot acc: 0.2565
top acc: 0.1773 ::: bot acc: 0.1225
top acc: 0.1481 ::: bot acc: 0.1095
top acc: 0.1190 ::: bot acc: 0.0793
top acc: 0.1502 ::: bot acc: 0.1167
current epoch: 6
train loss is 0.013176
average val loss: 0.003534, accuracy: 0.0641
average test loss: 0.003242, accuracy: 0.0598
case acc: 0.025205439
case acc: 0.17164692
case acc: 0.056863002
case acc: 0.038537845
case acc: 0.023860365
case acc: 0.042714577
top acc: 0.0384 ::: bot acc: 0.0117
top acc: 0.1909 ::: bot acc: 0.1524
top acc: 0.0843 ::: bot acc: 0.0302
top acc: 0.0566 ::: bot acc: 0.0200
top acc: 0.0396 ::: bot acc: 0.0102
top acc: 0.0580 ::: bot acc: 0.0265
current epoch: 7
train loss is 0.007534
average val loss: 0.001208, accuracy: 0.0367
average test loss: 0.001186, accuracy: 0.0374
case acc: 0.03344342
case acc: 0.096490726
case acc: 0.021582471
case acc: 0.025665956
case acc: 0.028735362
case acc: 0.018392688
top acc: 0.0196 ::: bot acc: 0.0476
top acc: 0.1161 ::: bot acc: 0.0770
top acc: 0.0181 ::: bot acc: 0.0364
top acc: 0.0123 ::: bot acc: 0.0429
top acc: 0.0106 ::: bot acc: 0.0487
top acc: 0.0070 ::: bot acc: 0.0327
current epoch: 8
train loss is 0.003560
average val loss: 0.001030, accuracy: 0.0311
average test loss: 0.000982, accuracy: 0.0310
case acc: 0.023739364
case acc: 0.093396686
case acc: 0.020480152
case acc: 0.018988676
case acc: 0.017011553
case acc: 0.01252226
top acc: 0.0108 ::: bot acc: 0.0374
top acc: 0.1125 ::: bot acc: 0.0739
top acc: 0.0224 ::: bot acc: 0.0321
top acc: 0.0098 ::: bot acc: 0.0342
top acc: 0.0090 ::: bot acc: 0.0321
top acc: 0.0109 ::: bot acc: 0.0218
current epoch: 9
train loss is 0.002605
average val loss: 0.001119, accuracy: 0.0313
average test loss: 0.001020, accuracy: 0.0297
case acc: 0.011407499
case acc: 0.099186815
case acc: 0.020436268
case acc: 0.014015613
case acc: 0.017187018
case acc: 0.016252156
top acc: 0.0078 ::: bot acc: 0.0207
top acc: 0.1184 ::: bot acc: 0.0799
top acc: 0.0342 ::: bot acc: 0.0207
top acc: 0.0191 ::: bot acc: 0.0192
top acc: 0.0280 ::: bot acc: 0.0132
top acc: 0.0270 ::: bot acc: 0.0093
current epoch: 10
train loss is 0.002470
average val loss: 0.001062, accuracy: 0.0317
average test loss: 0.000960, accuracy: 0.0299
case acc: 0.010552153
case acc: 0.094258256
case acc: 0.020754874
case acc: 0.014701016
case acc: 0.020549815
case acc: 0.018710066
top acc: 0.0119 ::: bot acc: 0.0165
top acc: 0.1134 ::: bot acc: 0.0756
top acc: 0.0350 ::: bot acc: 0.0196
top acc: 0.0222 ::: bot acc: 0.0166
top acc: 0.0347 ::: bot acc: 0.0102
top acc: 0.0309 ::: bot acc: 0.0095
current epoch: 11
train loss is 0.002324
average val loss: 0.000886, accuracy: 0.0292
average test loss: 0.000796, accuracy: 0.0277
case acc: 0.010781661
case acc: 0.083856866
case acc: 0.020067617
case acc: 0.013928391
case acc: 0.020312449
case acc: 0.017406318
top acc: 0.0108 ::: bot acc: 0.0176
top acc: 0.1031 ::: bot acc: 0.0647
top acc: 0.0311 ::: bot acc: 0.0238
top acc: 0.0198 ::: bot acc: 0.0180
top acc: 0.0343 ::: bot acc: 0.0101
top acc: 0.0289 ::: bot acc: 0.0095
current epoch: 12
train loss is 0.002020
average val loss: 0.000823, accuracy: 0.0291
average test loss: 0.000734, accuracy: 0.0274
case acc: 0.00998736
case acc: 0.07818922
case acc: 0.019889332
case acc: 0.014542341
case acc: 0.022828477
case acc: 0.018696109
top acc: 0.0129 ::: bot acc: 0.0143
top acc: 0.0974 ::: bot acc: 0.0592
top acc: 0.0306 ::: bot acc: 0.0239
top acc: 0.0220 ::: bot acc: 0.0171
top acc: 0.0382 ::: bot acc: 0.0099
top acc: 0.0309 ::: bot acc: 0.0095
current epoch: 13
train loss is 0.001804
average val loss: 0.000729, accuracy: 0.0278
average test loss: 0.000642, accuracy: 0.0260
case acc: 0.00981399
case acc: 0.0706944
case acc: 0.01990622
case acc: 0.014082131
case acc: 0.023341108
case acc: 0.018401967
top acc: 0.0133 ::: bot acc: 0.0137
top acc: 0.0899 ::: bot acc: 0.0515
top acc: 0.0289 ::: bot acc: 0.0262
top acc: 0.0209 ::: bot acc: 0.0171
top acc: 0.0387 ::: bot acc: 0.0104
top acc: 0.0305 ::: bot acc: 0.0094
current epoch: 14
train loss is 0.001589
average val loss: 0.000666, accuracy: 0.0272
average test loss: 0.000584, accuracy: 0.0254
case acc: 0.010139346
case acc: 0.064944096
case acc: 0.019793563
case acc: 0.014319765
case acc: 0.024171367
case acc: 0.018917127
top acc: 0.0152 ::: bot acc: 0.0117
top acc: 0.0841 ::: bot acc: 0.0460
top acc: 0.0285 ::: bot acc: 0.0266
top acc: 0.0220 ::: bot acc: 0.0165
top acc: 0.0399 ::: bot acc: 0.0106
top acc: 0.0316 ::: bot acc: 0.0092
current epoch: 15
train loss is 0.001426
average val loss: 0.000609, accuracy: 0.0263
average test loss: 0.000528, accuracy: 0.0245
case acc: 0.010596338
case acc: 0.059534766
case acc: 0.019668967
case acc: 0.014659594
case acc: 0.024098238
case acc: 0.018521331
top acc: 0.0166 ::: bot acc: 0.0111
top acc: 0.0788 ::: bot acc: 0.0404
top acc: 0.0277 ::: bot acc: 0.0272
top acc: 0.0224 ::: bot acc: 0.0165
top acc: 0.0400 ::: bot acc: 0.0105
top acc: 0.0309 ::: bot acc: 0.0093
current epoch: 16
train loss is 0.001344
average val loss: 0.000621, accuracy: 0.0274
average test loss: 0.000535, accuracy: 0.0253
case acc: 0.01160973
case acc: 0.057938006
case acc: 0.020012792
case acc: 0.015445761
case acc: 0.026004393
case acc: 0.020552421
top acc: 0.0200 ::: bot acc: 0.0074
top acc: 0.0771 ::: bot acc: 0.0386
top acc: 0.0306 ::: bot acc: 0.0248
top acc: 0.0255 ::: bot acc: 0.0134
top acc: 0.0424 ::: bot acc: 0.0111
top acc: 0.0332 ::: bot acc: 0.0099
current epoch: 17
train loss is 0.001283
average val loss: 0.000608, accuracy: 0.0274
average test loss: 0.000517, accuracy: 0.0253
case acc: 0.012610903
case acc: 0.05502562
case acc: 0.020229816
case acc: 0.016067069
case acc: 0.026703537
case acc: 0.020946287
top acc: 0.0221 ::: bot acc: 0.0061
top acc: 0.0744 ::: bot acc: 0.0357
top acc: 0.0318 ::: bot acc: 0.0234
top acc: 0.0269 ::: bot acc: 0.0122
top acc: 0.0433 ::: bot acc: 0.0112
top acc: 0.0341 ::: bot acc: 0.0101
current epoch: 18
train loss is 0.001196
average val loss: 0.000601, accuracy: 0.0277
average test loss: 0.000499, accuracy: 0.0251
case acc: 0.013814708
case acc: 0.05233896
case acc: 0.0201872
case acc: 0.01637096
case acc: 0.026591605
case acc: 0.021203706
top acc: 0.0241 ::: bot acc: 0.0055
top acc: 0.0718 ::: bot acc: 0.0330
top acc: 0.0333 ::: bot acc: 0.0215
top acc: 0.0281 ::: bot acc: 0.0109
top acc: 0.0431 ::: bot acc: 0.0115
top acc: 0.0342 ::: bot acc: 0.0102
current epoch: 19
train loss is 0.001162
average val loss: 0.000577, accuracy: 0.0274
average test loss: 0.000481, accuracy: 0.0248
case acc: 0.01467994
case acc: 0.04982761
case acc: 0.02048159
case acc: 0.01676775
case acc: 0.026276555
case acc: 0.020998674
top acc: 0.0257 ::: bot acc: 0.0052
top acc: 0.0690 ::: bot acc: 0.0304
top acc: 0.0346 ::: bot acc: 0.0202
top acc: 0.0285 ::: bot acc: 0.0105
top acc: 0.0430 ::: bot acc: 0.0110
top acc: 0.0340 ::: bot acc: 0.0102
current epoch: 20
train loss is 0.001106
average val loss: 0.000569, accuracy: 0.0274
average test loss: 0.000467, accuracy: 0.0246
case acc: 0.015612634
case acc: 0.04760969
case acc: 0.0208257
case acc: 0.016972613
case acc: 0.025900885
case acc: 0.020859279
top acc: 0.0274 ::: bot acc: 0.0052
top acc: 0.0671 ::: bot acc: 0.0282
top acc: 0.0359 ::: bot acc: 0.0189
top acc: 0.0295 ::: bot acc: 0.0098
top acc: 0.0425 ::: bot acc: 0.0110
top acc: 0.0338 ::: bot acc: 0.0102
current epoch: 21
train loss is 0.001092
average val loss: 0.000593, accuracy: 0.0284
average test loss: 0.000480, accuracy: 0.0254
case acc: 0.017602857
case acc: 0.04687893
case acc: 0.02183612
case acc: 0.01828948
case acc: 0.026254663
case acc: 0.021419391
top acc: 0.0296 ::: bot acc: 0.0061
top acc: 0.0663 ::: bot acc: 0.0275
top acc: 0.0388 ::: bot acc: 0.0169
top acc: 0.0318 ::: bot acc: 0.0090
top acc: 0.0427 ::: bot acc: 0.0112
top acc: 0.0344 ::: bot acc: 0.0101
current epoch: 22
train loss is 0.001064
average val loss: 0.000649, accuracy: 0.0303
average test loss: 0.000527, accuracy: 0.0270
case acc: 0.02049833
case acc: 0.04775897
case acc: 0.023362419
case acc: 0.019878533
case acc: 0.02779307
case acc: 0.0228516
top acc: 0.0333 ::: bot acc: 0.0077
top acc: 0.0673 ::: bot acc: 0.0283
top acc: 0.0426 ::: bot acc: 0.0143
top acc: 0.0347 ::: bot acc: 0.0079
top acc: 0.0445 ::: bot acc: 0.0118
top acc: 0.0365 ::: bot acc: 0.0109
current epoch: 23
train loss is 0.001073
average val loss: 0.000702, accuracy: 0.0319
average test loss: 0.000564, accuracy: 0.0284
case acc: 0.023227343
case acc: 0.048003186
case acc: 0.025050573
case acc: 0.021361832
case acc: 0.029066592
case acc: 0.02381564
top acc: 0.0359 ::: bot acc: 0.0102
top acc: 0.0673 ::: bot acc: 0.0287
top acc: 0.0460 ::: bot acc: 0.0126
top acc: 0.0369 ::: bot acc: 0.0081
top acc: 0.0462 ::: bot acc: 0.0125
top acc: 0.0374 ::: bot acc: 0.0114
current epoch: 24
train loss is 0.001022
average val loss: 0.000643, accuracy: 0.0303
average test loss: 0.000514, accuracy: 0.0270
case acc: 0.022209171
case acc: 0.04479412
case acc: 0.02508077
case acc: 0.020431984
case acc: 0.027194202
case acc: 0.022027696
top acc: 0.0350 ::: bot acc: 0.0091
top acc: 0.0642 ::: bot acc: 0.0256
top acc: 0.0454 ::: bot acc: 0.0134
top acc: 0.0355 ::: bot acc: 0.0078
top acc: 0.0441 ::: bot acc: 0.0116
top acc: 0.0351 ::: bot acc: 0.0105
current epoch: 25
train loss is 0.000996
average val loss: 0.000686, accuracy: 0.0316
average test loss: 0.000547, accuracy: 0.0281
case acc: 0.024420587
case acc: 0.045030456
case acc: 0.026327211
case acc: 0.021818142
case acc: 0.02825719
case acc: 0.02256701
top acc: 0.0375 ::: bot acc: 0.0110
top acc: 0.0645 ::: bot acc: 0.0256
top acc: 0.0482 ::: bot acc: 0.0120
top acc: 0.0374 ::: bot acc: 0.0083
top acc: 0.0451 ::: bot acc: 0.0121
top acc: 0.0360 ::: bot acc: 0.0107
current epoch: 26
train loss is 0.000978
average val loss: 0.000673, accuracy: 0.0313
average test loss: 0.000538, accuracy: 0.0278
case acc: 0.02470616
case acc: 0.04389519
case acc: 0.026947437
case acc: 0.021915503
case acc: 0.027614687
case acc: 0.022007484
top acc: 0.0377 ::: bot acc: 0.0113
top acc: 0.0634 ::: bot acc: 0.0245
top acc: 0.0492 ::: bot acc: 0.0119
top acc: 0.0375 ::: bot acc: 0.0084
top acc: 0.0444 ::: bot acc: 0.0118
top acc: 0.0353 ::: bot acc: 0.0105
current epoch: 27
train loss is 0.000981
average val loss: 0.000716, accuracy: 0.0325
average test loss: 0.000569, accuracy: 0.0289
case acc: 0.02644068
case acc: 0.044126593
case acc: 0.028542792
case acc: 0.023460135
case acc: 0.028457072
case acc: 0.0225082
top acc: 0.0395 ::: bot acc: 0.0130
top acc: 0.0638 ::: bot acc: 0.0248
top acc: 0.0515 ::: bot acc: 0.0117
top acc: 0.0392 ::: bot acc: 0.0091
top acc: 0.0451 ::: bot acc: 0.0125
top acc: 0.0361 ::: bot acc: 0.0106
current epoch: 28
train loss is 0.000983
average val loss: 0.000789, accuracy: 0.0346
average test loss: 0.000632, accuracy: 0.0309
case acc: 0.029289603
case acc: 0.045419693
case acc: 0.030877734
case acc: 0.02537017
case acc: 0.030113272
case acc: 0.024078669
top acc: 0.0423 ::: bot acc: 0.0155
top acc: 0.0649 ::: bot acc: 0.0260
top acc: 0.0549 ::: bot acc: 0.0118
top acc: 0.0419 ::: bot acc: 0.0100
top acc: 0.0474 ::: bot acc: 0.0130
top acc: 0.0380 ::: bot acc: 0.0113
current epoch: 29
train loss is 0.000979
average val loss: 0.000858, accuracy: 0.0364
average test loss: 0.000691, accuracy: 0.0327
case acc: 0.031737402
case acc: 0.04648911
case acc: 0.03290646
case acc: 0.027488299
case acc: 0.03202273
case acc: 0.025264766
top acc: 0.0450 ::: bot acc: 0.0179
top acc: 0.0661 ::: bot acc: 0.0271
top acc: 0.0575 ::: bot acc: 0.0123
top acc: 0.0440 ::: bot acc: 0.0116
top acc: 0.0498 ::: bot acc: 0.0144
top acc: 0.0393 ::: bot acc: 0.0120
current epoch: 30
train loss is 0.000995
average val loss: 0.000881, accuracy: 0.0370
average test loss: 0.000710, accuracy: 0.0332
case acc: 0.03249013
case acc: 0.046162523
case acc: 0.033884753
case acc: 0.028218592
case acc: 0.03299237
case acc: 0.025441514
top acc: 0.0456 ::: bot acc: 0.0188
top acc: 0.0658 ::: bot acc: 0.0267
top acc: 0.0592 ::: bot acc: 0.0127
top acc: 0.0452 ::: bot acc: 0.0119
top acc: 0.0507 ::: bot acc: 0.0153
top acc: 0.0396 ::: bot acc: 0.0123
current epoch: 31
train loss is 0.000976
average val loss: 0.000903, accuracy: 0.0375
average test loss: 0.000732, accuracy: 0.0338
case acc: 0.03338651
case acc: 0.04593735
case acc: 0.035125807
case acc: 0.029012779
case acc: 0.03376491
case acc: 0.025748119
top acc: 0.0469 ::: bot acc: 0.0194
top acc: 0.0657 ::: bot acc: 0.0264
top acc: 0.0607 ::: bot acc: 0.0131
top acc: 0.0461 ::: bot acc: 0.0127
top acc: 0.0516 ::: bot acc: 0.0157
top acc: 0.0399 ::: bot acc: 0.0124
current epoch: 32
train loss is 0.000981
average val loss: 0.000938, accuracy: 0.0385
average test loss: 0.000757, accuracy: 0.0346
case acc: 0.034196503
case acc: 0.04591064
case acc: 0.03644268
case acc: 0.03002606
case acc: 0.034728996
case acc: 0.026062392
top acc: 0.0475 ::: bot acc: 0.0202
top acc: 0.0654 ::: bot acc: 0.0266
top acc: 0.0622 ::: bot acc: 0.0139
top acc: 0.0470 ::: bot acc: 0.0135
top acc: 0.0528 ::: bot acc: 0.0166
top acc: 0.0403 ::: bot acc: 0.0127
current epoch: 33
train loss is 0.000970
average val loss: 0.000956, accuracy: 0.0389
average test loss: 0.000773, accuracy: 0.0350
case acc: 0.034619946
case acc: 0.04536208
case acc: 0.037337143
case acc: 0.030879254
case acc: 0.035381507
case acc: 0.02650143
top acc: 0.0478 ::: bot acc: 0.0208
top acc: 0.0650 ::: bot acc: 0.0259
top acc: 0.0633 ::: bot acc: 0.0144
top acc: 0.0482 ::: bot acc: 0.0141
top acc: 0.0535 ::: bot acc: 0.0168
top acc: 0.0408 ::: bot acc: 0.0126
current epoch: 34
train loss is 0.000960
average val loss: 0.000911, accuracy: 0.0378
average test loss: 0.000734, accuracy: 0.0340
case acc: 0.03347259
case acc: 0.04331099
case acc: 0.036701854
case acc: 0.03012762
case acc: 0.034917586
case acc: 0.025348667
top acc: 0.0465 ::: bot acc: 0.0198
top acc: 0.0627 ::: bot acc: 0.0241
top acc: 0.0627 ::: bot acc: 0.0141
top acc: 0.0473 ::: bot acc: 0.0133
top acc: 0.0530 ::: bot acc: 0.0168
top acc: 0.0393 ::: bot acc: 0.0122
current epoch: 35
train loss is 0.000938
average val loss: 0.000872, accuracy: 0.0368
average test loss: 0.000698, accuracy: 0.0330
case acc: 0.032320112
case acc: 0.041282594
case acc: 0.036205046
case acc: 0.029435791
case acc: 0.034260094
case acc: 0.024569932
top acc: 0.0453 ::: bot acc: 0.0188
top acc: 0.0609 ::: bot acc: 0.0220
top acc: 0.0619 ::: bot acc: 0.0138
top acc: 0.0466 ::: bot acc: 0.0130
top acc: 0.0521 ::: bot acc: 0.0162
top acc: 0.0386 ::: bot acc: 0.0116
current epoch: 36
train loss is 0.000913
average val loss: 0.000878, accuracy: 0.0370
average test loss: 0.000702, accuracy: 0.0331
case acc: 0.03208561
case acc: 0.040693533
case acc: 0.03651387
case acc: 0.030058024
case acc: 0.034570515
case acc: 0.024942376
top acc: 0.0452 ::: bot acc: 0.0182
top acc: 0.0602 ::: bot acc: 0.0213
top acc: 0.0622 ::: bot acc: 0.0139
top acc: 0.0471 ::: bot acc: 0.0133
top acc: 0.0527 ::: bot acc: 0.0165
top acc: 0.0388 ::: bot acc: 0.0120
current epoch: 37
train loss is 0.000889
average val loss: 0.000771, accuracy: 0.0342
average test loss: 0.000607, accuracy: 0.0304
case acc: 0.028611187
case acc: 0.03688304
case acc: 0.034264375
case acc: 0.02771097
case acc: 0.03219213
case acc: 0.022818543
top acc: 0.0417 ::: bot acc: 0.0149
top acc: 0.0564 ::: bot acc: 0.0179
top acc: 0.0595 ::: bot acc: 0.0128
top acc: 0.0443 ::: bot acc: 0.0118
top acc: 0.0500 ::: bot acc: 0.0146
top acc: 0.0364 ::: bot acc: 0.0108
current epoch: 38
train loss is 0.000847
average val loss: 0.000686, accuracy: 0.0319
average test loss: 0.000535, accuracy: 0.0282
case acc: 0.026015157
case acc: 0.03339111
case acc: 0.032096647
case acc: 0.025898173
case acc: 0.030580595
case acc: 0.021009669
top acc: 0.0390 ::: bot acc: 0.0126
top acc: 0.0526 ::: bot acc: 0.0146
top acc: 0.0567 ::: bot acc: 0.0119
top acc: 0.0423 ::: bot acc: 0.0104
top acc: 0.0480 ::: bot acc: 0.0135
top acc: 0.0339 ::: bot acc: 0.0098
current epoch: 39
train loss is 0.000819
average val loss: 0.000612, accuracy: 0.0297
average test loss: 0.000468, accuracy: 0.0260
case acc: 0.023229918
case acc: 0.030045439
case acc: 0.030250315
case acc: 0.023979956
case acc: 0.02898438
case acc: 0.019328801
top acc: 0.0361 ::: bot acc: 0.0101
top acc: 0.0491 ::: bot acc: 0.0116
top acc: 0.0539 ::: bot acc: 0.0117
top acc: 0.0402 ::: bot acc: 0.0095
top acc: 0.0463 ::: bot acc: 0.0126
top acc: 0.0318 ::: bot acc: 0.0094
current epoch: 40
train loss is 0.000782
average val loss: 0.000585, accuracy: 0.0289
average test loss: 0.000448, accuracy: 0.0253
case acc: 0.02222608
case acc: 0.028679382
case acc: 0.029629827
case acc: 0.023418883
case acc: 0.028810926
case acc: 0.018760562
top acc: 0.0349 ::: bot acc: 0.0093
top acc: 0.0477 ::: bot acc: 0.0106
top acc: 0.0533 ::: bot acc: 0.0116
top acc: 0.0394 ::: bot acc: 0.0091
top acc: 0.0458 ::: bot acc: 0.0125
top acc: 0.0311 ::: bot acc: 0.0093
current epoch: 41
train loss is 0.000763
average val loss: 0.000513, accuracy: 0.0266
average test loss: 0.000386, accuracy: 0.0230
case acc: 0.019512724
case acc: 0.025421053
case acc: 0.027569557
case acc: 0.021655455
case acc: 0.02679232
case acc: 0.017043386
top acc: 0.0320 ::: bot acc: 0.0073
top acc: 0.0439 ::: bot acc: 0.0081
top acc: 0.0502 ::: bot acc: 0.0113
top acc: 0.0371 ::: bot acc: 0.0084
top acc: 0.0435 ::: bot acc: 0.0114
top acc: 0.0287 ::: bot acc: 0.0088
current epoch: 42
train loss is 0.000733
average val loss: 0.000432, accuracy: 0.0238
average test loss: 0.000320, accuracy: 0.0205
case acc: 0.016173147
case acc: 0.022035934
case acc: 0.025479238
case acc: 0.019454565
case acc: 0.024434969
case acc: 0.015231698
top acc: 0.0278 ::: bot acc: 0.0055
top acc: 0.0396 ::: bot acc: 0.0064
top acc: 0.0465 ::: bot acc: 0.0123
top acc: 0.0338 ::: bot acc: 0.0083
top acc: 0.0403 ::: bot acc: 0.0106
top acc: 0.0257 ::: bot acc: 0.0092
current epoch: 43
train loss is 0.000710
average val loss: 0.000376, accuracy: 0.0217
average test loss: 0.000277, accuracy: 0.0187
case acc: 0.014012316
case acc: 0.019723937
case acc: 0.023733398
case acc: 0.01817669
case acc: 0.022645159
case acc: 0.014156137
top acc: 0.0248 ::: bot acc: 0.0051
top acc: 0.0361 ::: bot acc: 0.0065
top acc: 0.0434 ::: bot acc: 0.0135
top acc: 0.0316 ::: bot acc: 0.0090
top acc: 0.0378 ::: bot acc: 0.0102
top acc: 0.0235 ::: bot acc: 0.0105
current epoch: 44
train loss is 0.000694
average val loss: 0.000358, accuracy: 0.0210
average test loss: 0.000263, accuracy: 0.0182
case acc: 0.013158171
case acc: 0.018506499
case acc: 0.023320368
case acc: 0.017686483
case acc: 0.022414027
case acc: 0.013903906
top acc: 0.0232 ::: bot acc: 0.0054
top acc: 0.0342 ::: bot acc: 0.0068
top acc: 0.0423 ::: bot acc: 0.0146
top acc: 0.0308 ::: bot acc: 0.0090
top acc: 0.0375 ::: bot acc: 0.0103
top acc: 0.0230 ::: bot acc: 0.0105
current epoch: 45
train loss is 0.000686
average val loss: 0.000328, accuracy: 0.0199
average test loss: 0.000243, accuracy: 0.0173
case acc: 0.012095241
case acc: 0.017261611
case acc: 0.022212926
case acc: 0.017059231
case acc: 0.021636305
case acc: 0.013728382
top acc: 0.0211 ::: bot acc: 0.0065
top acc: 0.0319 ::: bot acc: 0.0078
top acc: 0.0401 ::: bot acc: 0.0159
top acc: 0.0298 ::: bot acc: 0.0095
top acc: 0.0362 ::: bot acc: 0.0104
top acc: 0.0226 ::: bot acc: 0.0111
current epoch: 46
train loss is 0.000672
average val loss: 0.000276, accuracy: 0.0178
average test loss: 0.000207, accuracy: 0.0157
case acc: 0.010521737
case acc: 0.015456815
case acc: 0.020697072
case acc: 0.015484281
case acc: 0.019362716
case acc: 0.012736302
top acc: 0.0171 ::: bot acc: 0.0098
top acc: 0.0269 ::: bot acc: 0.0118
top acc: 0.0354 ::: bot acc: 0.0197
top acc: 0.0261 ::: bot acc: 0.0119
top acc: 0.0328 ::: bot acc: 0.0105
top acc: 0.0196 ::: bot acc: 0.0136
current epoch: 47
train loss is 0.000661
average val loss: 0.000248, accuracy: 0.0166
average test loss: 0.000190, accuracy: 0.0149
case acc: 0.009853319
case acc: 0.0146220615
case acc: 0.02008004
case acc: 0.01455175
case acc: 0.018218566
case acc: 0.012074765
top acc: 0.0136 ::: bot acc: 0.0130
top acc: 0.0234 ::: bot acc: 0.0153
top acc: 0.0318 ::: bot acc: 0.0234
top acc: 0.0235 ::: bot acc: 0.0142
top acc: 0.0306 ::: bot acc: 0.0112
top acc: 0.0173 ::: bot acc: 0.0153
current epoch: 48
train loss is 0.000655
average val loss: 0.000228, accuracy: 0.0158
average test loss: 0.000185, accuracy: 0.0146
case acc: 0.010037153
case acc: 0.014311216
case acc: 0.019789489
case acc: 0.014095389
case acc: 0.01736701
case acc: 0.012103408
top acc: 0.0108 ::: bot acc: 0.0160
top acc: 0.0194 ::: bot acc: 0.0194
top acc: 0.0282 ::: bot acc: 0.0272
top acc: 0.0214 ::: bot acc: 0.0168
top acc: 0.0291 ::: bot acc: 0.0121
top acc: 0.0159 ::: bot acc: 0.0173
current epoch: 49
train loss is 0.000659
average val loss: 0.000214, accuracy: 0.0153
average test loss: 0.000187, accuracy: 0.0147
case acc: 0.011014981
case acc: 0.01484195
case acc: 0.020686246
case acc: 0.013421505
case acc: 0.01621175
case acc: 0.012113034
top acc: 0.0069 ::: bot acc: 0.0202
top acc: 0.0145 ::: bot acc: 0.0245
top acc: 0.0232 ::: bot acc: 0.0322
top acc: 0.0175 ::: bot acc: 0.0199
top acc: 0.0261 ::: bot acc: 0.0142
top acc: 0.0137 ::: bot acc: 0.0196
current epoch: 50
train loss is 0.000665
average val loss: 0.000217, accuracy: 0.0155
average test loss: 0.000216, accuracy: 0.0160
case acc: 0.014211115
case acc: 0.017403498
case acc: 0.02268372
case acc: 0.014220157
case acc: 0.014671106
case acc: 0.012795016
top acc: 0.0055 ::: bot acc: 0.0257
top acc: 0.0097 ::: bot acc: 0.0309
top acc: 0.0167 ::: bot acc: 0.0388
top acc: 0.0129 ::: bot acc: 0.0248
top acc: 0.0222 ::: bot acc: 0.0175
top acc: 0.0100 ::: bot acc: 0.0233
LME_Co_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6804 6804 6804
1.8562728 -0.6288155 0.12137239 -0.16228472
Validation: 762 762 762
Testing: 744 744 744
pre-processing time: 0.0002532005310058594
the split date is 2010-01-01
train dropout: 0.6 test dropout: 0.1
net initializing with time: 0.0024161338806152344
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012907
average val loss: 0.005432, accuracy: 0.0991
average test loss: 0.005650, accuracy: 0.0998
case acc: 0.1390967
case acc: 0.0844746
case acc: 0.09794747
case acc: 0.095797874
case acc: 0.12588221
case acc: 0.055825844
top acc: 0.1138 ::: bot acc: 0.1610
top acc: 0.1058 ::: bot acc: 0.0638
top acc: 0.0622 ::: bot acc: 0.1341
top acc: 0.0701 ::: bot acc: 0.1218
top acc: 0.1031 ::: bot acc: 0.1488
top acc: 0.0294 ::: bot acc: 0.0811
current epoch: 2
train loss is 0.007748
average val loss: 0.002964, accuracy: 0.0516
average test loss: 0.003088, accuracy: 0.0543
case acc: 0.035984036
case acc: 0.17304112
case acc: 0.027676541
case acc: 0.019217554
case acc: 0.034001723
case acc: 0.035814524
top acc: 0.0172 ::: bot acc: 0.0556
top acc: 0.1935 ::: bot acc: 0.1541
top acc: 0.0434 ::: bot acc: 0.0297
top acc: 0.0277 ::: bot acc: 0.0247
top acc: 0.0129 ::: bot acc: 0.0566
top acc: 0.0631 ::: bot acc: 0.0136
current epoch: 3
train loss is 0.008256
average val loss: 0.007674, accuracy: 0.1034
average test loss: 0.007784, accuracy: 0.1031
case acc: 0.053875256
case acc: 0.24464008
case acc: 0.092773736
case acc: 0.0797606
case acc: 0.044304036
case acc: 0.103161424
top acc: 0.0773 ::: bot acc: 0.0333
top acc: 0.2651 ::: bot acc: 0.2251
top acc: 0.1282 ::: bot acc: 0.0564
top acc: 0.1067 ::: bot acc: 0.0529
top acc: 0.0669 ::: bot acc: 0.0219
top acc: 0.1337 ::: bot acc: 0.0749
current epoch: 4
train loss is 0.010538
average val loss: 0.015996, accuracy: 0.1673
average test loss: 0.016041, accuracy: 0.1667
case acc: 0.12415235
case acc: 0.3016332
case acc: 0.1624086
case acc: 0.14245284
case acc: 0.11024305
case acc: 0.15901284
top acc: 0.1488 ::: bot acc: 0.1014
top acc: 0.3217 ::: bot acc: 0.2821
top acc: 0.1981 ::: bot acc: 0.1261
top acc: 0.1688 ::: bot acc: 0.1166
top acc: 0.1327 ::: bot acc: 0.0873
top acc: 0.1896 ::: bot acc: 0.1309
current epoch: 5
train loss is 0.012872
average val loss: 0.013642, accuracy: 0.1542
average test loss: 0.013706, accuracy: 0.1537
case acc: 0.11666707
case acc: 0.27880082
case acc: 0.15275557
case acc: 0.12845825
case acc: 0.104838975
case acc: 0.14082621
top acc: 0.1410 ::: bot acc: 0.0942
top acc: 0.2993 ::: bot acc: 0.2596
top acc: 0.1883 ::: bot acc: 0.1158
top acc: 0.1549 ::: bot acc: 0.1023
top acc: 0.1278 ::: bot acc: 0.0820
top acc: 0.1711 ::: bot acc: 0.1129
current epoch: 6
train loss is 0.013144
average val loss: 0.002873, accuracy: 0.0540
average test loss: 0.002984, accuracy: 0.0551
case acc: 0.023442207
case acc: 0.16401021
case acc: 0.050738934
case acc: 0.030892821
case acc: 0.020479774
case acc: 0.04080853
top acc: 0.0437 ::: bot acc: 0.0095
top acc: 0.1847 ::: bot acc: 0.1443
top acc: 0.0842 ::: bot acc: 0.0172
top acc: 0.0529 ::: bot acc: 0.0132
top acc: 0.0369 ::: bot acc: 0.0100
top acc: 0.0697 ::: bot acc: 0.0158
current epoch: 7
train loss is 0.007498
average val loss: 0.001167, accuracy: 0.0357
average test loss: 0.001308, accuracy: 0.0397
case acc: 0.03238221
case acc: 0.09851791
case acc: 0.02712936
case acc: 0.029489432
case acc: 0.02700975
case acc: 0.023776585
top acc: 0.0143 ::: bot acc: 0.0516
top acc: 0.1192 ::: bot acc: 0.0786
top acc: 0.0284 ::: bot acc: 0.0439
top acc: 0.0129 ::: bot acc: 0.0509
top acc: 0.0096 ::: bot acc: 0.0474
top acc: 0.0216 ::: bot acc: 0.0379
current epoch: 8
train loss is 0.003366
average val loss: 0.000983, accuracy: 0.0305
average test loss: 0.001121, accuracy: 0.0351
case acc: 0.02590535
case acc: 0.09408912
case acc: 0.02696032
case acc: 0.024215717
case acc: 0.0181787
case acc: 0.021120068
top acc: 0.0124 ::: bot acc: 0.0428
top acc: 0.1146 ::: bot acc: 0.0748
top acc: 0.0314 ::: bot acc: 0.0415
top acc: 0.0120 ::: bot acc: 0.0434
top acc: 0.0137 ::: bot acc: 0.0320
top acc: 0.0300 ::: bot acc: 0.0282
current epoch: 9
train loss is 0.002462
average val loss: 0.001075, accuracy: 0.0308
average test loss: 0.001204, accuracy: 0.0348
case acc: 0.017573869
case acc: 0.10087505
case acc: 0.027358416
case acc: 0.01934854
case acc: 0.018919457
case acc: 0.024790103
top acc: 0.0222 ::: bot acc: 0.0248
top acc: 0.1212 ::: bot acc: 0.0812
top acc: 0.0437 ::: bot acc: 0.0281
top acc: 0.0260 ::: bot acc: 0.0274
top acc: 0.0340 ::: bot acc: 0.0111
top acc: 0.0479 ::: bot acc: 0.0125
current epoch: 10
train loss is 0.002412
average val loss: 0.000996, accuracy: 0.0306
average test loss: 0.001126, accuracy: 0.0344
case acc: 0.017023629
case acc: 0.09495095
case acc: 0.027437406
case acc: 0.019297846
case acc: 0.021683652
case acc: 0.026079262
top acc: 0.0255 ::: bot acc: 0.0213
top acc: 0.1153 ::: bot acc: 0.0755
top acc: 0.0438 ::: bot acc: 0.0278
top acc: 0.0275 ::: bot acc: 0.0256
top acc: 0.0392 ::: bot acc: 0.0085
top acc: 0.0498 ::: bot acc: 0.0119
current epoch: 11
train loss is 0.002167
average val loss: 0.000856, accuracy: 0.0290
average test loss: 0.000986, accuracy: 0.0328
case acc: 0.017010622
case acc: 0.085902885
case acc: 0.027018424
case acc: 0.019320033
case acc: 0.022049256
case acc: 0.025452351
top acc: 0.0252 ::: bot acc: 0.0213
top acc: 0.1062 ::: bot acc: 0.0665
top acc: 0.0409 ::: bot acc: 0.0314
top acc: 0.0266 ::: bot acc: 0.0265
top acc: 0.0403 ::: bot acc: 0.0083
top acc: 0.0489 ::: bot acc: 0.0118
current epoch: 12
train loss is 0.001922
average val loss: 0.000701, accuracy: 0.0266
average test loss: 0.000836, accuracy: 0.0308
case acc: 0.017310392
case acc: 0.07549464
case acc: 0.027092336
case acc: 0.0192745
case acc: 0.02140269
case acc: 0.02423845
top acc: 0.0229 ::: bot acc: 0.0238
top acc: 0.0959 ::: bot acc: 0.0559
top acc: 0.0364 ::: bot acc: 0.0367
top acc: 0.0232 ::: bot acc: 0.0292
top acc: 0.0388 ::: bot acc: 0.0088
top acc: 0.0462 ::: bot acc: 0.0134
current epoch: 13
train loss is 0.001647
average val loss: 0.000656, accuracy: 0.0264
average test loss: 0.000784, accuracy: 0.0302
case acc: 0.017244665
case acc: 0.07052968
case acc: 0.026563851
case acc: 0.01939655
case acc: 0.022938399
case acc: 0.024688954
top acc: 0.0264 ::: bot acc: 0.0213
top acc: 0.0908 ::: bot acc: 0.0509
top acc: 0.0359 ::: bot acc: 0.0358
top acc: 0.0253 ::: bot acc: 0.0277
top acc: 0.0410 ::: bot acc: 0.0085
top acc: 0.0476 ::: bot acc: 0.0121
current epoch: 14
train loss is 0.001503
average val loss: 0.000598, accuracy: 0.0257
average test loss: 0.000731, accuracy: 0.0295
case acc: 0.017103685
case acc: 0.06496493
case acc: 0.026469752
case acc: 0.019252649
case acc: 0.023713073
case acc: 0.025465231
top acc: 0.0282 ::: bot acc: 0.0191
top acc: 0.0853 ::: bot acc: 0.0452
top acc: 0.0363 ::: bot acc: 0.0356
top acc: 0.0263 ::: bot acc: 0.0271
top acc: 0.0426 ::: bot acc: 0.0082
top acc: 0.0488 ::: bot acc: 0.0118
current epoch: 15
train loss is 0.001358
average val loss: 0.000549, accuracy: 0.0250
average test loss: 0.000682, accuracy: 0.0287
case acc: 0.017030524
case acc: 0.060193233
case acc: 0.026505815
case acc: 0.019138869
case acc: 0.023862606
case acc: 0.025538517
top acc: 0.0293 ::: bot acc: 0.0176
top acc: 0.0805 ::: bot acc: 0.0405
top acc: 0.0357 ::: bot acc: 0.0364
top acc: 0.0265 ::: bot acc: 0.0259
top acc: 0.0431 ::: bot acc: 0.0080
top acc: 0.0490 ::: bot acc: 0.0119
current epoch: 16
train loss is 0.001316
average val loss: 0.000546, accuracy: 0.0256
average test loss: 0.000682, accuracy: 0.0290
case acc: 0.018013008
case acc: 0.057929378
case acc: 0.026664978
case acc: 0.01903976
case acc: 0.025506817
case acc: 0.026902981
top acc: 0.0330 ::: bot acc: 0.0147
top acc: 0.0784 ::: bot acc: 0.0381
top acc: 0.0382 ::: bot acc: 0.0340
top acc: 0.0295 ::: bot acc: 0.0228
top acc: 0.0450 ::: bot acc: 0.0087
top acc: 0.0512 ::: bot acc: 0.0112
current epoch: 17
train loss is 0.001220
average val loss: 0.000522, accuracy: 0.0253
average test loss: 0.000654, accuracy: 0.0286
case acc: 0.018127387
case acc: 0.05448304
case acc: 0.026640944
case acc: 0.019439263
case acc: 0.02574845
case acc: 0.027072012
top acc: 0.0341 ::: bot acc: 0.0132
top acc: 0.0749 ::: bot acc: 0.0348
top acc: 0.0388 ::: bot acc: 0.0330
top acc: 0.0309 ::: bot acc: 0.0222
top acc: 0.0457 ::: bot acc: 0.0087
top acc: 0.0515 ::: bot acc: 0.0108
current epoch: 18
train loss is 0.001167
average val loss: 0.000488, accuracy: 0.0247
average test loss: 0.000622, accuracy: 0.0280
case acc: 0.018803688
case acc: 0.05066266
case acc: 0.026873209
case acc: 0.01945775
case acc: 0.025105616
case acc: 0.0270494
top acc: 0.0355 ::: bot acc: 0.0124
top acc: 0.0710 ::: bot acc: 0.0311
top acc: 0.0398 ::: bot acc: 0.0326
top acc: 0.0314 ::: bot acc: 0.0217
top acc: 0.0448 ::: bot acc: 0.0084
top acc: 0.0515 ::: bot acc: 0.0114
current epoch: 19
train loss is 0.001090
average val loss: 0.000462, accuracy: 0.0242
average test loss: 0.000597, accuracy: 0.0275
case acc: 0.019089175
case acc: 0.04791134
case acc: 0.026984999
case acc: 0.019510496
case acc: 0.024867844
case acc: 0.026775287
top acc: 0.0363 ::: bot acc: 0.0113
top acc: 0.0681 ::: bot acc: 0.0285
top acc: 0.0402 ::: bot acc: 0.0319
top acc: 0.0320 ::: bot acc: 0.0208
top acc: 0.0439 ::: bot acc: 0.0088
top acc: 0.0511 ::: bot acc: 0.0111
current epoch: 20
train loss is 0.001023
average val loss: 0.000449, accuracy: 0.0241
average test loss: 0.000583, accuracy: 0.0273
case acc: 0.019967986
case acc: 0.045859877
case acc: 0.026945135
case acc: 0.019673975
case acc: 0.024393762
case acc: 0.026873454
top acc: 0.0380 ::: bot acc: 0.0108
top acc: 0.0661 ::: bot acc: 0.0265
top acc: 0.0417 ::: bot acc: 0.0300
top acc: 0.0327 ::: bot acc: 0.0203
top acc: 0.0435 ::: bot acc: 0.0083
top acc: 0.0514 ::: bot acc: 0.0113
current epoch: 21
train loss is 0.000981
average val loss: 0.000446, accuracy: 0.0242
average test loss: 0.000582, accuracy: 0.0273
case acc: 0.021129612
case acc: 0.044158034
case acc: 0.02744268
case acc: 0.020272728
case acc: 0.02439445
case acc: 0.026620666
top acc: 0.0402 ::: bot acc: 0.0104
top acc: 0.0647 ::: bot acc: 0.0245
top acc: 0.0435 ::: bot acc: 0.0288
top acc: 0.0345 ::: bot acc: 0.0192
top acc: 0.0435 ::: bot acc: 0.0086
top acc: 0.0513 ::: bot acc: 0.0106
current epoch: 22
train loss is 0.000976
average val loss: 0.000460, accuracy: 0.0249
average test loss: 0.000598, accuracy: 0.0280
case acc: 0.022653397
case acc: 0.043863066
case acc: 0.028092392
case acc: 0.020953976
case acc: 0.024682371
case acc: 0.027492162
top acc: 0.0424 ::: bot acc: 0.0099
top acc: 0.0644 ::: bot acc: 0.0242
top acc: 0.0465 ::: bot acc: 0.0259
top acc: 0.0362 ::: bot acc: 0.0174
top acc: 0.0439 ::: bot acc: 0.0084
top acc: 0.0523 ::: bot acc: 0.0109
current epoch: 23
train loss is 0.000951
average val loss: 0.000428, accuracy: 0.0240
average test loss: 0.000564, accuracy: 0.0270
case acc: 0.022311088
case acc: 0.040957317
case acc: 0.028190233
case acc: 0.020681528
case acc: 0.02346158
case acc: 0.02634939
top acc: 0.0423 ::: bot acc: 0.0099
top acc: 0.0612 ::: bot acc: 0.0217
top acc: 0.0462 ::: bot acc: 0.0259
top acc: 0.0358 ::: bot acc: 0.0179
top acc: 0.0423 ::: bot acc: 0.0082
top acc: 0.0506 ::: bot acc: 0.0111
current epoch: 24
train loss is 0.000901
average val loss: 0.000424, accuracy: 0.0240
average test loss: 0.000560, accuracy: 0.0270
case acc: 0.0231309
case acc: 0.039746817
case acc: 0.028618064
case acc: 0.021159122
case acc: 0.023062345
case acc: 0.026235009
top acc: 0.0432 ::: bot acc: 0.0100
top acc: 0.0600 ::: bot acc: 0.0207
top acc: 0.0477 ::: bot acc: 0.0244
top acc: 0.0366 ::: bot acc: 0.0176
top acc: 0.0418 ::: bot acc: 0.0082
top acc: 0.0503 ::: bot acc: 0.0113
current epoch: 25
train loss is 0.000904
average val loss: 0.000490, accuracy: 0.0264
average test loss: 0.000627, accuracy: 0.0290
case acc: 0.02623431
case acc: 0.04194378
case acc: 0.029919801
case acc: 0.022759993
case acc: 0.025109151
case acc: 0.02788457
top acc: 0.0475 ::: bot acc: 0.0106
top acc: 0.0625 ::: bot acc: 0.0223
top acc: 0.0522 ::: bot acc: 0.0200
top acc: 0.0403 ::: bot acc: 0.0151
top acc: 0.0447 ::: bot acc: 0.0084
top acc: 0.0530 ::: bot acc: 0.0108
current epoch: 26
train loss is 0.000919
average val loss: 0.000547, accuracy: 0.0283
average test loss: 0.000683, accuracy: 0.0305
case acc: 0.028431753
case acc: 0.04334645
case acc: 0.031483624
case acc: 0.024224985
case acc: 0.026442058
case acc: 0.029127853
top acc: 0.0505 ::: bot acc: 0.0114
top acc: 0.0639 ::: bot acc: 0.0238
top acc: 0.0558 ::: bot acc: 0.0171
top acc: 0.0430 ::: bot acc: 0.0139
top acc: 0.0464 ::: bot acc: 0.0089
top acc: 0.0549 ::: bot acc: 0.0106
current epoch: 27
train loss is 0.000908
average val loss: 0.000531, accuracy: 0.0278
average test loss: 0.000667, accuracy: 0.0301
case acc: 0.028494637
case acc: 0.041998956
case acc: 0.031809017
case acc: 0.02405761
case acc: 0.025842959
case acc: 0.028209887
top acc: 0.0505 ::: bot acc: 0.0114
top acc: 0.0625 ::: bot acc: 0.0226
top acc: 0.0567 ::: bot acc: 0.0164
top acc: 0.0429 ::: bot acc: 0.0136
top acc: 0.0456 ::: bot acc: 0.0087
top acc: 0.0535 ::: bot acc: 0.0110
current epoch: 28
train loss is 0.000886
average val loss: 0.000547, accuracy: 0.0284
average test loss: 0.000683, accuracy: 0.0305
case acc: 0.029501496
case acc: 0.041857675
case acc: 0.032267947
case acc: 0.024777131
case acc: 0.026752729
case acc: 0.028122382
top acc: 0.0518 ::: bot acc: 0.0120
top acc: 0.0622 ::: bot acc: 0.0225
top acc: 0.0584 ::: bot acc: 0.0143
top acc: 0.0439 ::: bot acc: 0.0138
top acc: 0.0467 ::: bot acc: 0.0092
top acc: 0.0532 ::: bot acc: 0.0109
current epoch: 29
train loss is 0.000900
average val loss: 0.000631, accuracy: 0.0310
average test loss: 0.000764, accuracy: 0.0327
case acc: 0.03240245
case acc: 0.044008117
case acc: 0.034324564
case acc: 0.02654052
case acc: 0.028910061
case acc: 0.02979015
top acc: 0.0554 ::: bot acc: 0.0139
top acc: 0.0645 ::: bot acc: 0.0245
top acc: 0.0624 ::: bot acc: 0.0125
top acc: 0.0469 ::: bot acc: 0.0129
top acc: 0.0497 ::: bot acc: 0.0097
top acc: 0.0557 ::: bot acc: 0.0110
current epoch: 30
train loss is 0.000893
average val loss: 0.000662, accuracy: 0.0319
average test loss: 0.000800, accuracy: 0.0336
case acc: 0.033852395
case acc: 0.044334058
case acc: 0.035579123
case acc: 0.02734379
case acc: 0.030188331
case acc: 0.030154664
top acc: 0.0569 ::: bot acc: 0.0149
top acc: 0.0645 ::: bot acc: 0.0250
top acc: 0.0644 ::: bot acc: 0.0120
top acc: 0.0484 ::: bot acc: 0.0127
top acc: 0.0516 ::: bot acc: 0.0104
top acc: 0.0563 ::: bot acc: 0.0111
current epoch: 31
train loss is 0.000883
average val loss: 0.000646, accuracy: 0.0315
average test loss: 0.000784, accuracy: 0.0332
case acc: 0.033292837
case acc: 0.04302738
case acc: 0.035955667
case acc: 0.027114872
case acc: 0.030046307
case acc: 0.029554222
top acc: 0.0563 ::: bot acc: 0.0145
top acc: 0.0633 ::: bot acc: 0.0237
top acc: 0.0650 ::: bot acc: 0.0122
top acc: 0.0481 ::: bot acc: 0.0126
top acc: 0.0510 ::: bot acc: 0.0104
top acc: 0.0556 ::: bot acc: 0.0108
current epoch: 32
train loss is 0.000879
average val loss: 0.000661, accuracy: 0.0319
average test loss: 0.000796, accuracy: 0.0335
case acc: 0.034056816
case acc: 0.04256308
case acc: 0.03647909
case acc: 0.027637877
case acc: 0.030548355
case acc: 0.029634867
top acc: 0.0573 ::: bot acc: 0.0150
top acc: 0.0628 ::: bot acc: 0.0231
top acc: 0.0659 ::: bot acc: 0.0119
top acc: 0.0489 ::: bot acc: 0.0123
top acc: 0.0518 ::: bot acc: 0.0105
top acc: 0.0556 ::: bot acc: 0.0109
current epoch: 33
train loss is 0.000882
average val loss: 0.000676, accuracy: 0.0324
average test loss: 0.000814, accuracy: 0.0339
case acc: 0.034637805
case acc: 0.042513132
case acc: 0.03731236
case acc: 0.028395532
case acc: 0.03111207
case acc: 0.029725397
top acc: 0.0578 ::: bot acc: 0.0154
top acc: 0.0628 ::: bot acc: 0.0231
top acc: 0.0674 ::: bot acc: 0.0118
top acc: 0.0498 ::: bot acc: 0.0127
top acc: 0.0524 ::: bot acc: 0.0110
top acc: 0.0557 ::: bot acc: 0.0108
current epoch: 34
train loss is 0.000856
average val loss: 0.000687, accuracy: 0.0327
average test loss: 0.000817, accuracy: 0.0340
case acc: 0.03470263
case acc: 0.041850742
case acc: 0.037925042
case acc: 0.028660659
case acc: 0.031191014
case acc: 0.029806696
top acc: 0.0580 ::: bot acc: 0.0155
top acc: 0.0623 ::: bot acc: 0.0225
top acc: 0.0682 ::: bot acc: 0.0116
top acc: 0.0504 ::: bot acc: 0.0123
top acc: 0.0524 ::: bot acc: 0.0111
top acc: 0.0556 ::: bot acc: 0.0110
current epoch: 35
train loss is 0.000875
average val loss: 0.000691, accuracy: 0.0328
average test loss: 0.000823, accuracy: 0.0342
case acc: 0.034817927
case acc: 0.041476533
case acc: 0.038523097
case acc: 0.029014116
case acc: 0.031406503
case acc: 0.02972818
top acc: 0.0580 ::: bot acc: 0.0156
top acc: 0.0617 ::: bot acc: 0.0222
top acc: 0.0691 ::: bot acc: 0.0116
top acc: 0.0510 ::: bot acc: 0.0125
top acc: 0.0528 ::: bot acc: 0.0110
top acc: 0.0556 ::: bot acc: 0.0110
current epoch: 36
train loss is 0.000858
average val loss: 0.000752, accuracy: 0.0345
average test loss: 0.000891, accuracy: 0.0358
case acc: 0.036474798
case acc: 0.042914867
case acc: 0.04028411
case acc: 0.030909231
case acc: 0.033339355
case acc: 0.031025497
top acc: 0.0599 ::: bot acc: 0.0170
top acc: 0.0633 ::: bot acc: 0.0235
top acc: 0.0720 ::: bot acc: 0.0113
top acc: 0.0535 ::: bot acc: 0.0131
top acc: 0.0550 ::: bot acc: 0.0122
top acc: 0.0576 ::: bot acc: 0.0111
current epoch: 37
train loss is 0.000858
average val loss: 0.000746, accuracy: 0.0344
average test loss: 0.000884, accuracy: 0.0357
case acc: 0.03635554
case acc: 0.04177844
case acc: 0.040257078
case acc: 0.030896418
case acc: 0.033736322
case acc: 0.031084657
top acc: 0.0598 ::: bot acc: 0.0168
top acc: 0.0621 ::: bot acc: 0.0224
top acc: 0.0719 ::: bot acc: 0.0116
top acc: 0.0537 ::: bot acc: 0.0128
top acc: 0.0556 ::: bot acc: 0.0126
top acc: 0.0574 ::: bot acc: 0.0113
current epoch: 38
train loss is 0.000844
average val loss: 0.000725, accuracy: 0.0338
average test loss: 0.000864, accuracy: 0.0351
case acc: 0.035550054
case acc: 0.04011915
case acc: 0.039829656
case acc: 0.03070633
case acc: 0.03405135
case acc: 0.030632654
top acc: 0.0591 ::: bot acc: 0.0160
top acc: 0.0605 ::: bot acc: 0.0210
top acc: 0.0714 ::: bot acc: 0.0112
top acc: 0.0533 ::: bot acc: 0.0131
top acc: 0.0560 ::: bot acc: 0.0128
top acc: 0.0571 ::: bot acc: 0.0111
current epoch: 39
train loss is 0.000830
average val loss: 0.000666, accuracy: 0.0322
average test loss: 0.000803, accuracy: 0.0337
case acc: 0.033433482
case acc: 0.03740237
case acc: 0.03872323
case acc: 0.02963245
case acc: 0.03315349
case acc: 0.029564813
top acc: 0.0567 ::: bot acc: 0.0144
top acc: 0.0575 ::: bot acc: 0.0185
top acc: 0.0696 ::: bot acc: 0.0114
top acc: 0.0520 ::: bot acc: 0.0125
top acc: 0.0547 ::: bot acc: 0.0124
top acc: 0.0557 ::: bot acc: 0.0110
current epoch: 40
train loss is 0.000784
average val loss: 0.000530, accuracy: 0.0280
average test loss: 0.000665, accuracy: 0.0300
case acc: 0.028993594
case acc: 0.032196466
case acc: 0.035706766
case acc: 0.026821556
case acc: 0.029503508
case acc: 0.026692795
top acc: 0.0512 ::: bot acc: 0.0117
top acc: 0.0521 ::: bot acc: 0.0142
top acc: 0.0647 ::: bot acc: 0.0120
top acc: 0.0474 ::: bot acc: 0.0128
top acc: 0.0503 ::: bot acc: 0.0100
top acc: 0.0512 ::: bot acc: 0.0108
current epoch: 41
train loss is 0.000726
average val loss: 0.000480, accuracy: 0.0264
average test loss: 0.000616, accuracy: 0.0286
case acc: 0.027136158
case acc: 0.029669827
case acc: 0.034413073
case acc: 0.025942689
case acc: 0.028586317
case acc: 0.02574527
top acc: 0.0487 ::: bot acc: 0.0110
top acc: 0.0492 ::: bot acc: 0.0119
top acc: 0.0626 ::: bot acc: 0.0125
top acc: 0.0460 ::: bot acc: 0.0133
top acc: 0.0493 ::: bot acc: 0.0096
top acc: 0.0498 ::: bot acc: 0.0112
current epoch: 42
train loss is 0.000710
average val loss: 0.000402, accuracy: 0.0236
average test loss: 0.000538, accuracy: 0.0263
case acc: 0.024476208
case acc: 0.025838926
case acc: 0.03275638
case acc: 0.024004446
case acc: 0.026500484
case acc: 0.024196258
top acc: 0.0453 ::: bot acc: 0.0101
top acc: 0.0446 ::: bot acc: 0.0093
top acc: 0.0591 ::: bot acc: 0.0147
top acc: 0.0427 ::: bot acc: 0.0138
top acc: 0.0466 ::: bot acc: 0.0088
top acc: 0.0467 ::: bot acc: 0.0123
current epoch: 43
train loss is 0.000682
average val loss: 0.000366, accuracy: 0.0222
average test loss: 0.000502, accuracy: 0.0252
case acc: 0.023006221
case acc: 0.023609743
case acc: 0.03169881
case acc: 0.023352461
case acc: 0.025725553
case acc: 0.023859244
top acc: 0.0429 ::: bot acc: 0.0105
top acc: 0.0419 ::: bot acc: 0.0083
top acc: 0.0566 ::: bot acc: 0.0162
top acc: 0.0415 ::: bot acc: 0.0143
top acc: 0.0455 ::: bot acc: 0.0086
top acc: 0.0460 ::: bot acc: 0.0128
current epoch: 44
train loss is 0.000655
average val loss: 0.000289, accuracy: 0.0192
average test loss: 0.000428, accuracy: 0.0228
case acc: 0.019815251
case acc: 0.019629443
case acc: 0.029843213
case acc: 0.02164247
case acc: 0.02333181
case acc: 0.022553032
top acc: 0.0379 ::: bot acc: 0.0110
top acc: 0.0363 ::: bot acc: 0.0078
top acc: 0.0515 ::: bot acc: 0.0210
top acc: 0.0378 ::: bot acc: 0.0168
top acc: 0.0421 ::: bot acc: 0.0083
top acc: 0.0428 ::: bot acc: 0.0154
current epoch: 45
train loss is 0.000626
average val loss: 0.000243, accuracy: 0.0173
average test loss: 0.000381, accuracy: 0.0211
case acc: 0.01803289
case acc: 0.01692269
case acc: 0.028370269
case acc: 0.02022893
case acc: 0.021566657
case acc: 0.021695668
top acc: 0.0340 ::: bot acc: 0.0136
top acc: 0.0314 ::: bot acc: 0.0094
top acc: 0.0470 ::: bot acc: 0.0253
top acc: 0.0345 ::: bot acc: 0.0188
top acc: 0.0392 ::: bot acc: 0.0088
top acc: 0.0402 ::: bot acc: 0.0174
current epoch: 46
train loss is 0.000609
average val loss: 0.000198, accuracy: 0.0153
average test loss: 0.000337, accuracy: 0.0198
case acc: 0.01722054
case acc: 0.014871572
case acc: 0.026959524
case acc: 0.019318154
case acc: 0.019313496
case acc: 0.020934897
top acc: 0.0280 ::: bot acc: 0.0196
top acc: 0.0248 ::: bot acc: 0.0154
top acc: 0.0408 ::: bot acc: 0.0314
top acc: 0.0298 ::: bot acc: 0.0236
top acc: 0.0343 ::: bot acc: 0.0114
top acc: 0.0359 ::: bot acc: 0.0217
current epoch: 47
train loss is 0.000601
average val loss: 0.000188, accuracy: 0.0148
average test loss: 0.000328, accuracy: 0.0195
case acc: 0.017326534
case acc: 0.014556121
case acc: 0.026593514
case acc: 0.019238083
case acc: 0.018690292
case acc: 0.020844443
top acc: 0.0253 ::: bot acc: 0.0224
top acc: 0.0211 ::: bot acc: 0.0191
top acc: 0.0370 ::: bot acc: 0.0350
top acc: 0.0275 ::: bot acc: 0.0257
top acc: 0.0329 ::: bot acc: 0.0127
top acc: 0.0350 ::: bot acc: 0.0229
current epoch: 48
train loss is 0.000607
average val loss: 0.000188, accuracy: 0.0147
average test loss: 0.000329, accuracy: 0.0198
case acc: 0.01824088
case acc: 0.015424621
case acc: 0.026817912
case acc: 0.019496068
case acc: 0.017730001
case acc: 0.020800639
top acc: 0.0204 ::: bot acc: 0.0272
top acc: 0.0153 ::: bot acc: 0.0252
top acc: 0.0314 ::: bot acc: 0.0412
top acc: 0.0236 ::: bot acc: 0.0296
top acc: 0.0301 ::: bot acc: 0.0154
top acc: 0.0322 ::: bot acc: 0.0256
current epoch: 49
train loss is 0.000623
average val loss: 0.000218, accuracy: 0.0161
average test loss: 0.000359, accuracy: 0.0209
case acc: 0.020493094
case acc: 0.018412123
case acc: 0.02782114
case acc: 0.020659272
case acc: 0.016867246
case acc: 0.021214003
top acc: 0.0154 ::: bot acc: 0.0331
top acc: 0.0096 ::: bot acc: 0.0323
top acc: 0.0240 ::: bot acc: 0.0485
top acc: 0.0182 ::: bot acc: 0.0351
top acc: 0.0260 ::: bot acc: 0.0195
top acc: 0.0282 ::: bot acc: 0.0296
current epoch: 50
train loss is 0.000656
average val loss: 0.000286, accuracy: 0.0189
average test loss: 0.000429, accuracy: 0.0232
case acc: 0.023853788
case acc: 0.023630429
case acc: 0.030350134
case acc: 0.022864467
case acc: 0.016680282
case acc: 0.021997895
top acc: 0.0127 ::: bot acc: 0.0397
top acc: 0.0094 ::: bot acc: 0.0403
top acc: 0.0160 ::: bot acc: 0.0566
top acc: 0.0136 ::: bot acc: 0.0409
top acc: 0.0219 ::: bot acc: 0.0235
top acc: 0.0244 ::: bot acc: 0.0333
LME_Co_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6780 6780 6780
1.7082474 -0.6288155 0.12137239 -0.1537469
Validation: 756 756 756
Testing: 774 774 774
pre-processing time: 0.00023412704467773438
the split date is 2010-07-01
train dropout: 0.6 test dropout: 0.1
net initializing with time: 0.0028846263885498047
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012799
average val loss: 0.005838, accuracy: 0.1015
average test loss: 0.006011, accuracy: 0.1020
case acc: 0.145572
case acc: 0.07998362
case acc: 0.10452091
case acc: 0.09140213
case acc: 0.13331413
case acc: 0.057239216
top acc: 0.1226 ::: bot acc: 0.1672
top acc: 0.1023 ::: bot acc: 0.0593
top acc: 0.0708 ::: bot acc: 0.1370
top acc: 0.0592 ::: bot acc: 0.1226
top acc: 0.1004 ::: bot acc: 0.1622
top acc: 0.0295 ::: bot acc: 0.0841
current epoch: 2
train loss is 0.008189
average val loss: 0.002943, accuracy: 0.0542
average test loss: 0.003042, accuracy: 0.0561
case acc: 0.043003816
case acc: 0.16561888
case acc: 0.024214864
case acc: 0.023389723
case acc: 0.047738146
case acc: 0.032380763
top acc: 0.0217 ::: bot acc: 0.0641
top acc: 0.1876 ::: bot acc: 0.1444
top acc: 0.0317 ::: bot acc: 0.0344
top acc: 0.0370 ::: bot acc: 0.0274
top acc: 0.0222 ::: bot acc: 0.0724
top acc: 0.0590 ::: bot acc: 0.0109
current epoch: 3
train loss is 0.008525
average val loss: 0.007482, accuracy: 0.1004
average test loss: 0.007557, accuracy: 0.1000
case acc: 0.04755511
case acc: 0.24059191
case acc: 0.08801074
case acc: 0.08550297
case acc: 0.036466
case acc: 0.1021137
top acc: 0.0697 ::: bot acc: 0.0270
top acc: 0.2632 ::: bot acc: 0.2191
top acc: 0.1204 ::: bot acc: 0.0562
top acc: 0.1191 ::: bot acc: 0.0541
top acc: 0.0685 ::: bot acc: 0.0103
top acc: 0.1324 ::: bot acc: 0.0746
current epoch: 4
train loss is 0.010011
average val loss: 0.016119, accuracy: 0.1672
average test loss: 0.016111, accuracy: 0.1665
case acc: 0.121342905
case acc: 0.30067563
case acc: 0.1607817
case acc: 0.15131703
case acc: 0.10436998
case acc: 0.16072905
top acc: 0.1445 ::: bot acc: 0.0993
top acc: 0.3229 ::: bot acc: 0.2799
top acc: 0.1941 ::: bot acc: 0.1283
top acc: 0.1850 ::: bot acc: 0.1194
top acc: 0.1372 ::: bot acc: 0.0758
top acc: 0.1905 ::: bot acc: 0.1331
current epoch: 5
train loss is 0.012984
average val loss: 0.012688, accuracy: 0.1470
average test loss: 0.012702, accuracy: 0.1464
case acc: 0.10658476
case acc: 0.27079442
case acc: 0.14332376
case acc: 0.13024323
case acc: 0.091925144
case acc: 0.13568434
top acc: 0.1298 ::: bot acc: 0.0847
top acc: 0.2927 ::: bot acc: 0.2499
top acc: 0.1764 ::: bot acc: 0.1107
top acc: 0.1644 ::: bot acc: 0.0981
top acc: 0.1253 ::: bot acc: 0.0632
top acc: 0.1653 ::: bot acc: 0.1082
current epoch: 6
train loss is 0.012850
average val loss: 0.003143, accuracy: 0.0577
average test loss: 0.003191, accuracy: 0.0584
case acc: 0.02304605
case acc: 0.16584769
case acc: 0.052493673
case acc: 0.04103417
case acc: 0.023249622
case acc: 0.04467134
top acc: 0.0423 ::: bot acc: 0.0088
top acc: 0.1880 ::: bot acc: 0.1449
top acc: 0.0832 ::: bot acc: 0.0253
top acc: 0.0721 ::: bot acc: 0.0152
top acc: 0.0453 ::: bot acc: 0.0165
top acc: 0.0732 ::: bot acc: 0.0191
current epoch: 7
train loss is 0.007161
average val loss: 0.001304, accuracy: 0.0409
average test loss: 0.001383, accuracy: 0.0424
case acc: 0.038116485
case acc: 0.0930527
case acc: 0.027166288
case acc: 0.03167335
case acc: 0.040176526
case acc: 0.024293689
top acc: 0.0165 ::: bot acc: 0.0593
top acc: 0.1142 ::: bot acc: 0.0723
top acc: 0.0194 ::: bot acc: 0.0471
top acc: 0.0197 ::: bot acc: 0.0525
top acc: 0.0181 ::: bot acc: 0.0632
top acc: 0.0180 ::: bot acc: 0.0402
current epoch: 8
train loss is 0.003345
average val loss: 0.001108, accuracy: 0.0352
average test loss: 0.001173, accuracy: 0.0369
case acc: 0.028387044
case acc: 0.0915576
case acc: 0.02503296
case acc: 0.026554285
case acc: 0.028301014
case acc: 0.021630317
top acc: 0.0102 ::: bot acc: 0.0482
top acc: 0.1133 ::: bot acc: 0.0708
top acc: 0.0245 ::: bot acc: 0.0408
top acc: 0.0249 ::: bot acc: 0.0426
top acc: 0.0191 ::: bot acc: 0.0450
top acc: 0.0305 ::: bot acc: 0.0277
current epoch: 9
train loss is 0.002441
average val loss: 0.001136, accuracy: 0.0339
average test loss: 0.001201, accuracy: 0.0352
case acc: 0.01858204
case acc: 0.097066954
case acc: 0.024070442
case acc: 0.024165018
case acc: 0.023058055
case acc: 0.02448368
top acc: 0.0143 ::: bot acc: 0.0317
top acc: 0.1188 ::: bot acc: 0.0763
top acc: 0.0362 ::: bot acc: 0.0296
top acc: 0.0390 ::: bot acc: 0.0281
top acc: 0.0370 ::: bot acc: 0.0253
top acc: 0.0462 ::: bot acc: 0.0131
current epoch: 10
train loss is 0.002301
average val loss: 0.001063, accuracy: 0.0334
average test loss: 0.001128, accuracy: 0.0345
case acc: 0.017599367
case acc: 0.09158642
case acc: 0.024410894
case acc: 0.024503767
case acc: 0.022905877
case acc: 0.025827738
top acc: 0.0181 ::: bot acc: 0.0275
top acc: 0.1138 ::: bot acc: 0.0703
top acc: 0.0370 ::: bot acc: 0.0297
top acc: 0.0414 ::: bot acc: 0.0256
top acc: 0.0421 ::: bot acc: 0.0198
top acc: 0.0489 ::: bot acc: 0.0117
current epoch: 11
train loss is 0.002147
average val loss: 0.000914, accuracy: 0.0316
average test loss: 0.000967, accuracy: 0.0325
case acc: 0.01755958
case acc: 0.08138884
case acc: 0.023993323
case acc: 0.024077889
case acc: 0.02298174
case acc: 0.024898008
top acc: 0.0167 ::: bot acc: 0.0282
top acc: 0.1032 ::: bot acc: 0.0607
top acc: 0.0324 ::: bot acc: 0.0333
top acc: 0.0393 ::: bot acc: 0.0272
top acc: 0.0423 ::: bot acc: 0.0198
top acc: 0.0471 ::: bot acc: 0.0124
current epoch: 12
train loss is 0.001852
average val loss: 0.000852, accuracy: 0.0311
average test loss: 0.000914, accuracy: 0.0318
case acc: 0.0171073
case acc: 0.07603523
case acc: 0.024105424
case acc: 0.024236735
case acc: 0.023406133
case acc: 0.025909292
top acc: 0.0198 ::: bot acc: 0.0256
top acc: 0.0980 ::: bot acc: 0.0550
top acc: 0.0326 ::: bot acc: 0.0336
top acc: 0.0410 ::: bot acc: 0.0254
top acc: 0.0463 ::: bot acc: 0.0155
top acc: 0.0496 ::: bot acc: 0.0109
current epoch: 13
train loss is 0.001636
average val loss: 0.000752, accuracy: 0.0296
average test loss: 0.000805, accuracy: 0.0303
case acc: 0.017041793
case acc: 0.06744149
case acc: 0.024212161
case acc: 0.024022771
case acc: 0.02349505
case acc: 0.025305197
top acc: 0.0193 ::: bot acc: 0.0259
top acc: 0.0893 ::: bot acc: 0.0464
top acc: 0.0294 ::: bot acc: 0.0364
top acc: 0.0395 ::: bot acc: 0.0269
top acc: 0.0458 ::: bot acc: 0.0162
top acc: 0.0481 ::: bot acc: 0.0122
current epoch: 14
train loss is 0.001421
average val loss: 0.000685, accuracy: 0.0286
average test loss: 0.000745, accuracy: 0.0294
case acc: 0.016886618
case acc: 0.061318696
case acc: 0.024723567
case acc: 0.024137475
case acc: 0.023780309
case acc: 0.025331179
top acc: 0.0211 ::: bot acc: 0.0244
top acc: 0.0831 ::: bot acc: 0.0405
top acc: 0.0293 ::: bot acc: 0.0375
top acc: 0.0397 ::: bot acc: 0.0265
top acc: 0.0468 ::: bot acc: 0.0157
top acc: 0.0484 ::: bot acc: 0.0114
current epoch: 15
train loss is 0.001269
average val loss: 0.000640, accuracy: 0.0278
average test loss: 0.000698, accuracy: 0.0286
case acc: 0.016863234
case acc: 0.056486882
case acc: 0.024388602
case acc: 0.02415068
case acc: 0.02383635
case acc: 0.025651587
top acc: 0.0227 ::: bot acc: 0.0230
top acc: 0.0780 ::: bot acc: 0.0356
top acc: 0.0286 ::: bot acc: 0.0373
top acc: 0.0405 ::: bot acc: 0.0258
top acc: 0.0470 ::: bot acc: 0.0153
top acc: 0.0486 ::: bot acc: 0.0121
current epoch: 16
train loss is 0.001183
average val loss: 0.000642, accuracy: 0.0281
average test loss: 0.000702, accuracy: 0.0287
case acc: 0.016476726
case acc: 0.054891523
case acc: 0.023982828
case acc: 0.024936551
case acc: 0.02486192
case acc: 0.02699394
top acc: 0.0260 ::: bot acc: 0.0190
top acc: 0.0770 ::: bot acc: 0.0341
top acc: 0.0313 ::: bot acc: 0.0345
top acc: 0.0439 ::: bot acc: 0.0225
top acc: 0.0500 ::: bot acc: 0.0130
top acc: 0.0511 ::: bot acc: 0.0107
current epoch: 17
train loss is 0.001121
average val loss: 0.000620, accuracy: 0.0277
average test loss: 0.000675, accuracy: 0.0282
case acc: 0.016879197
case acc: 0.051574368
case acc: 0.024028173
case acc: 0.024897696
case acc: 0.024735611
case acc: 0.026968952
top acc: 0.0281 ::: bot acc: 0.0175
top acc: 0.0736 ::: bot acc: 0.0305
top acc: 0.0325 ::: bot acc: 0.0334
top acc: 0.0448 ::: bot acc: 0.0211
top acc: 0.0499 ::: bot acc: 0.0123
top acc: 0.0512 ::: bot acc: 0.0106
current epoch: 18
train loss is 0.001037
average val loss: 0.000604, accuracy: 0.0275
average test loss: 0.000657, accuracy: 0.0279
case acc: 0.01717917
case acc: 0.048814967
case acc: 0.023953982
case acc: 0.025547663
case acc: 0.024721615
case acc: 0.027159806
top acc: 0.0300 ::: bot acc: 0.0153
top acc: 0.0706 ::: bot acc: 0.0279
top acc: 0.0339 ::: bot acc: 0.0319
top acc: 0.0461 ::: bot acc: 0.0205
top acc: 0.0495 ::: bot acc: 0.0129
top acc: 0.0516 ::: bot acc: 0.0107
current epoch: 19
train loss is 0.001003
average val loss: 0.000592, accuracy: 0.0273
average test loss: 0.000650, accuracy: 0.0278
case acc: 0.01799666
case acc: 0.046868242
case acc: 0.024310168
case acc: 0.025880434
case acc: 0.024700364
case acc: 0.02728725
top acc: 0.0321 ::: bot acc: 0.0137
top acc: 0.0689 ::: bot acc: 0.0257
top acc: 0.0359 ::: bot acc: 0.0304
top acc: 0.0475 ::: bot acc: 0.0190
top acc: 0.0499 ::: bot acc: 0.0127
top acc: 0.0515 ::: bot acc: 0.0107
current epoch: 20
train loss is 0.000952
average val loss: 0.000592, accuracy: 0.0275
average test loss: 0.000647, accuracy: 0.0279
case acc: 0.01863079
case acc: 0.04549342
case acc: 0.024435567
case acc: 0.026384579
case acc: 0.02485324
case acc: 0.027304089
top acc: 0.0344 ::: bot acc: 0.0115
top acc: 0.0673 ::: bot acc: 0.0245
top acc: 0.0380 ::: bot acc: 0.0281
top acc: 0.0490 ::: bot acc: 0.0177
top acc: 0.0498 ::: bot acc: 0.0129
top acc: 0.0519 ::: bot acc: 0.0103
current epoch: 21
train loss is 0.000939
average val loss: 0.000606, accuracy: 0.0281
average test loss: 0.000660, accuracy: 0.0282
case acc: 0.019789485
case acc: 0.044795908
case acc: 0.024692306
case acc: 0.027142765
case acc: 0.02487142
case acc: 0.027968133
top acc: 0.0367 ::: bot acc: 0.0097
top acc: 0.0667 ::: bot acc: 0.0239
top acc: 0.0406 ::: bot acc: 0.0253
top acc: 0.0511 ::: bot acc: 0.0158
top acc: 0.0501 ::: bot acc: 0.0120
top acc: 0.0529 ::: bot acc: 0.0101
current epoch: 22
train loss is 0.000912
average val loss: 0.000653, accuracy: 0.0295
average test loss: 0.000705, accuracy: 0.0294
case acc: 0.021829935
case acc: 0.045751624
case acc: 0.025485799
case acc: 0.028614964
case acc: 0.02587082
case acc: 0.02911149
top acc: 0.0407 ::: bot acc: 0.0085
top acc: 0.0679 ::: bot acc: 0.0247
top acc: 0.0441 ::: bot acc: 0.0215
top acc: 0.0540 ::: bot acc: 0.0140
top acc: 0.0523 ::: bot acc: 0.0110
top acc: 0.0549 ::: bot acc: 0.0098
current epoch: 23
train loss is 0.000912
average val loss: 0.000693, accuracy: 0.0307
average test loss: 0.000745, accuracy: 0.0306
case acc: 0.024005227
case acc: 0.046029083
case acc: 0.02663906
case acc: 0.02993237
case acc: 0.026756883
case acc: 0.029979344
top acc: 0.0437 ::: bot acc: 0.0088
top acc: 0.0680 ::: bot acc: 0.0250
top acc: 0.0478 ::: bot acc: 0.0184
top acc: 0.0563 ::: bot acc: 0.0133
top acc: 0.0543 ::: bot acc: 0.0100
top acc: 0.0561 ::: bot acc: 0.0100
current epoch: 24
train loss is 0.000878
average val loss: 0.000640, accuracy: 0.0293
average test loss: 0.000693, accuracy: 0.0293
case acc: 0.023251463
case acc: 0.04276977
case acc: 0.026600331
case acc: 0.02904508
case acc: 0.025608517
case acc: 0.028362913
top acc: 0.0428 ::: bot acc: 0.0087
top acc: 0.0646 ::: bot acc: 0.0220
top acc: 0.0470 ::: bot acc: 0.0193
top acc: 0.0548 ::: bot acc: 0.0137
top acc: 0.0518 ::: bot acc: 0.0113
top acc: 0.0537 ::: bot acc: 0.0100
current epoch: 25
train loss is 0.000836
average val loss: 0.000675, accuracy: 0.0302
average test loss: 0.000722, accuracy: 0.0302
case acc: 0.024927542
case acc: 0.04304786
case acc: 0.027666314
case acc: 0.029985059
case acc: 0.02642217
case acc: 0.028864097
top acc: 0.0450 ::: bot acc: 0.0089
top acc: 0.0650 ::: bot acc: 0.0220
top acc: 0.0500 ::: bot acc: 0.0170
top acc: 0.0568 ::: bot acc: 0.0128
top acc: 0.0534 ::: bot acc: 0.0107
top acc: 0.0543 ::: bot acc: 0.0101
current epoch: 26
train loss is 0.000826
average val loss: 0.000681, accuracy: 0.0305
average test loss: 0.000736, accuracy: 0.0305
case acc: 0.025976827
case acc: 0.042694744
case acc: 0.028515853
case acc: 0.03057817
case acc: 0.026394237
case acc: 0.028837224
top acc: 0.0462 ::: bot acc: 0.0096
top acc: 0.0646 ::: bot acc: 0.0220
top acc: 0.0517 ::: bot acc: 0.0159
top acc: 0.0578 ::: bot acc: 0.0126
top acc: 0.0535 ::: bot acc: 0.0106
top acc: 0.0546 ::: bot acc: 0.0097
current epoch: 27
train loss is 0.000827
average val loss: 0.000700, accuracy: 0.0310
average test loss: 0.000752, accuracy: 0.0310
case acc: 0.027006844
case acc: 0.042408846
case acc: 0.029512767
case acc: 0.031401385
case acc: 0.026509857
case acc: 0.028884256
top acc: 0.0474 ::: bot acc: 0.0099
top acc: 0.0643 ::: bot acc: 0.0215
top acc: 0.0537 ::: bot acc: 0.0151
top acc: 0.0592 ::: bot acc: 0.0125
top acc: 0.0539 ::: bot acc: 0.0102
top acc: 0.0546 ::: bot acc: 0.0097
current epoch: 28
train loss is 0.000832
average val loss: 0.000761, accuracy: 0.0326
average test loss: 0.000813, accuracy: 0.0326
case acc: 0.029318912
case acc: 0.043660216
case acc: 0.031628255
case acc: 0.033166755
case acc: 0.02761644
case acc: 0.030234486
top acc: 0.0502 ::: bot acc: 0.0113
top acc: 0.0657 ::: bot acc: 0.0226
top acc: 0.0572 ::: bot acc: 0.0146
top acc: 0.0618 ::: bot acc: 0.0126
top acc: 0.0557 ::: bot acc: 0.0095
top acc: 0.0565 ::: bot acc: 0.0099
current epoch: 29
train loss is 0.000828
average val loss: 0.000802, accuracy: 0.0336
average test loss: 0.000852, accuracy: 0.0336
case acc: 0.030712184
case acc: 0.044108097
case acc: 0.03329
case acc: 0.034248862
case acc: 0.028488535
case acc: 0.030934852
top acc: 0.0523 ::: bot acc: 0.0118
top acc: 0.0658 ::: bot acc: 0.0233
top acc: 0.0594 ::: bot acc: 0.0147
top acc: 0.0633 ::: bot acc: 0.0124
top acc: 0.0574 ::: bot acc: 0.0091
top acc: 0.0575 ::: bot acc: 0.0100
current epoch: 30
train loss is 0.000828
average val loss: 0.000838, accuracy: 0.0346
average test loss: 0.000886, accuracy: 0.0345
case acc: 0.031969234
case acc: 0.04450611
case acc: 0.034558088
case acc: 0.035101175
case acc: 0.029390872
case acc: 0.03138817
top acc: 0.0536 ::: bot acc: 0.0129
top acc: 0.0664 ::: bot acc: 0.0233
top acc: 0.0613 ::: bot acc: 0.0149
top acc: 0.0647 ::: bot acc: 0.0123
top acc: 0.0590 ::: bot acc: 0.0084
top acc: 0.0578 ::: bot acc: 0.0105
current epoch: 31
train loss is 0.000827
average val loss: 0.000873, accuracy: 0.0355
average test loss: 0.000928, accuracy: 0.0355
case acc: 0.033231743
case acc: 0.044903874
case acc: 0.03609475
case acc: 0.036178797
case acc: 0.030386861
case acc: 0.032141246
top acc: 0.0552 ::: bot acc: 0.0135
top acc: 0.0669 ::: bot acc: 0.0240
top acc: 0.0637 ::: bot acc: 0.0152
top acc: 0.0663 ::: bot acc: 0.0123
top acc: 0.0607 ::: bot acc: 0.0082
top acc: 0.0591 ::: bot acc: 0.0103
current epoch: 32
train loss is 0.000836
average val loss: 0.000890, accuracy: 0.0359
average test loss: 0.000937, accuracy: 0.0358
case acc: 0.033527162
case acc: 0.04435345
case acc: 0.036871668
case acc: 0.036723986
case acc: 0.030914282
case acc: 0.032172296
top acc: 0.0554 ::: bot acc: 0.0138
top acc: 0.0663 ::: bot acc: 0.0234
top acc: 0.0644 ::: bot acc: 0.0155
top acc: 0.0669 ::: bot acc: 0.0125
top acc: 0.0612 ::: bot acc: 0.0086
top acc: 0.0591 ::: bot acc: 0.0104
current epoch: 33
train loss is 0.000824
average val loss: 0.000912, accuracy: 0.0364
average test loss: 0.000959, accuracy: 0.0363
case acc: 0.034231532
case acc: 0.043984395
case acc: 0.037632924
case acc: 0.03770308
case acc: 0.031383596
case acc: 0.032705538
top acc: 0.0562 ::: bot acc: 0.0144
top acc: 0.0658 ::: bot acc: 0.0229
top acc: 0.0655 ::: bot acc: 0.0156
top acc: 0.0682 ::: bot acc: 0.0129
top acc: 0.0623 ::: bot acc: 0.0081
top acc: 0.0597 ::: bot acc: 0.0109
current epoch: 34
train loss is 0.000817
average val loss: 0.000858, accuracy: 0.0351
average test loss: 0.000910, accuracy: 0.0351
case acc: 0.03289457
case acc: 0.041522127
case acc: 0.03683289
case acc: 0.036833
case acc: 0.031004472
case acc: 0.031563148
top acc: 0.0545 ::: bot acc: 0.0136
top acc: 0.0633 ::: bot acc: 0.0209
top acc: 0.0644 ::: bot acc: 0.0153
top acc: 0.0673 ::: bot acc: 0.0127
top acc: 0.0617 ::: bot acc: 0.0083
top acc: 0.0583 ::: bot acc: 0.0104
current epoch: 35
train loss is 0.000788
average val loss: 0.000807, accuracy: 0.0338
average test loss: 0.000859, accuracy: 0.0338
case acc: 0.031367734
case acc: 0.039029453
case acc: 0.036111232
case acc: 0.03592107
case acc: 0.030110443
case acc: 0.030468054
top acc: 0.0531 ::: bot acc: 0.0122
top acc: 0.0608 ::: bot acc: 0.0185
top acc: 0.0634 ::: bot acc: 0.0154
top acc: 0.0659 ::: bot acc: 0.0124
top acc: 0.0600 ::: bot acc: 0.0086
top acc: 0.0568 ::: bot acc: 0.0100
current epoch: 36
train loss is 0.000755
average val loss: 0.000803, accuracy: 0.0337
average test loss: 0.000855, accuracy: 0.0337
case acc: 0.030947875
case acc: 0.038135994
case acc: 0.0361293
case acc: 0.036204863
case acc: 0.030182745
case acc: 0.030485187
top acc: 0.0524 ::: bot acc: 0.0122
top acc: 0.0599 ::: bot acc: 0.0174
top acc: 0.0635 ::: bot acc: 0.0153
top acc: 0.0663 ::: bot acc: 0.0127
top acc: 0.0603 ::: bot acc: 0.0085
top acc: 0.0568 ::: bot acc: 0.0101
current epoch: 37
train loss is 0.000731
average val loss: 0.000722, accuracy: 0.0316
average test loss: 0.000773, accuracy: 0.0316
case acc: 0.028286017
case acc: 0.03485554
case acc: 0.03426081
case acc: 0.034511723
case acc: 0.02861697
case acc: 0.028977972
top acc: 0.0492 ::: bot acc: 0.0107
top acc: 0.0564 ::: bot acc: 0.0146
top acc: 0.0610 ::: bot acc: 0.0148
top acc: 0.0641 ::: bot acc: 0.0121
top acc: 0.0577 ::: bot acc: 0.0088
top acc: 0.0546 ::: bot acc: 0.0099
current epoch: 38
train loss is 0.000692
average val loss: 0.000640, accuracy: 0.0294
average test loss: 0.000696, accuracy: 0.0295
case acc: 0.025857313
case acc: 0.031137614
case acc: 0.032019526
case acc: 0.032831453
case acc: 0.027471295
case acc: 0.02743776
top acc: 0.0462 ::: bot acc: 0.0093
top acc: 0.0522 ::: bot acc: 0.0120
top acc: 0.0577 ::: bot acc: 0.0146
top acc: 0.0615 ::: bot acc: 0.0122
top acc: 0.0557 ::: bot acc: 0.0097
top acc: 0.0519 ::: bot acc: 0.0104
current epoch: 39
train loss is 0.000663
average val loss: 0.000569, accuracy: 0.0273
average test loss: 0.000624, accuracy: 0.0274
case acc: 0.023335844
case acc: 0.02756705
case acc: 0.03003431
case acc: 0.03121203
case acc: 0.026341178
case acc: 0.026102498
top acc: 0.0428 ::: bot acc: 0.0085
top acc: 0.0481 ::: bot acc: 0.0093
top acc: 0.0546 ::: bot acc: 0.0149
top acc: 0.0587 ::: bot acc: 0.0125
top acc: 0.0536 ::: bot acc: 0.0105
top acc: 0.0494 ::: bot acc: 0.0115
current epoch: 40
train loss is 0.000626
average val loss: 0.000545, accuracy: 0.0266
average test loss: 0.000600, accuracy: 0.0267
case acc: 0.022362428
case acc: 0.026263848
case acc: 0.02940309
case acc: 0.03058068
case acc: 0.026074886
case acc: 0.025522364
top acc: 0.0414 ::: bot acc: 0.0084
top acc: 0.0462 ::: bot acc: 0.0091
top acc: 0.0535 ::: bot acc: 0.0153
top acc: 0.0577 ::: bot acc: 0.0127
top acc: 0.0530 ::: bot acc: 0.0105
top acc: 0.0487 ::: bot acc: 0.0115
current epoch: 41
train loss is 0.000605
average val loss: 0.000480, accuracy: 0.0246
average test loss: 0.000540, accuracy: 0.0249
case acc: 0.020387672
case acc: 0.02307906
case acc: 0.0276224
case acc: 0.029149972
case acc: 0.02494691
case acc: 0.024221776
top acc: 0.0381 ::: bot acc: 0.0093
top acc: 0.0420 ::: bot acc: 0.0077
top acc: 0.0501 ::: bot acc: 0.0168
top acc: 0.0553 ::: bot acc: 0.0137
top acc: 0.0504 ::: bot acc: 0.0125
top acc: 0.0461 ::: bot acc: 0.0129
current epoch: 42
train loss is 0.000572
average val loss: 0.000420, accuracy: 0.0226
average test loss: 0.000481, accuracy: 0.0232
case acc: 0.01857092
case acc: 0.020208497
case acc: 0.026097002
case acc: 0.027396597
case acc: 0.023759961
case acc: 0.022909505
top acc: 0.0337 ::: bot acc: 0.0124
top acc: 0.0377 ::: bot acc: 0.0079
top acc: 0.0461 ::: bot acc: 0.0200
top acc: 0.0517 ::: bot acc: 0.0154
top acc: 0.0470 ::: bot acc: 0.0156
top acc: 0.0430 ::: bot acc: 0.0150
current epoch: 43
train loss is 0.000550
average val loss: 0.000378, accuracy: 0.0211
average test loss: 0.000443, accuracy: 0.0220
case acc: 0.017375303
case acc: 0.018118644
case acc: 0.025109746
case acc: 0.026417548
case acc: 0.023154322
case acc: 0.022017369
top acc: 0.0302 ::: bot acc: 0.0156
top acc: 0.0337 ::: bot acc: 0.0097
top acc: 0.0425 ::: bot acc: 0.0237
top acc: 0.0491 ::: bot acc: 0.0175
top acc: 0.0442 ::: bot acc: 0.0183
top acc: 0.0404 ::: bot acc: 0.0174
current epoch: 44
train loss is 0.000538
average val loss: 0.000367, accuracy: 0.0207
average test loss: 0.000431, accuracy: 0.0217
case acc: 0.017007219
case acc: 0.017279182
case acc: 0.02466839
case acc: 0.02601576
case acc: 0.023091368
case acc: 0.021954924
top acc: 0.0289 ::: bot acc: 0.0170
top acc: 0.0317 ::: bot acc: 0.0112
top acc: 0.0408 ::: bot acc: 0.0251
top acc: 0.0482 ::: bot acc: 0.0183
top acc: 0.0438 ::: bot acc: 0.0187
top acc: 0.0401 ::: bot acc: 0.0177
current epoch: 45
train loss is 0.000528
average val loss: 0.000348, accuracy: 0.0200
average test loss: 0.000416, accuracy: 0.0213
case acc: 0.01671653
case acc: 0.016525043
case acc: 0.024246216
case acc: 0.025490189
case acc: 0.02309576
case acc: 0.021733971
top acc: 0.0264 ::: bot acc: 0.0196
top acc: 0.0286 ::: bot acc: 0.0143
top acc: 0.0379 ::: bot acc: 0.0280
top acc: 0.0465 ::: bot acc: 0.0198
top acc: 0.0427 ::: bot acc: 0.0204
top acc: 0.0390 ::: bot acc: 0.0187
current epoch: 46
train loss is 0.000519
average val loss: 0.000325, accuracy: 0.0194
average test loss: 0.000396, accuracy: 0.0210
case acc: 0.016998274
case acc: 0.015982788
case acc: 0.024158828
case acc: 0.02454378
case acc: 0.022943141
case acc: 0.021372931
top acc: 0.0216 ::: bot acc: 0.0242
top acc: 0.0232 ::: bot acc: 0.0199
top acc: 0.0328 ::: bot acc: 0.0332
top acc: 0.0426 ::: bot acc: 0.0240
top acc: 0.0386 ::: bot acc: 0.0240
top acc: 0.0358 ::: bot acc: 0.0221
current epoch: 47
train loss is 0.000516
average val loss: 0.000322, accuracy: 0.0194
average test loss: 0.000392, accuracy: 0.0211
case acc: 0.017659899
case acc: 0.016052626
case acc: 0.024436243
case acc: 0.02407172
case acc: 0.022916827
case acc: 0.021237575
top acc: 0.0180 ::: bot acc: 0.0279
top acc: 0.0189 ::: bot acc: 0.0241
top acc: 0.0283 ::: bot acc: 0.0375
top acc: 0.0397 ::: bot acc: 0.0267
top acc: 0.0361 ::: bot acc: 0.0262
top acc: 0.0335 ::: bot acc: 0.0242
current epoch: 48
train loss is 0.000518
average val loss: 0.000329, accuracy: 0.0198
average test loss: 0.000403, accuracy: 0.0215
case acc: 0.018575853
case acc: 0.017204458
case acc: 0.025332924
case acc: 0.023793684
case acc: 0.023140162
case acc: 0.021234276
top acc: 0.0146 ::: bot acc: 0.0314
top acc: 0.0144 ::: bot acc: 0.0285
top acc: 0.0240 ::: bot acc: 0.0420
top acc: 0.0369 ::: bot acc: 0.0294
top acc: 0.0342 ::: bot acc: 0.0283
top acc: 0.0317 ::: bot acc: 0.0258
current epoch: 49
train loss is 0.000529
average val loss: 0.000358, accuracy: 0.0209
average test loss: 0.000435, accuracy: 0.0228
case acc: 0.020533891
case acc: 0.019406104
case acc: 0.027652917
case acc: 0.023969654
case acc: 0.023701482
case acc: 0.021496462
top acc: 0.0112 ::: bot acc: 0.0360
top acc: 0.0093 ::: bot acc: 0.0344
top acc: 0.0188 ::: bot acc: 0.0481
top acc: 0.0327 ::: bot acc: 0.0337
top acc: 0.0312 ::: bot acc: 0.0316
top acc: 0.0293 ::: bot acc: 0.0284
current epoch: 50
train loss is 0.000550
average val loss: 0.000423, accuracy: 0.0231
average test loss: 0.000504, accuracy: 0.0252
case acc: 0.024180258
case acc: 0.023715517
case acc: 0.03140035
case acc: 0.025116488
case acc: 0.02482088
case acc: 0.022068094
top acc: 0.0099 ::: bot acc: 0.0420
top acc: 0.0079 ::: bot acc: 0.0417
top acc: 0.0152 ::: bot acc: 0.0556
top acc: 0.0275 ::: bot acc: 0.0387
top acc: 0.0271 ::: bot acc: 0.0356
top acc: 0.0254 ::: bot acc: 0.0323
LME_Co_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6804 6804 6804
1.7082474 -0.6288155 0.12137239 -0.15229516
Validation: 762 762 762
Testing: 750 750 750
pre-processing time: 0.00021219253540039062
the split date is 2011-01-01
train dropout: 0.6 test dropout: 0.1
net initializing with time: 0.0022122859954833984
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012884
average val loss: 0.005480, accuracy: 0.0971
average test loss: 0.004841, accuracy: 0.0911
case acc: 0.13131204
case acc: 0.09283003
case acc: 0.084226
case acc: 0.07804684
case acc: 0.111955605
case acc: 0.04823405
top acc: 0.1072 ::: bot acc: 0.1570
top acc: 0.1146 ::: bot acc: 0.0697
top acc: 0.0481 ::: bot acc: 0.1200
top acc: 0.0525 ::: bot acc: 0.1047
top acc: 0.0759 ::: bot acc: 0.1478
top acc: 0.0192 ::: bot acc: 0.0769
current epoch: 2
train loss is 0.007976
average val loss: 0.003150, accuracy: 0.0564
average test loss: 0.003221, accuracy: 0.0552
case acc: 0.03182083
case acc: 0.17641051
case acc: 0.029120348
case acc: 0.022754986
case acc: 0.03264013
case acc: 0.03866223
top acc: 0.0104 ::: bot acc: 0.0563
top acc: 0.1980 ::: bot acc: 0.1532
top acc: 0.0527 ::: bot acc: 0.0197
top acc: 0.0410 ::: bot acc: 0.0130
top acc: 0.0141 ::: bot acc: 0.0605
top acc: 0.0663 ::: bot acc: 0.0147
current epoch: 3
train loss is 0.008389
average val loss: 0.008365, accuracy: 0.1080
average test loss: 0.009109, accuracy: 0.1154
case acc: 0.061832853
case acc: 0.2540943
case acc: 0.108085945
case acc: 0.09912819
case acc: 0.057427138
case acc: 0.11213104
top acc: 0.0862 ::: bot acc: 0.0361
top acc: 0.2753 ::: bot acc: 0.2321
top acc: 0.1448 ::: bot acc: 0.0721
top acc: 0.1251 ::: bot acc: 0.0726
top acc: 0.0923 ::: bot acc: 0.0219
top acc: 0.1425 ::: bot acc: 0.0817
current epoch: 4
train loss is 0.010298
average val loss: 0.017176, accuracy: 0.1730
average test loss: 0.018359, accuracy: 0.1803
case acc: 0.1339254
case acc: 0.31227055
case acc: 0.17902164
case acc: 0.16316701
case acc: 0.12377422
case acc: 0.16959032
top acc: 0.1583 ::: bot acc: 0.1084
top acc: 0.3332 ::: bot acc: 0.2906
top acc: 0.2158 ::: bot acc: 0.1431
top acc: 0.1896 ::: bot acc: 0.1361
top acc: 0.1586 ::: bot acc: 0.0887
top acc: 0.1998 ::: bot acc: 0.1397
current epoch: 5
train loss is 0.012955
average val loss: 0.013099, accuracy: 0.1493
average test loss: 0.014129, accuracy: 0.1565
case acc: 0.11540224
case acc: 0.27858588
case acc: 0.15793861
case acc: 0.13851115
case acc: 0.107740834
case acc: 0.1409051
top acc: 0.1403 ::: bot acc: 0.0893
top acc: 0.3002 ::: bot acc: 0.2562
top acc: 0.1945 ::: bot acc: 0.1219
top acc: 0.1660 ::: bot acc: 0.1103
top acc: 0.1428 ::: bot acc: 0.0725
top acc: 0.1712 ::: bot acc: 0.1106
current epoch: 6
train loss is 0.012510
average val loss: 0.003401, accuracy: 0.0615
average test loss: 0.003778, accuracy: 0.0675
case acc: 0.030939255
case acc: 0.17465077
case acc: 0.06616317
case acc: 0.04787349
case acc: 0.034274243
case acc: 0.05099685
top acc: 0.0532 ::: bot acc: 0.0094
top acc: 0.1957 ::: bot acc: 0.1530
top acc: 0.1017 ::: bot acc: 0.0316
top acc: 0.0743 ::: bot acc: 0.0209
top acc: 0.0640 ::: bot acc: 0.0104
top acc: 0.0802 ::: bot acc: 0.0231
current epoch: 7
train loss is 0.007302
average val loss: 0.001368, accuracy: 0.0417
average test loss: 0.001313, accuracy: 0.0387
case acc: 0.030518837
case acc: 0.09994406
case acc: 0.025970643
case acc: 0.022570422
case acc: 0.030227572
case acc: 0.022924209
top acc: 0.0090 ::: bot acc: 0.0545
top acc: 0.1211 ::: bot acc: 0.0776
top acc: 0.0361 ::: bot acc: 0.0365
top acc: 0.0150 ::: bot acc: 0.0405
top acc: 0.0167 ::: bot acc: 0.0550
top acc: 0.0223 ::: bot acc: 0.0382
current epoch: 8
train loss is 0.003288
average val loss: 0.001180, accuracy: 0.0364
average test loss: 0.001204, accuracy: 0.0358
case acc: 0.022010338
case acc: 0.0991005
case acc: 0.026734265
case acc: 0.019151576
case acc: 0.025610685
case acc: 0.022134004
top acc: 0.0086 ::: bot acc: 0.0420
top acc: 0.1202 ::: bot acc: 0.0769
top acc: 0.0430 ::: bot acc: 0.0304
top acc: 0.0240 ::: bot acc: 0.0291
top acc: 0.0343 ::: bot acc: 0.0360
top acc: 0.0351 ::: bot acc: 0.0250
current epoch: 9
train loss is 0.002435
average val loss: 0.001318, accuracy: 0.0367
average test loss: 0.001475, accuracy: 0.0404
case acc: 0.018719414
case acc: 0.107925944
case acc: 0.03109407
case acc: 0.023590323
case acc: 0.031321142
case acc: 0.02968549
top acc: 0.0278 ::: bot acc: 0.0223
top acc: 0.1290 ::: bot acc: 0.0858
top acc: 0.0573 ::: bot acc: 0.0164
top acc: 0.0426 ::: bot acc: 0.0122
top acc: 0.0574 ::: bot acc: 0.0143
top acc: 0.0541 ::: bot acc: 0.0114
current epoch: 10
train loss is 0.002286
average val loss: 0.001238, accuracy: 0.0360
average test loss: 0.001412, accuracy: 0.0403
case acc: 0.019376013
case acc: 0.10213747
case acc: 0.031081302
case acc: 0.024343004
case acc: 0.033797674
case acc: 0.031177688
top acc: 0.0313 ::: bot acc: 0.0188
top acc: 0.1232 ::: bot acc: 0.0803
top acc: 0.0572 ::: bot acc: 0.0159
top acc: 0.0442 ::: bot acc: 0.0111
top acc: 0.0625 ::: bot acc: 0.0112
top acc: 0.0566 ::: bot acc: 0.0113
current epoch: 11
train loss is 0.002103
average val loss: 0.001035, accuracy: 0.0334
average test loss: 0.001175, accuracy: 0.0371
case acc: 0.01872475
case acc: 0.090342425
case acc: 0.029077193
case acc: 0.022871526
case acc: 0.03298297
case acc: 0.028684441
top acc: 0.0285 ::: bot acc: 0.0211
top acc: 0.1118 ::: bot acc: 0.0681
top acc: 0.0521 ::: bot acc: 0.0211
top acc: 0.0409 ::: bot acc: 0.0137
top acc: 0.0610 ::: bot acc: 0.0120
top acc: 0.0527 ::: bot acc: 0.0114
current epoch: 12
train loss is 0.001806
average val loss: 0.001019, accuracy: 0.0336
average test loss: 0.001179, accuracy: 0.0379
case acc: 0.019777758
case acc: 0.08663496
case acc: 0.029425632
case acc: 0.024233654
case acc: 0.036166374
case acc: 0.03118176
top acc: 0.0331 ::: bot acc: 0.0164
top acc: 0.1076 ::: bot acc: 0.0646
top acc: 0.0534 ::: bot acc: 0.0190
top acc: 0.0437 ::: bot acc: 0.0112
top acc: 0.0669 ::: bot acc: 0.0098
top acc: 0.0566 ::: bot acc: 0.0111
current epoch: 13
train loss is 0.001622
average val loss: 0.000978, accuracy: 0.0333
average test loss: 0.001137, accuracy: 0.0378
case acc: 0.020713372
case acc: 0.081445016
case acc: 0.029581094
case acc: 0.025150646
case acc: 0.037617743
case acc: 0.03214323
top acc: 0.0361 ::: bot acc: 0.0139
top acc: 0.1027 ::: bot acc: 0.0596
top acc: 0.0539 ::: bot acc: 0.0192
top acc: 0.0459 ::: bot acc: 0.0107
top acc: 0.0694 ::: bot acc: 0.0096
top acc: 0.0578 ::: bot acc: 0.0115
current epoch: 14
train loss is 0.001451
average val loss: 0.000892, accuracy: 0.0321
average test loss: 0.001043, accuracy: 0.0365
case acc: 0.02094118
case acc: 0.07476828
case acc: 0.02886414
case acc: 0.02486599
case acc: 0.03763733
case acc: 0.032033738
top acc: 0.0367 ::: bot acc: 0.0131
top acc: 0.0961 ::: bot acc: 0.0528
top acc: 0.0523 ::: bot acc: 0.0196
top acc: 0.0450 ::: bot acc: 0.0109
top acc: 0.0691 ::: bot acc: 0.0097
top acc: 0.0577 ::: bot acc: 0.0114
current epoch: 15
train loss is 0.001300
average val loss: 0.000827, accuracy: 0.0311
average test loss: 0.000976, accuracy: 0.0355
case acc: 0.021427639
case acc: 0.06950987
case acc: 0.028596392
case acc: 0.024923284
case acc: 0.037175383
case acc: 0.03166388
top acc: 0.0377 ::: bot acc: 0.0129
top acc: 0.0906 ::: bot acc: 0.0475
top acc: 0.0519 ::: bot acc: 0.0205
top acc: 0.0455 ::: bot acc: 0.0103
top acc: 0.0687 ::: bot acc: 0.0096
top acc: 0.0568 ::: bot acc: 0.0117
current epoch: 16
train loss is 0.001201
average val loss: 0.000791, accuracy: 0.0306
average test loss: 0.000944, accuracy: 0.0352
case acc: 0.021782048
case acc: 0.06540403
case acc: 0.02892672
case acc: 0.025653092
case acc: 0.03726662
case acc: 0.032189827
top acc: 0.0391 ::: bot acc: 0.0110
top acc: 0.0865 ::: bot acc: 0.0433
top acc: 0.0523 ::: bot acc: 0.0203
top acc: 0.0470 ::: bot acc: 0.0102
top acc: 0.0689 ::: bot acc: 0.0096
top acc: 0.0579 ::: bot acc: 0.0116
current epoch: 17
train loss is 0.001107
average val loss: 0.000789, accuracy: 0.0308
average test loss: 0.000947, accuracy: 0.0356
case acc: 0.02332768
case acc: 0.06294201
case acc: 0.029610405
case acc: 0.026807373
case acc: 0.038203232
case acc: 0.03263594
top acc: 0.0418 ::: bot acc: 0.0101
top acc: 0.0837 ::: bot acc: 0.0412
top acc: 0.0545 ::: bot acc: 0.0183
top acc: 0.0487 ::: bot acc: 0.0097
top acc: 0.0697 ::: bot acc: 0.0096
top acc: 0.0586 ::: bot acc: 0.0113
current epoch: 18
train loss is 0.001049
average val loss: 0.000746, accuracy: 0.0300
average test loss: 0.000893, accuracy: 0.0347
case acc: 0.023497118
case acc: 0.058802404
case acc: 0.029748129
case acc: 0.026799513
case acc: 0.03711983
case acc: 0.0320864
top acc: 0.0422 ::: bot acc: 0.0099
top acc: 0.0799 ::: bot acc: 0.0368
top acc: 0.0545 ::: bot acc: 0.0184
top acc: 0.0485 ::: bot acc: 0.0098
top acc: 0.0683 ::: bot acc: 0.0097
top acc: 0.0577 ::: bot acc: 0.0113
current epoch: 19
train loss is 0.000977
average val loss: 0.000741, accuracy: 0.0300
average test loss: 0.000902, accuracy: 0.0350
case acc: 0.02470965
case acc: 0.05722338
case acc: 0.030400803
case acc: 0.02791219
case acc: 0.037291545
case acc: 0.03259802
top acc: 0.0444 ::: bot acc: 0.0089
top acc: 0.0783 ::: bot acc: 0.0352
top acc: 0.0562 ::: bot acc: 0.0169
top acc: 0.0502 ::: bot acc: 0.0097
top acc: 0.0687 ::: bot acc: 0.0093
top acc: 0.0582 ::: bot acc: 0.0117
current epoch: 20
train loss is 0.000943
average val loss: 0.000746, accuracy: 0.0303
average test loss: 0.000910, accuracy: 0.0354
case acc: 0.025818618
case acc: 0.055503406
case acc: 0.031394474
case acc: 0.028888682
case acc: 0.037682537
case acc: 0.033024825
top acc: 0.0463 ::: bot acc: 0.0083
top acc: 0.0765 ::: bot acc: 0.0338
top acc: 0.0584 ::: bot acc: 0.0153
top acc: 0.0517 ::: bot acc: 0.0098
top acc: 0.0691 ::: bot acc: 0.0098
top acc: 0.0588 ::: bot acc: 0.0118
current epoch: 21
train loss is 0.000908
average val loss: 0.000726, accuracy: 0.0299
average test loss: 0.000884, accuracy: 0.0349
case acc: 0.026467312
case acc: 0.052773852
case acc: 0.0316781
case acc: 0.029042223
case acc: 0.037019387
case acc: 0.03234473
top acc: 0.0477 ::: bot acc: 0.0080
top acc: 0.0737 ::: bot acc: 0.0309
top acc: 0.0593 ::: bot acc: 0.0148
top acc: 0.0520 ::: bot acc: 0.0097
top acc: 0.0682 ::: bot acc: 0.0099
top acc: 0.0579 ::: bot acc: 0.0114
current epoch: 22
train loss is 0.000882
average val loss: 0.000703, accuracy: 0.0294
average test loss: 0.000864, accuracy: 0.0345
case acc: 0.026995592
case acc: 0.05065117
case acc: 0.031987958
case acc: 0.029168256
case acc: 0.03641842
case acc: 0.031765185
top acc: 0.0482 ::: bot acc: 0.0082
top acc: 0.0716 ::: bot acc: 0.0289
top acc: 0.0599 ::: bot acc: 0.0142
top acc: 0.0522 ::: bot acc: 0.0097
top acc: 0.0671 ::: bot acc: 0.0099
top acc: 0.0576 ::: bot acc: 0.0110
current epoch: 23
train loss is 0.000845
average val loss: 0.000768, accuracy: 0.0312
average test loss: 0.000951, accuracy: 0.0366
case acc: 0.029970843
case acc: 0.05216616
case acc: 0.034522165
case acc: 0.031560846
case acc: 0.038142838
case acc: 0.03325955
top acc: 0.0520 ::: bot acc: 0.0092
top acc: 0.0731 ::: bot acc: 0.0304
top acc: 0.0643 ::: bot acc: 0.0127
top acc: 0.0554 ::: bot acc: 0.0102
top acc: 0.0697 ::: bot acc: 0.0098
top acc: 0.0594 ::: bot acc: 0.0114
current epoch: 24
train loss is 0.000824
average val loss: 0.000754, accuracy: 0.0309
average test loss: 0.000935, accuracy: 0.0363
case acc: 0.030574776
case acc: 0.050354756
case acc: 0.035234
case acc: 0.03182916
case acc: 0.037415516
case acc: 0.032639716
top acc: 0.0527 ::: bot acc: 0.0094
top acc: 0.0712 ::: bot acc: 0.0289
top acc: 0.0652 ::: bot acc: 0.0128
top acc: 0.0558 ::: bot acc: 0.0102
top acc: 0.0688 ::: bot acc: 0.0095
top acc: 0.0585 ::: bot acc: 0.0115
current epoch: 25
train loss is 0.000819
average val loss: 0.000798, accuracy: 0.0321
average test loss: 0.000996, accuracy: 0.0377
case acc: 0.032808278
case acc: 0.050878823
case acc: 0.037131384
case acc: 0.033592533
case acc: 0.03847691
case acc: 0.03360598
top acc: 0.0558 ::: bot acc: 0.0108
top acc: 0.0719 ::: bot acc: 0.0289
top acc: 0.0682 ::: bot acc: 0.0127
top acc: 0.0580 ::: bot acc: 0.0110
top acc: 0.0704 ::: bot acc: 0.0097
top acc: 0.0599 ::: bot acc: 0.0119
current epoch: 26
train loss is 0.000822
average val loss: 0.000835, accuracy: 0.0331
average test loss: 0.001039, accuracy: 0.0388
case acc: 0.03438936
case acc: 0.051094335
case acc: 0.03883509
case acc: 0.034977913
case acc: 0.03926374
case acc: 0.034223393
top acc: 0.0574 ::: bot acc: 0.0117
top acc: 0.0720 ::: bot acc: 0.0293
top acc: 0.0706 ::: bot acc: 0.0126
top acc: 0.0597 ::: bot acc: 0.0119
top acc: 0.0712 ::: bot acc: 0.0103
top acc: 0.0608 ::: bot acc: 0.0118
current epoch: 27
train loss is 0.000810
average val loss: 0.000929, accuracy: 0.0355
average test loss: 0.001160, accuracy: 0.0413
case acc: 0.03766463
case acc: 0.05343231
case acc: 0.04203311
case acc: 0.037763897
case acc: 0.041397605
case acc: 0.03576293
top acc: 0.0613 ::: bot acc: 0.0137
top acc: 0.0742 ::: bot acc: 0.0316
top acc: 0.0752 ::: bot acc: 0.0131
top acc: 0.0632 ::: bot acc: 0.0131
top acc: 0.0742 ::: bot acc: 0.0107
top acc: 0.0629 ::: bot acc: 0.0123
current epoch: 28
train loss is 0.000834
average val loss: 0.000891, accuracy: 0.0346
average test loss: 0.001116, accuracy: 0.0404
case acc: 0.03718949
case acc: 0.051519733
case acc: 0.042153247
case acc: 0.036966205
case acc: 0.040224943
case acc: 0.034601904
top acc: 0.0606 ::: bot acc: 0.0136
top acc: 0.0725 ::: bot acc: 0.0297
top acc: 0.0755 ::: bot acc: 0.0132
top acc: 0.0623 ::: bot acc: 0.0127
top acc: 0.0725 ::: bot acc: 0.0100
top acc: 0.0614 ::: bot acc: 0.0121
current epoch: 29
train loss is 0.000827
average val loss: 0.000995, accuracy: 0.0371
average test loss: 0.001241, accuracy: 0.0431
case acc: 0.04049069
case acc: 0.05372283
case acc: 0.04558907
case acc: 0.04031093
case acc: 0.04228029
case acc: 0.036420766
top acc: 0.0644 ::: bot acc: 0.0161
top acc: 0.0747 ::: bot acc: 0.0318
top acc: 0.0796 ::: bot acc: 0.0149
top acc: 0.0659 ::: bot acc: 0.0154
top acc: 0.0753 ::: bot acc: 0.0108
top acc: 0.0636 ::: bot acc: 0.0127
current epoch: 30
train loss is 0.000859
average val loss: 0.001063, accuracy: 0.0388
average test loss: 0.001330, accuracy: 0.0449
case acc: 0.042665254
case acc: 0.055083457
case acc: 0.048139755
case acc: 0.042270172
case acc: 0.04388647
case acc: 0.03753644
top acc: 0.0669 ::: bot acc: 0.0180
top acc: 0.0761 ::: bot acc: 0.0334
top acc: 0.0828 ::: bot acc: 0.0163
top acc: 0.0684 ::: bot acc: 0.0166
top acc: 0.0774 ::: bot acc: 0.0116
top acc: 0.0651 ::: bot acc: 0.0130
current epoch: 31
train loss is 0.000855
average val loss: 0.001132, accuracy: 0.0403
average test loss: 0.001415, accuracy: 0.0466
case acc: 0.044528827
case acc: 0.056194007
case acc: 0.050827656
case acc: 0.044260446
case acc: 0.045461625
case acc: 0.038609624
top acc: 0.0686 ::: bot acc: 0.0197
top acc: 0.0770 ::: bot acc: 0.0343
top acc: 0.0858 ::: bot acc: 0.0182
top acc: 0.0705 ::: bot acc: 0.0183
top acc: 0.0793 ::: bot acc: 0.0125
top acc: 0.0664 ::: bot acc: 0.0138
current epoch: 32
train loss is 0.000852
average val loss: 0.001193, accuracy: 0.0417
average test loss: 0.001486, accuracy: 0.0480
case acc: 0.04625377
case acc: 0.056813538
case acc: 0.052923802
case acc: 0.04599853
case acc: 0.047009785
case acc: 0.039274655
top acc: 0.0706 ::: bot acc: 0.0211
top acc: 0.0776 ::: bot acc: 0.0349
top acc: 0.0882 ::: bot acc: 0.0201
top acc: 0.0724 ::: bot acc: 0.0197
top acc: 0.0809 ::: bot acc: 0.0135
top acc: 0.0672 ::: bot acc: 0.0144
current epoch: 33
train loss is 0.000876
average val loss: 0.001263, accuracy: 0.0432
average test loss: 0.001571, accuracy: 0.0497
case acc: 0.04794867
case acc: 0.057786852
case acc: 0.055149168
case acc: 0.047818787
case acc: 0.04913305
case acc: 0.04046071
top acc: 0.0723 ::: bot acc: 0.0227
top acc: 0.0786 ::: bot acc: 0.0361
top acc: 0.0903 ::: bot acc: 0.0219
top acc: 0.0742 ::: bot acc: 0.0213
top acc: 0.0834 ::: bot acc: 0.0152
top acc: 0.0684 ::: bot acc: 0.0151
current epoch: 34
train loss is 0.000883
average val loss: 0.001225, accuracy: 0.0424
average test loss: 0.001526, accuracy: 0.0488
case acc: 0.046988353
case acc: 0.055750355
case acc: 0.054849423
case acc: 0.04715103
case acc: 0.048838098
case acc: 0.039497778
top acc: 0.0712 ::: bot acc: 0.0221
top acc: 0.0767 ::: bot acc: 0.0337
top acc: 0.0901 ::: bot acc: 0.0217
top acc: 0.0737 ::: bot acc: 0.0206
top acc: 0.0831 ::: bot acc: 0.0151
top acc: 0.0673 ::: bot acc: 0.0144
current epoch: 35
train loss is 0.000832
average val loss: 0.001188, accuracy: 0.0416
average test loss: 0.001484, accuracy: 0.0480
case acc: 0.046033222
case acc: 0.053889755
case acc: 0.054408055
case acc: 0.046324994
case acc: 0.04879778
case acc: 0.038586196
top acc: 0.0703 ::: bot acc: 0.0210
top acc: 0.0749 ::: bot acc: 0.0321
top acc: 0.0895 ::: bot acc: 0.0213
top acc: 0.0726 ::: bot acc: 0.0201
top acc: 0.0830 ::: bot acc: 0.0147
top acc: 0.0663 ::: bot acc: 0.0139
current epoch: 36
train loss is 0.000822
average val loss: 0.001105, accuracy: 0.0397
average test loss: 0.001393, accuracy: 0.0462
case acc: 0.043732733
case acc: 0.050754696
case acc: 0.05267656
case acc: 0.04505451
case acc: 0.04782482
case acc: 0.037100647
top acc: 0.0680 ::: bot acc: 0.0189
top acc: 0.0718 ::: bot acc: 0.0290
top acc: 0.0880 ::: bot acc: 0.0197
top acc: 0.0711 ::: bot acc: 0.0190
top acc: 0.0819 ::: bot acc: 0.0141
top acc: 0.0644 ::: bot acc: 0.0132
current epoch: 37
train loss is 0.000796
average val loss: 0.001050, accuracy: 0.0385
average test loss: 0.001325, accuracy: 0.0448
case acc: 0.042017087
case acc: 0.047955498
case acc: 0.051147025
case acc: 0.044065524
case acc: 0.04725813
case acc: 0.036283806
top acc: 0.0662 ::: bot acc: 0.0175
top acc: 0.0689 ::: bot acc: 0.0263
top acc: 0.0863 ::: bot acc: 0.0185
top acc: 0.0703 ::: bot acc: 0.0182
top acc: 0.0813 ::: bot acc: 0.0138
top acc: 0.0634 ::: bot acc: 0.0126
current epoch: 38
train loss is 0.000740
average val loss: 0.000922, accuracy: 0.0353
average test loss: 0.001169, accuracy: 0.0415
case acc: 0.038335368
case acc: 0.043106712
case acc: 0.047795597
case acc: 0.04096455
case acc: 0.044954192
case acc: 0.03400678
top acc: 0.0621 ::: bot acc: 0.0144
top acc: 0.0637 ::: bot acc: 0.0220
top acc: 0.0823 ::: bot acc: 0.0162
top acc: 0.0669 ::: bot acc: 0.0157
top acc: 0.0786 ::: bot acc: 0.0124
top acc: 0.0604 ::: bot acc: 0.0118
current epoch: 39
train loss is 0.000689
average val loss: 0.000820, accuracy: 0.0328
average test loss: 0.001050, accuracy: 0.0389
case acc: 0.03514406
case acc: 0.038850207
case acc: 0.045040946
case acc: 0.03883917
case acc: 0.043222252
case acc: 0.032225147
top acc: 0.0584 ::: bot acc: 0.0121
top acc: 0.0595 ::: bot acc: 0.0181
top acc: 0.0791 ::: bot acc: 0.0146
top acc: 0.0644 ::: bot acc: 0.0141
top acc: 0.0764 ::: bot acc: 0.0114
top acc: 0.0580 ::: bot acc: 0.0115
current epoch: 40
train loss is 0.000639
average val loss: 0.000677, accuracy: 0.0289
average test loss: 0.000863, accuracy: 0.0345
case acc: 0.03033624
case acc: 0.0329716
case acc: 0.0404286
case acc: 0.034681007
case acc: 0.039763607
case acc: 0.029109798
top acc: 0.0526 ::: bot acc: 0.0095
top acc: 0.0526 ::: bot acc: 0.0139
top acc: 0.0732 ::: bot acc: 0.0127
top acc: 0.0593 ::: bot acc: 0.0117
top acc: 0.0718 ::: bot acc: 0.0104
top acc: 0.0535 ::: bot acc: 0.0112
current epoch: 41
train loss is 0.000590
average val loss: 0.000601, accuracy: 0.0267
average test loss: 0.000763, accuracy: 0.0320
case acc: 0.027452802
case acc: 0.02900597
case acc: 0.037836935
case acc: 0.03245171
case acc: 0.037824865
case acc: 0.02767949
top acc: 0.0486 ::: bot acc: 0.0086
top acc: 0.0483 ::: bot acc: 0.0113
top acc: 0.0693 ::: bot acc: 0.0127
top acc: 0.0567 ::: bot acc: 0.0105
top acc: 0.0694 ::: bot acc: 0.0098
top acc: 0.0510 ::: bot acc: 0.0119
current epoch: 42
train loss is 0.000560
average val loss: 0.000497, accuracy: 0.0236
average test loss: 0.000617, accuracy: 0.0283
case acc: 0.023490848
case acc: 0.023568267
case acc: 0.033549253
case acc: 0.028762404
case acc: 0.034684725
case acc: 0.025468493
top acc: 0.0423 ::: bot acc: 0.0096
top acc: 0.0409 ::: bot acc: 0.0090
top acc: 0.0626 ::: bot acc: 0.0133
top acc: 0.0519 ::: bot acc: 0.0096
top acc: 0.0641 ::: bot acc: 0.0105
top acc: 0.0466 ::: bot acc: 0.0141
current epoch: 43
train loss is 0.000527
average val loss: 0.000433, accuracy: 0.0217
average test loss: 0.000512, accuracy: 0.0254
case acc: 0.020854615
case acc: 0.019424172
case acc: 0.030414388
case acc: 0.025787262
case acc: 0.032243952
case acc: 0.023701876
top acc: 0.0364 ::: bot acc: 0.0133
top acc: 0.0346 ::: bot acc: 0.0098
top acc: 0.0564 ::: bot acc: 0.0166
top acc: 0.0468 ::: bot acc: 0.0102
top acc: 0.0596 ::: bot acc: 0.0126
top acc: 0.0426 ::: bot acc: 0.0169
current epoch: 44
train loss is 0.000505
average val loss: 0.000395, accuracy: 0.0208
average test loss: 0.000432, accuracy: 0.0232
case acc: 0.019038642
case acc: 0.016529687
case acc: 0.028086163
case acc: 0.023110855
case acc: 0.029950062
case acc: 0.02249498
top acc: 0.0303 ::: bot acc: 0.0196
top acc: 0.0274 ::: bot acc: 0.0155
top acc: 0.0496 ::: bot acc: 0.0232
top acc: 0.0417 ::: bot acc: 0.0130
top acc: 0.0545 ::: bot acc: 0.0162
top acc: 0.0381 ::: bot acc: 0.0218
current epoch: 45
train loss is 0.000499
average val loss: 0.000391, accuracy: 0.0211
average test loss: 0.000385, accuracy: 0.0218
case acc: 0.018240657
case acc: 0.01547418
case acc: 0.026416723
case acc: 0.021119932
case acc: 0.02802917
case acc: 0.021708908
top acc: 0.0244 ::: bot acc: 0.0256
top acc: 0.0205 ::: bot acc: 0.0221
top acc: 0.0430 ::: bot acc: 0.0296
top acc: 0.0364 ::: bot acc: 0.0174
top acc: 0.0492 ::: bot acc: 0.0206
top acc: 0.0340 ::: bot acc: 0.0256
current epoch: 46
train loss is 0.000518
average val loss: 0.000409, accuracy: 0.0219
average test loss: 0.000375, accuracy: 0.0215
case acc: 0.01843376
case acc: 0.01604404
case acc: 0.02601693
case acc: 0.02020239
case acc: 0.027131142
case acc: 0.021459656
top acc: 0.0202 ::: bot acc: 0.0296
top acc: 0.0157 ::: bot acc: 0.0273
top acc: 0.0379 ::: bot acc: 0.0347
top acc: 0.0332 ::: bot acc: 0.0207
top acc: 0.0465 ::: bot acc: 0.0234
top acc: 0.0319 ::: bot acc: 0.0276
current epoch: 47
train loss is 0.000530
average val loss: 0.000455, accuracy: 0.0236
average test loss: 0.000384, accuracy: 0.0219
case acc: 0.019532211
case acc: 0.01791533
case acc: 0.026597153
case acc: 0.019437233
case acc: 0.026385626
case acc: 0.021365002
top acc: 0.0147 ::: bot acc: 0.0353
top acc: 0.0094 ::: bot acc: 0.0336
top acc: 0.0312 ::: bot acc: 0.0416
top acc: 0.0285 ::: bot acc: 0.0256
top acc: 0.0425 ::: bot acc: 0.0276
top acc: 0.0287 ::: bot acc: 0.0310
current epoch: 48
train loss is 0.000571
average val loss: 0.000553, accuracy: 0.0268
average test loss: 0.000433, accuracy: 0.0234
case acc: 0.022006657
case acc: 0.022295304
case acc: 0.028938971
case acc: 0.019658174
case acc: 0.02583891
case acc: 0.021951256
top acc: 0.0091 ::: bot acc: 0.0418
top acc: 0.0066 ::: bot acc: 0.0416
top acc: 0.0227 ::: bot acc: 0.0499
top acc: 0.0224 ::: bot acc: 0.0318
top acc: 0.0377 ::: bot acc: 0.0326
top acc: 0.0241 ::: bot acc: 0.0355
current epoch: 49
train loss is 0.000635
average val loss: 0.000654, accuracy: 0.0298
average test loss: 0.000499, accuracy: 0.0255
case acc: 0.024823619
case acc: 0.027142938
case acc: 0.032066017
case acc: 0.020660264
case acc: 0.02554397
case acc: 0.022673074
top acc: 0.0072 ::: bot acc: 0.0468
top acc: 0.0083 ::: bot acc: 0.0478
top acc: 0.0181 ::: bot acc: 0.0569
top acc: 0.0182 ::: bot acc: 0.0359
top acc: 0.0345 ::: bot acc: 0.0355
top acc: 0.0220 ::: bot acc: 0.0377
current epoch: 50
train loss is 0.000700
average val loss: 0.000919, accuracy: 0.0366
average test loss: 0.000691, accuracy: 0.0308
case acc: 0.03193405
case acc: 0.037656575
case acc: 0.039351135
case acc: 0.024818124
case acc: 0.02618602
case acc: 0.024941662
top acc: 0.0095 ::: bot acc: 0.0564
top acc: 0.0171 ::: bot acc: 0.0591
top acc: 0.0159 ::: bot acc: 0.0687
top acc: 0.0129 ::: bot acc: 0.0448
top acc: 0.0265 ::: bot acc: 0.0433
top acc: 0.0163 ::: bot acc: 0.0442
LME_Co_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6786 6786 6786
1.7082474 -0.6288155 0.12137239 -0.15229516
Validation: 756 756 756
Testing: 768 768 768
pre-processing time: 0.00020647048950195312
the split date is 2011-07-01
train dropout: 0.6 test dropout: 0.1
net initializing with time: 0.002241373062133789
preparing training and testing date with time: 4.76837158203125e-07
current epoch: 1
train loss is 0.012768
average val loss: 0.004989, accuracy: 0.0928
average test loss: 0.004996, accuracy: 0.0935
case acc: 0.13184033
case acc: 0.08965903
case acc: 0.09015292
case acc: 0.08062825
case acc: 0.117009796
case acc: 0.05158837
top acc: 0.1056 ::: bot acc: 0.1559
top acc: 0.1107 ::: bot acc: 0.0692
top acc: 0.0571 ::: bot acc: 0.1225
top acc: 0.0624 ::: bot acc: 0.1000
top acc: 0.0908 ::: bot acc: 0.1463
top acc: 0.0255 ::: bot acc: 0.0788
current epoch: 2
train loss is 0.007765
average val loss: 0.003084, accuracy: 0.0544
average test loss: 0.003028, accuracy: 0.0537
case acc: 0.0373917
case acc: 0.17073385
case acc: 0.025585277
case acc: 0.01734529
case acc: 0.034293175
case acc: 0.03663415
top acc: 0.0188 ::: bot acc: 0.0567
top acc: 0.1911 ::: bot acc: 0.1504
top acc: 0.0424 ::: bot acc: 0.0252
top acc: 0.0279 ::: bot acc: 0.0130
top acc: 0.0121 ::: bot acc: 0.0610
top acc: 0.0637 ::: bot acc: 0.0145
current epoch: 3
train loss is 0.008000
average val loss: 0.007947, accuracy: 0.1051
average test loss: 0.007900, accuracy: 0.1051
case acc: 0.0540607
case acc: 0.24303985
case acc: 0.09476314
case acc: 0.08826129
case acc: 0.047512308
case acc: 0.10291429
top acc: 0.0800 ::: bot acc: 0.0299
top acc: 0.2638 ::: bot acc: 0.2233
top acc: 0.1287 ::: bot acc: 0.0617
top acc: 0.1065 ::: bot acc: 0.0687
top acc: 0.0735 ::: bot acc: 0.0209
top acc: 0.1348 ::: bot acc: 0.0729
current epoch: 4
train loss is 0.009914
average val loss: 0.016709, accuracy: 0.1709
average test loss: 0.016617, accuracy: 0.1705
case acc: 0.12717439
case acc: 0.3019812
case acc: 0.16655387
case acc: 0.15281168
case acc: 0.114226595
case acc: 0.1602987
top acc: 0.1536 ::: bot acc: 0.1036
top acc: 0.3224 ::: bot acc: 0.2820
top acc: 0.2016 ::: bot acc: 0.1328
top acc: 0.1711 ::: bot acc: 0.1329
top acc: 0.1422 ::: bot acc: 0.0842
top acc: 0.1923 ::: bot acc: 0.1287
current epoch: 5
train loss is 0.012574
average val loss: 0.015739, accuracy: 0.1666
average test loss: 0.015638, accuracy: 0.1663
case acc: 0.12842253
case acc: 0.28831398
case acc: 0.16551414
case acc: 0.14752038
case acc: 0.11723578
case acc: 0.15079239
top acc: 0.1544 ::: bot acc: 0.1053
top acc: 0.3085 ::: bot acc: 0.2689
top acc: 0.2001 ::: bot acc: 0.1319
top acc: 0.1658 ::: bot acc: 0.1278
top acc: 0.1451 ::: bot acc: 0.0876
top acc: 0.1833 ::: bot acc: 0.1188
current epoch: 6
train loss is 0.013033
average val loss: 0.005209, accuracy: 0.0849
average test loss: 0.005139, accuracy: 0.0848
case acc: 0.050827548
case acc: 0.1935592
case acc: 0.08252833
case acc: 0.06526349
case acc: 0.04756654
case acc: 0.06880385
top acc: 0.0765 ::: bot acc: 0.0275
top acc: 0.2145 ::: bot acc: 0.1734
top acc: 0.1176 ::: bot acc: 0.0485
top acc: 0.0834 ::: bot acc: 0.0457
top acc: 0.0742 ::: bot acc: 0.0205
top acc: 0.1006 ::: bot acc: 0.0382
current epoch: 7
train loss is 0.008108
average val loss: 0.001452, accuracy: 0.0384
average test loss: 0.001394, accuracy: 0.0366
case acc: 0.023639478
case acc: 0.11111376
case acc: 0.025787473
case acc: 0.013819824
case acc: 0.0217001
case acc: 0.023575302
top acc: 0.0156 ::: bot acc: 0.0378
top acc: 0.1320 ::: bot acc: 0.0916
top acc: 0.0436 ::: bot acc: 0.0245
top acc: 0.0151 ::: bot acc: 0.0227
top acc: 0.0178 ::: bot acc: 0.0394
top acc: 0.0344 ::: bot acc: 0.0295
current epoch: 8
train loss is 0.003684
average val loss: 0.001151, accuracy: 0.0357
average test loss: 0.001098, accuracy: 0.0341
case acc: 0.026090186
case acc: 0.09324166
case acc: 0.025169525
case acc: 0.015258414
case acc: 0.021418817
case acc: 0.02358155
top acc: 0.0145 ::: bot acc: 0.0421
top acc: 0.1135 ::: bot acc: 0.0738
top acc: 0.0339 ::: bot acc: 0.0351
top acc: 0.0103 ::: bot acc: 0.0284
top acc: 0.0209 ::: bot acc: 0.0368
top acc: 0.0315 ::: bot acc: 0.0329
current epoch: 9
train loss is 0.002366
average val loss: 0.001379, accuracy: 0.0388
average test loss: 0.001322, accuracy: 0.0369
case acc: 0.01742865
case acc: 0.10389478
case acc: 0.02749529
case acc: 0.01747717
case acc: 0.026537366
case acc: 0.028640343
top acc: 0.0293 ::: bot acc: 0.0194
top acc: 0.1243 ::: bot acc: 0.0848
top acc: 0.0501 ::: bot acc: 0.0187
top acc: 0.0291 ::: bot acc: 0.0107
top acc: 0.0460 ::: bot acc: 0.0138
top acc: 0.0528 ::: bot acc: 0.0133
current epoch: 10
train loss is 0.002265
average val loss: 0.001344, accuracy: 0.0391
average test loss: 0.001293, accuracy: 0.0375
case acc: 0.01784299
case acc: 0.09919162
case acc: 0.02794012
case acc: 0.019402515
case acc: 0.02994902
case acc: 0.030867815
top acc: 0.0337 ::: bot acc: 0.0155
top acc: 0.1193 ::: bot acc: 0.0800
top acc: 0.0513 ::: bot acc: 0.0174
top acc: 0.0325 ::: bot acc: 0.0098
top acc: 0.0524 ::: bot acc: 0.0119
top acc: 0.0566 ::: bot acc: 0.0126
current epoch: 11
train loss is 0.002069
average val loss: 0.001161, accuracy: 0.0368
average test loss: 0.001109, accuracy: 0.0351
case acc: 0.017645277
case acc: 0.08914723
case acc: 0.02680705
case acc: 0.018019587
case acc: 0.02960278
case acc: 0.0295439
top acc: 0.0328 ::: bot acc: 0.0161
top acc: 0.1098 ::: bot acc: 0.0696
top acc: 0.0476 ::: bot acc: 0.0213
top acc: 0.0306 ::: bot acc: 0.0100
top acc: 0.0513 ::: bot acc: 0.0120
top acc: 0.0543 ::: bot acc: 0.0127
current epoch: 12
train loss is 0.001796
average val loss: 0.001080, accuracy: 0.0360
average test loss: 0.001023, accuracy: 0.0343
case acc: 0.018111862
case acc: 0.08239414
case acc: 0.026365202
case acc: 0.018331962
case acc: 0.031147085
case acc: 0.029703822
top acc: 0.0342 ::: bot acc: 0.0152
top acc: 0.1028 ::: bot acc: 0.0631
top acc: 0.0461 ::: bot acc: 0.0226
top acc: 0.0310 ::: bot acc: 0.0102
top acc: 0.0540 ::: bot acc: 0.0116
top acc: 0.0547 ::: bot acc: 0.0125
current epoch: 13
train loss is 0.001653
average val loss: 0.001002, accuracy: 0.0351
average test loss: 0.000943, accuracy: 0.0335
case acc: 0.018440306
case acc: 0.07587676
case acc: 0.026176142
case acc: 0.018499218
case acc: 0.03198331
case acc: 0.029874811
top acc: 0.0355 ::: bot acc: 0.0135
top acc: 0.0962 ::: bot acc: 0.0565
top acc: 0.0449 ::: bot acc: 0.0236
top acc: 0.0316 ::: bot acc: 0.0099
top acc: 0.0549 ::: bot acc: 0.0121
top acc: 0.0550 ::: bot acc: 0.0125
current epoch: 14
train loss is 0.001399
average val loss: 0.001015, accuracy: 0.0357
average test loss: 0.000960, accuracy: 0.0345
case acc: 0.019859197
case acc: 0.0727555
case acc: 0.026820201
case acc: 0.020690525
case acc: 0.034676053
case acc: 0.032240964
top acc: 0.0399 ::: bot acc: 0.0096
top acc: 0.0931 ::: bot acc: 0.0533
top acc: 0.0475 ::: bot acc: 0.0213
top acc: 0.0347 ::: bot acc: 0.0101
top acc: 0.0589 ::: bot acc: 0.0122
top acc: 0.0584 ::: bot acc: 0.0126
current epoch: 15
train loss is 0.001331
average val loss: 0.000928, accuracy: 0.0344
average test loss: 0.000880, accuracy: 0.0332
case acc: 0.0202381
case acc: 0.066972286
case acc: 0.026443776
case acc: 0.020194974
case acc: 0.034003023
case acc: 0.031431764
top acc: 0.0401 ::: bot acc: 0.0098
top acc: 0.0870 ::: bot acc: 0.0477
top acc: 0.0465 ::: bot acc: 0.0225
top acc: 0.0342 ::: bot acc: 0.0092
top acc: 0.0580 ::: bot acc: 0.0119
top acc: 0.0571 ::: bot acc: 0.0127
current epoch: 16
train loss is 0.001188
average val loss: 0.000890, accuracy: 0.0339
average test loss: 0.000845, accuracy: 0.0328
case acc: 0.020847637
case acc: 0.062847875
case acc: 0.026561413
case acc: 0.020904716
case acc: 0.034004357
case acc: 0.031588353
top acc: 0.0417 ::: bot acc: 0.0089
top acc: 0.0831 ::: bot acc: 0.0435
top acc: 0.0468 ::: bot acc: 0.0220
top acc: 0.0349 ::: bot acc: 0.0095
top acc: 0.0582 ::: bot acc: 0.0120
top acc: 0.0577 ::: bot acc: 0.0124
current epoch: 17
train loss is 0.001118
average val loss: 0.000843, accuracy: 0.0332
average test loss: 0.000798, accuracy: 0.0320
case acc: 0.021224469
case acc: 0.05841895
case acc: 0.026661322
case acc: 0.0212057
case acc: 0.03374283
case acc: 0.0310453
top acc: 0.0426 ::: bot acc: 0.0087
top acc: 0.0789 ::: bot acc: 0.0390
top acc: 0.0466 ::: bot acc: 0.0226
top acc: 0.0354 ::: bot acc: 0.0096
top acc: 0.0575 ::: bot acc: 0.0122
top acc: 0.0569 ::: bot acc: 0.0122
current epoch: 18
train loss is 0.001054
average val loss: 0.000777, accuracy: 0.0319
average test loss: 0.000725, accuracy: 0.0305
case acc: 0.02105293
case acc: 0.053091776
case acc: 0.026312232
case acc: 0.020275172
case acc: 0.03216586
case acc: 0.030055948
top acc: 0.0421 ::: bot acc: 0.0088
top acc: 0.0736 ::: bot acc: 0.0335
top acc: 0.0455 ::: bot acc: 0.0231
top acc: 0.0347 ::: bot acc: 0.0089
top acc: 0.0553 ::: bot acc: 0.0121
top acc: 0.0554 ::: bot acc: 0.0119
current epoch: 19
train loss is 0.000961
average val loss: 0.000791, accuracy: 0.0324
average test loss: 0.000748, accuracy: 0.0313
case acc: 0.02276837
case acc: 0.05195373
case acc: 0.026889108
case acc: 0.022419648
case acc: 0.032581646
case acc: 0.031367045
top acc: 0.0450 ::: bot acc: 0.0083
top acc: 0.0724 ::: bot acc: 0.0325
top acc: 0.0480 ::: bot acc: 0.0205
top acc: 0.0369 ::: bot acc: 0.0097
top acc: 0.0562 ::: bot acc: 0.0119
top acc: 0.0575 ::: bot acc: 0.0123
current epoch: 20
train loss is 0.000930
average val loss: 0.000830, accuracy: 0.0334
average test loss: 0.000784, accuracy: 0.0325
case acc: 0.024864022
case acc: 0.051474217
case acc: 0.028054856
case acc: 0.02442738
case acc: 0.033425927
case acc: 0.032517046
top acc: 0.0480 ::: bot acc: 0.0083
top acc: 0.0719 ::: bot acc: 0.0322
top acc: 0.0514 ::: bot acc: 0.0177
top acc: 0.0400 ::: bot acc: 0.0104
top acc: 0.0573 ::: bot acc: 0.0117
top acc: 0.0588 ::: bot acc: 0.0125
current epoch: 21
train loss is 0.000892
average val loss: 0.000814, accuracy: 0.0331
average test loss: 0.000769, accuracy: 0.0323
case acc: 0.025886595
case acc: 0.04919435
case acc: 0.02845835
case acc: 0.025023859
case acc: 0.032893397
case acc: 0.032193456
top acc: 0.0493 ::: bot acc: 0.0083
top acc: 0.0694 ::: bot acc: 0.0298
top acc: 0.0521 ::: bot acc: 0.0168
top acc: 0.0405 ::: bot acc: 0.0107
top acc: 0.0567 ::: bot acc: 0.0116
top acc: 0.0586 ::: bot acc: 0.0124
current epoch: 22
train loss is 0.000870
average val loss: 0.000830, accuracy: 0.0336
average test loss: 0.000787, accuracy: 0.0329
case acc: 0.027468191
case acc: 0.04851494
case acc: 0.029746713
case acc: 0.026171159
case acc: 0.033106357
case acc: 0.03235042
top acc: 0.0513 ::: bot acc: 0.0092
top acc: 0.0689 ::: bot acc: 0.0289
top acc: 0.0546 ::: bot acc: 0.0156
top acc: 0.0421 ::: bot acc: 0.0113
top acc: 0.0566 ::: bot acc: 0.0119
top acc: 0.0586 ::: bot acc: 0.0123
current epoch: 23
train loss is 0.000841
average val loss: 0.000908, accuracy: 0.0355
average test loss: 0.000860, accuracy: 0.0348
case acc: 0.030394198
case acc: 0.049785636
case acc: 0.03191777
case acc: 0.0285701
case acc: 0.03457401
case acc: 0.033529922
top acc: 0.0552 ::: bot acc: 0.0104
top acc: 0.0702 ::: bot acc: 0.0304
top acc: 0.0589 ::: bot acc: 0.0138
top acc: 0.0446 ::: bot acc: 0.0129
top acc: 0.0589 ::: bot acc: 0.0122
top acc: 0.0601 ::: bot acc: 0.0127
current epoch: 24
train loss is 0.000846
average val loss: 0.000941, accuracy: 0.0363
average test loss: 0.000895, accuracy: 0.0357
case acc: 0.032080043
case acc: 0.04976377
case acc: 0.03339241
case acc: 0.030125357
case acc: 0.03498028
case acc: 0.034002963
top acc: 0.0574 ::: bot acc: 0.0113
top acc: 0.0700 ::: bot acc: 0.0305
top acc: 0.0616 ::: bot acc: 0.0130
top acc: 0.0464 ::: bot acc: 0.0141
top acc: 0.0594 ::: bot acc: 0.0123
top acc: 0.0609 ::: bot acc: 0.0129
current epoch: 25
train loss is 0.000834
average val loss: 0.001045, accuracy: 0.0387
average test loss: 0.001000, accuracy: 0.0382
case acc: 0.035611846
case acc: 0.05178728
case acc: 0.03600026
case acc: 0.033171434
case acc: 0.03715668
case acc: 0.035680577
top acc: 0.0609 ::: bot acc: 0.0140
top acc: 0.0721 ::: bot acc: 0.0326
top acc: 0.0662 ::: bot acc: 0.0115
top acc: 0.0498 ::: bot acc: 0.0161
top acc: 0.0623 ::: bot acc: 0.0130
top acc: 0.0632 ::: bot acc: 0.0137
current epoch: 26
train loss is 0.000850
average val loss: 0.001064, accuracy: 0.0391
average test loss: 0.001014, accuracy: 0.0386
case acc: 0.036678717
case acc: 0.051269025
case acc: 0.03715818
case acc: 0.03403134
case acc: 0.037084263
case acc: 0.035605263
top acc: 0.0624 ::: bot acc: 0.0148
top acc: 0.0715 ::: bot acc: 0.0320
top acc: 0.0678 ::: bot acc: 0.0115
top acc: 0.0508 ::: bot acc: 0.0171
top acc: 0.0620 ::: bot acc: 0.0131
top acc: 0.0627 ::: bot acc: 0.0136
current epoch: 27
train loss is 0.000842
average val loss: 0.001143, accuracy: 0.0408
average test loss: 0.001094, accuracy: 0.0404
case acc: 0.03925904
case acc: 0.052248936
case acc: 0.039222866
case acc: 0.03630065
case acc: 0.039041523
case acc: 0.03662137
top acc: 0.0649 ::: bot acc: 0.0172
top acc: 0.0726 ::: bot acc: 0.0330
top acc: 0.0708 ::: bot acc: 0.0114
top acc: 0.0531 ::: bot acc: 0.0191
top acc: 0.0645 ::: bot acc: 0.0142
top acc: 0.0645 ::: bot acc: 0.0138
current epoch: 28
train loss is 0.000865
average val loss: 0.001216, accuracy: 0.0424
average test loss: 0.001171, accuracy: 0.0422
case acc: 0.04161918
case acc: 0.05307348
case acc: 0.04149708
case acc: 0.03829798
case acc: 0.040732697
case acc: 0.037851688
top acc: 0.0677 ::: bot acc: 0.0192
top acc: 0.0734 ::: bot acc: 0.0336
top acc: 0.0739 ::: bot acc: 0.0118
top acc: 0.0558 ::: bot acc: 0.0206
top acc: 0.0664 ::: bot acc: 0.0152
top acc: 0.0657 ::: bot acc: 0.0147
current epoch: 29
train loss is 0.000839
average val loss: 0.001186, accuracy: 0.0418
average test loss: 0.001140, accuracy: 0.0415
case acc: 0.04121975
case acc: 0.051406767
case acc: 0.04161078
case acc: 0.03784791
case acc: 0.04014226
case acc: 0.036749475
top acc: 0.0672 ::: bot acc: 0.0188
top acc: 0.0716 ::: bot acc: 0.0323
top acc: 0.0744 ::: bot acc: 0.0117
top acc: 0.0549 ::: bot acc: 0.0202
top acc: 0.0657 ::: bot acc: 0.0149
top acc: 0.0644 ::: bot acc: 0.0142
current epoch: 30
train loss is 0.000836
average val loss: 0.001234, accuracy: 0.0428
average test loss: 0.001187, accuracy: 0.0425
case acc: 0.04268545
case acc: 0.05172088
case acc: 0.043070205
case acc: 0.039078787
case acc: 0.041339457
case acc: 0.037049197
top acc: 0.0689 ::: bot acc: 0.0201
top acc: 0.0722 ::: bot acc: 0.0323
top acc: 0.0764 ::: bot acc: 0.0120
top acc: 0.0562 ::: bot acc: 0.0213
top acc: 0.0674 ::: bot acc: 0.0153
top acc: 0.0648 ::: bot acc: 0.0145
current epoch: 31
train loss is 0.000839
average val loss: 0.001248, accuracy: 0.0432
average test loss: 0.001207, accuracy: 0.0430
case acc: 0.043092087
case acc: 0.05132155
case acc: 0.044081494
case acc: 0.03996759
case acc: 0.041871537
case acc: 0.037389547
top acc: 0.0691 ::: bot acc: 0.0206
top acc: 0.0716 ::: bot acc: 0.0320
top acc: 0.0774 ::: bot acc: 0.0125
top acc: 0.0572 ::: bot acc: 0.0220
top acc: 0.0677 ::: bot acc: 0.0162
top acc: 0.0653 ::: bot acc: 0.0147
current epoch: 32
train loss is 0.000809
average val loss: 0.001296, accuracy: 0.0441
average test loss: 0.001248, accuracy: 0.0438
case acc: 0.044020977
case acc: 0.05146321
case acc: 0.04543789
case acc: 0.041321598
case acc: 0.04285231
case acc: 0.03777176
top acc: 0.0700 ::: bot acc: 0.0215
top acc: 0.0719 ::: bot acc: 0.0321
top acc: 0.0794 ::: bot acc: 0.0133
top acc: 0.0585 ::: bot acc: 0.0233
top acc: 0.0691 ::: bot acc: 0.0166
top acc: 0.0656 ::: bot acc: 0.0148
current epoch: 33
train loss is 0.000798
average val loss: 0.001328, accuracy: 0.0448
average test loss: 0.001277, accuracy: 0.0445
case acc: 0.044583242
case acc: 0.05138883
case acc: 0.046413034
case acc: 0.0423854
case acc: 0.043835618
case acc: 0.038251255
top acc: 0.0705 ::: bot acc: 0.0220
top acc: 0.0717 ::: bot acc: 0.0320
top acc: 0.0802 ::: bot acc: 0.0141
top acc: 0.0598 ::: bot acc: 0.0243
top acc: 0.0700 ::: bot acc: 0.0174
top acc: 0.0661 ::: bot acc: 0.0152
current epoch: 34
train loss is 0.000803
average val loss: 0.001267, accuracy: 0.0435
average test loss: 0.001220, accuracy: 0.0432
case acc: 0.043213636
case acc: 0.048926193
case acc: 0.045575622
case acc: 0.041384723
case acc: 0.0431046
case acc: 0.037221983
top acc: 0.0692 ::: bot acc: 0.0207
top acc: 0.0692 ::: bot acc: 0.0297
top acc: 0.0793 ::: bot acc: 0.0135
top acc: 0.0587 ::: bot acc: 0.0234
top acc: 0.0693 ::: bot acc: 0.0168
top acc: 0.0652 ::: bot acc: 0.0143
current epoch: 35
train loss is 0.000799
average val loss: 0.001309, accuracy: 0.0444
average test loss: 0.001259, accuracy: 0.0441
case acc: 0.043933876
case acc: 0.048697226
case acc: 0.046649903
case acc: 0.042510323
case acc: 0.044508114
case acc: 0.03818428
top acc: 0.0700 ::: bot acc: 0.0213
top acc: 0.0689 ::: bot acc: 0.0294
top acc: 0.0807 ::: bot acc: 0.0143
top acc: 0.0599 ::: bot acc: 0.0243
top acc: 0.0708 ::: bot acc: 0.0179
top acc: 0.0662 ::: bot acc: 0.0149
current epoch: 36
train loss is 0.000746
average val loss: 0.001192, accuracy: 0.0419
average test loss: 0.001140, accuracy: 0.0415
case acc: 0.04098796
case acc: 0.044788964
case acc: 0.044404946
case acc: 0.040387698
case acc: 0.042515125
case acc: 0.035915934
top acc: 0.0670 ::: bot acc: 0.0188
top acc: 0.0650 ::: bot acc: 0.0255
top acc: 0.0780 ::: bot acc: 0.0127
top acc: 0.0578 ::: bot acc: 0.0223
top acc: 0.0685 ::: bot acc: 0.0165
top acc: 0.0634 ::: bot acc: 0.0136
current epoch: 37
train loss is 0.000712
average val loss: 0.001106, accuracy: 0.0401
average test loss: 0.001053, accuracy: 0.0395
case acc: 0.03849358
case acc: 0.04158591
case acc: 0.042634584
case acc: 0.038531397
case acc: 0.04110416
case acc: 0.034535818
top acc: 0.0644 ::: bot acc: 0.0164
top acc: 0.0617 ::: bot acc: 0.0221
top acc: 0.0758 ::: bot acc: 0.0119
top acc: 0.0557 ::: bot acc: 0.0208
top acc: 0.0671 ::: bot acc: 0.0152
top acc: 0.0618 ::: bot acc: 0.0130
current epoch: 38
train loss is 0.000674
average val loss: 0.000998, accuracy: 0.0376
average test loss: 0.000948, accuracy: 0.0370
case acc: 0.03568606
case acc: 0.03762295
case acc: 0.040461466
case acc: 0.036364526
case acc: 0.039140645
case acc: 0.032710988
top acc: 0.0614 ::: bot acc: 0.0140
top acc: 0.0578 ::: bot acc: 0.0184
top acc: 0.0728 ::: bot acc: 0.0115
top acc: 0.0534 ::: bot acc: 0.0188
top acc: 0.0647 ::: bot acc: 0.0143
top acc: 0.0591 ::: bot acc: 0.0123
current epoch: 39
train loss is 0.000640
average val loss: 0.000898, accuracy: 0.0353
average test loss: 0.000848, accuracy: 0.0345
case acc: 0.03271001
case acc: 0.033771407
case acc: 0.03858349
case acc: 0.03399753
case acc: 0.037228793
case acc: 0.030986447
top acc: 0.0578 ::: bot acc: 0.0120
top acc: 0.0539 ::: bot acc: 0.0148
top acc: 0.0698 ::: bot acc: 0.0116
top acc: 0.0507 ::: bot acc: 0.0170
top acc: 0.0624 ::: bot acc: 0.0132
top acc: 0.0567 ::: bot acc: 0.0122
current epoch: 40
train loss is 0.000598
average val loss: 0.000813, accuracy: 0.0333
average test loss: 0.000769, accuracy: 0.0325
case acc: 0.030226473
case acc: 0.030551616
case acc: 0.036664795
case acc: 0.032198988
case acc: 0.03546093
case acc: 0.02960252
top acc: 0.0553 ::: bot acc: 0.0103
top acc: 0.0505 ::: bot acc: 0.0118
top acc: 0.0671 ::: bot acc: 0.0116
top acc: 0.0487 ::: bot acc: 0.0157
top acc: 0.0599 ::: bot acc: 0.0124
top acc: 0.0548 ::: bot acc: 0.0121
current epoch: 41
train loss is 0.000585
average val loss: 0.000778, accuracy: 0.0324
average test loss: 0.000733, accuracy: 0.0315
case acc: 0.028985448
case acc: 0.028530642
case acc: 0.03584286
case acc: 0.031452574
case acc: 0.03487016
case acc: 0.029424135
top acc: 0.0535 ::: bot acc: 0.0097
top acc: 0.0482 ::: bot acc: 0.0104
top acc: 0.0657 ::: bot acc: 0.0120
top acc: 0.0481 ::: bot acc: 0.0150
top acc: 0.0593 ::: bot acc: 0.0122
top acc: 0.0542 ::: bot acc: 0.0124
current epoch: 42
train loss is 0.000556
average val loss: 0.000686, accuracy: 0.0300
average test loss: 0.000641, accuracy: 0.0289
case acc: 0.02610064
case acc: 0.024772631
case acc: 0.033621714
case acc: 0.028624471
case acc: 0.032884497
case acc: 0.0276523
top acc: 0.0497 ::: bot acc: 0.0086
top acc: 0.0440 ::: bot acc: 0.0077
top acc: 0.0618 ::: bot acc: 0.0129
top acc: 0.0448 ::: bot acc: 0.0129
top acc: 0.0565 ::: bot acc: 0.0117
top acc: 0.0514 ::: bot acc: 0.0133
current epoch: 43
train loss is 0.000522
average val loss: 0.000624, accuracy: 0.0284
average test loss: 0.000580, accuracy: 0.0272
case acc: 0.024172524
case acc: 0.022109402
case acc: 0.03195976
case acc: 0.026781369
case acc: 0.03130409
case acc: 0.026666157
top acc: 0.0469 ::: bot acc: 0.0084
top acc: 0.0405 ::: bot acc: 0.0065
top acc: 0.0591 ::: bot acc: 0.0138
top acc: 0.0426 ::: bot acc: 0.0116
top acc: 0.0542 ::: bot acc: 0.0115
top acc: 0.0493 ::: bot acc: 0.0145
current epoch: 44
train loss is 0.000509
average val loss: 0.000554, accuracy: 0.0266
average test loss: 0.000512, accuracy: 0.0251
case acc: 0.0215811
case acc: 0.019266263
case acc: 0.03007882
case acc: 0.024408892
case acc: 0.029433481
case acc: 0.025549244
top acc: 0.0430 ::: bot acc: 0.0085
top acc: 0.0365 ::: bot acc: 0.0063
top acc: 0.0553 ::: bot acc: 0.0155
top acc: 0.0398 ::: bot acc: 0.0104
top acc: 0.0514 ::: bot acc: 0.0119
top acc: 0.0467 ::: bot acc: 0.0162
current epoch: 45
train loss is 0.000504
average val loss: 0.000512, accuracy: 0.0254
average test loss: 0.000469, accuracy: 0.0237
case acc: 0.020017335
case acc: 0.017518353
case acc: 0.028585482
case acc: 0.0227364
case acc: 0.02829758
case acc: 0.025128383
top acc: 0.0403 ::: bot acc: 0.0092
top acc: 0.0330 ::: bot acc: 0.0080
top acc: 0.0522 ::: bot acc: 0.0173
top acc: 0.0376 ::: bot acc: 0.0096
top acc: 0.0496 ::: bot acc: 0.0121
top acc: 0.0452 ::: bot acc: 0.0176
current epoch: 46
train loss is 0.000480
average val loss: 0.000452, accuracy: 0.0239
average test loss: 0.000411, accuracy: 0.0218
case acc: 0.018258987
case acc: 0.015374277
case acc: 0.026739916
case acc: 0.020117909
case acc: 0.026370663
case acc: 0.024101537
top acc: 0.0361 ::: bot acc: 0.0126
top acc: 0.0280 ::: bot acc: 0.0114
top acc: 0.0472 ::: bot acc: 0.0215
top acc: 0.0339 ::: bot acc: 0.0093
top acc: 0.0463 ::: bot acc: 0.0132
top acc: 0.0419 ::: bot acc: 0.0206
current epoch: 47
train loss is 0.000480
average val loss: 0.000412, accuracy: 0.0227
average test loss: 0.000368, accuracy: 0.0205
case acc: 0.017289072
case acc: 0.014341925
case acc: 0.02562688
case acc: 0.017869173
case acc: 0.024395928
case acc: 0.023548871
top acc: 0.0313 ::: bot acc: 0.0171
top acc: 0.0233 ::: bot acc: 0.0164
top acc: 0.0426 ::: bot acc: 0.0263
top acc: 0.0303 ::: bot acc: 0.0098
top acc: 0.0423 ::: bot acc: 0.0151
top acc: 0.0391 ::: bot acc: 0.0236
current epoch: 48
train loss is 0.000484
average val loss: 0.000383, accuracy: 0.0219
average test loss: 0.000343, accuracy: 0.0198
case acc: 0.017578736
case acc: 0.014456101
case acc: 0.02495557
case acc: 0.015810713
case acc: 0.02305758
case acc: 0.023152942
top acc: 0.0265 ::: bot acc: 0.0219
top acc: 0.0176 ::: bot acc: 0.0219
top acc: 0.0366 ::: bot acc: 0.0320
top acc: 0.0261 ::: bot acc: 0.0122
top acc: 0.0384 ::: bot acc: 0.0189
top acc: 0.0356 ::: bot acc: 0.0270
current epoch: 49
train loss is 0.000502
average val loss: 0.000382, accuracy: 0.0219
average test loss: 0.000342, accuracy: 0.0199
case acc: 0.017985836
case acc: 0.015180756
case acc: 0.02492707
case acc: 0.015060931
case acc: 0.022967283
case acc: 0.023133326
top acc: 0.0245 ::: bot acc: 0.0241
top acc: 0.0149 ::: bot acc: 0.0246
top acc: 0.0335 ::: bot acc: 0.0351
top acc: 0.0244 ::: bot acc: 0.0132
top acc: 0.0381 ::: bot acc: 0.0193
top acc: 0.0354 ::: bot acc: 0.0272
current epoch: 50
train loss is 0.000509
average val loss: 0.000384, accuracy: 0.0220
average test loss: 0.000347, accuracy: 0.0202
case acc: 0.01899435
case acc: 0.0174331
case acc: 0.025714733
case acc: 0.014120904
case acc: 0.022177964
case acc: 0.023041058
top acc: 0.0204 ::: bot acc: 0.0280
top acc: 0.0118 ::: bot acc: 0.0296
top acc: 0.0283 ::: bot acc: 0.0406
top acc: 0.0211 ::: bot acc: 0.0165
top acc: 0.0359 ::: bot acc: 0.0214
top acc: 0.0334 ::: bot acc: 0.0290

		{"drop_out": 0.6, "drop_out_mc": 0.15, "repeat_mc": 50, "hidden": 30, "embedding_size": 5, "batch": 512, "lag": 4}
LME_Co_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6780 6780 6780
1.8562728 -0.6288155 0.15869391 -0.16256663
Validation: 756 756 756
Testing: 774 774 774
pre-processing time: 0.00030684471130371094
the split date is 2009-07-01
train dropout: 0.6 test dropout: 0.15
net initializing with time: 0.004323005676269531
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012803
average val loss: 0.005624, accuracy: 0.1002
average test loss: 0.005948, accuracy: 0.1037
case acc: 0.14647111
case acc: 0.077940874
case acc: 0.104231715
case acc: 0.09788073
case acc: 0.13115612
case acc: 0.0642213
top acc: 0.1340 ::: bot acc: 0.1589
top acc: 0.0974 ::: bot acc: 0.0589
top acc: 0.0759 ::: bot acc: 0.1327
top acc: 0.0803 ::: bot acc: 0.1168
top acc: 0.1122 ::: bot acc: 0.1511
top acc: 0.0480 ::: bot acc: 0.0804
current epoch: 2
train loss is 0.008285
average val loss: 0.002816, accuracy: 0.0519
average test loss: 0.002738, accuracy: 0.0514
case acc: 0.04503166
case acc: 0.16271976
case acc: 0.020543177
case acc: 0.014695807
case acc: 0.043198835
case acc: 0.022116045
top acc: 0.0317 ::: bot acc: 0.0590
top acc: 0.1824 ::: bot acc: 0.1436
top acc: 0.0246 ::: bot acc: 0.0312
top acc: 0.0152 ::: bot acc: 0.0238
top acc: 0.0252 ::: bot acc: 0.0627
top acc: 0.0351 ::: bot acc: 0.0109
current epoch: 3
train loss is 0.008670
average val loss: 0.007291, accuracy: 0.0994
average test loss: 0.006891, accuracy: 0.0953
case acc: 0.04413562
case acc: 0.23710458
case acc: 0.08509315
case acc: 0.07641135
case acc: 0.03639644
case acc: 0.0927381
top acc: 0.0575 ::: bot acc: 0.0301
top acc: 0.2560 ::: bot acc: 0.2181
top acc: 0.1131 ::: bot acc: 0.0587
top acc: 0.0954 ::: bot acc: 0.0567
top acc: 0.0541 ::: bot acc: 0.0182
top acc: 0.1089 ::: bot acc: 0.0762
current epoch: 4
train loss is 0.010076
average val loss: 0.016532, accuracy: 0.1701
average test loss: 0.015813, accuracy: 0.1660
case acc: 0.12237518
case acc: 0.30149764
case acc: 0.1622053
case acc: 0.1459987
case acc: 0.10866037
case acc: 0.15497251
top acc: 0.1364 ::: bot acc: 0.1078
top acc: 0.3207 ::: bot acc: 0.2824
top acc: 0.1907 ::: bot acc: 0.1349
top acc: 0.1646 ::: bot acc: 0.1270
top acc: 0.1278 ::: bot acc: 0.0883
top acc: 0.1711 ::: bot acc: 0.1380
current epoch: 5
train loss is 0.013129
average val loss: 0.013695, accuracy: 0.1542
average test loss: 0.013041, accuracy: 0.1500
case acc: 0.1119002
case acc: 0.27587023
case acc: 0.14937648
case acc: 0.1295612
case acc: 0.09984868
case acc: 0.13364112
top acc: 0.1253 ::: bot acc: 0.0981
top acc: 0.2956 ::: bot acc: 0.2565
top acc: 0.1773 ::: bot acc: 0.1225
top acc: 0.1484 ::: bot acc: 0.1098
top acc: 0.1189 ::: bot acc: 0.0793
top acc: 0.1499 ::: bot acc: 0.1163
current epoch: 6
train loss is 0.013176
average val loss: 0.003517, accuracy: 0.0639
average test loss: 0.003236, accuracy: 0.0597
case acc: 0.025125422
case acc: 0.17151168
case acc: 0.056788385
case acc: 0.038293615
case acc: 0.023816096
case acc: 0.042558484
top acc: 0.0383 ::: bot acc: 0.0116
top acc: 0.1912 ::: bot acc: 0.1521
top acc: 0.0846 ::: bot acc: 0.0299
top acc: 0.0561 ::: bot acc: 0.0202
top acc: 0.0398 ::: bot acc: 0.0101
top acc: 0.0581 ::: bot acc: 0.0261
current epoch: 7
train loss is 0.007534
average val loss: 0.001203, accuracy: 0.0367
average test loss: 0.001187, accuracy: 0.0375
case acc: 0.033789128
case acc: 0.09614181
case acc: 0.021644345
case acc: 0.025844714
case acc: 0.029127358
case acc: 0.018419178
top acc: 0.0197 ::: bot acc: 0.0481
top acc: 0.1158 ::: bot acc: 0.0766
top acc: 0.0179 ::: bot acc: 0.0365
top acc: 0.0122 ::: bot acc: 0.0430
top acc: 0.0111 ::: bot acc: 0.0491
top acc: 0.0064 ::: bot acc: 0.0332
current epoch: 8
train loss is 0.003560
average val loss: 0.001032, accuracy: 0.0312
average test loss: 0.000975, accuracy: 0.0311
case acc: 0.023938501
case acc: 0.09276967
case acc: 0.020554205
case acc: 0.019131444
case acc: 0.017373217
case acc: 0.012559586
top acc: 0.0110 ::: bot acc: 0.0374
top acc: 0.1121 ::: bot acc: 0.0732
top acc: 0.0220 ::: bot acc: 0.0325
top acc: 0.0098 ::: bot acc: 0.0339
top acc: 0.0092 ::: bot acc: 0.0326
top acc: 0.0106 ::: bot acc: 0.0219
current epoch: 9
train loss is 0.002605
average val loss: 0.001108, accuracy: 0.0312
average test loss: 0.001007, accuracy: 0.0296
case acc: 0.011733261
case acc: 0.09852303
case acc: 0.020295646
case acc: 0.013925293
case acc: 0.01704225
case acc: 0.01596453
top acc: 0.0081 ::: bot acc: 0.0212
top acc: 0.1175 ::: bot acc: 0.0794
top acc: 0.0338 ::: bot acc: 0.0209
top acc: 0.0185 ::: bot acc: 0.0196
top acc: 0.0275 ::: bot acc: 0.0137
top acc: 0.0266 ::: bot acc: 0.0091
current epoch: 10
train loss is 0.002470
average val loss: 0.001051, accuracy: 0.0314
average test loss: 0.000952, accuracy: 0.0298
case acc: 0.010740098
case acc: 0.09390164
case acc: 0.02065137
case acc: 0.014516489
case acc: 0.020432131
case acc: 0.01830214
top acc: 0.0117 ::: bot acc: 0.0169
top acc: 0.1131 ::: bot acc: 0.0750
top acc: 0.0346 ::: bot acc: 0.0201
top acc: 0.0220 ::: bot acc: 0.0166
top acc: 0.0344 ::: bot acc: 0.0106
top acc: 0.0301 ::: bot acc: 0.0094
current epoch: 11
train loss is 0.002324
average val loss: 0.000871, accuracy: 0.0289
average test loss: 0.000785, accuracy: 0.0275
case acc: 0.010877561
case acc: 0.08318515
case acc: 0.020130029
case acc: 0.013654683
case acc: 0.020089889
case acc: 0.016895177
top acc: 0.0103 ::: bot acc: 0.0183
top acc: 0.1026 ::: bot acc: 0.0640
top acc: 0.0305 ::: bot acc: 0.0248
top acc: 0.0193 ::: bot acc: 0.0183
top acc: 0.0339 ::: bot acc: 0.0100
top acc: 0.0281 ::: bot acc: 0.0093
current epoch: 12
train loss is 0.002020
average val loss: 0.000807, accuracy: 0.0287
average test loss: 0.000718, accuracy: 0.0270
case acc: 0.010141729
case acc: 0.07725763
case acc: 0.019977171
case acc: 0.014307867
case acc: 0.022334801
case acc: 0.018109864
top acc: 0.0122 ::: bot acc: 0.0150
top acc: 0.0965 ::: bot acc: 0.0585
top acc: 0.0300 ::: bot acc: 0.0249
top acc: 0.0213 ::: bot acc: 0.0176
top acc: 0.0376 ::: bot acc: 0.0098
top acc: 0.0303 ::: bot acc: 0.0094
current epoch: 13
train loss is 0.001804
average val loss: 0.000711, accuracy: 0.0274
average test loss: 0.000628, accuracy: 0.0257
case acc: 0.00979002
case acc: 0.06987124
case acc: 0.01996167
case acc: 0.014071659
case acc: 0.022709673
case acc: 0.017829318
top acc: 0.0127 ::: bot acc: 0.0142
top acc: 0.0891 ::: bot acc: 0.0506
top acc: 0.0281 ::: bot acc: 0.0273
top acc: 0.0204 ::: bot acc: 0.0180
top acc: 0.0380 ::: bot acc: 0.0102
top acc: 0.0295 ::: bot acc: 0.0094
current epoch: 14
train loss is 0.001589
average val loss: 0.000649, accuracy: 0.0267
average test loss: 0.000572, accuracy: 0.0251
case acc: 0.009964769
case acc: 0.06422381
case acc: 0.020014677
case acc: 0.014155158
case acc: 0.023651334
case acc: 0.018409522
top acc: 0.0143 ::: bot acc: 0.0124
top acc: 0.0832 ::: bot acc: 0.0453
top acc: 0.0280 ::: bot acc: 0.0275
top acc: 0.0213 ::: bot acc: 0.0172
top acc: 0.0392 ::: bot acc: 0.0103
top acc: 0.0308 ::: bot acc: 0.0091
current epoch: 15
train loss is 0.001426
average val loss: 0.000594, accuracy: 0.0259
average test loss: 0.000514, accuracy: 0.0241
case acc: 0.010343374
case acc: 0.058708828
case acc: 0.01964462
case acc: 0.014473644
case acc: 0.023608174
case acc: 0.017851356
top acc: 0.0157 ::: bot acc: 0.0119
top acc: 0.0780 ::: bot acc: 0.0397
top acc: 0.0268 ::: bot acc: 0.0280
top acc: 0.0216 ::: bot acc: 0.0171
top acc: 0.0393 ::: bot acc: 0.0103
top acc: 0.0302 ::: bot acc: 0.0091
current epoch: 16
train loss is 0.001344
average val loss: 0.000603, accuracy: 0.0268
average test loss: 0.000520, accuracy: 0.0248
case acc: 0.011251479
case acc: 0.05705026
case acc: 0.020100348
case acc: 0.01526984
case acc: 0.02548698
case acc: 0.019882083
top acc: 0.0192 ::: bot acc: 0.0080
top acc: 0.0764 ::: bot acc: 0.0377
top acc: 0.0301 ::: bot acc: 0.0257
top acc: 0.0248 ::: bot acc: 0.0143
top acc: 0.0417 ::: bot acc: 0.0111
top acc: 0.0323 ::: bot acc: 0.0097
current epoch: 17
train loss is 0.001283
average val loss: 0.000589, accuracy: 0.0269
average test loss: 0.000502, accuracy: 0.0248
case acc: 0.012441945
case acc: 0.054233767
case acc: 0.020182088
case acc: 0.015424381
case acc: 0.026137033
case acc: 0.020415682
top acc: 0.0214 ::: bot acc: 0.0068
top acc: 0.0736 ::: bot acc: 0.0350
top acc: 0.0312 ::: bot acc: 0.0243
top acc: 0.0258 ::: bot acc: 0.0125
top acc: 0.0424 ::: bot acc: 0.0112
top acc: 0.0332 ::: bot acc: 0.0100
current epoch: 18
train loss is 0.001196
average val loss: 0.000580, accuracy: 0.0271
average test loss: 0.000483, accuracy: 0.0246
case acc: 0.013349202
case acc: 0.05159104
case acc: 0.020166108
case acc: 0.016156776
case acc: 0.025894491
case acc: 0.02050158
top acc: 0.0235 ::: bot acc: 0.0055
top acc: 0.0710 ::: bot acc: 0.0325
top acc: 0.0324 ::: bot acc: 0.0225
top acc: 0.0275 ::: bot acc: 0.0115
top acc: 0.0422 ::: bot acc: 0.0111
top acc: 0.0334 ::: bot acc: 0.0098
current epoch: 19
train loss is 0.001162
average val loss: 0.000559, accuracy: 0.0268
average test loss: 0.000465, accuracy: 0.0243
case acc: 0.014225034
case acc: 0.049046457
case acc: 0.020327426
case acc: 0.016341202
case acc: 0.025661374
case acc: 0.020232588
top acc: 0.0249 ::: bot acc: 0.0053
top acc: 0.0683 ::: bot acc: 0.0296
top acc: 0.0336 ::: bot acc: 0.0214
top acc: 0.0277 ::: bot acc: 0.0110
top acc: 0.0422 ::: bot acc: 0.0107
top acc: 0.0331 ::: bot acc: 0.0098
current epoch: 20
train loss is 0.001106
average val loss: 0.000550, accuracy: 0.0268
average test loss: 0.000452, accuracy: 0.0241
case acc: 0.015009113
case acc: 0.046816934
case acc: 0.020569159
case acc: 0.01664799
case acc: 0.025342522
case acc: 0.020331416
top acc: 0.0265 ::: bot acc: 0.0051
top acc: 0.0666 ::: bot acc: 0.0275
top acc: 0.0352 ::: bot acc: 0.0196
top acc: 0.0287 ::: bot acc: 0.0104
top acc: 0.0416 ::: bot acc: 0.0108
top acc: 0.0331 ::: bot acc: 0.0100
current epoch: 21
train loss is 0.001092
average val loss: 0.000574, accuracy: 0.0278
average test loss: 0.000465, accuracy: 0.0249
case acc: 0.016906394
case acc: 0.046287484
case acc: 0.02157826
case acc: 0.01790434
case acc: 0.02576328
case acc: 0.020721259
top acc: 0.0286 ::: bot acc: 0.0058
top acc: 0.0658 ::: bot acc: 0.0269
top acc: 0.0380 ::: bot acc: 0.0175
top acc: 0.0308 ::: bot acc: 0.0095
top acc: 0.0420 ::: bot acc: 0.0111
top acc: 0.0336 ::: bot acc: 0.0097
current epoch: 22
train loss is 0.001064
average val loss: 0.000627, accuracy: 0.0297
average test loss: 0.000509, accuracy: 0.0264
case acc: 0.019919218
case acc: 0.04700909
case acc: 0.023062795
case acc: 0.019395888
case acc: 0.02717121
case acc: 0.022098793
top acc: 0.0325 ::: bot acc: 0.0072
top acc: 0.0665 ::: bot acc: 0.0278
top acc: 0.0419 ::: bot acc: 0.0148
top acc: 0.0338 ::: bot acc: 0.0080
top acc: 0.0439 ::: bot acc: 0.0114
top acc: 0.0356 ::: bot acc: 0.0105
current epoch: 23
train loss is 0.001073
average val loss: 0.000680, accuracy: 0.0313
average test loss: 0.000545, accuracy: 0.0278
case acc: 0.022507345
case acc: 0.047179293
case acc: 0.024526311
case acc: 0.0208663
case acc: 0.028424334
case acc: 0.023258675
top acc: 0.0351 ::: bot acc: 0.0096
top acc: 0.0664 ::: bot acc: 0.0279
top acc: 0.0452 ::: bot acc: 0.0128
top acc: 0.0363 ::: bot acc: 0.0079
top acc: 0.0456 ::: bot acc: 0.0121
top acc: 0.0368 ::: bot acc: 0.0109
current epoch: 24
train loss is 0.001022
average val loss: 0.000622, accuracy: 0.0297
average test loss: 0.000497, accuracy: 0.0264
case acc: 0.021586576
case acc: 0.04403354
case acc: 0.024731118
case acc: 0.019980688
case acc: 0.026626335
case acc: 0.021390649
top acc: 0.0344 ::: bot acc: 0.0087
top acc: 0.0637 ::: bot acc: 0.0247
top acc: 0.0446 ::: bot acc: 0.0139
top acc: 0.0348 ::: bot acc: 0.0076
top acc: 0.0433 ::: bot acc: 0.0113
top acc: 0.0343 ::: bot acc: 0.0103
current epoch: 25
train loss is 0.000996
average val loss: 0.000668, accuracy: 0.0311
average test loss: 0.000530, accuracy: 0.0275
case acc: 0.023805082
case acc: 0.0442961
case acc: 0.02606928
case acc: 0.021381097
case acc: 0.027585642
case acc: 0.022099376
top acc: 0.0369 ::: bot acc: 0.0102
top acc: 0.0637 ::: bot acc: 0.0249
top acc: 0.0476 ::: bot acc: 0.0124
top acc: 0.0368 ::: bot acc: 0.0085
top acc: 0.0444 ::: bot acc: 0.0117
top acc: 0.0355 ::: bot acc: 0.0104
current epoch: 26
train loss is 0.000978
average val loss: 0.000653, accuracy: 0.0307
average test loss: 0.000521, accuracy: 0.0273
case acc: 0.02409622
case acc: 0.043136634
case acc: 0.026531443
case acc: 0.021477778
case acc: 0.027058763
case acc: 0.021481615
top acc: 0.0370 ::: bot acc: 0.0109
top acc: 0.0628 ::: bot acc: 0.0238
top acc: 0.0485 ::: bot acc: 0.0119
top acc: 0.0370 ::: bot acc: 0.0082
top acc: 0.0437 ::: bot acc: 0.0117
top acc: 0.0346 ::: bot acc: 0.0103
current epoch: 27
train loss is 0.000981
average val loss: 0.000695, accuracy: 0.0320
average test loss: 0.000551, accuracy: 0.0283
case acc: 0.025839854
case acc: 0.043364767
case acc: 0.028060947
case acc: 0.022846488
case acc: 0.027856395
case acc: 0.02199779
top acc: 0.0389 ::: bot acc: 0.0124
top acc: 0.0631 ::: bot acc: 0.0239
top acc: 0.0510 ::: bot acc: 0.0115
top acc: 0.0386 ::: bot acc: 0.0087
top acc: 0.0443 ::: bot acc: 0.0120
top acc: 0.0355 ::: bot acc: 0.0103
current epoch: 28
train loss is 0.000983
average val loss: 0.000767, accuracy: 0.0340
average test loss: 0.000613, accuracy: 0.0303
case acc: 0.028636519
case acc: 0.044748187
case acc: 0.030273473
case acc: 0.024814188
case acc: 0.029556163
case acc: 0.023616375
top acc: 0.0416 ::: bot acc: 0.0148
top acc: 0.0643 ::: bot acc: 0.0254
top acc: 0.0543 ::: bot acc: 0.0116
top acc: 0.0412 ::: bot acc: 0.0097
top acc: 0.0468 ::: bot acc: 0.0127
top acc: 0.0375 ::: bot acc: 0.0111
current epoch: 29
train loss is 0.000979
average val loss: 0.000837, accuracy: 0.0358
average test loss: 0.000671, accuracy: 0.0321
case acc: 0.031198435
case acc: 0.045892987
case acc: 0.032345846
case acc: 0.026883433
case acc: 0.031441443
case acc: 0.024759278
top acc: 0.0443 ::: bot acc: 0.0176
top acc: 0.0656 ::: bot acc: 0.0264
top acc: 0.0568 ::: bot acc: 0.0120
top acc: 0.0434 ::: bot acc: 0.0112
top acc: 0.0490 ::: bot acc: 0.0139
top acc: 0.0386 ::: bot acc: 0.0118
current epoch: 30
train loss is 0.000995
average val loss: 0.000856, accuracy: 0.0364
average test loss: 0.000688, accuracy: 0.0326
case acc: 0.031840067
case acc: 0.045428462
case acc: 0.033336926
case acc: 0.027584601
case acc: 0.032386098
case acc: 0.024922796
top acc: 0.0449 ::: bot acc: 0.0182
top acc: 0.0651 ::: bot acc: 0.0259
top acc: 0.0584 ::: bot acc: 0.0123
top acc: 0.0445 ::: bot acc: 0.0115
top acc: 0.0501 ::: bot acc: 0.0148
top acc: 0.0389 ::: bot acc: 0.0120
current epoch: 31
train loss is 0.000976
average val loss: 0.000879, accuracy: 0.0369
average test loss: 0.000713, accuracy: 0.0333
case acc: 0.03281163
case acc: 0.045238577
case acc: 0.034733575
case acc: 0.028489752
case acc: 0.033256836
case acc: 0.025232742
top acc: 0.0464 ::: bot acc: 0.0190
top acc: 0.0650 ::: bot acc: 0.0258
top acc: 0.0602 ::: bot acc: 0.0130
top acc: 0.0456 ::: bot acc: 0.0123
top acc: 0.0511 ::: bot acc: 0.0153
top acc: 0.0392 ::: bot acc: 0.0122
current epoch: 32
train loss is 0.000981
average val loss: 0.000913, accuracy: 0.0379
average test loss: 0.000737, accuracy: 0.0340
case acc: 0.033652123
case acc: 0.045192666
case acc: 0.035981607
case acc: 0.029445188
case acc: 0.034258004
case acc: 0.025558077
top acc: 0.0471 ::: bot acc: 0.0196
top acc: 0.0646 ::: bot acc: 0.0259
top acc: 0.0617 ::: bot acc: 0.0137
top acc: 0.0462 ::: bot acc: 0.0133
top acc: 0.0522 ::: bot acc: 0.0162
top acc: 0.0397 ::: bot acc: 0.0125
current epoch: 33
train loss is 0.000970
average val loss: 0.000931, accuracy: 0.0383
average test loss: 0.000751, accuracy: 0.0344
case acc: 0.033999443
case acc: 0.044660512
case acc: 0.036783297
case acc: 0.030201409
case acc: 0.034722216
case acc: 0.025984589
top acc: 0.0472 ::: bot acc: 0.0202
top acc: 0.0643 ::: bot acc: 0.0252
top acc: 0.0628 ::: bot acc: 0.0139
top acc: 0.0476 ::: bot acc: 0.0135
top acc: 0.0526 ::: bot acc: 0.0163
top acc: 0.0403 ::: bot acc: 0.0123
current epoch: 34
train loss is 0.000960
average val loss: 0.000891, accuracy: 0.0372
average test loss: 0.000715, accuracy: 0.0335
case acc: 0.033036824
case acc: 0.04285651
case acc: 0.036116526
case acc: 0.029643577
case acc: 0.034337893
case acc: 0.024946451
top acc: 0.0463 ::: bot acc: 0.0194
top acc: 0.0621 ::: bot acc: 0.0238
top acc: 0.0620 ::: bot acc: 0.0137
top acc: 0.0468 ::: bot acc: 0.0132
top acc: 0.0522 ::: bot acc: 0.0164
top acc: 0.0388 ::: bot acc: 0.0120
current epoch: 35
train loss is 0.000938
average val loss: 0.000854, accuracy: 0.0363
average test loss: 0.000681, accuracy: 0.0325
case acc: 0.031768024
case acc: 0.040720303
case acc: 0.03585907
case acc: 0.02892178
case acc: 0.033739094
case acc: 0.02401092
top acc: 0.0448 ::: bot acc: 0.0182
top acc: 0.0604 ::: bot acc: 0.0214
top acc: 0.0615 ::: bot acc: 0.0136
top acc: 0.0461 ::: bot acc: 0.0124
top acc: 0.0515 ::: bot acc: 0.0158
top acc: 0.0380 ::: bot acc: 0.0113
current epoch: 36
train loss is 0.000913
average val loss: 0.000860, accuracy: 0.0365
average test loss: 0.000686, accuracy: 0.0327
case acc: 0.031562205
case acc: 0.040088
case acc: 0.03611824
case acc: 0.029621169
case acc: 0.034209706
case acc: 0.02436487
top acc: 0.0446 ::: bot acc: 0.0179
top acc: 0.0598 ::: bot acc: 0.0206
top acc: 0.0618 ::: bot acc: 0.0136
top acc: 0.0466 ::: bot acc: 0.0129
top acc: 0.0522 ::: bot acc: 0.0162
top acc: 0.0381 ::: bot acc: 0.0117
current epoch: 37
train loss is 0.000889
average val loss: 0.000756, accuracy: 0.0338
average test loss: 0.000594, accuracy: 0.0300
case acc: 0.028209008
case acc: 0.036382657
case acc: 0.033839863
case acc: 0.027370814
case acc: 0.031717457
case acc: 0.022530684
top acc: 0.0412 ::: bot acc: 0.0144
top acc: 0.0558 ::: bot acc: 0.0176
top acc: 0.0589 ::: bot acc: 0.0127
top acc: 0.0439 ::: bot acc: 0.0115
top acc: 0.0495 ::: bot acc: 0.0143
top acc: 0.0361 ::: bot acc: 0.0107
current epoch: 38
train loss is 0.000847
average val loss: 0.000673, accuracy: 0.0315
average test loss: 0.000523, accuracy: 0.0278
case acc: 0.025600879
case acc: 0.032922603
case acc: 0.031841263
case acc: 0.025501162
case acc: 0.03015127
case acc: 0.020713337
top acc: 0.0385 ::: bot acc: 0.0122
top acc: 0.0524 ::: bot acc: 0.0141
top acc: 0.0561 ::: bot acc: 0.0120
top acc: 0.0420 ::: bot acc: 0.0102
top acc: 0.0475 ::: bot acc: 0.0130
top acc: 0.0335 ::: bot acc: 0.0098
current epoch: 39
train loss is 0.000819
average val loss: 0.000602, accuracy: 0.0294
average test loss: 0.000459, accuracy: 0.0256
case acc: 0.022807831
case acc: 0.029628586
case acc: 0.029943129
case acc: 0.023718333
case acc: 0.028694952
case acc: 0.018968528
top acc: 0.0357 ::: bot acc: 0.0097
top acc: 0.0488 ::: bot acc: 0.0113
top acc: 0.0534 ::: bot acc: 0.0116
top acc: 0.0398 ::: bot acc: 0.0094
top acc: 0.0459 ::: bot acc: 0.0123
top acc: 0.0312 ::: bot acc: 0.0092
current epoch: 40
train loss is 0.000782
average val loss: 0.000573, accuracy: 0.0285
average test loss: 0.000439, accuracy: 0.0249
case acc: 0.021824732
case acc: 0.028358532
case acc: 0.0293401
case acc: 0.023103448
case acc: 0.028486155
case acc: 0.018541392
top acc: 0.0344 ::: bot acc: 0.0092
top acc: 0.0474 ::: bot acc: 0.0103
top acc: 0.0528 ::: bot acc: 0.0115
top acc: 0.0389 ::: bot acc: 0.0090
top acc: 0.0454 ::: bot acc: 0.0123
top acc: 0.0308 ::: bot acc: 0.0093
current epoch: 41
train loss is 0.000763
average val loss: 0.000504, accuracy: 0.0263
average test loss: 0.000378, accuracy: 0.0227
case acc: 0.01920221
case acc: 0.025056722
case acc: 0.027273543
case acc: 0.021301154
case acc: 0.026365446
case acc: 0.016828397
top acc: 0.0318 ::: bot acc: 0.0069
top acc: 0.0436 ::: bot acc: 0.0078
top acc: 0.0498 ::: bot acc: 0.0113
top acc: 0.0365 ::: bot acc: 0.0084
top acc: 0.0429 ::: bot acc: 0.0111
top acc: 0.0284 ::: bot acc: 0.0088
current epoch: 42
train loss is 0.000733
average val loss: 0.000426, accuracy: 0.0235
average test loss: 0.000316, accuracy: 0.0203
case acc: 0.015978042
case acc: 0.021772292
case acc: 0.025417548
case acc: 0.019223368
case acc: 0.024241189
case acc: 0.015155282
top acc: 0.0274 ::: bot acc: 0.0055
top acc: 0.0394 ::: bot acc: 0.0063
top acc: 0.0465 ::: bot acc: 0.0124
top acc: 0.0334 ::: bot acc: 0.0083
top acc: 0.0400 ::: bot acc: 0.0106
top acc: 0.0254 ::: bot acc: 0.0095
current epoch: 43
train loss is 0.000710
average val loss: 0.000371, accuracy: 0.0215
average test loss: 0.000274, accuracy: 0.0186
case acc: 0.013820807
case acc: 0.019585699
case acc: 0.02366168
case acc: 0.017911421
case acc: 0.02246269
case acc: 0.01397097
top acc: 0.0245 ::: bot acc: 0.0051
top acc: 0.0360 ::: bot acc: 0.0065
top acc: 0.0433 ::: bot acc: 0.0137
top acc: 0.0313 ::: bot acc: 0.0090
top acc: 0.0375 ::: bot acc: 0.0103
top acc: 0.0231 ::: bot acc: 0.0108
current epoch: 44
train loss is 0.000694
average val loss: 0.000352, accuracy: 0.0209
average test loss: 0.000259, accuracy: 0.0180
case acc: 0.012942269
case acc: 0.018303161
case acc: 0.023266638
case acc: 0.01749534
case acc: 0.022182973
case acc: 0.013830623
top acc: 0.0229 ::: bot acc: 0.0055
top acc: 0.0339 ::: bot acc: 0.0069
top acc: 0.0422 ::: bot acc: 0.0148
top acc: 0.0304 ::: bot acc: 0.0090
top acc: 0.0370 ::: bot acc: 0.0102
top acc: 0.0229 ::: bot acc: 0.0106
current epoch: 45
train loss is 0.000686
average val loss: 0.000325, accuracy: 0.0198
average test loss: 0.000240, accuracy: 0.0172
case acc: 0.0120350635
case acc: 0.017152678
case acc: 0.022114957
case acc: 0.016968109
case acc: 0.021348745
case acc: 0.013633346
top acc: 0.0209 ::: bot acc: 0.0067
top acc: 0.0316 ::: bot acc: 0.0080
top acc: 0.0397 ::: bot acc: 0.0161
top acc: 0.0296 ::: bot acc: 0.0098
top acc: 0.0359 ::: bot acc: 0.0102
top acc: 0.0224 ::: bot acc: 0.0113
current epoch: 46
train loss is 0.000672
average val loss: 0.000273, accuracy: 0.0176
average test loss: 0.000205, accuracy: 0.0156
case acc: 0.010390699
case acc: 0.015304351
case acc: 0.020739123
case acc: 0.015364527
case acc: 0.019214995
case acc: 0.01275727
top acc: 0.0169 ::: bot acc: 0.0099
top acc: 0.0265 ::: bot acc: 0.0121
top acc: 0.0352 ::: bot acc: 0.0201
top acc: 0.0259 ::: bot acc: 0.0121
top acc: 0.0325 ::: bot acc: 0.0106
top acc: 0.0194 ::: bot acc: 0.0140
current epoch: 47
train loss is 0.000661
average val loss: 0.000245, accuracy: 0.0164
average test loss: 0.000189, accuracy: 0.0148
case acc: 0.009761832
case acc: 0.014596124
case acc: 0.020054828
case acc: 0.014436739
case acc: 0.018064333
case acc: 0.012096475
top acc: 0.0133 ::: bot acc: 0.0133
top acc: 0.0232 ::: bot acc: 0.0157
top acc: 0.0315 ::: bot acc: 0.0237
top acc: 0.0234 ::: bot acc: 0.0144
top acc: 0.0304 ::: bot acc: 0.0113
top acc: 0.0173 ::: bot acc: 0.0155
current epoch: 48
train loss is 0.000655
average val loss: 0.000227, accuracy: 0.0158
average test loss: 0.000185, accuracy: 0.0146
case acc: 0.010027968
case acc: 0.014315913
case acc: 0.019846695
case acc: 0.013997511
case acc: 0.01737679
case acc: 0.01211593
top acc: 0.0106 ::: bot acc: 0.0161
top acc: 0.0193 ::: bot acc: 0.0196
top acc: 0.0280 ::: bot acc: 0.0275
top acc: 0.0211 ::: bot acc: 0.0170
top acc: 0.0290 ::: bot acc: 0.0124
top acc: 0.0158 ::: bot acc: 0.0176
current epoch: 49
train loss is 0.000659
average val loss: 0.000214, accuracy: 0.0153
average test loss: 0.000188, accuracy: 0.0147
case acc: 0.011057786
case acc: 0.014836131
case acc: 0.020785645
case acc: 0.013419925
case acc: 0.01619736
case acc: 0.012123703
top acc: 0.0068 ::: bot acc: 0.0203
top acc: 0.0142 ::: bot acc: 0.0246
top acc: 0.0231 ::: bot acc: 0.0323
top acc: 0.0172 ::: bot acc: 0.0201
top acc: 0.0260 ::: bot acc: 0.0144
top acc: 0.0135 ::: bot acc: 0.0199
current epoch: 50
train loss is 0.000665
average val loss: 0.000218, accuracy: 0.0155
average test loss: 0.000218, accuracy: 0.0161
case acc: 0.014270914
case acc: 0.017572217
case acc: 0.022761207
case acc: 0.014384214
case acc: 0.014601955
case acc: 0.012937871
top acc: 0.0054 ::: bot acc: 0.0259
top acc: 0.0096 ::: bot acc: 0.0312
top acc: 0.0165 ::: bot acc: 0.0389
top acc: 0.0128 ::: bot acc: 0.0252
top acc: 0.0220 ::: bot acc: 0.0177
top acc: 0.0099 ::: bot acc: 0.0235
LME_Co_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6804 6804 6804
1.8562728 -0.6288155 0.12137239 -0.16228472
Validation: 762 762 762
Testing: 744 744 744
pre-processing time: 0.00019478797912597656
the split date is 2010-01-01
train dropout: 0.6 test dropout: 0.15
net initializing with time: 0.002350330352783203
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012907
average val loss: 0.005387, accuracy: 0.0986
average test loss: 0.005601, accuracy: 0.0994
case acc: 0.13827236
case acc: 0.08513631
case acc: 0.097021446
case acc: 0.094981186
case acc: 0.12546958
case acc: 0.055324357
top acc: 0.1126 ::: bot acc: 0.1603
top acc: 0.1064 ::: bot acc: 0.0648
top acc: 0.0616 ::: bot acc: 0.1331
top acc: 0.0690 ::: bot acc: 0.1215
top acc: 0.1021 ::: bot acc: 0.1483
top acc: 0.0289 ::: bot acc: 0.0797
current epoch: 2
train loss is 0.007748
average val loss: 0.002972, accuracy: 0.0516
average test loss: 0.003093, accuracy: 0.0543
case acc: 0.035696674
case acc: 0.17335704
case acc: 0.027596895
case acc: 0.019460235
case acc: 0.034058128
case acc: 0.035454415
top acc: 0.0170 ::: bot acc: 0.0553
top acc: 0.1938 ::: bot acc: 0.1543
top acc: 0.0427 ::: bot acc: 0.0298
top acc: 0.0281 ::: bot acc: 0.0246
top acc: 0.0131 ::: bot acc: 0.0568
top acc: 0.0624 ::: bot acc: 0.0134
current epoch: 3
train loss is 0.008256
average val loss: 0.007663, accuracy: 0.1033
average test loss: 0.007772, accuracy: 0.1029
case acc: 0.05375071
case acc: 0.24451955
case acc: 0.09250442
case acc: 0.07931075
case acc: 0.044295195
case acc: 0.10315934
top acc: 0.0775 ::: bot acc: 0.0327
top acc: 0.2646 ::: bot acc: 0.2249
top acc: 0.1281 ::: bot acc: 0.0563
top acc: 0.1064 ::: bot acc: 0.0522
top acc: 0.0668 ::: bot acc: 0.0217
top acc: 0.1339 ::: bot acc: 0.0746
current epoch: 4
train loss is 0.010538
average val loss: 0.016018, accuracy: 0.1674
average test loss: 0.016071, accuracy: 0.1668
case acc: 0.1242422
case acc: 0.30192512
case acc: 0.16251913
case acc: 0.14239572
case acc: 0.11060937
case acc: 0.15923093
top acc: 0.1487 ::: bot acc: 0.1013
top acc: 0.3219 ::: bot acc: 0.2825
top acc: 0.1980 ::: bot acc: 0.1263
top acc: 0.1686 ::: bot acc: 0.1167
top acc: 0.1330 ::: bot acc: 0.0879
top acc: 0.1901 ::: bot acc: 0.1313
current epoch: 5
train loss is 0.012872
average val loss: 0.013589, accuracy: 0.1539
average test loss: 0.013654, accuracy: 0.1534
case acc: 0.116288014
case acc: 0.2784539
case acc: 0.15232575
case acc: 0.12834747
case acc: 0.10455005
case acc: 0.14031367
top acc: 0.1407 ::: bot acc: 0.0934
top acc: 0.2994 ::: bot acc: 0.2591
top acc: 0.1877 ::: bot acc: 0.1156
top acc: 0.1548 ::: bot acc: 0.1024
top acc: 0.1275 ::: bot acc: 0.0815
top acc: 0.1705 ::: bot acc: 0.1126
current epoch: 6
train loss is 0.013144
average val loss: 0.002865, accuracy: 0.0538
average test loss: 0.002988, accuracy: 0.0551
case acc: 0.02354626
case acc: 0.16410571
case acc: 0.05079869
case acc: 0.03089494
case acc: 0.0202934
case acc: 0.040791284
top acc: 0.0436 ::: bot acc: 0.0097
top acc: 0.1850 ::: bot acc: 0.1445
top acc: 0.0847 ::: bot acc: 0.0169
top acc: 0.0529 ::: bot acc: 0.0131
top acc: 0.0368 ::: bot acc: 0.0096
top acc: 0.0695 ::: bot acc: 0.0157
current epoch: 7
train loss is 0.007498
average val loss: 0.001162, accuracy: 0.0357
average test loss: 0.001310, accuracy: 0.0399
case acc: 0.032669734
case acc: 0.0983136
case acc: 0.027312465
case acc: 0.02971356
case acc: 0.027441753
case acc: 0.02374832
top acc: 0.0146 ::: bot acc: 0.0517
top acc: 0.1186 ::: bot acc: 0.0787
top acc: 0.0285 ::: bot acc: 0.0442
top acc: 0.0132 ::: bot acc: 0.0511
top acc: 0.0103 ::: bot acc: 0.0474
top acc: 0.0212 ::: bot acc: 0.0376
current epoch: 8
train loss is 0.003366
average val loss: 0.000983, accuracy: 0.0305
average test loss: 0.001116, accuracy: 0.0351
case acc: 0.02619139
case acc: 0.09361736
case acc: 0.026755963
case acc: 0.024550065
case acc: 0.018115783
case acc: 0.021348976
top acc: 0.0126 ::: bot acc: 0.0433
top acc: 0.1142 ::: bot acc: 0.0744
top acc: 0.0310 ::: bot acc: 0.0414
top acc: 0.0124 ::: bot acc: 0.0436
top acc: 0.0134 ::: bot acc: 0.0320
top acc: 0.0299 ::: bot acc: 0.0287
current epoch: 9
train loss is 0.002462
average val loss: 0.001068, accuracy: 0.0307
average test loss: 0.001194, accuracy: 0.0348
case acc: 0.017683746
case acc: 0.10024785
case acc: 0.027503839
case acc: 0.01957586
case acc: 0.01880635
case acc: 0.024686348
top acc: 0.0222 ::: bot acc: 0.0252
top acc: 0.1206 ::: bot acc: 0.0805
top acc: 0.0435 ::: bot acc: 0.0285
top acc: 0.0257 ::: bot acc: 0.0280
top acc: 0.0338 ::: bot acc: 0.0115
top acc: 0.0476 ::: bot acc: 0.0130
current epoch: 10
train loss is 0.002412
average val loss: 0.000986, accuracy: 0.0305
average test loss: 0.001120, accuracy: 0.0342
case acc: 0.017194616
case acc: 0.09483252
case acc: 0.027307386
case acc: 0.019074917
case acc: 0.021289418
case acc: 0.025678594
top acc: 0.0253 ::: bot acc: 0.0217
top acc: 0.1151 ::: bot acc: 0.0754
top acc: 0.0437 ::: bot acc: 0.0280
top acc: 0.0270 ::: bot acc: 0.0256
top acc: 0.0387 ::: bot acc: 0.0085
top acc: 0.0495 ::: bot acc: 0.0116
current epoch: 11
train loss is 0.002167
average val loss: 0.000846, accuracy: 0.0287
average test loss: 0.000974, accuracy: 0.0326
case acc: 0.017095847
case acc: 0.08529683
case acc: 0.026959393
case acc: 0.019372711
case acc: 0.021839645
case acc: 0.02510004
top acc: 0.0246 ::: bot acc: 0.0219
top acc: 0.1056 ::: bot acc: 0.0660
top acc: 0.0403 ::: bot acc: 0.0319
top acc: 0.0263 ::: bot acc: 0.0269
top acc: 0.0397 ::: bot acc: 0.0084
top acc: 0.0482 ::: bot acc: 0.0122
current epoch: 12
train loss is 0.001922
average val loss: 0.000693, accuracy: 0.0264
average test loss: 0.000825, accuracy: 0.0306
case acc: 0.017629422
case acc: 0.07485099
case acc: 0.026979458
case acc: 0.019290712
case acc: 0.021107456
case acc: 0.023877056
top acc: 0.0226 ::: bot acc: 0.0244
top acc: 0.0955 ::: bot acc: 0.0554
top acc: 0.0357 ::: bot acc: 0.0371
top acc: 0.0230 ::: bot acc: 0.0295
top acc: 0.0384 ::: bot acc: 0.0089
top acc: 0.0457 ::: bot acc: 0.0134
current epoch: 13
train loss is 0.001647
average val loss: 0.000645, accuracy: 0.0262
average test loss: 0.000775, accuracy: 0.0300
case acc: 0.01749839
case acc: 0.069982305
case acc: 0.026410231
case acc: 0.019376924
case acc: 0.022504179
case acc: 0.024393288
top acc: 0.0262 ::: bot acc: 0.0221
top acc: 0.0902 ::: bot acc: 0.0501
top acc: 0.0354 ::: bot acc: 0.0362
top acc: 0.0246 ::: bot acc: 0.0282
top acc: 0.0404 ::: bot acc: 0.0085
top acc: 0.0471 ::: bot acc: 0.0123
current epoch: 14
train loss is 0.001503
average val loss: 0.000587, accuracy: 0.0254
average test loss: 0.000721, accuracy: 0.0293
case acc: 0.017160986
case acc: 0.06438459
case acc: 0.026528528
case acc: 0.019194715
case acc: 0.02328741
case acc: 0.025099544
top acc: 0.0277 ::: bot acc: 0.0197
top acc: 0.0844 ::: bot acc: 0.0448
top acc: 0.0357 ::: bot acc: 0.0363
top acc: 0.0255 ::: bot acc: 0.0277
top acc: 0.0421 ::: bot acc: 0.0081
top acc: 0.0483 ::: bot acc: 0.0119
current epoch: 15
train loss is 0.001358
average val loss: 0.000542, accuracy: 0.0249
average test loss: 0.000673, accuracy: 0.0284
case acc: 0.016990634
case acc: 0.059620004
case acc: 0.026452634
case acc: 0.018998094
case acc: 0.023394337
case acc: 0.025165077
top acc: 0.0287 ::: bot acc: 0.0184
top acc: 0.0801 ::: bot acc: 0.0397
top acc: 0.0352 ::: bot acc: 0.0368
top acc: 0.0257 ::: bot acc: 0.0266
top acc: 0.0424 ::: bot acc: 0.0079
top acc: 0.0483 ::: bot acc: 0.0116
current epoch: 16
train loss is 0.001316
average val loss: 0.000536, accuracy: 0.0253
average test loss: 0.000670, accuracy: 0.0287
case acc: 0.017804952
case acc: 0.05732295
case acc: 0.026526418
case acc: 0.01914394
case acc: 0.02499736
case acc: 0.026538124
top acc: 0.0324 ::: bot acc: 0.0151
top acc: 0.0777 ::: bot acc: 0.0374
top acc: 0.0372 ::: bot acc: 0.0347
top acc: 0.0292 ::: bot acc: 0.0235
top acc: 0.0443 ::: bot acc: 0.0085
top acc: 0.0506 ::: bot acc: 0.0112
current epoch: 17
train loss is 0.001220
average val loss: 0.000511, accuracy: 0.0249
average test loss: 0.000643, accuracy: 0.0283
case acc: 0.01805648
case acc: 0.053882215
case acc: 0.026616333
case acc: 0.019475527
case acc: 0.025354754
case acc: 0.026688555
top acc: 0.0336 ::: bot acc: 0.0138
top acc: 0.0742 ::: bot acc: 0.0342
top acc: 0.0383 ::: bot acc: 0.0336
top acc: 0.0304 ::: bot acc: 0.0230
top acc: 0.0449 ::: bot acc: 0.0087
top acc: 0.0509 ::: bot acc: 0.0109
current epoch: 18
train loss is 0.001167
average val loss: 0.000478, accuracy: 0.0244
average test loss: 0.000612, accuracy: 0.0277
case acc: 0.018683318
case acc: 0.050106354
case acc: 0.026773619
case acc: 0.019429151
case acc: 0.024553934
case acc: 0.02678591
top acc: 0.0350 ::: bot acc: 0.0129
top acc: 0.0704 ::: bot acc: 0.0305
top acc: 0.0391 ::: bot acc: 0.0335
top acc: 0.0308 ::: bot acc: 0.0224
top acc: 0.0441 ::: bot acc: 0.0081
top acc: 0.0508 ::: bot acc: 0.0117
current epoch: 19
train loss is 0.001090
average val loss: 0.000452, accuracy: 0.0239
average test loss: 0.000587, accuracy: 0.0273
case acc: 0.018833017
case acc: 0.04741837
case acc: 0.026915805
case acc: 0.019490346
case acc: 0.024519088
case acc: 0.026378183
top acc: 0.0356 ::: bot acc: 0.0119
top acc: 0.0676 ::: bot acc: 0.0279
top acc: 0.0394 ::: bot acc: 0.0329
top acc: 0.0315 ::: bot acc: 0.0214
top acc: 0.0434 ::: bot acc: 0.0088
top acc: 0.0504 ::: bot acc: 0.0111
current epoch: 20
train loss is 0.001023
average val loss: 0.000437, accuracy: 0.0237
average test loss: 0.000572, accuracy: 0.0270
case acc: 0.019673768
case acc: 0.045318637
case acc: 0.026918834
case acc: 0.019434338
case acc: 0.02398021
case acc: 0.02643935
top acc: 0.0375 ::: bot acc: 0.0113
top acc: 0.0654 ::: bot acc: 0.0260
top acc: 0.0410 ::: bot acc: 0.0306
top acc: 0.0318 ::: bot acc: 0.0208
top acc: 0.0430 ::: bot acc: 0.0082
top acc: 0.0508 ::: bot acc: 0.0113
current epoch: 21
train loss is 0.000981
average val loss: 0.000435, accuracy: 0.0238
average test loss: 0.000571, accuracy: 0.0270
case acc: 0.020709418
case acc: 0.043786168
case acc: 0.027413892
case acc: 0.020068023
case acc: 0.023859687
case acc: 0.026303926
top acc: 0.0395 ::: bot acc: 0.0105
top acc: 0.0645 ::: bot acc: 0.0240
top acc: 0.0431 ::: bot acc: 0.0294
top acc: 0.0338 ::: bot acc: 0.0198
top acc: 0.0428 ::: bot acc: 0.0083
top acc: 0.0506 ::: bot acc: 0.0109
current epoch: 22
train loss is 0.000976
average val loss: 0.000451, accuracy: 0.0246
average test loss: 0.000588, accuracy: 0.0276
case acc: 0.022095794
case acc: 0.04347763
case acc: 0.027981613
case acc: 0.020769896
case acc: 0.024414994
case acc: 0.02712598
top acc: 0.0416 ::: bot acc: 0.0098
top acc: 0.0640 ::: bot acc: 0.0239
top acc: 0.0460 ::: bot acc: 0.0266
top acc: 0.0358 ::: bot acc: 0.0179
top acc: 0.0434 ::: bot acc: 0.0084
top acc: 0.0518 ::: bot acc: 0.0108
current epoch: 23
train loss is 0.000951
average val loss: 0.000419, accuracy: 0.0237
average test loss: 0.000556, accuracy: 0.0268
case acc: 0.022135025
case acc: 0.040563542
case acc: 0.02803128
case acc: 0.020639015
case acc: 0.02308675
case acc: 0.026166065
top acc: 0.0418 ::: bot acc: 0.0101
top acc: 0.0609 ::: bot acc: 0.0215
top acc: 0.0457 ::: bot acc: 0.0262
top acc: 0.0357 ::: bot acc: 0.0182
top acc: 0.0419 ::: bot acc: 0.0081
top acc: 0.0504 ::: bot acc: 0.0113
current epoch: 24
train loss is 0.000901
average val loss: 0.000413, accuracy: 0.0236
average test loss: 0.000551, accuracy: 0.0267
case acc: 0.022954049
case acc: 0.03927984
case acc: 0.02848389
case acc: 0.020912433
case acc: 0.022769332
case acc: 0.026005903
top acc: 0.0429 ::: bot acc: 0.0102
top acc: 0.0594 ::: bot acc: 0.0202
top acc: 0.0474 ::: bot acc: 0.0249
top acc: 0.0363 ::: bot acc: 0.0177
top acc: 0.0414 ::: bot acc: 0.0082
top acc: 0.0499 ::: bot acc: 0.0114
current epoch: 25
train loss is 0.000904
average val loss: 0.000479, accuracy: 0.0260
average test loss: 0.000617, accuracy: 0.0287
case acc: 0.025730828
case acc: 0.041605335
case acc: 0.029885786
case acc: 0.02258734
case acc: 0.024712862
case acc: 0.027549725
top acc: 0.0468 ::: bot acc: 0.0103
top acc: 0.0621 ::: bot acc: 0.0222
top acc: 0.0517 ::: bot acc: 0.0203
top acc: 0.0399 ::: bot acc: 0.0153
top acc: 0.0442 ::: bot acc: 0.0083
top acc: 0.0525 ::: bot acc: 0.0106
current epoch: 26
train loss is 0.000919
average val loss: 0.000535, accuracy: 0.0279
average test loss: 0.000672, accuracy: 0.0302
case acc: 0.02803272
case acc: 0.043078337
case acc: 0.031400878
case acc: 0.023918469
case acc: 0.026081165
case acc: 0.02879444
top acc: 0.0500 ::: bot acc: 0.0111
top acc: 0.0637 ::: bot acc: 0.0236
top acc: 0.0555 ::: bot acc: 0.0174
top acc: 0.0425 ::: bot acc: 0.0139
top acc: 0.0458 ::: bot acc: 0.0088
top acc: 0.0544 ::: bot acc: 0.0106
current epoch: 27
train loss is 0.000908
average val loss: 0.000519, accuracy: 0.0274
average test loss: 0.000657, accuracy: 0.0298
case acc: 0.028226752
case acc: 0.04152833
case acc: 0.031642366
case acc: 0.023762602
case acc: 0.025691312
case acc: 0.027864926
top acc: 0.0502 ::: bot acc: 0.0113
top acc: 0.0619 ::: bot acc: 0.0220
top acc: 0.0561 ::: bot acc: 0.0169
top acc: 0.0422 ::: bot acc: 0.0139
top acc: 0.0452 ::: bot acc: 0.0090
top acc: 0.0533 ::: bot acc: 0.0109
current epoch: 28
train loss is 0.000886
average val loss: 0.000535, accuracy: 0.0280
average test loss: 0.000671, accuracy: 0.0302
case acc: 0.029130379
case acc: 0.041377634
case acc: 0.032182407
case acc: 0.024496596
case acc: 0.026382161
case acc: 0.027834939
top acc: 0.0514 ::: bot acc: 0.0118
top acc: 0.0618 ::: bot acc: 0.0219
top acc: 0.0581 ::: bot acc: 0.0146
top acc: 0.0434 ::: bot acc: 0.0139
top acc: 0.0463 ::: bot acc: 0.0090
top acc: 0.0527 ::: bot acc: 0.0108
current epoch: 29
train loss is 0.000900
average val loss: 0.000617, accuracy: 0.0305
average test loss: 0.000751, accuracy: 0.0323
case acc: 0.03212103
case acc: 0.043593556
case acc: 0.03416991
case acc: 0.02621542
case acc: 0.02849587
case acc: 0.029478509
top acc: 0.0550 ::: bot acc: 0.0137
top acc: 0.0641 ::: bot acc: 0.0241
top acc: 0.0619 ::: bot acc: 0.0129
top acc: 0.0462 ::: bot acc: 0.0129
top acc: 0.0491 ::: bot acc: 0.0094
top acc: 0.0552 ::: bot acc: 0.0109
current epoch: 30
train loss is 0.000893
average val loss: 0.000647, accuracy: 0.0315
average test loss: 0.000783, accuracy: 0.0332
case acc: 0.03335085
case acc: 0.0437735
case acc: 0.035219707
case acc: 0.02712943
case acc: 0.029677885
case acc: 0.02980888
top acc: 0.0565 ::: bot acc: 0.0145
top acc: 0.0639 ::: bot acc: 0.0246
top acc: 0.0639 ::: bot acc: 0.0121
top acc: 0.0478 ::: bot acc: 0.0129
top acc: 0.0511 ::: bot acc: 0.0101
top acc: 0.0559 ::: bot acc: 0.0110
current epoch: 31
train loss is 0.000883
average val loss: 0.000630, accuracy: 0.0310
average test loss: 0.000765, accuracy: 0.0327
case acc: 0.03280162
case acc: 0.042539757
case acc: 0.035562236
case acc: 0.026781121
case acc: 0.02943452
case acc: 0.029225746
top acc: 0.0556 ::: bot acc: 0.0140
top acc: 0.0627 ::: bot acc: 0.0233
top acc: 0.0642 ::: bot acc: 0.0124
top acc: 0.0474 ::: bot acc: 0.0128
top acc: 0.0503 ::: bot acc: 0.0101
top acc: 0.0552 ::: bot acc: 0.0109
current epoch: 32
train loss is 0.000879
average val loss: 0.000646, accuracy: 0.0315
average test loss: 0.000780, accuracy: 0.0331
case acc: 0.033579316
case acc: 0.041925203
case acc: 0.03623946
case acc: 0.027362235
case acc: 0.03017086
case acc: 0.029258704
top acc: 0.0568 ::: bot acc: 0.0147
top acc: 0.0621 ::: bot acc: 0.0222
top acc: 0.0655 ::: bot acc: 0.0120
top acc: 0.0484 ::: bot acc: 0.0125
top acc: 0.0515 ::: bot acc: 0.0102
top acc: 0.0551 ::: bot acc: 0.0110
current epoch: 33
train loss is 0.000882
average val loss: 0.000661, accuracy: 0.0320
average test loss: 0.000797, accuracy: 0.0335
case acc: 0.03410863
case acc: 0.042028084
case acc: 0.036976285
case acc: 0.027993565
case acc: 0.030658647
case acc: 0.029337414
top acc: 0.0573 ::: bot acc: 0.0149
top acc: 0.0623 ::: bot acc: 0.0225
top acc: 0.0668 ::: bot acc: 0.0119
top acc: 0.0492 ::: bot acc: 0.0127
top acc: 0.0518 ::: bot acc: 0.0107
top acc: 0.0552 ::: bot acc: 0.0108
current epoch: 34
train loss is 0.000856
average val loss: 0.000673, accuracy: 0.0323
average test loss: 0.000801, accuracy: 0.0337
case acc: 0.034290165
case acc: 0.041492984
case acc: 0.037678313
case acc: 0.028331703
case acc: 0.030682186
case acc: 0.029487608
top acc: 0.0574 ::: bot acc: 0.0151
top acc: 0.0618 ::: bot acc: 0.0223
top acc: 0.0677 ::: bot acc: 0.0117
top acc: 0.0498 ::: bot acc: 0.0123
top acc: 0.0518 ::: bot acc: 0.0107
top acc: 0.0551 ::: bot acc: 0.0110
current epoch: 35
train loss is 0.000875
average val loss: 0.000679, accuracy: 0.0325
average test loss: 0.000811, accuracy: 0.0338
case acc: 0.034468967
case acc: 0.04118333
case acc: 0.038227297
case acc: 0.028701741
case acc: 0.031066813
case acc: 0.02938836
top acc: 0.0575 ::: bot acc: 0.0154
top acc: 0.0614 ::: bot acc: 0.0219
top acc: 0.0687 ::: bot acc: 0.0117
top acc: 0.0506 ::: bot acc: 0.0124
top acc: 0.0523 ::: bot acc: 0.0108
top acc: 0.0552 ::: bot acc: 0.0108
current epoch: 36
train loss is 0.000858
average val loss: 0.000739, accuracy: 0.0342
average test loss: 0.000877, accuracy: 0.0355
case acc: 0.03612069
case acc: 0.04236997
case acc: 0.040004563
case acc: 0.030637942
case acc: 0.03296531
case acc: 0.03077372
top acc: 0.0595 ::: bot acc: 0.0167
top acc: 0.0630 ::: bot acc: 0.0229
top acc: 0.0715 ::: bot acc: 0.0112
top acc: 0.0533 ::: bot acc: 0.0129
top acc: 0.0545 ::: bot acc: 0.0121
top acc: 0.0575 ::: bot acc: 0.0110
current epoch: 37
train loss is 0.000858
average val loss: 0.000733, accuracy: 0.0340
average test loss: 0.000871, accuracy: 0.0354
case acc: 0.035994485
case acc: 0.041401107
case acc: 0.040029973
case acc: 0.030703377
case acc: 0.033423424
case acc: 0.03078804
top acc: 0.0593 ::: bot acc: 0.0166
top acc: 0.0617 ::: bot acc: 0.0222
top acc: 0.0714 ::: bot acc: 0.0116
top acc: 0.0533 ::: bot acc: 0.0128
top acc: 0.0552 ::: bot acc: 0.0125
top acc: 0.0572 ::: bot acc: 0.0111
current epoch: 38
train loss is 0.000844
average val loss: 0.000714, accuracy: 0.0335
average test loss: 0.000850, accuracy: 0.0348
case acc: 0.03509101
case acc: 0.03973609
case acc: 0.039633136
case acc: 0.030308632
case acc: 0.03370179
case acc: 0.030327985
top acc: 0.0586 ::: bot acc: 0.0155
top acc: 0.0600 ::: bot acc: 0.0206
top acc: 0.0711 ::: bot acc: 0.0111
top acc: 0.0527 ::: bot acc: 0.0129
top acc: 0.0555 ::: bot acc: 0.0126
top acc: 0.0567 ::: bot acc: 0.0108
current epoch: 39
train loss is 0.000830
average val loss: 0.000658, accuracy: 0.0319
average test loss: 0.000793, accuracy: 0.0334
case acc: 0.03315045
case acc: 0.03711735
case acc: 0.038516942
case acc: 0.029351808
case acc: 0.032960143
case acc: 0.029375097
top acc: 0.0564 ::: bot acc: 0.0141
top acc: 0.0572 ::: bot acc: 0.0183
top acc: 0.0692 ::: bot acc: 0.0115
top acc: 0.0515 ::: bot acc: 0.0124
top acc: 0.0547 ::: bot acc: 0.0120
top acc: 0.0554 ::: bot acc: 0.0110
current epoch: 40
train loss is 0.000784
average val loss: 0.000523, accuracy: 0.0278
average test loss: 0.000657, accuracy: 0.0298
case acc: 0.028757906
case acc: 0.031964313
case acc: 0.035449985
case acc: 0.026631895
case acc: 0.029284194
case acc: 0.02655311
top acc: 0.0509 ::: bot acc: 0.0115
top acc: 0.0519 ::: bot acc: 0.0139
top acc: 0.0643 ::: bot acc: 0.0120
top acc: 0.0472 ::: bot acc: 0.0127
top acc: 0.0499 ::: bot acc: 0.0100
top acc: 0.0509 ::: bot acc: 0.0109
current epoch: 41
train loss is 0.000726
average val loss: 0.000474, accuracy: 0.0261
average test loss: 0.000610, accuracy: 0.0284
case acc: 0.026927298
case acc: 0.02943488
case acc: 0.034276884
case acc: 0.025791718
case acc: 0.028327413
case acc: 0.02561443
top acc: 0.0485 ::: bot acc: 0.0111
top acc: 0.0489 ::: bot acc: 0.0118
top acc: 0.0623 ::: bot acc: 0.0126
top acc: 0.0458 ::: bot acc: 0.0134
top acc: 0.0491 ::: bot acc: 0.0095
top acc: 0.0495 ::: bot acc: 0.0113
current epoch: 42
train loss is 0.000710
average val loss: 0.000397, accuracy: 0.0234
average test loss: 0.000532, accuracy: 0.0262
case acc: 0.024280472
case acc: 0.025639689
case acc: 0.0325848
case acc: 0.023811018
case acc: 0.026383845
case acc: 0.024216402
top acc: 0.0452 ::: bot acc: 0.0100
top acc: 0.0443 ::: bot acc: 0.0092
top acc: 0.0587 ::: bot acc: 0.0147
top acc: 0.0423 ::: bot acc: 0.0138
top acc: 0.0464 ::: bot acc: 0.0089
top acc: 0.0468 ::: bot acc: 0.0126
current epoch: 43
train loss is 0.000682
average val loss: 0.000363, accuracy: 0.0221
average test loss: 0.000498, accuracy: 0.0251
case acc: 0.022844903
case acc: 0.023456225
case acc: 0.03166561
case acc: 0.023379162
case acc: 0.025549587
case acc: 0.023811067
top acc: 0.0429 ::: bot acc: 0.0103
top acc: 0.0417 ::: bot acc: 0.0082
top acc: 0.0564 ::: bot acc: 0.0165
top acc: 0.0414 ::: bot acc: 0.0145
top acc: 0.0451 ::: bot acc: 0.0086
top acc: 0.0458 ::: bot acc: 0.0131
current epoch: 44
train loss is 0.000655
average val loss: 0.000287, accuracy: 0.0191
average test loss: 0.000425, accuracy: 0.0227
case acc: 0.019819116
case acc: 0.019466747
case acc: 0.029689303
case acc: 0.021579538
case acc: 0.02312871
case acc: 0.022512745
top acc: 0.0377 ::: bot acc: 0.0111
top acc: 0.0360 ::: bot acc: 0.0079
top acc: 0.0512 ::: bot acc: 0.0211
top acc: 0.0375 ::: bot acc: 0.0169
top acc: 0.0418 ::: bot acc: 0.0082
top acc: 0.0425 ::: bot acc: 0.0157
current epoch: 45
train loss is 0.000626
average val loss: 0.000241, accuracy: 0.0172
average test loss: 0.000379, accuracy: 0.0211
case acc: 0.017917415
case acc: 0.016944615
case acc: 0.028337637
case acc: 0.0201879
case acc: 0.021424854
case acc: 0.021663327
top acc: 0.0337 ::: bot acc: 0.0137
top acc: 0.0313 ::: bot acc: 0.0095
top acc: 0.0470 ::: bot acc: 0.0254
top acc: 0.0343 ::: bot acc: 0.0190
top acc: 0.0390 ::: bot acc: 0.0088
top acc: 0.0400 ::: bot acc: 0.0174
current epoch: 46
train loss is 0.000609
average val loss: 0.000198, accuracy: 0.0153
average test loss: 0.000336, accuracy: 0.0197
case acc: 0.017225198
case acc: 0.014860325
case acc: 0.026945872
case acc: 0.0192749
case acc: 0.019241171
case acc: 0.020882009
top acc: 0.0279 ::: bot acc: 0.0198
top acc: 0.0246 ::: bot acc: 0.0156
top acc: 0.0407 ::: bot acc: 0.0315
top acc: 0.0297 ::: bot acc: 0.0237
top acc: 0.0341 ::: bot acc: 0.0114
top acc: 0.0359 ::: bot acc: 0.0217
current epoch: 47
train loss is 0.000601
average val loss: 0.000189, accuracy: 0.0148
average test loss: 0.000327, accuracy: 0.0195
case acc: 0.017160924
case acc: 0.014589301
case acc: 0.026583364
case acc: 0.019247293
case acc: 0.018588964
case acc: 0.020854222
top acc: 0.0250 ::: bot acc: 0.0223
top acc: 0.0210 ::: bot acc: 0.0194
top acc: 0.0369 ::: bot acc: 0.0352
top acc: 0.0272 ::: bot acc: 0.0260
top acc: 0.0327 ::: bot acc: 0.0126
top acc: 0.0350 ::: bot acc: 0.0230
current epoch: 48
train loss is 0.000607
average val loss: 0.000188, accuracy: 0.0148
average test loss: 0.000330, accuracy: 0.0198
case acc: 0.018237336
case acc: 0.015463396
case acc: 0.026853051
case acc: 0.019516652
case acc: 0.017693957
case acc: 0.020851769
top acc: 0.0203 ::: bot acc: 0.0274
top acc: 0.0149 ::: bot acc: 0.0254
top acc: 0.0312 ::: bot acc: 0.0414
top acc: 0.0234 ::: bot acc: 0.0298
top acc: 0.0300 ::: bot acc: 0.0156
top acc: 0.0320 ::: bot acc: 0.0259
current epoch: 49
train loss is 0.000623
average val loss: 0.000219, accuracy: 0.0162
average test loss: 0.000359, accuracy: 0.0209
case acc: 0.020588571
case acc: 0.01838559
case acc: 0.027765678
case acc: 0.020777455
case acc: 0.016763123
case acc: 0.02128767
top acc: 0.0153 ::: bot acc: 0.0333
top acc: 0.0094 ::: bot acc: 0.0324
top acc: 0.0237 ::: bot acc: 0.0487
top acc: 0.0180 ::: bot acc: 0.0353
top acc: 0.0255 ::: bot acc: 0.0196
top acc: 0.0281 ::: bot acc: 0.0298
current epoch: 50
train loss is 0.000656
average val loss: 0.000288, accuracy: 0.0190
average test loss: 0.000430, accuracy: 0.0233
case acc: 0.023933593
case acc: 0.02367543
case acc: 0.030414121
case acc: 0.02286016
case acc: 0.016639212
case acc: 0.02204502
top acc: 0.0127 ::: bot acc: 0.0398
top acc: 0.0094 ::: bot acc: 0.0403
top acc: 0.0159 ::: bot acc: 0.0568
top acc: 0.0135 ::: bot acc: 0.0409
top acc: 0.0217 ::: bot acc: 0.0235
top acc: 0.0242 ::: bot acc: 0.0335
LME_Co_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6780 6780 6780
1.7082474 -0.6288155 0.12137239 -0.1537469
Validation: 756 756 756
Testing: 774 774 774
pre-processing time: 0.0001957416534423828
the split date is 2010-07-01
train dropout: 0.6 test dropout: 0.15
net initializing with time: 0.002284526824951172
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012799
average val loss: 0.005793, accuracy: 0.1011
average test loss: 0.005961, accuracy: 0.1016
case acc: 0.14482549
case acc: 0.080452874
case acc: 0.10351997
case acc: 0.09069305
case acc: 0.13299881
case acc: 0.0568183
top acc: 0.1216 ::: bot acc: 0.1665
top acc: 0.1027 ::: bot acc: 0.0596
top acc: 0.0697 ::: bot acc: 0.1358
top acc: 0.0587 ::: bot acc: 0.1216
top acc: 0.0999 ::: bot acc: 0.1622
top acc: 0.0287 ::: bot acc: 0.0840
current epoch: 2
train loss is 0.008189
average val loss: 0.002946, accuracy: 0.0542
average test loss: 0.003051, accuracy: 0.0560
case acc: 0.04268043
case acc: 0.16589066
case acc: 0.02404026
case acc: 0.023414101
case acc: 0.047744174
case acc: 0.032389414
top acc: 0.0209 ::: bot acc: 0.0643
top acc: 0.1881 ::: bot acc: 0.1448
top acc: 0.0312 ::: bot acc: 0.0341
top acc: 0.0372 ::: bot acc: 0.0273
top acc: 0.0221 ::: bot acc: 0.0724
top acc: 0.0591 ::: bot acc: 0.0111
current epoch: 3
train loss is 0.008525
average val loss: 0.007492, accuracy: 0.1005
average test loss: 0.007563, accuracy: 0.1000
case acc: 0.04754132
case acc: 0.24073638
case acc: 0.0880053
case acc: 0.08528726
case acc: 0.036364917
case acc: 0.10221224
top acc: 0.0700 ::: bot acc: 0.0265
top acc: 0.2635 ::: bot acc: 0.2194
top acc: 0.1203 ::: bot acc: 0.0563
top acc: 0.1189 ::: bot acc: 0.0542
top acc: 0.0688 ::: bot acc: 0.0099
top acc: 0.1325 ::: bot acc: 0.0747
current epoch: 4
train loss is 0.010011
average val loss: 0.016127, accuracy: 0.1672
average test loss: 0.016113, accuracy: 0.1666
case acc: 0.1214251
case acc: 0.30080733
case acc: 0.16063002
case acc: 0.1510869
case acc: 0.10464852
case acc: 0.16074586
top acc: 0.1445 ::: bot acc: 0.0997
top acc: 0.3233 ::: bot acc: 0.2799
top acc: 0.1935 ::: bot acc: 0.1287
top acc: 0.1847 ::: bot acc: 0.1194
top acc: 0.1375 ::: bot acc: 0.0763
top acc: 0.1903 ::: bot acc: 0.1334
current epoch: 5
train loss is 0.012984
average val loss: 0.012660, accuracy: 0.1468
average test loss: 0.012671, accuracy: 0.1462
case acc: 0.106253624
case acc: 0.27054477
case acc: 0.14316328
case acc: 0.13040885
case acc: 0.091717355
case acc: 0.13520807
top acc: 0.1295 ::: bot acc: 0.0843
top acc: 0.2927 ::: bot acc: 0.2494
top acc: 0.1762 ::: bot acc: 0.1105
top acc: 0.1645 ::: bot acc: 0.0988
top acc: 0.1247 ::: bot acc: 0.0635
top acc: 0.1648 ::: bot acc: 0.1078
current epoch: 6
train loss is 0.012850
average val loss: 0.003122, accuracy: 0.0573
average test loss: 0.003178, accuracy: 0.0582
case acc: 0.022938086
case acc: 0.16558257
case acc: 0.052338686
case acc: 0.04063984
case acc: 0.02325893
case acc: 0.04459086
top acc: 0.0422 ::: bot acc: 0.0089
top acc: 0.1879 ::: bot acc: 0.1446
top acc: 0.0831 ::: bot acc: 0.0251
top acc: 0.0715 ::: bot acc: 0.0150
top acc: 0.0450 ::: bot acc: 0.0167
top acc: 0.0729 ::: bot acc: 0.0193
current epoch: 7
train loss is 0.007161
average val loss: 0.001303, accuracy: 0.0410
average test loss: 0.001386, accuracy: 0.0426
case acc: 0.038678154
case acc: 0.092578284
case acc: 0.027362203
case acc: 0.031938557
case acc: 0.040502645
case acc: 0.024594668
top acc: 0.0167 ::: bot acc: 0.0602
top acc: 0.1134 ::: bot acc: 0.0719
top acc: 0.0192 ::: bot acc: 0.0475
top acc: 0.0197 ::: bot acc: 0.0530
top acc: 0.0184 ::: bot acc: 0.0637
top acc: 0.0180 ::: bot acc: 0.0407
current epoch: 8
train loss is 0.003345
average val loss: 0.001109, accuracy: 0.0353
average test loss: 0.001167, accuracy: 0.0369
case acc: 0.028602382
case acc: 0.090961315
case acc: 0.025305808
case acc: 0.02657213
case acc: 0.028510537
case acc: 0.021739077
top acc: 0.0099 ::: bot acc: 0.0486
top acc: 0.1124 ::: bot acc: 0.0706
top acc: 0.0243 ::: bot acc: 0.0410
top acc: 0.0245 ::: bot acc: 0.0430
top acc: 0.0191 ::: bot acc: 0.0455
top acc: 0.0302 ::: bot acc: 0.0279
current epoch: 9
train loss is 0.002441
average val loss: 0.001129, accuracy: 0.0339
average test loss: 0.001188, accuracy: 0.0351
case acc: 0.018509
case acc: 0.096413
case acc: 0.024171598
case acc: 0.02394628
case acc: 0.023016807
case acc: 0.024380416
top acc: 0.0138 ::: bot acc: 0.0319
top acc: 0.1182 ::: bot acc: 0.0755
top acc: 0.0359 ::: bot acc: 0.0300
top acc: 0.0380 ::: bot acc: 0.0282
top acc: 0.0365 ::: bot acc: 0.0257
top acc: 0.0459 ::: bot acc: 0.0135
current epoch: 10
train loss is 0.002301
average val loss: 0.001055, accuracy: 0.0333
average test loss: 0.001119, accuracy: 0.0343
case acc: 0.017701786
case acc: 0.09119815
case acc: 0.024271185
case acc: 0.024372827
case acc: 0.022845624
case acc: 0.025445582
top acc: 0.0175 ::: bot acc: 0.0278
top acc: 0.1133 ::: bot acc: 0.0698
top acc: 0.0366 ::: bot acc: 0.0300
top acc: 0.0409 ::: bot acc: 0.0256
top acc: 0.0417 ::: bot acc: 0.0203
top acc: 0.0482 ::: bot acc: 0.0117
current epoch: 11
train loss is 0.002147
average val loss: 0.000903, accuracy: 0.0315
average test loss: 0.000957, accuracy: 0.0323
case acc: 0.017634254
case acc: 0.08075939
case acc: 0.02398543
case acc: 0.024130337
case acc: 0.022854635
case acc: 0.024380835
top acc: 0.0158 ::: bot acc: 0.0288
top acc: 0.1027 ::: bot acc: 0.0599
top acc: 0.0317 ::: bot acc: 0.0340
top acc: 0.0388 ::: bot acc: 0.0278
top acc: 0.0420 ::: bot acc: 0.0201
top acc: 0.0463 ::: bot acc: 0.0124
current epoch: 12
train loss is 0.001852
average val loss: 0.000840, accuracy: 0.0308
average test loss: 0.000898, accuracy: 0.0316
case acc: 0.017270662
case acc: 0.07512035
case acc: 0.024193062
case acc: 0.024074247
case acc: 0.023267765
case acc: 0.025572702
top acc: 0.0191 ::: bot acc: 0.0263
top acc: 0.0971 ::: bot acc: 0.0541
top acc: 0.0320 ::: bot acc: 0.0342
top acc: 0.0405 ::: bot acc: 0.0259
top acc: 0.0457 ::: bot acc: 0.0160
top acc: 0.0489 ::: bot acc: 0.0113
current epoch: 13
train loss is 0.001636
average val loss: 0.000742, accuracy: 0.0294
average test loss: 0.000793, accuracy: 0.0301
case acc: 0.01722706
case acc: 0.06666554
case acc: 0.024107996
case acc: 0.02406988
case acc: 0.02349837
case acc: 0.024783468
top acc: 0.0187 ::: bot acc: 0.0266
top acc: 0.0885 ::: bot acc: 0.0455
top acc: 0.0283 ::: bot acc: 0.0370
top acc: 0.0389 ::: bot acc: 0.0279
top acc: 0.0451 ::: bot acc: 0.0170
top acc: 0.0469 ::: bot acc: 0.0123
current epoch: 14
train loss is 0.001421
average val loss: 0.000675, accuracy: 0.0283
average test loss: 0.000735, accuracy: 0.0292
case acc: 0.016894814
case acc: 0.0606786
case acc: 0.024668293
case acc: 0.02411225
case acc: 0.023601446
case acc: 0.025092166
top acc: 0.0201 ::: bot acc: 0.0249
top acc: 0.0827 ::: bot acc: 0.0397
top acc: 0.0285 ::: bot acc: 0.0381
top acc: 0.0392 ::: bot acc: 0.0271
top acc: 0.0465 ::: bot acc: 0.0162
top acc: 0.0478 ::: bot acc: 0.0119
current epoch: 15
train loss is 0.001269
average val loss: 0.000631, accuracy: 0.0276
average test loss: 0.000688, accuracy: 0.0284
case acc: 0.016947834
case acc: 0.055745542
case acc: 0.02468259
case acc: 0.024186306
case acc: 0.023649242
case acc: 0.025203368
top acc: 0.0218 ::: bot acc: 0.0236
top acc: 0.0772 ::: bot acc: 0.0351
top acc: 0.0280 ::: bot acc: 0.0383
top acc: 0.0399 ::: bot acc: 0.0267
top acc: 0.0464 ::: bot acc: 0.0160
top acc: 0.0478 ::: bot acc: 0.0124
current epoch: 16
train loss is 0.001183
average val loss: 0.000631, accuracy: 0.0278
average test loss: 0.000686, accuracy: 0.0283
case acc: 0.016380457
case acc: 0.054049198
case acc: 0.023943199
case acc: 0.024627257
case acc: 0.02453502
case acc: 0.026462637
top acc: 0.0253 ::: bot acc: 0.0196
top acc: 0.0761 ::: bot acc: 0.0333
top acc: 0.0302 ::: bot acc: 0.0352
top acc: 0.0432 ::: bot acc: 0.0231
top acc: 0.0490 ::: bot acc: 0.0137
top acc: 0.0501 ::: bot acc: 0.0109
current epoch: 17
train loss is 0.001121
average val loss: 0.000608, accuracy: 0.0274
average test loss: 0.000663, accuracy: 0.0279
case acc: 0.016732907
case acc: 0.050846227
case acc: 0.024090197
case acc: 0.024726702
case acc: 0.024552954
case acc: 0.026645435
top acc: 0.0275 ::: bot acc: 0.0181
top acc: 0.0729 ::: bot acc: 0.0297
top acc: 0.0317 ::: bot acc: 0.0342
top acc: 0.0439 ::: bot acc: 0.0222
top acc: 0.0492 ::: bot acc: 0.0132
top acc: 0.0504 ::: bot acc: 0.0110
current epoch: 18
train loss is 0.001037
average val loss: 0.000589, accuracy: 0.0271
average test loss: 0.000645, accuracy: 0.0276
case acc: 0.01709685
case acc: 0.048135735
case acc: 0.024004601
case acc: 0.025290605
case acc: 0.024483025
case acc: 0.026810426
top acc: 0.0294 ::: bot acc: 0.0159
top acc: 0.0700 ::: bot acc: 0.0271
top acc: 0.0331 ::: bot acc: 0.0327
top acc: 0.0454 ::: bot acc: 0.0210
top acc: 0.0488 ::: bot acc: 0.0135
top acc: 0.0510 ::: bot acc: 0.0110
current epoch: 19
train loss is 0.001003
average val loss: 0.000578, accuracy: 0.0269
average test loss: 0.000636, accuracy: 0.0275
case acc: 0.017688917
case acc: 0.04614836
case acc: 0.024396168
case acc: 0.025689594
case acc: 0.024260625
case acc: 0.026758546
top acc: 0.0315 ::: bot acc: 0.0143
top acc: 0.0682 ::: bot acc: 0.0249
top acc: 0.0353 ::: bot acc: 0.0313
top acc: 0.0468 ::: bot acc: 0.0199
top acc: 0.0489 ::: bot acc: 0.0133
top acc: 0.0507 ::: bot acc: 0.0108
current epoch: 20
train loss is 0.000952
average val loss: 0.000578, accuracy: 0.0271
average test loss: 0.000632, accuracy: 0.0275
case acc: 0.018260298
case acc: 0.04475595
case acc: 0.024370786
case acc: 0.025968157
case acc: 0.02466558
case acc: 0.026929213
top acc: 0.0336 ::: bot acc: 0.0120
top acc: 0.0663 ::: bot acc: 0.0239
top acc: 0.0375 ::: bot acc: 0.0290
top acc: 0.0481 ::: bot acc: 0.0182
top acc: 0.0492 ::: bot acc: 0.0135
top acc: 0.0512 ::: bot acc: 0.0105
current epoch: 21
train loss is 0.000939
average val loss: 0.000591, accuracy: 0.0276
average test loss: 0.000649, accuracy: 0.0279
case acc: 0.019465264
case acc: 0.044249654
case acc: 0.024567464
case acc: 0.026718268
case acc: 0.024660243
case acc: 0.027543163
top acc: 0.0361 ::: bot acc: 0.0103
top acc: 0.0664 ::: bot acc: 0.0232
top acc: 0.0400 ::: bot acc: 0.0259
top acc: 0.0503 ::: bot acc: 0.0164
top acc: 0.0496 ::: bot acc: 0.0127
top acc: 0.0523 ::: bot acc: 0.0103
current epoch: 22
train loss is 0.000912
average val loss: 0.000639, accuracy: 0.0291
average test loss: 0.000689, accuracy: 0.0290
case acc: 0.02143609
case acc: 0.045039568
case acc: 0.0253773
case acc: 0.028173693
case acc: 0.02555504
case acc: 0.028555032
top acc: 0.0400 ::: bot acc: 0.0083
top acc: 0.0670 ::: bot acc: 0.0242
top acc: 0.0435 ::: bot acc: 0.0221
top acc: 0.0532 ::: bot acc: 0.0141
top acc: 0.0518 ::: bot acc: 0.0113
top acc: 0.0540 ::: bot acc: 0.0099
current epoch: 23
train loss is 0.000912
average val loss: 0.000674, accuracy: 0.0302
average test loss: 0.000727, accuracy: 0.0301
case acc: 0.02347124
case acc: 0.04522675
case acc: 0.026379356
case acc: 0.029606743
case acc: 0.026376259
case acc: 0.029572308
top acc: 0.0428 ::: bot acc: 0.0088
top acc: 0.0670 ::: bot acc: 0.0245
top acc: 0.0471 ::: bot acc: 0.0191
top acc: 0.0558 ::: bot acc: 0.0135
top acc: 0.0535 ::: bot acc: 0.0102
top acc: 0.0553 ::: bot acc: 0.0100
current epoch: 24
train loss is 0.000878
average val loss: 0.000622, accuracy: 0.0288
average test loss: 0.000677, accuracy: 0.0288
case acc: 0.02280081
case acc: 0.04200684
case acc: 0.026301969
case acc: 0.028845971
case acc: 0.025186704
case acc: 0.0278993
top acc: 0.0422 ::: bot acc: 0.0085
top acc: 0.0638 ::: bot acc: 0.0211
top acc: 0.0464 ::: bot acc: 0.0198
top acc: 0.0544 ::: bot acc: 0.0143
top acc: 0.0511 ::: bot acc: 0.0115
top acc: 0.0529 ::: bot acc: 0.0102
current epoch: 25
train loss is 0.000836
average val loss: 0.000660, accuracy: 0.0298
average test loss: 0.000704, accuracy: 0.0297
case acc: 0.024489233
case acc: 0.042351853
case acc: 0.02741568
case acc: 0.029587172
case acc: 0.025974106
case acc: 0.028407075
top acc: 0.0442 ::: bot acc: 0.0088
top acc: 0.0643 ::: bot acc: 0.0215
top acc: 0.0494 ::: bot acc: 0.0173
top acc: 0.0562 ::: bot acc: 0.0131
top acc: 0.0525 ::: bot acc: 0.0111
top acc: 0.0535 ::: bot acc: 0.0103
current epoch: 26
train loss is 0.000826
average val loss: 0.000664, accuracy: 0.0300
average test loss: 0.000720, accuracy: 0.0301
case acc: 0.02548303
case acc: 0.041912284
case acc: 0.028338907
case acc: 0.030273058
case acc: 0.02607203
case acc: 0.028388545
top acc: 0.0456 ::: bot acc: 0.0093
top acc: 0.0637 ::: bot acc: 0.0212
top acc: 0.0512 ::: bot acc: 0.0164
top acc: 0.0572 ::: bot acc: 0.0129
top acc: 0.0528 ::: bot acc: 0.0109
top acc: 0.0537 ::: bot acc: 0.0101
current epoch: 27
train loss is 0.000827
average val loss: 0.000685, accuracy: 0.0305
average test loss: 0.000734, accuracy: 0.0305
case acc: 0.026542168
case acc: 0.041656293
case acc: 0.029208295
case acc: 0.031037485
case acc: 0.026151998
case acc: 0.028292561
top acc: 0.0468 ::: bot acc: 0.0095
top acc: 0.0635 ::: bot acc: 0.0206
top acc: 0.0531 ::: bot acc: 0.0154
top acc: 0.0585 ::: bot acc: 0.0125
top acc: 0.0531 ::: bot acc: 0.0106
top acc: 0.0539 ::: bot acc: 0.0097
current epoch: 28
train loss is 0.000832
average val loss: 0.000744, accuracy: 0.0321
average test loss: 0.000794, accuracy: 0.0321
case acc: 0.028789865
case acc: 0.043040387
case acc: 0.031067224
case acc: 0.032722127
case acc: 0.027220445
case acc: 0.029856015
top acc: 0.0497 ::: bot acc: 0.0108
top acc: 0.0650 ::: bot acc: 0.0220
top acc: 0.0564 ::: bot acc: 0.0145
top acc: 0.0611 ::: bot acc: 0.0124
top acc: 0.0550 ::: bot acc: 0.0097
top acc: 0.0559 ::: bot acc: 0.0100
current epoch: 29
train loss is 0.000828
average val loss: 0.000785, accuracy: 0.0332
average test loss: 0.000835, accuracy: 0.0332
case acc: 0.030384079
case acc: 0.04356768
case acc: 0.032878794
case acc: 0.033895124
case acc: 0.028090304
case acc: 0.030457338
top acc: 0.0517 ::: bot acc: 0.0116
top acc: 0.0655 ::: bot acc: 0.0227
top acc: 0.0587 ::: bot acc: 0.0148
top acc: 0.0628 ::: bot acc: 0.0127
top acc: 0.0566 ::: bot acc: 0.0092
top acc: 0.0568 ::: bot acc: 0.0100
current epoch: 30
train loss is 0.000828
average val loss: 0.000818, accuracy: 0.0341
average test loss: 0.000867, accuracy: 0.0340
case acc: 0.031542145
case acc: 0.04385796
case acc: 0.034135655
case acc: 0.03465596
case acc: 0.028955568
case acc: 0.031000374
top acc: 0.0530 ::: bot acc: 0.0127
top acc: 0.0658 ::: bot acc: 0.0226
top acc: 0.0608 ::: bot acc: 0.0147
top acc: 0.0641 ::: bot acc: 0.0123
top acc: 0.0582 ::: bot acc: 0.0085
top acc: 0.0573 ::: bot acc: 0.0104
current epoch: 31
train loss is 0.000827
average val loss: 0.000853, accuracy: 0.0350
average test loss: 0.000910, accuracy: 0.0351
case acc: 0.032887068
case acc: 0.04429912
case acc: 0.035665557
case acc: 0.035888743
case acc: 0.0300555
case acc: 0.031644236
top acc: 0.0548 ::: bot acc: 0.0133
top acc: 0.0662 ::: bot acc: 0.0236
top acc: 0.0631 ::: bot acc: 0.0149
top acc: 0.0658 ::: bot acc: 0.0124
top acc: 0.0602 ::: bot acc: 0.0083
top acc: 0.0585 ::: bot acc: 0.0101
current epoch: 32
train loss is 0.000836
average val loss: 0.000872, accuracy: 0.0354
average test loss: 0.000918, accuracy: 0.0353
case acc: 0.0331061
case acc: 0.04374189
case acc: 0.036479987
case acc: 0.036314417
case acc: 0.03056423
case acc: 0.031699948
top acc: 0.0549 ::: bot acc: 0.0136
top acc: 0.0657 ::: bot acc: 0.0227
top acc: 0.0639 ::: bot acc: 0.0155
top acc: 0.0663 ::: bot acc: 0.0124
top acc: 0.0605 ::: bot acc: 0.0086
top acc: 0.0584 ::: bot acc: 0.0103
current epoch: 33
train loss is 0.000824
average val loss: 0.000895, accuracy: 0.0360
average test loss: 0.000940, accuracy: 0.0358
case acc: 0.03374051
case acc: 0.04333985
case acc: 0.037232958
case acc: 0.03733018
case acc: 0.030985288
case acc: 0.03227337
top acc: 0.0557 ::: bot acc: 0.0140
top acc: 0.0652 ::: bot acc: 0.0224
top acc: 0.0649 ::: bot acc: 0.0154
top acc: 0.0676 ::: bot acc: 0.0128
top acc: 0.0617 ::: bot acc: 0.0082
top acc: 0.0590 ::: bot acc: 0.0107
current epoch: 34
train loss is 0.000817
average val loss: 0.000841, accuracy: 0.0347
average test loss: 0.000895, accuracy: 0.0347
case acc: 0.032594778
case acc: 0.041120995
case acc: 0.03641101
case acc: 0.036464434
case acc: 0.030615851
case acc: 0.03125263
top acc: 0.0541 ::: bot acc: 0.0134
top acc: 0.0627 ::: bot acc: 0.0207
top acc: 0.0639 ::: bot acc: 0.0152
top acc: 0.0669 ::: bot acc: 0.0126
top acc: 0.0612 ::: bot acc: 0.0083
top acc: 0.0578 ::: bot acc: 0.0103
current epoch: 35
train loss is 0.000788
average val loss: 0.000794, accuracy: 0.0335
average test loss: 0.000846, accuracy: 0.0335
case acc: 0.031002099
case acc: 0.038578924
case acc: 0.035898104
case acc: 0.03555933
case acc: 0.02979792
case acc: 0.030124834
top acc: 0.0528 ::: bot acc: 0.0119
top acc: 0.0603 ::: bot acc: 0.0180
top acc: 0.0631 ::: bot acc: 0.0153
top acc: 0.0655 ::: bot acc: 0.0124
top acc: 0.0595 ::: bot acc: 0.0087
top acc: 0.0563 ::: bot acc: 0.0100
current epoch: 36
train loss is 0.000755
average val loss: 0.000789, accuracy: 0.0334
average test loss: 0.000842, accuracy: 0.0333
case acc: 0.03055326
case acc: 0.03762778
case acc: 0.035910923
case acc: 0.035941042
case acc: 0.02994375
case acc: 0.030032054
top acc: 0.0519 ::: bot acc: 0.0120
top acc: 0.0594 ::: bot acc: 0.0170
top acc: 0.0633 ::: bot acc: 0.0152
top acc: 0.0661 ::: bot acc: 0.0125
top acc: 0.0601 ::: bot acc: 0.0085
top acc: 0.0562 ::: bot acc: 0.0099
current epoch: 37
train loss is 0.000731
average val loss: 0.000710, accuracy: 0.0313
average test loss: 0.000763, accuracy: 0.0313
case acc: 0.027989913
case acc: 0.03443355
case acc: 0.033945054
case acc: 0.034334004
case acc: 0.028366067
case acc: 0.028808972
top acc: 0.0489 ::: bot acc: 0.0103
top acc: 0.0560 ::: bot acc: 0.0142
top acc: 0.0603 ::: bot acc: 0.0149
top acc: 0.0639 ::: bot acc: 0.0121
top acc: 0.0573 ::: bot acc: 0.0089
top acc: 0.0542 ::: bot acc: 0.0100
current epoch: 38
train loss is 0.000692
average val loss: 0.000632, accuracy: 0.0291
average test loss: 0.000689, accuracy: 0.0292
case acc: 0.025627276
case acc: 0.030775908
case acc: 0.031888165
case acc: 0.032546848
case acc: 0.027203768
case acc: 0.02737503
top acc: 0.0460 ::: bot acc: 0.0092
top acc: 0.0519 ::: bot acc: 0.0115
top acc: 0.0575 ::: bot acc: 0.0147
top acc: 0.0611 ::: bot acc: 0.0122
top acc: 0.0553 ::: bot acc: 0.0097
top acc: 0.0518 ::: bot acc: 0.0106
current epoch: 39
train loss is 0.000663
average val loss: 0.000562, accuracy: 0.0271
average test loss: 0.000617, accuracy: 0.0272
case acc: 0.023100073
case acc: 0.027233282
case acc: 0.029776096
case acc: 0.031138634
case acc: 0.026234612
case acc: 0.025891423
top acc: 0.0424 ::: bot acc: 0.0086
top acc: 0.0477 ::: bot acc: 0.0092
top acc: 0.0542 ::: bot acc: 0.0149
top acc: 0.0585 ::: bot acc: 0.0126
top acc: 0.0534 ::: bot acc: 0.0107
top acc: 0.0490 ::: bot acc: 0.0116
current epoch: 40
train loss is 0.000626
average val loss: 0.000537, accuracy: 0.0263
average test loss: 0.000594, accuracy: 0.0265
case acc: 0.02209303
case acc: 0.026031509
case acc: 0.029243035
case acc: 0.030361371
case acc: 0.025994759
case acc: 0.025437884
top acc: 0.0410 ::: bot acc: 0.0083
top acc: 0.0458 ::: bot acc: 0.0090
top acc: 0.0531 ::: bot acc: 0.0155
top acc: 0.0575 ::: bot acc: 0.0128
top acc: 0.0528 ::: bot acc: 0.0108
top acc: 0.0486 ::: bot acc: 0.0116
current epoch: 41
train loss is 0.000605
average val loss: 0.000475, accuracy: 0.0244
average test loss: 0.000535, accuracy: 0.0247
case acc: 0.020307552
case acc: 0.022781232
case acc: 0.02749254
case acc: 0.028970197
case acc: 0.02480246
case acc: 0.024070397
top acc: 0.0378 ::: bot acc: 0.0097
top acc: 0.0417 ::: bot acc: 0.0076
top acc: 0.0498 ::: bot acc: 0.0170
top acc: 0.0550 ::: bot acc: 0.0138
top acc: 0.0499 ::: bot acc: 0.0128
top acc: 0.0457 ::: bot acc: 0.0132
current epoch: 42
train loss is 0.000572
average val loss: 0.000415, accuracy: 0.0224
average test loss: 0.000478, accuracy: 0.0231
case acc: 0.018509854
case acc: 0.020114448
case acc: 0.025944218
case acc: 0.027336277
case acc: 0.023686524
case acc: 0.022776151
top acc: 0.0336 ::: bot acc: 0.0126
top acc: 0.0376 ::: bot acc: 0.0080
top acc: 0.0457 ::: bot acc: 0.0201
top acc: 0.0515 ::: bot acc: 0.0156
top acc: 0.0469 ::: bot acc: 0.0158
top acc: 0.0426 ::: bot acc: 0.0151
current epoch: 43
train loss is 0.000550
average val loss: 0.000375, accuracy: 0.0210
average test loss: 0.000440, accuracy: 0.0220
case acc: 0.017322566
case acc: 0.018032722
case acc: 0.025102664
case acc: 0.026200268
case acc: 0.023118928
case acc: 0.021992473
top acc: 0.0300 ::: bot acc: 0.0158
top acc: 0.0335 ::: bot acc: 0.0097
top acc: 0.0423 ::: bot acc: 0.0240
top acc: 0.0486 ::: bot acc: 0.0177
top acc: 0.0440 ::: bot acc: 0.0185
top acc: 0.0401 ::: bot acc: 0.0178
current epoch: 44
train loss is 0.000538
average val loss: 0.000364, accuracy: 0.0206
average test loss: 0.000429, accuracy: 0.0216
case acc: 0.016981157
case acc: 0.017162045
case acc: 0.024626756
case acc: 0.025968004
case acc: 0.022981511
case acc: 0.02191945
top acc: 0.0286 ::: bot acc: 0.0172
top acc: 0.0314 ::: bot acc: 0.0115
top acc: 0.0406 ::: bot acc: 0.0252
top acc: 0.0480 ::: bot acc: 0.0185
top acc: 0.0435 ::: bot acc: 0.0189
top acc: 0.0399 ::: bot acc: 0.0179
current epoch: 45
train loss is 0.000528
average val loss: 0.000346, accuracy: 0.0200
average test loss: 0.000415, accuracy: 0.0212
case acc: 0.016690882
case acc: 0.016423875
case acc: 0.024207318
case acc: 0.02544578
case acc: 0.023114856
case acc: 0.021559948
top acc: 0.0262 ::: bot acc: 0.0199
top acc: 0.0285 ::: bot acc: 0.0144
top acc: 0.0377 ::: bot acc: 0.0283
top acc: 0.0463 ::: bot acc: 0.0201
top acc: 0.0425 ::: bot acc: 0.0208
top acc: 0.0388 ::: bot acc: 0.0187
current epoch: 46
train loss is 0.000519
average val loss: 0.000325, accuracy: 0.0194
average test loss: 0.000395, accuracy: 0.0210
case acc: 0.01707365
case acc: 0.015955275
case acc: 0.024109663
case acc: 0.024484174
case acc: 0.022947192
case acc: 0.021274742
top acc: 0.0215 ::: bot acc: 0.0244
top acc: 0.0229 ::: bot acc: 0.0203
top acc: 0.0326 ::: bot acc: 0.0332
top acc: 0.0424 ::: bot acc: 0.0241
top acc: 0.0385 ::: bot acc: 0.0243
top acc: 0.0355 ::: bot acc: 0.0221
current epoch: 47
train loss is 0.000516
average val loss: 0.000322, accuracy: 0.0194
average test loss: 0.000391, accuracy: 0.0211
case acc: 0.017716845
case acc: 0.016033716
case acc: 0.024400564
case acc: 0.024099166
case acc: 0.022903683
case acc: 0.021240259
top acc: 0.0177 ::: bot acc: 0.0282
top acc: 0.0186 ::: bot acc: 0.0244
top acc: 0.0280 ::: bot acc: 0.0376
top acc: 0.0395 ::: bot acc: 0.0271
top acc: 0.0358 ::: bot acc: 0.0264
top acc: 0.0334 ::: bot acc: 0.0242
current epoch: 48
train loss is 0.000518
average val loss: 0.000329, accuracy: 0.0198
average test loss: 0.000404, accuracy: 0.0216
case acc: 0.018616661
case acc: 0.017297627
case acc: 0.02536929
case acc: 0.023872081
case acc: 0.023170138
case acc: 0.021257728
top acc: 0.0146 ::: bot acc: 0.0315
top acc: 0.0143 ::: bot acc: 0.0287
top acc: 0.0238 ::: bot acc: 0.0422
top acc: 0.0367 ::: bot acc: 0.0297
top acc: 0.0342 ::: bot acc: 0.0284
top acc: 0.0315 ::: bot acc: 0.0260
current epoch: 49
train loss is 0.000529
average val loss: 0.000358, accuracy: 0.0209
average test loss: 0.000436, accuracy: 0.0228
case acc: 0.020657074
case acc: 0.01943565
case acc: 0.027692618
case acc: 0.023996657
case acc: 0.023737041
case acc: 0.021480788
top acc: 0.0113 ::: bot acc: 0.0361
top acc: 0.0091 ::: bot acc: 0.0346
top acc: 0.0187 ::: bot acc: 0.0482
top acc: 0.0326 ::: bot acc: 0.0339
top acc: 0.0310 ::: bot acc: 0.0317
top acc: 0.0290 ::: bot acc: 0.0286
current epoch: 50
train loss is 0.000550
average val loss: 0.000425, accuracy: 0.0231
average test loss: 0.000506, accuracy: 0.0253
case acc: 0.024216928
case acc: 0.023850027
case acc: 0.031518754
case acc: 0.025230696
case acc: 0.024808185
case acc: 0.022130534
top acc: 0.0097 ::: bot acc: 0.0423
top acc: 0.0079 ::: bot acc: 0.0419
top acc: 0.0152 ::: bot acc: 0.0559
top acc: 0.0273 ::: bot acc: 0.0389
top acc: 0.0268 ::: bot acc: 0.0357
top acc: 0.0253 ::: bot acc: 0.0325
LME_Co_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6804 6804 6804
1.7082474 -0.6288155 0.12137239 -0.15229516
Validation: 762 762 762
Testing: 750 750 750
pre-processing time: 0.00019860267639160156
the split date is 2011-01-01
train dropout: 0.6 test dropout: 0.15
net initializing with time: 0.0021424293518066406
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012884
average val loss: 0.005433, accuracy: 0.0966
average test loss: 0.004804, accuracy: 0.0906
case acc: 0.13061327
case acc: 0.093308076
case acc: 0.083242364
case acc: 0.07729143
case acc: 0.111600004
case acc: 0.04768832
top acc: 0.1063 ::: bot acc: 0.1562
top acc: 0.1154 ::: bot acc: 0.0700
top acc: 0.0468 ::: bot acc: 0.1199
top acc: 0.0518 ::: bot acc: 0.1038
top acc: 0.0755 ::: bot acc: 0.1477
top acc: 0.0186 ::: bot acc: 0.0762
current epoch: 2
train loss is 0.007976
average val loss: 0.003151, accuracy: 0.0564
average test loss: 0.003222, accuracy: 0.0552
case acc: 0.0319522
case acc: 0.1765223
case acc: 0.028729543
case acc: 0.022708368
case acc: 0.032744132
case acc: 0.038331546
top acc: 0.0105 ::: bot acc: 0.0560
top acc: 0.1981 ::: bot acc: 0.1535
top acc: 0.0522 ::: bot acc: 0.0197
top acc: 0.0412 ::: bot acc: 0.0130
top acc: 0.0144 ::: bot acc: 0.0606
top acc: 0.0656 ::: bot acc: 0.0149
current epoch: 3
train loss is 0.008389
average val loss: 0.008348, accuracy: 0.1078
average test loss: 0.009086, accuracy: 0.1152
case acc: 0.061634533
case acc: 0.2539457
case acc: 0.10770336
case acc: 0.09869781
case acc: 0.05728574
case acc: 0.11197066
top acc: 0.0863 ::: bot acc: 0.0363
top acc: 0.2754 ::: bot acc: 0.2319
top acc: 0.1444 ::: bot acc: 0.0719
top acc: 0.1247 ::: bot acc: 0.0723
top acc: 0.0922 ::: bot acc: 0.0220
top acc: 0.1425 ::: bot acc: 0.0810
current epoch: 4
train loss is 0.010298
average val loss: 0.017082, accuracy: 0.1724
average test loss: 0.018272, accuracy: 0.1798
case acc: 0.13330607
case acc: 0.31190118
case acc: 0.17839913
case acc: 0.16242114
case acc: 0.123479106
case acc: 0.16912502
top acc: 0.1581 ::: bot acc: 0.1074
top acc: 0.3327 ::: bot acc: 0.2902
top acc: 0.2151 ::: bot acc: 0.1425
top acc: 0.1884 ::: bot acc: 0.1356
top acc: 0.1583 ::: bot acc: 0.0886
top acc: 0.1998 ::: bot acc: 0.1390
current epoch: 5
train loss is 0.012955
average val loss: 0.012998, accuracy: 0.1485
average test loss: 0.014022, accuracy: 0.1558
case acc: 0.1146915
case acc: 0.27786976
case acc: 0.15713918
case acc: 0.13811225
case acc: 0.10709642
case acc: 0.14004362
top acc: 0.1396 ::: bot acc: 0.0889
top acc: 0.2995 ::: bot acc: 0.2556
top acc: 0.1938 ::: bot acc: 0.1210
top acc: 0.1655 ::: bot acc: 0.1100
top acc: 0.1424 ::: bot acc: 0.0715
top acc: 0.1702 ::: bot acc: 0.1099
current epoch: 6
train loss is 0.012510
average val loss: 0.003345, accuracy: 0.0607
average test loss: 0.003727, accuracy: 0.0668
case acc: 0.030315023
case acc: 0.17393611
case acc: 0.06553271
case acc: 0.04702555
case acc: 0.033768635
case acc: 0.05038285
top acc: 0.0524 ::: bot acc: 0.0091
top acc: 0.1950 ::: bot acc: 0.1526
top acc: 0.1007 ::: bot acc: 0.0312
top acc: 0.0734 ::: bot acc: 0.0205
top acc: 0.0630 ::: bot acc: 0.0104
top acc: 0.0797 ::: bot acc: 0.0227
current epoch: 7
train loss is 0.007302
average val loss: 0.001370, accuracy: 0.0418
average test loss: 0.001306, accuracy: 0.0387
case acc: 0.0309941
case acc: 0.09916132
case acc: 0.02567097
case acc: 0.022809422
case acc: 0.030557252
case acc: 0.022998529
top acc: 0.0092 ::: bot acc: 0.0549
top acc: 0.1203 ::: bot acc: 0.0768
top acc: 0.0348 ::: bot acc: 0.0369
top acc: 0.0144 ::: bot acc: 0.0411
top acc: 0.0160 ::: bot acc: 0.0556
top acc: 0.0215 ::: bot acc: 0.0388
current epoch: 8
train loss is 0.003288
average val loss: 0.001178, accuracy: 0.0366
average test loss: 0.001190, accuracy: 0.0357
case acc: 0.022333933
case acc: 0.098221354
case acc: 0.026342565
case acc: 0.019413002
case acc: 0.02591775
case acc: 0.021883061
top acc: 0.0082 ::: bot acc: 0.0426
top acc: 0.1190 ::: bot acc: 0.0763
top acc: 0.0419 ::: bot acc: 0.0308
top acc: 0.0236 ::: bot acc: 0.0300
top acc: 0.0339 ::: bot acc: 0.0369
top acc: 0.0342 ::: bot acc: 0.0253
current epoch: 9
train loss is 0.002435
average val loss: 0.001298, accuracy: 0.0364
average test loss: 0.001444, accuracy: 0.0399
case acc: 0.018602084
case acc: 0.106757745
case acc: 0.030818041
case acc: 0.023117283
case acc: 0.03100565
case acc: 0.029118324
top acc: 0.0272 ::: bot acc: 0.0229
top acc: 0.1277 ::: bot acc: 0.0846
top acc: 0.0569 ::: bot acc: 0.0170
top acc: 0.0416 ::: bot acc: 0.0130
top acc: 0.0566 ::: bot acc: 0.0152
top acc: 0.0532 ::: bot acc: 0.0116
current epoch: 10
train loss is 0.002286
average val loss: 0.001208, accuracy: 0.0356
average test loss: 0.001382, accuracy: 0.0398
case acc: 0.019207876
case acc: 0.10126626
case acc: 0.030626493
case acc: 0.023789486
case acc: 0.033270642
case acc: 0.030596893
top acc: 0.0306 ::: bot acc: 0.0199
top acc: 0.1222 ::: bot acc: 0.0794
top acc: 0.0561 ::: bot acc: 0.0166
top acc: 0.0432 ::: bot acc: 0.0114
top acc: 0.0615 ::: bot acc: 0.0117
top acc: 0.0558 ::: bot acc: 0.0115
current epoch: 11
train loss is 0.002103
average val loss: 0.001012, accuracy: 0.0330
average test loss: 0.001143, accuracy: 0.0365
case acc: 0.01839257
case acc: 0.089136414
case acc: 0.028780263
case acc: 0.022364777
case acc: 0.032524876
case acc: 0.028088465
top acc: 0.0273 ::: bot acc: 0.0219
top acc: 0.1106 ::: bot acc: 0.0668
top acc: 0.0509 ::: bot acc: 0.0225
top acc: 0.0399 ::: bot acc: 0.0143
top acc: 0.0602 ::: bot acc: 0.0124
top acc: 0.0513 ::: bot acc: 0.0121
current epoch: 12
train loss is 0.001806
average val loss: 0.000995, accuracy: 0.0333
average test loss: 0.001145, accuracy: 0.0373
case acc: 0.019506652
case acc: 0.08538909
case acc: 0.02905515
case acc: 0.023745166
case acc: 0.035631076
case acc: 0.030552398
top acc: 0.0320 ::: bot acc: 0.0177
top acc: 0.1065 ::: bot acc: 0.0633
top acc: 0.0525 ::: bot acc: 0.0202
top acc: 0.0429 ::: bot acc: 0.0119
top acc: 0.0660 ::: bot acc: 0.0103
top acc: 0.0557 ::: bot acc: 0.0110
current epoch: 13
train loss is 0.001622
average val loss: 0.000949, accuracy: 0.0328
average test loss: 0.001103, accuracy: 0.0371
case acc: 0.020211875
case acc: 0.08031748
case acc: 0.029154295
case acc: 0.024716824
case acc: 0.036952056
case acc: 0.031310562
top acc: 0.0350 ::: bot acc: 0.0147
top acc: 0.1014 ::: bot acc: 0.0583
top acc: 0.0528 ::: bot acc: 0.0202
top acc: 0.0448 ::: bot acc: 0.0115
top acc: 0.0681 ::: bot acc: 0.0100
top acc: 0.0566 ::: bot acc: 0.0113
current epoch: 14
train loss is 0.001451
average val loss: 0.000865, accuracy: 0.0316
average test loss: 0.001013, accuracy: 0.0359
case acc: 0.020684052
case acc: 0.07377598
case acc: 0.028599702
case acc: 0.024135873
case acc: 0.03686359
case acc: 0.031364027
top acc: 0.0354 ::: bot acc: 0.0145
top acc: 0.0951 ::: bot acc: 0.0519
top acc: 0.0512 ::: bot acc: 0.0209
top acc: 0.0439 ::: bot acc: 0.0111
top acc: 0.0681 ::: bot acc: 0.0098
top acc: 0.0571 ::: bot acc: 0.0112
current epoch: 15
train loss is 0.001300
average val loss: 0.000804, accuracy: 0.0306
average test loss: 0.000943, accuracy: 0.0349
case acc: 0.020935569
case acc: 0.06821211
case acc: 0.02819084
case acc: 0.02443244
case acc: 0.036571287
case acc: 0.030878609
top acc: 0.0365 ::: bot acc: 0.0140
top acc: 0.0894 ::: bot acc: 0.0460
top acc: 0.0507 ::: bot acc: 0.0216
top acc: 0.0444 ::: bot acc: 0.0111
top acc: 0.0680 ::: bot acc: 0.0097
top acc: 0.0555 ::: bot acc: 0.0116
current epoch: 16
train loss is 0.001201
average val loss: 0.000767, accuracy: 0.0301
average test loss: 0.000910, accuracy: 0.0345
case acc: 0.02136672
case acc: 0.06420763
case acc: 0.028564591
case acc: 0.025084157
case acc: 0.03633748
case acc: 0.03129369
top acc: 0.0378 ::: bot acc: 0.0119
top acc: 0.0850 ::: bot acc: 0.0424
top acc: 0.0510 ::: bot acc: 0.0217
top acc: 0.0461 ::: bot acc: 0.0104
top acc: 0.0676 ::: bot acc: 0.0095
top acc: 0.0569 ::: bot acc: 0.0114
current epoch: 17
train loss is 0.001107
average val loss: 0.000764, accuracy: 0.0302
average test loss: 0.000911, accuracy: 0.0348
case acc: 0.022828672
case acc: 0.06172822
case acc: 0.029183423
case acc: 0.026078703
case acc: 0.037370175
case acc: 0.031848706
top acc: 0.0407 ::: bot acc: 0.0109
top acc: 0.0825 ::: bot acc: 0.0399
top acc: 0.0532 ::: bot acc: 0.0195
top acc: 0.0475 ::: bot acc: 0.0101
top acc: 0.0684 ::: bot acc: 0.0096
top acc: 0.0576 ::: bot acc: 0.0112
current epoch: 18
train loss is 0.001049
average val loss: 0.000719, accuracy: 0.0294
average test loss: 0.000860, accuracy: 0.0339
case acc: 0.022927836
case acc: 0.057629637
case acc: 0.029330825
case acc: 0.026070619
case acc: 0.036309063
case acc: 0.03122136
top acc: 0.0410 ::: bot acc: 0.0104
top acc: 0.0788 ::: bot acc: 0.0356
top acc: 0.0533 ::: bot acc: 0.0197
top acc: 0.0476 ::: bot acc: 0.0098
top acc: 0.0669 ::: bot acc: 0.0099
top acc: 0.0567 ::: bot acc: 0.0111
current epoch: 19
train loss is 0.000977
average val loss: 0.000720, accuracy: 0.0295
average test loss: 0.000869, accuracy: 0.0343
case acc: 0.023920113
case acc: 0.05625245
case acc: 0.029855551
case acc: 0.02725098
case acc: 0.03652125
case acc: 0.031787273
top acc: 0.0432 ::: bot acc: 0.0091
top acc: 0.0772 ::: bot acc: 0.0344
top acc: 0.0549 ::: bot acc: 0.0180
top acc: 0.0491 ::: bot acc: 0.0098
top acc: 0.0675 ::: bot acc: 0.0091
top acc: 0.0568 ::: bot acc: 0.0113
current epoch: 20
train loss is 0.000943
average val loss: 0.000722, accuracy: 0.0297
average test loss: 0.000878, accuracy: 0.0347
case acc: 0.025145272
case acc: 0.054405484
case acc: 0.03093672
case acc: 0.02824847
case acc: 0.03708532
case acc: 0.03222197
top acc: 0.0454 ::: bot acc: 0.0084
top acc: 0.0755 ::: bot acc: 0.0328
top acc: 0.0573 ::: bot acc: 0.0161
top acc: 0.0507 ::: bot acc: 0.0099
top acc: 0.0682 ::: bot acc: 0.0100
top acc: 0.0577 ::: bot acc: 0.0117
current epoch: 21
train loss is 0.000908
average val loss: 0.000701, accuracy: 0.0293
average test loss: 0.000857, accuracy: 0.0343
case acc: 0.025848616
case acc: 0.05196983
case acc: 0.03129105
case acc: 0.028394343
case acc: 0.036519554
case acc: 0.0316236
top acc: 0.0467 ::: bot acc: 0.0083
top acc: 0.0728 ::: bot acc: 0.0302
top acc: 0.0584 ::: bot acc: 0.0156
top acc: 0.0510 ::: bot acc: 0.0097
top acc: 0.0671 ::: bot acc: 0.0104
top acc: 0.0570 ::: bot acc: 0.0111
current epoch: 22
train loss is 0.000882
average val loss: 0.000683, accuracy: 0.0289
average test loss: 0.000832, accuracy: 0.0337
case acc: 0.026169354
case acc: 0.04963789
case acc: 0.03147111
case acc: 0.02845069
case acc: 0.035874628
case acc: 0.030865885
top acc: 0.0469 ::: bot acc: 0.0082
top acc: 0.0707 ::: bot acc: 0.0279
top acc: 0.0588 ::: bot acc: 0.0148
top acc: 0.0511 ::: bot acc: 0.0097
top acc: 0.0662 ::: bot acc: 0.0104
top acc: 0.0564 ::: bot acc: 0.0107
current epoch: 23
train loss is 0.000845
average val loss: 0.000743, accuracy: 0.0306
average test loss: 0.000918, accuracy: 0.0359
case acc: 0.029218966
case acc: 0.051113028
case acc: 0.034004092
case acc: 0.030869417
case acc: 0.037395474
case acc: 0.032611866
top acc: 0.0510 ::: bot acc: 0.0089
top acc: 0.0721 ::: bot acc: 0.0293
top acc: 0.0634 ::: bot acc: 0.0130
top acc: 0.0545 ::: bot acc: 0.0099
top acc: 0.0686 ::: bot acc: 0.0099
top acc: 0.0584 ::: bot acc: 0.0112
current epoch: 24
train loss is 0.000824
average val loss: 0.000728, accuracy: 0.0302
average test loss: 0.000903, accuracy: 0.0356
case acc: 0.029700613
case acc: 0.049401272
case acc: 0.03468971
case acc: 0.031096019
case acc: 0.03675447
case acc: 0.0319489
top acc: 0.0518 ::: bot acc: 0.0088
top acc: 0.0703 ::: bot acc: 0.0280
top acc: 0.0641 ::: bot acc: 0.0130
top acc: 0.0549 ::: bot acc: 0.0100
top acc: 0.0676 ::: bot acc: 0.0097
top acc: 0.0575 ::: bot acc: 0.0115
current epoch: 25
train loss is 0.000819
average val loss: 0.000771, accuracy: 0.0314
average test loss: 0.000965, accuracy: 0.0370
case acc: 0.031786397
case acc: 0.04997826
case acc: 0.036657743
case acc: 0.032974727
case acc: 0.03784522
case acc: 0.03294812
top acc: 0.0545 ::: bot acc: 0.0101
top acc: 0.0710 ::: bot acc: 0.0280
top acc: 0.0675 ::: bot acc: 0.0129
top acc: 0.0573 ::: bot acc: 0.0108
top acc: 0.0695 ::: bot acc: 0.0097
top acc: 0.0591 ::: bot acc: 0.0116
current epoch: 26
train loss is 0.000822
average val loss: 0.000806, accuracy: 0.0324
average test loss: 0.001006, accuracy: 0.0380
case acc: 0.033624
case acc: 0.050201584
case acc: 0.038134612
case acc: 0.034342878
case acc: 0.03853985
case acc: 0.033407092
top acc: 0.0563 ::: bot acc: 0.0113
top acc: 0.0711 ::: bot acc: 0.0285
top acc: 0.0698 ::: bot acc: 0.0125
top acc: 0.0591 ::: bot acc: 0.0117
top acc: 0.0702 ::: bot acc: 0.0100
top acc: 0.0597 ::: bot acc: 0.0113
current epoch: 27
train loss is 0.000810
average val loss: 0.000897, accuracy: 0.0347
average test loss: 0.001123, accuracy: 0.0406
case acc: 0.036899086
case acc: 0.052475203
case acc: 0.041261084
case acc: 0.036902744
case acc: 0.040790934
case acc: 0.034985747
top acc: 0.0605 ::: bot acc: 0.0131
top acc: 0.0732 ::: bot acc: 0.0308
top acc: 0.0743 ::: bot acc: 0.0127
top acc: 0.0623 ::: bot acc: 0.0126
top acc: 0.0731 ::: bot acc: 0.0108
top acc: 0.0619 ::: bot acc: 0.0120
current epoch: 28
train loss is 0.000834
average val loss: 0.000863, accuracy: 0.0338
average test loss: 0.001081, accuracy: 0.0397
case acc: 0.036475565
case acc: 0.05059028
case acc: 0.041411847
case acc: 0.036384597
case acc: 0.03954851
case acc: 0.03390265
top acc: 0.0598 ::: bot acc: 0.0133
top acc: 0.0715 ::: bot acc: 0.0288
top acc: 0.0745 ::: bot acc: 0.0129
top acc: 0.0615 ::: bot acc: 0.0124
top acc: 0.0717 ::: bot acc: 0.0101
top acc: 0.0602 ::: bot acc: 0.0121
current epoch: 29
train loss is 0.000827
average val loss: 0.000965, accuracy: 0.0364
average test loss: 0.001207, accuracy: 0.0424
case acc: 0.039757777
case acc: 0.052957684
case acc: 0.04478672
case acc: 0.039597407
case acc: 0.04158844
case acc: 0.035746012
top acc: 0.0636 ::: bot acc: 0.0154
top acc: 0.0740 ::: bot acc: 0.0310
top acc: 0.0787 ::: bot acc: 0.0144
top acc: 0.0651 ::: bot acc: 0.0147
top acc: 0.0746 ::: bot acc: 0.0105
top acc: 0.0628 ::: bot acc: 0.0123
current epoch: 30
train loss is 0.000859
average val loss: 0.001032, accuracy: 0.0381
average test loss: 0.001294, accuracy: 0.0442
case acc: 0.041877277
case acc: 0.054179464
case acc: 0.047523595
case acc: 0.041627917
case acc: 0.0432038
case acc: 0.036839582
top acc: 0.0660 ::: bot acc: 0.0174
top acc: 0.0752 ::: bot acc: 0.0327
top acc: 0.0820 ::: bot acc: 0.0160
top acc: 0.0676 ::: bot acc: 0.0163
top acc: 0.0765 ::: bot acc: 0.0113
top acc: 0.0644 ::: bot acc: 0.0126
current epoch: 31
train loss is 0.000855
average val loss: 0.001098, accuracy: 0.0395
average test loss: 0.001374, accuracy: 0.0458
case acc: 0.043682616
case acc: 0.05528365
case acc: 0.05011608
case acc: 0.04343056
case acc: 0.044583708
case acc: 0.037945047
top acc: 0.0676 ::: bot acc: 0.0191
top acc: 0.0761 ::: bot acc: 0.0336
top acc: 0.0848 ::: bot acc: 0.0180
top acc: 0.0697 ::: bot acc: 0.0175
top acc: 0.0783 ::: bot acc: 0.0119
top acc: 0.0655 ::: bot acc: 0.0134
current epoch: 32
train loss is 0.000852
average val loss: 0.001158, accuracy: 0.0409
average test loss: 0.001441, accuracy: 0.0472
case acc: 0.04538076
case acc: 0.05577421
case acc: 0.05208546
case acc: 0.045109354
case acc: 0.04636335
case acc: 0.038472712
top acc: 0.0697 ::: bot acc: 0.0203
top acc: 0.0764 ::: bot acc: 0.0339
top acc: 0.0874 ::: bot acc: 0.0194
top acc: 0.0714 ::: bot acc: 0.0189
top acc: 0.0800 ::: bot acc: 0.0132
top acc: 0.0661 ::: bot acc: 0.0141
current epoch: 33
train loss is 0.000876
average val loss: 0.001226, accuracy: 0.0424
average test loss: 0.001527, accuracy: 0.0488
case acc: 0.04700598
case acc: 0.05686503
case acc: 0.054218058
case acc: 0.047028393
case acc: 0.048321262
case acc: 0.039655395
top acc: 0.0716 ::: bot acc: 0.0218
top acc: 0.0777 ::: bot acc: 0.0351
top acc: 0.0894 ::: bot acc: 0.0211
top acc: 0.0733 ::: bot acc: 0.0206
top acc: 0.0825 ::: bot acc: 0.0147
top acc: 0.0675 ::: bot acc: 0.0146
current epoch: 34
train loss is 0.000883
average val loss: 0.001191, accuracy: 0.0417
average test loss: 0.001485, accuracy: 0.0480
case acc: 0.046202075
case acc: 0.05496006
case acc: 0.054055218
case acc: 0.046343442
case acc: 0.04798915
case acc: 0.03870766
top acc: 0.0704 ::: bot acc: 0.0215
top acc: 0.0760 ::: bot acc: 0.0330
top acc: 0.0893 ::: bot acc: 0.0210
top acc: 0.0729 ::: bot acc: 0.0199
top acc: 0.0820 ::: bot acc: 0.0145
top acc: 0.0663 ::: bot acc: 0.0139
current epoch: 35
train loss is 0.000832
average val loss: 0.001160, accuracy: 0.0410
average test loss: 0.001447, accuracy: 0.0473
case acc: 0.04524649
case acc: 0.053228203
case acc: 0.05366035
case acc: 0.045581754
case acc: 0.048183013
case acc: 0.037850197
top acc: 0.0695 ::: bot acc: 0.0203
top acc: 0.0744 ::: bot acc: 0.0315
top acc: 0.0887 ::: bot acc: 0.0206
top acc: 0.0717 ::: bot acc: 0.0195
top acc: 0.0821 ::: bot acc: 0.0145
top acc: 0.0654 ::: bot acc: 0.0133
current epoch: 36
train loss is 0.000822
average val loss: 0.001081, accuracy: 0.0392
average test loss: 0.001361, accuracy: 0.0455
case acc: 0.043026164
case acc: 0.049912933
case acc: 0.05207587
case acc: 0.044508442
case acc: 0.04730331
case acc: 0.0364218
top acc: 0.0673 ::: bot acc: 0.0183
top acc: 0.0709 ::: bot acc: 0.0284
top acc: 0.0873 ::: bot acc: 0.0193
top acc: 0.0707 ::: bot acc: 0.0186
top acc: 0.0811 ::: bot acc: 0.0142
top acc: 0.0637 ::: bot acc: 0.0126
current epoch: 37
train loss is 0.000796
average val loss: 0.001026, accuracy: 0.0379
average test loss: 0.001300, accuracy: 0.0443
case acc: 0.041467078
case acc: 0.04739587
case acc: 0.05059876
case acc: 0.043581344
case acc: 0.046713833
case acc: 0.035840753
top acc: 0.0656 ::: bot acc: 0.0170
top acc: 0.0684 ::: bot acc: 0.0257
top acc: 0.0856 ::: bot acc: 0.0181
top acc: 0.0699 ::: bot acc: 0.0178
top acc: 0.0808 ::: bot acc: 0.0133
top acc: 0.0627 ::: bot acc: 0.0123
current epoch: 38
train loss is 0.000740
average val loss: 0.000903, accuracy: 0.0349
average test loss: 0.001146, accuracy: 0.0411
case acc: 0.037718724
case acc: 0.042601474
case acc: 0.047409106
case acc: 0.040500365
case acc: 0.044499207
case acc: 0.03358684
top acc: 0.0614 ::: bot acc: 0.0139
top acc: 0.0630 ::: bot acc: 0.0215
top acc: 0.0817 ::: bot acc: 0.0161
top acc: 0.0662 ::: bot acc: 0.0154
top acc: 0.0780 ::: bot acc: 0.0121
top acc: 0.0599 ::: bot acc: 0.0116
current epoch: 39
train loss is 0.000689
average val loss: 0.000805, accuracy: 0.0324
average test loss: 0.001030, accuracy: 0.0385
case acc: 0.034630958
case acc: 0.038346432
case acc: 0.044589605
case acc: 0.03850901
case acc: 0.042867284
case acc: 0.031814564
top acc: 0.0577 ::: bot acc: 0.0120
top acc: 0.0589 ::: bot acc: 0.0177
top acc: 0.0784 ::: bot acc: 0.0145
top acc: 0.0640 ::: bot acc: 0.0138
top acc: 0.0760 ::: bot acc: 0.0113
top acc: 0.0573 ::: bot acc: 0.0115
current epoch: 40
train loss is 0.000639
average val loss: 0.000665, accuracy: 0.0286
average test loss: 0.000848, accuracy: 0.0342
case acc: 0.029794747
case acc: 0.03252208
case acc: 0.04006303
case acc: 0.03442665
case acc: 0.039414767
case acc: 0.028758561
top acc: 0.0521 ::: bot acc: 0.0092
top acc: 0.0521 ::: bot acc: 0.0136
top acc: 0.0726 ::: bot acc: 0.0125
top acc: 0.0590 ::: bot acc: 0.0118
top acc: 0.0714 ::: bot acc: 0.0102
top acc: 0.0529 ::: bot acc: 0.0113
current epoch: 41
train loss is 0.000590
average val loss: 0.000590, accuracy: 0.0264
average test loss: 0.000749, accuracy: 0.0317
case acc: 0.027028993
case acc: 0.02862692
case acc: 0.03747078
case acc: 0.032176614
case acc: 0.03753215
case acc: 0.027353754
top acc: 0.0482 ::: bot acc: 0.0085
top acc: 0.0476 ::: bot acc: 0.0111
top acc: 0.0686 ::: bot acc: 0.0127
top acc: 0.0563 ::: bot acc: 0.0106
top acc: 0.0689 ::: bot acc: 0.0099
top acc: 0.0504 ::: bot acc: 0.0119
current epoch: 42
train loss is 0.000560
average val loss: 0.000491, accuracy: 0.0233
average test loss: 0.000607, accuracy: 0.0280
case acc: 0.023202939
case acc: 0.023254171
case acc: 0.033290908
case acc: 0.028414069
case acc: 0.034515336
case acc: 0.025270509
top acc: 0.0419 ::: bot acc: 0.0097
top acc: 0.0405 ::: bot acc: 0.0089
top acc: 0.0621 ::: bot acc: 0.0135
top acc: 0.0514 ::: bot acc: 0.0096
top acc: 0.0638 ::: bot acc: 0.0107
top acc: 0.0462 ::: bot acc: 0.0143
current epoch: 43
train loss is 0.000527
average val loss: 0.000430, accuracy: 0.0216
average test loss: 0.000507, accuracy: 0.0253
case acc: 0.020730335
case acc: 0.019129032
case acc: 0.030312186
case acc: 0.025677122
case acc: 0.03209572
case acc: 0.023637623
top acc: 0.0362 ::: bot acc: 0.0136
top acc: 0.0341 ::: bot acc: 0.0098
top acc: 0.0561 ::: bot acc: 0.0170
top acc: 0.0467 ::: bot acc: 0.0104
top acc: 0.0593 ::: bot acc: 0.0128
top acc: 0.0422 ::: bot acc: 0.0175
current epoch: 44
train loss is 0.000505
average val loss: 0.000394, accuracy: 0.0208
average test loss: 0.000428, accuracy: 0.0231
case acc: 0.019022342
case acc: 0.016370844
case acc: 0.027873697
case acc: 0.023020256
case acc: 0.029818425
case acc: 0.022379475
top acc: 0.0302 ::: bot acc: 0.0198
top acc: 0.0269 ::: bot acc: 0.0157
top acc: 0.0492 ::: bot acc: 0.0236
top acc: 0.0415 ::: bot acc: 0.0132
top acc: 0.0542 ::: bot acc: 0.0163
top acc: 0.0378 ::: bot acc: 0.0221
current epoch: 45
train loss is 0.000499
average val loss: 0.000393, accuracy: 0.0212
average test loss: 0.000383, accuracy: 0.0218
case acc: 0.01829464
case acc: 0.015439812
case acc: 0.02639015
case acc: 0.021065807
case acc: 0.02793751
case acc: 0.021689842
top acc: 0.0242 ::: bot acc: 0.0258
top acc: 0.0203 ::: bot acc: 0.0222
top acc: 0.0426 ::: bot acc: 0.0298
top acc: 0.0363 ::: bot acc: 0.0176
top acc: 0.0490 ::: bot acc: 0.0208
top acc: 0.0337 ::: bot acc: 0.0258
current epoch: 46
train loss is 0.000518
average val loss: 0.000409, accuracy: 0.0219
average test loss: 0.000375, accuracy: 0.0215
case acc: 0.018447395
case acc: 0.016066456
case acc: 0.0260303
case acc: 0.02013471
case acc: 0.026986672
case acc: 0.021432096
top acc: 0.0200 ::: bot acc: 0.0297
top acc: 0.0153 ::: bot acc: 0.0276
top acc: 0.0377 ::: bot acc: 0.0349
top acc: 0.0329 ::: bot acc: 0.0210
top acc: 0.0462 ::: bot acc: 0.0235
top acc: 0.0316 ::: bot acc: 0.0279
current epoch: 47
train loss is 0.000530
average val loss: 0.000457, accuracy: 0.0237
average test loss: 0.000385, accuracy: 0.0219
case acc: 0.01945956
case acc: 0.01803123
case acc: 0.02672358
case acc: 0.01939676
case acc: 0.02636946
case acc: 0.021406634
top acc: 0.0144 ::: bot acc: 0.0353
top acc: 0.0093 ::: bot acc: 0.0339
top acc: 0.0309 ::: bot acc: 0.0420
top acc: 0.0282 ::: bot acc: 0.0260
top acc: 0.0424 ::: bot acc: 0.0278
top acc: 0.0284 ::: bot acc: 0.0314
current epoch: 48
train loss is 0.000571
average val loss: 0.000557, accuracy: 0.0269
average test loss: 0.000436, accuracy: 0.0235
case acc: 0.022210054
case acc: 0.02258124
case acc: 0.029052498
case acc: 0.019581055
case acc: 0.02576694
case acc: 0.021997703
top acc: 0.0091 ::: bot acc: 0.0421
top acc: 0.0068 ::: bot acc: 0.0419
top acc: 0.0225 ::: bot acc: 0.0503
top acc: 0.0221 ::: bot acc: 0.0320
top acc: 0.0374 ::: bot acc: 0.0328
top acc: 0.0238 ::: bot acc: 0.0358
current epoch: 49
train loss is 0.000635
average val loss: 0.000658, accuracy: 0.0299
average test loss: 0.000503, accuracy: 0.0256
case acc: 0.025005206
case acc: 0.027350772
case acc: 0.032231394
case acc: 0.020777976
case acc: 0.025555193
case acc: 0.022766203
top acc: 0.0074 ::: bot acc: 0.0470
top acc: 0.0084 ::: bot acc: 0.0480
top acc: 0.0181 ::: bot acc: 0.0573
top acc: 0.0181 ::: bot acc: 0.0361
top acc: 0.0342 ::: bot acc: 0.0358
top acc: 0.0218 ::: bot acc: 0.0380
current epoch: 50
train loss is 0.000700
average val loss: 0.000925, accuracy: 0.0367
average test loss: 0.000696, accuracy: 0.0309
case acc: 0.032131568
case acc: 0.03778587
case acc: 0.039496187
case acc: 0.02487642
case acc: 0.026215298
case acc: 0.025088714
top acc: 0.0097 ::: bot acc: 0.0565
top acc: 0.0170 ::: bot acc: 0.0594
top acc: 0.0159 ::: bot acc: 0.0691
top acc: 0.0128 ::: bot acc: 0.0450
top acc: 0.0263 ::: bot acc: 0.0435
top acc: 0.0163 ::: bot acc: 0.0444
LME_Co_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6786 6786 6786
1.7082474 -0.6288155 0.12137239 -0.15229516
Validation: 756 756 756
Testing: 768 768 768
pre-processing time: 0.000213623046875
the split date is 2011-07-01
train dropout: 0.6 test dropout: 0.15
net initializing with time: 0.0022878646850585938
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012768
average val loss: 0.004953, accuracy: 0.0924
average test loss: 0.004962, accuracy: 0.0931
case acc: 0.13107702
case acc: 0.09016903
case acc: 0.089347765
case acc: 0.07999186
case acc: 0.11669976
case acc: 0.051128514
top acc: 0.1047 ::: bot acc: 0.1554
top acc: 0.1114 ::: bot acc: 0.0703
top acc: 0.0558 ::: bot acc: 0.1215
top acc: 0.0618 ::: bot acc: 0.0995
top acc: 0.0904 ::: bot acc: 0.1460
top acc: 0.0253 ::: bot acc: 0.0790
current epoch: 2
train loss is 0.007765
average val loss: 0.003095, accuracy: 0.0546
average test loss: 0.003035, accuracy: 0.0537
case acc: 0.037107855
case acc: 0.17096049
case acc: 0.025760084
case acc: 0.017577263
case acc: 0.034333125
case acc: 0.036412444
top acc: 0.0187 ::: bot acc: 0.0560
top acc: 0.1916 ::: bot acc: 0.1505
top acc: 0.0427 ::: bot acc: 0.0257
top acc: 0.0280 ::: bot acc: 0.0137
top acc: 0.0126 ::: bot acc: 0.0612
top acc: 0.0635 ::: bot acc: 0.0141
current epoch: 3
train loss is 0.008000
average val loss: 0.007964, accuracy: 0.1053
average test loss: 0.007910, accuracy: 0.1052
case acc: 0.05416928
case acc: 0.24323982
case acc: 0.09466331
case acc: 0.088051945
case acc: 0.047799576
case acc: 0.10300435
top acc: 0.0801 ::: bot acc: 0.0298
top acc: 0.2640 ::: bot acc: 0.2237
top acc: 0.1288 ::: bot acc: 0.0617
top acc: 0.1060 ::: bot acc: 0.0688
top acc: 0.0736 ::: bot acc: 0.0214
top acc: 0.1350 ::: bot acc: 0.0725
current epoch: 4
train loss is 0.009914
average val loss: 0.016651, accuracy: 0.1705
average test loss: 0.016569, accuracy: 0.1702
case acc: 0.12684038
case acc: 0.3018334
case acc: 0.16604644
case acc: 0.15232113
case acc: 0.114138566
case acc: 0.16003034
top acc: 0.1529 ::: bot acc: 0.1037
top acc: 0.3222 ::: bot acc: 0.2823
top acc: 0.2011 ::: bot acc: 0.1319
top acc: 0.1709 ::: bot acc: 0.1322
top acc: 0.1422 ::: bot acc: 0.0839
top acc: 0.1919 ::: bot acc: 0.1285
current epoch: 5
train loss is 0.012574
average val loss: 0.015662, accuracy: 0.1662
average test loss: 0.015570, accuracy: 0.1659
case acc: 0.12798262
case acc: 0.28789502
case acc: 0.1650523
case acc: 0.14740431
case acc: 0.11676181
case acc: 0.150136
top acc: 0.1538 ::: bot acc: 0.1047
top acc: 0.3080 ::: bot acc: 0.2688
top acc: 0.1996 ::: bot acc: 0.1315
top acc: 0.1656 ::: bot acc: 0.1273
top acc: 0.1448 ::: bot acc: 0.0868
top acc: 0.1826 ::: bot acc: 0.1183
current epoch: 6
train loss is 0.013033
average val loss: 0.005175, accuracy: 0.0845
average test loss: 0.005117, accuracy: 0.0845
case acc: 0.050588153
case acc: 0.1933037
case acc: 0.082241915
case acc: 0.06487489
case acc: 0.04745708
case acc: 0.06859384
top acc: 0.0763 ::: bot acc: 0.0273
top acc: 0.2143 ::: bot acc: 0.1731
top acc: 0.1172 ::: bot acc: 0.0484
top acc: 0.0831 ::: bot acc: 0.0453
top acc: 0.0739 ::: bot acc: 0.0208
top acc: 0.1005 ::: bot acc: 0.0379
current epoch: 7
train loss is 0.008108
average val loss: 0.001436, accuracy: 0.0383
average test loss: 0.001387, accuracy: 0.0367
case acc: 0.023970913
case acc: 0.11059454
case acc: 0.025880888
case acc: 0.014004477
case acc: 0.02204562
case acc: 0.023422467
top acc: 0.0152 ::: bot acc: 0.0383
top acc: 0.1312 ::: bot acc: 0.0912
top acc: 0.0433 ::: bot acc: 0.0251
top acc: 0.0149 ::: bot acc: 0.0234
top acc: 0.0175 ::: bot acc: 0.0402
top acc: 0.0338 ::: bot acc: 0.0298
current epoch: 8
train loss is 0.003684
average val loss: 0.001150, accuracy: 0.0358
average test loss: 0.001090, accuracy: 0.0341
case acc: 0.026208047
case acc: 0.09251136
case acc: 0.025300754
case acc: 0.0154249
case acc: 0.021601234
case acc: 0.023385106
top acc: 0.0141 ::: bot acc: 0.0424
top acc: 0.1128 ::: bot acc: 0.0730
top acc: 0.0335 ::: bot acc: 0.0359
top acc: 0.0098 ::: bot acc: 0.0289
top acc: 0.0206 ::: bot acc: 0.0375
top acc: 0.0303 ::: bot acc: 0.0333
current epoch: 9
train loss is 0.002366
average val loss: 0.001366, accuracy: 0.0385
average test loss: 0.001301, accuracy: 0.0366
case acc: 0.017293222
case acc: 0.10302618
case acc: 0.027576018
case acc: 0.017095163
case acc: 0.026111111
case acc: 0.028351177
top acc: 0.0288 ::: bot acc: 0.0198
top acc: 0.1237 ::: bot acc: 0.0837
top acc: 0.0500 ::: bot acc: 0.0194
top acc: 0.0285 ::: bot acc: 0.0109
top acc: 0.0453 ::: bot acc: 0.0140
top acc: 0.0521 ::: bot acc: 0.0138
current epoch: 10
train loss is 0.002265
average val loss: 0.001327, accuracy: 0.0388
average test loss: 0.001280, accuracy: 0.0373
case acc: 0.017860735
case acc: 0.09879029
case acc: 0.027814101
case acc: 0.019260645
case acc: 0.02954201
case acc: 0.030498197
top acc: 0.0334 ::: bot acc: 0.0159
top acc: 0.1190 ::: bot acc: 0.0794
top acc: 0.0511 ::: bot acc: 0.0178
top acc: 0.0323 ::: bot acc: 0.0101
top acc: 0.0518 ::: bot acc: 0.0120
top acc: 0.0560 ::: bot acc: 0.0128
current epoch: 11
train loss is 0.002069
average val loss: 0.001144, accuracy: 0.0365
average test loss: 0.001093, accuracy: 0.0348
case acc: 0.01745141
case acc: 0.08850267
case acc: 0.026539791
case acc: 0.017885756
case acc: 0.029518746
case acc: 0.02901084
top acc: 0.0317 ::: bot acc: 0.0167
top acc: 0.1093 ::: bot acc: 0.0691
top acc: 0.0470 ::: bot acc: 0.0220
top acc: 0.0302 ::: bot acc: 0.0103
top acc: 0.0512 ::: bot acc: 0.0122
top acc: 0.0534 ::: bot acc: 0.0129
current epoch: 12
train loss is 0.001796
average val loss: 0.001062, accuracy: 0.0357
average test loss: 0.001004, accuracy: 0.0340
case acc: 0.018004838
case acc: 0.08157128
case acc: 0.026191002
case acc: 0.0180409
case acc: 0.030775838
case acc: 0.029385889
top acc: 0.0337 ::: bot acc: 0.0158
top acc: 0.1019 ::: bot acc: 0.0625
top acc: 0.0454 ::: bot acc: 0.0233
top acc: 0.0306 ::: bot acc: 0.0103
top acc: 0.0532 ::: bot acc: 0.0117
top acc: 0.0538 ::: bot acc: 0.0128
current epoch: 13
train loss is 0.001653
average val loss: 0.000982, accuracy: 0.0347
average test loss: 0.000926, accuracy: 0.0331
case acc: 0.01812772
case acc: 0.075151995
case acc: 0.02610543
case acc: 0.018217452
case acc: 0.031514984
case acc: 0.029413462
top acc: 0.0348 ::: bot acc: 0.0137
top acc: 0.0956 ::: bot acc: 0.0557
top acc: 0.0443 ::: bot acc: 0.0246
top acc: 0.0309 ::: bot acc: 0.0105
top acc: 0.0542 ::: bot acc: 0.0120
top acc: 0.0543 ::: bot acc: 0.0127
current epoch: 14
train loss is 0.001399
average val loss: 0.000995, accuracy: 0.0353
average test loss: 0.000944, accuracy: 0.0341
case acc: 0.019519698
case acc: 0.07215367
case acc: 0.026600594
case acc: 0.020273399
case acc: 0.034197573
case acc: 0.031837374
top acc: 0.0391 ::: bot acc: 0.0101
top acc: 0.0928 ::: bot acc: 0.0526
top acc: 0.0469 ::: bot acc: 0.0219
top acc: 0.0341 ::: bot acc: 0.0102
top acc: 0.0582 ::: bot acc: 0.0123
top acc: 0.0578 ::: bot acc: 0.0125
current epoch: 15
train loss is 0.001331
average val loss: 0.000910, accuracy: 0.0340
average test loss: 0.000861, accuracy: 0.0328
case acc: 0.019795448
case acc: 0.066249005
case acc: 0.026447488
case acc: 0.019700361
case acc: 0.033542514
case acc: 0.03095815
top acc: 0.0393 ::: bot acc: 0.0101
top acc: 0.0863 ::: bot acc: 0.0471
top acc: 0.0460 ::: bot acc: 0.0235
top acc: 0.0333 ::: bot acc: 0.0092
top acc: 0.0574 ::: bot acc: 0.0118
top acc: 0.0563 ::: bot acc: 0.0127
current epoch: 16
train loss is 0.001188
average val loss: 0.000872, accuracy: 0.0336
average test loss: 0.000823, accuracy: 0.0323
case acc: 0.02055917
case acc: 0.061999235
case acc: 0.026535604
case acc: 0.020420995
case acc: 0.033400312
case acc: 0.030941239
top acc: 0.0412 ::: bot acc: 0.0095
top acc: 0.0822 ::: bot acc: 0.0427
top acc: 0.0462 ::: bot acc: 0.0230
top acc: 0.0340 ::: bot acc: 0.0095
top acc: 0.0571 ::: bot acc: 0.0121
top acc: 0.0568 ::: bot acc: 0.0124
current epoch: 17
train loss is 0.001118
average val loss: 0.000821, accuracy: 0.0327
average test loss: 0.000776, accuracy: 0.0315
case acc: 0.02073872
case acc: 0.05762329
case acc: 0.026445717
case acc: 0.020490622
case acc: 0.033095084
case acc: 0.030441623
top acc: 0.0418 ::: bot acc: 0.0087
top acc: 0.0781 ::: bot acc: 0.0383
top acc: 0.0457 ::: bot acc: 0.0235
top acc: 0.0345 ::: bot acc: 0.0094
top acc: 0.0566 ::: bot acc: 0.0119
top acc: 0.0562 ::: bot acc: 0.0119
current epoch: 18
train loss is 0.001054
average val loss: 0.000755, accuracy: 0.0315
average test loss: 0.000708, accuracy: 0.0301
case acc: 0.020775728
case acc: 0.052288115
case acc: 0.026214985
case acc: 0.019789223
case acc: 0.03163105
case acc: 0.029620018
top acc: 0.0416 ::: bot acc: 0.0089
top acc: 0.0728 ::: bot acc: 0.0327
top acc: 0.0449 ::: bot acc: 0.0241
top acc: 0.0338 ::: bot acc: 0.0091
top acc: 0.0545 ::: bot acc: 0.0118
top acc: 0.0549 ::: bot acc: 0.0122
current epoch: 19
train loss is 0.000961
average val loss: 0.000771, accuracy: 0.0319
average test loss: 0.000726, accuracy: 0.0308
case acc: 0.02219981
case acc: 0.051222246
case acc: 0.026579674
case acc: 0.021753863
case acc: 0.032058872
case acc: 0.03089259
top acc: 0.0439 ::: bot acc: 0.0085
top acc: 0.0716 ::: bot acc: 0.0318
top acc: 0.0469 ::: bot acc: 0.0217
top acc: 0.0362 ::: bot acc: 0.0094
top acc: 0.0553 ::: bot acc: 0.0120
top acc: 0.0566 ::: bot acc: 0.0126
current epoch: 20
train loss is 0.000930
average val loss: 0.000804, accuracy: 0.0328
average test loss: 0.000761, accuracy: 0.0318
case acc: 0.024227938
case acc: 0.05062377
case acc: 0.027738184
case acc: 0.02366206
case acc: 0.03292573
case acc: 0.031919114
top acc: 0.0470 ::: bot acc: 0.0082
top acc: 0.0711 ::: bot acc: 0.0313
top acc: 0.0508 ::: bot acc: 0.0184
top acc: 0.0390 ::: bot acc: 0.0099
top acc: 0.0567 ::: bot acc: 0.0119
top acc: 0.0576 ::: bot acc: 0.0125
current epoch: 21
train loss is 0.000892
average val loss: 0.000788, accuracy: 0.0325
average test loss: 0.000745, accuracy: 0.0316
case acc: 0.025140358
case acc: 0.048448835
case acc: 0.028182823
case acc: 0.024246698
case acc: 0.032207888
case acc: 0.03166171
top acc: 0.0484 ::: bot acc: 0.0082
top acc: 0.0686 ::: bot acc: 0.0292
top acc: 0.0512 ::: bot acc: 0.0177
top acc: 0.0394 ::: bot acc: 0.0103
top acc: 0.0558 ::: bot acc: 0.0113
top acc: 0.0577 ::: bot acc: 0.0125
current epoch: 22
train loss is 0.000870
average val loss: 0.000809, accuracy: 0.0330
average test loss: 0.000763, accuracy: 0.0323
case acc: 0.026782
case acc: 0.047582828
case acc: 0.029343497
case acc: 0.025519617
case acc: 0.03256519
case acc: 0.03174636
top acc: 0.0503 ::: bot acc: 0.0089
top acc: 0.0680 ::: bot acc: 0.0279
top acc: 0.0538 ::: bot acc: 0.0161
top acc: 0.0413 ::: bot acc: 0.0109
top acc: 0.0558 ::: bot acc: 0.0121
top acc: 0.0577 ::: bot acc: 0.0124
current epoch: 23
train loss is 0.000841
average val loss: 0.000881, accuracy: 0.0348
average test loss: 0.000832, accuracy: 0.0341
case acc: 0.029738998
case acc: 0.0488907
case acc: 0.031554792
case acc: 0.027809547
case acc: 0.033853598
case acc: 0.03292881
top acc: 0.0542 ::: bot acc: 0.0101
top acc: 0.0694 ::: bot acc: 0.0293
top acc: 0.0582 ::: bot acc: 0.0142
top acc: 0.0437 ::: bot acc: 0.0122
top acc: 0.0580 ::: bot acc: 0.0120
top acc: 0.0593 ::: bot acc: 0.0125
current epoch: 24
train loss is 0.000846
average val loss: 0.000912, accuracy: 0.0356
average test loss: 0.000867, accuracy: 0.0350
case acc: 0.031360395
case acc: 0.048993193
case acc: 0.032989573
case acc: 0.02931888
case acc: 0.03434322
case acc: 0.033265095
top acc: 0.0565 ::: bot acc: 0.0111
top acc: 0.0693 ::: bot acc: 0.0296
top acc: 0.0608 ::: bot acc: 0.0135
top acc: 0.0454 ::: bot acc: 0.0135
top acc: 0.0584 ::: bot acc: 0.0122
top acc: 0.0599 ::: bot acc: 0.0125
current epoch: 25
train loss is 0.000834
average val loss: 0.001017, accuracy: 0.0381
average test loss: 0.000970, accuracy: 0.0375
case acc: 0.034790095
case acc: 0.050941207
case acc: 0.035583016
case acc: 0.0324188
case acc: 0.03639795
case acc: 0.035019867
top acc: 0.0601 ::: bot acc: 0.0133
top acc: 0.0714 ::: bot acc: 0.0318
top acc: 0.0655 ::: bot acc: 0.0117
top acc: 0.0490 ::: bot acc: 0.0155
top acc: 0.0612 ::: bot acc: 0.0125
top acc: 0.0625 ::: bot acc: 0.0134
current epoch: 26
train loss is 0.000850
average val loss: 0.001034, accuracy: 0.0385
average test loss: 0.000985, accuracy: 0.0379
case acc: 0.035966292
case acc: 0.050403096
case acc: 0.036731213
case acc: 0.033229742
case acc: 0.0363367
case acc: 0.034924474
top acc: 0.0616 ::: bot acc: 0.0142
top acc: 0.0707 ::: bot acc: 0.0311
top acc: 0.0673 ::: bot acc: 0.0117
top acc: 0.0500 ::: bot acc: 0.0164
top acc: 0.0611 ::: bot acc: 0.0128
top acc: 0.0619 ::: bot acc: 0.0132
current epoch: 27
train loss is 0.000842
average val loss: 0.001113, accuracy: 0.0402
average test loss: 0.001066, accuracy: 0.0398
case acc: 0.038554072
case acc: 0.051394492
case acc: 0.0388232
case acc: 0.03566491
case acc: 0.038403723
case acc: 0.035943586
top acc: 0.0642 ::: bot acc: 0.0164
top acc: 0.0718 ::: bot acc: 0.0321
top acc: 0.0702 ::: bot acc: 0.0114
top acc: 0.0524 ::: bot acc: 0.0185
top acc: 0.0638 ::: bot acc: 0.0137
top acc: 0.0635 ::: bot acc: 0.0136
current epoch: 28
train loss is 0.000865
average val loss: 0.001183, accuracy: 0.0417
average test loss: 0.001140, accuracy: 0.0415
case acc: 0.040933467
case acc: 0.05225226
case acc: 0.040936597
case acc: 0.03766547
case acc: 0.04001329
case acc: 0.03731195
top acc: 0.0668 ::: bot acc: 0.0185
top acc: 0.0725 ::: bot acc: 0.0329
top acc: 0.0732 ::: bot acc: 0.0118
top acc: 0.0548 ::: bot acc: 0.0202
top acc: 0.0656 ::: bot acc: 0.0147
top acc: 0.0651 ::: bot acc: 0.0144
current epoch: 29
train loss is 0.000839
average val loss: 0.001155, accuracy: 0.0411
average test loss: 0.001109, accuracy: 0.0408
case acc: 0.040558428
case acc: 0.050698787
case acc: 0.04097476
case acc: 0.037147753
case acc: 0.039436456
case acc: 0.036060896
top acc: 0.0664 ::: bot acc: 0.0184
top acc: 0.0709 ::: bot acc: 0.0316
top acc: 0.0735 ::: bot acc: 0.0116
top acc: 0.0543 ::: bot acc: 0.0195
top acc: 0.0650 ::: bot acc: 0.0143
top acc: 0.0635 ::: bot acc: 0.0138
current epoch: 30
train loss is 0.000836
average val loss: 0.001202, accuracy: 0.0422
average test loss: 0.001158, accuracy: 0.0419
case acc: 0.041997433
case acc: 0.050915204
case acc: 0.042574167
case acc: 0.0384488
case acc: 0.0407821
case acc: 0.036474973
top acc: 0.0680 ::: bot acc: 0.0196
top acc: 0.0713 ::: bot acc: 0.0315
top acc: 0.0758 ::: bot acc: 0.0118
top acc: 0.0556 ::: bot acc: 0.0206
top acc: 0.0667 ::: bot acc: 0.0152
top acc: 0.0640 ::: bot acc: 0.0142
current epoch: 31
train loss is 0.000839
average val loss: 0.001214, accuracy: 0.0424
average test loss: 0.001175, accuracy: 0.0423
case acc: 0.042410515
case acc: 0.050614618
case acc: 0.043495394
case acc: 0.03925226
case acc: 0.041185014
case acc: 0.036763795
top acc: 0.0682 ::: bot acc: 0.0201
top acc: 0.0709 ::: bot acc: 0.0313
top acc: 0.0765 ::: bot acc: 0.0123
top acc: 0.0565 ::: bot acc: 0.0214
top acc: 0.0669 ::: bot acc: 0.0158
top acc: 0.0645 ::: bot acc: 0.0143
current epoch: 32
train loss is 0.000809
average val loss: 0.001262, accuracy: 0.0434
average test loss: 0.001215, accuracy: 0.0431
case acc: 0.043397672
case acc: 0.05050389
case acc: 0.044803187
case acc: 0.04052665
case acc: 0.042276867
case acc: 0.03708919
top acc: 0.0693 ::: bot acc: 0.0210
top acc: 0.0710 ::: bot acc: 0.0312
top acc: 0.0788 ::: bot acc: 0.0130
top acc: 0.0576 ::: bot acc: 0.0225
top acc: 0.0685 ::: bot acc: 0.0161
top acc: 0.0648 ::: bot acc: 0.0145
current epoch: 33
train loss is 0.000798
average val loss: 0.001293, accuracy: 0.0441
average test loss: 0.001242, accuracy: 0.0437
case acc: 0.043795034
case acc: 0.050528683
case acc: 0.045765977
case acc: 0.041660473
case acc: 0.04304489
case acc: 0.0376498
top acc: 0.0697 ::: bot acc: 0.0211
top acc: 0.0710 ::: bot acc: 0.0311
top acc: 0.0794 ::: bot acc: 0.0136
top acc: 0.0589 ::: bot acc: 0.0238
top acc: 0.0692 ::: bot acc: 0.0167
top acc: 0.0653 ::: bot acc: 0.0150
current epoch: 34
train loss is 0.000803
average val loss: 0.001235, accuracy: 0.0429
average test loss: 0.001191, accuracy: 0.0426
case acc: 0.04259394
case acc: 0.048255272
case acc: 0.045040153
case acc: 0.040775005
case acc: 0.042385988
case acc: 0.036624946
top acc: 0.0684 ::: bot acc: 0.0203
top acc: 0.0686 ::: bot acc: 0.0288
top acc: 0.0786 ::: bot acc: 0.0132
top acc: 0.0580 ::: bot acc: 0.0228
top acc: 0.0687 ::: bot acc: 0.0163
top acc: 0.0645 ::: bot acc: 0.0139
current epoch: 35
train loss is 0.000799
average val loss: 0.001275, accuracy: 0.0437
average test loss: 0.001226, accuracy: 0.0434
case acc: 0.04318901
case acc: 0.04796606
case acc: 0.04600774
case acc: 0.04175037
case acc: 0.04379357
case acc: 0.037496854
top acc: 0.0694 ::: bot acc: 0.0206
top acc: 0.0683 ::: bot acc: 0.0286
top acc: 0.0802 ::: bot acc: 0.0137
top acc: 0.0590 ::: bot acc: 0.0237
top acc: 0.0699 ::: bot acc: 0.0174
top acc: 0.0652 ::: bot acc: 0.0145
current epoch: 36
train loss is 0.000746
average val loss: 0.001163, accuracy: 0.0413
average test loss: 0.001111, accuracy: 0.0409
case acc: 0.040341757
case acc: 0.04401515
case acc: 0.043864623
case acc: 0.03977216
case acc: 0.04200886
case acc: 0.035301212
top acc: 0.0663 ::: bot acc: 0.0183
top acc: 0.0641 ::: bot acc: 0.0248
top acc: 0.0774 ::: bot acc: 0.0124
top acc: 0.0570 ::: bot acc: 0.0218
top acc: 0.0679 ::: bot acc: 0.0161
top acc: 0.0626 ::: bot acc: 0.0135
current epoch: 37
train loss is 0.000712
average val loss: 0.001079, accuracy: 0.0394
average test loss: 0.001028, accuracy: 0.0389
case acc: 0.038000435
case acc: 0.04082506
case acc: 0.04206522
case acc: 0.037940264
case acc: 0.040490538
case acc: 0.034082342
top acc: 0.0638 ::: bot acc: 0.0162
top acc: 0.0611 ::: bot acc: 0.0213
top acc: 0.0751 ::: bot acc: 0.0117
top acc: 0.0551 ::: bot acc: 0.0201
top acc: 0.0663 ::: bot acc: 0.0149
top acc: 0.0610 ::: bot acc: 0.0129
current epoch: 38
train loss is 0.000674
average val loss: 0.000975, accuracy: 0.0371
average test loss: 0.000925, accuracy: 0.0364
case acc: 0.03511695
case acc: 0.036974926
case acc: 0.04007756
case acc: 0.03566426
case acc: 0.038560055
case acc: 0.03224012
top acc: 0.0608 ::: bot acc: 0.0136
top acc: 0.0571 ::: bot acc: 0.0179
top acc: 0.0724 ::: bot acc: 0.0114
top acc: 0.0526 ::: bot acc: 0.0182
top acc: 0.0639 ::: bot acc: 0.0140
top acc: 0.0586 ::: bot acc: 0.0121
current epoch: 39
train loss is 0.000640
average val loss: 0.000879, accuracy: 0.0349
average test loss: 0.000828, accuracy: 0.0340
case acc: 0.032099143
case acc: 0.033074394
case acc: 0.038174734
case acc: 0.033492208
case acc: 0.036711633
case acc: 0.030590756
top acc: 0.0570 ::: bot acc: 0.0114
top acc: 0.0530 ::: bot acc: 0.0142
top acc: 0.0692 ::: bot acc: 0.0116
top acc: 0.0502 ::: bot acc: 0.0165
top acc: 0.0619 ::: bot acc: 0.0128
top acc: 0.0560 ::: bot acc: 0.0123
current epoch: 40
train loss is 0.000598
average val loss: 0.000795, accuracy: 0.0328
average test loss: 0.000753, accuracy: 0.0320
case acc: 0.02966299
case acc: 0.030042434
case acc: 0.03628394
case acc: 0.031653643
case acc: 0.03512237
case acc: 0.02937417
top acc: 0.0545 ::: bot acc: 0.0101
top acc: 0.0499 ::: bot acc: 0.0113
top acc: 0.0666 ::: bot acc: 0.0116
top acc: 0.0482 ::: bot acc: 0.0153
top acc: 0.0595 ::: bot acc: 0.0124
top acc: 0.0544 ::: bot acc: 0.0124
current epoch: 41
train loss is 0.000585
average val loss: 0.000761, accuracy: 0.0320
average test loss: 0.000716, accuracy: 0.0311
case acc: 0.028549854
case acc: 0.027956856
case acc: 0.035471592
case acc: 0.030892966
case acc: 0.034453966
case acc: 0.029016886
top acc: 0.0530 ::: bot acc: 0.0095
top acc: 0.0476 ::: bot acc: 0.0099
top acc: 0.0650 ::: bot acc: 0.0122
top acc: 0.0475 ::: bot acc: 0.0144
top acc: 0.0587 ::: bot acc: 0.0121
top acc: 0.0535 ::: bot acc: 0.0125
current epoch: 42
train loss is 0.000556
average val loss: 0.000672, accuracy: 0.0296
average test loss: 0.000629, accuracy: 0.0286
case acc: 0.025736714
case acc: 0.024267867
case acc: 0.03341537
case acc: 0.028109299
case acc: 0.03252725
case acc: 0.027439948
top acc: 0.0492 ::: bot acc: 0.0083
top acc: 0.0434 ::: bot acc: 0.0072
top acc: 0.0615 ::: bot acc: 0.0133
top acc: 0.0442 ::: bot acc: 0.0125
top acc: 0.0560 ::: bot acc: 0.0117
top acc: 0.0508 ::: bot acc: 0.0136
current epoch: 43
train loss is 0.000522
average val loss: 0.000613, accuracy: 0.0281
average test loss: 0.000569, accuracy: 0.0268
case acc: 0.023879671
case acc: 0.02176721
case acc: 0.03174977
case acc: 0.026273794
case acc: 0.03096202
case acc: 0.026403945
top acc: 0.0465 ::: bot acc: 0.0084
top acc: 0.0401 ::: bot acc: 0.0063
top acc: 0.0586 ::: bot acc: 0.0142
top acc: 0.0421 ::: bot acc: 0.0113
top acc: 0.0537 ::: bot acc: 0.0115
top acc: 0.0486 ::: bot acc: 0.0149
current epoch: 44
train loss is 0.000509
average val loss: 0.000544, accuracy: 0.0263
average test loss: 0.000503, accuracy: 0.0248
case acc: 0.021333415
case acc: 0.018999878
case acc: 0.029845133
case acc: 0.023987278
case acc: 0.029190756
case acc: 0.025426172
top acc: 0.0426 ::: bot acc: 0.0087
top acc: 0.0360 ::: bot acc: 0.0065
top acc: 0.0548 ::: bot acc: 0.0158
top acc: 0.0392 ::: bot acc: 0.0102
top acc: 0.0511 ::: bot acc: 0.0121
top acc: 0.0461 ::: bot acc: 0.0167
current epoch: 45
train loss is 0.000504
average val loss: 0.000504, accuracy: 0.0252
average test loss: 0.000460, accuracy: 0.0235
case acc: 0.019767832
case acc: 0.01727384
case acc: 0.02837494
case acc: 0.022369657
case acc: 0.02798901
case acc: 0.025034832
top acc: 0.0399 ::: bot acc: 0.0093
top acc: 0.0325 ::: bot acc: 0.0081
top acc: 0.0518 ::: bot acc: 0.0176
top acc: 0.0370 ::: bot acc: 0.0096
top acc: 0.0490 ::: bot acc: 0.0122
top acc: 0.0447 ::: bot acc: 0.0182
current epoch: 46
train loss is 0.000480
average val loss: 0.000447, accuracy: 0.0237
average test loss: 0.000406, accuracy: 0.0217
case acc: 0.018157315
case acc: 0.015199693
case acc: 0.026721697
case acc: 0.019804977
case acc: 0.026207525
case acc: 0.023959424
top acc: 0.0358 ::: bot acc: 0.0130
top acc: 0.0275 ::: bot acc: 0.0118
top acc: 0.0469 ::: bot acc: 0.0221
top acc: 0.0332 ::: bot acc: 0.0094
top acc: 0.0458 ::: bot acc: 0.0135
top acc: 0.0414 ::: bot acc: 0.0211
current epoch: 47
train loss is 0.000480
average val loss: 0.000408, accuracy: 0.0226
average test loss: 0.000365, accuracy: 0.0204
case acc: 0.017242258
case acc: 0.01420916
case acc: 0.025601048
case acc: 0.017598715
case acc: 0.024196098
case acc: 0.023525711
top acc: 0.0308 ::: bot acc: 0.0175
top acc: 0.0227 ::: bot acc: 0.0166
top acc: 0.0422 ::: bot acc: 0.0267
top acc: 0.0298 ::: bot acc: 0.0099
top acc: 0.0419 ::: bot acc: 0.0153
top acc: 0.0388 ::: bot acc: 0.0240
current epoch: 48
train loss is 0.000484
average val loss: 0.000382, accuracy: 0.0219
average test loss: 0.000342, accuracy: 0.0198
case acc: 0.017528132
case acc: 0.014559007
case acc: 0.024976254
case acc: 0.015671916
case acc: 0.022952586
case acc: 0.023222065
top acc: 0.0261 ::: bot acc: 0.0221
top acc: 0.0172 ::: bot acc: 0.0224
top acc: 0.0363 ::: bot acc: 0.0325
top acc: 0.0259 ::: bot acc: 0.0125
top acc: 0.0381 ::: bot acc: 0.0192
top acc: 0.0353 ::: bot acc: 0.0276
current epoch: 49
train loss is 0.000502
average val loss: 0.000381, accuracy: 0.0218
average test loss: 0.000341, accuracy: 0.0199
case acc: 0.017967772
case acc: 0.015241979
case acc: 0.025056615
case acc: 0.014963804
case acc: 0.022838999
case acc: 0.023163825
top acc: 0.0240 ::: bot acc: 0.0244
top acc: 0.0144 ::: bot acc: 0.0250
top acc: 0.0333 ::: bot acc: 0.0357
top acc: 0.0240 ::: bot acc: 0.0136
top acc: 0.0376 ::: bot acc: 0.0196
top acc: 0.0350 ::: bot acc: 0.0277
current epoch: 50
train loss is 0.000509
average val loss: 0.000385, accuracy: 0.0220
average test loss: 0.000348, accuracy: 0.0203
case acc: 0.01905376
case acc: 0.017592093
case acc: 0.02579702
case acc: 0.014161641
case acc: 0.022212166
case acc: 0.023035897
top acc: 0.0201 ::: bot acc: 0.0283
top acc: 0.0117 ::: bot acc: 0.0299
top acc: 0.0280 ::: bot acc: 0.0411
top acc: 0.0208 ::: bot acc: 0.0168
top acc: 0.0357 ::: bot acc: 0.0219
top acc: 0.0332 ::: bot acc: 0.0293
