
		{"drop_out": 0.6, "drop_out_mc": 0.05, "repeat_mc": 50, "hidden": 30, "embedding_size": 5, "batch": 512, "lag": 4}
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
LME_Co_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5376 5376 5376
1.7082474 -0.6288155 0.24786325 -0.22413088
Validation: 600 600 600
Testing: 774 774 774
pre-processing time: 0.00817251205444336
the split date is 2010-07-01
net initializing with time: 0.21709036827087402
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.122204
average val loss: 0.126463, accuracy: 0.1270
average test loss: 0.140188, accuracy: 0.1421
case acc: 0.20283651
case acc: 0.04280199
case acc: 0.16201632
case acc: 0.15006983
case acc: 0.17732626
case acc: 0.117443234
top acc: 0.1819 ::: bot acc: 0.2250
top acc: 0.0783 ::: bot acc: 0.0125
top acc: 0.1164 ::: bot acc: 0.2129
top acc: 0.1170 ::: bot acc: 0.1847
top acc: 0.1445 ::: bot acc: 0.2104
top acc: 0.0882 ::: bot acc: 0.1509
current epoch: 2
train loss is 0.116520
average val loss: 0.062123, accuracy: 0.0621
average test loss: 0.058654, accuracy: 0.0595
case acc: 0.047236945
case acc: 0.17807989
case acc: 0.034769118
case acc: 0.025305381
case acc: 0.036935173
case acc: 0.034496468
top acc: 0.0265 ::: bot acc: 0.0696
top acc: 0.2194 ::: bot acc: 0.1354
top acc: 0.0389 ::: bot acc: 0.0562
top acc: 0.0327 ::: bot acc: 0.0351
top acc: 0.0091 ::: bot acc: 0.0676
top acc: 0.0546 ::: bot acc: 0.0182
current epoch: 3
train loss is 0.118699
average val loss: 0.151132, accuracy: 0.1512
average test loss: 0.132290, accuracy: 0.1322
case acc: 0.073967144
case acc: 0.28277463
case acc: 0.11283248
case acc: 0.11262916
case acc: 0.078707375
case acc: 0.13228488
top acc: 0.0938 ::: bot acc: 0.0533
top acc: 0.3239 ::: bot acc: 0.2400
top acc: 0.1574 ::: bot acc: 0.0631
top acc: 0.1458 ::: bot acc: 0.0783
top acc: 0.1124 ::: bot acc: 0.0452
top acc: 0.1613 ::: bot acc: 0.0979
current epoch: 4
train loss is 0.129216
average val loss: 0.094124, accuracy: 0.0952
average test loss: 0.077849, accuracy: 0.0760
case acc: 0.024158046
case acc: 0.21536075
case acc: 0.061146323
case acc: 0.05419107
case acc: 0.031681392
case acc: 0.06959157
top acc: 0.0398 ::: bot acc: 0.0119
top acc: 0.2564 ::: bot acc: 0.1727
top acc: 0.1011 ::: bot acc: 0.0207
top acc: 0.0867 ::: bot acc: 0.0213
top acc: 0.0592 ::: bot acc: 0.0108
top acc: 0.0985 ::: bot acc: 0.0352
current epoch: 5
train loss is 0.106572
average val loss: 0.055514, accuracy: 0.0551
average test loss: 0.050041, accuracy: 0.0506
case acc: 0.031094337
case acc: 0.15325639
case acc: 0.035006043
case acc: 0.025313813
case acc: 0.028652536
case acc: 0.030141992
top acc: 0.0118 ::: bot acc: 0.0530
top acc: 0.1945 ::: bot acc: 0.1104
top acc: 0.0471 ::: bot acc: 0.0473
top acc: 0.0339 ::: bot acc: 0.0335
top acc: 0.0164 ::: bot acc: 0.0513
top acc: 0.0461 ::: bot acc: 0.0217
current epoch: 6
train loss is 0.088275
average val loss: 0.051561, accuracy: 0.0514
average test loss: 0.046775, accuracy: 0.0478
case acc: 0.032684427
case acc: 0.13834928
case acc: 0.034419
case acc: 0.025369948
case acc: 0.027040666
case acc: 0.028819844
top acc: 0.0130 ::: bot acc: 0.0548
top acc: 0.1796 ::: bot acc: 0.0954
top acc: 0.0388 ::: bot acc: 0.0554
top acc: 0.0293 ::: bot acc: 0.0380
top acc: 0.0223 ::: bot acc: 0.0454
top acc: 0.0433 ::: bot acc: 0.0233
current epoch: 7
train loss is 0.077490
average val loss: 0.055603, accuracy: 0.0561
average test loss: 0.046585, accuracy: 0.0471
case acc: 0.021319656
case acc: 0.13907795
case acc: 0.034884665
case acc: 0.025812449
case acc: 0.026171375
case acc: 0.035196695
top acc: 0.0072 ::: bot acc: 0.0408
top acc: 0.1803 ::: bot acc: 0.0961
top acc: 0.0467 ::: bot acc: 0.0473
top acc: 0.0400 ::: bot acc: 0.0273
top acc: 0.0425 ::: bot acc: 0.0254
top acc: 0.0548 ::: bot acc: 0.0191
current epoch: 8
train loss is 0.073660
average val loss: 0.063920, accuracy: 0.0646
average test loss: 0.050836, accuracy: 0.0503
case acc: 0.015911482
case acc: 0.14269884
case acc: 0.037957035
case acc: 0.029130854
case acc: 0.033559468
case acc: 0.042596187
top acc: 0.0198 ::: bot acc: 0.0235
top acc: 0.1841 ::: bot acc: 0.0997
top acc: 0.0592 ::: bot acc: 0.0351
top acc: 0.0526 ::: bot acc: 0.0147
top acc: 0.0629 ::: bot acc: 0.0096
top acc: 0.0666 ::: bot acc: 0.0177
current epoch: 9
train loss is 0.074770
average val loss: 0.070800, accuracy: 0.0715
average test loss: 0.055591, accuracy: 0.0550
case acc: 0.0199227
case acc: 0.14322896
case acc: 0.042220358
case acc: 0.03351
case acc: 0.043368667
case acc: 0.04760443
top acc: 0.0328 ::: bot acc: 0.0129
top acc: 0.1848 ::: bot acc: 0.1002
top acc: 0.0691 ::: bot acc: 0.0281
top acc: 0.0617 ::: bot acc: 0.0098
top acc: 0.0764 ::: bot acc: 0.0121
top acc: 0.0736 ::: bot acc: 0.0188
current epoch: 10
train loss is 0.073374
average val loss: 0.078368, accuracy: 0.0788
average test loss: 0.061693, accuracy: 0.0613
case acc: 0.02859752
case acc: 0.14445592
case acc: 0.04770734
case acc: 0.03970371
case acc: 0.054765962
case acc: 0.052777797
top acc: 0.0455 ::: bot acc: 0.0134
top acc: 0.1861 ::: bot acc: 0.1014
top acc: 0.0799 ::: bot acc: 0.0232
top acc: 0.0706 ::: bot acc: 0.0105
top acc: 0.0886 ::: bot acc: 0.0217
top acc: 0.0800 ::: bot acc: 0.0215
current epoch: 11
train loss is 0.072876
average val loss: 0.078617, accuracy: 0.0789
average test loss: 0.061720, accuracy: 0.0615
case acc: 0.031958766
case acc: 0.1377613
case acc: 0.048959
case acc: 0.04031912
case acc: 0.058356967
case acc: 0.051634237
top acc: 0.0495 ::: bot acc: 0.0156
top acc: 0.1795 ::: bot acc: 0.0946
top acc: 0.0820 ::: bot acc: 0.0226
top acc: 0.0714 ::: bot acc: 0.0107
top acc: 0.0924 ::: bot acc: 0.0250
top acc: 0.0786 ::: bot acc: 0.0207
current epoch: 12
train loss is 0.070515
average val loss: 0.074632, accuracy: 0.0749
average test loss: 0.058004, accuracy: 0.0579
case acc: 0.03073899
case acc: 0.12649515
case acc: 0.04711105
case acc: 0.037724603
case acc: 0.057421226
case acc: 0.04773477
top acc: 0.0480 ::: bot acc: 0.0148
top acc: 0.1681 ::: bot acc: 0.0834
top acc: 0.0788 ::: bot acc: 0.0236
top acc: 0.0680 ::: bot acc: 0.0098
top acc: 0.0914 ::: bot acc: 0.0241
top acc: 0.0737 ::: bot acc: 0.0187
current epoch: 13
train loss is 0.066905
average val loss: 0.064599, accuracy: 0.0649
average test loss: 0.049275, accuracy: 0.0492
case acc: 0.023767352
case acc: 0.10836597
case acc: 0.041614383
case acc: 0.031642444
case acc: 0.049291234
case acc: 0.04053757
top acc: 0.0390 ::: bot acc: 0.0119
top acc: 0.1500 ::: bot acc: 0.0653
top acc: 0.0680 ::: bot acc: 0.0286
top acc: 0.0585 ::: bot acc: 0.0108
top acc: 0.0830 ::: bot acc: 0.0166
top acc: 0.0632 ::: bot acc: 0.0179
current epoch: 14
train loss is 0.062373
average val loss: 0.058535, accuracy: 0.0588
average test loss: 0.044222, accuracy: 0.0442
case acc: 0.020667804
case acc: 0.09454703
case acc: 0.03863243
case acc: 0.02947001
case acc: 0.044600774
case acc: 0.03726297
top acc: 0.0340 ::: bot acc: 0.0126
top acc: 0.1361 ::: bot acc: 0.0515
top acc: 0.0614 ::: bot acc: 0.0330
top acc: 0.0535 ::: bot acc: 0.0142
top acc: 0.0778 ::: bot acc: 0.0130
top acc: 0.0578 ::: bot acc: 0.0186
current epoch: 15
train loss is 0.058778
average val loss: 0.053365, accuracy: 0.0537
average test loss: 0.040087, accuracy: 0.0401
case acc: 0.018358316
case acc: 0.08197187
case acc: 0.0368131
case acc: 0.028225321
case acc: 0.04012058
case acc: 0.034888357
top acc: 0.0297 ::: bot acc: 0.0143
top acc: 0.1235 ::: bot acc: 0.0389
top acc: 0.0558 ::: bot acc: 0.0384
top acc: 0.0498 ::: bot acc: 0.0178
top acc: 0.0722 ::: bot acc: 0.0107
top acc: 0.0537 ::: bot acc: 0.0196
current epoch: 16
train loss is 0.055674
average val loss: 0.049417, accuracy: 0.0497
average test loss: 0.036980, accuracy: 0.0368
case acc: 0.017132478
case acc: 0.07137923
case acc: 0.035743322
case acc: 0.02749626
case acc: 0.036164623
case acc: 0.033148848
top acc: 0.0268 ::: bot acc: 0.0164
top acc: 0.1129 ::: bot acc: 0.0284
top acc: 0.0521 ::: bot acc: 0.0422
top acc: 0.0474 ::: bot acc: 0.0201
top acc: 0.0668 ::: bot acc: 0.0096
top acc: 0.0506 ::: bot acc: 0.0205
current epoch: 17
train loss is 0.052776
average val loss: 0.047066, accuracy: 0.0473
average test loss: 0.035075, accuracy: 0.0348
case acc: 0.016902616
case acc: 0.06365468
case acc: 0.035540204
case acc: 0.027377902
case acc: 0.033370733
case acc: 0.032087747
top acc: 0.0262 ::: bot acc: 0.0169
top acc: 0.1050 ::: bot acc: 0.0210
top acc: 0.0510 ::: bot acc: 0.0434
top acc: 0.0469 ::: bot acc: 0.0207
top acc: 0.0625 ::: bot acc: 0.0099
top acc: 0.0488 ::: bot acc: 0.0211
current epoch: 18
train loss is 0.050982
average val loss: 0.047846, accuracy: 0.0480
average test loss: 0.035298, accuracy: 0.0350
case acc: 0.01823923
case acc: 0.060965493
case acc: 0.036527537
case acc: 0.028447963
case acc: 0.032840032
case acc: 0.032834783
top acc: 0.0295 ::: bot acc: 0.0144
top acc: 0.1022 ::: bot acc: 0.0186
top acc: 0.0546 ::: bot acc: 0.0401
top acc: 0.0502 ::: bot acc: 0.0176
top acc: 0.0615 ::: bot acc: 0.0103
top acc: 0.0505 ::: bot acc: 0.0204
current epoch: 19
train loss is 0.050131
average val loss: 0.045862, accuracy: 0.0459
average test loss: 0.033806, accuracy: 0.0334
case acc: 0.018193765
case acc: 0.05599701
case acc: 0.036602516
case acc: 0.028060915
case acc: 0.030439116
case acc: 0.030933835
top acc: 0.0294 ::: bot acc: 0.0147
top acc: 0.0964 ::: bot acc: 0.0151
top acc: 0.0548 ::: bot acc: 0.0400
top acc: 0.0491 ::: bot acc: 0.0186
top acc: 0.0573 ::: bot acc: 0.0118
top acc: 0.0473 ::: bot acc: 0.0212
current epoch: 20
train loss is 0.049021
average val loss: 0.048305, accuracy: 0.0483
average test loss: 0.035306, accuracy: 0.0348
case acc: 0.021065645
case acc: 0.056860857
case acc: 0.038552
case acc: 0.029436123
case acc: 0.030943064
case acc: 0.031808384
top acc: 0.0347 ::: bot acc: 0.0126
top acc: 0.0974 ::: bot acc: 0.0156
top acc: 0.0610 ::: bot acc: 0.0338
top acc: 0.0533 ::: bot acc: 0.0145
top acc: 0.0583 ::: bot acc: 0.0113
top acc: 0.0489 ::: bot acc: 0.0207
current epoch: 21
train loss is 0.049657
average val loss: 0.054692, accuracy: 0.0547
average test loss: 0.039959, accuracy: 0.0394
case acc: 0.027540972
case acc: 0.06252446
case acc: 0.043382682
case acc: 0.03378988
case acc: 0.03394407
case acc: 0.03517961
top acc: 0.0442 ::: bot acc: 0.0131
top acc: 0.1036 ::: bot acc: 0.0201
top acc: 0.0718 ::: bot acc: 0.0266
top acc: 0.0620 ::: bot acc: 0.0102
top acc: 0.0635 ::: bot acc: 0.0099
top acc: 0.0548 ::: bot acc: 0.0190
current epoch: 22
train loss is 0.050340
average val loss: 0.061519, accuracy: 0.0616
average test loss: 0.045559, accuracy: 0.0450
case acc: 0.035438154
case acc: 0.06944973
case acc: 0.049014512
case acc: 0.039469
case acc: 0.03808115
case acc: 0.038528204
top acc: 0.0534 ::: bot acc: 0.0185
top acc: 0.1108 ::: bot acc: 0.0265
top acc: 0.0825 ::: bot acc: 0.0220
top acc: 0.0704 ::: bot acc: 0.0106
top acc: 0.0695 ::: bot acc: 0.0103
top acc: 0.0602 ::: bot acc: 0.0179
current epoch: 23
train loss is 0.052447
average val loss: 0.071073, accuracy: 0.0712
average test loss: 0.053967, accuracy: 0.0534
case acc: 0.04616397
case acc: 0.0792652
case acc: 0.056958184
case acc: 0.04842061
case acc: 0.045849755
case acc: 0.043945767
top acc: 0.0648 ::: bot acc: 0.0278
top acc: 0.1206 ::: bot acc: 0.0363
top acc: 0.0955 ::: bot acc: 0.0199
top acc: 0.0809 ::: bot acc: 0.0164
top acc: 0.0793 ::: bot acc: 0.0141
top acc: 0.0684 ::: bot acc: 0.0178
current epoch: 24
train loss is 0.055376
average val loss: 0.075318, accuracy: 0.0754
average test loss: 0.057879, accuracy: 0.0574
case acc: 0.0512067
case acc: 0.08223547
case acc: 0.061328903
case acc: 0.0525416
case acc: 0.051268283
case acc: 0.04588332
top acc: 0.0701 ::: bot acc: 0.0324
top acc: 0.1236 ::: bot acc: 0.0393
top acc: 0.1018 ::: bot acc: 0.0207
top acc: 0.0853 ::: bot acc: 0.0201
top acc: 0.0851 ::: bot acc: 0.0185
top acc: 0.0712 ::: bot acc: 0.0182
current epoch: 25
train loss is 0.057019
average val loss: 0.073059, accuracy: 0.0731
average test loss: 0.055748, accuracy: 0.0553
case acc: 0.049330693
case acc: 0.07661666
case acc: 0.05971472
case acc: 0.050252065
case acc: 0.052140266
case acc: 0.043769658
top acc: 0.0682 ::: bot acc: 0.0307
top acc: 0.1180 ::: bot acc: 0.0337
top acc: 0.0995 ::: bot acc: 0.0204
top acc: 0.0828 ::: bot acc: 0.0181
top acc: 0.0861 ::: bot acc: 0.0193
top acc: 0.0682 ::: bot acc: 0.0177
current epoch: 26
train loss is 0.057254
average val loss: 0.066647, accuracy: 0.0666
average test loss: 0.049858, accuracy: 0.0494
case acc: 0.04264402
case acc: 0.06496453
case acc: 0.053995375
case acc: 0.044512395
case acc: 0.050078142
case acc: 0.040267866
top acc: 0.0611 ::: bot acc: 0.0247
top acc: 0.1062 ::: bot acc: 0.0222
top acc: 0.0910 ::: bot acc: 0.0203
top acc: 0.0765 ::: bot acc: 0.0136
top acc: 0.0839 ::: bot acc: 0.0174
top acc: 0.0630 ::: bot acc: 0.0175
current epoch: 27
train loss is 0.055575
average val loss: 0.052501, accuracy: 0.0524
average test loss: 0.037899, accuracy: 0.0376
case acc: 0.027530907
case acc: 0.046714846
case acc: 0.043263946
case acc: 0.033792164
case acc: 0.040882297
case acc: 0.033229534
top acc: 0.0443 ::: bot acc: 0.0129
top acc: 0.0841 ::: bot acc: 0.0117
top acc: 0.0716 ::: bot acc: 0.0268
top acc: 0.0619 ::: bot acc: 0.0105
top acc: 0.0731 ::: bot acc: 0.0113
top acc: 0.0512 ::: bot acc: 0.0199
current epoch: 28
train loss is 0.049456
average val loss: 0.035736, accuracy: 0.0360
average test loss: 0.027547, accuracy: 0.0275
case acc: 0.0158749
case acc: 0.03352446
case acc: 0.034517102
case acc: 0.025907168
case acc: 0.029586788
case acc: 0.02537315
top acc: 0.0207 ::: bot acc: 0.0225
top acc: 0.0552 ::: bot acc: 0.0291
top acc: 0.0450 ::: bot acc: 0.0496
top acc: 0.0414 ::: bot acc: 0.0260
top acc: 0.0552 ::: bot acc: 0.0134
top acc: 0.0344 ::: bot acc: 0.0299
current epoch: 29
train loss is 0.044308
average val loss: 0.027286, accuracy: 0.0274
average test loss: 0.028743, accuracy: 0.0290
case acc: 0.025972098
case acc: 0.032758962
case acc: 0.039356504
case acc: 0.026695173
case acc: 0.02536625
case acc: 0.023753732
top acc: 0.0089 ::: bot acc: 0.0466
top acc: 0.0272 ::: bot acc: 0.0573
top acc: 0.0201 ::: bot acc: 0.0762
top acc: 0.0204 ::: bot acc: 0.0469
top acc: 0.0349 ::: bot acc: 0.0332
top acc: 0.0167 ::: bot acc: 0.0473
current epoch: 30
train loss is 0.044938
average val loss: 0.035182, accuracy: 0.0343
average test loss: 0.046717, accuracy: 0.0465
case acc: 0.053922158
case acc: 0.049538754
case acc: 0.06174626
case acc: 0.04266119
case acc: 0.033354294
case acc: 0.03793037
top acc: 0.0332 ::: bot acc: 0.0763
top acc: 0.0133 ::: bot acc: 0.0899
top acc: 0.0236 ::: bot acc: 0.1080
top acc: 0.0139 ::: bot acc: 0.0742
top acc: 0.0093 ::: bot acc: 0.0618
top acc: 0.0107 ::: bot acc: 0.0723
current epoch: 31
train loss is 0.051663
average val loss: 0.052675, accuracy: 0.0517
average test loss: 0.067968, accuracy: 0.0677
case acc: 0.077493265
case acc: 0.0720346
case acc: 0.084189735
case acc: 0.06249195
case acc: 0.053025905
case acc: 0.057060238
top acc: 0.0568 ::: bot acc: 0.0998
top acc: 0.0310 ::: bot acc: 0.1149
top acc: 0.0406 ::: bot acc: 0.1332
top acc: 0.0296 ::: bot acc: 0.0960
top acc: 0.0188 ::: bot acc: 0.0866
top acc: 0.0281 ::: bot acc: 0.0923
current epoch: 32
train loss is 0.061415
average val loss: 0.050639, accuracy: 0.0496
average test loss: 0.065670, accuracy: 0.0654
case acc: 0.07509656
case acc: 0.06795371
case acc: 0.08179223
case acc: 0.05922284
case acc: 0.054857165
case acc: 0.05333281
top acc: 0.0544 ::: bot acc: 0.0974
top acc: 0.0271 ::: bot acc: 0.1107
top acc: 0.0383 ::: bot acc: 0.1308
top acc: 0.0265 ::: bot acc: 0.0927
top acc: 0.0205 ::: bot acc: 0.0884
top acc: 0.0244 ::: bot acc: 0.0885
current epoch: 33
train loss is 0.062952
average val loss: 0.031762, accuracy: 0.0308
average test loss: 0.041829, accuracy: 0.0415
case acc: 0.047404632
case acc: 0.04115
case acc: 0.05540597
case acc: 0.036150273
case acc: 0.03751168
case acc: 0.031337775
top acc: 0.0268 ::: bot acc: 0.0696
top acc: 0.0119 ::: bot acc: 0.0781
top acc: 0.0201 ::: bot acc: 0.1003
top acc: 0.0114 ::: bot acc: 0.0656
top acc: 0.0084 ::: bot acc: 0.0685
top acc: 0.0076 ::: bot acc: 0.0638
current epoch: 34
train loss is 0.052815
average val loss: 0.027152, accuracy: 0.0266
average test loss: 0.031408, accuracy: 0.0314
case acc: 0.030388957
case acc: 0.032406267
case acc: 0.040616367
case acc: 0.027643781
case acc: 0.031995
case acc: 0.025393981
top acc: 0.0116 ::: bot acc: 0.0516
top acc: 0.0304 ::: bot acc: 0.0547
top acc: 0.0195 ::: bot acc: 0.0784
top acc: 0.0166 ::: bot acc: 0.0502
top acc: 0.0103 ::: bot acc: 0.0592
top acc: 0.0116 ::: bot acc: 0.0529
current epoch: 35
train loss is 0.044832
average val loss: 0.028481, accuracy: 0.0279
average test loss: 0.027653, accuracy: 0.0275
case acc: 0.019954635
case acc: 0.032532148
case acc: 0.034807034
case acc: 0.025084658
case acc: 0.029013904
case acc: 0.023789076
top acc: 0.0074 ::: bot acc: 0.0381
top acc: 0.0480 ::: bot acc: 0.0371
top acc: 0.0333 ::: bot acc: 0.0618
top acc: 0.0269 ::: bot acc: 0.0397
top acc: 0.0160 ::: bot acc: 0.0519
top acc: 0.0181 ::: bot acc: 0.0463
current epoch: 36
train loss is 0.041910
average val loss: 0.035745, accuracy: 0.0352
average test loss: 0.028104, accuracy: 0.0275
case acc: 0.015654622
case acc: 0.03768819
case acc: 0.03591696
case acc: 0.025796518
case acc: 0.025401922
case acc: 0.02439277
top acc: 0.0224 ::: bot acc: 0.0203
top acc: 0.0677 ::: bot acc: 0.0181
top acc: 0.0525 ::: bot acc: 0.0426
top acc: 0.0419 ::: bot acc: 0.0247
top acc: 0.0316 ::: bot acc: 0.0362
top acc: 0.0309 ::: bot acc: 0.0336
current epoch: 37
train loss is 0.042794
average val loss: 0.044889, accuracy: 0.0445
average test loss: 0.032830, accuracy: 0.0321
case acc: 0.022034124
case acc: 0.04477595
case acc: 0.04099307
case acc: 0.029582877
case acc: 0.026796889
case acc: 0.028311282
top acc: 0.0366 ::: bot acc: 0.0120
top acc: 0.0816 ::: bot acc: 0.0115
top acc: 0.0668 ::: bot acc: 0.0298
top acc: 0.0542 ::: bot acc: 0.0129
top acc: 0.0468 ::: bot acc: 0.0210
top acc: 0.0423 ::: bot acc: 0.0236
current epoch: 38
train loss is 0.045784
average val loss: 0.051631, accuracy: 0.0515
average test loss: 0.037404, accuracy: 0.0367
case acc: 0.028391894
case acc: 0.04949952
case acc: 0.045004904
case acc: 0.033815
case acc: 0.030983044
case acc: 0.03241099
top acc: 0.0453 ::: bot acc: 0.0136
top acc: 0.0886 ::: bot acc: 0.0116
top acc: 0.0751 ::: bot acc: 0.0252
top acc: 0.0622 ::: bot acc: 0.0098
top acc: 0.0583 ::: bot acc: 0.0114
top acc: 0.0500 ::: bot acc: 0.0203
current epoch: 39
train loss is 0.047879
average val loss: 0.051137, accuracy: 0.0510
average test loss: 0.036908, accuracy: 0.0363
case acc: 0.02786125
case acc: 0.047027472
case acc: 0.04412866
case acc: 0.03347249
case acc: 0.032869928
case acc: 0.03256022
top acc: 0.0447 ::: bot acc: 0.0133
top acc: 0.0851 ::: bot acc: 0.0113
top acc: 0.0734 ::: bot acc: 0.0261
top acc: 0.0616 ::: bot acc: 0.0100
top acc: 0.0618 ::: bot acc: 0.0102
top acc: 0.0501 ::: bot acc: 0.0204
current epoch: 40
train loss is 0.047504
average val loss: 0.043021, accuracy: 0.0430
average test loss: 0.031252, accuracy: 0.0309
case acc: 0.020531675
case acc: 0.0390038
case acc: 0.038451526
case acc: 0.028735386
case acc: 0.030028084
case acc: 0.028541949
top acc: 0.0341 ::: bot acc: 0.0125
top acc: 0.0707 ::: bot acc: 0.0161
top acc: 0.0609 ::: bot acc: 0.0341
top acc: 0.0520 ::: bot acc: 0.0151
top acc: 0.0564 ::: bot acc: 0.0124
top acc: 0.0425 ::: bot acc: 0.0236
current epoch: 41
train loss is 0.044717
average val loss: 0.033815, accuracy: 0.0339
average test loss: 0.026755, accuracy: 0.0267
case acc: 0.015526524
case acc: 0.03320626
case acc: 0.034490872
case acc: 0.02533349
case acc: 0.026764264
case acc: 0.024754247
top acc: 0.0193 ::: bot acc: 0.0234
top acc: 0.0522 ::: bot acc: 0.0330
top acc: 0.0440 ::: bot acc: 0.0511
top acc: 0.0391 ::: bot acc: 0.0277
top acc: 0.0467 ::: bot acc: 0.0211
top acc: 0.0325 ::: bot acc: 0.0319
current epoch: 42
train loss is 0.041978
average val loss: 0.027132, accuracy: 0.0271
average test loss: 0.028052, accuracy: 0.0282
case acc: 0.023259353
case acc: 0.032622114
case acc: 0.037821174
case acc: 0.02608588
case acc: 0.025494654
case acc: 0.023798428
top acc: 0.0074 ::: bot acc: 0.0431
top acc: 0.0299 ::: bot acc: 0.0554
top acc: 0.0227 ::: bot acc: 0.0727
top acc: 0.0216 ::: bot acc: 0.0451
top acc: 0.0306 ::: bot acc: 0.0372
top acc: 0.0177 ::: bot acc: 0.0468
current epoch: 43
train loss is 0.042560
average val loss: 0.030853, accuracy: 0.0302
average test loss: 0.040576, accuracy: 0.0404
case acc: 0.04440128
case acc: 0.042500827
case acc: 0.0532588
case acc: 0.03705935
case acc: 0.031912126
case acc: 0.03319833
top acc: 0.0239 ::: bot acc: 0.0665
top acc: 0.0116 ::: bot acc: 0.0804
top acc: 0.0191 ::: bot acc: 0.0976
top acc: 0.0114 ::: bot acc: 0.0668
top acc: 0.0104 ::: bot acc: 0.0591
top acc: 0.0080 ::: bot acc: 0.0665
current epoch: 44
train loss is 0.046889
average val loss: 0.042279, accuracy: 0.0415
average test loss: 0.055974, accuracy: 0.0557
case acc: 0.063009724
case acc: 0.057245165
case acc: 0.069664165
case acc: 0.051369984
case acc: 0.045453228
case acc: 0.047284007
top acc: 0.0425 ::: bot acc: 0.0851
top acc: 0.0177 ::: bot acc: 0.0995
top acc: 0.0286 ::: bot acc: 0.1174
top acc: 0.0195 ::: bot acc: 0.0842
top acc: 0.0126 ::: bot acc: 0.0783
top acc: 0.0185 ::: bot acc: 0.0824
current epoch: 45
train loss is 0.052919
average val loss: 0.042759, accuracy: 0.0421
average test loss: 0.056569, accuracy: 0.0563
case acc: 0.06360391
case acc: 0.05631106
case acc: 0.0701752
case acc: 0.05139405
case acc: 0.049061295
case acc: 0.04699731
top acc: 0.0431 ::: bot acc: 0.0857
top acc: 0.0170 ::: bot acc: 0.0984
top acc: 0.0290 ::: bot acc: 0.1180
top acc: 0.0195 ::: bot acc: 0.0843
top acc: 0.0154 ::: bot acc: 0.0823
top acc: 0.0182 ::: bot acc: 0.0820
current epoch: 46
train loss is 0.054604
average val loss: 0.031158, accuracy: 0.0305
average test loss: 0.041071, accuracy: 0.0408
case acc: 0.044830568
case acc: 0.039718807
case acc: 0.05287629
case acc: 0.036405664
case acc: 0.03861533
case acc: 0.032555364
top acc: 0.0243 ::: bot acc: 0.0669
top acc: 0.0134 ::: bot acc: 0.0754
top acc: 0.0189 ::: bot acc: 0.0971
top acc: 0.0111 ::: bot acc: 0.0660
top acc: 0.0087 ::: bot acc: 0.0700
top acc: 0.0077 ::: bot acc: 0.0656
current epoch: 47
train loss is 0.048816
average val loss: 0.027082, accuracy: 0.0266
average test loss: 0.030554, accuracy: 0.0305
case acc: 0.027531555
case acc: 0.0320544
case acc: 0.038960453
case acc: 0.027361035
case acc: 0.031800773
case acc: 0.025392829
top acc: 0.0094 ::: bot acc: 0.0484
top acc: 0.0340 ::: bot acc: 0.0517
top acc: 0.0208 ::: bot acc: 0.0752
top acc: 0.0168 ::: bot acc: 0.0495
top acc: 0.0106 ::: bot acc: 0.0589
top acc: 0.0121 ::: bot acc: 0.0526
current epoch: 48
train loss is 0.042797
average val loss: 0.030140, accuracy: 0.0295
average test loss: 0.027037, accuracy: 0.0268
case acc: 0.01690567
case acc: 0.033729497
case acc: 0.03431141
case acc: 0.024798743
case acc: 0.027558466
case acc: 0.023607524
top acc: 0.0106 ::: bot acc: 0.0318
top acc: 0.0547 ::: bot acc: 0.0309
top acc: 0.0392 ::: bot acc: 0.0559
top acc: 0.0306 ::: bot acc: 0.0355
top acc: 0.0209 ::: bot acc: 0.0471
top acc: 0.0228 ::: bot acc: 0.0419
current epoch: 49
train loss is 0.041078
average val loss: 0.038546, accuracy: 0.0380
average test loss: 0.029279, accuracy: 0.0286
case acc: 0.017153956
case acc: 0.040085245
case acc: 0.03728764
case acc: 0.02644868
case acc: 0.025266374
case acc: 0.02554227
top acc: 0.0275 ::: bot acc: 0.0154
top acc: 0.0731 ::: bot acc: 0.0147
top acc: 0.0572 ::: bot acc: 0.0378
top acc: 0.0450 ::: bot acc: 0.0212
top acc: 0.0367 ::: bot acc: 0.0313
top acc: 0.0352 ::: bot acc: 0.0296
current epoch: 50
train loss is 0.043013
average val loss: 0.047153, accuracy: 0.0468
average test loss: 0.034297, accuracy: 0.0335
case acc: 0.024291124
case acc: 0.04682654
case acc: 0.042365152
case acc: 0.030151216
case acc: 0.027908577
case acc: 0.029747332
top acc: 0.0399 ::: bot acc: 0.0121
top acc: 0.0850 ::: bot acc: 0.0110
top acc: 0.0697 ::: bot acc: 0.0283
top acc: 0.0558 ::: bot acc: 0.0114
top acc: 0.0507 ::: bot acc: 0.0172
top acc: 0.0453 ::: bot acc: 0.0221
LME_Co_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5400 5400 5400
1.7082474 -0.6288155 0.17559324 -0.22413088
Validation: 606 606 606
Testing: 744 744 744
pre-processing time: 0.0002491474151611328
the split date is 2011-01-01
net initializing with time: 0.14450526237487793
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.174861
average val loss: 0.076451, accuracy: 0.0773
average test loss: 0.081493, accuracy: 0.0773
case acc: 0.04507204
case acc: 0.03547679
case acc: 0.045830164
case acc: 0.24472517
case acc: 0.04778341
case acc: 0.045181133
top acc: 0.0860 ::: bot acc: 0.0212
top acc: 0.0114 ::: bot acc: 0.0690
top acc: 0.0731 ::: bot acc: 0.0501
top acc: 0.2746 ::: bot acc: 0.2125
top acc: 0.0223 ::: bot acc: 0.0802
top acc: 0.0251 ::: bot acc: 0.0817
current epoch: 2
train loss is 0.136887
average val loss: 0.100870, accuracy: 0.1001
average test loss: 0.100218, accuracy: 0.0961
case acc: 0.05084215
case acc: 0.09377984
case acc: 0.07987267
case acc: 0.16206375
case acc: 0.09859221
case acc: 0.09161489
top acc: 0.0184 ::: bot acc: 0.0876
top acc: 0.0613 ::: bot acc: 0.1303
top acc: 0.0311 ::: bot acc: 0.1306
top acc: 0.1928 ::: bot acc: 0.1291
top acc: 0.0515 ::: bot acc: 0.1416
top acc: 0.0395 ::: bot acc: 0.1436
current epoch: 3
train loss is 0.125688
average val loss: 0.084770, accuracy: 0.0837
average test loss: 0.085767, accuracy: 0.0820
case acc: 0.042098716
case acc: 0.07073892
case acc: 0.07164596
case acc: 0.17091541
case acc: 0.070558794
case acc: 0.066313505
top acc: 0.0276 ::: bot acc: 0.0700
top acc: 0.0384 ::: bot acc: 0.1065
top acc: 0.0288 ::: bot acc: 0.1200
top acc: 0.2021 ::: bot acc: 0.1374
top acc: 0.0266 ::: bot acc: 0.1119
top acc: 0.0209 ::: bot acc: 0.1146
current epoch: 4
train loss is 0.104190
average val loss: 0.071938, accuracy: 0.0702
average test loss: 0.075234, accuracy: 0.0719
case acc: 0.036812913
case acc: 0.050259605
case acc: 0.06670637
case acc: 0.17587197
case acc: 0.05148952
case acc: 0.05046322
top acc: 0.0389 ::: bot acc: 0.0560
top acc: 0.0197 ::: bot acc: 0.0845
top acc: 0.0296 ::: bot acc: 0.1126
top acc: 0.2075 ::: bot acc: 0.1419
top acc: 0.0190 ::: bot acc: 0.0870
top acc: 0.0210 ::: bot acc: 0.0905
current epoch: 5
train loss is 0.095267
average val loss: 0.064281, accuracy: 0.0624
average test loss: 0.069279, accuracy: 0.0663
case acc: 0.035051588
case acc: 0.03739104
case acc: 0.06586587
case acc: 0.17328914
case acc: 0.042189535
case acc: 0.04396302
top acc: 0.0440 ::: bot acc: 0.0496
top acc: 0.0116 ::: bot acc: 0.0687
top acc: 0.0294 ::: bot acc: 0.1119
top acc: 0.2054 ::: bot acc: 0.1389
top acc: 0.0237 ::: bot acc: 0.0708
top acc: 0.0321 ::: bot acc: 0.0751
current epoch: 6
train loss is 0.090845
average val loss: 0.065136, accuracy: 0.0633
average test loss: 0.069275, accuracy: 0.0661
case acc: 0.037734203
case acc: 0.04092539
case acc: 0.07647004
case acc: 0.15377289
case acc: 0.042710315
case acc: 0.0447634
top acc: 0.0323 ::: bot acc: 0.0610
top acc: 0.0133 ::: bot acc: 0.0727
top acc: 0.0297 ::: bot acc: 0.1278
top acc: 0.1862 ::: bot acc: 0.1191
top acc: 0.0226 ::: bot acc: 0.0720
top acc: 0.0306 ::: bot acc: 0.0768
current epoch: 7
train loss is 0.087781
average val loss: 0.069716, accuracy: 0.0682
average test loss: 0.072172, accuracy: 0.0685
case acc: 0.04429244
case acc: 0.051126353
case acc: 0.09228107
case acc: 0.12984087
case acc: 0.04589891
case acc: 0.04763118
top acc: 0.0199 ::: bot acc: 0.0770
top acc: 0.0205 ::: bot acc: 0.0841
top acc: 0.0365 ::: bot acc: 0.1483
top acc: 0.1625 ::: bot acc: 0.0949
top acc: 0.0197 ::: bot acc: 0.0782
top acc: 0.0255 ::: bot acc: 0.0834
current epoch: 8
train loss is 0.087018
average val loss: 0.080303, accuracy: 0.0792
average test loss: 0.080432, accuracy: 0.0761
case acc: 0.05905283
case acc: 0.07101471
case acc: 0.11672668
case acc: 0.09889286
case acc: 0.055155862
case acc: 0.05567408
top acc: 0.0183 ::: bot acc: 0.0999
top acc: 0.0389 ::: bot acc: 0.1046
top acc: 0.0544 ::: bot acc: 0.1762
top acc: 0.1315 ::: bot acc: 0.0642
top acc: 0.0178 ::: bot acc: 0.0929
top acc: 0.0189 ::: bot acc: 0.0985
current epoch: 9
train loss is 0.092223
average val loss: 0.094165, accuracy: 0.0932
average test loss: 0.092796, accuracy: 0.0879
case acc: 0.08146176
case acc: 0.09314381
case acc: 0.14573908
case acc: 0.065491356
case acc: 0.071009584
case acc: 0.0708254
top acc: 0.0342 ::: bot acc: 0.1255
top acc: 0.0608 ::: bot acc: 0.1267
top acc: 0.0805 ::: bot acc: 0.2068
top acc: 0.0970 ::: bot acc: 0.0332
top acc: 0.0248 ::: bot acc: 0.1133
top acc: 0.0222 ::: bot acc: 0.1194
current epoch: 10
train loss is 0.108370
average val loss: 0.078063, accuracy: 0.0770
average test loss: 0.078235, accuracy: 0.0736
case acc: 0.06351563
case acc: 0.064083025
case acc: 0.1283421
case acc: 0.07573763
case acc: 0.05408929
case acc: 0.05596495
top acc: 0.0205 ::: bot acc: 0.1057
top acc: 0.0319 ::: bot acc: 0.0976
top acc: 0.0645 ::: bot acc: 0.1887
top acc: 0.1079 ::: bot acc: 0.0421
top acc: 0.0167 ::: bot acc: 0.0920
top acc: 0.0189 ::: bot acc: 0.0987
current epoch: 11
train loss is 0.101851
average val loss: 0.058495, accuracy: 0.0576
average test loss: 0.062657, accuracy: 0.0589
case acc: 0.043429777
case acc: 0.029418008
case acc: 0.09999344
case acc: 0.09596706
case acc: 0.04028637
case acc: 0.04413486
top acc: 0.0204 ::: bot acc: 0.0759
top acc: 0.0129 ::: bot acc: 0.0552
top acc: 0.0419 ::: bot acc: 0.1575
top acc: 0.1287 ::: bot acc: 0.0612
top acc: 0.0278 ::: bot acc: 0.0660
top acc: 0.0328 ::: bot acc: 0.0742
current epoch: 12
train loss is 0.082798
average val loss: 0.053930, accuracy: 0.0536
average test loss: 0.058874, accuracy: 0.0557
case acc: 0.04005984
case acc: 0.024422381
case acc: 0.09143552
case acc: 0.094949305
case acc: 0.03935575
case acc: 0.043774102
top acc: 0.0250 ::: bot acc: 0.0687
top acc: 0.0302 ::: bot acc: 0.0364
top acc: 0.0359 ::: bot acc: 0.1476
top acc: 0.1277 ::: bot acc: 0.0602
top acc: 0.0302 ::: bot acc: 0.0634
top acc: 0.0340 ::: bot acc: 0.0730
current epoch: 13
train loss is 0.070473
average val loss: 0.052123, accuracy: 0.0520
average test loss: 0.057119, accuracy: 0.0542
case acc: 0.038994413
case acc: 0.024797602
case acc: 0.08722994
case acc: 0.09005715
case acc: 0.03963259
case acc: 0.044311166
top acc: 0.0269 ::: bot acc: 0.0661
top acc: 0.0390 ::: bot acc: 0.0276
top acc: 0.0333 ::: bot acc: 0.1427
top acc: 0.1228 ::: bot acc: 0.0555
top acc: 0.0295 ::: bot acc: 0.0643
top acc: 0.0327 ::: bot acc: 0.0744
current epoch: 14
train loss is 0.066678
average val loss: 0.049086, accuracy: 0.0487
average test loss: 0.054651, accuracy: 0.0524
case acc: 0.036571447
case acc: 0.026728416
case acc: 0.080803044
case acc: 0.089463316
case acc: 0.038143102
case acc: 0.04283696
top acc: 0.0321 ::: bot acc: 0.0600
top acc: 0.0456 ::: bot acc: 0.0213
top acc: 0.0304 ::: bot acc: 0.1346
top acc: 0.1222 ::: bot acc: 0.0548
top acc: 0.0345 ::: bot acc: 0.0594
top acc: 0.0380 ::: bot acc: 0.0693
current epoch: 15
train loss is 0.063948
average val loss: 0.046911, accuracy: 0.0464
average test loss: 0.052819, accuracy: 0.0507
case acc: 0.035883673
case acc: 0.025930565
case acc: 0.07795309
case acc: 0.08551002
case acc: 0.037344914
case acc: 0.041835703
top acc: 0.0340 ::: bot acc: 0.0580
top acc: 0.0436 ::: bot acc: 0.0231
top acc: 0.0294 ::: bot acc: 0.1308
top acc: 0.1183 ::: bot acc: 0.0510
top acc: 0.0374 ::: bot acc: 0.0564
top acc: 0.0418 ::: bot acc: 0.0654
current epoch: 16
train loss is 0.063052
average val loss: 0.046294, accuracy: 0.0458
average test loss: 0.052093, accuracy: 0.0496
case acc: 0.037318736
case acc: 0.024378752
case acc: 0.08042691
case acc: 0.07564609
case acc: 0.037630808
case acc: 0.041931015
top acc: 0.0298 ::: bot acc: 0.0622
top acc: 0.0332 ::: bot acc: 0.0337
top acc: 0.0301 ::: bot acc: 0.1342
top acc: 0.1081 ::: bot acc: 0.0417
top acc: 0.0361 ::: bot acc: 0.0577
top acc: 0.0415 ::: bot acc: 0.0659
current epoch: 17
train loss is 0.062927
average val loss: 0.050569, accuracy: 0.0497
average test loss: 0.054929, accuracy: 0.0511
case acc: 0.04355975
case acc: 0.029386086
case acc: 0.09175771
case acc: 0.056597542
case acc: 0.040898897
case acc: 0.044651005
top acc: 0.0197 ::: bot acc: 0.0766
top acc: 0.0134 ::: bot acc: 0.0549
top acc: 0.0357 ::: bot acc: 0.1483
top acc: 0.0879 ::: bot acc: 0.0251
top acc: 0.0260 ::: bot acc: 0.0680
top acc: 0.0322 ::: bot acc: 0.0752
current epoch: 18
train loss is 0.064605
average val loss: 0.063035, accuracy: 0.0615
average test loss: 0.064514, accuracy: 0.0599
case acc: 0.05884961
case acc: 0.05017574
case acc: 0.112985045
case acc: 0.033158045
case acc: 0.05079248
case acc: 0.053371523
top acc: 0.0185 ::: bot acc: 0.1001
top acc: 0.0191 ::: bot acc: 0.0832
top acc: 0.0512 ::: bot acc: 0.1724
top acc: 0.0581 ::: bot acc: 0.0143
top acc: 0.0165 ::: bot acc: 0.0874
top acc: 0.0200 ::: bot acc: 0.0943
current epoch: 19
train loss is 0.073233
average val loss: 0.067761, accuracy: 0.0662
average test loss: 0.068588, accuracy: 0.0641
case acc: 0.065780826
case acc: 0.05788772
case acc: 0.12184956
case acc: 0.02662673
case acc: 0.055018492
case acc: 0.05759896
top acc: 0.0222 ::: bot acc: 0.1086
top acc: 0.0257 ::: bot acc: 0.0915
top acc: 0.0585 ::: bot acc: 0.1819
top acc: 0.0425 ::: bot acc: 0.0259
top acc: 0.0164 ::: bot acc: 0.0938
top acc: 0.0191 ::: bot acc: 0.1010
current epoch: 20
train loss is 0.081422
average val loss: 0.051782, accuracy: 0.0504
average test loss: 0.055417, accuracy: 0.0512
case acc: 0.0492468
case acc: 0.034122072
case acc: 0.10181571
case acc: 0.032661114
case acc: 0.042725313
case acc: 0.046831027
top acc: 0.0165 ::: bot acc: 0.0869
top acc: 0.0116 ::: bot acc: 0.0630
top acc: 0.0429 ::: bot acc: 0.1597
top acc: 0.0571 ::: bot acc: 0.0147
top acc: 0.0219 ::: bot acc: 0.0727
top acc: 0.0273 ::: bot acc: 0.0808
current epoch: 21
train loss is 0.071298
average val loss: 0.042553, accuracy: 0.0419
average test loss: 0.048147, accuracy: 0.0451
case acc: 0.040579956
case acc: 0.024477122
case acc: 0.08623392
case acc: 0.038604125
case acc: 0.038080707
case acc: 0.04278271
top acc: 0.0239 ::: bot acc: 0.0703
top acc: 0.0311 ::: bot acc: 0.0360
top acc: 0.0326 ::: bot acc: 0.1415
top acc: 0.0666 ::: bot acc: 0.0134
top acc: 0.0341 ::: bot acc: 0.0595
top acc: 0.0377 ::: bot acc: 0.0693
current epoch: 22
train loss is 0.058844
average val loss: 0.040824, accuracy: 0.0403
average test loss: 0.046598, accuracy: 0.0440
case acc: 0.039014403
case acc: 0.025555022
case acc: 0.081771925
case acc: 0.036817502
case acc: 0.037941948
case acc: 0.04296806
top acc: 0.0267 ::: bot acc: 0.0665
top acc: 0.0418 ::: bot acc: 0.0253
top acc: 0.0306 ::: bot acc: 0.1358
top acc: 0.0641 ::: bot acc: 0.0130
top acc: 0.0345 ::: bot acc: 0.0591
top acc: 0.0370 ::: bot acc: 0.0700
current epoch: 23
train loss is 0.053893
average val loss: 0.038731, accuracy: 0.0380
average test loss: 0.044900, accuracy: 0.0429
case acc: 0.037110325
case acc: 0.02749152
case acc: 0.07665087
case acc: 0.03646151
case acc: 0.037181146
case acc: 0.0422674
top acc: 0.0308 ::: bot acc: 0.0617
top acc: 0.0472 ::: bot acc: 0.0206
top acc: 0.0289 ::: bot acc: 0.1290
top acc: 0.0636 ::: bot acc: 0.0130
top acc: 0.0374 ::: bot acc: 0.0562
top acc: 0.0399 ::: bot acc: 0.0672
current epoch: 24
train loss is 0.051445
average val loss: 0.037433, accuracy: 0.0366
average test loss: 0.043821, accuracy: 0.0418
case acc: 0.03679126
case acc: 0.02639288
case acc: 0.074779056
case acc: 0.034077615
case acc: 0.036958
case acc: 0.041904
top acc: 0.0317 ::: bot acc: 0.0608
top acc: 0.0443 ::: bot acc: 0.0230
top acc: 0.0284 ::: bot acc: 0.1264
top acc: 0.0597 ::: bot acc: 0.0138
top acc: 0.0383 ::: bot acc: 0.0552
top acc: 0.0414 ::: bot acc: 0.0658
current epoch: 25
train loss is 0.050525
average val loss: 0.037632, accuracy: 0.0368
average test loss: 0.043925, accuracy: 0.0415
case acc: 0.038238216
case acc: 0.024422392
case acc: 0.07677706
case acc: 0.02977896
case acc: 0.037485342
case acc: 0.042333134
top acc: 0.0282 ::: bot acc: 0.0646
top acc: 0.0342 ::: bot acc: 0.0330
top acc: 0.0289 ::: bot acc: 0.1291
top acc: 0.0512 ::: bot acc: 0.0180
top acc: 0.0358 ::: bot acc: 0.0577
top acc: 0.0397 ::: bot acc: 0.0675
current epoch: 26
train loss is 0.050546
average val loss: 0.042095, accuracy: 0.0412
average test loss: 0.047376, accuracy: 0.0442
case acc: 0.043136455
case acc: 0.027231274
case acc: 0.08479989
case acc: 0.025050763
case acc: 0.040301796
case acc: 0.04479389
top acc: 0.0205 ::: bot acc: 0.0757
top acc: 0.0172 ::: bot acc: 0.0500
top acc: 0.0317 ::: bot acc: 0.1398
top acc: 0.0353 ::: bot acc: 0.0331
top acc: 0.0271 ::: bot acc: 0.0665
top acc: 0.0316 ::: bot acc: 0.0757
current epoch: 27
train loss is 0.052487
average val loss: 0.050162, accuracy: 0.0495
average test loss: 0.053795, accuracy: 0.0507
case acc: 0.050391115
case acc: 0.036621626
case acc: 0.095759965
case acc: 0.027501417
case acc: 0.044711336
case acc: 0.049148433
top acc: 0.0165 ::: bot acc: 0.0886
top acc: 0.0118 ::: bot acc: 0.0667
top acc: 0.0384 ::: bot acc: 0.1528
top acc: 0.0179 ::: bot acc: 0.0512
top acc: 0.0195 ::: bot acc: 0.0769
top acc: 0.0240 ::: bot acc: 0.0859
current epoch: 28
train loss is 0.057206
average val loss: 0.053795, accuracy: 0.0532
average test loss: 0.056863, accuracy: 0.0543
case acc: 0.053831883
case acc: 0.04010159
case acc: 0.10047023
case acc: 0.034320068
case acc: 0.046399098
case acc: 0.05088598
top acc: 0.0168 ::: bot acc: 0.0936
top acc: 0.0128 ::: bot acc: 0.0714
top acc: 0.0418 ::: bot acc: 0.1582
top acc: 0.0169 ::: bot acc: 0.0619
top acc: 0.0181 ::: bot acc: 0.0801
top acc: 0.0217 ::: bot acc: 0.0896
current epoch: 29
train loss is 0.061356
average val loss: 0.046141, accuracy: 0.0458
average test loss: 0.050734, accuracy: 0.0483
case acc: 0.047438353
case acc: 0.030330557
case acc: 0.09180368
case acc: 0.031504724
case acc: 0.041740622
case acc: 0.046892997
top acc: 0.0170 ::: bot acc: 0.0839
top acc: 0.0127 ::: bot acc: 0.0569
top acc: 0.0358 ::: bot acc: 0.1482
top acc: 0.0162 ::: bot acc: 0.0581
top acc: 0.0234 ::: bot acc: 0.0705
top acc: 0.0270 ::: bot acc: 0.0810
current epoch: 30
train loss is 0.058302
average val loss: 0.037367, accuracy: 0.0373
average test loss: 0.043668, accuracy: 0.0419
case acc: 0.040328477
case acc: 0.024440425
case acc: 0.0794063
case acc: 0.02659469
case acc: 0.03764104
case acc: 0.042988267
top acc: 0.0244 ::: bot acc: 0.0695
top acc: 0.0321 ::: bot acc: 0.0350
top acc: 0.0298 ::: bot acc: 0.1326
top acc: 0.0193 ::: bot acc: 0.0491
top acc: 0.0348 ::: bot acc: 0.0585
top acc: 0.0364 ::: bot acc: 0.0703
current epoch: 31
train loss is 0.050418
average val loss: 0.033487, accuracy: 0.0331
average test loss: 0.040486, accuracy: 0.0396
case acc: 0.03633943
case acc: 0.027995657
case acc: 0.07075161
case acc: 0.024881987
case acc: 0.036265213
case acc: 0.041514874
top acc: 0.0332 ::: bot acc: 0.0592
top acc: 0.0481 ::: bot acc: 0.0199
top acc: 0.0279 ::: bot acc: 0.1206
top acc: 0.0250 ::: bot acc: 0.0434
top acc: 0.0416 ::: bot acc: 0.0518
top acc: 0.0422 ::: bot acc: 0.0646
current epoch: 32
train loss is 0.044955
average val loss: 0.031317, accuracy: 0.0305
average test loss: 0.038732, accuracy: 0.0386
case acc: 0.03427439
case acc: 0.033301666
case acc: 0.06383201
case acc: 0.024573274
case acc: 0.03541371
case acc: 0.040311284
top acc: 0.0416 ::: bot acc: 0.0506
top acc: 0.0573 ::: bot acc: 0.0174
top acc: 0.0286 ::: bot acc: 0.1100
top acc: 0.0303 ::: bot acc: 0.0380
top acc: 0.0471 ::: bot acc: 0.0464
top acc: 0.0475 ::: bot acc: 0.0592
current epoch: 33
train loss is 0.042671
average val loss: 0.029949, accuracy: 0.0290
average test loss: 0.037576, accuracy: 0.0378
case acc: 0.033691674
case acc: 0.034640297
case acc: 0.059116233
case acc: 0.024929859
case acc: 0.03515204
case acc: 0.039563295
top acc: 0.0477 ::: bot acc: 0.0443
top acc: 0.0593 ::: bot acc: 0.0174
top acc: 0.0315 ::: bot acc: 0.1015
top acc: 0.0346 ::: bot acc: 0.0338
top acc: 0.0512 ::: bot acc: 0.0423
top acc: 0.0525 ::: bot acc: 0.0543
current epoch: 34
train loss is 0.041761
average val loss: 0.028989, accuracy: 0.0281
average test loss: 0.036748, accuracy: 0.0370
case acc: 0.033472285
case acc: 0.032950256
case acc: 0.05608066
case acc: 0.025285382
case acc: 0.035144895
case acc: 0.039233286
top acc: 0.0514 ::: bot acc: 0.0405
top acc: 0.0569 ::: bot acc: 0.0173
top acc: 0.0344 ::: bot acc: 0.0956
top acc: 0.0370 ::: bot acc: 0.0314
top acc: 0.0539 ::: bot acc: 0.0396
top acc: 0.0565 ::: bot acc: 0.0504
current epoch: 35
train loss is 0.041142
average val loss: 0.028329, accuracy: 0.0276
average test loss: 0.036254, accuracy: 0.0361
case acc: 0.03342796
case acc: 0.02839906
case acc: 0.055736843
case acc: 0.024914477
case acc: 0.035154365
case acc: 0.03922453
top acc: 0.0500 ::: bot acc: 0.0417
top acc: 0.0492 ::: bot acc: 0.0193
top acc: 0.0348 ::: bot acc: 0.0949
top acc: 0.0346 ::: bot acc: 0.0338
top acc: 0.0528 ::: bot acc: 0.0407
top acc: 0.0568 ::: bot acc: 0.0501
current epoch: 36
train loss is 0.040693
average val loss: 0.029017, accuracy: 0.0285
average test loss: 0.037051, accuracy: 0.0363
case acc: 0.033923063
case acc: 0.024496377
case acc: 0.058989342
case acc: 0.024915064
case acc: 0.03558422
case acc: 0.039675593
top acc: 0.0421 ::: bot acc: 0.0495
top acc: 0.0364 ::: bot acc: 0.0309
top acc: 0.0318 ::: bot acc: 0.1012
top acc: 0.0253 ::: bot acc: 0.0432
top acc: 0.0463 ::: bot acc: 0.0473
top acc: 0.0515 ::: bot acc: 0.0555
current epoch: 37
train loss is 0.041018
average val loss: 0.032503, accuracy: 0.0321
average test loss: 0.039988, accuracy: 0.0391
case acc: 0.03634479
case acc: 0.025731193
case acc: 0.06425766
case acc: 0.030211702
case acc: 0.036995217
case acc: 0.041123938
top acc: 0.0320 ::: bot acc: 0.0597
top acc: 0.0232 ::: bot acc: 0.0442
top acc: 0.0284 ::: bot acc: 0.1108
top acc: 0.0163 ::: bot acc: 0.0559
top acc: 0.0382 ::: bot acc: 0.0554
top acc: 0.0440 ::: bot acc: 0.0629
current epoch: 38
train loss is 0.042176
average val loss: 0.039295, accuracy: 0.0389
average test loss: 0.045192, accuracy: 0.0446
case acc: 0.0409955
case acc: 0.03050781
case acc: 0.07181417
case acc: 0.04136022
case acc: 0.039448075
case acc: 0.04340022
top acc: 0.0228 ::: bot acc: 0.0712
top acc: 0.0127 ::: bot acc: 0.0571
top acc: 0.0278 ::: bot acc: 0.1224
top acc: 0.0199 ::: bot acc: 0.0708
top acc: 0.0295 ::: bot acc: 0.0641
top acc: 0.0353 ::: bot acc: 0.0716
current epoch: 39
train loss is 0.044644
average val loss: 0.048425, accuracy: 0.0480
average test loss: 0.052122, accuracy: 0.0518
case acc: 0.047008168
case acc: 0.03787105
case acc: 0.08102823
case acc: 0.055265542
case acc: 0.042753525
case acc: 0.04694822
top acc: 0.0171 ::: bot acc: 0.0831
top acc: 0.0120 ::: bot acc: 0.0684
top acc: 0.0300 ::: bot acc: 0.1351
top acc: 0.0293 ::: bot acc: 0.0869
top acc: 0.0214 ::: bot acc: 0.0730
top acc: 0.0270 ::: bot acc: 0.0811
current epoch: 40
train loss is 0.048747
average val loss: 0.051707, accuracy: 0.0515
average test loss: 0.054682, accuracy: 0.0546
case acc: 0.049370624
case acc: 0.03814003
case acc: 0.084792376
case acc: 0.063347526
case acc: 0.04351642
case acc: 0.048230484
top acc: 0.0165 ::: bot acc: 0.0869
top acc: 0.0121 ::: bot acc: 0.0688
top acc: 0.0314 ::: bot acc: 0.1400
top acc: 0.0359 ::: bot acc: 0.0957
top acc: 0.0205 ::: bot acc: 0.0746
top acc: 0.0251 ::: bot acc: 0.0839
current epoch: 41
train loss is 0.051745
average val loss: 0.045004, accuracy: 0.0451
average test loss: 0.049548, accuracy: 0.0497
case acc: 0.044625904
case acc: 0.029163541
case acc: 0.07884947
case acc: 0.06025404
case acc: 0.040120475
case acc: 0.045168594
top acc: 0.0188 ::: bot acc: 0.0787
top acc: 0.0138 ::: bot acc: 0.0546
top acc: 0.0294 ::: bot acc: 0.1320
top acc: 0.0334 ::: bot acc: 0.0924
top acc: 0.0275 ::: bot acc: 0.0661
top acc: 0.0300 ::: bot acc: 0.0769
current epoch: 42
train loss is 0.051752
average val loss: 0.033904, accuracy: 0.0342
average test loss: 0.040955, accuracy: 0.0421
case acc: 0.03676426
case acc: 0.024761405
case acc: 0.06585473
case acc: 0.047768213
case acc: 0.036141902
case acc: 0.04113658
top acc: 0.0316 ::: bot acc: 0.0606
top acc: 0.0381 ::: bot acc: 0.0293
top acc: 0.0282 ::: bot acc: 0.1132
top acc: 0.0239 ::: bot acc: 0.0784
top acc: 0.0427 ::: bot acc: 0.0508
top acc: 0.0435 ::: bot acc: 0.0632
current epoch: 43
train loss is 0.047540
average val loss: 0.028948, accuracy: 0.0287
average test loss: 0.036940, accuracy: 0.0390
case acc: 0.033617537
case acc: 0.03634405
case acc: 0.054876518
case acc: 0.03485677
case acc: 0.035146587
case acc: 0.03922358
top acc: 0.0495 ::: bot acc: 0.0426
top acc: 0.0619 ::: bot acc: 0.0176
top acc: 0.0357 ::: bot acc: 0.0931
top acc: 0.0170 ::: bot acc: 0.0625
top acc: 0.0551 ::: bot acc: 0.0384
top acc: 0.0550 ::: bot acc: 0.0517
current epoch: 44
train loss is 0.043811
average val loss: 0.031279, accuracy: 0.0313
average test loss: 0.038527, accuracy: 0.0402
case acc: 0.03813611
case acc: 0.055251956
case acc: 0.046622787
case acc: 0.024479922
case acc: 0.036782317
case acc: 0.03985485
top acc: 0.0721 ::: bot acc: 0.0204
top acc: 0.0859 ::: bot acc: 0.0262
top acc: 0.0593 ::: bot acc: 0.0671
top acc: 0.0293 ::: bot acc: 0.0390
top acc: 0.0703 ::: bot acc: 0.0232
top acc: 0.0705 ::: bot acc: 0.0362
current epoch: 45
train loss is 0.045162
average val loss: 0.045604, accuracy: 0.0464
average test loss: 0.051182, accuracy: 0.0517
case acc: 0.05575677
case acc: 0.0775059
case acc: 0.049411967
case acc: 0.03464707
case acc: 0.045850873
case acc: 0.047315996
top acc: 0.0995 ::: bot acc: 0.0181
top acc: 0.1103 ::: bot acc: 0.0442
top acc: 0.0916 ::: bot acc: 0.0349
top acc: 0.0609 ::: bot acc: 0.0135
top acc: 0.0898 ::: bot acc: 0.0127
top acc: 0.0918 ::: bot acc: 0.0175
current epoch: 46
train loss is 0.050625
average val loss: 0.060161, accuracy: 0.0605
average test loss: 0.064758, accuracy: 0.0648
case acc: 0.07227797
case acc: 0.08795524
case acc: 0.06093154
case acc: 0.055271436
case acc: 0.05522648
case acc: 0.056955144
top acc: 0.1186 ::: bot acc: 0.0293
top acc: 0.1210 ::: bot acc: 0.0542
top acc: 0.1158 ::: bot acc: 0.0212
top acc: 0.0864 ::: bot acc: 0.0243
top acc: 0.1023 ::: bot acc: 0.0159
top acc: 0.1072 ::: bot acc: 0.0155
current epoch: 47
train loss is 0.056793
average val loss: 0.044732, accuracy: 0.0449
average test loss: 0.050325, accuracy: 0.0500
case acc: 0.05700859
case acc: 0.05816435
case acc: 0.052879494
case acc: 0.045004208
case acc: 0.040784758
case acc: 0.04635954
top acc: 0.1010 ::: bot acc: 0.0188
top acc: 0.0893 ::: bot acc: 0.0281
top acc: 0.1011 ::: bot acc: 0.0265
top acc: 0.0749 ::: bot acc: 0.0165
top acc: 0.0816 ::: bot acc: 0.0141
top acc: 0.0899 ::: bot acc: 0.0185
current epoch: 48
train loss is 0.053296
average val loss: 0.029617, accuracy: 0.0297
average test loss: 0.037032, accuracy: 0.0362
case acc: 0.038647164
case acc: 0.027960042
case acc: 0.04613262
case acc: 0.029651787
case acc: 0.035292596
case acc: 0.039461486
top acc: 0.0733 ::: bot acc: 0.0193
top acc: 0.0483 ::: bot acc: 0.0200
top acc: 0.0742 ::: bot acc: 0.0525
top acc: 0.0513 ::: bot acc: 0.0179
top acc: 0.0567 ::: bot acc: 0.0369
top acc: 0.0680 ::: bot acc: 0.0388
current epoch: 49
train loss is 0.045413
average val loss: 0.027524, accuracy: 0.0267
average test loss: 0.035775, accuracy: 0.0346
case acc: 0.033529412
case acc: 0.026348818
case acc: 0.047810808
case acc: 0.024677929
case acc: 0.03595885
case acc: 0.039139368
top acc: 0.0536 ::: bot acc: 0.0380
top acc: 0.0211 ::: bot acc: 0.0465
top acc: 0.0535 ::: bot acc: 0.0731
top acc: 0.0322 ::: bot acc: 0.0363
top acc: 0.0443 ::: bot acc: 0.0494
top acc: 0.0573 ::: bot acc: 0.0496
current epoch: 50
train loss is 0.042114
average val loss: 0.035014, accuracy: 0.0339
average test loss: 0.041929, accuracy: 0.0415
case acc: 0.036854587
case acc: 0.038585622
case acc: 0.057748824
case acc: 0.035217375
case acc: 0.039236005
case acc: 0.04131033
top acc: 0.0304 ::: bot acc: 0.0613
top acc: 0.0123 ::: bot acc: 0.0694
top acc: 0.0330 ::: bot acc: 0.0989
top acc: 0.0173 ::: bot acc: 0.0629
top acc: 0.0302 ::: bot acc: 0.0635
top acc: 0.0431 ::: bot acc: 0.0637
LME_Co_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5376 5376 5376
1.7082474 -0.6288155 0.17559324 -0.22413088
Validation: 600 600 600
Testing: 774 774 774
pre-processing time: 0.00023102760314941406
the split date is 2011-07-01
net initializing with time: 0.0030214786529541016
preparing training and testing date with time: 0.0
current epoch: 1
train loss is 0.274154
average val loss: 0.192574, accuracy: 0.1940
average test loss: 0.195168, accuracy: 0.1933
case acc: 0.30230916
case acc: 0.2466287
case acc: 0.2824972
case acc: 0.071480975
case acc: 0.060357854
case acc: 0.19675227
top acc: 0.2578 ::: bot acc: 0.3465
top acc: 0.2066 ::: bot acc: 0.2841
top acc: 0.2321 ::: bot acc: 0.3306
top acc: 0.0313 ::: bot acc: 0.1179
top acc: 0.1146 ::: bot acc: 0.0159
top acc: 0.1418 ::: bot acc: 0.2476
current epoch: 2
train loss is 0.221166
average val loss: 0.134367, accuracy: 0.1316
average test loss: 0.136126, accuracy: 0.1367
case acc: 0.20037907
case acc: 0.14692998
case acc: 0.18243514
case acc: 0.050799254
case acc: 0.14602433
case acc: 0.093804516
top acc: 0.1557 ::: bot acc: 0.2446
top acc: 0.1072 ::: bot acc: 0.1841
top acc: 0.1319 ::: bot acc: 0.2304
top acc: 0.0918 ::: bot acc: 0.0262
top acc: 0.2027 ::: bot acc: 0.0970
top acc: 0.0412 ::: bot acc: 0.1437
current epoch: 3
train loss is 0.154446
average val loss: 0.122240, accuracy: 0.1195
average test loss: 0.123746, accuracy: 0.1246
case acc: 0.17768373
case acc: 0.12696776
case acc: 0.15996218
case acc: 0.057161976
case acc: 0.14935288
case acc: 0.0764659
top acc: 0.1329 ::: bot acc: 0.2220
top acc: 0.0875 ::: bot acc: 0.1640
top acc: 0.1094 ::: bot acc: 0.2079
top acc: 0.1033 ::: bot acc: 0.0222
top acc: 0.2063 ::: bot acc: 0.1004
top acc: 0.0274 ::: bot acc: 0.1247
current epoch: 4
train loss is 0.152184
average val loss: 0.112789, accuracy: 0.1103
average test loss: 0.114144, accuracy: 0.1151
case acc: 0.16149132
case acc: 0.11403238
case acc: 0.14431673
case acc: 0.059237562
case acc: 0.14549702
case acc: 0.06582772
top acc: 0.1166 ::: bot acc: 0.2059
top acc: 0.0747 ::: bot acc: 0.1509
top acc: 0.0937 ::: bot acc: 0.1922
top acc: 0.1069 ::: bot acc: 0.0211
top acc: 0.2026 ::: bot acc: 0.0967
top acc: 0.0213 ::: bot acc: 0.1118
current epoch: 5
train loss is 0.135172
average val loss: 0.106726, accuracy: 0.1044
average test loss: 0.108152, accuracy: 0.1090
case acc: 0.15467705
case acc: 0.11014082
case acc: 0.13710794
case acc: 0.05519128
case acc: 0.13281043
case acc: 0.06421484
top acc: 0.1097 ::: bot acc: 0.1992
top acc: 0.0710 ::: bot acc: 0.1469
top acc: 0.0865 ::: bot acc: 0.1850
top acc: 0.1003 ::: bot acc: 0.0221
top acc: 0.1900 ::: bot acc: 0.0841
top acc: 0.0206 ::: bot acc: 0.1098
current epoch: 6
train loss is 0.131039
average val loss: 0.101180, accuracy: 0.0991
average test loss: 0.102708, accuracy: 0.1034
case acc: 0.14864527
case acc: 0.1072483
case acc: 0.13094224
case acc: 0.051089406
case acc: 0.11969422
case acc: 0.0630185
top acc: 0.1035 ::: bot acc: 0.1932
top acc: 0.0681 ::: bot acc: 0.1440
top acc: 0.0803 ::: bot acc: 0.1789
top acc: 0.0931 ::: bot acc: 0.0242
top acc: 0.1770 ::: bot acc: 0.0711
top acc: 0.0201 ::: bot acc: 0.1082
current epoch: 7
train loss is 0.123331
average val loss: 0.093745, accuracy: 0.0920
average test loss: 0.095244, accuracy: 0.0959
case acc: 0.137023
case acc: 0.09865987
case acc: 0.11888397
case acc: 0.050201084
case acc: 0.11268398
case acc: 0.05822745
top acc: 0.0919 ::: bot acc: 0.1815
top acc: 0.0596 ::: bot acc: 0.1354
top acc: 0.0683 ::: bot acc: 0.1669
top acc: 0.0915 ::: bot acc: 0.0245
top acc: 0.1700 ::: bot acc: 0.0642
top acc: 0.0195 ::: bot acc: 0.1014
current epoch: 8
train loss is 0.113072
average val loss: 0.091716, accuracy: 0.0902
average test loss: 0.093526, accuracy: 0.0935
case acc: 0.1390349
case acc: 0.10376556
case acc: 0.120456085
case acc: 0.042793095
case acc: 0.09234121
case acc: 0.06290047
top acc: 0.0939 ::: bot acc: 0.1834
top acc: 0.0647 ::: bot acc: 0.1405
top acc: 0.0698 ::: bot acc: 0.1686
top acc: 0.0765 ::: bot acc: 0.0324
top acc: 0.1497 ::: bot acc: 0.0439
top acc: 0.0199 ::: bot acc: 0.1082
current epoch: 9
train loss is 0.110861
average val loss: 0.084743, accuracy: 0.0835
average test loss: 0.086503, accuracy: 0.0865
case acc: 0.12782176
case acc: 0.095817804
case acc: 0.1093738
case acc: 0.042137492
case acc: 0.08547059
case acc: 0.0581871
top acc: 0.0826 ::: bot acc: 0.1722
top acc: 0.0568 ::: bot acc: 0.1325
top acc: 0.0591 ::: bot acc: 0.1573
top acc: 0.0749 ::: bot acc: 0.0335
top acc: 0.1428 ::: bot acc: 0.0372
top acc: 0.0193 ::: bot acc: 0.1015
current epoch: 10
train loss is 0.102173
average val loss: 0.083156, accuracy: 0.0822
average test loss: 0.085038, accuracy: 0.0845
case acc: 0.1284934
case acc: 0.09990598
case acc: 0.110205844
case acc: 0.03873497
case acc: 0.06789085
case acc: 0.061570797
top acc: 0.0832 ::: bot acc: 0.1729
top acc: 0.0608 ::: bot acc: 0.1366
top acc: 0.0599 ::: bot acc: 0.1582
top acc: 0.0616 ::: bot acc: 0.0463
top acc: 0.1242 ::: bot acc: 0.0218
top acc: 0.0194 ::: bot acc: 0.1065
current epoch: 11
train loss is 0.098795
average val loss: 0.076941, accuracy: 0.0762
average test loss: 0.078792, accuracy: 0.0782
case acc: 0.11814235
case acc: 0.09318635
case acc: 0.10059217
case acc: 0.038574744
case acc: 0.06123294
case acc: 0.05739356
top acc: 0.0728 ::: bot acc: 0.1626
top acc: 0.0542 ::: bot acc: 0.1298
top acc: 0.0508 ::: bot acc: 0.1483
top acc: 0.0597 ::: bot acc: 0.0482
top acc: 0.1168 ::: bot acc: 0.0168
top acc: 0.0192 ::: bot acc: 0.1003
current epoch: 12
train loss is 0.092927
average val loss: 0.073402, accuracy: 0.0731
average test loss: 0.075331, accuracy: 0.0746
case acc: 0.112909704
case acc: 0.0916303
case acc: 0.0959804
case acc: 0.038596947
case acc: 0.05159918
case acc: 0.056797422
top acc: 0.0675 ::: bot acc: 0.1573
top acc: 0.0526 ::: bot acc: 0.1283
top acc: 0.0466 ::: bot acc: 0.1435
top acc: 0.0527 ::: bot acc: 0.0551
top acc: 0.1045 ::: bot acc: 0.0127
top acc: 0.0192 ::: bot acc: 0.0994
current epoch: 13
train loss is 0.085328
average val loss: 0.072080, accuracy: 0.0722
average test loss: 0.074247, accuracy: 0.0736
case acc: 0.111140124
case acc: 0.093671545
case acc: 0.095148414
case acc: 0.040690962
case acc: 0.042670414
case acc: 0.05833211
top acc: 0.0657 ::: bot acc: 0.1556
top acc: 0.0548 ::: bot acc: 0.1303
top acc: 0.0458 ::: bot acc: 0.1426
top acc: 0.0426 ::: bot acc: 0.0652
top acc: 0.0888 ::: bot acc: 0.0172
top acc: 0.0191 ::: bot acc: 0.1018
current epoch: 14
train loss is 0.083031
average val loss: 0.069478, accuracy: 0.0699
average test loss: 0.071854, accuracy: 0.0716
case acc: 0.10576873
case acc: 0.092331834
case acc: 0.09157837
case acc: 0.042783715
case acc: 0.040053546
case acc: 0.057302296
top acc: 0.0603 ::: bot acc: 0.1502
top acc: 0.0534 ::: bot acc: 0.1289
top acc: 0.0426 ::: bot acc: 0.1389
top acc: 0.0365 ::: bot acc: 0.0714
top acc: 0.0768 ::: bot acc: 0.0290
top acc: 0.0192 ::: bot acc: 0.1003
current epoch: 15
train loss is 0.078615
average val loss: 0.064916, accuracy: 0.0656
average test loss: 0.067386, accuracy: 0.0675
case acc: 0.09646293
case acc: 0.08693357
case acc: 0.084152974
case acc: 0.043613166
case acc: 0.039600056
case acc: 0.053957622
top acc: 0.0513 ::: bot acc: 0.1407
top acc: 0.0480 ::: bot acc: 0.1235
top acc: 0.0359 ::: bot acc: 0.1310
top acc: 0.0349 ::: bot acc: 0.0734
top acc: 0.0691 ::: bot acc: 0.0367
top acc: 0.0201 ::: bot acc: 0.0948
current epoch: 16
train loss is 0.071725
average val loss: 0.064441, accuracy: 0.0658
average test loss: 0.067196, accuracy: 0.0677
case acc: 0.093334906
case acc: 0.08770102
case acc: 0.08271583
case acc: 0.047260605
case acc: 0.040544234
case acc: 0.054418318
top acc: 0.0483 ::: bot acc: 0.1375
top acc: 0.0488 ::: bot acc: 0.1243
top acc: 0.0346 ::: bot acc: 0.1295
top acc: 0.0301 ::: bot acc: 0.0813
top acc: 0.0556 ::: bot acc: 0.0502
top acc: 0.0198 ::: bot acc: 0.0956
current epoch: 17
train loss is 0.068645
average val loss: 0.060981, accuracy: 0.0627
average test loss: 0.063721, accuracy: 0.0643
case acc: 0.08485232
case acc: 0.082724676
case acc: 0.07641051
case acc: 0.048270676
case acc: 0.04194707
case acc: 0.051747378
top acc: 0.0404 ::: bot acc: 0.1287
top acc: 0.0440 ::: bot acc: 0.1192
top acc: 0.0295 ::: bot acc: 0.1226
top acc: 0.0291 ::: bot acc: 0.0832
top acc: 0.0480 ::: bot acc: 0.0577
top acc: 0.0213 ::: bot acc: 0.0909
current epoch: 18
train loss is 0.062655
average val loss: 0.059073, accuracy: 0.0610
average test loss: 0.061827, accuracy: 0.0626
case acc: 0.078423664
case acc: 0.07914934
case acc: 0.0723161
case acc: 0.050300498
case acc: 0.044767305
case acc: 0.050380405
top acc: 0.0346 ::: bot acc: 0.1220
top acc: 0.0407 ::: bot acc: 0.1155
top acc: 0.0270 ::: bot acc: 0.1177
top acc: 0.0282 ::: bot acc: 0.0867
top acc: 0.0393 ::: bot acc: 0.0668
top acc: 0.0223 ::: bot acc: 0.0884
current epoch: 19
train loss is 0.058795
average val loss: 0.058368, accuracy: 0.0603
average test loss: 0.061108, accuracy: 0.0619
case acc: 0.07336167
case acc: 0.0767328
case acc: 0.06971977
case acc: 0.05297471
case acc: 0.048998397
case acc: 0.049729478
top acc: 0.0302 ::: bot acc: 0.1165
top acc: 0.0384 ::: bot acc: 0.1131
top acc: 0.0255 ::: bot acc: 0.1145
top acc: 0.0275 ::: bot acc: 0.0911
top acc: 0.0326 ::: bot acc: 0.0765
top acc: 0.0228 ::: bot acc: 0.0872
current epoch: 20
train loss is 0.056337
average val loss: 0.055670, accuracy: 0.0576
average test loss: 0.058215, accuracy: 0.0590
case acc: 0.06566394
case acc: 0.07095461
case acc: 0.064744785
case acc: 0.053355783
case acc: 0.051792085
case acc: 0.047503494
top acc: 0.0244 ::: bot acc: 0.1079
top acc: 0.0330 ::: bot acc: 0.1071
top acc: 0.0231 ::: bot acc: 0.1082
top acc: 0.0275 ::: bot acc: 0.0916
top acc: 0.0295 ::: bot acc: 0.0822
top acc: 0.0251 ::: bot acc: 0.0827
current epoch: 21
train loss is 0.052170
average val loss: 0.051036, accuracy: 0.0529
average test loss: 0.053183, accuracy: 0.0538
case acc: 0.055660233
case acc: 0.061650604
case acc: 0.05763431
case acc: 0.05119167
case acc: 0.052419893
case acc: 0.044381745
top acc: 0.0183 ::: bot acc: 0.0958
top acc: 0.0243 ::: bot acc: 0.0974
top acc: 0.0210 ::: bot acc: 0.0986
top acc: 0.0281 ::: bot acc: 0.0881
top acc: 0.0288 ::: bot acc: 0.0835
top acc: 0.0312 ::: bot acc: 0.0750
current epoch: 22
train loss is 0.047624
average val loss: 0.049180, accuracy: 0.0509
average test loss: 0.051143, accuracy: 0.0516
case acc: 0.049992625
case acc: 0.056507245
case acc: 0.054227266
case acc: 0.05106902
case acc: 0.054756608
case acc: 0.043255497
top acc: 0.0161 ::: bot acc: 0.0883
top acc: 0.0199 ::: bot acc: 0.0919
top acc: 0.0209 ::: bot acc: 0.0935
top acc: 0.0281 ::: bot acc: 0.0879
top acc: 0.0274 ::: bot acc: 0.0876
top acc: 0.0345 ::: bot acc: 0.0717
current epoch: 23
train loss is 0.046025
average val loss: 0.045732, accuracy: 0.0474
average test loss: 0.047425, accuracy: 0.0477
case acc: 0.043419898
case acc: 0.048835415
case acc: 0.049283694
case acc: 0.048550915
case acc: 0.054500677
case acc: 0.041595504
top acc: 0.0168 ::: bot acc: 0.0781
top acc: 0.0147 ::: bot acc: 0.0830
top acc: 0.0218 ::: bot acc: 0.0856
top acc: 0.0291 ::: bot acc: 0.0836
top acc: 0.0275 ::: bot acc: 0.0871
top acc: 0.0407 ::: bot acc: 0.0654
current epoch: 24
train loss is 0.044387
average val loss: 0.041540, accuracy: 0.0429
average test loss: 0.042984, accuracy: 0.0431
case acc: 0.037968613
case acc: 0.040140744
case acc: 0.0437016
case acc: 0.044611342
case acc: 0.05201131
case acc: 0.040368855
top acc: 0.0247 ::: bot acc: 0.0659
top acc: 0.0118 ::: bot acc: 0.0714
top acc: 0.0259 ::: bot acc: 0.0752
top acc: 0.0332 ::: bot acc: 0.0757
top acc: 0.0289 ::: bot acc: 0.0826
top acc: 0.0493 ::: bot acc: 0.0568
current epoch: 25
train loss is 0.043149
average val loss: 0.038309, accuracy: 0.0394
average test loss: 0.039631, accuracy: 0.0397
case acc: 0.034415126
case acc: 0.03409816
case acc: 0.039621938
case acc: 0.041029193
case acc: 0.049020663
case acc: 0.039766498
top acc: 0.0348 ::: bot acc: 0.0548
top acc: 0.0167 ::: bot acc: 0.0598
top acc: 0.0348 ::: bot acc: 0.0646
top acc: 0.0407 ::: bot acc: 0.0665
top acc: 0.0321 ::: bot acc: 0.0765
top acc: 0.0574 ::: bot acc: 0.0486
current epoch: 26
train loss is 0.042336
average val loss: 0.037270, accuracy: 0.0382
average test loss: 0.038521, accuracy: 0.0385
case acc: 0.03351889
case acc: 0.031677715
case acc: 0.038368788
case acc: 0.039588068
case acc: 0.04795329
case acc: 0.039602436
top acc: 0.0393 ::: bot acc: 0.0503
top acc: 0.0212 ::: bot acc: 0.0539
top acc: 0.0400 ::: bot acc: 0.0594
top acc: 0.0454 ::: bot acc: 0.0618
top acc: 0.0333 ::: bot acc: 0.0742
top acc: 0.0598 ::: bot acc: 0.0461
current epoch: 27
train loss is 0.041422
average val loss: 0.036423, accuracy: 0.0370
average test loss: 0.037576, accuracy: 0.0374
case acc: 0.033078603
case acc: 0.029801613
case acc: 0.03745507
case acc: 0.03847919
case acc: 0.046154145
case acc: 0.039604194
top acc: 0.0432 ::: bot acc: 0.0463
top acc: 0.0270 ::: bot acc: 0.0480
top acc: 0.0454 ::: bot acc: 0.0540
top acc: 0.0512 ::: bot acc: 0.0560
top acc: 0.0357 ::: bot acc: 0.0704
top acc: 0.0624 ::: bot acc: 0.0434
current epoch: 28
train loss is 0.040399
average val loss: 0.036071, accuracy: 0.0364
average test loss: 0.037202, accuracy: 0.0370
case acc: 0.03302535
case acc: 0.029024368
case acc: 0.03723236
case acc: 0.03818425
case acc: 0.045126937
case acc: 0.039569657
top acc: 0.0437 ::: bot acc: 0.0457
top acc: 0.0298 ::: bot acc: 0.0451
top acc: 0.0477 ::: bot acc: 0.0518
top acc: 0.0548 ::: bot acc: 0.0523
top acc: 0.0374 ::: bot acc: 0.0679
top acc: 0.0623 ::: bot acc: 0.0435
current epoch: 29
train loss is 0.039683
average val loss: 0.036058, accuracy: 0.0362
average test loss: 0.037243, accuracy: 0.0371
case acc: 0.03324921
case acc: 0.02915522
case acc: 0.037372544
case acc: 0.03813063
case acc: 0.045059543
case acc: 0.039518535
top acc: 0.0407 ::: bot acc: 0.0487
top acc: 0.0292 ::: bot acc: 0.0457
top acc: 0.0462 ::: bot acc: 0.0533
top acc: 0.0556 ::: bot acc: 0.0514
top acc: 0.0374 ::: bot acc: 0.0678
top acc: 0.0592 ::: bot acc: 0.0465
current epoch: 30
train loss is 0.038312
average val loss: 0.036276, accuracy: 0.0363
average test loss: 0.037524, accuracy: 0.0374
case acc: 0.033952862
case acc: 0.029611211
case acc: 0.037834205
case acc: 0.03811662
case acc: 0.045285873
case acc: 0.039685927
top acc: 0.0367 ::: bot acc: 0.0527
top acc: 0.0274 ::: bot acc: 0.0475
top acc: 0.0429 ::: bot acc: 0.0566
top acc: 0.0554 ::: bot acc: 0.0516
top acc: 0.0369 ::: bot acc: 0.0683
top acc: 0.0553 ::: bot acc: 0.0503
current epoch: 31
train loss is 0.037498
average val loss: 0.036890, accuracy: 0.0369
average test loss: 0.038182, accuracy: 0.0381
case acc: 0.03510584
case acc: 0.03049756
case acc: 0.038854092
case acc: 0.038194012
case acc: 0.045904122
case acc: 0.03998036
top acc: 0.0318 ::: bot acc: 0.0576
top acc: 0.0242 ::: bot acc: 0.0506
top acc: 0.0379 ::: bot acc: 0.0615
top acc: 0.0539 ::: bot acc: 0.0531
top acc: 0.0357 ::: bot acc: 0.0698
top acc: 0.0508 ::: bot acc: 0.0548
current epoch: 32
train loss is 0.037051
average val loss: 0.037874, accuracy: 0.0378
average test loss: 0.039173, accuracy: 0.0391
case acc: 0.036670297
case acc: 0.031700723
case acc: 0.040688735
case acc: 0.038370952
case acc: 0.046820935
case acc: 0.040339086
top acc: 0.0271 ::: bot acc: 0.0625
top acc: 0.0206 ::: bot acc: 0.0542
top acc: 0.0321 ::: bot acc: 0.0674
top acc: 0.0515 ::: bot acc: 0.0554
top acc: 0.0344 ::: bot acc: 0.0718
top acc: 0.0463 ::: bot acc: 0.0592
current epoch: 33
train loss is 0.036943
average val loss: 0.039499, accuracy: 0.0394
average test loss: 0.040796, accuracy: 0.0408
case acc: 0.038895328
case acc: 0.03363604
case acc: 0.04364735
case acc: 0.038973004
case acc: 0.04839028
case acc: 0.04110169
top acc: 0.0228 ::: bot acc: 0.0680
top acc: 0.0170 ::: bot acc: 0.0589
top acc: 0.0262 ::: bot acc: 0.0747
top acc: 0.0477 ::: bot acc: 0.0592
top acc: 0.0325 ::: bot acc: 0.0751
top acc: 0.0413 ::: bot acc: 0.0643
current epoch: 34
train loss is 0.037317
average val loss: 0.042229, accuracy: 0.0422
average test loss: 0.043550, accuracy: 0.0436
case acc: 0.041823868
case acc: 0.036732342
case acc: 0.048552938
case acc: 0.040703084
case acc: 0.051020965
case acc: 0.042593874
top acc: 0.0185 ::: bot acc: 0.0745
top acc: 0.0133 ::: bot acc: 0.0654
top acc: 0.0221 ::: bot acc: 0.0841
top acc: 0.0414 ::: bot acc: 0.0655
top acc: 0.0295 ::: bot acc: 0.0806
top acc: 0.0350 ::: bot acc: 0.0706
current epoch: 35
train loss is 0.038086
average val loss: 0.045018, accuracy: 0.0452
average test loss: 0.046483, accuracy: 0.0466
case acc: 0.044445623
case acc: 0.039925937
case acc: 0.054125015
case acc: 0.042964928
case acc: 0.053726375
case acc: 0.044200484
top acc: 0.0165 ::: bot acc: 0.0795
top acc: 0.0115 ::: bot acc: 0.0710
top acc: 0.0210 ::: bot acc: 0.0930
top acc: 0.0358 ::: bot acc: 0.0718
top acc: 0.0275 ::: bot acc: 0.0856
top acc: 0.0301 ::: bot acc: 0.0755
current epoch: 36
train loss is 0.038951
average val loss: 0.048977, accuracy: 0.0493
average test loss: 0.050841, accuracy: 0.0510
case acc: 0.04805486
case acc: 0.045041904
case acc: 0.061359424
case acc: 0.046891753
case acc: 0.05815503
case acc: 0.04658144
top acc: 0.0159 ::: bot acc: 0.0852
top acc: 0.0127 ::: bot acc: 0.0782
top acc: 0.0218 ::: bot acc: 0.1035
top acc: 0.0306 ::: bot acc: 0.0802
top acc: 0.0265 ::: bot acc: 0.0928
top acc: 0.0252 ::: bot acc: 0.0816
current epoch: 37
train loss is 0.039813
average val loss: 0.054157, accuracy: 0.0546
average test loss: 0.056612, accuracy: 0.0568
case acc: 0.05217656
case acc: 0.051461134
case acc: 0.06991452
case acc: 0.052751046
case acc: 0.06485268
case acc: 0.049699415
top acc: 0.0171 ::: bot acc: 0.0908
top acc: 0.0162 ::: bot acc: 0.0860
top acc: 0.0254 ::: bot acc: 0.1145
top acc: 0.0277 ::: bot acc: 0.0905
top acc: 0.0275 ::: bot acc: 0.1024
top acc: 0.0214 ::: bot acc: 0.0881
current epoch: 38
train loss is 0.040835
average val loss: 0.057420, accuracy: 0.0579
average test loss: 0.060145, accuracy: 0.0603
case acc: 0.05261424
case acc: 0.054671742
case acc: 0.075219825
case acc: 0.057623673
case acc: 0.07077866
case acc: 0.050990622
top acc: 0.0173 ::: bot acc: 0.0914
top acc: 0.0186 ::: bot acc: 0.0896
top acc: 0.0286 ::: bot acc: 0.1209
top acc: 0.0277 ::: bot acc: 0.0978
top acc: 0.0296 ::: bot acc: 0.1103
top acc: 0.0204 ::: bot acc: 0.0906
current epoch: 39
train loss is 0.041442
average val loss: 0.055709, accuracy: 0.0564
average test loss: 0.058262, accuracy: 0.0583
case acc: 0.046915203
case acc: 0.050750695
case acc: 0.0730431
case acc: 0.058093343
case acc: 0.07266531
case acc: 0.048447397
top acc: 0.0158 ::: bot acc: 0.0835
top acc: 0.0157 ::: bot acc: 0.0852
top acc: 0.0272 ::: bot acc: 0.1183
top acc: 0.0277 ::: bot acc: 0.0985
top acc: 0.0302 ::: bot acc: 0.1128
top acc: 0.0226 ::: bot acc: 0.0856
current epoch: 40
train loss is 0.042261
average val loss: 0.048215, accuracy: 0.0493
average test loss: 0.050141, accuracy: 0.0503
case acc: 0.03789803
case acc: 0.039443534
case acc: 0.06150462
case acc: 0.052154362
case acc: 0.06794521
case acc: 0.042936753
top acc: 0.0246 ::: bot acc: 0.0656
top acc: 0.0118 ::: bot acc: 0.0702
top acc: 0.0218 ::: bot acc: 0.1037
top acc: 0.0278 ::: bot acc: 0.0895
top acc: 0.0286 ::: bot acc: 0.1065
top acc: 0.0340 ::: bot acc: 0.0716
current epoch: 41
train loss is 0.046609
average val loss: 0.037411, accuracy: 0.0389
average test loss: 0.038362, accuracy: 0.0386
case acc: 0.034761805
case acc: 0.027638188
case acc: 0.040211223
case acc: 0.03889548
case acc: 0.050391685
case acc: 0.039882142
top acc: 0.0610 ::: bot acc: 0.0284
top acc: 0.0406 ::: bot acc: 0.0342
top acc: 0.0335 ::: bot acc: 0.0659
top acc: 0.0482 ::: bot acc: 0.0588
top acc: 0.0302 ::: bot acc: 0.0794
top acc: 0.0672 ::: bot acc: 0.0384
current epoch: 42
train loss is 0.053208
average val loss: 0.050742, accuracy: 0.0514
average test loss: 0.048552, accuracy: 0.0496
case acc: 0.058990028
case acc: 0.048974372
case acc: 0.043086503
case acc: 0.050211333
case acc: 0.03888951
case acc: 0.057643186
top acc: 0.1032 ::: bot acc: 0.0178
top acc: 0.0852 ::: bot acc: 0.0174
top acc: 0.0814 ::: bot acc: 0.0212
top acc: 0.0915 ::: bot acc: 0.0242
top acc: 0.0645 ::: bot acc: 0.0396
top acc: 0.1080 ::: bot acc: 0.0140
current epoch: 43
train loss is 0.054538
average val loss: 0.045462, accuracy: 0.0461
average test loss: 0.043892, accuracy: 0.0446
case acc: 0.04850887
case acc: 0.041865986
case acc: 0.04058912
case acc: 0.04756378
case acc: 0.039204177
case acc: 0.050024573
top acc: 0.0899 ::: bot acc: 0.0131
top acc: 0.0767 ::: bot acc: 0.0132
top acc: 0.0765 ::: bot acc: 0.0233
top acc: 0.0867 ::: bot acc: 0.0260
top acc: 0.0597 ::: bot acc: 0.0444
top acc: 0.0964 ::: bot acc: 0.0143
current epoch: 44
train loss is 0.048941
average val loss: 0.037440, accuracy: 0.0381
average test loss: 0.037626, accuracy: 0.0377
case acc: 0.03572049
case acc: 0.030290585
case acc: 0.037027553
case acc: 0.04143224
case acc: 0.04043333
case acc: 0.0415082
top acc: 0.0652 ::: bot acc: 0.0240
top acc: 0.0570 ::: bot acc: 0.0178
top acc: 0.0587 ::: bot acc: 0.0406
top acc: 0.0737 ::: bot acc: 0.0335
top acc: 0.0512 ::: bot acc: 0.0529
top acc: 0.0754 ::: bot acc: 0.0300
current epoch: 45
train loss is 0.041080
average val loss: 0.035308, accuracy: 0.0354
average test loss: 0.036271, accuracy: 0.0362
case acc: 0.032856625
case acc: 0.027678095
case acc: 0.037451338
case acc: 0.039257564
case acc: 0.04086649
case acc: 0.039366532
top acc: 0.0472 ::: bot acc: 0.0420
top acc: 0.0432 ::: bot acc: 0.0316
top acc: 0.0451 ::: bot acc: 0.0542
top acc: 0.0669 ::: bot acc: 0.0401
top acc: 0.0489 ::: bot acc: 0.0551
top acc: 0.0615 ::: bot acc: 0.0438
current epoch: 46
train loss is 0.037225
average val loss: 0.036481, accuracy: 0.0363
average test loss: 0.037771, accuracy: 0.0377
case acc: 0.035383195
case acc: 0.0289746
case acc: 0.0411683
case acc: 0.03810418
case acc: 0.042313155
case acc: 0.040014416
top acc: 0.0308 ::: bot acc: 0.0585
top acc: 0.0295 ::: bot acc: 0.0451
top acc: 0.0311 ::: bot acc: 0.0685
top acc: 0.0587 ::: bot acc: 0.0483
top acc: 0.0433 ::: bot acc: 0.0606
top acc: 0.0483 ::: bot acc: 0.0570
current epoch: 47
train loss is 0.036547
average val loss: 0.040919, accuracy: 0.0407
average test loss: 0.042206, accuracy: 0.0421
case acc: 0.04173504
case acc: 0.034027953
case acc: 0.048751645
case acc: 0.03915146
case acc: 0.046480637
case acc: 0.042581543
top acc: 0.0186 ::: bot acc: 0.0742
top acc: 0.0163 ::: bot acc: 0.0597
top acc: 0.0221 ::: bot acc: 0.0844
top acc: 0.0470 ::: bot acc: 0.0600
top acc: 0.0347 ::: bot acc: 0.0712
top acc: 0.0345 ::: bot acc: 0.0708
current epoch: 48
train loss is 0.038195
average val loss: 0.048169, accuracy: 0.0481
average test loss: 0.049962, accuracy: 0.0500
case acc: 0.05092005
case acc: 0.0427603
case acc: 0.060096078
case acc: 0.0444131
case acc: 0.053812847
case acc: 0.047968026
top acc: 0.0166 ::: bot acc: 0.0890
top acc: 0.0118 ::: bot acc: 0.0751
top acc: 0.0214 ::: bot acc: 0.1017
top acc: 0.0336 ::: bot acc: 0.0749
top acc: 0.0274 ::: bot acc: 0.0858
top acc: 0.0228 ::: bot acc: 0.0849
current epoch: 49
train loss is 0.040770
average val loss: 0.055420, accuracy: 0.0555
average test loss: 0.057966, accuracy: 0.0581
case acc: 0.057981905
case acc: 0.0518491
case acc: 0.07080335
case acc: 0.05146643
case acc: 0.06309296
case acc: 0.053239558
top acc: 0.0196 ::: bot acc: 0.0981
top acc: 0.0165 ::: bot acc: 0.0863
top acc: 0.0258 ::: bot acc: 0.1156
top acc: 0.0280 ::: bot acc: 0.0884
top acc: 0.0272 ::: bot acc: 0.0999
top acc: 0.0190 ::: bot acc: 0.0947
current epoch: 50
train loss is 0.042147
average val loss: 0.054545, accuracy: 0.0547
average test loss: 0.057014, accuracy: 0.0571
case acc: 0.053161696
case acc: 0.049444545
case acc: 0.070571996
case acc: 0.05247029
case acc: 0.066002674
case acc: 0.05100824
top acc: 0.0175 ::: bot acc: 0.0920
top acc: 0.0149 ::: bot acc: 0.0836
top acc: 0.0257 ::: bot acc: 0.1153
top acc: 0.0278 ::: bot acc: 0.0899
top acc: 0.0279 ::: bot acc: 0.1039
top acc: 0.0202 ::: bot acc: 0.0907
LME_Co_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5406 5406 5406
1.7082474 -0.6288155 0.16982092 -0.17269358
Validation: 606 606 606
Testing: 750 750 750
pre-processing time: 0.00022530555725097656
the split date is 2012-01-01
net initializing with time: 0.05350446701049805
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.163436
average val loss: 0.204613, accuracy: 0.2047
average test loss: 0.180068, accuracy: 0.1808
case acc: 0.23214968
case acc: 0.06758334
case acc: 0.12611315
case acc: 0.14197591
case acc: 0.046166886
case acc: 0.4708279
top acc: 0.1850 ::: bot acc: 0.2811
top acc: 0.0311 ::: bot acc: 0.1091
top acc: 0.0637 ::: bot acc: 0.1902
top acc: 0.0930 ::: bot acc: 0.1914
top acc: 0.0286 ::: bot acc: 0.0762
top acc: 0.4101 ::: bot acc: 0.5261
current epoch: 2
train loss is 0.168737
average val loss: 0.273163, accuracy: 0.2732
average test loss: 0.247108, accuracy: 0.2471
case acc: 0.29655555
case acc: 0.14534305
case acc: 0.19734375
case acc: 0.2159206
case acc: 0.1090381
case acc: 0.5185608
top acc: 0.2495 ::: bot acc: 0.3451
top acc: 0.1068 ::: bot acc: 0.1879
top acc: 0.1351 ::: bot acc: 0.2612
top acc: 0.1668 ::: bot acc: 0.2655
top acc: 0.0549 ::: bot acc: 0.1571
top acc: 0.4577 ::: bot acc: 0.5739
current epoch: 3
train loss is 0.147318
average val loss: 0.227521, accuracy: 0.2276
average test loss: 0.201707, accuracy: 0.2019
case acc: 0.24326777
case acc: 0.10985762
case acc: 0.15415072
case acc: 0.17573534
case acc: 0.07770343
case acc: 0.45094198
top acc: 0.1963 ::: bot acc: 0.2917
top acc: 0.0709 ::: bot acc: 0.1527
top acc: 0.0921 ::: bot acc: 0.2178
top acc: 0.1267 ::: bot acc: 0.2255
top acc: 0.0290 ::: bot acc: 0.1232
top acc: 0.3902 ::: bot acc: 0.5062
current epoch: 4
train loss is 0.123911
average val loss: 0.169956, accuracy: 0.1701
average test loss: 0.145800, accuracy: 0.1464
case acc: 0.17718148
case acc: 0.06357002
case acc: 0.09838981
case acc: 0.12286678
case acc: 0.046460196
case acc: 0.36982703
top acc: 0.1304 ::: bot acc: 0.2255
top acc: 0.0272 ::: bot acc: 0.1053
top acc: 0.0371 ::: bot acc: 0.1616
top acc: 0.0739 ::: bot acc: 0.1727
top acc: 0.0275 ::: bot acc: 0.0776
top acc: 0.3092 ::: bot acc: 0.4251
current epoch: 5
train loss is 0.115161
average val loss: 0.176732, accuracy: 0.1768
average test loss: 0.151615, accuracy: 0.1521
case acc: 0.1779904
case acc: 0.07994156
case acc: 0.10761622
case acc: 0.13450076
case acc: 0.05765766
case acc: 0.3546569
top acc: 0.1312 ::: bot acc: 0.2263
top acc: 0.0410 ::: bot acc: 0.1229
top acc: 0.0460 ::: bot acc: 0.1709
top acc: 0.0855 ::: bot acc: 0.1844
top acc: 0.0218 ::: bot acc: 0.0971
top acc: 0.2940 ::: bot acc: 0.4100
current epoch: 6
train loss is 0.104520
average val loss: 0.152169, accuracy: 0.1522
average test loss: 0.127843, accuracy: 0.1283
case acc: 0.14677261
case acc: 0.0661132
case acc: 0.08576009
case acc: 0.11388792
case acc: 0.04977822
case acc: 0.30765033
top acc: 0.1001 ::: bot acc: 0.1949
top acc: 0.0289 ::: bot acc: 0.1083
top acc: 0.0259 ::: bot acc: 0.1481
top acc: 0.0653 ::: bot acc: 0.1636
top acc: 0.0242 ::: bot acc: 0.0843
top acc: 0.2470 ::: bot acc: 0.3630
current epoch: 7
train loss is 0.096398
average val loss: 0.142975, accuracy: 0.1429
average test loss: 0.118656, accuracy: 0.1191
case acc: 0.1321014
case acc: 0.06670638
case acc: 0.07952939
case acc: 0.10833205
case acc: 0.051158868
case acc: 0.27681583
top acc: 0.0854 ::: bot acc: 0.1802
top acc: 0.0293 ::: bot acc: 0.1089
top acc: 0.0212 ::: bot acc: 0.1411
top acc: 0.0599 ::: bot acc: 0.1580
top acc: 0.0233 ::: bot acc: 0.0867
top acc: 0.2162 ::: bot acc: 0.3322
current epoch: 8
train loss is 0.086980
average val loss: 0.118198, accuracy: 0.1181
average test loss: 0.095551, accuracy: 0.0961
case acc: 0.101779364
case acc: 0.052494146
case acc: 0.06172671
case acc: 0.08665882
case acc: 0.044367306
case acc: 0.2297586
top acc: 0.0551 ::: bot acc: 0.1499
top acc: 0.0189 ::: bot acc: 0.0929
top acc: 0.0149 ::: bot acc: 0.1177
top acc: 0.0400 ::: bot acc: 0.1356
top acc: 0.0316 ::: bot acc: 0.0726
top acc: 0.1691 ::: bot acc: 0.2851
current epoch: 9
train loss is 0.077605
average val loss: 0.090149, accuracy: 0.0896
average test loss: 0.071782, accuracy: 0.0725
case acc: 0.068921015
case acc: 0.036975395
case acc: 0.049762268
case acc: 0.06278506
case acc: 0.038373064
case acc: 0.17799436
top acc: 0.0231 ::: bot acc: 0.1165
top acc: 0.0160 ::: bot acc: 0.0713
top acc: 0.0351 ::: bot acc: 0.0893
top acc: 0.0262 ::: bot acc: 0.1068
top acc: 0.0504 ::: bot acc: 0.0531
top acc: 0.1174 ::: bot acc: 0.2334
current epoch: 10
train loss is 0.071783
average val loss: 0.066474, accuracy: 0.0656
average test loss: 0.055086, accuracy: 0.0552
case acc: 0.04593392
case acc: 0.029591057
case acc: 0.047099415
case acc: 0.04226778
case acc: 0.038583223
case acc: 0.1278574
top acc: 0.0119 ::: bot acc: 0.0877
top acc: 0.0339 ::: bot acc: 0.0496
top acc: 0.0621 ::: bot acc: 0.0621
top acc: 0.0246 ::: bot acc: 0.0771
top acc: 0.0699 ::: bot acc: 0.0339
top acc: 0.0673 ::: bot acc: 0.1832
current epoch: 11
train loss is 0.070664
average val loss: 0.058858, accuracy: 0.0577
average test loss: 0.049265, accuracy: 0.0493
case acc: 0.043359816
case acc: 0.029544005
case acc: 0.047574658
case acc: 0.037452385
case acc: 0.038724817
case acc: 0.09933669
top acc: 0.0141 ::: bot acc: 0.0828
top acc: 0.0371 ::: bot acc: 0.0466
top acc: 0.0691 ::: bot acc: 0.0551
top acc: 0.0334 ::: bot acc: 0.0653
top acc: 0.0707 ::: bot acc: 0.0333
top acc: 0.0395 ::: bot acc: 0.1544
current epoch: 12
train loss is 0.068904
average val loss: 0.062351, accuracy: 0.0615
average test loss: 0.049207, accuracy: 0.0493
case acc: 0.05188523
case acc: 0.031670574
case acc: 0.046941582
case acc: 0.038696755
case acc: 0.037809845
case acc: 0.088951886
top acc: 0.0124 ::: bot acc: 0.0963
top acc: 0.0244 ::: bot acc: 0.0594
top acc: 0.0593 ::: bot acc: 0.0648
top acc: 0.0292 ::: bot acc: 0.0695
top acc: 0.0558 ::: bot acc: 0.0482
top acc: 0.0299 ::: bot acc: 0.1436
current epoch: 13
train loss is 0.064241
average val loss: 0.069821, accuracy: 0.0693
average test loss: 0.052597, accuracy: 0.0532
case acc: 0.06470971
case acc: 0.039191414
case acc: 0.047407392
case acc: 0.042654697
case acc: 0.041788362
case acc: 0.083191186
top acc: 0.0196 ::: bot acc: 0.1119
top acc: 0.0150 ::: bot acc: 0.0753
top acc: 0.0464 ::: bot acc: 0.0777
top acc: 0.0242 ::: bot acc: 0.0778
top acc: 0.0382 ::: bot acc: 0.0657
top acc: 0.0256 ::: bot acc: 0.1372
current epoch: 14
train loss is 0.059875
average val loss: 0.075910, accuracy: 0.0756
average test loss: 0.056274, accuracy: 0.0570
case acc: 0.073545106
case acc: 0.04865087
case acc: 0.049130615
case acc: 0.047075648
case acc: 0.047631312
case acc: 0.07611065
top acc: 0.0274 ::: bot acc: 0.1213
top acc: 0.0167 ::: bot acc: 0.0885
top acc: 0.0367 ::: bot acc: 0.0875
top acc: 0.0228 ::: bot acc: 0.0850
top acc: 0.0267 ::: bot acc: 0.0799
top acc: 0.0209 ::: bot acc: 0.1289
current epoch: 15
train loss is 0.054574
average val loss: 0.077694, accuracy: 0.0775
average test loss: 0.057316, accuracy: 0.0581
case acc: 0.07460053
case acc: 0.055439837
case acc: 0.050216403
case acc: 0.04945791
case acc: 0.052247595
case acc: 0.06666339
top acc: 0.0284 ::: bot acc: 0.1223
top acc: 0.0208 ::: bot acc: 0.0967
top acc: 0.0324 ::: bot acc: 0.0917
top acc: 0.0228 ::: bot acc: 0.0887
top acc: 0.0231 ::: bot acc: 0.0886
top acc: 0.0171 ::: bot acc: 0.1168
current epoch: 16
train loss is 0.049364
average val loss: 0.077584, accuracy: 0.0775
average test loss: 0.057186, accuracy: 0.0580
case acc: 0.071443625
case acc: 0.060196884
case acc: 0.050679483
case acc: 0.051049795
case acc: 0.055595685
case acc: 0.059139583
top acc: 0.0255 ::: bot acc: 0.1190
top acc: 0.0242 ::: bot acc: 0.1021
top acc: 0.0306 ::: bot acc: 0.0933
top acc: 0.0230 ::: bot acc: 0.0910
top acc: 0.0222 ::: bot acc: 0.0941
top acc: 0.0185 ::: bot acc: 0.1049
current epoch: 17
train loss is 0.044297
average val loss: 0.075676, accuracy: 0.0758
average test loss: 0.055814, accuracy: 0.0566
case acc: 0.06538108
case acc: 0.06192538
case acc: 0.0504558
case acc: 0.051330436
case acc: 0.05655749
case acc: 0.054067224
top acc: 0.0202 ::: bot acc: 0.1126
top acc: 0.0255 ::: bot acc: 0.1041
top acc: 0.0313 ::: bot acc: 0.0926
top acc: 0.0230 ::: bot acc: 0.0914
top acc: 0.0221 ::: bot acc: 0.0956
top acc: 0.0232 ::: bot acc: 0.0950
current epoch: 18
train loss is 0.041807
average val loss: 0.076311, accuracy: 0.0764
average test loss: 0.056325, accuracy: 0.0570
case acc: 0.061662335
case acc: 0.06459675
case acc: 0.05097836
case acc: 0.053676043
case acc: 0.057891272
case acc: 0.053263582
top acc: 0.0172 ::: bot acc: 0.1085
top acc: 0.0274 ::: bot acc: 0.1071
top acc: 0.0296 ::: bot acc: 0.0943
top acc: 0.0234 ::: bot acc: 0.0947
top acc: 0.0220 ::: bot acc: 0.0976
top acc: 0.0244 ::: bot acc: 0.0932
current epoch: 19
train loss is 0.040216
average val loss: 0.074868, accuracy: 0.0749
average test loss: 0.055192, accuracy: 0.0558
case acc: 0.05603339
case acc: 0.06379378
case acc: 0.050710358
case acc: 0.05439687
case acc: 0.05646111
case acc: 0.05328975
top acc: 0.0138 ::: bot acc: 0.1018
top acc: 0.0268 ::: bot acc: 0.1061
top acc: 0.0306 ::: bot acc: 0.0933
top acc: 0.0237 ::: bot acc: 0.0957
top acc: 0.0221 ::: bot acc: 0.0954
top acc: 0.0244 ::: bot acc: 0.0933
current epoch: 20
train loss is 0.039686
average val loss: 0.071609, accuracy: 0.0716
average test loss: 0.052766, accuracy: 0.0533
case acc: 0.04956207
case acc: 0.06031827
case acc: 0.049759842
case acc: 0.053447526
case acc: 0.05325636
case acc: 0.053279553
top acc: 0.0118 ::: bot acc: 0.0931
top acc: 0.0243 ::: bot acc: 0.1022
top acc: 0.0340 ::: bot acc: 0.0899
top acc: 0.0235 ::: bot acc: 0.0944
top acc: 0.0229 ::: bot acc: 0.0901
top acc: 0.0244 ::: bot acc: 0.0933
current epoch: 21
train loss is 0.038811
average val loss: 0.066806, accuracy: 0.0668
average test loss: 0.049416, accuracy: 0.0498
case acc: 0.043573216
case acc: 0.054322015
case acc: 0.04842323
case acc: 0.050643288
case acc: 0.048809364
case acc: 0.053074125
top acc: 0.0135 ::: bot acc: 0.0834
top acc: 0.0199 ::: bot acc: 0.0954
top acc: 0.0398 ::: bot acc: 0.0843
top acc: 0.0230 ::: bot acc: 0.0904
top acc: 0.0255 ::: bot acc: 0.0822
top acc: 0.0248 ::: bot acc: 0.0928
current epoch: 22
train loss is 0.038716
average val loss: 0.062069, accuracy: 0.0620
average test loss: 0.046473, accuracy: 0.0468
case acc: 0.03977286
case acc: 0.0481309
case acc: 0.047474932
case acc: 0.04732807
case acc: 0.04501491
case acc: 0.052900083
top acc: 0.0193 ::: bot acc: 0.0749
top acc: 0.0164 ::: bot acc: 0.0880
top acc: 0.0459 ::: bot acc: 0.0783
top acc: 0.0228 ::: bot acc: 0.0856
top acc: 0.0308 ::: bot acc: 0.0739
top acc: 0.0252 ::: bot acc: 0.0924
current epoch: 23
train loss is 0.038638
average val loss: 0.054836, accuracy: 0.0547
average test loss: 0.042605, accuracy: 0.0427
case acc: 0.03652862
case acc: 0.03970182
case acc: 0.04694824
case acc: 0.041505966
case acc: 0.04048128
case acc: 0.05082317
top acc: 0.0297 ::: bot acc: 0.0644
top acc: 0.0146 ::: bot acc: 0.0763
top acc: 0.0559 ::: bot acc: 0.0683
top acc: 0.0251 ::: bot acc: 0.0758
top acc: 0.0414 ::: bot acc: 0.0620
top acc: 0.0292 ::: bot acc: 0.0873
current epoch: 24
train loss is 0.038271
average val loss: 0.048959, accuracy: 0.0486
average test loss: 0.040299, accuracy: 0.0401
case acc: 0.035047036
case acc: 0.033767972
case acc: 0.047235284
case acc: 0.037481524
case acc: 0.03803016
case acc: 0.04915258
top acc: 0.0372 ::: bot acc: 0.0569
top acc: 0.0194 ::: bot acc: 0.0652
top acc: 0.0651 ::: bot acc: 0.0590
top acc: 0.0333 ::: bot acc: 0.0656
top acc: 0.0524 ::: bot acc: 0.0510
top acc: 0.0337 ::: bot acc: 0.0825
current epoch: 25
train loss is 0.038291
average val loss: 0.044073, accuracy: 0.0436
average test loss: 0.039257, accuracy: 0.0391
case acc: 0.034690905
case acc: 0.030188434
case acc: 0.048267897
case acc: 0.036059834
case acc: 0.037605114
case acc: 0.047592957
top acc: 0.0424 ::: bot acc: 0.0517
top acc: 0.0302 ::: bot acc: 0.0537
top acc: 0.0743 ::: bot acc: 0.0499
top acc: 0.0447 ::: bot acc: 0.0542
top acc: 0.0633 ::: bot acc: 0.0398
top acc: 0.0386 ::: bot acc: 0.0777
current epoch: 26
train loss is 0.037829
average val loss: 0.040096, accuracy: 0.0394
average test loss: 0.039549, accuracy: 0.0395
case acc: 0.034759853
case acc: 0.030140143
case acc: 0.049779713
case acc: 0.03723691
case acc: 0.039236862
case acc: 0.045965977
top acc: 0.0472 ::: bot acc: 0.0469
top acc: 0.0428 ::: bot acc: 0.0412
top acc: 0.0843 ::: bot acc: 0.0398
top acc: 0.0580 ::: bot acc: 0.0409
top acc: 0.0752 ::: bot acc: 0.0280
top acc: 0.0446 ::: bot acc: 0.0717
current epoch: 27
train loss is 0.037593
average val loss: 0.037284, accuracy: 0.0363
average test loss: 0.042184, accuracy: 0.0426
case acc: 0.03521608
case acc: 0.03580761
case acc: 0.0537348
case acc: 0.042370386
case acc: 0.044614755
case acc: 0.043844257
top acc: 0.0531 ::: bot acc: 0.0411
top acc: 0.0579 ::: bot acc: 0.0287
top acc: 0.0969 ::: bot acc: 0.0276
top acc: 0.0749 ::: bot acc: 0.0255
top acc: 0.0895 ::: bot acc: 0.0160
top acc: 0.0545 ::: bot acc: 0.0617
current epoch: 28
train loss is 0.039195
average val loss: 0.037400, accuracy: 0.0365
average test loss: 0.047893, accuracy: 0.0487
case acc: 0.03612611
case acc: 0.04532458
case acc: 0.060953766
case acc: 0.05315858
case acc: 0.053998742
case acc: 0.042847
top acc: 0.0587 ::: bot acc: 0.0355
top acc: 0.0740 ::: bot acc: 0.0247
top acc: 0.1107 ::: bot acc: 0.0214
top acc: 0.0943 ::: bot acc: 0.0188
top acc: 0.1046 ::: bot acc: 0.0135
top acc: 0.0676 ::: bot acc: 0.0486
current epoch: 29
train loss is 0.043523
average val loss: 0.039586, accuracy: 0.0390
average test loss: 0.053120, accuracy: 0.0542
case acc: 0.03597177
case acc: 0.05255672
case acc: 0.06653126
case acc: 0.064827845
case acc: 0.061880175
case acc: 0.04332725
top acc: 0.0578 ::: bot acc: 0.0363
top acc: 0.0847 ::: bot acc: 0.0248
top acc: 0.1191 ::: bot acc: 0.0209
top acc: 0.1097 ::: bot acc: 0.0227
top acc: 0.1148 ::: bot acc: 0.0168
top acc: 0.0773 ::: bot acc: 0.0389
current epoch: 30
train loss is 0.052080
average val loss: 0.038681, accuracy: 0.0384
average test loss: 0.047708, accuracy: 0.0492
case acc: 0.035467636
case acc: 0.044919968
case acc: 0.05833102
case acc: 0.060735337
case acc: 0.052804127
case acc: 0.04281559
top acc: 0.0345 ::: bot acc: 0.0597
top acc: 0.0733 ::: bot acc: 0.0249
top acc: 0.1059 ::: bot acc: 0.0230
top acc: 0.1046 ::: bot acc: 0.0206
top acc: 0.1030 ::: bot acc: 0.0133
top acc: 0.0689 ::: bot acc: 0.0472
current epoch: 31
train loss is 0.063124
average val loss: 0.064960, accuracy: 0.0647
average test loss: 0.049480, accuracy: 0.0509
case acc: 0.08384175
case acc: 0.03912899
case acc: 0.048024513
case acc: 0.036058437
case acc: 0.041519355
case acc: 0.056967948
top acc: 0.0375 ::: bot acc: 0.1318
top acc: 0.0147 ::: bot acc: 0.0755
top acc: 0.0412 ::: bot acc: 0.0828
top acc: 0.0466 ::: bot acc: 0.0522
top acc: 0.0380 ::: bot acc: 0.0650
top acc: 0.0201 ::: bot acc: 0.1010
current epoch: 32
train loss is 0.071113
average val loss: 0.109428, accuracy: 0.1096
average test loss: 0.084764, accuracy: 0.0851
case acc: 0.12911944
case acc: 0.088539325
case acc: 0.07406781
case acc: 0.06141784
case acc: 0.07664326
case acc: 0.08090468
top acc: 0.0828 ::: bot acc: 0.1772
top acc: 0.0486 ::: bot acc: 0.1326
top acc: 0.0182 ::: bot acc: 0.1345
top acc: 0.0254 ::: bot acc: 0.1053
top acc: 0.0288 ::: bot acc: 0.1218
top acc: 0.0239 ::: bot acc: 0.1347
current epoch: 33
train loss is 0.057355
average val loss: 0.095881, accuracy: 0.0963
average test loss: 0.072553, accuracy: 0.0732
case acc: 0.1031532
case acc: 0.08243123
case acc: 0.064662345
case acc: 0.057074286
case acc: 0.0716887
case acc: 0.059967395
top acc: 0.0568 ::: bot acc: 0.1512
top acc: 0.0428 ::: bot acc: 0.1262
top acc: 0.0152 ::: bot acc: 0.1220
top acc: 0.0242 ::: bot acc: 0.0995
top acc: 0.0258 ::: bot acc: 0.1159
top acc: 0.0180 ::: bot acc: 0.1063
current epoch: 34
train loss is 0.052498
average val loss: 0.087468, accuracy: 0.0879
average test loss: 0.065292, accuracy: 0.0658
case acc: 0.0806372
case acc: 0.07760179
case acc: 0.0589276
case acc: 0.056814175
case acc: 0.06718416
case acc: 0.053889226
top acc: 0.0344 ::: bot acc: 0.1286
top acc: 0.0384 ::: bot acc: 0.1212
top acc: 0.0155 ::: bot acc: 0.1133
top acc: 0.0241 ::: bot acc: 0.0992
top acc: 0.0234 ::: bot acc: 0.1104
top acc: 0.0235 ::: bot acc: 0.0946
current epoch: 35
train loss is 0.050084
average val loss: 0.068806, accuracy: 0.0692
average test loss: 0.050871, accuracy: 0.0513
case acc: 0.05019549
case acc: 0.058998886
case acc: 0.050150458
case acc: 0.047464125
case acc: 0.052434787
case acc: 0.048361458
top acc: 0.0120 ::: bot acc: 0.0942
top acc: 0.0233 ::: bot acc: 0.1009
top acc: 0.0324 ::: bot acc: 0.0916
top acc: 0.0226 ::: bot acc: 0.0860
top acc: 0.0230 ::: bot acc: 0.0886
top acc: 0.0358 ::: bot acc: 0.0803
current epoch: 36
train loss is 0.043770
average val loss: 0.050975, accuracy: 0.0513
average test loss: 0.041018, accuracy: 0.0408
case acc: 0.035457723
case acc: 0.03906475
case acc: 0.046930537
case acc: 0.037993886
case acc: 0.040070105
case acc: 0.045135047
top acc: 0.0342 ::: bot acc: 0.0600
top acc: 0.0149 ::: bot acc: 0.0753
top acc: 0.0574 ::: bot acc: 0.0666
top acc: 0.0312 ::: bot acc: 0.0677
top acc: 0.0416 ::: bot acc: 0.0612
top acc: 0.0476 ::: bot acc: 0.0685
current epoch: 37
train loss is 0.041203
average val loss: 0.039506, accuracy: 0.0393
average test loss: 0.039851, accuracy: 0.0391
case acc: 0.03696981
case acc: 0.029566461
case acc: 0.049459938
case acc: 0.036724765
case acc: 0.038367577
case acc: 0.043349966
top acc: 0.0618 ::: bot acc: 0.0324
top acc: 0.0366 ::: bot acc: 0.0476
top acc: 0.0826 ::: bot acc: 0.0414
top acc: 0.0539 ::: bot acc: 0.0450
top acc: 0.0709 ::: bot acc: 0.0319
top acc: 0.0582 ::: bot acc: 0.0579
current epoch: 38
train loss is 0.040229
average val loss: 0.036796, accuracy: 0.0359
average test loss: 0.045120, accuracy: 0.0451
case acc: 0.043238763
case acc: 0.03732418
case acc: 0.05630983
case acc: 0.04290611
case acc: 0.047898117
case acc: 0.04294703
top acc: 0.0769 ::: bot acc: 0.0210
top acc: 0.0606 ::: bot acc: 0.0278
top acc: 0.1023 ::: bot acc: 0.0242
top acc: 0.0757 ::: bot acc: 0.0251
top acc: 0.0956 ::: bot acc: 0.0132
top acc: 0.0647 ::: bot acc: 0.0514
current epoch: 39
train loss is 0.040298
average val loss: 0.038118, accuracy: 0.0370
average test loss: 0.050600, accuracy: 0.0513
case acc: 0.044335894
case acc: 0.046826385
case acc: 0.062808
case acc: 0.052214347
case acc: 0.058999535
case acc: 0.042859614
top acc: 0.0790 ::: bot acc: 0.0201
top acc: 0.0762 ::: bot acc: 0.0248
top acc: 0.1136 ::: bot acc: 0.0209
top acc: 0.0927 ::: bot acc: 0.0188
top acc: 0.1110 ::: bot acc: 0.0155
top acc: 0.0683 ::: bot acc: 0.0478
current epoch: 40
train loss is 0.046018
average val loss: 0.037207, accuracy: 0.0361
average test loss: 0.047001, accuracy: 0.0481
case acc: 0.03657763
case acc: 0.043769818
case acc: 0.058289908
case acc: 0.052280407
case acc: 0.054376133
case acc: 0.043037526
top acc: 0.0603 ::: bot acc: 0.0339
top acc: 0.0713 ::: bot acc: 0.0254
top acc: 0.1059 ::: bot acc: 0.0229
top acc: 0.0928 ::: bot acc: 0.0188
top acc: 0.1050 ::: bot acc: 0.0137
top acc: 0.0626 ::: bot acc: 0.0535
current epoch: 41
train loss is 0.049769
average val loss: 0.044246, accuracy: 0.0436
average test loss: 0.039619, accuracy: 0.0405
case acc: 0.041379742
case acc: 0.029560383
case acc: 0.047813453
case acc: 0.03882258
case acc: 0.038062543
case acc: 0.047361407
top acc: 0.0163 ::: bot acc: 0.0790
top acc: 0.0366 ::: bot acc: 0.0475
top acc: 0.0705 ::: bot acc: 0.0535
top acc: 0.0653 ::: bot acc: 0.0335
top acc: 0.0688 ::: bot acc: 0.0340
top acc: 0.0388 ::: bot acc: 0.0773
current epoch: 42
train loss is 0.050090
average val loss: 0.071071, accuracy: 0.0711
average test loss: 0.052303, accuracy: 0.0531
case acc: 0.07308754
case acc: 0.050675765
case acc: 0.05094513
case acc: 0.0395081
case acc: 0.047092587
case acc: 0.05713406
top acc: 0.0272 ::: bot acc: 0.1209
top acc: 0.0177 ::: bot acc: 0.0913
top acc: 0.0296 ::: bot acc: 0.0944
top acc: 0.0274 ::: bot acc: 0.0719
top acc: 0.0269 ::: bot acc: 0.0788
top acc: 0.0198 ::: bot acc: 0.1014
current epoch: 43
train loss is 0.047037
average val loss: 0.078799, accuracy: 0.0791
average test loss: 0.058083, accuracy: 0.0588
case acc: 0.074833065
case acc: 0.06465481
case acc: 0.054839794
case acc: 0.04748323
case acc: 0.056762654
case acc: 0.054095224
top acc: 0.0289 ::: bot acc: 0.1227
top acc: 0.0275 ::: bot acc: 0.1073
top acc: 0.0201 ::: bot acc: 0.1049
top acc: 0.0226 ::: bot acc: 0.0861
top acc: 0.0224 ::: bot acc: 0.0953
top acc: 0.0232 ::: bot acc: 0.0952
current epoch: 44
train loss is 0.045493
average val loss: 0.072978, accuracy: 0.0735
average test loss: 0.053871, accuracy: 0.0543
case acc: 0.05855135
case acc: 0.06295294
case acc: 0.05241308
case acc: 0.04775771
case acc: 0.05537794
case acc: 0.048854604
top acc: 0.0153 ::: bot acc: 0.1051
top acc: 0.0262 ::: bot acc: 0.1054
top acc: 0.0251 ::: bot acc: 0.0989
top acc: 0.0225 ::: bot acc: 0.0865
top acc: 0.0225 ::: bot acc: 0.0932
top acc: 0.0342 ::: bot acc: 0.0819
current epoch: 45
train loss is 0.045847
average val loss: 0.051904, accuracy: 0.0526
average test loss: 0.041525, accuracy: 0.0412
case acc: 0.036881905
case acc: 0.041240588
case acc: 0.04691896
case acc: 0.0373739
case acc: 0.041475352
case acc: 0.04354761
top acc: 0.0280 ::: bot acc: 0.0662
top acc: 0.0146 ::: bot acc: 0.0788
top acc: 0.0531 ::: bot acc: 0.0708
top acc: 0.0331 ::: bot acc: 0.0656
top acc: 0.0375 ::: bot acc: 0.0652
top acc: 0.0560 ::: bot acc: 0.0601
current epoch: 46
train loss is 0.042172
average val loss: 0.038590, accuracy: 0.0387
average test loss: 0.040192, accuracy: 0.0391
case acc: 0.037134953
case acc: 0.029631268
case acc: 0.0494212
case acc: 0.037269313
case acc: 0.038249828
case acc: 0.042864516
top acc: 0.0624 ::: bot acc: 0.0317
top acc: 0.0365 ::: bot acc: 0.0478
top acc: 0.0824 ::: bot acc: 0.0415
top acc: 0.0583 ::: bot acc: 0.0405
top acc: 0.0703 ::: bot acc: 0.0324
top acc: 0.0707 ::: bot acc: 0.0454
current epoch: 47
train loss is 0.040347
average val loss: 0.036437, accuracy: 0.0359
average test loss: 0.045693, accuracy: 0.0454
case acc: 0.044059943
case acc: 0.037131738
case acc: 0.056033418
case acc: 0.0444052
case acc: 0.04771995
case acc: 0.043021176
top acc: 0.0784 ::: bot acc: 0.0204
top acc: 0.0602 ::: bot acc: 0.0280
top acc: 0.1017 ::: bot acc: 0.0246
top acc: 0.0788 ::: bot acc: 0.0234
top acc: 0.0954 ::: bot acc: 0.0131
top acc: 0.0739 ::: bot acc: 0.0422
current epoch: 48
train loss is 0.038995
average val loss: 0.037071, accuracy: 0.0361
average test loss: 0.048203, accuracy: 0.0488
case acc: 0.042448256
case acc: 0.0432354
case acc: 0.058998007
case acc: 0.05017523
case acc: 0.055131365
case acc: 0.042852994
top acc: 0.0753 ::: bot acc: 0.0218
top acc: 0.0705 ::: bot acc: 0.0256
top acc: 0.1071 ::: bot acc: 0.0225
top acc: 0.0894 ::: bot acc: 0.0193
top acc: 0.1060 ::: bot acc: 0.0139
top acc: 0.0687 ::: bot acc: 0.0475
current epoch: 49
train loss is 0.042499
average val loss: 0.036829, accuracy: 0.0359
average test loss: 0.043091, accuracy: 0.0440
case acc: 0.035268914
case acc: 0.037458766
case acc: 0.05282048
case acc: 0.04713624
case acc: 0.047766708
case acc: 0.043428
top acc: 0.0527 ::: bot acc: 0.0414
top acc: 0.0608 ::: bot acc: 0.0279
top acc: 0.0944 ::: bot acc: 0.0295
top acc: 0.0840 ::: bot acc: 0.0210
top acc: 0.0955 ::: bot acc: 0.0131
top acc: 0.0574 ::: bot acc: 0.0587
current epoch: 50
train loss is 0.043181
average val loss: 0.045148, accuracy: 0.0446
average test loss: 0.039386, accuracy: 0.0402
case acc: 0.041007068
case acc: 0.030025234
case acc: 0.04718256
case acc: 0.037946485
case acc: 0.037649043
case acc: 0.047152344
top acc: 0.0168 ::: bot acc: 0.0782
top acc: 0.0321 ::: bot acc: 0.0523
top acc: 0.0654 ::: bot acc: 0.0584
top acc: 0.0615 ::: bot acc: 0.0374
top acc: 0.0653 ::: bot acc: 0.0375
top acc: 0.0397 ::: bot acc: 0.0765
LME_Co_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5394 5394 5394
1.7082474 -0.6288155 0.16982092 -0.17269358
Validation: 600 600 600
Testing: 768 768 768
pre-processing time: 0.0004050731658935547
the split date is 2012-07-01
net initializing with time: 0.003526926040649414
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.163238
average val loss: 0.070599, accuracy: 0.0692
average test loss: 0.070121, accuracy: 0.0710
case acc: 0.07556543
case acc: 0.12284644
case acc: 0.087197825
case acc: 0.03945163
case acc: 0.05891297
case acc: 0.042089358
top acc: 0.0327 ::: bot acc: 0.1151
top acc: 0.0849 ::: bot acc: 0.1581
top acc: 0.0378 ::: bot acc: 0.1363
top acc: 0.0144 ::: bot acc: 0.0650
top acc: 0.0164 ::: bot acc: 0.1076
top acc: 0.0588 ::: bot acc: 0.0544
current epoch: 2
train loss is 0.089910
average val loss: 0.145404, accuracy: 0.1454
average test loss: 0.145014, accuracy: 0.1453
case acc: 0.15331106
case acc: 0.21002576
case acc: 0.17306942
case acc: 0.11898792
case acc: 0.13898279
case acc: 0.07714133
top acc: 0.0971 ::: bot acc: 0.2001
top acc: 0.1723 ::: bot acc: 0.2453
top acc: 0.1185 ::: bot acc: 0.2260
top acc: 0.0886 ::: bot acc: 0.1467
top acc: 0.0897 ::: bot acc: 0.1909
top acc: 0.0343 ::: bot acc: 0.1321
current epoch: 3
train loss is 0.121592
average val loss: 0.136757, accuracy: 0.1368
average test loss: 0.136258, accuracy: 0.1365
case acc: 0.14158708
case acc: 0.19847947
case acc: 0.16659828
case acc: 0.11134909
case acc: 0.13370526
case acc: 0.06733483
top acc: 0.0853 ::: bot acc: 0.1882
top acc: 0.1613 ::: bot acc: 0.2336
top acc: 0.1122 ::: bot acc: 0.2200
top acc: 0.0805 ::: bot acc: 0.1392
top acc: 0.0845 ::: bot acc: 0.1855
top acc: 0.0274 ::: bot acc: 0.1206
current epoch: 4
train loss is 0.140731
average val loss: 0.053826, accuracy: 0.0532
average test loss: 0.052530, accuracy: 0.0539
case acc: 0.041809916
case acc: 0.032379188
case acc: 0.039304186
case acc: 0.055232625
case acc: 0.046870288
case acc: 0.107834175
top acc: 0.0887 ::: bot acc: 0.0149
top acc: 0.0188 ::: bot acc: 0.0560
top acc: 0.0602 ::: bot acc: 0.0472
top acc: 0.0865 ::: bot acc: 0.0272
top acc: 0.0813 ::: bot acc: 0.0251
top acc: 0.1563 ::: bot acc: 0.0557
current epoch: 5
train loss is 0.070225
average val loss: 0.052454, accuracy: 0.0504
average test loss: 0.051289, accuracy: 0.0530
case acc: 0.04940754
case acc: 0.08102512
case acc: 0.063822314
case acc: 0.025875695
case acc: 0.04587059
case acc: 0.051806934
top acc: 0.0278 ::: bot acc: 0.0790
top acc: 0.0438 ::: bot acc: 0.1164
top acc: 0.0258 ::: bot acc: 0.1091
top acc: 0.0170 ::: bot acc: 0.0432
top acc: 0.0172 ::: bot acc: 0.0876
top acc: 0.0838 ::: bot acc: 0.0338
current epoch: 6
train loss is 0.067825
average val loss: 0.081198, accuracy: 0.0810
average test loss: 0.080975, accuracy: 0.0814
case acc: 0.08124707
case acc: 0.11956024
case acc: 0.098269075
case acc: 0.06380877
case acc: 0.08421144
case acc: 0.041426085
top acc: 0.0361 ::: bot acc: 0.1224
top acc: 0.0823 ::: bot acc: 0.1551
top acc: 0.0458 ::: bot acc: 0.1505
top acc: 0.0322 ::: bot acc: 0.0923
top acc: 0.0371 ::: bot acc: 0.1349
top acc: 0.0322 ::: bot acc: 0.0799
current epoch: 7
train loss is 0.081241
average val loss: 0.063979, accuracy: 0.0630
average test loss: 0.063383, accuracy: 0.0643
case acc: 0.06201242
case acc: 0.09091221
case acc: 0.07653636
case acc: 0.04837545
case acc: 0.06801628
case acc: 0.039784685
top acc: 0.0267 ::: bot acc: 0.0984
top acc: 0.0536 ::: bot acc: 0.1266
top acc: 0.0307 ::: bot acc: 0.1254
top acc: 0.0187 ::: bot acc: 0.0760
top acc: 0.0228 ::: bot acc: 0.1178
top acc: 0.0477 ::: bot acc: 0.0648
current epoch: 8
train loss is 0.075734
average val loss: 0.044855, accuracy: 0.0432
average test loss: 0.042824, accuracy: 0.0440
case acc: 0.041747622
case acc: 0.051274832
case acc: 0.049011473
case acc: 0.027640246
case acc: 0.046133384
case acc: 0.048266493
top acc: 0.0406 ::: bot acc: 0.0614
top acc: 0.0183 ::: bot acc: 0.0849
top acc: 0.0250 ::: bot acc: 0.0871
top acc: 0.0147 ::: bot acc: 0.0472
top acc: 0.0169 ::: bot acc: 0.0881
top acc: 0.0756 ::: bot acc: 0.0400
current epoch: 9
train loss is 0.065441
average val loss: 0.041148, accuracy: 0.0396
average test loss: 0.038500, accuracy: 0.0391
case acc: 0.03831184
case acc: 0.03736783
case acc: 0.041624673
case acc: 0.024850767
case acc: 0.041625097
case acc: 0.05087903
top acc: 0.0552 ::: bot acc: 0.0468
top acc: 0.0158 ::: bot acc: 0.0655
top acc: 0.0361 ::: bot acc: 0.0705
top acc: 0.0222 ::: bot acc: 0.0393
top acc: 0.0221 ::: bot acc: 0.0788
top acc: 0.0816 ::: bot acc: 0.0359
current epoch: 10
train loss is 0.060235
average val loss: 0.039854, accuracy: 0.0381
average test loss: 0.036913, accuracy: 0.0365
case acc: 0.03751172
case acc: 0.028231997
case acc: 0.03893047
case acc: 0.023321578
case acc: 0.03775858
case acc: 0.053514324
top acc: 0.0688 ::: bot acc: 0.0330
top acc: 0.0263 ::: bot acc: 0.0468
top acc: 0.0520 ::: bot acc: 0.0544
top acc: 0.0309 ::: bot acc: 0.0309
top acc: 0.0319 ::: bot acc: 0.0681
top acc: 0.0871 ::: bot acc: 0.0326
current epoch: 11
train loss is 0.056851
average val loss: 0.040500, accuracy: 0.0386
average test loss: 0.037821, accuracy: 0.0366
case acc: 0.039007533
case acc: 0.026597712
case acc: 0.039517008
case acc: 0.023163883
case acc: 0.03611674
case acc: 0.05519045
top acc: 0.0795 ::: bot acc: 0.0221
top acc: 0.0421 ::: bot acc: 0.0310
top acc: 0.0656 ::: bot acc: 0.0408
top acc: 0.0381 ::: bot acc: 0.0239
top acc: 0.0417 ::: bot acc: 0.0582
top acc: 0.0903 ::: bot acc: 0.0311
current epoch: 12
train loss is 0.055230
average val loss: 0.044716, accuracy: 0.0430
average test loss: 0.042827, accuracy: 0.0412
case acc: 0.04523363
case acc: 0.03339037
case acc: 0.04538818
case acc: 0.025869938
case acc: 0.03734586
case acc: 0.060025163
top acc: 0.0945 ::: bot acc: 0.0114
top acc: 0.0621 ::: bot acc: 0.0149
top acc: 0.0836 ::: bot acc: 0.0248
top acc: 0.0509 ::: bot acc: 0.0122
top acc: 0.0578 ::: bot acc: 0.0421
top acc: 0.0984 ::: bot acc: 0.0293
current epoch: 13
train loss is 0.055072
average val loss: 0.050690, accuracy: 0.0492
average test loss: 0.049712, accuracy: 0.0485
case acc: 0.05316115
case acc: 0.044096068
case acc: 0.054625154
case acc: 0.032667775
case acc: 0.04273926
case acc: 0.06373505
top acc: 0.1060 ::: bot acc: 0.0117
top acc: 0.0788 ::: bot acc: 0.0132
top acc: 0.0991 ::: bot acc: 0.0212
top acc: 0.0621 ::: bot acc: 0.0102
top acc: 0.0730 ::: bot acc: 0.0285
top acc: 0.1040 ::: bot acc: 0.0291
current epoch: 14
train loss is 0.055173
average val loss: 0.060298, accuracy: 0.0589
average test loss: 0.060409, accuracy: 0.0599
case acc: 0.06467711
case acc: 0.059448503
case acc: 0.068046406
case acc: 0.044580244
case acc: 0.05346557
case acc: 0.0693367
top acc: 0.1191 ::: bot acc: 0.0202
top acc: 0.0967 ::: bot acc: 0.0235
top acc: 0.1166 ::: bot acc: 0.0262
top acc: 0.0763 ::: bot acc: 0.0173
top acc: 0.0923 ::: bot acc: 0.0217
top acc: 0.1117 ::: bot acc: 0.0302
current epoch: 15
train loss is 0.056135
average val loss: 0.064155, accuracy: 0.0631
average test loss: 0.064642, accuracy: 0.0644
case acc: 0.06653398
case acc: 0.065607004
case acc: 0.074291706
case acc: 0.05006229
case acc: 0.061542578
case acc: 0.068313465
top acc: 0.1211 ::: bot acc: 0.0216
top acc: 0.1029 ::: bot acc: 0.0295
top acc: 0.1239 ::: bot acc: 0.0303
top acc: 0.0822 ::: bot acc: 0.0220
top acc: 0.1043 ::: bot acc: 0.0219
top acc: 0.1102 ::: bot acc: 0.0301
current epoch: 16
train loss is 0.056167
average val loss: 0.063863, accuracy: 0.0634
average test loss: 0.064341, accuracy: 0.0641
case acc: 0.062283788
case acc: 0.0640126
case acc: 0.07485598
case acc: 0.051692747
case acc: 0.06744517
case acc: 0.06412242
top acc: 0.1165 ::: bot acc: 0.0181
top acc: 0.1013 ::: bot acc: 0.0279
top acc: 0.1245 ::: bot acc: 0.0307
top acc: 0.0840 ::: bot acc: 0.0234
top acc: 0.1122 ::: bot acc: 0.0236
top acc: 0.1044 ::: bot acc: 0.0293
current epoch: 17
train loss is 0.056338
average val loss: 0.060415, accuracy: 0.0606
average test loss: 0.060591, accuracy: 0.0601
case acc: 0.05415843
case acc: 0.056053594
case acc: 0.07030295
case acc: 0.05066453
case acc: 0.07071844
case acc: 0.058997337
top acc: 0.1074 ::: bot acc: 0.0121
top acc: 0.0931 ::: bot acc: 0.0205
top acc: 0.1192 ::: bot acc: 0.0276
top acc: 0.0829 ::: bot acc: 0.0225
top acc: 0.1164 ::: bot acc: 0.0250
top acc: 0.0966 ::: bot acc: 0.0295
current epoch: 18
train loss is 0.056197
average val loss: 0.047047, accuracy: 0.0480
average test loss: 0.045685, accuracy: 0.0454
case acc: 0.03986297
case acc: 0.035601463
case acc: 0.05222889
case acc: 0.0358548
case acc: 0.060772784
case acc: 0.048147652
top acc: 0.0819 ::: bot acc: 0.0204
top acc: 0.0659 ::: bot acc: 0.0137
top acc: 0.0953 ::: bot acc: 0.0215
top acc: 0.0663 ::: bot acc: 0.0113
top acc: 0.1032 ::: bot acc: 0.0217
top acc: 0.0753 ::: bot acc: 0.0401
current epoch: 19
train loss is 0.050324
average val loss: 0.039033, accuracy: 0.0403
average test loss: 0.036068, accuracy: 0.0362
case acc: 0.038817666
case acc: 0.026914572
case acc: 0.03952666
case acc: 0.023937665
case acc: 0.047975563
case acc: 0.04025171
top acc: 0.0526 ::: bot acc: 0.0499
top acc: 0.0331 ::: bot acc: 0.0401
top acc: 0.0649 ::: bot acc: 0.0416
top acc: 0.0452 ::: bot acc: 0.0171
top acc: 0.0830 ::: bot acc: 0.0240
top acc: 0.0517 ::: bot acc: 0.0607
current epoch: 20
train loss is 0.048321
average val loss: 0.042514, accuracy: 0.0436
average test loss: 0.040153, accuracy: 0.0407
case acc: 0.050301522
case acc: 0.04418048
case acc: 0.043395385
case acc: 0.026077634
case acc: 0.036803186
case acc: 0.043487452
top acc: 0.0273 ::: bot acc: 0.0811
top acc: 0.0158 ::: bot acc: 0.0758
top acc: 0.0304 ::: bot acc: 0.0763
top acc: 0.0197 ::: bot acc: 0.0424
top acc: 0.0559 ::: bot acc: 0.0439
top acc: 0.0272 ::: bot acc: 0.0857
current epoch: 21
train loss is 0.053848
average val loss: 0.068816, accuracy: 0.0696
average test loss: 0.068779, accuracy: 0.0691
case acc: 0.08199869
case acc: 0.08776297
case acc: 0.07570566
case acc: 0.054293543
case acc: 0.046639234
case acc: 0.068307966
top acc: 0.0372 ::: bot acc: 0.1235
top acc: 0.0507 ::: bot acc: 0.1236
top acc: 0.0301 ::: bot acc: 0.1246
top acc: 0.0229 ::: bot acc: 0.0828
top acc: 0.0166 ::: bot acc: 0.0891
top acc: 0.0280 ::: bot acc: 0.1223
current epoch: 22
train loss is 0.062345
average val loss: 0.074363, accuracy: 0.0749
average test loss: 0.074547, accuracy: 0.0746
case acc: 0.084472135
case acc: 0.095360026
case acc: 0.08451785
case acc: 0.06029891
case acc: 0.055038117
case acc: 0.068037
top acc: 0.0388 ::: bot acc: 0.1264
top acc: 0.0583 ::: bot acc: 0.1311
top acc: 0.0354 ::: bot acc: 0.1352
top acc: 0.0282 ::: bot acc: 0.0891
top acc: 0.0153 ::: bot acc: 0.1022
top acc: 0.0278 ::: bot acc: 0.1220
current epoch: 23
train loss is 0.062412
average val loss: 0.063021, accuracy: 0.0632
average test loss: 0.062729, accuracy: 0.0629
case acc: 0.06882602
case acc: 0.07867627
case acc: 0.074449785
case acc: 0.050080016
case acc: 0.05205287
case acc: 0.053549506
top acc: 0.0297 ::: bot acc: 0.1074
top acc: 0.0417 ::: bot acc: 0.1145
top acc: 0.0296 ::: bot acc: 0.1230
top acc: 0.0195 ::: bot acc: 0.0782
top acc: 0.0148 ::: bot acc: 0.0980
top acc: 0.0213 ::: bot acc: 0.1036
current epoch: 24
train loss is 0.057831
average val loss: 0.048284, accuracy: 0.0481
average test loss: 0.046830, accuracy: 0.0470
case acc: 0.049973153
case acc: 0.052758373
case acc: 0.05726853
case acc: 0.03583326
case acc: 0.044660144
case acc: 0.041654117
top acc: 0.0273 ::: bot acc: 0.0804
top acc: 0.0192 ::: bot acc: 0.0868
top acc: 0.0241 ::: bot acc: 0.1002
top acc: 0.0115 ::: bot acc: 0.0609
top acc: 0.0177 ::: bot acc: 0.0856
top acc: 0.0317 ::: bot acc: 0.0807
current epoch: 25
train loss is 0.048777
average val loss: 0.042428, accuracy: 0.0421
average test loss: 0.040029, accuracy: 0.0401
case acc: 0.04225771
case acc: 0.038500242
case acc: 0.047450546
case acc: 0.030472558
case acc: 0.04207882
case acc: 0.03981763
top acc: 0.0395 ::: bot acc: 0.0629
top acc: 0.0157 ::: bot acc: 0.0673
top acc: 0.0254 ::: bot acc: 0.0849
top acc: 0.0122 ::: bot acc: 0.0526
top acc: 0.0206 ::: bot acc: 0.0804
top acc: 0.0436 ::: bot acc: 0.0689
current epoch: 26
train loss is 0.043045
average val loss: 0.038644, accuracy: 0.0379
average test loss: 0.035416, accuracy: 0.0350
case acc: 0.0378094
case acc: 0.027403757
case acc: 0.04032497
case acc: 0.025115715
case acc: 0.03776873
case acc: 0.041330114
top acc: 0.0600 ::: bot acc: 0.0425
top acc: 0.0284 ::: bot acc: 0.0441
top acc: 0.0415 ::: bot acc: 0.0654
top acc: 0.0225 ::: bot acc: 0.0394
top acc: 0.0311 ::: bot acc: 0.0688
top acc: 0.0580 ::: bot acc: 0.0543
current epoch: 27
train loss is 0.041483
average val loss: 0.040216, accuracy: 0.0391
average test loss: 0.037641, accuracy: 0.0365
case acc: 0.040594332
case acc: 0.030430451
case acc: 0.040095635
case acc: 0.023239726
case acc: 0.035930578
case acc: 0.048824407
top acc: 0.0847 ::: bot acc: 0.0180
top acc: 0.0563 ::: bot acc: 0.0174
top acc: 0.0669 ::: bot acc: 0.0401
top acc: 0.0421 ::: bot acc: 0.0198
top acc: 0.0511 ::: bot acc: 0.0488
top acc: 0.0770 ::: bot acc: 0.0386
current epoch: 28
train loss is 0.044534
average val loss: 0.052399, accuracy: 0.0510
average test loss: 0.051685, accuracy: 0.0510
case acc: 0.05799461
case acc: 0.051643007
case acc: 0.053487934
case acc: 0.036678035
case acc: 0.045788527
case acc: 0.060598377
top acc: 0.1123 ::: bot acc: 0.0145
top acc: 0.0878 ::: bot acc: 0.0175
top acc: 0.0972 ::: bot acc: 0.0212
top acc: 0.0674 ::: bot acc: 0.0119
top acc: 0.0790 ::: bot acc: 0.0255
top acc: 0.0992 ::: bot acc: 0.0290
current epoch: 29
train loss is 0.049655
average val loss: 0.065918, accuracy: 0.0648
average test loss: 0.066573, accuracy: 0.0666
case acc: 0.07391725
case acc: 0.07204771
case acc: 0.070355356
case acc: 0.053356938
case acc: 0.06026087
case acc: 0.06963526
top acc: 0.1293 ::: bot acc: 0.0282
top acc: 0.1089 ::: bot acc: 0.0364
top acc: 0.1193 ::: bot acc: 0.0275
top acc: 0.0859 ::: bot acc: 0.0249
top acc: 0.1024 ::: bot acc: 0.0217
top acc: 0.1121 ::: bot acc: 0.0301
current epoch: 30
train loss is 0.054237
average val loss: 0.065191, accuracy: 0.0645
average test loss: 0.065796, accuracy: 0.0658
case acc: 0.06924474
case acc: 0.06985865
case acc: 0.07117072
case acc: 0.05405858
case acc: 0.06533686
case acc: 0.06518395
top acc: 0.1243 ::: bot acc: 0.0240
top acc: 0.1067 ::: bot acc: 0.0343
top acc: 0.1203 ::: bot acc: 0.0279
top acc: 0.0866 ::: bot acc: 0.0255
top acc: 0.1093 ::: bot acc: 0.0229
top acc: 0.1060 ::: bot acc: 0.0292
current epoch: 31
train loss is 0.055352
average val loss: 0.050814, accuracy: 0.0510
average test loss: 0.049961, accuracy: 0.0496
case acc: 0.048067376
case acc: 0.046394527
case acc: 0.05469236
case acc: 0.039078206
case acc: 0.0571896
case acc: 0.05193855
top acc: 0.0994 ::: bot acc: 0.0104
top acc: 0.0816 ::: bot acc: 0.0144
top acc: 0.0991 ::: bot acc: 0.0211
top acc: 0.0701 ::: bot acc: 0.0136
top acc: 0.0979 ::: bot acc: 0.0215
top acc: 0.0837 ::: bot acc: 0.0344
current epoch: 32
train loss is 0.048197
average val loss: 0.041600, accuracy: 0.0423
average test loss: 0.039378, accuracy: 0.0391
case acc: 0.03883548
case acc: 0.030378778
case acc: 0.04284642
case acc: 0.028395643
case acc: 0.04999601
case acc: 0.044350535
top acc: 0.0767 ::: bot acc: 0.0261
top acc: 0.0562 ::: bot acc: 0.0176
top acc: 0.0773 ::: bot acc: 0.0297
top acc: 0.0561 ::: bot acc: 0.0096
top acc: 0.0865 ::: bot acc: 0.0229
top acc: 0.0663 ::: bot acc: 0.0467
current epoch: 33
train loss is 0.041417
average val loss: 0.038040, accuracy: 0.0389
average test loss: 0.034789, accuracy: 0.0351
case acc: 0.03844101
case acc: 0.026796352
case acc: 0.038991146
case acc: 0.023272455
case acc: 0.04283139
case acc: 0.04005765
top acc: 0.0563 ::: bot acc: 0.0467
top acc: 0.0319 ::: bot acc: 0.0406
top acc: 0.0557 ::: bot acc: 0.0515
top acc: 0.0425 ::: bot acc: 0.0193
top acc: 0.0732 ::: bot acc: 0.0283
top acc: 0.0518 ::: bot acc: 0.0603
current epoch: 34
train loss is 0.039897
average val loss: 0.040779, accuracy: 0.0415
average test loss: 0.038052, accuracy: 0.0385
case acc: 0.045202598
case acc: 0.03960356
case acc: 0.044381198
case acc: 0.025067363
case acc: 0.03589661
case acc: 0.04104742
top acc: 0.0330 ::: bot acc: 0.0708
top acc: 0.0155 ::: bot acc: 0.0689
top acc: 0.0287 ::: bot acc: 0.0788
top acc: 0.0223 ::: bot acc: 0.0395
top acc: 0.0509 ::: bot acc: 0.0490
top acc: 0.0331 ::: bot acc: 0.0788
current epoch: 35
train loss is 0.043321
average val loss: 0.051589, accuracy: 0.0519
average test loss: 0.050484, accuracy: 0.0508
case acc: 0.0577866
case acc: 0.06039148
case acc: 0.061401777
case acc: 0.03620448
case acc: 0.040121384
case acc: 0.048973877
top acc: 0.0258 ::: bot acc: 0.0930
top acc: 0.0243 ::: bot acc: 0.0956
top acc: 0.0245 ::: bot acc: 0.1062
top acc: 0.0115 ::: bot acc: 0.0613
top acc: 0.0247 ::: bot acc: 0.0755
top acc: 0.0218 ::: bot acc: 0.0965
current epoch: 36
train loss is 0.047751
average val loss: 0.055011, accuracy: 0.0550
average test loss: 0.054183, accuracy: 0.0544
case acc: 0.059119076
case acc: 0.065466315
case acc: 0.06747922
case acc: 0.040382244
case acc: 0.04542046
case acc: 0.048636325
top acc: 0.0260 ::: bot acc: 0.0949
top acc: 0.0289 ::: bot acc: 0.1009
top acc: 0.0266 ::: bot acc: 0.1143
top acc: 0.0130 ::: bot acc: 0.0668
top acc: 0.0171 ::: bot acc: 0.0871
top acc: 0.0220 ::: bot acc: 0.0959
current epoch: 37
train loss is 0.049988
average val loss: 0.049025, accuracy: 0.0487
average test loss: 0.047646, accuracy: 0.0478
case acc: 0.05017491
case acc: 0.053685516
case acc: 0.060842387
case acc: 0.035062518
case acc: 0.044695392
case acc: 0.042272873
top acc: 0.0274 ::: bot acc: 0.0809
top acc: 0.0196 ::: bot acc: 0.0878
top acc: 0.0243 ::: bot acc: 0.1055
top acc: 0.0112 ::: bot acc: 0.0598
top acc: 0.0176 ::: bot acc: 0.0858
top acc: 0.0291 ::: bot acc: 0.0830
current epoch: 38
train loss is 0.047384
average val loss: 0.040583, accuracy: 0.0400
average test loss: 0.037791, accuracy: 0.0380
case acc: 0.04060123
case acc: 0.035224766
case acc: 0.04676132
case acc: 0.02653151
case acc: 0.039363846
case acc: 0.039671585
top acc: 0.0466 ::: bot acc: 0.0565
top acc: 0.0164 ::: bot acc: 0.0619
top acc: 0.0257 ::: bot acc: 0.0839
top acc: 0.0181 ::: bot acc: 0.0437
top acc: 0.0265 ::: bot acc: 0.0735
top acc: 0.0491 ::: bot acc: 0.0629
current epoch: 39
train loss is 0.041740
average val loss: 0.038120, accuracy: 0.0372
average test loss: 0.034857, accuracy: 0.0345
case acc: 0.037785918
case acc: 0.02619621
case acc: 0.039965056
case acc: 0.023093063
case acc: 0.036201283
case acc: 0.04395166
top acc: 0.0686 ::: bot acc: 0.0345
top acc: 0.0353 ::: bot acc: 0.0369
top acc: 0.0452 ::: bot acc: 0.0622
top acc: 0.0327 ::: bot acc: 0.0291
top acc: 0.0395 ::: bot acc: 0.0604
top acc: 0.0655 ::: bot acc: 0.0470
current epoch: 40
train loss is 0.040756
average val loss: 0.043494, accuracy: 0.0425
average test loss: 0.041563, accuracy: 0.0407
case acc: 0.045837864
case acc: 0.03597537
case acc: 0.04204116
case acc: 0.027717035
case acc: 0.03886205
case acc: 0.053505674
top acc: 0.0959 ::: bot acc: 0.0113
top acc: 0.0665 ::: bot acc: 0.0133
top acc: 0.0744 ::: bot acc: 0.0330
top acc: 0.0551 ::: bot acc: 0.0095
top acc: 0.0631 ::: bot acc: 0.0368
top acc: 0.0869 ::: bot acc: 0.0322
current epoch: 41
train loss is 0.043425
average val loss: 0.056661, accuracy: 0.0555
average test loss: 0.056485, accuracy: 0.0563
case acc: 0.06374773
case acc: 0.057015814
case acc: 0.0562745
case acc: 0.045073338
case acc: 0.051341947
case acc: 0.06447442
top acc: 0.1189 ::: bot acc: 0.0189
top acc: 0.0937 ::: bot acc: 0.0218
top acc: 0.1015 ::: bot acc: 0.0211
top acc: 0.0769 ::: bot acc: 0.0178
top acc: 0.0886 ::: bot acc: 0.0225
top acc: 0.1047 ::: bot acc: 0.0292
current epoch: 42
train loss is 0.048028
average val loss: 0.060338, accuracy: 0.0595
average test loss: 0.060575, accuracy: 0.0606
case acc: 0.065810785
case acc: 0.062347736
case acc: 0.062182315
case acc: 0.05024558
case acc: 0.05877236
case acc: 0.06408699
top acc: 0.1210 ::: bot acc: 0.0207
top acc: 0.0991 ::: bot acc: 0.0269
top acc: 0.1095 ::: bot acc: 0.0228
top acc: 0.0824 ::: bot acc: 0.0222
top acc: 0.1001 ::: bot acc: 0.0217
top acc: 0.1042 ::: bot acc: 0.0292
current epoch: 43
train loss is 0.049125
average val loss: 0.050682, accuracy: 0.0506
average test loss: 0.049844, accuracy: 0.0495
case acc: 0.05006877
case acc: 0.04636758
case acc: 0.05241824
case acc: 0.039926093
case acc: 0.054870553
case acc: 0.05346202
top acc: 0.1026 ::: bot acc: 0.0104
top acc: 0.0815 ::: bot acc: 0.0144
top acc: 0.0958 ::: bot acc: 0.0212
top acc: 0.0710 ::: bot acc: 0.0141
top acc: 0.0943 ::: bot acc: 0.0217
top acc: 0.0868 ::: bot acc: 0.0323
current epoch: 44
train loss is 0.044514
average val loss: 0.042023, accuracy: 0.0425
average test loss: 0.039946, accuracy: 0.0397
case acc: 0.039779946
case acc: 0.03160023
case acc: 0.04272346
case acc: 0.029519109
case acc: 0.048946314
case acc: 0.04585903
top acc: 0.0816 ::: bot acc: 0.0214
top acc: 0.0587 ::: bot acc: 0.0161
top acc: 0.0769 ::: bot acc: 0.0305
top acc: 0.0579 ::: bot acc: 0.0092
top acc: 0.0846 ::: bot acc: 0.0235
top acc: 0.0700 ::: bot acc: 0.0435
current epoch: 45
train loss is 0.038546
average val loss: 0.037893, accuracy: 0.0386
average test loss: 0.034629, accuracy: 0.0348
case acc: 0.037942328
case acc: 0.026365638
case acc: 0.039053284
case acc: 0.023282353
case acc: 0.04166773
case acc: 0.040328648
top acc: 0.0604 ::: bot acc: 0.0426
top acc: 0.0343 ::: bot acc: 0.0380
top acc: 0.0553 ::: bot acc: 0.0520
top acc: 0.0431 ::: bot acc: 0.0186
top acc: 0.0706 ::: bot acc: 0.0300
top acc: 0.0540 ::: bot acc: 0.0580
current epoch: 46
train loss is 0.036845
average val loss: 0.039128, accuracy: 0.0397
average test loss: 0.035998, accuracy: 0.0365
case acc: 0.041960206
case acc: 0.034510598
case acc: 0.042552136
case acc: 0.023610955
case acc: 0.036259245
case acc: 0.039869975
top acc: 0.0414 ::: bot acc: 0.0616
top acc: 0.0167 ::: bot acc: 0.0607
top acc: 0.0338 ::: bot acc: 0.0735
top acc: 0.0275 ::: bot acc: 0.0342
top acc: 0.0531 ::: bot acc: 0.0467
top acc: 0.0397 ::: bot acc: 0.0722
current epoch: 47
train loss is 0.039226
average val loss: 0.045539, accuracy: 0.0457
average test loss: 0.043739, accuracy: 0.0441
case acc: 0.049370006
case acc: 0.049212664
case acc: 0.054136187
case acc: 0.030186245
case acc: 0.037805185
case acc: 0.04363867
top acc: 0.0278 ::: bot acc: 0.0794
top acc: 0.0177 ::: bot acc: 0.0821
top acc: 0.0238 ::: bot acc: 0.0958
top acc: 0.0121 ::: bot acc: 0.0521
top acc: 0.0310 ::: bot acc: 0.0688
top acc: 0.0265 ::: bot acc: 0.0863
current epoch: 48
train loss is 0.042587
average val loss: 0.046719, accuracy: 0.0466
average test loss: 0.045111, accuracy: 0.0453
case acc: 0.04898813
case acc: 0.050557382
case acc: 0.057203367
case acc: 0.031913288
case acc: 0.040743556
case acc: 0.04262485
top acc: 0.0281 ::: bot acc: 0.0787
top acc: 0.0182 ::: bot acc: 0.0838
top acc: 0.0239 ::: bot acc: 0.1003
top acc: 0.0115 ::: bot acc: 0.0550
top acc: 0.0232 ::: bot acc: 0.0772
top acc: 0.0282 ::: bot acc: 0.0839
current epoch: 49
train loss is 0.043203
average val loss: 0.041897, accuracy: 0.0415
average test loss: 0.039437, accuracy: 0.0397
case acc: 0.04260027
case acc: 0.039706506
case acc: 0.04990893
case acc: 0.027373118
case acc: 0.03921148
case acc: 0.039654203
top acc: 0.0395 ::: bot acc: 0.0635
top acc: 0.0156 ::: bot acc: 0.0689
top acc: 0.0243 ::: bot acc: 0.0892
top acc: 0.0158 ::: bot acc: 0.0461
top acc: 0.0268 ::: bot acc: 0.0732
top acc: 0.0416 ::: bot acc: 0.0703
current epoch: 50
train loss is 0.040620
average val loss: 0.038228, accuracy: 0.0376
average test loss: 0.034897, accuracy: 0.0348
case acc: 0.037976306
case acc: 0.02796713
case acc: 0.041545734
case acc: 0.02331674
case acc: 0.036430996
case acc: 0.04134036
top acc: 0.0601 ::: bot acc: 0.0429
top acc: 0.0257 ::: bot acc: 0.0464
top acc: 0.0371 ::: bot acc: 0.0703
top acc: 0.0293 ::: bot acc: 0.0324
top acc: 0.0376 ::: bot acc: 0.0622
top acc: 0.0582 ::: bot acc: 0.0537

		{"drop_out": 0.6, "drop_out_mc": 0.1, "repeat_mc": 50, "hidden": 30, "embedding_size": 5, "batch": 512, "lag": 4}
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
LME_Co_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5376 5376 5376
1.7082474 -0.6288155 0.24786325 -0.22413088
Validation: 600 600 600
Testing: 774 774 774
pre-processing time: 0.0004649162292480469
the split date is 2010-07-01
net initializing with time: 0.005902290344238281
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.122204
average val loss: 0.126463, accuracy: 0.1270
average test loss: 0.140188, accuracy: 0.1421
case acc: 0.20283651
case acc: 0.04280199
case acc: 0.16201632
case acc: 0.15006983
case acc: 0.17732626
case acc: 0.117443234
top acc: 0.1819 ::: bot acc: 0.2250
top acc: 0.0783 ::: bot acc: 0.0125
top acc: 0.1164 ::: bot acc: 0.2129
top acc: 0.1170 ::: bot acc: 0.1847
top acc: 0.1445 ::: bot acc: 0.2104
top acc: 0.0882 ::: bot acc: 0.1509
current epoch: 2
train loss is 0.116520
average val loss: 0.062123, accuracy: 0.0621
average test loss: 0.058654, accuracy: 0.0595
case acc: 0.047236945
case acc: 0.17807989
case acc: 0.034769118
case acc: 0.025305381
case acc: 0.036935173
case acc: 0.034496468
top acc: 0.0265 ::: bot acc: 0.0696
top acc: 0.2194 ::: bot acc: 0.1354
top acc: 0.0389 ::: bot acc: 0.0562
top acc: 0.0327 ::: bot acc: 0.0351
top acc: 0.0091 ::: bot acc: 0.0676
top acc: 0.0546 ::: bot acc: 0.0182
current epoch: 3
train loss is 0.118699
average val loss: 0.151132, accuracy: 0.1512
average test loss: 0.132290, accuracy: 0.1322
case acc: 0.073967144
case acc: 0.28277463
case acc: 0.11283248
case acc: 0.11262916
case acc: 0.078707375
case acc: 0.13228488
top acc: 0.0938 ::: bot acc: 0.0533
top acc: 0.3239 ::: bot acc: 0.2400
top acc: 0.1574 ::: bot acc: 0.0631
top acc: 0.1458 ::: bot acc: 0.0783
top acc: 0.1124 ::: bot acc: 0.0452
top acc: 0.1613 ::: bot acc: 0.0979
current epoch: 4
train loss is 0.129216
average val loss: 0.094124, accuracy: 0.0952
average test loss: 0.077849, accuracy: 0.0760
case acc: 0.024158046
case acc: 0.21536075
case acc: 0.061146323
case acc: 0.05419107
case acc: 0.031681392
case acc: 0.06959157
top acc: 0.0398 ::: bot acc: 0.0119
top acc: 0.2564 ::: bot acc: 0.1727
top acc: 0.1011 ::: bot acc: 0.0207
top acc: 0.0867 ::: bot acc: 0.0213
top acc: 0.0592 ::: bot acc: 0.0108
top acc: 0.0985 ::: bot acc: 0.0352
current epoch: 5
train loss is 0.106572
average val loss: 0.055514, accuracy: 0.0551
average test loss: 0.050041, accuracy: 0.0506
case acc: 0.031094337
case acc: 0.15325639
case acc: 0.035006043
case acc: 0.025313813
case acc: 0.028652536
case acc: 0.030141992
top acc: 0.0118 ::: bot acc: 0.0530
top acc: 0.1945 ::: bot acc: 0.1104
top acc: 0.0471 ::: bot acc: 0.0473
top acc: 0.0339 ::: bot acc: 0.0335
top acc: 0.0164 ::: bot acc: 0.0513
top acc: 0.0461 ::: bot acc: 0.0217
current epoch: 6
train loss is 0.088275
average val loss: 0.051561, accuracy: 0.0514
average test loss: 0.046775, accuracy: 0.0478
case acc: 0.032684427
case acc: 0.13834928
case acc: 0.034419
case acc: 0.025369948
case acc: 0.027040666
case acc: 0.028819844
top acc: 0.0130 ::: bot acc: 0.0548
top acc: 0.1796 ::: bot acc: 0.0954
top acc: 0.0388 ::: bot acc: 0.0554
top acc: 0.0293 ::: bot acc: 0.0380
top acc: 0.0223 ::: bot acc: 0.0454
top acc: 0.0433 ::: bot acc: 0.0233
current epoch: 7
train loss is 0.077490
average val loss: 0.055603, accuracy: 0.0561
average test loss: 0.046585, accuracy: 0.0471
case acc: 0.021319656
case acc: 0.13907795
case acc: 0.034884665
case acc: 0.025812449
case acc: 0.026171375
case acc: 0.035196695
top acc: 0.0072 ::: bot acc: 0.0408
top acc: 0.1803 ::: bot acc: 0.0961
top acc: 0.0467 ::: bot acc: 0.0473
top acc: 0.0400 ::: bot acc: 0.0273
top acc: 0.0425 ::: bot acc: 0.0254
top acc: 0.0548 ::: bot acc: 0.0191
current epoch: 8
train loss is 0.073660
average val loss: 0.063920, accuracy: 0.0646
average test loss: 0.050836, accuracy: 0.0503
case acc: 0.015911482
case acc: 0.14269884
case acc: 0.037957035
case acc: 0.029130854
case acc: 0.033559468
case acc: 0.042596187
top acc: 0.0198 ::: bot acc: 0.0235
top acc: 0.1841 ::: bot acc: 0.0997
top acc: 0.0592 ::: bot acc: 0.0351
top acc: 0.0526 ::: bot acc: 0.0147
top acc: 0.0629 ::: bot acc: 0.0096
top acc: 0.0666 ::: bot acc: 0.0177
current epoch: 9
train loss is 0.074770
average val loss: 0.070800, accuracy: 0.0715
average test loss: 0.055591, accuracy: 0.0550
case acc: 0.0199227
case acc: 0.14322896
case acc: 0.042220358
case acc: 0.03351
case acc: 0.043368667
case acc: 0.04760443
top acc: 0.0328 ::: bot acc: 0.0129
top acc: 0.1848 ::: bot acc: 0.1002
top acc: 0.0691 ::: bot acc: 0.0281
top acc: 0.0617 ::: bot acc: 0.0098
top acc: 0.0764 ::: bot acc: 0.0121
top acc: 0.0736 ::: bot acc: 0.0188
current epoch: 10
train loss is 0.073374
average val loss: 0.078368, accuracy: 0.0788
average test loss: 0.061693, accuracy: 0.0613
case acc: 0.02859752
case acc: 0.14445592
case acc: 0.04770734
case acc: 0.03970371
case acc: 0.054765962
case acc: 0.052777797
top acc: 0.0455 ::: bot acc: 0.0134
top acc: 0.1861 ::: bot acc: 0.1014
top acc: 0.0799 ::: bot acc: 0.0232
top acc: 0.0706 ::: bot acc: 0.0105
top acc: 0.0886 ::: bot acc: 0.0217
top acc: 0.0800 ::: bot acc: 0.0215
current epoch: 11
train loss is 0.072876
average val loss: 0.078617, accuracy: 0.0789
average test loss: 0.061720, accuracy: 0.0615
case acc: 0.031958766
case acc: 0.1377613
case acc: 0.048959
case acc: 0.04031912
case acc: 0.058356967
case acc: 0.051634237
top acc: 0.0495 ::: bot acc: 0.0156
top acc: 0.1795 ::: bot acc: 0.0946
top acc: 0.0820 ::: bot acc: 0.0226
top acc: 0.0714 ::: bot acc: 0.0107
top acc: 0.0924 ::: bot acc: 0.0250
top acc: 0.0786 ::: bot acc: 0.0207
current epoch: 12
train loss is 0.070515
average val loss: 0.074632, accuracy: 0.0749
average test loss: 0.058004, accuracy: 0.0579
case acc: 0.03073899
case acc: 0.12649515
case acc: 0.04711105
case acc: 0.037724603
case acc: 0.057421226
case acc: 0.04773477
top acc: 0.0480 ::: bot acc: 0.0148
top acc: 0.1681 ::: bot acc: 0.0834
top acc: 0.0788 ::: bot acc: 0.0236
top acc: 0.0680 ::: bot acc: 0.0098
top acc: 0.0914 ::: bot acc: 0.0241
top acc: 0.0737 ::: bot acc: 0.0187
current epoch: 13
train loss is 0.066905
average val loss: 0.064599, accuracy: 0.0649
average test loss: 0.049275, accuracy: 0.0492
case acc: 0.023767352
case acc: 0.10836597
case acc: 0.041614383
case acc: 0.031642444
case acc: 0.049291234
case acc: 0.04053757
top acc: 0.0390 ::: bot acc: 0.0119
top acc: 0.1500 ::: bot acc: 0.0653
top acc: 0.0680 ::: bot acc: 0.0286
top acc: 0.0585 ::: bot acc: 0.0108
top acc: 0.0830 ::: bot acc: 0.0166
top acc: 0.0632 ::: bot acc: 0.0179
current epoch: 14
train loss is 0.062373
average val loss: 0.058535, accuracy: 0.0588
average test loss: 0.044222, accuracy: 0.0442
case acc: 0.020667804
case acc: 0.09454703
case acc: 0.03863243
case acc: 0.02947001
case acc: 0.044600774
case acc: 0.03726297
top acc: 0.0340 ::: bot acc: 0.0126
top acc: 0.1361 ::: bot acc: 0.0515
top acc: 0.0614 ::: bot acc: 0.0330
top acc: 0.0535 ::: bot acc: 0.0142
top acc: 0.0778 ::: bot acc: 0.0130
top acc: 0.0578 ::: bot acc: 0.0186
current epoch: 15
train loss is 0.058778
average val loss: 0.053365, accuracy: 0.0537
average test loss: 0.040087, accuracy: 0.0401
case acc: 0.018358316
case acc: 0.08197187
case acc: 0.0368131
case acc: 0.028225321
case acc: 0.04012058
case acc: 0.034888357
top acc: 0.0297 ::: bot acc: 0.0143
top acc: 0.1235 ::: bot acc: 0.0389
top acc: 0.0558 ::: bot acc: 0.0384
top acc: 0.0498 ::: bot acc: 0.0178
top acc: 0.0722 ::: bot acc: 0.0107
top acc: 0.0537 ::: bot acc: 0.0196
current epoch: 16
train loss is 0.055674
average val loss: 0.049417, accuracy: 0.0497
average test loss: 0.036980, accuracy: 0.0368
case acc: 0.017132478
case acc: 0.07137923
case acc: 0.035743322
case acc: 0.02749626
case acc: 0.036164623
case acc: 0.033148848
top acc: 0.0268 ::: bot acc: 0.0164
top acc: 0.1129 ::: bot acc: 0.0284
top acc: 0.0521 ::: bot acc: 0.0422
top acc: 0.0474 ::: bot acc: 0.0201
top acc: 0.0668 ::: bot acc: 0.0096
top acc: 0.0506 ::: bot acc: 0.0205
current epoch: 17
train loss is 0.052776
average val loss: 0.047066, accuracy: 0.0473
average test loss: 0.035075, accuracy: 0.0348
case acc: 0.016902616
case acc: 0.06365468
case acc: 0.035540204
case acc: 0.027377902
case acc: 0.033370733
case acc: 0.032087747
top acc: 0.0262 ::: bot acc: 0.0169
top acc: 0.1050 ::: bot acc: 0.0210
top acc: 0.0510 ::: bot acc: 0.0434
top acc: 0.0469 ::: bot acc: 0.0207
top acc: 0.0625 ::: bot acc: 0.0099
top acc: 0.0488 ::: bot acc: 0.0211
current epoch: 18
train loss is 0.050982
average val loss: 0.047846, accuracy: 0.0480
average test loss: 0.035298, accuracy: 0.0350
case acc: 0.01823923
case acc: 0.060965493
case acc: 0.036527537
case acc: 0.028447963
case acc: 0.032840032
case acc: 0.032834783
top acc: 0.0295 ::: bot acc: 0.0144
top acc: 0.1022 ::: bot acc: 0.0186
top acc: 0.0546 ::: bot acc: 0.0401
top acc: 0.0502 ::: bot acc: 0.0176
top acc: 0.0615 ::: bot acc: 0.0103
top acc: 0.0505 ::: bot acc: 0.0204
current epoch: 19
train loss is 0.050131
average val loss: 0.045862, accuracy: 0.0459
average test loss: 0.033806, accuracy: 0.0334
case acc: 0.018193765
case acc: 0.05599701
case acc: 0.036602516
case acc: 0.028060915
case acc: 0.030439116
case acc: 0.030933835
top acc: 0.0294 ::: bot acc: 0.0147
top acc: 0.0964 ::: bot acc: 0.0151
top acc: 0.0548 ::: bot acc: 0.0400
top acc: 0.0491 ::: bot acc: 0.0186
top acc: 0.0573 ::: bot acc: 0.0118
top acc: 0.0473 ::: bot acc: 0.0212
current epoch: 20
train loss is 0.049021
average val loss: 0.048305, accuracy: 0.0483
average test loss: 0.035306, accuracy: 0.0348
case acc: 0.021065645
case acc: 0.056860857
case acc: 0.038552
case acc: 0.029436123
case acc: 0.030943064
case acc: 0.031808384
top acc: 0.0347 ::: bot acc: 0.0126
top acc: 0.0974 ::: bot acc: 0.0156
top acc: 0.0610 ::: bot acc: 0.0338
top acc: 0.0533 ::: bot acc: 0.0145
top acc: 0.0583 ::: bot acc: 0.0113
top acc: 0.0489 ::: bot acc: 0.0207
current epoch: 21
train loss is 0.049657
average val loss: 0.054692, accuracy: 0.0547
average test loss: 0.039959, accuracy: 0.0394
case acc: 0.027540972
case acc: 0.06252446
case acc: 0.043382682
case acc: 0.03378988
case acc: 0.03394407
case acc: 0.03517961
top acc: 0.0442 ::: bot acc: 0.0131
top acc: 0.1036 ::: bot acc: 0.0201
top acc: 0.0718 ::: bot acc: 0.0266
top acc: 0.0620 ::: bot acc: 0.0102
top acc: 0.0635 ::: bot acc: 0.0099
top acc: 0.0548 ::: bot acc: 0.0190
current epoch: 22
train loss is 0.050340
average val loss: 0.061519, accuracy: 0.0616
average test loss: 0.045559, accuracy: 0.0450
case acc: 0.035438154
case acc: 0.06944973
case acc: 0.049014512
case acc: 0.039469
case acc: 0.03808115
case acc: 0.038528204
top acc: 0.0534 ::: bot acc: 0.0185
top acc: 0.1108 ::: bot acc: 0.0265
top acc: 0.0825 ::: bot acc: 0.0220
top acc: 0.0704 ::: bot acc: 0.0106
top acc: 0.0695 ::: bot acc: 0.0103
top acc: 0.0602 ::: bot acc: 0.0179
current epoch: 23
train loss is 0.052447
average val loss: 0.071073, accuracy: 0.0712
average test loss: 0.053967, accuracy: 0.0534
case acc: 0.04616397
case acc: 0.0792652
case acc: 0.056958184
case acc: 0.04842061
case acc: 0.045849755
case acc: 0.043945767
top acc: 0.0648 ::: bot acc: 0.0278
top acc: 0.1206 ::: bot acc: 0.0363
top acc: 0.0955 ::: bot acc: 0.0199
top acc: 0.0809 ::: bot acc: 0.0164
top acc: 0.0793 ::: bot acc: 0.0141
top acc: 0.0684 ::: bot acc: 0.0178
current epoch: 24
train loss is 0.055376
average val loss: 0.075318, accuracy: 0.0754
average test loss: 0.057879, accuracy: 0.0574
case acc: 0.0512067
case acc: 0.08223547
case acc: 0.061328903
case acc: 0.0525416
case acc: 0.051268283
case acc: 0.04588332
top acc: 0.0701 ::: bot acc: 0.0324
top acc: 0.1236 ::: bot acc: 0.0393
top acc: 0.1018 ::: bot acc: 0.0207
top acc: 0.0853 ::: bot acc: 0.0201
top acc: 0.0851 ::: bot acc: 0.0185
top acc: 0.0712 ::: bot acc: 0.0182
current epoch: 25
train loss is 0.057019
average val loss: 0.073059, accuracy: 0.0731
average test loss: 0.055748, accuracy: 0.0553
case acc: 0.049330693
case acc: 0.07661666
case acc: 0.05971472
case acc: 0.050252065
case acc: 0.052140266
case acc: 0.043769658
top acc: 0.0682 ::: bot acc: 0.0307
top acc: 0.1180 ::: bot acc: 0.0337
top acc: 0.0995 ::: bot acc: 0.0204
top acc: 0.0828 ::: bot acc: 0.0181
top acc: 0.0861 ::: bot acc: 0.0193
top acc: 0.0682 ::: bot acc: 0.0177
current epoch: 26
train loss is 0.057254
average val loss: 0.066647, accuracy: 0.0666
average test loss: 0.049858, accuracy: 0.0494
case acc: 0.04264402
case acc: 0.06496453
case acc: 0.053995375
case acc: 0.044512395
case acc: 0.050078142
case acc: 0.040267866
top acc: 0.0611 ::: bot acc: 0.0247
top acc: 0.1062 ::: bot acc: 0.0222
top acc: 0.0910 ::: bot acc: 0.0203
top acc: 0.0765 ::: bot acc: 0.0136
top acc: 0.0839 ::: bot acc: 0.0174
top acc: 0.0630 ::: bot acc: 0.0175
current epoch: 27
train loss is 0.055575
average val loss: 0.052501, accuracy: 0.0524
average test loss: 0.037899, accuracy: 0.0376
case acc: 0.027530907
case acc: 0.046714846
case acc: 0.043263946
case acc: 0.033792164
case acc: 0.040882297
case acc: 0.033229534
top acc: 0.0443 ::: bot acc: 0.0129
top acc: 0.0841 ::: bot acc: 0.0117
top acc: 0.0716 ::: bot acc: 0.0268
top acc: 0.0619 ::: bot acc: 0.0105
top acc: 0.0731 ::: bot acc: 0.0113
top acc: 0.0512 ::: bot acc: 0.0199
current epoch: 28
train loss is 0.049456
average val loss: 0.035736, accuracy: 0.0360
average test loss: 0.027547, accuracy: 0.0275
case acc: 0.0158749
case acc: 0.03352446
case acc: 0.034517102
case acc: 0.025907168
case acc: 0.029586788
case acc: 0.02537315
top acc: 0.0207 ::: bot acc: 0.0225
top acc: 0.0552 ::: bot acc: 0.0291
top acc: 0.0450 ::: bot acc: 0.0496
top acc: 0.0414 ::: bot acc: 0.0260
top acc: 0.0552 ::: bot acc: 0.0134
top acc: 0.0344 ::: bot acc: 0.0299
current epoch: 29
train loss is 0.044308
average val loss: 0.027286, accuracy: 0.0274
average test loss: 0.028743, accuracy: 0.0290
case acc: 0.025972098
case acc: 0.032758962
case acc: 0.039356504
case acc: 0.026695173
case acc: 0.02536625
case acc: 0.023753732
top acc: 0.0089 ::: bot acc: 0.0466
top acc: 0.0272 ::: bot acc: 0.0573
top acc: 0.0201 ::: bot acc: 0.0762
top acc: 0.0204 ::: bot acc: 0.0469
top acc: 0.0349 ::: bot acc: 0.0332
top acc: 0.0167 ::: bot acc: 0.0473
current epoch: 30
train loss is 0.044938
average val loss: 0.035182, accuracy: 0.0343
average test loss: 0.046717, accuracy: 0.0465
case acc: 0.053922158
case acc: 0.049538754
case acc: 0.06174626
case acc: 0.04266119
case acc: 0.033354294
case acc: 0.03793037
top acc: 0.0332 ::: bot acc: 0.0763
top acc: 0.0133 ::: bot acc: 0.0899
top acc: 0.0236 ::: bot acc: 0.1080
top acc: 0.0139 ::: bot acc: 0.0742
top acc: 0.0093 ::: bot acc: 0.0618
top acc: 0.0107 ::: bot acc: 0.0723
current epoch: 31
train loss is 0.051663
average val loss: 0.052675, accuracy: 0.0517
average test loss: 0.067968, accuracy: 0.0677
case acc: 0.077493265
case acc: 0.0720346
case acc: 0.084189735
case acc: 0.06249195
case acc: 0.053025905
case acc: 0.057060238
top acc: 0.0568 ::: bot acc: 0.0998
top acc: 0.0310 ::: bot acc: 0.1149
top acc: 0.0406 ::: bot acc: 0.1332
top acc: 0.0296 ::: bot acc: 0.0960
top acc: 0.0188 ::: bot acc: 0.0866
top acc: 0.0281 ::: bot acc: 0.0923
current epoch: 32
train loss is 0.061415
average val loss: 0.050639, accuracy: 0.0496
average test loss: 0.065670, accuracy: 0.0654
case acc: 0.07509656
case acc: 0.06795371
case acc: 0.08179223
case acc: 0.05922284
case acc: 0.054857165
case acc: 0.05333281
top acc: 0.0544 ::: bot acc: 0.0974
top acc: 0.0271 ::: bot acc: 0.1107
top acc: 0.0383 ::: bot acc: 0.1308
top acc: 0.0265 ::: bot acc: 0.0927
top acc: 0.0205 ::: bot acc: 0.0884
top acc: 0.0244 ::: bot acc: 0.0885
current epoch: 33
train loss is 0.062952
average val loss: 0.031762, accuracy: 0.0308
average test loss: 0.041829, accuracy: 0.0415
case acc: 0.047404632
case acc: 0.04115
case acc: 0.05540597
case acc: 0.036150273
case acc: 0.03751168
case acc: 0.031337775
top acc: 0.0268 ::: bot acc: 0.0696
top acc: 0.0119 ::: bot acc: 0.0781
top acc: 0.0201 ::: bot acc: 0.1003
top acc: 0.0114 ::: bot acc: 0.0656
top acc: 0.0084 ::: bot acc: 0.0685
top acc: 0.0076 ::: bot acc: 0.0638
current epoch: 34
train loss is 0.052815
average val loss: 0.027152, accuracy: 0.0266
average test loss: 0.031408, accuracy: 0.0314
case acc: 0.030388957
case acc: 0.032406267
case acc: 0.040616367
case acc: 0.027643781
case acc: 0.031995
case acc: 0.025393981
top acc: 0.0116 ::: bot acc: 0.0516
top acc: 0.0304 ::: bot acc: 0.0547
top acc: 0.0195 ::: bot acc: 0.0784
top acc: 0.0166 ::: bot acc: 0.0502
top acc: 0.0103 ::: bot acc: 0.0592
top acc: 0.0116 ::: bot acc: 0.0529
current epoch: 35
train loss is 0.044832
average val loss: 0.028481, accuracy: 0.0279
average test loss: 0.027653, accuracy: 0.0275
case acc: 0.019954635
case acc: 0.032532148
case acc: 0.034807034
case acc: 0.025084658
case acc: 0.029013904
case acc: 0.023789076
top acc: 0.0074 ::: bot acc: 0.0381
top acc: 0.0480 ::: bot acc: 0.0371
top acc: 0.0333 ::: bot acc: 0.0618
top acc: 0.0269 ::: bot acc: 0.0397
top acc: 0.0160 ::: bot acc: 0.0519
top acc: 0.0181 ::: bot acc: 0.0463
current epoch: 36
train loss is 0.041910
average val loss: 0.035745, accuracy: 0.0352
average test loss: 0.028104, accuracy: 0.0275
case acc: 0.015654622
case acc: 0.03768819
case acc: 0.03591696
case acc: 0.025796518
case acc: 0.025401922
case acc: 0.02439277
top acc: 0.0224 ::: bot acc: 0.0203
top acc: 0.0677 ::: bot acc: 0.0181
top acc: 0.0525 ::: bot acc: 0.0426
top acc: 0.0419 ::: bot acc: 0.0247
top acc: 0.0316 ::: bot acc: 0.0362
top acc: 0.0309 ::: bot acc: 0.0336
current epoch: 37
train loss is 0.042794
average val loss: 0.044889, accuracy: 0.0445
average test loss: 0.032830, accuracy: 0.0321
case acc: 0.022034124
case acc: 0.04477595
case acc: 0.04099307
case acc: 0.029582877
case acc: 0.026796889
case acc: 0.028311282
top acc: 0.0366 ::: bot acc: 0.0120
top acc: 0.0816 ::: bot acc: 0.0115
top acc: 0.0668 ::: bot acc: 0.0298
top acc: 0.0542 ::: bot acc: 0.0129
top acc: 0.0468 ::: bot acc: 0.0210
top acc: 0.0423 ::: bot acc: 0.0236
current epoch: 38
train loss is 0.045784
average val loss: 0.051631, accuracy: 0.0515
average test loss: 0.037404, accuracy: 0.0367
case acc: 0.028391894
case acc: 0.04949952
case acc: 0.045004904
case acc: 0.033815
case acc: 0.030983044
case acc: 0.03241099
top acc: 0.0453 ::: bot acc: 0.0136
top acc: 0.0886 ::: bot acc: 0.0116
top acc: 0.0751 ::: bot acc: 0.0252
top acc: 0.0622 ::: bot acc: 0.0098
top acc: 0.0583 ::: bot acc: 0.0114
top acc: 0.0500 ::: bot acc: 0.0203
current epoch: 39
train loss is 0.047879
average val loss: 0.051137, accuracy: 0.0510
average test loss: 0.036908, accuracy: 0.0363
case acc: 0.02786125
case acc: 0.047027472
case acc: 0.04412866
case acc: 0.03347249
case acc: 0.032869928
case acc: 0.03256022
top acc: 0.0447 ::: bot acc: 0.0133
top acc: 0.0851 ::: bot acc: 0.0113
top acc: 0.0734 ::: bot acc: 0.0261
top acc: 0.0616 ::: bot acc: 0.0100
top acc: 0.0618 ::: bot acc: 0.0102
top acc: 0.0501 ::: bot acc: 0.0204
current epoch: 40
train loss is 0.047504
average val loss: 0.043021, accuracy: 0.0430
average test loss: 0.031252, accuracy: 0.0309
case acc: 0.020531675
case acc: 0.0390038
case acc: 0.038451526
case acc: 0.028735386
case acc: 0.030028084
case acc: 0.028541949
top acc: 0.0341 ::: bot acc: 0.0125
top acc: 0.0707 ::: bot acc: 0.0161
top acc: 0.0609 ::: bot acc: 0.0341
top acc: 0.0520 ::: bot acc: 0.0151
top acc: 0.0564 ::: bot acc: 0.0124
top acc: 0.0425 ::: bot acc: 0.0236
current epoch: 41
train loss is 0.044717
average val loss: 0.033815, accuracy: 0.0339
average test loss: 0.026755, accuracy: 0.0267
case acc: 0.015526524
case acc: 0.03320626
case acc: 0.034490872
case acc: 0.02533349
case acc: 0.026764264
case acc: 0.024754247
top acc: 0.0193 ::: bot acc: 0.0234
top acc: 0.0522 ::: bot acc: 0.0330
top acc: 0.0440 ::: bot acc: 0.0511
top acc: 0.0391 ::: bot acc: 0.0277
top acc: 0.0467 ::: bot acc: 0.0211
top acc: 0.0325 ::: bot acc: 0.0319
current epoch: 42
train loss is 0.041978
average val loss: 0.027132, accuracy: 0.0271
average test loss: 0.028052, accuracy: 0.0282
case acc: 0.023259353
case acc: 0.032622114
case acc: 0.037821174
case acc: 0.02608588
case acc: 0.025494654
case acc: 0.023798428
top acc: 0.0074 ::: bot acc: 0.0431
top acc: 0.0299 ::: bot acc: 0.0554
top acc: 0.0227 ::: bot acc: 0.0727
top acc: 0.0216 ::: bot acc: 0.0451
top acc: 0.0306 ::: bot acc: 0.0372
top acc: 0.0177 ::: bot acc: 0.0468
current epoch: 43
train loss is 0.042560
average val loss: 0.030853, accuracy: 0.0302
average test loss: 0.040576, accuracy: 0.0404
case acc: 0.04440128
case acc: 0.042500827
case acc: 0.0532588
case acc: 0.03705935
case acc: 0.031912126
case acc: 0.03319833
top acc: 0.0239 ::: bot acc: 0.0665
top acc: 0.0116 ::: bot acc: 0.0804
top acc: 0.0191 ::: bot acc: 0.0976
top acc: 0.0114 ::: bot acc: 0.0668
top acc: 0.0104 ::: bot acc: 0.0591
top acc: 0.0080 ::: bot acc: 0.0665
current epoch: 44
train loss is 0.046889
average val loss: 0.042279, accuracy: 0.0415
average test loss: 0.055974, accuracy: 0.0557
case acc: 0.063009724
case acc: 0.057245165
case acc: 0.069664165
case acc: 0.051369984
case acc: 0.045453228
case acc: 0.047284007
top acc: 0.0425 ::: bot acc: 0.0851
top acc: 0.0177 ::: bot acc: 0.0995
top acc: 0.0286 ::: bot acc: 0.1174
top acc: 0.0195 ::: bot acc: 0.0842
top acc: 0.0126 ::: bot acc: 0.0783
top acc: 0.0185 ::: bot acc: 0.0824
current epoch: 45
train loss is 0.052919
average val loss: 0.042759, accuracy: 0.0421
average test loss: 0.056569, accuracy: 0.0563
case acc: 0.06360391
case acc: 0.05631106
case acc: 0.0701752
case acc: 0.05139405
case acc: 0.049061295
case acc: 0.04699731
top acc: 0.0431 ::: bot acc: 0.0857
top acc: 0.0170 ::: bot acc: 0.0984
top acc: 0.0290 ::: bot acc: 0.1180
top acc: 0.0195 ::: bot acc: 0.0843
top acc: 0.0154 ::: bot acc: 0.0823
top acc: 0.0182 ::: bot acc: 0.0820
current epoch: 46
train loss is 0.054604
average val loss: 0.031158, accuracy: 0.0305
average test loss: 0.041071, accuracy: 0.0408
case acc: 0.044830568
case acc: 0.039718807
case acc: 0.05287629
case acc: 0.036405664
case acc: 0.03861533
case acc: 0.032555364
top acc: 0.0243 ::: bot acc: 0.0669
top acc: 0.0134 ::: bot acc: 0.0754
top acc: 0.0189 ::: bot acc: 0.0971
top acc: 0.0111 ::: bot acc: 0.0660
top acc: 0.0087 ::: bot acc: 0.0700
top acc: 0.0077 ::: bot acc: 0.0656
current epoch: 47
train loss is 0.048816
average val loss: 0.027082, accuracy: 0.0266
average test loss: 0.030554, accuracy: 0.0305
case acc: 0.027531555
case acc: 0.0320544
case acc: 0.038960453
case acc: 0.027361035
case acc: 0.031800773
case acc: 0.025392829
top acc: 0.0094 ::: bot acc: 0.0484
top acc: 0.0340 ::: bot acc: 0.0517
top acc: 0.0208 ::: bot acc: 0.0752
top acc: 0.0168 ::: bot acc: 0.0495
top acc: 0.0106 ::: bot acc: 0.0589
top acc: 0.0121 ::: bot acc: 0.0526
current epoch: 48
train loss is 0.042797
average val loss: 0.030140, accuracy: 0.0295
average test loss: 0.027037, accuracy: 0.0268
case acc: 0.01690567
case acc: 0.033729497
case acc: 0.03431141
case acc: 0.024798743
case acc: 0.027558466
case acc: 0.023607524
top acc: 0.0106 ::: bot acc: 0.0318
top acc: 0.0547 ::: bot acc: 0.0309
top acc: 0.0392 ::: bot acc: 0.0559
top acc: 0.0306 ::: bot acc: 0.0355
top acc: 0.0209 ::: bot acc: 0.0471
top acc: 0.0228 ::: bot acc: 0.0419
current epoch: 49
train loss is 0.041078
average val loss: 0.038546, accuracy: 0.0380
average test loss: 0.029279, accuracy: 0.0286
case acc: 0.017153956
case acc: 0.040085245
case acc: 0.03728764
case acc: 0.02644868
case acc: 0.025266374
case acc: 0.02554227
top acc: 0.0275 ::: bot acc: 0.0154
top acc: 0.0731 ::: bot acc: 0.0147
top acc: 0.0572 ::: bot acc: 0.0378
top acc: 0.0450 ::: bot acc: 0.0212
top acc: 0.0367 ::: bot acc: 0.0313
top acc: 0.0352 ::: bot acc: 0.0296
current epoch: 50
train loss is 0.043013
average val loss: 0.047153, accuracy: 0.0468
average test loss: 0.034297, accuracy: 0.0335
case acc: 0.024291124
case acc: 0.04682654
case acc: 0.042365152
case acc: 0.030151216
case acc: 0.027908577
case acc: 0.029747332
top acc: 0.0399 ::: bot acc: 0.0121
top acc: 0.0850 ::: bot acc: 0.0110
top acc: 0.0697 ::: bot acc: 0.0283
top acc: 0.0558 ::: bot acc: 0.0114
top acc: 0.0507 ::: bot acc: 0.0172
top acc: 0.0453 ::: bot acc: 0.0221
LME_Co_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5400 5400 5400
1.7082474 -0.6288155 0.17559324 -0.22413088
Validation: 606 606 606
Testing: 744 744 744
pre-processing time: 0.00044798851013183594
the split date is 2011-01-01
net initializing with time: 0.004045724868774414
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.174861
average val loss: 0.076451, accuracy: 0.0773
average test loss: 0.081493, accuracy: 0.0773
case acc: 0.04507204
case acc: 0.03547679
case acc: 0.045830164
case acc: 0.24472517
case acc: 0.04778341
case acc: 0.045181133
top acc: 0.0860 ::: bot acc: 0.0212
top acc: 0.0114 ::: bot acc: 0.0690
top acc: 0.0731 ::: bot acc: 0.0501
top acc: 0.2746 ::: bot acc: 0.2125
top acc: 0.0223 ::: bot acc: 0.0802
top acc: 0.0251 ::: bot acc: 0.0817
current epoch: 2
train loss is 0.136887
average val loss: 0.100870, accuracy: 0.1001
average test loss: 0.100218, accuracy: 0.0961
case acc: 0.05084215
case acc: 0.09377984
case acc: 0.07987267
case acc: 0.16206375
case acc: 0.09859221
case acc: 0.09161489
top acc: 0.0184 ::: bot acc: 0.0876
top acc: 0.0613 ::: bot acc: 0.1303
top acc: 0.0311 ::: bot acc: 0.1306
top acc: 0.1928 ::: bot acc: 0.1291
top acc: 0.0515 ::: bot acc: 0.1416
top acc: 0.0395 ::: bot acc: 0.1436
current epoch: 3
train loss is 0.125688
average val loss: 0.084770, accuracy: 0.0837
average test loss: 0.085767, accuracy: 0.0820
case acc: 0.042098716
case acc: 0.07073892
case acc: 0.07164596
case acc: 0.17091541
case acc: 0.070558794
case acc: 0.066313505
top acc: 0.0276 ::: bot acc: 0.0700
top acc: 0.0384 ::: bot acc: 0.1065
top acc: 0.0288 ::: bot acc: 0.1200
top acc: 0.2021 ::: bot acc: 0.1374
top acc: 0.0266 ::: bot acc: 0.1119
top acc: 0.0209 ::: bot acc: 0.1146
current epoch: 4
train loss is 0.104190
average val loss: 0.071938, accuracy: 0.0702
average test loss: 0.075234, accuracy: 0.0719
case acc: 0.036812913
case acc: 0.050259605
case acc: 0.06670637
case acc: 0.17587197
case acc: 0.05148952
case acc: 0.05046322
top acc: 0.0389 ::: bot acc: 0.0560
top acc: 0.0197 ::: bot acc: 0.0845
top acc: 0.0296 ::: bot acc: 0.1126
top acc: 0.2075 ::: bot acc: 0.1419
top acc: 0.0190 ::: bot acc: 0.0870
top acc: 0.0210 ::: bot acc: 0.0905
current epoch: 5
train loss is 0.095267
average val loss: 0.064281, accuracy: 0.0624
average test loss: 0.069279, accuracy: 0.0663
case acc: 0.035051588
case acc: 0.03739104
case acc: 0.06586587
case acc: 0.17328914
case acc: 0.042189535
case acc: 0.04396302
top acc: 0.0440 ::: bot acc: 0.0496
top acc: 0.0116 ::: bot acc: 0.0687
top acc: 0.0294 ::: bot acc: 0.1119
top acc: 0.2054 ::: bot acc: 0.1389
top acc: 0.0237 ::: bot acc: 0.0708
top acc: 0.0321 ::: bot acc: 0.0751
current epoch: 6
train loss is 0.090845
average val loss: 0.065136, accuracy: 0.0633
average test loss: 0.069275, accuracy: 0.0661
case acc: 0.037734203
case acc: 0.04092539
case acc: 0.07647004
case acc: 0.15377289
case acc: 0.042710315
case acc: 0.0447634
top acc: 0.0323 ::: bot acc: 0.0610
top acc: 0.0133 ::: bot acc: 0.0727
top acc: 0.0297 ::: bot acc: 0.1278
top acc: 0.1862 ::: bot acc: 0.1191
top acc: 0.0226 ::: bot acc: 0.0720
top acc: 0.0306 ::: bot acc: 0.0768
current epoch: 7
train loss is 0.087781
average val loss: 0.069716, accuracy: 0.0682
average test loss: 0.072172, accuracy: 0.0685
case acc: 0.04429244
case acc: 0.051126353
case acc: 0.09228107
case acc: 0.12984087
case acc: 0.04589891
case acc: 0.04763118
top acc: 0.0199 ::: bot acc: 0.0770
top acc: 0.0205 ::: bot acc: 0.0841
top acc: 0.0365 ::: bot acc: 0.1483
top acc: 0.1625 ::: bot acc: 0.0949
top acc: 0.0197 ::: bot acc: 0.0782
top acc: 0.0255 ::: bot acc: 0.0834
current epoch: 8
train loss is 0.087018
average val loss: 0.080303, accuracy: 0.0792
average test loss: 0.080432, accuracy: 0.0761
case acc: 0.05905283
case acc: 0.07101471
case acc: 0.11672668
case acc: 0.09889286
case acc: 0.055155862
case acc: 0.05567408
top acc: 0.0183 ::: bot acc: 0.0999
top acc: 0.0389 ::: bot acc: 0.1046
top acc: 0.0544 ::: bot acc: 0.1762
top acc: 0.1315 ::: bot acc: 0.0642
top acc: 0.0178 ::: bot acc: 0.0929
top acc: 0.0189 ::: bot acc: 0.0985
current epoch: 9
train loss is 0.092223
average val loss: 0.094165, accuracy: 0.0932
average test loss: 0.092796, accuracy: 0.0879
case acc: 0.08146176
case acc: 0.09314381
case acc: 0.14573908
case acc: 0.065491356
case acc: 0.071009584
case acc: 0.0708254
top acc: 0.0342 ::: bot acc: 0.1255
top acc: 0.0608 ::: bot acc: 0.1267
top acc: 0.0805 ::: bot acc: 0.2068
top acc: 0.0970 ::: bot acc: 0.0332
top acc: 0.0248 ::: bot acc: 0.1133
top acc: 0.0222 ::: bot acc: 0.1194
current epoch: 10
train loss is 0.108370
average val loss: 0.078063, accuracy: 0.0770
average test loss: 0.078235, accuracy: 0.0736
case acc: 0.06351563
case acc: 0.064083025
case acc: 0.1283421
case acc: 0.07573763
case acc: 0.05408929
case acc: 0.05596495
top acc: 0.0205 ::: bot acc: 0.1057
top acc: 0.0319 ::: bot acc: 0.0976
top acc: 0.0645 ::: bot acc: 0.1887
top acc: 0.1079 ::: bot acc: 0.0421
top acc: 0.0167 ::: bot acc: 0.0920
top acc: 0.0189 ::: bot acc: 0.0987
current epoch: 11
train loss is 0.101851
average val loss: 0.058495, accuracy: 0.0576
average test loss: 0.062657, accuracy: 0.0589
case acc: 0.043429777
case acc: 0.029418008
case acc: 0.09999344
case acc: 0.09596706
case acc: 0.04028637
case acc: 0.04413486
top acc: 0.0204 ::: bot acc: 0.0759
top acc: 0.0129 ::: bot acc: 0.0552
top acc: 0.0419 ::: bot acc: 0.1575
top acc: 0.1287 ::: bot acc: 0.0612
top acc: 0.0278 ::: bot acc: 0.0660
top acc: 0.0328 ::: bot acc: 0.0742
current epoch: 12
train loss is 0.082798
average val loss: 0.053930, accuracy: 0.0536
average test loss: 0.058874, accuracy: 0.0557
case acc: 0.04005984
case acc: 0.024422381
case acc: 0.09143552
case acc: 0.094949305
case acc: 0.03935575
case acc: 0.043774102
top acc: 0.0250 ::: bot acc: 0.0687
top acc: 0.0302 ::: bot acc: 0.0364
top acc: 0.0359 ::: bot acc: 0.1476
top acc: 0.1277 ::: bot acc: 0.0602
top acc: 0.0302 ::: bot acc: 0.0634
top acc: 0.0340 ::: bot acc: 0.0730
current epoch: 13
train loss is 0.070473
average val loss: 0.052123, accuracy: 0.0520
average test loss: 0.057119, accuracy: 0.0542
case acc: 0.038994413
case acc: 0.024797602
case acc: 0.08722994
case acc: 0.09005715
case acc: 0.03963259
case acc: 0.044311166
top acc: 0.0269 ::: bot acc: 0.0661
top acc: 0.0390 ::: bot acc: 0.0276
top acc: 0.0333 ::: bot acc: 0.1427
top acc: 0.1228 ::: bot acc: 0.0555
top acc: 0.0295 ::: bot acc: 0.0643
top acc: 0.0327 ::: bot acc: 0.0744
current epoch: 14
train loss is 0.066678
average val loss: 0.049086, accuracy: 0.0487
average test loss: 0.054651, accuracy: 0.0524
case acc: 0.036571447
case acc: 0.026728416
case acc: 0.080803044
case acc: 0.089463316
case acc: 0.038143102
case acc: 0.04283696
top acc: 0.0321 ::: bot acc: 0.0600
top acc: 0.0456 ::: bot acc: 0.0213
top acc: 0.0304 ::: bot acc: 0.1346
top acc: 0.1222 ::: bot acc: 0.0548
top acc: 0.0345 ::: bot acc: 0.0594
top acc: 0.0380 ::: bot acc: 0.0693
current epoch: 15
train loss is 0.063948
average val loss: 0.046911, accuracy: 0.0464
average test loss: 0.052819, accuracy: 0.0507
case acc: 0.035883673
case acc: 0.025930565
case acc: 0.07795309
case acc: 0.08551002
case acc: 0.037344914
case acc: 0.041835703
top acc: 0.0340 ::: bot acc: 0.0580
top acc: 0.0436 ::: bot acc: 0.0231
top acc: 0.0294 ::: bot acc: 0.1308
top acc: 0.1183 ::: bot acc: 0.0510
top acc: 0.0374 ::: bot acc: 0.0564
top acc: 0.0418 ::: bot acc: 0.0654
current epoch: 16
train loss is 0.063052
average val loss: 0.046294, accuracy: 0.0458
average test loss: 0.052093, accuracy: 0.0496
case acc: 0.037318736
case acc: 0.024378752
case acc: 0.08042691
case acc: 0.07564609
case acc: 0.037630808
case acc: 0.041931015
top acc: 0.0298 ::: bot acc: 0.0622
top acc: 0.0332 ::: bot acc: 0.0337
top acc: 0.0301 ::: bot acc: 0.1342
top acc: 0.1081 ::: bot acc: 0.0417
top acc: 0.0361 ::: bot acc: 0.0577
top acc: 0.0415 ::: bot acc: 0.0659
current epoch: 17
train loss is 0.062927
average val loss: 0.050569, accuracy: 0.0497
average test loss: 0.054929, accuracy: 0.0511
case acc: 0.04355975
case acc: 0.029386086
case acc: 0.09175771
case acc: 0.056597542
case acc: 0.040898897
case acc: 0.044651005
top acc: 0.0197 ::: bot acc: 0.0766
top acc: 0.0134 ::: bot acc: 0.0549
top acc: 0.0357 ::: bot acc: 0.1483
top acc: 0.0879 ::: bot acc: 0.0251
top acc: 0.0260 ::: bot acc: 0.0680
top acc: 0.0322 ::: bot acc: 0.0752
current epoch: 18
train loss is 0.064605
average val loss: 0.063035, accuracy: 0.0615
average test loss: 0.064514, accuracy: 0.0599
case acc: 0.05884961
case acc: 0.05017574
case acc: 0.112985045
case acc: 0.033158045
case acc: 0.05079248
case acc: 0.053371523
top acc: 0.0185 ::: bot acc: 0.1001
top acc: 0.0191 ::: bot acc: 0.0832
top acc: 0.0512 ::: bot acc: 0.1724
top acc: 0.0581 ::: bot acc: 0.0143
top acc: 0.0165 ::: bot acc: 0.0874
top acc: 0.0200 ::: bot acc: 0.0943
current epoch: 19
train loss is 0.073233
average val loss: 0.067761, accuracy: 0.0662
average test loss: 0.068588, accuracy: 0.0641
case acc: 0.065780826
case acc: 0.05788772
case acc: 0.12184956
case acc: 0.02662673
case acc: 0.055018492
case acc: 0.05759896
top acc: 0.0222 ::: bot acc: 0.1086
top acc: 0.0257 ::: bot acc: 0.0915
top acc: 0.0585 ::: bot acc: 0.1819
top acc: 0.0425 ::: bot acc: 0.0259
top acc: 0.0164 ::: bot acc: 0.0938
top acc: 0.0191 ::: bot acc: 0.1010
current epoch: 20
train loss is 0.081422
average val loss: 0.051782, accuracy: 0.0504
average test loss: 0.055417, accuracy: 0.0512
case acc: 0.0492468
case acc: 0.034122072
case acc: 0.10181571
case acc: 0.032661114
case acc: 0.042725313
case acc: 0.046831027
top acc: 0.0165 ::: bot acc: 0.0869
top acc: 0.0116 ::: bot acc: 0.0630
top acc: 0.0429 ::: bot acc: 0.1597
top acc: 0.0571 ::: bot acc: 0.0147
top acc: 0.0219 ::: bot acc: 0.0727
top acc: 0.0273 ::: bot acc: 0.0808
current epoch: 21
train loss is 0.071298
average val loss: 0.042553, accuracy: 0.0419
average test loss: 0.048147, accuracy: 0.0451
case acc: 0.040579956
case acc: 0.024477122
case acc: 0.08623392
case acc: 0.038604125
case acc: 0.038080707
case acc: 0.04278271
top acc: 0.0239 ::: bot acc: 0.0703
top acc: 0.0311 ::: bot acc: 0.0360
top acc: 0.0326 ::: bot acc: 0.1415
top acc: 0.0666 ::: bot acc: 0.0134
top acc: 0.0341 ::: bot acc: 0.0595
top acc: 0.0377 ::: bot acc: 0.0693
current epoch: 22
train loss is 0.058844
average val loss: 0.040824, accuracy: 0.0403
average test loss: 0.046598, accuracy: 0.0440
case acc: 0.039014403
case acc: 0.025555022
case acc: 0.081771925
case acc: 0.036817502
case acc: 0.037941948
case acc: 0.04296806
top acc: 0.0267 ::: bot acc: 0.0665
top acc: 0.0418 ::: bot acc: 0.0253
top acc: 0.0306 ::: bot acc: 0.1358
top acc: 0.0641 ::: bot acc: 0.0130
top acc: 0.0345 ::: bot acc: 0.0591
top acc: 0.0370 ::: bot acc: 0.0700
current epoch: 23
train loss is 0.053893
average val loss: 0.038731, accuracy: 0.0380
average test loss: 0.044900, accuracy: 0.0429
case acc: 0.037110325
case acc: 0.02749152
case acc: 0.07665087
case acc: 0.03646151
case acc: 0.037181146
case acc: 0.0422674
top acc: 0.0308 ::: bot acc: 0.0617
top acc: 0.0472 ::: bot acc: 0.0206
top acc: 0.0289 ::: bot acc: 0.1290
top acc: 0.0636 ::: bot acc: 0.0130
top acc: 0.0374 ::: bot acc: 0.0562
top acc: 0.0399 ::: bot acc: 0.0672
current epoch: 24
train loss is 0.051445
average val loss: 0.037433, accuracy: 0.0366
average test loss: 0.043821, accuracy: 0.0418
case acc: 0.03679126
case acc: 0.02639288
case acc: 0.074779056
case acc: 0.034077615
case acc: 0.036958
case acc: 0.041904
top acc: 0.0317 ::: bot acc: 0.0608
top acc: 0.0443 ::: bot acc: 0.0230
top acc: 0.0284 ::: bot acc: 0.1264
top acc: 0.0597 ::: bot acc: 0.0138
top acc: 0.0383 ::: bot acc: 0.0552
top acc: 0.0414 ::: bot acc: 0.0658
current epoch: 25
train loss is 0.050525
average val loss: 0.037632, accuracy: 0.0368
average test loss: 0.043925, accuracy: 0.0415
case acc: 0.038238216
case acc: 0.024422392
case acc: 0.07677706
case acc: 0.02977896
case acc: 0.037485342
case acc: 0.042333134
top acc: 0.0282 ::: bot acc: 0.0646
top acc: 0.0342 ::: bot acc: 0.0330
top acc: 0.0289 ::: bot acc: 0.1291
top acc: 0.0512 ::: bot acc: 0.0180
top acc: 0.0358 ::: bot acc: 0.0577
top acc: 0.0397 ::: bot acc: 0.0675
current epoch: 26
train loss is 0.050546
average val loss: 0.042095, accuracy: 0.0412
average test loss: 0.047376, accuracy: 0.0442
case acc: 0.043136455
case acc: 0.027231274
case acc: 0.08479989
case acc: 0.025050763
case acc: 0.040301796
case acc: 0.04479389
top acc: 0.0205 ::: bot acc: 0.0757
top acc: 0.0172 ::: bot acc: 0.0500
top acc: 0.0317 ::: bot acc: 0.1398
top acc: 0.0353 ::: bot acc: 0.0331
top acc: 0.0271 ::: bot acc: 0.0665
top acc: 0.0316 ::: bot acc: 0.0757
current epoch: 27
train loss is 0.052487
average val loss: 0.050162, accuracy: 0.0495
average test loss: 0.053795, accuracy: 0.0507
case acc: 0.050391115
case acc: 0.036621626
case acc: 0.095759965
case acc: 0.027501417
case acc: 0.044711336
case acc: 0.049148433
top acc: 0.0165 ::: bot acc: 0.0886
top acc: 0.0118 ::: bot acc: 0.0667
top acc: 0.0384 ::: bot acc: 0.1528
top acc: 0.0179 ::: bot acc: 0.0512
top acc: 0.0195 ::: bot acc: 0.0769
top acc: 0.0240 ::: bot acc: 0.0859
current epoch: 28
train loss is 0.057206
average val loss: 0.053795, accuracy: 0.0532
average test loss: 0.056863, accuracy: 0.0543
case acc: 0.053831883
case acc: 0.04010159
case acc: 0.10047023
case acc: 0.034320068
case acc: 0.046399098
case acc: 0.05088598
top acc: 0.0168 ::: bot acc: 0.0936
top acc: 0.0128 ::: bot acc: 0.0714
top acc: 0.0418 ::: bot acc: 0.1582
top acc: 0.0169 ::: bot acc: 0.0619
top acc: 0.0181 ::: bot acc: 0.0801
top acc: 0.0217 ::: bot acc: 0.0896
current epoch: 29
train loss is 0.061356
average val loss: 0.046141, accuracy: 0.0458
average test loss: 0.050734, accuracy: 0.0483
case acc: 0.047438353
case acc: 0.030330557
case acc: 0.09180368
case acc: 0.031504724
case acc: 0.041740622
case acc: 0.046892997
top acc: 0.0170 ::: bot acc: 0.0839
top acc: 0.0127 ::: bot acc: 0.0569
top acc: 0.0358 ::: bot acc: 0.1482
top acc: 0.0162 ::: bot acc: 0.0581
top acc: 0.0234 ::: bot acc: 0.0705
top acc: 0.0270 ::: bot acc: 0.0810
current epoch: 30
train loss is 0.058302
average val loss: 0.037367, accuracy: 0.0373
average test loss: 0.043668, accuracy: 0.0419
case acc: 0.040328477
case acc: 0.024440425
case acc: 0.0794063
case acc: 0.02659469
case acc: 0.03764104
case acc: 0.042988267
top acc: 0.0244 ::: bot acc: 0.0695
top acc: 0.0321 ::: bot acc: 0.0350
top acc: 0.0298 ::: bot acc: 0.1326
top acc: 0.0193 ::: bot acc: 0.0491
top acc: 0.0348 ::: bot acc: 0.0585
top acc: 0.0364 ::: bot acc: 0.0703
current epoch: 31
train loss is 0.050418
average val loss: 0.033487, accuracy: 0.0331
average test loss: 0.040486, accuracy: 0.0396
case acc: 0.03633943
case acc: 0.027995657
case acc: 0.07075161
case acc: 0.024881987
case acc: 0.036265213
case acc: 0.041514874
top acc: 0.0332 ::: bot acc: 0.0592
top acc: 0.0481 ::: bot acc: 0.0199
top acc: 0.0279 ::: bot acc: 0.1206
top acc: 0.0250 ::: bot acc: 0.0434
top acc: 0.0416 ::: bot acc: 0.0518
top acc: 0.0422 ::: bot acc: 0.0646
current epoch: 32
train loss is 0.044955
average val loss: 0.031317, accuracy: 0.0305
average test loss: 0.038732, accuracy: 0.0386
case acc: 0.03427439
case acc: 0.033301666
case acc: 0.06383201
case acc: 0.024573274
case acc: 0.03541371
case acc: 0.040311284
top acc: 0.0416 ::: bot acc: 0.0506
top acc: 0.0573 ::: bot acc: 0.0174
top acc: 0.0286 ::: bot acc: 0.1100
top acc: 0.0303 ::: bot acc: 0.0380
top acc: 0.0471 ::: bot acc: 0.0464
top acc: 0.0475 ::: bot acc: 0.0592
current epoch: 33
train loss is 0.042671
average val loss: 0.029949, accuracy: 0.0290
average test loss: 0.037576, accuracy: 0.0378
case acc: 0.033691674
case acc: 0.034640297
case acc: 0.059116233
case acc: 0.024929859
case acc: 0.03515204
case acc: 0.039563295
top acc: 0.0477 ::: bot acc: 0.0443
top acc: 0.0593 ::: bot acc: 0.0174
top acc: 0.0315 ::: bot acc: 0.1015
top acc: 0.0346 ::: bot acc: 0.0338
top acc: 0.0512 ::: bot acc: 0.0423
top acc: 0.0525 ::: bot acc: 0.0543
current epoch: 34
train loss is 0.041761
average val loss: 0.028989, accuracy: 0.0281
average test loss: 0.036748, accuracy: 0.0370
case acc: 0.033472285
case acc: 0.032950256
case acc: 0.05608066
case acc: 0.025285382
case acc: 0.035144895
case acc: 0.039233286
top acc: 0.0514 ::: bot acc: 0.0405
top acc: 0.0569 ::: bot acc: 0.0173
top acc: 0.0344 ::: bot acc: 0.0956
top acc: 0.0370 ::: bot acc: 0.0314
top acc: 0.0539 ::: bot acc: 0.0396
top acc: 0.0565 ::: bot acc: 0.0504
current epoch: 35
train loss is 0.041142
average val loss: 0.028329, accuracy: 0.0276
average test loss: 0.036254, accuracy: 0.0361
case acc: 0.03342796
case acc: 0.02839906
case acc: 0.055736843
case acc: 0.024914477
case acc: 0.035154365
case acc: 0.03922453
top acc: 0.0500 ::: bot acc: 0.0417
top acc: 0.0492 ::: bot acc: 0.0193
top acc: 0.0348 ::: bot acc: 0.0949
top acc: 0.0346 ::: bot acc: 0.0338
top acc: 0.0528 ::: bot acc: 0.0407
top acc: 0.0568 ::: bot acc: 0.0501
current epoch: 36
train loss is 0.040693
average val loss: 0.029017, accuracy: 0.0285
average test loss: 0.037051, accuracy: 0.0363
case acc: 0.033923063
case acc: 0.024496377
case acc: 0.058989342
case acc: 0.024915064
case acc: 0.03558422
case acc: 0.039675593
top acc: 0.0421 ::: bot acc: 0.0495
top acc: 0.0364 ::: bot acc: 0.0309
top acc: 0.0318 ::: bot acc: 0.1012
top acc: 0.0253 ::: bot acc: 0.0432
top acc: 0.0463 ::: bot acc: 0.0473
top acc: 0.0515 ::: bot acc: 0.0555
current epoch: 37
train loss is 0.041018
average val loss: 0.032503, accuracy: 0.0321
average test loss: 0.039988, accuracy: 0.0391
case acc: 0.03634479
case acc: 0.025731193
case acc: 0.06425766
case acc: 0.030211702
case acc: 0.036995217
case acc: 0.041123938
top acc: 0.0320 ::: bot acc: 0.0597
top acc: 0.0232 ::: bot acc: 0.0442
top acc: 0.0284 ::: bot acc: 0.1108
top acc: 0.0163 ::: bot acc: 0.0559
top acc: 0.0382 ::: bot acc: 0.0554
top acc: 0.0440 ::: bot acc: 0.0629
current epoch: 38
train loss is 0.042176
average val loss: 0.039295, accuracy: 0.0389
average test loss: 0.045192, accuracy: 0.0446
case acc: 0.0409955
case acc: 0.03050781
case acc: 0.07181417
case acc: 0.04136022
case acc: 0.039448075
case acc: 0.04340022
top acc: 0.0228 ::: bot acc: 0.0712
top acc: 0.0127 ::: bot acc: 0.0571
top acc: 0.0278 ::: bot acc: 0.1224
top acc: 0.0199 ::: bot acc: 0.0708
top acc: 0.0295 ::: bot acc: 0.0641
top acc: 0.0353 ::: bot acc: 0.0716
current epoch: 39
train loss is 0.044644
average val loss: 0.048425, accuracy: 0.0480
average test loss: 0.052122, accuracy: 0.0518
case acc: 0.047008168
case acc: 0.03787105
case acc: 0.08102823
case acc: 0.055265542
case acc: 0.042753525
case acc: 0.04694822
top acc: 0.0171 ::: bot acc: 0.0831
top acc: 0.0120 ::: bot acc: 0.0684
top acc: 0.0300 ::: bot acc: 0.1351
top acc: 0.0293 ::: bot acc: 0.0869
top acc: 0.0214 ::: bot acc: 0.0730
top acc: 0.0270 ::: bot acc: 0.0811
current epoch: 40
train loss is 0.048747
average val loss: 0.051707, accuracy: 0.0515
average test loss: 0.054682, accuracy: 0.0546
case acc: 0.049370624
case acc: 0.03814003
case acc: 0.084792376
case acc: 0.063347526
case acc: 0.04351642
case acc: 0.048230484
top acc: 0.0165 ::: bot acc: 0.0869
top acc: 0.0121 ::: bot acc: 0.0688
top acc: 0.0314 ::: bot acc: 0.1400
top acc: 0.0359 ::: bot acc: 0.0957
top acc: 0.0205 ::: bot acc: 0.0746
top acc: 0.0251 ::: bot acc: 0.0839
current epoch: 41
train loss is 0.051745
average val loss: 0.045004, accuracy: 0.0451
average test loss: 0.049548, accuracy: 0.0497
case acc: 0.044625904
case acc: 0.029163541
case acc: 0.07884947
case acc: 0.06025404
case acc: 0.040120475
case acc: 0.045168594
top acc: 0.0188 ::: bot acc: 0.0787
top acc: 0.0138 ::: bot acc: 0.0546
top acc: 0.0294 ::: bot acc: 0.1320
top acc: 0.0334 ::: bot acc: 0.0924
top acc: 0.0275 ::: bot acc: 0.0661
top acc: 0.0300 ::: bot acc: 0.0769
current epoch: 42
train loss is 0.051752
average val loss: 0.033904, accuracy: 0.0342
average test loss: 0.040955, accuracy: 0.0421
case acc: 0.03676426
case acc: 0.024761405
case acc: 0.06585473
case acc: 0.047768213
case acc: 0.036141902
case acc: 0.04113658
top acc: 0.0316 ::: bot acc: 0.0606
top acc: 0.0381 ::: bot acc: 0.0293
top acc: 0.0282 ::: bot acc: 0.1132
top acc: 0.0239 ::: bot acc: 0.0784
top acc: 0.0427 ::: bot acc: 0.0508
top acc: 0.0435 ::: bot acc: 0.0632
current epoch: 43
train loss is 0.047540
average val loss: 0.028948, accuracy: 0.0287
average test loss: 0.036940, accuracy: 0.0390
case acc: 0.033617537
case acc: 0.03634405
case acc: 0.054876518
case acc: 0.03485677
case acc: 0.035146587
case acc: 0.03922358
top acc: 0.0495 ::: bot acc: 0.0426
top acc: 0.0619 ::: bot acc: 0.0176
top acc: 0.0357 ::: bot acc: 0.0931
top acc: 0.0170 ::: bot acc: 0.0625
top acc: 0.0551 ::: bot acc: 0.0384
top acc: 0.0550 ::: bot acc: 0.0517
current epoch: 44
train loss is 0.043811
average val loss: 0.031279, accuracy: 0.0313
average test loss: 0.038527, accuracy: 0.0402
case acc: 0.03813611
case acc: 0.055251956
case acc: 0.046622787
case acc: 0.024479922
case acc: 0.036782317
case acc: 0.03985485
top acc: 0.0721 ::: bot acc: 0.0204
top acc: 0.0859 ::: bot acc: 0.0262
top acc: 0.0593 ::: bot acc: 0.0671
top acc: 0.0293 ::: bot acc: 0.0390
top acc: 0.0703 ::: bot acc: 0.0232
top acc: 0.0705 ::: bot acc: 0.0362
current epoch: 45
train loss is 0.045162
average val loss: 0.045604, accuracy: 0.0464
average test loss: 0.051182, accuracy: 0.0517
case acc: 0.05575677
case acc: 0.0775059
case acc: 0.049411967
case acc: 0.03464707
case acc: 0.045850873
case acc: 0.047315996
top acc: 0.0995 ::: bot acc: 0.0181
top acc: 0.1103 ::: bot acc: 0.0442
top acc: 0.0916 ::: bot acc: 0.0349
top acc: 0.0609 ::: bot acc: 0.0135
top acc: 0.0898 ::: bot acc: 0.0127
top acc: 0.0918 ::: bot acc: 0.0175
current epoch: 46
train loss is 0.050625
average val loss: 0.060161, accuracy: 0.0605
average test loss: 0.064758, accuracy: 0.0648
case acc: 0.07227797
case acc: 0.08795524
case acc: 0.06093154
case acc: 0.055271436
case acc: 0.05522648
case acc: 0.056955144
top acc: 0.1186 ::: bot acc: 0.0293
top acc: 0.1210 ::: bot acc: 0.0542
top acc: 0.1158 ::: bot acc: 0.0212
top acc: 0.0864 ::: bot acc: 0.0243
top acc: 0.1023 ::: bot acc: 0.0159
top acc: 0.1072 ::: bot acc: 0.0155
current epoch: 47
train loss is 0.056793
average val loss: 0.044732, accuracy: 0.0449
average test loss: 0.050325, accuracy: 0.0500
case acc: 0.05700859
case acc: 0.05816435
case acc: 0.052879494
case acc: 0.045004208
case acc: 0.040784758
case acc: 0.04635954
top acc: 0.1010 ::: bot acc: 0.0188
top acc: 0.0893 ::: bot acc: 0.0281
top acc: 0.1011 ::: bot acc: 0.0265
top acc: 0.0749 ::: bot acc: 0.0165
top acc: 0.0816 ::: bot acc: 0.0141
top acc: 0.0899 ::: bot acc: 0.0185
current epoch: 48
train loss is 0.053296
average val loss: 0.029617, accuracy: 0.0297
average test loss: 0.037032, accuracy: 0.0362
case acc: 0.038647164
case acc: 0.027960042
case acc: 0.04613262
case acc: 0.029651787
case acc: 0.035292596
case acc: 0.039461486
top acc: 0.0733 ::: bot acc: 0.0193
top acc: 0.0483 ::: bot acc: 0.0200
top acc: 0.0742 ::: bot acc: 0.0525
top acc: 0.0513 ::: bot acc: 0.0179
top acc: 0.0567 ::: bot acc: 0.0369
top acc: 0.0680 ::: bot acc: 0.0388
current epoch: 49
train loss is 0.045413
average val loss: 0.027524, accuracy: 0.0267
average test loss: 0.035775, accuracy: 0.0346
case acc: 0.033529412
case acc: 0.026348818
case acc: 0.047810808
case acc: 0.024677929
case acc: 0.03595885
case acc: 0.039139368
top acc: 0.0536 ::: bot acc: 0.0380
top acc: 0.0211 ::: bot acc: 0.0465
top acc: 0.0535 ::: bot acc: 0.0731
top acc: 0.0322 ::: bot acc: 0.0363
top acc: 0.0443 ::: bot acc: 0.0494
top acc: 0.0573 ::: bot acc: 0.0496
current epoch: 50
train loss is 0.042114
average val loss: 0.035014, accuracy: 0.0339
average test loss: 0.041929, accuracy: 0.0415
case acc: 0.036854587
case acc: 0.038585622
case acc: 0.057748824
case acc: 0.035217375
case acc: 0.039236005
case acc: 0.04131033
top acc: 0.0304 ::: bot acc: 0.0613
top acc: 0.0123 ::: bot acc: 0.0694
top acc: 0.0330 ::: bot acc: 0.0989
top acc: 0.0173 ::: bot acc: 0.0629
top acc: 0.0302 ::: bot acc: 0.0635
top acc: 0.0431 ::: bot acc: 0.0637
LME_Co_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5376 5376 5376
1.7082474 -0.6288155 0.17559324 -0.22413088
Validation: 600 600 600
Testing: 774 774 774
pre-processing time: 0.0004038810729980469
the split date is 2011-07-01
net initializing with time: 0.003618955612182617
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.274154
average val loss: 0.192574, accuracy: 0.1940
average test loss: 0.195168, accuracy: 0.1933
case acc: 0.30230916
case acc: 0.2466287
case acc: 0.2824972
case acc: 0.071480975
case acc: 0.060357854
case acc: 0.19675227
top acc: 0.2578 ::: bot acc: 0.3465
top acc: 0.2066 ::: bot acc: 0.2841
top acc: 0.2321 ::: bot acc: 0.3306
top acc: 0.0313 ::: bot acc: 0.1179
top acc: 0.1146 ::: bot acc: 0.0159
top acc: 0.1418 ::: bot acc: 0.2476
current epoch: 2
train loss is 0.221166
average val loss: 0.134367, accuracy: 0.1316
average test loss: 0.136126, accuracy: 0.1367
case acc: 0.20037907
case acc: 0.14692998
case acc: 0.18243514
case acc: 0.050799254
case acc: 0.14602433
case acc: 0.093804516
top acc: 0.1557 ::: bot acc: 0.2446
top acc: 0.1072 ::: bot acc: 0.1841
top acc: 0.1319 ::: bot acc: 0.2304
top acc: 0.0918 ::: bot acc: 0.0262
top acc: 0.2027 ::: bot acc: 0.0970
top acc: 0.0412 ::: bot acc: 0.1437
current epoch: 3
train loss is 0.154446
average val loss: 0.122240, accuracy: 0.1195
average test loss: 0.123746, accuracy: 0.1246
case acc: 0.17768373
case acc: 0.12696776
case acc: 0.15996218
case acc: 0.057161976
case acc: 0.14935288
case acc: 0.0764659
top acc: 0.1329 ::: bot acc: 0.2220
top acc: 0.0875 ::: bot acc: 0.1640
top acc: 0.1094 ::: bot acc: 0.2079
top acc: 0.1033 ::: bot acc: 0.0222
top acc: 0.2063 ::: bot acc: 0.1004
top acc: 0.0274 ::: bot acc: 0.1247
current epoch: 4
train loss is 0.152184
average val loss: 0.112789, accuracy: 0.1103
average test loss: 0.114144, accuracy: 0.1151
case acc: 0.16149132
case acc: 0.11403238
case acc: 0.14431673
case acc: 0.059237562
case acc: 0.14549702
case acc: 0.06582772
top acc: 0.1166 ::: bot acc: 0.2059
top acc: 0.0747 ::: bot acc: 0.1509
top acc: 0.0937 ::: bot acc: 0.1922
top acc: 0.1069 ::: bot acc: 0.0211
top acc: 0.2026 ::: bot acc: 0.0967
top acc: 0.0213 ::: bot acc: 0.1118
current epoch: 5
train loss is 0.135172
average val loss: 0.106726, accuracy: 0.1044
average test loss: 0.108152, accuracy: 0.1090
case acc: 0.15467705
case acc: 0.11014082
case acc: 0.13710794
case acc: 0.05519128
case acc: 0.13281043
case acc: 0.06421484
top acc: 0.1097 ::: bot acc: 0.1992
top acc: 0.0710 ::: bot acc: 0.1469
top acc: 0.0865 ::: bot acc: 0.1850
top acc: 0.1003 ::: bot acc: 0.0221
top acc: 0.1900 ::: bot acc: 0.0841
top acc: 0.0206 ::: bot acc: 0.1098
current epoch: 6
train loss is 0.131039
average val loss: 0.101180, accuracy: 0.0991
average test loss: 0.102708, accuracy: 0.1034
case acc: 0.14864527
case acc: 0.1072483
case acc: 0.13094224
case acc: 0.051089406
case acc: 0.11969422
case acc: 0.0630185
top acc: 0.1035 ::: bot acc: 0.1932
top acc: 0.0681 ::: bot acc: 0.1440
top acc: 0.0803 ::: bot acc: 0.1789
top acc: 0.0931 ::: bot acc: 0.0242
top acc: 0.1770 ::: bot acc: 0.0711
top acc: 0.0201 ::: bot acc: 0.1082
current epoch: 7
train loss is 0.123331
average val loss: 0.093745, accuracy: 0.0920
average test loss: 0.095244, accuracy: 0.0959
case acc: 0.137023
case acc: 0.09865987
case acc: 0.11888397
case acc: 0.050201084
case acc: 0.11268398
case acc: 0.05822745
top acc: 0.0919 ::: bot acc: 0.1815
top acc: 0.0596 ::: bot acc: 0.1354
top acc: 0.0683 ::: bot acc: 0.1669
top acc: 0.0915 ::: bot acc: 0.0245
top acc: 0.1700 ::: bot acc: 0.0642
top acc: 0.0195 ::: bot acc: 0.1014
current epoch: 8
train loss is 0.113072
average val loss: 0.091716, accuracy: 0.0902
average test loss: 0.093526, accuracy: 0.0935
case acc: 0.1390349
case acc: 0.10376556
case acc: 0.120456085
case acc: 0.042793095
case acc: 0.09234121
case acc: 0.06290047
top acc: 0.0939 ::: bot acc: 0.1834
top acc: 0.0647 ::: bot acc: 0.1405
top acc: 0.0698 ::: bot acc: 0.1686
top acc: 0.0765 ::: bot acc: 0.0324
top acc: 0.1497 ::: bot acc: 0.0439
top acc: 0.0199 ::: bot acc: 0.1082
current epoch: 9
train loss is 0.110861
average val loss: 0.084743, accuracy: 0.0835
average test loss: 0.086503, accuracy: 0.0865
case acc: 0.12782176
case acc: 0.095817804
case acc: 0.1093738
case acc: 0.042137492
case acc: 0.08547059
case acc: 0.0581871
top acc: 0.0826 ::: bot acc: 0.1722
top acc: 0.0568 ::: bot acc: 0.1325
top acc: 0.0591 ::: bot acc: 0.1573
top acc: 0.0749 ::: bot acc: 0.0335
top acc: 0.1428 ::: bot acc: 0.0372
top acc: 0.0193 ::: bot acc: 0.1015
current epoch: 10
train loss is 0.102173
average val loss: 0.083156, accuracy: 0.0822
average test loss: 0.085038, accuracy: 0.0845
case acc: 0.1284934
case acc: 0.09990598
case acc: 0.110205844
case acc: 0.03873497
case acc: 0.06789085
case acc: 0.061570797
top acc: 0.0832 ::: bot acc: 0.1729
top acc: 0.0608 ::: bot acc: 0.1366
top acc: 0.0599 ::: bot acc: 0.1582
top acc: 0.0616 ::: bot acc: 0.0463
top acc: 0.1242 ::: bot acc: 0.0218
top acc: 0.0194 ::: bot acc: 0.1065
current epoch: 11
train loss is 0.098795
average val loss: 0.076941, accuracy: 0.0762
average test loss: 0.078792, accuracy: 0.0782
case acc: 0.11814235
case acc: 0.09318635
case acc: 0.10059217
case acc: 0.038574744
case acc: 0.06123294
case acc: 0.05739356
top acc: 0.0728 ::: bot acc: 0.1626
top acc: 0.0542 ::: bot acc: 0.1298
top acc: 0.0508 ::: bot acc: 0.1483
top acc: 0.0597 ::: bot acc: 0.0482
top acc: 0.1168 ::: bot acc: 0.0168
top acc: 0.0192 ::: bot acc: 0.1003
current epoch: 12
train loss is 0.092927
average val loss: 0.073402, accuracy: 0.0731
average test loss: 0.075331, accuracy: 0.0746
case acc: 0.112909704
case acc: 0.0916303
case acc: 0.0959804
case acc: 0.038596947
case acc: 0.05159918
case acc: 0.056797422
top acc: 0.0675 ::: bot acc: 0.1573
top acc: 0.0526 ::: bot acc: 0.1283
top acc: 0.0466 ::: bot acc: 0.1435
top acc: 0.0527 ::: bot acc: 0.0551
top acc: 0.1045 ::: bot acc: 0.0127
top acc: 0.0192 ::: bot acc: 0.0994
current epoch: 13
train loss is 0.085328
average val loss: 0.072080, accuracy: 0.0722
average test loss: 0.074247, accuracy: 0.0736
case acc: 0.111140124
case acc: 0.093671545
case acc: 0.095148414
case acc: 0.040690962
case acc: 0.042670414
case acc: 0.05833211
top acc: 0.0657 ::: bot acc: 0.1556
top acc: 0.0548 ::: bot acc: 0.1303
top acc: 0.0458 ::: bot acc: 0.1426
top acc: 0.0426 ::: bot acc: 0.0652
top acc: 0.0888 ::: bot acc: 0.0172
top acc: 0.0191 ::: bot acc: 0.1018
current epoch: 14
train loss is 0.083031
average val loss: 0.069478, accuracy: 0.0699
average test loss: 0.071854, accuracy: 0.0716
case acc: 0.10576873
case acc: 0.092331834
case acc: 0.09157837
case acc: 0.042783715
case acc: 0.040053546
case acc: 0.057302296
top acc: 0.0603 ::: bot acc: 0.1502
top acc: 0.0534 ::: bot acc: 0.1289
top acc: 0.0426 ::: bot acc: 0.1389
top acc: 0.0365 ::: bot acc: 0.0714
top acc: 0.0768 ::: bot acc: 0.0290
top acc: 0.0192 ::: bot acc: 0.1003
current epoch: 15
train loss is 0.078615
average val loss: 0.064916, accuracy: 0.0656
average test loss: 0.067386, accuracy: 0.0675
case acc: 0.09646293
case acc: 0.08693357
case acc: 0.084152974
case acc: 0.043613166
case acc: 0.039600056
case acc: 0.053957622
top acc: 0.0513 ::: bot acc: 0.1407
top acc: 0.0480 ::: bot acc: 0.1235
top acc: 0.0359 ::: bot acc: 0.1310
top acc: 0.0349 ::: bot acc: 0.0734
top acc: 0.0691 ::: bot acc: 0.0367
top acc: 0.0201 ::: bot acc: 0.0948
current epoch: 16
train loss is 0.071725
average val loss: 0.064441, accuracy: 0.0658
average test loss: 0.067196, accuracy: 0.0677
case acc: 0.093334906
case acc: 0.08770102
case acc: 0.08271583
case acc: 0.047260605
case acc: 0.040544234
case acc: 0.054418318
top acc: 0.0483 ::: bot acc: 0.1375
top acc: 0.0488 ::: bot acc: 0.1243
top acc: 0.0346 ::: bot acc: 0.1295
top acc: 0.0301 ::: bot acc: 0.0813
top acc: 0.0556 ::: bot acc: 0.0502
top acc: 0.0198 ::: bot acc: 0.0956
current epoch: 17
train loss is 0.068645
average val loss: 0.060981, accuracy: 0.0627
average test loss: 0.063721, accuracy: 0.0643
case acc: 0.08485232
case acc: 0.082724676
case acc: 0.07641051
case acc: 0.048270676
case acc: 0.04194707
case acc: 0.051747378
top acc: 0.0404 ::: bot acc: 0.1287
top acc: 0.0440 ::: bot acc: 0.1192
top acc: 0.0295 ::: bot acc: 0.1226
top acc: 0.0291 ::: bot acc: 0.0832
top acc: 0.0480 ::: bot acc: 0.0577
top acc: 0.0213 ::: bot acc: 0.0909
current epoch: 18
train loss is 0.062655
average val loss: 0.059073, accuracy: 0.0610
average test loss: 0.061827, accuracy: 0.0626
case acc: 0.078423664
case acc: 0.07914934
case acc: 0.0723161
case acc: 0.050300498
case acc: 0.044767305
case acc: 0.050380405
top acc: 0.0346 ::: bot acc: 0.1220
top acc: 0.0407 ::: bot acc: 0.1155
top acc: 0.0270 ::: bot acc: 0.1177
top acc: 0.0282 ::: bot acc: 0.0867
top acc: 0.0393 ::: bot acc: 0.0668
top acc: 0.0223 ::: bot acc: 0.0884
current epoch: 19
train loss is 0.058795
average val loss: 0.058368, accuracy: 0.0603
average test loss: 0.061108, accuracy: 0.0619
case acc: 0.07336167
case acc: 0.0767328
case acc: 0.06971977
case acc: 0.05297471
case acc: 0.048998397
case acc: 0.049729478
top acc: 0.0302 ::: bot acc: 0.1165
top acc: 0.0384 ::: bot acc: 0.1131
top acc: 0.0255 ::: bot acc: 0.1145
top acc: 0.0275 ::: bot acc: 0.0911
top acc: 0.0326 ::: bot acc: 0.0765
top acc: 0.0228 ::: bot acc: 0.0872
current epoch: 20
train loss is 0.056337
average val loss: 0.055670, accuracy: 0.0576
average test loss: 0.058215, accuracy: 0.0590
case acc: 0.06566394
case acc: 0.07095461
case acc: 0.064744785
case acc: 0.053355783
case acc: 0.051792085
case acc: 0.047503494
top acc: 0.0244 ::: bot acc: 0.1079
top acc: 0.0330 ::: bot acc: 0.1071
top acc: 0.0231 ::: bot acc: 0.1082
top acc: 0.0275 ::: bot acc: 0.0916
top acc: 0.0295 ::: bot acc: 0.0822
top acc: 0.0251 ::: bot acc: 0.0827
current epoch: 21
train loss is 0.052170
average val loss: 0.051036, accuracy: 0.0529
average test loss: 0.053183, accuracy: 0.0538
case acc: 0.055660233
case acc: 0.061650604
case acc: 0.05763431
case acc: 0.05119167
case acc: 0.052419893
case acc: 0.044381745
top acc: 0.0183 ::: bot acc: 0.0958
top acc: 0.0243 ::: bot acc: 0.0974
top acc: 0.0210 ::: bot acc: 0.0986
top acc: 0.0281 ::: bot acc: 0.0881
top acc: 0.0288 ::: bot acc: 0.0835
top acc: 0.0312 ::: bot acc: 0.0750
current epoch: 22
train loss is 0.047624
average val loss: 0.049180, accuracy: 0.0509
average test loss: 0.051143, accuracy: 0.0516
case acc: 0.049992625
case acc: 0.056507245
case acc: 0.054227266
case acc: 0.05106902
case acc: 0.054756608
case acc: 0.043255497
top acc: 0.0161 ::: bot acc: 0.0883
top acc: 0.0199 ::: bot acc: 0.0919
top acc: 0.0209 ::: bot acc: 0.0935
top acc: 0.0281 ::: bot acc: 0.0879
top acc: 0.0274 ::: bot acc: 0.0876
top acc: 0.0345 ::: bot acc: 0.0717
current epoch: 23
train loss is 0.046025
average val loss: 0.045732, accuracy: 0.0474
average test loss: 0.047425, accuracy: 0.0477
case acc: 0.043419898
case acc: 0.048835415
case acc: 0.049283694
case acc: 0.048550915
case acc: 0.054500677
case acc: 0.041595504
top acc: 0.0168 ::: bot acc: 0.0781
top acc: 0.0147 ::: bot acc: 0.0830
top acc: 0.0218 ::: bot acc: 0.0856
top acc: 0.0291 ::: bot acc: 0.0836
top acc: 0.0275 ::: bot acc: 0.0871
top acc: 0.0407 ::: bot acc: 0.0654
current epoch: 24
train loss is 0.044387
average val loss: 0.041540, accuracy: 0.0429
average test loss: 0.042984, accuracy: 0.0431
case acc: 0.037968613
case acc: 0.040140744
case acc: 0.0437016
case acc: 0.044611342
case acc: 0.05201131
case acc: 0.040368855
top acc: 0.0247 ::: bot acc: 0.0659
top acc: 0.0118 ::: bot acc: 0.0714
top acc: 0.0259 ::: bot acc: 0.0752
top acc: 0.0332 ::: bot acc: 0.0757
top acc: 0.0289 ::: bot acc: 0.0826
top acc: 0.0493 ::: bot acc: 0.0568
current epoch: 25
train loss is 0.043149
average val loss: 0.038309, accuracy: 0.0394
average test loss: 0.039631, accuracy: 0.0397
case acc: 0.034415126
case acc: 0.03409816
case acc: 0.039621938
case acc: 0.041029193
case acc: 0.049020663
case acc: 0.039766498
top acc: 0.0348 ::: bot acc: 0.0548
top acc: 0.0167 ::: bot acc: 0.0598
top acc: 0.0348 ::: bot acc: 0.0646
top acc: 0.0407 ::: bot acc: 0.0665
top acc: 0.0321 ::: bot acc: 0.0765
top acc: 0.0574 ::: bot acc: 0.0486
current epoch: 26
train loss is 0.042336
average val loss: 0.037270, accuracy: 0.0382
average test loss: 0.038521, accuracy: 0.0385
case acc: 0.03351889
case acc: 0.031677715
case acc: 0.038368788
case acc: 0.039588068
case acc: 0.04795329
case acc: 0.039602436
top acc: 0.0393 ::: bot acc: 0.0503
top acc: 0.0212 ::: bot acc: 0.0539
top acc: 0.0400 ::: bot acc: 0.0594
top acc: 0.0454 ::: bot acc: 0.0618
top acc: 0.0333 ::: bot acc: 0.0742
top acc: 0.0598 ::: bot acc: 0.0461
current epoch: 27
train loss is 0.041422
average val loss: 0.036423, accuracy: 0.0370
average test loss: 0.037576, accuracy: 0.0374
case acc: 0.033078603
case acc: 0.029801613
case acc: 0.03745507
case acc: 0.03847919
case acc: 0.046154145
case acc: 0.039604194
top acc: 0.0432 ::: bot acc: 0.0463
top acc: 0.0270 ::: bot acc: 0.0480
top acc: 0.0454 ::: bot acc: 0.0540
top acc: 0.0512 ::: bot acc: 0.0560
top acc: 0.0357 ::: bot acc: 0.0704
top acc: 0.0624 ::: bot acc: 0.0434
current epoch: 28
train loss is 0.040399
average val loss: 0.036071, accuracy: 0.0364
average test loss: 0.037202, accuracy: 0.0370
case acc: 0.03302535
case acc: 0.029024368
case acc: 0.03723236
case acc: 0.03818425
case acc: 0.045126937
case acc: 0.039569657
top acc: 0.0437 ::: bot acc: 0.0457
top acc: 0.0298 ::: bot acc: 0.0451
top acc: 0.0477 ::: bot acc: 0.0518
top acc: 0.0548 ::: bot acc: 0.0523
top acc: 0.0374 ::: bot acc: 0.0679
top acc: 0.0623 ::: bot acc: 0.0435
current epoch: 29
train loss is 0.039683
average val loss: 0.036058, accuracy: 0.0362
average test loss: 0.037243, accuracy: 0.0371
case acc: 0.03324921
case acc: 0.02915522
case acc: 0.037372544
case acc: 0.03813063
case acc: 0.045059543
case acc: 0.039518535
top acc: 0.0407 ::: bot acc: 0.0487
top acc: 0.0292 ::: bot acc: 0.0457
top acc: 0.0462 ::: bot acc: 0.0533
top acc: 0.0556 ::: bot acc: 0.0514
top acc: 0.0374 ::: bot acc: 0.0678
top acc: 0.0592 ::: bot acc: 0.0465
current epoch: 30
train loss is 0.038312
average val loss: 0.036276, accuracy: 0.0363
average test loss: 0.037524, accuracy: 0.0374
case acc: 0.033952862
case acc: 0.029611211
case acc: 0.037834205
case acc: 0.03811662
case acc: 0.045285873
case acc: 0.039685927
top acc: 0.0367 ::: bot acc: 0.0527
top acc: 0.0274 ::: bot acc: 0.0475
top acc: 0.0429 ::: bot acc: 0.0566
top acc: 0.0554 ::: bot acc: 0.0516
top acc: 0.0369 ::: bot acc: 0.0683
top acc: 0.0553 ::: bot acc: 0.0503
current epoch: 31
train loss is 0.037498
average val loss: 0.036890, accuracy: 0.0369
average test loss: 0.038182, accuracy: 0.0381
case acc: 0.03510584
case acc: 0.03049756
case acc: 0.038854092
case acc: 0.038194012
case acc: 0.045904122
case acc: 0.03998036
top acc: 0.0318 ::: bot acc: 0.0576
top acc: 0.0242 ::: bot acc: 0.0506
top acc: 0.0379 ::: bot acc: 0.0615
top acc: 0.0539 ::: bot acc: 0.0531
top acc: 0.0357 ::: bot acc: 0.0698
top acc: 0.0508 ::: bot acc: 0.0548
current epoch: 32
train loss is 0.037051
average val loss: 0.037874, accuracy: 0.0378
average test loss: 0.039173, accuracy: 0.0391
case acc: 0.036670297
case acc: 0.031700723
case acc: 0.040688735
case acc: 0.038370952
case acc: 0.046820935
case acc: 0.040339086
top acc: 0.0271 ::: bot acc: 0.0625
top acc: 0.0206 ::: bot acc: 0.0542
top acc: 0.0321 ::: bot acc: 0.0674
top acc: 0.0515 ::: bot acc: 0.0554
top acc: 0.0344 ::: bot acc: 0.0718
top acc: 0.0463 ::: bot acc: 0.0592
current epoch: 33
train loss is 0.036943
average val loss: 0.039499, accuracy: 0.0394
average test loss: 0.040796, accuracy: 0.0408
case acc: 0.038895328
case acc: 0.03363604
case acc: 0.04364735
case acc: 0.038973004
case acc: 0.04839028
case acc: 0.04110169
top acc: 0.0228 ::: bot acc: 0.0680
top acc: 0.0170 ::: bot acc: 0.0589
top acc: 0.0262 ::: bot acc: 0.0747
top acc: 0.0477 ::: bot acc: 0.0592
top acc: 0.0325 ::: bot acc: 0.0751
top acc: 0.0413 ::: bot acc: 0.0643
current epoch: 34
train loss is 0.037317
average val loss: 0.042229, accuracy: 0.0422
average test loss: 0.043550, accuracy: 0.0436
case acc: 0.041823868
case acc: 0.036732342
case acc: 0.048552938
case acc: 0.040703084
case acc: 0.051020965
case acc: 0.042593874
top acc: 0.0185 ::: bot acc: 0.0745
top acc: 0.0133 ::: bot acc: 0.0654
top acc: 0.0221 ::: bot acc: 0.0841
top acc: 0.0414 ::: bot acc: 0.0655
top acc: 0.0295 ::: bot acc: 0.0806
top acc: 0.0350 ::: bot acc: 0.0706
current epoch: 35
train loss is 0.038086
average val loss: 0.045018, accuracy: 0.0452
average test loss: 0.046483, accuracy: 0.0466
case acc: 0.044445623
case acc: 0.039925937
case acc: 0.054125015
case acc: 0.042964928
case acc: 0.053726375
case acc: 0.044200484
top acc: 0.0165 ::: bot acc: 0.0795
top acc: 0.0115 ::: bot acc: 0.0710
top acc: 0.0210 ::: bot acc: 0.0930
top acc: 0.0358 ::: bot acc: 0.0718
top acc: 0.0275 ::: bot acc: 0.0856
top acc: 0.0301 ::: bot acc: 0.0755
current epoch: 36
train loss is 0.038951
average val loss: 0.048977, accuracy: 0.0493
average test loss: 0.050841, accuracy: 0.0510
case acc: 0.04805486
case acc: 0.045041904
case acc: 0.061359424
case acc: 0.046891753
case acc: 0.05815503
case acc: 0.04658144
top acc: 0.0159 ::: bot acc: 0.0852
top acc: 0.0127 ::: bot acc: 0.0782
top acc: 0.0218 ::: bot acc: 0.1035
top acc: 0.0306 ::: bot acc: 0.0802
top acc: 0.0265 ::: bot acc: 0.0928
top acc: 0.0252 ::: bot acc: 0.0816
current epoch: 37
train loss is 0.039813
average val loss: 0.054157, accuracy: 0.0546
average test loss: 0.056612, accuracy: 0.0568
case acc: 0.05217656
case acc: 0.051461134
case acc: 0.06991452
case acc: 0.052751046
case acc: 0.06485268
case acc: 0.049699415
top acc: 0.0171 ::: bot acc: 0.0908
top acc: 0.0162 ::: bot acc: 0.0860
top acc: 0.0254 ::: bot acc: 0.1145
top acc: 0.0277 ::: bot acc: 0.0905
top acc: 0.0275 ::: bot acc: 0.1024
top acc: 0.0214 ::: bot acc: 0.0881
current epoch: 38
train loss is 0.040835
average val loss: 0.057420, accuracy: 0.0579
average test loss: 0.060145, accuracy: 0.0603
case acc: 0.05261424
case acc: 0.054671742
case acc: 0.075219825
case acc: 0.057623673
case acc: 0.07077866
case acc: 0.050990622
top acc: 0.0173 ::: bot acc: 0.0914
top acc: 0.0186 ::: bot acc: 0.0896
top acc: 0.0286 ::: bot acc: 0.1209
top acc: 0.0277 ::: bot acc: 0.0978
top acc: 0.0296 ::: bot acc: 0.1103
top acc: 0.0204 ::: bot acc: 0.0906
current epoch: 39
train loss is 0.041442
average val loss: 0.055709, accuracy: 0.0564
average test loss: 0.058262, accuracy: 0.0583
case acc: 0.046915203
case acc: 0.050750695
case acc: 0.0730431
case acc: 0.058093343
case acc: 0.07266531
case acc: 0.048447397
top acc: 0.0158 ::: bot acc: 0.0835
top acc: 0.0157 ::: bot acc: 0.0852
top acc: 0.0272 ::: bot acc: 0.1183
top acc: 0.0277 ::: bot acc: 0.0985
top acc: 0.0302 ::: bot acc: 0.1128
top acc: 0.0226 ::: bot acc: 0.0856
current epoch: 40
train loss is 0.042261
average val loss: 0.048215, accuracy: 0.0493
average test loss: 0.050141, accuracy: 0.0503
case acc: 0.03789803
case acc: 0.039443534
case acc: 0.06150462
case acc: 0.052154362
case acc: 0.06794521
case acc: 0.042936753
top acc: 0.0246 ::: bot acc: 0.0656
top acc: 0.0118 ::: bot acc: 0.0702
top acc: 0.0218 ::: bot acc: 0.1037
top acc: 0.0278 ::: bot acc: 0.0895
top acc: 0.0286 ::: bot acc: 0.1065
top acc: 0.0340 ::: bot acc: 0.0716
current epoch: 41
train loss is 0.046609
average val loss: 0.037411, accuracy: 0.0389
average test loss: 0.038362, accuracy: 0.0386
case acc: 0.034761805
case acc: 0.027638188
case acc: 0.040211223
case acc: 0.03889548
case acc: 0.050391685
case acc: 0.039882142
top acc: 0.0610 ::: bot acc: 0.0284
top acc: 0.0406 ::: bot acc: 0.0342
top acc: 0.0335 ::: bot acc: 0.0659
top acc: 0.0482 ::: bot acc: 0.0588
top acc: 0.0302 ::: bot acc: 0.0794
top acc: 0.0672 ::: bot acc: 0.0384
current epoch: 42
train loss is 0.053208
average val loss: 0.050742, accuracy: 0.0514
average test loss: 0.048552, accuracy: 0.0496
case acc: 0.058990028
case acc: 0.048974372
case acc: 0.043086503
case acc: 0.050211333
case acc: 0.03888951
case acc: 0.057643186
top acc: 0.1032 ::: bot acc: 0.0178
top acc: 0.0852 ::: bot acc: 0.0174
top acc: 0.0814 ::: bot acc: 0.0212
top acc: 0.0915 ::: bot acc: 0.0242
top acc: 0.0645 ::: bot acc: 0.0396
top acc: 0.1080 ::: bot acc: 0.0140
current epoch: 43
train loss is 0.054538
average val loss: 0.045462, accuracy: 0.0461
average test loss: 0.043892, accuracy: 0.0446
case acc: 0.04850887
case acc: 0.041865986
case acc: 0.04058912
case acc: 0.04756378
case acc: 0.039204177
case acc: 0.050024573
top acc: 0.0899 ::: bot acc: 0.0131
top acc: 0.0767 ::: bot acc: 0.0132
top acc: 0.0765 ::: bot acc: 0.0233
top acc: 0.0867 ::: bot acc: 0.0260
top acc: 0.0597 ::: bot acc: 0.0444
top acc: 0.0964 ::: bot acc: 0.0143
current epoch: 44
train loss is 0.048941
average val loss: 0.037440, accuracy: 0.0381
average test loss: 0.037626, accuracy: 0.0377
case acc: 0.03572049
case acc: 0.030290585
case acc: 0.037027553
case acc: 0.04143224
case acc: 0.04043333
case acc: 0.0415082
top acc: 0.0652 ::: bot acc: 0.0240
top acc: 0.0570 ::: bot acc: 0.0178
top acc: 0.0587 ::: bot acc: 0.0406
top acc: 0.0737 ::: bot acc: 0.0335
top acc: 0.0512 ::: bot acc: 0.0529
top acc: 0.0754 ::: bot acc: 0.0300
current epoch: 45
train loss is 0.041080
average val loss: 0.035308, accuracy: 0.0354
average test loss: 0.036271, accuracy: 0.0362
case acc: 0.032856625
case acc: 0.027678095
case acc: 0.037451338
case acc: 0.039257564
case acc: 0.04086649
case acc: 0.039366532
top acc: 0.0472 ::: bot acc: 0.0420
top acc: 0.0432 ::: bot acc: 0.0316
top acc: 0.0451 ::: bot acc: 0.0542
top acc: 0.0669 ::: bot acc: 0.0401
top acc: 0.0489 ::: bot acc: 0.0551
top acc: 0.0615 ::: bot acc: 0.0438
current epoch: 46
train loss is 0.037225
average val loss: 0.036481, accuracy: 0.0363
average test loss: 0.037771, accuracy: 0.0377
case acc: 0.035383195
case acc: 0.0289746
case acc: 0.0411683
case acc: 0.03810418
case acc: 0.042313155
case acc: 0.040014416
top acc: 0.0308 ::: bot acc: 0.0585
top acc: 0.0295 ::: bot acc: 0.0451
top acc: 0.0311 ::: bot acc: 0.0685
top acc: 0.0587 ::: bot acc: 0.0483
top acc: 0.0433 ::: bot acc: 0.0606
top acc: 0.0483 ::: bot acc: 0.0570
current epoch: 47
train loss is 0.036547
average val loss: 0.040919, accuracy: 0.0407
average test loss: 0.042206, accuracy: 0.0421
case acc: 0.04173504
case acc: 0.034027953
case acc: 0.048751645
case acc: 0.03915146
case acc: 0.046480637
case acc: 0.042581543
top acc: 0.0186 ::: bot acc: 0.0742
top acc: 0.0163 ::: bot acc: 0.0597
top acc: 0.0221 ::: bot acc: 0.0844
top acc: 0.0470 ::: bot acc: 0.0600
top acc: 0.0347 ::: bot acc: 0.0712
top acc: 0.0345 ::: bot acc: 0.0708
current epoch: 48
train loss is 0.038195
average val loss: 0.048169, accuracy: 0.0481
average test loss: 0.049962, accuracy: 0.0500
case acc: 0.05092005
case acc: 0.0427603
case acc: 0.060096078
case acc: 0.0444131
case acc: 0.053812847
case acc: 0.047968026
top acc: 0.0166 ::: bot acc: 0.0890
top acc: 0.0118 ::: bot acc: 0.0751
top acc: 0.0214 ::: bot acc: 0.1017
top acc: 0.0336 ::: bot acc: 0.0749
top acc: 0.0274 ::: bot acc: 0.0858
top acc: 0.0228 ::: bot acc: 0.0849
current epoch: 49
train loss is 0.040770
average val loss: 0.055420, accuracy: 0.0555
average test loss: 0.057966, accuracy: 0.0581
case acc: 0.057981905
case acc: 0.0518491
case acc: 0.07080335
case acc: 0.05146643
case acc: 0.06309296
case acc: 0.053239558
top acc: 0.0196 ::: bot acc: 0.0981
top acc: 0.0165 ::: bot acc: 0.0863
top acc: 0.0258 ::: bot acc: 0.1156
top acc: 0.0280 ::: bot acc: 0.0884
top acc: 0.0272 ::: bot acc: 0.0999
top acc: 0.0190 ::: bot acc: 0.0947
current epoch: 50
train loss is 0.042147
average val loss: 0.054545, accuracy: 0.0547
average test loss: 0.057014, accuracy: 0.0571
case acc: 0.053161696
case acc: 0.049444545
case acc: 0.070571996
case acc: 0.05247029
case acc: 0.066002674
case acc: 0.05100824
top acc: 0.0175 ::: bot acc: 0.0920
top acc: 0.0149 ::: bot acc: 0.0836
top acc: 0.0257 ::: bot acc: 0.1153
top acc: 0.0278 ::: bot acc: 0.0899
top acc: 0.0279 ::: bot acc: 0.1039
top acc: 0.0202 ::: bot acc: 0.0907
LME_Co_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5406 5406 5406
1.7082474 -0.6288155 0.16982092 -0.17269358
Validation: 606 606 606
Testing: 750 750 750
pre-processing time: 0.0006015300750732422
the split date is 2012-01-01
net initializing with time: 0.005736589431762695
preparing training and testing date with time: 7.152557373046875e-07
current epoch: 1
train loss is 0.163436
average val loss: 0.204613, accuracy: 0.2047
average test loss: 0.180068, accuracy: 0.1808
case acc: 0.23214968
case acc: 0.06758334
case acc: 0.12611315
case acc: 0.14197591
case acc: 0.046166886
case acc: 0.4708279
top acc: 0.1850 ::: bot acc: 0.2811
top acc: 0.0311 ::: bot acc: 0.1091
top acc: 0.0637 ::: bot acc: 0.1902
top acc: 0.0930 ::: bot acc: 0.1914
top acc: 0.0286 ::: bot acc: 0.0762
top acc: 0.4101 ::: bot acc: 0.5261
current epoch: 2
train loss is 0.168737
average val loss: 0.273163, accuracy: 0.2732
average test loss: 0.247108, accuracy: 0.2471
case acc: 0.29655555
case acc: 0.14534305
case acc: 0.19734375
case acc: 0.2159206
case acc: 0.1090381
case acc: 0.5185608
top acc: 0.2495 ::: bot acc: 0.3451
top acc: 0.1068 ::: bot acc: 0.1879
top acc: 0.1351 ::: bot acc: 0.2612
top acc: 0.1668 ::: bot acc: 0.2655
top acc: 0.0549 ::: bot acc: 0.1571
top acc: 0.4577 ::: bot acc: 0.5739
current epoch: 3
train loss is 0.147318
average val loss: 0.227521, accuracy: 0.2276
average test loss: 0.201707, accuracy: 0.2019
case acc: 0.24326777
case acc: 0.10985762
case acc: 0.15415072
case acc: 0.17573534
case acc: 0.07770343
case acc: 0.45094198
top acc: 0.1963 ::: bot acc: 0.2917
top acc: 0.0709 ::: bot acc: 0.1527
top acc: 0.0921 ::: bot acc: 0.2178
top acc: 0.1267 ::: bot acc: 0.2255
top acc: 0.0290 ::: bot acc: 0.1232
top acc: 0.3902 ::: bot acc: 0.5062
current epoch: 4
train loss is 0.123911
average val loss: 0.169956, accuracy: 0.1701
average test loss: 0.145800, accuracy: 0.1464
case acc: 0.17718148
case acc: 0.06357002
case acc: 0.09838981
case acc: 0.12286678
case acc: 0.046460196
case acc: 0.36982703
top acc: 0.1304 ::: bot acc: 0.2255
top acc: 0.0272 ::: bot acc: 0.1053
top acc: 0.0371 ::: bot acc: 0.1616
top acc: 0.0739 ::: bot acc: 0.1727
top acc: 0.0275 ::: bot acc: 0.0776
top acc: 0.3092 ::: bot acc: 0.4251
current epoch: 5
train loss is 0.115161
average val loss: 0.176732, accuracy: 0.1768
average test loss: 0.151615, accuracy: 0.1521
case acc: 0.1779904
case acc: 0.07994156
case acc: 0.10761622
case acc: 0.13450076
case acc: 0.05765766
case acc: 0.3546569
top acc: 0.1312 ::: bot acc: 0.2263
top acc: 0.0410 ::: bot acc: 0.1229
top acc: 0.0460 ::: bot acc: 0.1709
top acc: 0.0855 ::: bot acc: 0.1844
top acc: 0.0218 ::: bot acc: 0.0971
top acc: 0.2940 ::: bot acc: 0.4100
current epoch: 6
train loss is 0.104520
average val loss: 0.152169, accuracy: 0.1522
average test loss: 0.127843, accuracy: 0.1283
case acc: 0.14677261
case acc: 0.0661132
case acc: 0.08576009
case acc: 0.11388792
case acc: 0.04977822
case acc: 0.30765033
top acc: 0.1001 ::: bot acc: 0.1949
top acc: 0.0289 ::: bot acc: 0.1083
top acc: 0.0259 ::: bot acc: 0.1481
top acc: 0.0653 ::: bot acc: 0.1636
top acc: 0.0242 ::: bot acc: 0.0843
top acc: 0.2470 ::: bot acc: 0.3630
current epoch: 7
train loss is 0.096398
average val loss: 0.142975, accuracy: 0.1429
average test loss: 0.118656, accuracy: 0.1191
case acc: 0.1321014
case acc: 0.06670638
case acc: 0.07952939
case acc: 0.10833205
case acc: 0.051158868
case acc: 0.27681583
top acc: 0.0854 ::: bot acc: 0.1802
top acc: 0.0293 ::: bot acc: 0.1089
top acc: 0.0212 ::: bot acc: 0.1411
top acc: 0.0599 ::: bot acc: 0.1580
top acc: 0.0233 ::: bot acc: 0.0867
top acc: 0.2162 ::: bot acc: 0.3322
current epoch: 8
train loss is 0.086980
average val loss: 0.118198, accuracy: 0.1181
average test loss: 0.095551, accuracy: 0.0961
case acc: 0.101779364
case acc: 0.052494146
case acc: 0.06172671
case acc: 0.08665882
case acc: 0.044367306
case acc: 0.2297586
top acc: 0.0551 ::: bot acc: 0.1499
top acc: 0.0189 ::: bot acc: 0.0929
top acc: 0.0149 ::: bot acc: 0.1177
top acc: 0.0400 ::: bot acc: 0.1356
top acc: 0.0316 ::: bot acc: 0.0726
top acc: 0.1691 ::: bot acc: 0.2851
current epoch: 9
train loss is 0.077605
average val loss: 0.090149, accuracy: 0.0896
average test loss: 0.071782, accuracy: 0.0725
case acc: 0.068921015
case acc: 0.036975395
case acc: 0.049762268
case acc: 0.06278506
case acc: 0.038373064
case acc: 0.17799436
top acc: 0.0231 ::: bot acc: 0.1165
top acc: 0.0160 ::: bot acc: 0.0713
top acc: 0.0351 ::: bot acc: 0.0893
top acc: 0.0262 ::: bot acc: 0.1068
top acc: 0.0504 ::: bot acc: 0.0531
top acc: 0.1174 ::: bot acc: 0.2334
current epoch: 10
train loss is 0.071783
average val loss: 0.066474, accuracy: 0.0656
average test loss: 0.055086, accuracy: 0.0552
case acc: 0.04593392
case acc: 0.029591057
case acc: 0.047099415
case acc: 0.04226778
case acc: 0.038583223
case acc: 0.1278574
top acc: 0.0119 ::: bot acc: 0.0877
top acc: 0.0339 ::: bot acc: 0.0496
top acc: 0.0621 ::: bot acc: 0.0621
top acc: 0.0246 ::: bot acc: 0.0771
top acc: 0.0699 ::: bot acc: 0.0339
top acc: 0.0673 ::: bot acc: 0.1832
current epoch: 11
train loss is 0.070664
average val loss: 0.058858, accuracy: 0.0577
average test loss: 0.049265, accuracy: 0.0493
case acc: 0.043359816
case acc: 0.029544005
case acc: 0.047574658
case acc: 0.037452385
case acc: 0.038724817
case acc: 0.09933669
top acc: 0.0141 ::: bot acc: 0.0828
top acc: 0.0371 ::: bot acc: 0.0466
top acc: 0.0691 ::: bot acc: 0.0551
top acc: 0.0334 ::: bot acc: 0.0653
top acc: 0.0707 ::: bot acc: 0.0333
top acc: 0.0395 ::: bot acc: 0.1544
current epoch: 12
train loss is 0.068904
average val loss: 0.062351, accuracy: 0.0615
average test loss: 0.049207, accuracy: 0.0493
case acc: 0.05188523
case acc: 0.031670574
case acc: 0.046941582
case acc: 0.038696755
case acc: 0.037809845
case acc: 0.088951886
top acc: 0.0124 ::: bot acc: 0.0963
top acc: 0.0244 ::: bot acc: 0.0594
top acc: 0.0593 ::: bot acc: 0.0648
top acc: 0.0292 ::: bot acc: 0.0695
top acc: 0.0558 ::: bot acc: 0.0482
top acc: 0.0299 ::: bot acc: 0.1436
current epoch: 13
train loss is 0.064241
average val loss: 0.069821, accuracy: 0.0693
average test loss: 0.052597, accuracy: 0.0532
case acc: 0.06470971
case acc: 0.039191414
case acc: 0.047407392
case acc: 0.042654697
case acc: 0.041788362
case acc: 0.083191186
top acc: 0.0196 ::: bot acc: 0.1119
top acc: 0.0150 ::: bot acc: 0.0753
top acc: 0.0464 ::: bot acc: 0.0777
top acc: 0.0242 ::: bot acc: 0.0778
top acc: 0.0382 ::: bot acc: 0.0657
top acc: 0.0256 ::: bot acc: 0.1372
current epoch: 14
train loss is 0.059875
average val loss: 0.075910, accuracy: 0.0756
average test loss: 0.056274, accuracy: 0.0570
case acc: 0.073545106
case acc: 0.04865087
case acc: 0.049130615
case acc: 0.047075648
case acc: 0.047631312
case acc: 0.07611065
top acc: 0.0274 ::: bot acc: 0.1213
top acc: 0.0167 ::: bot acc: 0.0885
top acc: 0.0367 ::: bot acc: 0.0875
top acc: 0.0228 ::: bot acc: 0.0850
top acc: 0.0267 ::: bot acc: 0.0799
top acc: 0.0209 ::: bot acc: 0.1289
current epoch: 15
train loss is 0.054574
average val loss: 0.077694, accuracy: 0.0775
average test loss: 0.057316, accuracy: 0.0581
case acc: 0.07460053
case acc: 0.055439837
case acc: 0.050216403
case acc: 0.04945791
case acc: 0.052247595
case acc: 0.06666339
top acc: 0.0284 ::: bot acc: 0.1223
top acc: 0.0208 ::: bot acc: 0.0967
top acc: 0.0324 ::: bot acc: 0.0917
top acc: 0.0228 ::: bot acc: 0.0887
top acc: 0.0231 ::: bot acc: 0.0886
top acc: 0.0171 ::: bot acc: 0.1168
current epoch: 16
train loss is 0.049364
average val loss: 0.077584, accuracy: 0.0775
average test loss: 0.057186, accuracy: 0.0580
case acc: 0.071443625
case acc: 0.060196884
case acc: 0.050679483
case acc: 0.051049795
case acc: 0.055595685
case acc: 0.059139583
top acc: 0.0255 ::: bot acc: 0.1190
top acc: 0.0242 ::: bot acc: 0.1021
top acc: 0.0306 ::: bot acc: 0.0933
top acc: 0.0230 ::: bot acc: 0.0910
top acc: 0.0222 ::: bot acc: 0.0941
top acc: 0.0185 ::: bot acc: 0.1049
current epoch: 17
train loss is 0.044297
average val loss: 0.075676, accuracy: 0.0758
average test loss: 0.055814, accuracy: 0.0566
case acc: 0.06538108
case acc: 0.06192538
case acc: 0.0504558
case acc: 0.051330436
case acc: 0.05655749
case acc: 0.054067224
top acc: 0.0202 ::: bot acc: 0.1126
top acc: 0.0255 ::: bot acc: 0.1041
top acc: 0.0313 ::: bot acc: 0.0926
top acc: 0.0230 ::: bot acc: 0.0914
top acc: 0.0221 ::: bot acc: 0.0956
top acc: 0.0232 ::: bot acc: 0.0950
current epoch: 18
train loss is 0.041807
average val loss: 0.076311, accuracy: 0.0764
average test loss: 0.056325, accuracy: 0.0570
case acc: 0.061662335
case acc: 0.06459675
case acc: 0.05097836
case acc: 0.053676043
case acc: 0.057891272
case acc: 0.053263582
top acc: 0.0172 ::: bot acc: 0.1085
top acc: 0.0274 ::: bot acc: 0.1071
top acc: 0.0296 ::: bot acc: 0.0943
top acc: 0.0234 ::: bot acc: 0.0947
top acc: 0.0220 ::: bot acc: 0.0976
top acc: 0.0244 ::: bot acc: 0.0932
current epoch: 19
train loss is 0.040216
average val loss: 0.074868, accuracy: 0.0749
average test loss: 0.055192, accuracy: 0.0558
case acc: 0.05603339
case acc: 0.06379378
case acc: 0.050710358
case acc: 0.05439687
case acc: 0.05646111
case acc: 0.05328975
top acc: 0.0138 ::: bot acc: 0.1018
top acc: 0.0268 ::: bot acc: 0.1061
top acc: 0.0306 ::: bot acc: 0.0933
top acc: 0.0237 ::: bot acc: 0.0957
top acc: 0.0221 ::: bot acc: 0.0954
top acc: 0.0244 ::: bot acc: 0.0933
current epoch: 20
train loss is 0.039686
average val loss: 0.071609, accuracy: 0.0716
average test loss: 0.052766, accuracy: 0.0533
case acc: 0.04956207
case acc: 0.06031827
case acc: 0.049759842
case acc: 0.053447526
case acc: 0.05325636
case acc: 0.053279553
top acc: 0.0118 ::: bot acc: 0.0931
top acc: 0.0243 ::: bot acc: 0.1022
top acc: 0.0340 ::: bot acc: 0.0899
top acc: 0.0235 ::: bot acc: 0.0944
top acc: 0.0229 ::: bot acc: 0.0901
top acc: 0.0244 ::: bot acc: 0.0933
current epoch: 21
train loss is 0.038811
average val loss: 0.066806, accuracy: 0.0668
average test loss: 0.049416, accuracy: 0.0498
case acc: 0.043573216
case acc: 0.054322015
case acc: 0.04842323
case acc: 0.050643288
case acc: 0.048809364
case acc: 0.053074125
top acc: 0.0135 ::: bot acc: 0.0834
top acc: 0.0199 ::: bot acc: 0.0954
top acc: 0.0398 ::: bot acc: 0.0843
top acc: 0.0230 ::: bot acc: 0.0904
top acc: 0.0255 ::: bot acc: 0.0822
top acc: 0.0248 ::: bot acc: 0.0928
current epoch: 22
train loss is 0.038716
average val loss: 0.062069, accuracy: 0.0620
average test loss: 0.046473, accuracy: 0.0468
case acc: 0.03977286
case acc: 0.0481309
case acc: 0.047474932
case acc: 0.04732807
case acc: 0.04501491
case acc: 0.052900083
top acc: 0.0193 ::: bot acc: 0.0749
top acc: 0.0164 ::: bot acc: 0.0880
top acc: 0.0459 ::: bot acc: 0.0783
top acc: 0.0228 ::: bot acc: 0.0856
top acc: 0.0308 ::: bot acc: 0.0739
top acc: 0.0252 ::: bot acc: 0.0924
current epoch: 23
train loss is 0.038638
average val loss: 0.054836, accuracy: 0.0547
average test loss: 0.042605, accuracy: 0.0427
case acc: 0.03652862
case acc: 0.03970182
case acc: 0.04694824
case acc: 0.041505966
case acc: 0.04048128
case acc: 0.05082317
top acc: 0.0297 ::: bot acc: 0.0644
top acc: 0.0146 ::: bot acc: 0.0763
top acc: 0.0559 ::: bot acc: 0.0683
top acc: 0.0251 ::: bot acc: 0.0758
top acc: 0.0414 ::: bot acc: 0.0620
top acc: 0.0292 ::: bot acc: 0.0873
current epoch: 24
train loss is 0.038271
average val loss: 0.048959, accuracy: 0.0486
average test loss: 0.040299, accuracy: 0.0401
case acc: 0.035047036
case acc: 0.033767972
case acc: 0.047235284
case acc: 0.037481524
case acc: 0.03803016
case acc: 0.04915258
top acc: 0.0372 ::: bot acc: 0.0569
top acc: 0.0194 ::: bot acc: 0.0652
top acc: 0.0651 ::: bot acc: 0.0590
top acc: 0.0333 ::: bot acc: 0.0656
top acc: 0.0524 ::: bot acc: 0.0510
top acc: 0.0337 ::: bot acc: 0.0825
current epoch: 25
train loss is 0.038291
average val loss: 0.044073, accuracy: 0.0436
average test loss: 0.039257, accuracy: 0.0391
case acc: 0.034690905
case acc: 0.030188434
case acc: 0.048267897
case acc: 0.036059834
case acc: 0.037605114
case acc: 0.047592957
top acc: 0.0424 ::: bot acc: 0.0517
top acc: 0.0302 ::: bot acc: 0.0537
top acc: 0.0743 ::: bot acc: 0.0499
top acc: 0.0447 ::: bot acc: 0.0542
top acc: 0.0633 ::: bot acc: 0.0398
top acc: 0.0386 ::: bot acc: 0.0777
current epoch: 26
train loss is 0.037829
average val loss: 0.040096, accuracy: 0.0394
average test loss: 0.039549, accuracy: 0.0395
case acc: 0.034759853
case acc: 0.030140143
case acc: 0.049779713
case acc: 0.03723691
case acc: 0.039236862
case acc: 0.045965977
top acc: 0.0472 ::: bot acc: 0.0469
top acc: 0.0428 ::: bot acc: 0.0412
top acc: 0.0843 ::: bot acc: 0.0398
top acc: 0.0580 ::: bot acc: 0.0409
top acc: 0.0752 ::: bot acc: 0.0280
top acc: 0.0446 ::: bot acc: 0.0717
current epoch: 27
train loss is 0.037593
average val loss: 0.037284, accuracy: 0.0363
average test loss: 0.042184, accuracy: 0.0426
case acc: 0.03521608
case acc: 0.03580761
case acc: 0.0537348
case acc: 0.042370386
case acc: 0.044614755
case acc: 0.043844257
top acc: 0.0531 ::: bot acc: 0.0411
top acc: 0.0579 ::: bot acc: 0.0287
top acc: 0.0969 ::: bot acc: 0.0276
top acc: 0.0749 ::: bot acc: 0.0255
top acc: 0.0895 ::: bot acc: 0.0160
top acc: 0.0545 ::: bot acc: 0.0617
current epoch: 28
train loss is 0.039195
average val loss: 0.037400, accuracy: 0.0365
average test loss: 0.047893, accuracy: 0.0487
case acc: 0.03612611
case acc: 0.04532458
case acc: 0.060953766
case acc: 0.05315858
case acc: 0.053998742
case acc: 0.042847
top acc: 0.0587 ::: bot acc: 0.0355
top acc: 0.0740 ::: bot acc: 0.0247
top acc: 0.1107 ::: bot acc: 0.0214
top acc: 0.0943 ::: bot acc: 0.0188
top acc: 0.1046 ::: bot acc: 0.0135
top acc: 0.0676 ::: bot acc: 0.0486
current epoch: 29
train loss is 0.043523
average val loss: 0.039586, accuracy: 0.0390
average test loss: 0.053120, accuracy: 0.0542
case acc: 0.03597177
case acc: 0.05255672
case acc: 0.06653126
case acc: 0.064827845
case acc: 0.061880175
case acc: 0.04332725
top acc: 0.0578 ::: bot acc: 0.0363
top acc: 0.0847 ::: bot acc: 0.0248
top acc: 0.1191 ::: bot acc: 0.0209
top acc: 0.1097 ::: bot acc: 0.0227
top acc: 0.1148 ::: bot acc: 0.0168
top acc: 0.0773 ::: bot acc: 0.0389
current epoch: 30
train loss is 0.052080
average val loss: 0.038681, accuracy: 0.0384
average test loss: 0.047708, accuracy: 0.0492
case acc: 0.035467636
case acc: 0.044919968
case acc: 0.05833102
case acc: 0.060735337
case acc: 0.052804127
case acc: 0.04281559
top acc: 0.0345 ::: bot acc: 0.0597
top acc: 0.0733 ::: bot acc: 0.0249
top acc: 0.1059 ::: bot acc: 0.0230
top acc: 0.1046 ::: bot acc: 0.0206
top acc: 0.1030 ::: bot acc: 0.0133
top acc: 0.0689 ::: bot acc: 0.0472
current epoch: 31
train loss is 0.063124
average val loss: 0.064960, accuracy: 0.0647
average test loss: 0.049480, accuracy: 0.0509
case acc: 0.08384175
case acc: 0.03912899
case acc: 0.048024513
case acc: 0.036058437
case acc: 0.041519355
case acc: 0.056967948
top acc: 0.0375 ::: bot acc: 0.1318
top acc: 0.0147 ::: bot acc: 0.0755
top acc: 0.0412 ::: bot acc: 0.0828
top acc: 0.0466 ::: bot acc: 0.0522
top acc: 0.0380 ::: bot acc: 0.0650
top acc: 0.0201 ::: bot acc: 0.1010
current epoch: 32
train loss is 0.071113
average val loss: 0.109428, accuracy: 0.1096
average test loss: 0.084764, accuracy: 0.0851
case acc: 0.12911944
case acc: 0.088539325
case acc: 0.07406781
case acc: 0.06141784
case acc: 0.07664326
case acc: 0.08090468
top acc: 0.0828 ::: bot acc: 0.1772
top acc: 0.0486 ::: bot acc: 0.1326
top acc: 0.0182 ::: bot acc: 0.1345
top acc: 0.0254 ::: bot acc: 0.1053
top acc: 0.0288 ::: bot acc: 0.1218
top acc: 0.0239 ::: bot acc: 0.1347
current epoch: 33
train loss is 0.057355
average val loss: 0.095881, accuracy: 0.0963
average test loss: 0.072553, accuracy: 0.0732
case acc: 0.1031532
case acc: 0.08243123
case acc: 0.064662345
case acc: 0.057074286
case acc: 0.0716887
case acc: 0.059967395
top acc: 0.0568 ::: bot acc: 0.1512
top acc: 0.0428 ::: bot acc: 0.1262
top acc: 0.0152 ::: bot acc: 0.1220
top acc: 0.0242 ::: bot acc: 0.0995
top acc: 0.0258 ::: bot acc: 0.1159
top acc: 0.0180 ::: bot acc: 0.1063
current epoch: 34
train loss is 0.052498
average val loss: 0.087468, accuracy: 0.0879
average test loss: 0.065292, accuracy: 0.0658
case acc: 0.0806372
case acc: 0.07760179
case acc: 0.0589276
case acc: 0.056814175
case acc: 0.06718416
case acc: 0.053889226
top acc: 0.0344 ::: bot acc: 0.1286
top acc: 0.0384 ::: bot acc: 0.1212
top acc: 0.0155 ::: bot acc: 0.1133
top acc: 0.0241 ::: bot acc: 0.0992
top acc: 0.0234 ::: bot acc: 0.1104
top acc: 0.0235 ::: bot acc: 0.0946
current epoch: 35
train loss is 0.050084
average val loss: 0.068806, accuracy: 0.0692
average test loss: 0.050871, accuracy: 0.0513
case acc: 0.05019549
case acc: 0.058998886
case acc: 0.050150458
case acc: 0.047464125
case acc: 0.052434787
case acc: 0.048361458
top acc: 0.0120 ::: bot acc: 0.0942
top acc: 0.0233 ::: bot acc: 0.1009
top acc: 0.0324 ::: bot acc: 0.0916
top acc: 0.0226 ::: bot acc: 0.0860
top acc: 0.0230 ::: bot acc: 0.0886
top acc: 0.0358 ::: bot acc: 0.0803
current epoch: 36
train loss is 0.043770
average val loss: 0.050975, accuracy: 0.0513
average test loss: 0.041018, accuracy: 0.0408
case acc: 0.035457723
case acc: 0.03906475
case acc: 0.046930537
case acc: 0.037993886
case acc: 0.040070105
case acc: 0.045135047
top acc: 0.0342 ::: bot acc: 0.0600
top acc: 0.0149 ::: bot acc: 0.0753
top acc: 0.0574 ::: bot acc: 0.0666
top acc: 0.0312 ::: bot acc: 0.0677
top acc: 0.0416 ::: bot acc: 0.0612
top acc: 0.0476 ::: bot acc: 0.0685
current epoch: 37
train loss is 0.041203
average val loss: 0.039506, accuracy: 0.0393
average test loss: 0.039851, accuracy: 0.0391
case acc: 0.03696981
case acc: 0.029566461
case acc: 0.049459938
case acc: 0.036724765
case acc: 0.038367577
case acc: 0.043349966
top acc: 0.0618 ::: bot acc: 0.0324
top acc: 0.0366 ::: bot acc: 0.0476
top acc: 0.0826 ::: bot acc: 0.0414
top acc: 0.0539 ::: bot acc: 0.0450
top acc: 0.0709 ::: bot acc: 0.0319
top acc: 0.0582 ::: bot acc: 0.0579
current epoch: 38
train loss is 0.040229
average val loss: 0.036796, accuracy: 0.0359
average test loss: 0.045120, accuracy: 0.0451
case acc: 0.043238763
case acc: 0.03732418
case acc: 0.05630983
case acc: 0.04290611
case acc: 0.047898117
case acc: 0.04294703
top acc: 0.0769 ::: bot acc: 0.0210
top acc: 0.0606 ::: bot acc: 0.0278
top acc: 0.1023 ::: bot acc: 0.0242
top acc: 0.0757 ::: bot acc: 0.0251
top acc: 0.0956 ::: bot acc: 0.0132
top acc: 0.0647 ::: bot acc: 0.0514
current epoch: 39
train loss is 0.040298
average val loss: 0.038118, accuracy: 0.0370
average test loss: 0.050600, accuracy: 0.0513
case acc: 0.044335894
case acc: 0.046826385
case acc: 0.062808
case acc: 0.052214347
case acc: 0.058999535
case acc: 0.042859614
top acc: 0.0790 ::: bot acc: 0.0201
top acc: 0.0762 ::: bot acc: 0.0248
top acc: 0.1136 ::: bot acc: 0.0209
top acc: 0.0927 ::: bot acc: 0.0188
top acc: 0.1110 ::: bot acc: 0.0155
top acc: 0.0683 ::: bot acc: 0.0478
current epoch: 40
train loss is 0.046018
average val loss: 0.037207, accuracy: 0.0361
average test loss: 0.047001, accuracy: 0.0481
case acc: 0.03657763
case acc: 0.043769818
case acc: 0.058289908
case acc: 0.052280407
case acc: 0.054376133
case acc: 0.043037526
top acc: 0.0603 ::: bot acc: 0.0339
top acc: 0.0713 ::: bot acc: 0.0254
top acc: 0.1059 ::: bot acc: 0.0229
top acc: 0.0928 ::: bot acc: 0.0188
top acc: 0.1050 ::: bot acc: 0.0137
top acc: 0.0626 ::: bot acc: 0.0535
current epoch: 41
train loss is 0.049769
average val loss: 0.044246, accuracy: 0.0436
average test loss: 0.039619, accuracy: 0.0405
case acc: 0.041379742
case acc: 0.029560383
case acc: 0.047813453
case acc: 0.03882258
case acc: 0.038062543
case acc: 0.047361407
top acc: 0.0163 ::: bot acc: 0.0790
top acc: 0.0366 ::: bot acc: 0.0475
top acc: 0.0705 ::: bot acc: 0.0535
top acc: 0.0653 ::: bot acc: 0.0335
top acc: 0.0688 ::: bot acc: 0.0340
top acc: 0.0388 ::: bot acc: 0.0773
current epoch: 42
train loss is 0.050090
average val loss: 0.071071, accuracy: 0.0711
average test loss: 0.052303, accuracy: 0.0531
case acc: 0.07308754
case acc: 0.050675765
case acc: 0.05094513
case acc: 0.0395081
case acc: 0.047092587
case acc: 0.05713406
top acc: 0.0272 ::: bot acc: 0.1209
top acc: 0.0177 ::: bot acc: 0.0913
top acc: 0.0296 ::: bot acc: 0.0944
top acc: 0.0274 ::: bot acc: 0.0719
top acc: 0.0269 ::: bot acc: 0.0788
top acc: 0.0198 ::: bot acc: 0.1014
current epoch: 43
train loss is 0.047037
average val loss: 0.078799, accuracy: 0.0791
average test loss: 0.058083, accuracy: 0.0588
case acc: 0.074833065
case acc: 0.06465481
case acc: 0.054839794
case acc: 0.04748323
case acc: 0.056762654
case acc: 0.054095224
top acc: 0.0289 ::: bot acc: 0.1227
top acc: 0.0275 ::: bot acc: 0.1073
top acc: 0.0201 ::: bot acc: 0.1049
top acc: 0.0226 ::: bot acc: 0.0861
top acc: 0.0224 ::: bot acc: 0.0953
top acc: 0.0232 ::: bot acc: 0.0952
current epoch: 44
train loss is 0.045493
average val loss: 0.072978, accuracy: 0.0735
average test loss: 0.053871, accuracy: 0.0543
case acc: 0.05855135
case acc: 0.06295294
case acc: 0.05241308
case acc: 0.04775771
case acc: 0.05537794
case acc: 0.048854604
top acc: 0.0153 ::: bot acc: 0.1051
top acc: 0.0262 ::: bot acc: 0.1054
top acc: 0.0251 ::: bot acc: 0.0989
top acc: 0.0225 ::: bot acc: 0.0865
top acc: 0.0225 ::: bot acc: 0.0932
top acc: 0.0342 ::: bot acc: 0.0819
current epoch: 45
train loss is 0.045847
average val loss: 0.051904, accuracy: 0.0526
average test loss: 0.041525, accuracy: 0.0412
case acc: 0.036881905
case acc: 0.041240588
case acc: 0.04691896
case acc: 0.0373739
case acc: 0.041475352
case acc: 0.04354761
top acc: 0.0280 ::: bot acc: 0.0662
top acc: 0.0146 ::: bot acc: 0.0788
top acc: 0.0531 ::: bot acc: 0.0708
top acc: 0.0331 ::: bot acc: 0.0656
top acc: 0.0375 ::: bot acc: 0.0652
top acc: 0.0560 ::: bot acc: 0.0601
current epoch: 46
train loss is 0.042172
average val loss: 0.038590, accuracy: 0.0387
average test loss: 0.040192, accuracy: 0.0391
case acc: 0.037134953
case acc: 0.029631268
case acc: 0.0494212
case acc: 0.037269313
case acc: 0.038249828
case acc: 0.042864516
top acc: 0.0624 ::: bot acc: 0.0317
top acc: 0.0365 ::: bot acc: 0.0478
top acc: 0.0824 ::: bot acc: 0.0415
top acc: 0.0583 ::: bot acc: 0.0405
top acc: 0.0703 ::: bot acc: 0.0324
top acc: 0.0707 ::: bot acc: 0.0454
current epoch: 47
train loss is 0.040347
average val loss: 0.036437, accuracy: 0.0359
average test loss: 0.045693, accuracy: 0.0454
case acc: 0.044059943
case acc: 0.037131738
case acc: 0.056033418
case acc: 0.0444052
case acc: 0.04771995
case acc: 0.043021176
top acc: 0.0784 ::: bot acc: 0.0204
top acc: 0.0602 ::: bot acc: 0.0280
top acc: 0.1017 ::: bot acc: 0.0246
top acc: 0.0788 ::: bot acc: 0.0234
top acc: 0.0954 ::: bot acc: 0.0131
top acc: 0.0739 ::: bot acc: 0.0422
current epoch: 48
train loss is 0.038995
average val loss: 0.037071, accuracy: 0.0361
average test loss: 0.048203, accuracy: 0.0488
case acc: 0.042448256
case acc: 0.0432354
case acc: 0.058998007
case acc: 0.05017523
case acc: 0.055131365
case acc: 0.042852994
top acc: 0.0753 ::: bot acc: 0.0218
top acc: 0.0705 ::: bot acc: 0.0256
top acc: 0.1071 ::: bot acc: 0.0225
top acc: 0.0894 ::: bot acc: 0.0193
top acc: 0.1060 ::: bot acc: 0.0139
top acc: 0.0687 ::: bot acc: 0.0475
current epoch: 49
train loss is 0.042499
average val loss: 0.036829, accuracy: 0.0359
average test loss: 0.043091, accuracy: 0.0440
case acc: 0.035268914
case acc: 0.037458766
case acc: 0.05282048
case acc: 0.04713624
case acc: 0.047766708
case acc: 0.043428
top acc: 0.0527 ::: bot acc: 0.0414
top acc: 0.0608 ::: bot acc: 0.0279
top acc: 0.0944 ::: bot acc: 0.0295
top acc: 0.0840 ::: bot acc: 0.0210
top acc: 0.0955 ::: bot acc: 0.0131
top acc: 0.0574 ::: bot acc: 0.0587
current epoch: 50
train loss is 0.043181
average val loss: 0.045148, accuracy: 0.0446
average test loss: 0.039386, accuracy: 0.0402
case acc: 0.041007068
case acc: 0.030025234
case acc: 0.04718256
case acc: 0.037946485
case acc: 0.037649043
case acc: 0.047152344
top acc: 0.0168 ::: bot acc: 0.0782
top acc: 0.0321 ::: bot acc: 0.0523
top acc: 0.0654 ::: bot acc: 0.0584
top acc: 0.0615 ::: bot acc: 0.0374
top acc: 0.0653 ::: bot acc: 0.0375
top acc: 0.0397 ::: bot acc: 0.0765
LME_Co_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5394 5394 5394
1.7082474 -0.6288155 0.16982092 -0.17269358
Validation: 600 600 600
Testing: 768 768 768
pre-processing time: 0.0013239383697509766
the split date is 2012-07-01
net initializing with time: 0.007246255874633789
preparing training and testing date with time: 7.152557373046875e-07
current epoch: 1
train loss is 0.163238
average val loss: 0.070599, accuracy: 0.0692
average test loss: 0.070121, accuracy: 0.0710
case acc: 0.07556543
case acc: 0.12284644
case acc: 0.087197825
case acc: 0.03945163
case acc: 0.05891297
case acc: 0.042089358
top acc: 0.0327 ::: bot acc: 0.1151
top acc: 0.0849 ::: bot acc: 0.1581
top acc: 0.0378 ::: bot acc: 0.1363
top acc: 0.0144 ::: bot acc: 0.0650
top acc: 0.0164 ::: bot acc: 0.1076
top acc: 0.0588 ::: bot acc: 0.0544
current epoch: 2
train loss is 0.089910
average val loss: 0.145404, accuracy: 0.1454
average test loss: 0.145014, accuracy: 0.1453
case acc: 0.15331106
case acc: 0.21002576
case acc: 0.17306942
case acc: 0.11898792
case acc: 0.13898279
case acc: 0.07714133
top acc: 0.0971 ::: bot acc: 0.2001
top acc: 0.1723 ::: bot acc: 0.2453
top acc: 0.1185 ::: bot acc: 0.2260
top acc: 0.0886 ::: bot acc: 0.1467
top acc: 0.0897 ::: bot acc: 0.1909
top acc: 0.0343 ::: bot acc: 0.1321
current epoch: 3
train loss is 0.121592
average val loss: 0.136757, accuracy: 0.1368
average test loss: 0.136258, accuracy: 0.1365
case acc: 0.14158708
case acc: 0.19847947
case acc: 0.16659828
case acc: 0.11134909
case acc: 0.13370526
case acc: 0.06733483
top acc: 0.0853 ::: bot acc: 0.1882
top acc: 0.1613 ::: bot acc: 0.2336
top acc: 0.1122 ::: bot acc: 0.2200
top acc: 0.0805 ::: bot acc: 0.1392
top acc: 0.0845 ::: bot acc: 0.1855
top acc: 0.0274 ::: bot acc: 0.1206
current epoch: 4
train loss is 0.140731
average val loss: 0.053826, accuracy: 0.0532
average test loss: 0.052530, accuracy: 0.0539
case acc: 0.041809916
case acc: 0.032379188
case acc: 0.039304186
case acc: 0.055232625
case acc: 0.046870288
case acc: 0.107834175
top acc: 0.0887 ::: bot acc: 0.0149
top acc: 0.0188 ::: bot acc: 0.0560
top acc: 0.0602 ::: bot acc: 0.0472
top acc: 0.0865 ::: bot acc: 0.0272
top acc: 0.0813 ::: bot acc: 0.0251
top acc: 0.1563 ::: bot acc: 0.0557
current epoch: 5
train loss is 0.070225
average val loss: 0.052454, accuracy: 0.0504
average test loss: 0.051289, accuracy: 0.0530
case acc: 0.04940754
case acc: 0.08102512
case acc: 0.063822314
case acc: 0.025875695
case acc: 0.04587059
case acc: 0.051806934
top acc: 0.0278 ::: bot acc: 0.0790
top acc: 0.0438 ::: bot acc: 0.1164
top acc: 0.0258 ::: bot acc: 0.1091
top acc: 0.0170 ::: bot acc: 0.0432
top acc: 0.0172 ::: bot acc: 0.0876
top acc: 0.0838 ::: bot acc: 0.0338
current epoch: 6
train loss is 0.067825
average val loss: 0.081198, accuracy: 0.0810
average test loss: 0.080975, accuracy: 0.0814
case acc: 0.08124707
case acc: 0.11956024
case acc: 0.098269075
case acc: 0.06380877
case acc: 0.08421144
case acc: 0.041426085
top acc: 0.0361 ::: bot acc: 0.1224
top acc: 0.0823 ::: bot acc: 0.1551
top acc: 0.0458 ::: bot acc: 0.1505
top acc: 0.0322 ::: bot acc: 0.0923
top acc: 0.0371 ::: bot acc: 0.1349
top acc: 0.0322 ::: bot acc: 0.0799
current epoch: 7
train loss is 0.081241
average val loss: 0.063979, accuracy: 0.0630
average test loss: 0.063383, accuracy: 0.0643
case acc: 0.06201242
case acc: 0.09091221
case acc: 0.07653636
case acc: 0.04837545
case acc: 0.06801628
case acc: 0.039784685
top acc: 0.0267 ::: bot acc: 0.0984
top acc: 0.0536 ::: bot acc: 0.1266
top acc: 0.0307 ::: bot acc: 0.1254
top acc: 0.0187 ::: bot acc: 0.0760
top acc: 0.0228 ::: bot acc: 0.1178
top acc: 0.0477 ::: bot acc: 0.0648
current epoch: 8
train loss is 0.075734
average val loss: 0.044855, accuracy: 0.0432
average test loss: 0.042824, accuracy: 0.0440
case acc: 0.041747622
case acc: 0.051274832
case acc: 0.049011473
case acc: 0.027640246
case acc: 0.046133384
case acc: 0.048266493
top acc: 0.0406 ::: bot acc: 0.0614
top acc: 0.0183 ::: bot acc: 0.0849
top acc: 0.0250 ::: bot acc: 0.0871
top acc: 0.0147 ::: bot acc: 0.0472
top acc: 0.0169 ::: bot acc: 0.0881
top acc: 0.0756 ::: bot acc: 0.0400
current epoch: 9
train loss is 0.065441
average val loss: 0.041148, accuracy: 0.0396
average test loss: 0.038500, accuracy: 0.0391
case acc: 0.03831184
case acc: 0.03736783
case acc: 0.041624673
case acc: 0.024850767
case acc: 0.041625097
case acc: 0.05087903
top acc: 0.0552 ::: bot acc: 0.0468
top acc: 0.0158 ::: bot acc: 0.0655
top acc: 0.0361 ::: bot acc: 0.0705
top acc: 0.0222 ::: bot acc: 0.0393
top acc: 0.0221 ::: bot acc: 0.0788
top acc: 0.0816 ::: bot acc: 0.0359
current epoch: 10
train loss is 0.060235
average val loss: 0.039854, accuracy: 0.0381
average test loss: 0.036913, accuracy: 0.0365
case acc: 0.03751172
case acc: 0.028231997
case acc: 0.03893047
case acc: 0.023321578
case acc: 0.03775858
case acc: 0.053514324
top acc: 0.0688 ::: bot acc: 0.0330
top acc: 0.0263 ::: bot acc: 0.0468
top acc: 0.0520 ::: bot acc: 0.0544
top acc: 0.0309 ::: bot acc: 0.0309
top acc: 0.0319 ::: bot acc: 0.0681
top acc: 0.0871 ::: bot acc: 0.0326
current epoch: 11
train loss is 0.056851
average val loss: 0.040500, accuracy: 0.0386
average test loss: 0.037821, accuracy: 0.0366
case acc: 0.039007533
case acc: 0.026597712
case acc: 0.039517008
case acc: 0.023163883
case acc: 0.03611674
case acc: 0.05519045
top acc: 0.0795 ::: bot acc: 0.0221
top acc: 0.0421 ::: bot acc: 0.0310
top acc: 0.0656 ::: bot acc: 0.0408
top acc: 0.0381 ::: bot acc: 0.0239
top acc: 0.0417 ::: bot acc: 0.0582
top acc: 0.0903 ::: bot acc: 0.0311
current epoch: 12
train loss is 0.055230
average val loss: 0.044716, accuracy: 0.0430
average test loss: 0.042827, accuracy: 0.0412
case acc: 0.04523363
case acc: 0.03339037
case acc: 0.04538818
case acc: 0.025869938
case acc: 0.03734586
case acc: 0.060025163
top acc: 0.0945 ::: bot acc: 0.0114
top acc: 0.0621 ::: bot acc: 0.0149
top acc: 0.0836 ::: bot acc: 0.0248
top acc: 0.0509 ::: bot acc: 0.0122
top acc: 0.0578 ::: bot acc: 0.0421
top acc: 0.0984 ::: bot acc: 0.0293
current epoch: 13
train loss is 0.055072
average val loss: 0.050690, accuracy: 0.0492
average test loss: 0.049712, accuracy: 0.0485
case acc: 0.05316115
case acc: 0.044096068
case acc: 0.054625154
case acc: 0.032667775
case acc: 0.04273926
case acc: 0.06373505
top acc: 0.1060 ::: bot acc: 0.0117
top acc: 0.0788 ::: bot acc: 0.0132
top acc: 0.0991 ::: bot acc: 0.0212
top acc: 0.0621 ::: bot acc: 0.0102
top acc: 0.0730 ::: bot acc: 0.0285
top acc: 0.1040 ::: bot acc: 0.0291
current epoch: 14
train loss is 0.055173
average val loss: 0.060298, accuracy: 0.0589
average test loss: 0.060409, accuracy: 0.0599
case acc: 0.06467711
case acc: 0.059448503
case acc: 0.068046406
case acc: 0.044580244
case acc: 0.05346557
case acc: 0.0693367
top acc: 0.1191 ::: bot acc: 0.0202
top acc: 0.0967 ::: bot acc: 0.0235
top acc: 0.1166 ::: bot acc: 0.0262
top acc: 0.0763 ::: bot acc: 0.0173
top acc: 0.0923 ::: bot acc: 0.0217
top acc: 0.1117 ::: bot acc: 0.0302
current epoch: 15
train loss is 0.056135
average val loss: 0.064155, accuracy: 0.0631
average test loss: 0.064642, accuracy: 0.0644
case acc: 0.06653398
case acc: 0.065607004
case acc: 0.074291706
case acc: 0.05006229
case acc: 0.061542578
case acc: 0.068313465
top acc: 0.1211 ::: bot acc: 0.0216
top acc: 0.1029 ::: bot acc: 0.0295
top acc: 0.1239 ::: bot acc: 0.0303
top acc: 0.0822 ::: bot acc: 0.0220
top acc: 0.1043 ::: bot acc: 0.0219
top acc: 0.1102 ::: bot acc: 0.0301
current epoch: 16
train loss is 0.056167
average val loss: 0.063863, accuracy: 0.0634
average test loss: 0.064341, accuracy: 0.0641
case acc: 0.062283788
case acc: 0.0640126
case acc: 0.07485598
case acc: 0.051692747
case acc: 0.06744517
case acc: 0.06412242
top acc: 0.1165 ::: bot acc: 0.0181
top acc: 0.1013 ::: bot acc: 0.0279
top acc: 0.1245 ::: bot acc: 0.0307
top acc: 0.0840 ::: bot acc: 0.0234
top acc: 0.1122 ::: bot acc: 0.0236
top acc: 0.1044 ::: bot acc: 0.0293
current epoch: 17
train loss is 0.056338
average val loss: 0.060415, accuracy: 0.0606
average test loss: 0.060591, accuracy: 0.0601
case acc: 0.05415843
case acc: 0.056053594
case acc: 0.07030295
case acc: 0.05066453
case acc: 0.07071844
case acc: 0.058997337
top acc: 0.1074 ::: bot acc: 0.0121
top acc: 0.0931 ::: bot acc: 0.0205
top acc: 0.1192 ::: bot acc: 0.0276
top acc: 0.0829 ::: bot acc: 0.0225
top acc: 0.1164 ::: bot acc: 0.0250
top acc: 0.0966 ::: bot acc: 0.0295
current epoch: 18
train loss is 0.056197
average val loss: 0.047047, accuracy: 0.0480
average test loss: 0.045685, accuracy: 0.0454
case acc: 0.03986297
case acc: 0.035601463
case acc: 0.05222889
case acc: 0.0358548
case acc: 0.060772784
case acc: 0.048147652
top acc: 0.0819 ::: bot acc: 0.0204
top acc: 0.0659 ::: bot acc: 0.0137
top acc: 0.0953 ::: bot acc: 0.0215
top acc: 0.0663 ::: bot acc: 0.0113
top acc: 0.1032 ::: bot acc: 0.0217
top acc: 0.0753 ::: bot acc: 0.0401
current epoch: 19
train loss is 0.050324
average val loss: 0.039033, accuracy: 0.0403
average test loss: 0.036068, accuracy: 0.0362
case acc: 0.038817666
case acc: 0.026914572
case acc: 0.03952666
case acc: 0.023937665
case acc: 0.047975563
case acc: 0.04025171
top acc: 0.0526 ::: bot acc: 0.0499
top acc: 0.0331 ::: bot acc: 0.0401
top acc: 0.0649 ::: bot acc: 0.0416
top acc: 0.0452 ::: bot acc: 0.0171
top acc: 0.0830 ::: bot acc: 0.0240
top acc: 0.0517 ::: bot acc: 0.0607
current epoch: 20
train loss is 0.048321
average val loss: 0.042514, accuracy: 0.0436
average test loss: 0.040153, accuracy: 0.0407
case acc: 0.050301522
case acc: 0.04418048
case acc: 0.043395385
case acc: 0.026077634
case acc: 0.036803186
case acc: 0.043487452
top acc: 0.0273 ::: bot acc: 0.0811
top acc: 0.0158 ::: bot acc: 0.0758
top acc: 0.0304 ::: bot acc: 0.0763
top acc: 0.0197 ::: bot acc: 0.0424
top acc: 0.0559 ::: bot acc: 0.0439
top acc: 0.0272 ::: bot acc: 0.0857
current epoch: 21
train loss is 0.053848
average val loss: 0.068816, accuracy: 0.0696
average test loss: 0.068779, accuracy: 0.0691
case acc: 0.08199869
case acc: 0.08776297
case acc: 0.07570566
case acc: 0.054293543
case acc: 0.046639234
case acc: 0.068307966
top acc: 0.0372 ::: bot acc: 0.1235
top acc: 0.0507 ::: bot acc: 0.1236
top acc: 0.0301 ::: bot acc: 0.1246
top acc: 0.0229 ::: bot acc: 0.0828
top acc: 0.0166 ::: bot acc: 0.0891
top acc: 0.0280 ::: bot acc: 0.1223
current epoch: 22
train loss is 0.062345
average val loss: 0.074363, accuracy: 0.0749
average test loss: 0.074547, accuracy: 0.0746
case acc: 0.084472135
case acc: 0.095360026
case acc: 0.08451785
case acc: 0.06029891
case acc: 0.055038117
case acc: 0.068037
top acc: 0.0388 ::: bot acc: 0.1264
top acc: 0.0583 ::: bot acc: 0.1311
top acc: 0.0354 ::: bot acc: 0.1352
top acc: 0.0282 ::: bot acc: 0.0891
top acc: 0.0153 ::: bot acc: 0.1022
top acc: 0.0278 ::: bot acc: 0.1220
current epoch: 23
train loss is 0.062412
average val loss: 0.063021, accuracy: 0.0632
average test loss: 0.062729, accuracy: 0.0629
case acc: 0.06882602
case acc: 0.07867627
case acc: 0.074449785
case acc: 0.050080016
case acc: 0.05205287
case acc: 0.053549506
top acc: 0.0297 ::: bot acc: 0.1074
top acc: 0.0417 ::: bot acc: 0.1145
top acc: 0.0296 ::: bot acc: 0.1230
top acc: 0.0195 ::: bot acc: 0.0782
top acc: 0.0148 ::: bot acc: 0.0980
top acc: 0.0213 ::: bot acc: 0.1036
current epoch: 24
train loss is 0.057831
average val loss: 0.048284, accuracy: 0.0481
average test loss: 0.046830, accuracy: 0.0470
case acc: 0.049973153
case acc: 0.052758373
case acc: 0.05726853
case acc: 0.03583326
case acc: 0.044660144
case acc: 0.041654117
top acc: 0.0273 ::: bot acc: 0.0804
top acc: 0.0192 ::: bot acc: 0.0868
top acc: 0.0241 ::: bot acc: 0.1002
top acc: 0.0115 ::: bot acc: 0.0609
top acc: 0.0177 ::: bot acc: 0.0856
top acc: 0.0317 ::: bot acc: 0.0807
current epoch: 25
train loss is 0.048777
average val loss: 0.042428, accuracy: 0.0421
average test loss: 0.040029, accuracy: 0.0401
case acc: 0.04225771
case acc: 0.038500242
case acc: 0.047450546
case acc: 0.030472558
case acc: 0.04207882
case acc: 0.03981763
top acc: 0.0395 ::: bot acc: 0.0629
top acc: 0.0157 ::: bot acc: 0.0673
top acc: 0.0254 ::: bot acc: 0.0849
top acc: 0.0122 ::: bot acc: 0.0526
top acc: 0.0206 ::: bot acc: 0.0804
top acc: 0.0436 ::: bot acc: 0.0689
current epoch: 26
train loss is 0.043045
average val loss: 0.038644, accuracy: 0.0379
average test loss: 0.035416, accuracy: 0.0350
case acc: 0.0378094
case acc: 0.027403757
case acc: 0.04032497
case acc: 0.025115715
case acc: 0.03776873
case acc: 0.041330114
top acc: 0.0600 ::: bot acc: 0.0425
top acc: 0.0284 ::: bot acc: 0.0441
top acc: 0.0415 ::: bot acc: 0.0654
top acc: 0.0225 ::: bot acc: 0.0394
top acc: 0.0311 ::: bot acc: 0.0688
top acc: 0.0580 ::: bot acc: 0.0543
current epoch: 27
train loss is 0.041483
average val loss: 0.040216, accuracy: 0.0391
average test loss: 0.037641, accuracy: 0.0365
case acc: 0.040594332
case acc: 0.030430451
case acc: 0.040095635
case acc: 0.023239726
case acc: 0.035930578
case acc: 0.048824407
top acc: 0.0847 ::: bot acc: 0.0180
top acc: 0.0563 ::: bot acc: 0.0174
top acc: 0.0669 ::: bot acc: 0.0401
top acc: 0.0421 ::: bot acc: 0.0198
top acc: 0.0511 ::: bot acc: 0.0488
top acc: 0.0770 ::: bot acc: 0.0386
current epoch: 28
train loss is 0.044534
average val loss: 0.052399, accuracy: 0.0510
average test loss: 0.051685, accuracy: 0.0510
case acc: 0.05799461
case acc: 0.051643007
case acc: 0.053487934
case acc: 0.036678035
case acc: 0.045788527
case acc: 0.060598377
top acc: 0.1123 ::: bot acc: 0.0145
top acc: 0.0878 ::: bot acc: 0.0175
top acc: 0.0972 ::: bot acc: 0.0212
top acc: 0.0674 ::: bot acc: 0.0119
top acc: 0.0790 ::: bot acc: 0.0255
top acc: 0.0992 ::: bot acc: 0.0290
current epoch: 29
train loss is 0.049655
average val loss: 0.065918, accuracy: 0.0648
average test loss: 0.066573, accuracy: 0.0666
case acc: 0.07391725
case acc: 0.07204771
case acc: 0.070355356
case acc: 0.053356938
case acc: 0.06026087
case acc: 0.06963526
top acc: 0.1293 ::: bot acc: 0.0282
top acc: 0.1089 ::: bot acc: 0.0364
top acc: 0.1193 ::: bot acc: 0.0275
top acc: 0.0859 ::: bot acc: 0.0249
top acc: 0.1024 ::: bot acc: 0.0217
top acc: 0.1121 ::: bot acc: 0.0301
current epoch: 30
train loss is 0.054237
average val loss: 0.065191, accuracy: 0.0645
average test loss: 0.065796, accuracy: 0.0658
case acc: 0.06924474
case acc: 0.06985865
case acc: 0.07117072
case acc: 0.05405858
case acc: 0.06533686
case acc: 0.06518395
top acc: 0.1243 ::: bot acc: 0.0240
top acc: 0.1067 ::: bot acc: 0.0343
top acc: 0.1203 ::: bot acc: 0.0279
top acc: 0.0866 ::: bot acc: 0.0255
top acc: 0.1093 ::: bot acc: 0.0229
top acc: 0.1060 ::: bot acc: 0.0292
current epoch: 31
train loss is 0.055352
average val loss: 0.050814, accuracy: 0.0510
average test loss: 0.049961, accuracy: 0.0496
case acc: 0.048067376
case acc: 0.046394527
case acc: 0.05469236
case acc: 0.039078206
case acc: 0.0571896
case acc: 0.05193855
top acc: 0.0994 ::: bot acc: 0.0104
top acc: 0.0816 ::: bot acc: 0.0144
top acc: 0.0991 ::: bot acc: 0.0211
top acc: 0.0701 ::: bot acc: 0.0136
top acc: 0.0979 ::: bot acc: 0.0215
top acc: 0.0837 ::: bot acc: 0.0344
current epoch: 32
train loss is 0.048197
average val loss: 0.041600, accuracy: 0.0423
average test loss: 0.039378, accuracy: 0.0391
case acc: 0.03883548
case acc: 0.030378778
case acc: 0.04284642
case acc: 0.028395643
case acc: 0.04999601
case acc: 0.044350535
top acc: 0.0767 ::: bot acc: 0.0261
top acc: 0.0562 ::: bot acc: 0.0176
top acc: 0.0773 ::: bot acc: 0.0297
top acc: 0.0561 ::: bot acc: 0.0096
top acc: 0.0865 ::: bot acc: 0.0229
top acc: 0.0663 ::: bot acc: 0.0467
current epoch: 33
train loss is 0.041417
average val loss: 0.038040, accuracy: 0.0389
average test loss: 0.034789, accuracy: 0.0351
case acc: 0.03844101
case acc: 0.026796352
case acc: 0.038991146
case acc: 0.023272455
case acc: 0.04283139
case acc: 0.04005765
top acc: 0.0563 ::: bot acc: 0.0467
top acc: 0.0319 ::: bot acc: 0.0406
top acc: 0.0557 ::: bot acc: 0.0515
top acc: 0.0425 ::: bot acc: 0.0193
top acc: 0.0732 ::: bot acc: 0.0283
top acc: 0.0518 ::: bot acc: 0.0603
current epoch: 34
train loss is 0.039897
average val loss: 0.040779, accuracy: 0.0415
average test loss: 0.038052, accuracy: 0.0385
case acc: 0.045202598
case acc: 0.03960356
case acc: 0.044381198
case acc: 0.025067363
case acc: 0.03589661
case acc: 0.04104742
top acc: 0.0330 ::: bot acc: 0.0708
top acc: 0.0155 ::: bot acc: 0.0689
top acc: 0.0287 ::: bot acc: 0.0788
top acc: 0.0223 ::: bot acc: 0.0395
top acc: 0.0509 ::: bot acc: 0.0490
top acc: 0.0331 ::: bot acc: 0.0788
current epoch: 35
train loss is 0.043321
average val loss: 0.051589, accuracy: 0.0519
average test loss: 0.050484, accuracy: 0.0508
case acc: 0.0577866
case acc: 0.06039148
case acc: 0.061401777
case acc: 0.03620448
case acc: 0.040121384
case acc: 0.048973877
top acc: 0.0258 ::: bot acc: 0.0930
top acc: 0.0243 ::: bot acc: 0.0956
top acc: 0.0245 ::: bot acc: 0.1062
top acc: 0.0115 ::: bot acc: 0.0613
top acc: 0.0247 ::: bot acc: 0.0755
top acc: 0.0218 ::: bot acc: 0.0965
current epoch: 36
train loss is 0.047751
average val loss: 0.055011, accuracy: 0.0550
average test loss: 0.054183, accuracy: 0.0544
case acc: 0.059119076
case acc: 0.065466315
case acc: 0.06747922
case acc: 0.040382244
case acc: 0.04542046
case acc: 0.048636325
top acc: 0.0260 ::: bot acc: 0.0949
top acc: 0.0289 ::: bot acc: 0.1009
top acc: 0.0266 ::: bot acc: 0.1143
top acc: 0.0130 ::: bot acc: 0.0668
top acc: 0.0171 ::: bot acc: 0.0871
top acc: 0.0220 ::: bot acc: 0.0959
current epoch: 37
train loss is 0.049988
average val loss: 0.049025, accuracy: 0.0487
average test loss: 0.047646, accuracy: 0.0478
case acc: 0.05017491
case acc: 0.053685516
case acc: 0.060842387
case acc: 0.035062518
case acc: 0.044695392
case acc: 0.042272873
top acc: 0.0274 ::: bot acc: 0.0809
top acc: 0.0196 ::: bot acc: 0.0878
top acc: 0.0243 ::: bot acc: 0.1055
top acc: 0.0112 ::: bot acc: 0.0598
top acc: 0.0176 ::: bot acc: 0.0858
top acc: 0.0291 ::: bot acc: 0.0830
current epoch: 38
train loss is 0.047384
average val loss: 0.040583, accuracy: 0.0400
average test loss: 0.037791, accuracy: 0.0380
case acc: 0.04060123
case acc: 0.035224766
case acc: 0.04676132
case acc: 0.02653151
case acc: 0.039363846
case acc: 0.039671585
top acc: 0.0466 ::: bot acc: 0.0565
top acc: 0.0164 ::: bot acc: 0.0619
top acc: 0.0257 ::: bot acc: 0.0839
top acc: 0.0181 ::: bot acc: 0.0437
top acc: 0.0265 ::: bot acc: 0.0735
top acc: 0.0491 ::: bot acc: 0.0629
current epoch: 39
train loss is 0.041740
average val loss: 0.038120, accuracy: 0.0372
average test loss: 0.034857, accuracy: 0.0345
case acc: 0.037785918
case acc: 0.02619621
case acc: 0.039965056
case acc: 0.023093063
case acc: 0.036201283
case acc: 0.04395166
top acc: 0.0686 ::: bot acc: 0.0345
top acc: 0.0353 ::: bot acc: 0.0369
top acc: 0.0452 ::: bot acc: 0.0622
top acc: 0.0327 ::: bot acc: 0.0291
top acc: 0.0395 ::: bot acc: 0.0604
top acc: 0.0655 ::: bot acc: 0.0470
current epoch: 40
train loss is 0.040756
average val loss: 0.043494, accuracy: 0.0425
average test loss: 0.041563, accuracy: 0.0407
case acc: 0.045837864
case acc: 0.03597537
case acc: 0.04204116
case acc: 0.027717035
case acc: 0.03886205
case acc: 0.053505674
top acc: 0.0959 ::: bot acc: 0.0113
top acc: 0.0665 ::: bot acc: 0.0133
top acc: 0.0744 ::: bot acc: 0.0330
top acc: 0.0551 ::: bot acc: 0.0095
top acc: 0.0631 ::: bot acc: 0.0368
top acc: 0.0869 ::: bot acc: 0.0322
current epoch: 41
train loss is 0.043425
average val loss: 0.056661, accuracy: 0.0555
average test loss: 0.056485, accuracy: 0.0563
case acc: 0.06374773
case acc: 0.057015814
case acc: 0.0562745
case acc: 0.045073338
case acc: 0.051341947
case acc: 0.06447442
top acc: 0.1189 ::: bot acc: 0.0189
top acc: 0.0937 ::: bot acc: 0.0218
top acc: 0.1015 ::: bot acc: 0.0211
top acc: 0.0769 ::: bot acc: 0.0178
top acc: 0.0886 ::: bot acc: 0.0225
top acc: 0.1047 ::: bot acc: 0.0292
current epoch: 42
train loss is 0.048028
average val loss: 0.060338, accuracy: 0.0595
average test loss: 0.060575, accuracy: 0.0606
case acc: 0.065810785
case acc: 0.062347736
case acc: 0.062182315
case acc: 0.05024558
case acc: 0.05877236
case acc: 0.06408699
top acc: 0.1210 ::: bot acc: 0.0207
top acc: 0.0991 ::: bot acc: 0.0269
top acc: 0.1095 ::: bot acc: 0.0228
top acc: 0.0824 ::: bot acc: 0.0222
top acc: 0.1001 ::: bot acc: 0.0217
top acc: 0.1042 ::: bot acc: 0.0292
current epoch: 43
train loss is 0.049125
average val loss: 0.050682, accuracy: 0.0506
average test loss: 0.049844, accuracy: 0.0495
case acc: 0.05006877
case acc: 0.04636758
case acc: 0.05241824
case acc: 0.039926093
case acc: 0.054870553
case acc: 0.05346202
top acc: 0.1026 ::: bot acc: 0.0104
top acc: 0.0815 ::: bot acc: 0.0144
top acc: 0.0958 ::: bot acc: 0.0212
top acc: 0.0710 ::: bot acc: 0.0141
top acc: 0.0943 ::: bot acc: 0.0217
top acc: 0.0868 ::: bot acc: 0.0323
current epoch: 44
train loss is 0.044514
average val loss: 0.042023, accuracy: 0.0425
average test loss: 0.039946, accuracy: 0.0397
case acc: 0.039779946
case acc: 0.03160023
case acc: 0.04272346
case acc: 0.029519109
case acc: 0.048946314
case acc: 0.04585903
top acc: 0.0816 ::: bot acc: 0.0214
top acc: 0.0587 ::: bot acc: 0.0161
top acc: 0.0769 ::: bot acc: 0.0305
top acc: 0.0579 ::: bot acc: 0.0092
top acc: 0.0846 ::: bot acc: 0.0235
top acc: 0.0700 ::: bot acc: 0.0435
current epoch: 45
train loss is 0.038546
average val loss: 0.037893, accuracy: 0.0386
average test loss: 0.034629, accuracy: 0.0348
case acc: 0.037942328
case acc: 0.026365638
case acc: 0.039053284
case acc: 0.023282353
case acc: 0.04166773
case acc: 0.040328648
top acc: 0.0604 ::: bot acc: 0.0426
top acc: 0.0343 ::: bot acc: 0.0380
top acc: 0.0553 ::: bot acc: 0.0520
top acc: 0.0431 ::: bot acc: 0.0186
top acc: 0.0706 ::: bot acc: 0.0300
top acc: 0.0540 ::: bot acc: 0.0580
current epoch: 46
train loss is 0.036845
average val loss: 0.039128, accuracy: 0.0397
average test loss: 0.035998, accuracy: 0.0365
case acc: 0.041960206
case acc: 0.034510598
case acc: 0.042552136
case acc: 0.023610955
case acc: 0.036259245
case acc: 0.039869975
top acc: 0.0414 ::: bot acc: 0.0616
top acc: 0.0167 ::: bot acc: 0.0607
top acc: 0.0338 ::: bot acc: 0.0735
top acc: 0.0275 ::: bot acc: 0.0342
top acc: 0.0531 ::: bot acc: 0.0467
top acc: 0.0397 ::: bot acc: 0.0722
current epoch: 47
train loss is 0.039226
average val loss: 0.045539, accuracy: 0.0457
average test loss: 0.043739, accuracy: 0.0441
case acc: 0.049370006
case acc: 0.049212664
case acc: 0.054136187
case acc: 0.030186245
case acc: 0.037805185
case acc: 0.04363867
top acc: 0.0278 ::: bot acc: 0.0794
top acc: 0.0177 ::: bot acc: 0.0821
top acc: 0.0238 ::: bot acc: 0.0958
top acc: 0.0121 ::: bot acc: 0.0521
top acc: 0.0310 ::: bot acc: 0.0688
top acc: 0.0265 ::: bot acc: 0.0863
current epoch: 48
train loss is 0.042587
average val loss: 0.046719, accuracy: 0.0466
average test loss: 0.045111, accuracy: 0.0453
case acc: 0.04898813
case acc: 0.050557382
case acc: 0.057203367
case acc: 0.031913288
case acc: 0.040743556
case acc: 0.04262485
top acc: 0.0281 ::: bot acc: 0.0787
top acc: 0.0182 ::: bot acc: 0.0838
top acc: 0.0239 ::: bot acc: 0.1003
top acc: 0.0115 ::: bot acc: 0.0550
top acc: 0.0232 ::: bot acc: 0.0772
top acc: 0.0282 ::: bot acc: 0.0839
current epoch: 49
train loss is 0.043203
average val loss: 0.041897, accuracy: 0.0415
average test loss: 0.039437, accuracy: 0.0397
case acc: 0.04260027
case acc: 0.039706506
case acc: 0.04990893
case acc: 0.027373118
case acc: 0.03921148
case acc: 0.039654203
top acc: 0.0395 ::: bot acc: 0.0635
top acc: 0.0156 ::: bot acc: 0.0689
top acc: 0.0243 ::: bot acc: 0.0892
top acc: 0.0158 ::: bot acc: 0.0461
top acc: 0.0268 ::: bot acc: 0.0732
top acc: 0.0416 ::: bot acc: 0.0703
current epoch: 50
train loss is 0.040620
average val loss: 0.038228, accuracy: 0.0376
average test loss: 0.034897, accuracy: 0.0348
case acc: 0.037976306
case acc: 0.02796713
case acc: 0.041545734
case acc: 0.02331674
case acc: 0.036430996
case acc: 0.04134036
top acc: 0.0601 ::: bot acc: 0.0429
top acc: 0.0257 ::: bot acc: 0.0464
top acc: 0.0371 ::: bot acc: 0.0703
top acc: 0.0293 ::: bot acc: 0.0324
top acc: 0.0376 ::: bot acc: 0.0622
top acc: 0.0582 ::: bot acc: 0.0537

		{"drop_out": 0.6, "drop_out_mc": 0.15, "repeat_mc": 50, "hidden": 30, "embedding_size": 5, "batch": 512, "lag": 4}
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
LME_Co_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5376 5376 5376
1.7082474 -0.6288155 0.24786325 -0.22413088
Validation: 600 600 600
Testing: 774 774 774
pre-processing time: 0.00044274330139160156
the split date is 2010-07-01
net initializing with time: 0.005576133728027344
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.122204
average val loss: 0.126463, accuracy: 0.1270
average test loss: 0.140188, accuracy: 0.1421
case acc: 0.20283651
case acc: 0.04280199
case acc: 0.16201632
case acc: 0.15006983
case acc: 0.17732626
case acc: 0.117443234
top acc: 0.1819 ::: bot acc: 0.2250
top acc: 0.0783 ::: bot acc: 0.0125
top acc: 0.1164 ::: bot acc: 0.2129
top acc: 0.1170 ::: bot acc: 0.1847
top acc: 0.1445 ::: bot acc: 0.2104
top acc: 0.0882 ::: bot acc: 0.1509
current epoch: 2
train loss is 0.116520
average val loss: 0.062123, accuracy: 0.0621
average test loss: 0.058654, accuracy: 0.0595
case acc: 0.047236945
case acc: 0.17807989
case acc: 0.034769118
case acc: 0.025305381
case acc: 0.036935173
case acc: 0.034496468
top acc: 0.0265 ::: bot acc: 0.0696
top acc: 0.2194 ::: bot acc: 0.1354
top acc: 0.0389 ::: bot acc: 0.0562
top acc: 0.0327 ::: bot acc: 0.0351
top acc: 0.0091 ::: bot acc: 0.0676
top acc: 0.0546 ::: bot acc: 0.0182
current epoch: 3
train loss is 0.118699
average val loss: 0.151132, accuracy: 0.1512
average test loss: 0.132290, accuracy: 0.1322
case acc: 0.073967144
case acc: 0.28277463
case acc: 0.11283248
case acc: 0.11262916
case acc: 0.078707375
case acc: 0.13228488
top acc: 0.0938 ::: bot acc: 0.0533
top acc: 0.3239 ::: bot acc: 0.2400
top acc: 0.1574 ::: bot acc: 0.0631
top acc: 0.1458 ::: bot acc: 0.0783
top acc: 0.1124 ::: bot acc: 0.0452
top acc: 0.1613 ::: bot acc: 0.0979
current epoch: 4
train loss is 0.129216
average val loss: 0.094124, accuracy: 0.0952
average test loss: 0.077849, accuracy: 0.0760
case acc: 0.024158046
case acc: 0.21536075
case acc: 0.061146323
case acc: 0.05419107
case acc: 0.031681392
case acc: 0.06959157
top acc: 0.0398 ::: bot acc: 0.0119
top acc: 0.2564 ::: bot acc: 0.1727
top acc: 0.1011 ::: bot acc: 0.0207
top acc: 0.0867 ::: bot acc: 0.0213
top acc: 0.0592 ::: bot acc: 0.0108
top acc: 0.0985 ::: bot acc: 0.0352
current epoch: 5
train loss is 0.106572
average val loss: 0.055514, accuracy: 0.0551
average test loss: 0.050041, accuracy: 0.0506
case acc: 0.031094337
case acc: 0.15325639
case acc: 0.035006043
case acc: 0.025313813
case acc: 0.028652536
case acc: 0.030141992
top acc: 0.0118 ::: bot acc: 0.0530
top acc: 0.1945 ::: bot acc: 0.1104
top acc: 0.0471 ::: bot acc: 0.0473
top acc: 0.0339 ::: bot acc: 0.0335
top acc: 0.0164 ::: bot acc: 0.0513
top acc: 0.0461 ::: bot acc: 0.0217
current epoch: 6
train loss is 0.088275
average val loss: 0.051561, accuracy: 0.0514
average test loss: 0.046775, accuracy: 0.0478
case acc: 0.032684427
case acc: 0.13834928
case acc: 0.034419
case acc: 0.025369948
case acc: 0.027040666
case acc: 0.028819844
top acc: 0.0130 ::: bot acc: 0.0548
top acc: 0.1796 ::: bot acc: 0.0954
top acc: 0.0388 ::: bot acc: 0.0554
top acc: 0.0293 ::: bot acc: 0.0380
top acc: 0.0223 ::: bot acc: 0.0454
top acc: 0.0433 ::: bot acc: 0.0233
current epoch: 7
train loss is 0.077490
average val loss: 0.055603, accuracy: 0.0561
average test loss: 0.046585, accuracy: 0.0471
case acc: 0.021319656
case acc: 0.13907795
case acc: 0.034884665
case acc: 0.025812449
case acc: 0.026171375
case acc: 0.035196695
top acc: 0.0072 ::: bot acc: 0.0408
top acc: 0.1803 ::: bot acc: 0.0961
top acc: 0.0467 ::: bot acc: 0.0473
top acc: 0.0400 ::: bot acc: 0.0273
top acc: 0.0425 ::: bot acc: 0.0254
top acc: 0.0548 ::: bot acc: 0.0191
current epoch: 8
train loss is 0.073660
average val loss: 0.063920, accuracy: 0.0646
average test loss: 0.050836, accuracy: 0.0503
case acc: 0.015911482
case acc: 0.14269884
case acc: 0.037957035
case acc: 0.029130854
case acc: 0.033559468
case acc: 0.042596187
top acc: 0.0198 ::: bot acc: 0.0235
top acc: 0.1841 ::: bot acc: 0.0997
top acc: 0.0592 ::: bot acc: 0.0351
top acc: 0.0526 ::: bot acc: 0.0147
top acc: 0.0629 ::: bot acc: 0.0096
top acc: 0.0666 ::: bot acc: 0.0177
current epoch: 9
train loss is 0.074770
average val loss: 0.070800, accuracy: 0.0715
average test loss: 0.055591, accuracy: 0.0550
case acc: 0.0199227
case acc: 0.14322896
case acc: 0.042220358
case acc: 0.03351
case acc: 0.043368667
case acc: 0.04760443
top acc: 0.0328 ::: bot acc: 0.0129
top acc: 0.1848 ::: bot acc: 0.1002
top acc: 0.0691 ::: bot acc: 0.0281
top acc: 0.0617 ::: bot acc: 0.0098
top acc: 0.0764 ::: bot acc: 0.0121
top acc: 0.0736 ::: bot acc: 0.0188
current epoch: 10
train loss is 0.073374
average val loss: 0.078368, accuracy: 0.0788
average test loss: 0.061693, accuracy: 0.0613
case acc: 0.02859752
case acc: 0.14445592
case acc: 0.04770734
case acc: 0.03970371
case acc: 0.054765962
case acc: 0.052777797
top acc: 0.0455 ::: bot acc: 0.0134
top acc: 0.1861 ::: bot acc: 0.1014
top acc: 0.0799 ::: bot acc: 0.0232
top acc: 0.0706 ::: bot acc: 0.0105
top acc: 0.0886 ::: bot acc: 0.0217
top acc: 0.0800 ::: bot acc: 0.0215
current epoch: 11
train loss is 0.072876
average val loss: 0.078617, accuracy: 0.0789
average test loss: 0.061720, accuracy: 0.0615
case acc: 0.031958766
case acc: 0.1377613
case acc: 0.048959
case acc: 0.04031912
case acc: 0.058356967
case acc: 0.051634237
top acc: 0.0495 ::: bot acc: 0.0156
top acc: 0.1795 ::: bot acc: 0.0946
top acc: 0.0820 ::: bot acc: 0.0226
top acc: 0.0714 ::: bot acc: 0.0107
top acc: 0.0924 ::: bot acc: 0.0250
top acc: 0.0786 ::: bot acc: 0.0207
current epoch: 12
train loss is 0.070515
average val loss: 0.074632, accuracy: 0.0749
average test loss: 0.058004, accuracy: 0.0579
case acc: 0.03073899
case acc: 0.12649515
case acc: 0.04711105
case acc: 0.037724603
case acc: 0.057421226
case acc: 0.04773477
top acc: 0.0480 ::: bot acc: 0.0148
top acc: 0.1681 ::: bot acc: 0.0834
top acc: 0.0788 ::: bot acc: 0.0236
top acc: 0.0680 ::: bot acc: 0.0098
top acc: 0.0914 ::: bot acc: 0.0241
top acc: 0.0737 ::: bot acc: 0.0187
current epoch: 13
train loss is 0.066905
average val loss: 0.064599, accuracy: 0.0649
average test loss: 0.049275, accuracy: 0.0492
case acc: 0.023767352
case acc: 0.10836597
case acc: 0.041614383
case acc: 0.031642444
case acc: 0.049291234
case acc: 0.04053757
top acc: 0.0390 ::: bot acc: 0.0119
top acc: 0.1500 ::: bot acc: 0.0653
top acc: 0.0680 ::: bot acc: 0.0286
top acc: 0.0585 ::: bot acc: 0.0108
top acc: 0.0830 ::: bot acc: 0.0166
top acc: 0.0632 ::: bot acc: 0.0179
current epoch: 14
train loss is 0.062373
average val loss: 0.058535, accuracy: 0.0588
average test loss: 0.044222, accuracy: 0.0442
case acc: 0.020667804
case acc: 0.09454703
case acc: 0.03863243
case acc: 0.02947001
case acc: 0.044600774
case acc: 0.03726297
top acc: 0.0340 ::: bot acc: 0.0126
top acc: 0.1361 ::: bot acc: 0.0515
top acc: 0.0614 ::: bot acc: 0.0330
top acc: 0.0535 ::: bot acc: 0.0142
top acc: 0.0778 ::: bot acc: 0.0130
top acc: 0.0578 ::: bot acc: 0.0186
current epoch: 15
train loss is 0.058778
average val loss: 0.053365, accuracy: 0.0537
average test loss: 0.040087, accuracy: 0.0401
case acc: 0.018358316
case acc: 0.08197187
case acc: 0.0368131
case acc: 0.028225321
case acc: 0.04012058
case acc: 0.034888357
top acc: 0.0297 ::: bot acc: 0.0143
top acc: 0.1235 ::: bot acc: 0.0389
top acc: 0.0558 ::: bot acc: 0.0384
top acc: 0.0498 ::: bot acc: 0.0178
top acc: 0.0722 ::: bot acc: 0.0107
top acc: 0.0537 ::: bot acc: 0.0196
current epoch: 16
train loss is 0.055674
average val loss: 0.049417, accuracy: 0.0497
average test loss: 0.036980, accuracy: 0.0368
case acc: 0.017132478
case acc: 0.07137923
case acc: 0.035743322
case acc: 0.02749626
case acc: 0.036164623
case acc: 0.033148848
top acc: 0.0268 ::: bot acc: 0.0164
top acc: 0.1129 ::: bot acc: 0.0284
top acc: 0.0521 ::: bot acc: 0.0422
top acc: 0.0474 ::: bot acc: 0.0201
top acc: 0.0668 ::: bot acc: 0.0096
top acc: 0.0506 ::: bot acc: 0.0205
current epoch: 17
train loss is 0.052776
average val loss: 0.047066, accuracy: 0.0473
average test loss: 0.035075, accuracy: 0.0348
case acc: 0.016902616
case acc: 0.06365468
case acc: 0.035540204
case acc: 0.027377902
case acc: 0.033370733
case acc: 0.032087747
top acc: 0.0262 ::: bot acc: 0.0169
top acc: 0.1050 ::: bot acc: 0.0210
top acc: 0.0510 ::: bot acc: 0.0434
top acc: 0.0469 ::: bot acc: 0.0207
top acc: 0.0625 ::: bot acc: 0.0099
top acc: 0.0488 ::: bot acc: 0.0211
current epoch: 18
train loss is 0.050982
average val loss: 0.047846, accuracy: 0.0480
average test loss: 0.035298, accuracy: 0.0350
case acc: 0.01823923
case acc: 0.060965493
case acc: 0.036527537
case acc: 0.028447963
case acc: 0.032840032
case acc: 0.032834783
top acc: 0.0295 ::: bot acc: 0.0144
top acc: 0.1022 ::: bot acc: 0.0186
top acc: 0.0546 ::: bot acc: 0.0401
top acc: 0.0502 ::: bot acc: 0.0176
top acc: 0.0615 ::: bot acc: 0.0103
top acc: 0.0505 ::: bot acc: 0.0204
current epoch: 19
train loss is 0.050131
average val loss: 0.045862, accuracy: 0.0459
average test loss: 0.033806, accuracy: 0.0334
case acc: 0.018193765
case acc: 0.05599701
case acc: 0.036602516
case acc: 0.028060915
case acc: 0.030439116
case acc: 0.030933835
top acc: 0.0294 ::: bot acc: 0.0147
top acc: 0.0964 ::: bot acc: 0.0151
top acc: 0.0548 ::: bot acc: 0.0400
top acc: 0.0491 ::: bot acc: 0.0186
top acc: 0.0573 ::: bot acc: 0.0118
top acc: 0.0473 ::: bot acc: 0.0212
current epoch: 20
train loss is 0.049021
average val loss: 0.048305, accuracy: 0.0483
average test loss: 0.035306, accuracy: 0.0348
case acc: 0.021065645
case acc: 0.056860857
case acc: 0.038552
case acc: 0.029436123
case acc: 0.030943064
case acc: 0.031808384
top acc: 0.0347 ::: bot acc: 0.0126
top acc: 0.0974 ::: bot acc: 0.0156
top acc: 0.0610 ::: bot acc: 0.0338
top acc: 0.0533 ::: bot acc: 0.0145
top acc: 0.0583 ::: bot acc: 0.0113
top acc: 0.0489 ::: bot acc: 0.0207
current epoch: 21
train loss is 0.049657
average val loss: 0.054692, accuracy: 0.0547
average test loss: 0.039959, accuracy: 0.0394
case acc: 0.027540972
case acc: 0.06252446
case acc: 0.043382682
case acc: 0.03378988
case acc: 0.03394407
case acc: 0.03517961
top acc: 0.0442 ::: bot acc: 0.0131
top acc: 0.1036 ::: bot acc: 0.0201
top acc: 0.0718 ::: bot acc: 0.0266
top acc: 0.0620 ::: bot acc: 0.0102
top acc: 0.0635 ::: bot acc: 0.0099
top acc: 0.0548 ::: bot acc: 0.0190
current epoch: 22
train loss is 0.050340
average val loss: 0.061519, accuracy: 0.0616
average test loss: 0.045559, accuracy: 0.0450
case acc: 0.035438154
case acc: 0.06944973
case acc: 0.049014512
case acc: 0.039469
case acc: 0.03808115
case acc: 0.038528204
top acc: 0.0534 ::: bot acc: 0.0185
top acc: 0.1108 ::: bot acc: 0.0265
top acc: 0.0825 ::: bot acc: 0.0220
top acc: 0.0704 ::: bot acc: 0.0106
top acc: 0.0695 ::: bot acc: 0.0103
top acc: 0.0602 ::: bot acc: 0.0179
current epoch: 23
train loss is 0.052447
average val loss: 0.071073, accuracy: 0.0712
average test loss: 0.053967, accuracy: 0.0534
case acc: 0.04616397
case acc: 0.0792652
case acc: 0.056958184
case acc: 0.04842061
case acc: 0.045849755
case acc: 0.043945767
top acc: 0.0648 ::: bot acc: 0.0278
top acc: 0.1206 ::: bot acc: 0.0363
top acc: 0.0955 ::: bot acc: 0.0199
top acc: 0.0809 ::: bot acc: 0.0164
top acc: 0.0793 ::: bot acc: 0.0141
top acc: 0.0684 ::: bot acc: 0.0178
current epoch: 24
train loss is 0.055376
average val loss: 0.075318, accuracy: 0.0754
average test loss: 0.057879, accuracy: 0.0574
case acc: 0.0512067
case acc: 0.08223547
case acc: 0.061328903
case acc: 0.0525416
case acc: 0.051268283
case acc: 0.04588332
top acc: 0.0701 ::: bot acc: 0.0324
top acc: 0.1236 ::: bot acc: 0.0393
top acc: 0.1018 ::: bot acc: 0.0207
top acc: 0.0853 ::: bot acc: 0.0201
top acc: 0.0851 ::: bot acc: 0.0185
top acc: 0.0712 ::: bot acc: 0.0182
current epoch: 25
train loss is 0.057019
average val loss: 0.073059, accuracy: 0.0731
average test loss: 0.055748, accuracy: 0.0553
case acc: 0.049330693
case acc: 0.07661666
case acc: 0.05971472
case acc: 0.050252065
case acc: 0.052140266
case acc: 0.043769658
top acc: 0.0682 ::: bot acc: 0.0307
top acc: 0.1180 ::: bot acc: 0.0337
top acc: 0.0995 ::: bot acc: 0.0204
top acc: 0.0828 ::: bot acc: 0.0181
top acc: 0.0861 ::: bot acc: 0.0193
top acc: 0.0682 ::: bot acc: 0.0177
current epoch: 26
train loss is 0.057254
average val loss: 0.066647, accuracy: 0.0666
average test loss: 0.049858, accuracy: 0.0494
case acc: 0.04264402
case acc: 0.06496453
case acc: 0.053995375
case acc: 0.044512395
case acc: 0.050078142
case acc: 0.040267866
top acc: 0.0611 ::: bot acc: 0.0247
top acc: 0.1062 ::: bot acc: 0.0222
top acc: 0.0910 ::: bot acc: 0.0203
top acc: 0.0765 ::: bot acc: 0.0136
top acc: 0.0839 ::: bot acc: 0.0174
top acc: 0.0630 ::: bot acc: 0.0175
current epoch: 27
train loss is 0.055575
average val loss: 0.052501, accuracy: 0.0524
average test loss: 0.037899, accuracy: 0.0376
case acc: 0.027530907
case acc: 0.046714846
case acc: 0.043263946
case acc: 0.033792164
case acc: 0.040882297
case acc: 0.033229534
top acc: 0.0443 ::: bot acc: 0.0129
top acc: 0.0841 ::: bot acc: 0.0117
top acc: 0.0716 ::: bot acc: 0.0268
top acc: 0.0619 ::: bot acc: 0.0105
top acc: 0.0731 ::: bot acc: 0.0113
top acc: 0.0512 ::: bot acc: 0.0199
current epoch: 28
train loss is 0.049456
average val loss: 0.035736, accuracy: 0.0360
average test loss: 0.027547, accuracy: 0.0275
case acc: 0.0158749
case acc: 0.03352446
case acc: 0.034517102
case acc: 0.025907168
case acc: 0.029586788
case acc: 0.02537315
top acc: 0.0207 ::: bot acc: 0.0225
top acc: 0.0552 ::: bot acc: 0.0291
top acc: 0.0450 ::: bot acc: 0.0496
top acc: 0.0414 ::: bot acc: 0.0260
top acc: 0.0552 ::: bot acc: 0.0134
top acc: 0.0344 ::: bot acc: 0.0299
current epoch: 29
train loss is 0.044308
average val loss: 0.027286, accuracy: 0.0274
average test loss: 0.028743, accuracy: 0.0290
case acc: 0.025972098
case acc: 0.032758962
case acc: 0.039356504
case acc: 0.026695173
case acc: 0.02536625
case acc: 0.023753732
top acc: 0.0089 ::: bot acc: 0.0466
top acc: 0.0272 ::: bot acc: 0.0573
top acc: 0.0201 ::: bot acc: 0.0762
top acc: 0.0204 ::: bot acc: 0.0469
top acc: 0.0349 ::: bot acc: 0.0332
top acc: 0.0167 ::: bot acc: 0.0473
current epoch: 30
train loss is 0.044938
average val loss: 0.035182, accuracy: 0.0343
average test loss: 0.046717, accuracy: 0.0465
case acc: 0.053922158
case acc: 0.049538754
case acc: 0.06174626
case acc: 0.04266119
case acc: 0.033354294
case acc: 0.03793037
top acc: 0.0332 ::: bot acc: 0.0763
top acc: 0.0133 ::: bot acc: 0.0899
top acc: 0.0236 ::: bot acc: 0.1080
top acc: 0.0139 ::: bot acc: 0.0742
top acc: 0.0093 ::: bot acc: 0.0618
top acc: 0.0107 ::: bot acc: 0.0723
current epoch: 31
train loss is 0.051663
average val loss: 0.052675, accuracy: 0.0517
average test loss: 0.067968, accuracy: 0.0677
case acc: 0.077493265
case acc: 0.0720346
case acc: 0.084189735
case acc: 0.06249195
case acc: 0.053025905
case acc: 0.057060238
top acc: 0.0568 ::: bot acc: 0.0998
top acc: 0.0310 ::: bot acc: 0.1149
top acc: 0.0406 ::: bot acc: 0.1332
top acc: 0.0296 ::: bot acc: 0.0960
top acc: 0.0188 ::: bot acc: 0.0866
top acc: 0.0281 ::: bot acc: 0.0923
current epoch: 32
train loss is 0.061415
average val loss: 0.050639, accuracy: 0.0496
average test loss: 0.065670, accuracy: 0.0654
case acc: 0.07509656
case acc: 0.06795371
case acc: 0.08179223
case acc: 0.05922284
case acc: 0.054857165
case acc: 0.05333281
top acc: 0.0544 ::: bot acc: 0.0974
top acc: 0.0271 ::: bot acc: 0.1107
top acc: 0.0383 ::: bot acc: 0.1308
top acc: 0.0265 ::: bot acc: 0.0927
top acc: 0.0205 ::: bot acc: 0.0884
top acc: 0.0244 ::: bot acc: 0.0885
current epoch: 33
train loss is 0.062952
average val loss: 0.031762, accuracy: 0.0308
average test loss: 0.041829, accuracy: 0.0415
case acc: 0.047404632
case acc: 0.04115
case acc: 0.05540597
case acc: 0.036150273
case acc: 0.03751168
case acc: 0.031337775
top acc: 0.0268 ::: bot acc: 0.0696
top acc: 0.0119 ::: bot acc: 0.0781
top acc: 0.0201 ::: bot acc: 0.1003
top acc: 0.0114 ::: bot acc: 0.0656
top acc: 0.0084 ::: bot acc: 0.0685
top acc: 0.0076 ::: bot acc: 0.0638
current epoch: 34
train loss is 0.052815
average val loss: 0.027152, accuracy: 0.0266
average test loss: 0.031408, accuracy: 0.0314
case acc: 0.030388957
case acc: 0.032406267
case acc: 0.040616367
case acc: 0.027643781
case acc: 0.031995
case acc: 0.025393981
top acc: 0.0116 ::: bot acc: 0.0516
top acc: 0.0304 ::: bot acc: 0.0547
top acc: 0.0195 ::: bot acc: 0.0784
top acc: 0.0166 ::: bot acc: 0.0502
top acc: 0.0103 ::: bot acc: 0.0592
top acc: 0.0116 ::: bot acc: 0.0529
current epoch: 35
train loss is 0.044832
average val loss: 0.028481, accuracy: 0.0279
average test loss: 0.027653, accuracy: 0.0275
case acc: 0.019954635
case acc: 0.032532148
case acc: 0.034807034
case acc: 0.025084658
case acc: 0.029013904
case acc: 0.023789076
top acc: 0.0074 ::: bot acc: 0.0381
top acc: 0.0480 ::: bot acc: 0.0371
top acc: 0.0333 ::: bot acc: 0.0618
top acc: 0.0269 ::: bot acc: 0.0397
top acc: 0.0160 ::: bot acc: 0.0519
top acc: 0.0181 ::: bot acc: 0.0463
current epoch: 36
train loss is 0.041910
average val loss: 0.035745, accuracy: 0.0352
average test loss: 0.028104, accuracy: 0.0275
case acc: 0.015654622
case acc: 0.03768819
case acc: 0.03591696
case acc: 0.025796518
case acc: 0.025401922
case acc: 0.02439277
top acc: 0.0224 ::: bot acc: 0.0203
top acc: 0.0677 ::: bot acc: 0.0181
top acc: 0.0525 ::: bot acc: 0.0426
top acc: 0.0419 ::: bot acc: 0.0247
top acc: 0.0316 ::: bot acc: 0.0362
top acc: 0.0309 ::: bot acc: 0.0336
current epoch: 37
train loss is 0.042794
average val loss: 0.044889, accuracy: 0.0445
average test loss: 0.032830, accuracy: 0.0321
case acc: 0.022034124
case acc: 0.04477595
case acc: 0.04099307
case acc: 0.029582877
case acc: 0.026796889
case acc: 0.028311282
top acc: 0.0366 ::: bot acc: 0.0120
top acc: 0.0816 ::: bot acc: 0.0115
top acc: 0.0668 ::: bot acc: 0.0298
top acc: 0.0542 ::: bot acc: 0.0129
top acc: 0.0468 ::: bot acc: 0.0210
top acc: 0.0423 ::: bot acc: 0.0236
current epoch: 38
train loss is 0.045784
average val loss: 0.051631, accuracy: 0.0515
average test loss: 0.037404, accuracy: 0.0367
case acc: 0.028391894
case acc: 0.04949952
case acc: 0.045004904
case acc: 0.033815
case acc: 0.030983044
case acc: 0.03241099
top acc: 0.0453 ::: bot acc: 0.0136
top acc: 0.0886 ::: bot acc: 0.0116
top acc: 0.0751 ::: bot acc: 0.0252
top acc: 0.0622 ::: bot acc: 0.0098
top acc: 0.0583 ::: bot acc: 0.0114
top acc: 0.0500 ::: bot acc: 0.0203
current epoch: 39
train loss is 0.047879
average val loss: 0.051137, accuracy: 0.0510
average test loss: 0.036908, accuracy: 0.0363
case acc: 0.02786125
case acc: 0.047027472
case acc: 0.04412866
case acc: 0.03347249
case acc: 0.032869928
case acc: 0.03256022
top acc: 0.0447 ::: bot acc: 0.0133
top acc: 0.0851 ::: bot acc: 0.0113
top acc: 0.0734 ::: bot acc: 0.0261
top acc: 0.0616 ::: bot acc: 0.0100
top acc: 0.0618 ::: bot acc: 0.0102
top acc: 0.0501 ::: bot acc: 0.0204
current epoch: 40
train loss is 0.047504
average val loss: 0.043021, accuracy: 0.0430
average test loss: 0.031252, accuracy: 0.0309
case acc: 0.020531675
case acc: 0.0390038
case acc: 0.038451526
case acc: 0.028735386
case acc: 0.030028084
case acc: 0.028541949
top acc: 0.0341 ::: bot acc: 0.0125
top acc: 0.0707 ::: bot acc: 0.0161
top acc: 0.0609 ::: bot acc: 0.0341
top acc: 0.0520 ::: bot acc: 0.0151
top acc: 0.0564 ::: bot acc: 0.0124
top acc: 0.0425 ::: bot acc: 0.0236
current epoch: 41
train loss is 0.044717
average val loss: 0.033815, accuracy: 0.0339
average test loss: 0.026755, accuracy: 0.0267
case acc: 0.015526524
case acc: 0.03320626
case acc: 0.034490872
case acc: 0.02533349
case acc: 0.026764264
case acc: 0.024754247
top acc: 0.0193 ::: bot acc: 0.0234
top acc: 0.0522 ::: bot acc: 0.0330
top acc: 0.0440 ::: bot acc: 0.0511
top acc: 0.0391 ::: bot acc: 0.0277
top acc: 0.0467 ::: bot acc: 0.0211
top acc: 0.0325 ::: bot acc: 0.0319
current epoch: 42
train loss is 0.041978
average val loss: 0.027132, accuracy: 0.0271
average test loss: 0.028052, accuracy: 0.0282
case acc: 0.023259353
case acc: 0.032622114
case acc: 0.037821174
case acc: 0.02608588
case acc: 0.025494654
case acc: 0.023798428
top acc: 0.0074 ::: bot acc: 0.0431
top acc: 0.0299 ::: bot acc: 0.0554
top acc: 0.0227 ::: bot acc: 0.0727
top acc: 0.0216 ::: bot acc: 0.0451
top acc: 0.0306 ::: bot acc: 0.0372
top acc: 0.0177 ::: bot acc: 0.0468
current epoch: 43
train loss is 0.042560
average val loss: 0.030853, accuracy: 0.0302
average test loss: 0.040576, accuracy: 0.0404
case acc: 0.04440128
case acc: 0.042500827
case acc: 0.0532588
case acc: 0.03705935
case acc: 0.031912126
case acc: 0.03319833
top acc: 0.0239 ::: bot acc: 0.0665
top acc: 0.0116 ::: bot acc: 0.0804
top acc: 0.0191 ::: bot acc: 0.0976
top acc: 0.0114 ::: bot acc: 0.0668
top acc: 0.0104 ::: bot acc: 0.0591
top acc: 0.0080 ::: bot acc: 0.0665
current epoch: 44
train loss is 0.046889
average val loss: 0.042279, accuracy: 0.0415
average test loss: 0.055974, accuracy: 0.0557
case acc: 0.063009724
case acc: 0.057245165
case acc: 0.069664165
case acc: 0.051369984
case acc: 0.045453228
case acc: 0.047284007
top acc: 0.0425 ::: bot acc: 0.0851
top acc: 0.0177 ::: bot acc: 0.0995
top acc: 0.0286 ::: bot acc: 0.1174
top acc: 0.0195 ::: bot acc: 0.0842
top acc: 0.0126 ::: bot acc: 0.0783
top acc: 0.0185 ::: bot acc: 0.0824
current epoch: 45
train loss is 0.052919
average val loss: 0.042759, accuracy: 0.0421
average test loss: 0.056569, accuracy: 0.0563
case acc: 0.06360391
case acc: 0.05631106
case acc: 0.0701752
case acc: 0.05139405
case acc: 0.049061295
case acc: 0.04699731
top acc: 0.0431 ::: bot acc: 0.0857
top acc: 0.0170 ::: bot acc: 0.0984
top acc: 0.0290 ::: bot acc: 0.1180
top acc: 0.0195 ::: bot acc: 0.0843
top acc: 0.0154 ::: bot acc: 0.0823
top acc: 0.0182 ::: bot acc: 0.0820
current epoch: 46
train loss is 0.054604
average val loss: 0.031158, accuracy: 0.0305
average test loss: 0.041071, accuracy: 0.0408
case acc: 0.044830568
case acc: 0.039718807
case acc: 0.05287629
case acc: 0.036405664
case acc: 0.03861533
case acc: 0.032555364
top acc: 0.0243 ::: bot acc: 0.0669
top acc: 0.0134 ::: bot acc: 0.0754
top acc: 0.0189 ::: bot acc: 0.0971
top acc: 0.0111 ::: bot acc: 0.0660
top acc: 0.0087 ::: bot acc: 0.0700
top acc: 0.0077 ::: bot acc: 0.0656
current epoch: 47
train loss is 0.048816
average val loss: 0.027082, accuracy: 0.0266
average test loss: 0.030554, accuracy: 0.0305
case acc: 0.027531555
case acc: 0.0320544
case acc: 0.038960453
case acc: 0.027361035
case acc: 0.031800773
case acc: 0.025392829
top acc: 0.0094 ::: bot acc: 0.0484
top acc: 0.0340 ::: bot acc: 0.0517
top acc: 0.0208 ::: bot acc: 0.0752
top acc: 0.0168 ::: bot acc: 0.0495
top acc: 0.0106 ::: bot acc: 0.0589
top acc: 0.0121 ::: bot acc: 0.0526
current epoch: 48
train loss is 0.042797
average val loss: 0.030140, accuracy: 0.0295
average test loss: 0.027037, accuracy: 0.0268
case acc: 0.01690567
case acc: 0.033729497
case acc: 0.03431141
case acc: 0.024798743
case acc: 0.027558466
case acc: 0.023607524
top acc: 0.0106 ::: bot acc: 0.0318
top acc: 0.0547 ::: bot acc: 0.0309
top acc: 0.0392 ::: bot acc: 0.0559
top acc: 0.0306 ::: bot acc: 0.0355
top acc: 0.0209 ::: bot acc: 0.0471
top acc: 0.0228 ::: bot acc: 0.0419
current epoch: 49
train loss is 0.041078
average val loss: 0.038546, accuracy: 0.0380
average test loss: 0.029279, accuracy: 0.0286
case acc: 0.017153956
case acc: 0.040085245
case acc: 0.03728764
case acc: 0.02644868
case acc: 0.025266374
case acc: 0.02554227
top acc: 0.0275 ::: bot acc: 0.0154
top acc: 0.0731 ::: bot acc: 0.0147
top acc: 0.0572 ::: bot acc: 0.0378
top acc: 0.0450 ::: bot acc: 0.0212
top acc: 0.0367 ::: bot acc: 0.0313
top acc: 0.0352 ::: bot acc: 0.0296
current epoch: 50
train loss is 0.043013
average val loss: 0.047153, accuracy: 0.0468
average test loss: 0.034297, accuracy: 0.0335
case acc: 0.024291124
case acc: 0.04682654
case acc: 0.042365152
case acc: 0.030151216
case acc: 0.027908577
case acc: 0.029747332
top acc: 0.0399 ::: bot acc: 0.0121
top acc: 0.0850 ::: bot acc: 0.0110
top acc: 0.0697 ::: bot acc: 0.0283
top acc: 0.0558 ::: bot acc: 0.0114
top acc: 0.0507 ::: bot acc: 0.0172
top acc: 0.0453 ::: bot acc: 0.0221
LME_Co_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5400 5400 5400
1.7082474 -0.6288155 0.17559324 -0.22413088
Validation: 606 606 606
Testing: 744 744 744
pre-processing time: 0.0004634857177734375
the split date is 2011-01-01
net initializing with time: 0.004003286361694336
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.174861
average val loss: 0.076451, accuracy: 0.0773
average test loss: 0.081493, accuracy: 0.0773
case acc: 0.04507204
case acc: 0.03547679
case acc: 0.045830164
case acc: 0.24472517
case acc: 0.04778341
case acc: 0.045181133
top acc: 0.0860 ::: bot acc: 0.0212
top acc: 0.0114 ::: bot acc: 0.0690
top acc: 0.0731 ::: bot acc: 0.0501
top acc: 0.2746 ::: bot acc: 0.2125
top acc: 0.0223 ::: bot acc: 0.0802
top acc: 0.0251 ::: bot acc: 0.0817
current epoch: 2
train loss is 0.136887
average val loss: 0.100870, accuracy: 0.1001
average test loss: 0.100218, accuracy: 0.0961
case acc: 0.05084215
case acc: 0.09377984
case acc: 0.07987267
case acc: 0.16206375
case acc: 0.09859221
case acc: 0.09161489
top acc: 0.0184 ::: bot acc: 0.0876
top acc: 0.0613 ::: bot acc: 0.1303
top acc: 0.0311 ::: bot acc: 0.1306
top acc: 0.1928 ::: bot acc: 0.1291
top acc: 0.0515 ::: bot acc: 0.1416
top acc: 0.0395 ::: bot acc: 0.1436
current epoch: 3
train loss is 0.125688
average val loss: 0.084770, accuracy: 0.0837
average test loss: 0.085767, accuracy: 0.0820
case acc: 0.042098716
case acc: 0.07073892
case acc: 0.07164596
case acc: 0.17091541
case acc: 0.070558794
case acc: 0.066313505
top acc: 0.0276 ::: bot acc: 0.0700
top acc: 0.0384 ::: bot acc: 0.1065
top acc: 0.0288 ::: bot acc: 0.1200
top acc: 0.2021 ::: bot acc: 0.1374
top acc: 0.0266 ::: bot acc: 0.1119
top acc: 0.0209 ::: bot acc: 0.1146
current epoch: 4
train loss is 0.104190
average val loss: 0.071938, accuracy: 0.0702
average test loss: 0.075234, accuracy: 0.0719
case acc: 0.036812913
case acc: 0.050259605
case acc: 0.06670637
case acc: 0.17587197
case acc: 0.05148952
case acc: 0.05046322
top acc: 0.0389 ::: bot acc: 0.0560
top acc: 0.0197 ::: bot acc: 0.0845
top acc: 0.0296 ::: bot acc: 0.1126
top acc: 0.2075 ::: bot acc: 0.1419
top acc: 0.0190 ::: bot acc: 0.0870
top acc: 0.0210 ::: bot acc: 0.0905
current epoch: 5
train loss is 0.095267
average val loss: 0.064281, accuracy: 0.0624
average test loss: 0.069279, accuracy: 0.0663
case acc: 0.035051588
case acc: 0.03739104
case acc: 0.06586587
case acc: 0.17328914
case acc: 0.042189535
case acc: 0.04396302
top acc: 0.0440 ::: bot acc: 0.0496
top acc: 0.0116 ::: bot acc: 0.0687
top acc: 0.0294 ::: bot acc: 0.1119
top acc: 0.2054 ::: bot acc: 0.1389
top acc: 0.0237 ::: bot acc: 0.0708
top acc: 0.0321 ::: bot acc: 0.0751
current epoch: 6
train loss is 0.090845
average val loss: 0.065136, accuracy: 0.0633
average test loss: 0.069275, accuracy: 0.0661
case acc: 0.037734203
case acc: 0.04092539
case acc: 0.07647004
case acc: 0.15377289
case acc: 0.042710315
case acc: 0.0447634
top acc: 0.0323 ::: bot acc: 0.0610
top acc: 0.0133 ::: bot acc: 0.0727
top acc: 0.0297 ::: bot acc: 0.1278
top acc: 0.1862 ::: bot acc: 0.1191
top acc: 0.0226 ::: bot acc: 0.0720
top acc: 0.0306 ::: bot acc: 0.0768
current epoch: 7
train loss is 0.087781
average val loss: 0.069716, accuracy: 0.0682
average test loss: 0.072172, accuracy: 0.0685
case acc: 0.04429244
case acc: 0.051126353
case acc: 0.09228107
case acc: 0.12984087
case acc: 0.04589891
case acc: 0.04763118
top acc: 0.0199 ::: bot acc: 0.0770
top acc: 0.0205 ::: bot acc: 0.0841
top acc: 0.0365 ::: bot acc: 0.1483
top acc: 0.1625 ::: bot acc: 0.0949
top acc: 0.0197 ::: bot acc: 0.0782
top acc: 0.0255 ::: bot acc: 0.0834
current epoch: 8
train loss is 0.087018
average val loss: 0.080303, accuracy: 0.0792
average test loss: 0.080432, accuracy: 0.0761
case acc: 0.05905283
case acc: 0.07101471
case acc: 0.11672668
case acc: 0.09889286
case acc: 0.055155862
case acc: 0.05567408
top acc: 0.0183 ::: bot acc: 0.0999
top acc: 0.0389 ::: bot acc: 0.1046
top acc: 0.0544 ::: bot acc: 0.1762
top acc: 0.1315 ::: bot acc: 0.0642
top acc: 0.0178 ::: bot acc: 0.0929
top acc: 0.0189 ::: bot acc: 0.0985
current epoch: 9
train loss is 0.092223
average val loss: 0.094165, accuracy: 0.0932
average test loss: 0.092796, accuracy: 0.0879
case acc: 0.08146176
case acc: 0.09314381
case acc: 0.14573908
case acc: 0.065491356
case acc: 0.071009584
case acc: 0.0708254
top acc: 0.0342 ::: bot acc: 0.1255
top acc: 0.0608 ::: bot acc: 0.1267
top acc: 0.0805 ::: bot acc: 0.2068
top acc: 0.0970 ::: bot acc: 0.0332
top acc: 0.0248 ::: bot acc: 0.1133
top acc: 0.0222 ::: bot acc: 0.1194
current epoch: 10
train loss is 0.108370
average val loss: 0.078063, accuracy: 0.0770
average test loss: 0.078235, accuracy: 0.0736
case acc: 0.06351563
case acc: 0.064083025
case acc: 0.1283421
case acc: 0.07573763
case acc: 0.05408929
case acc: 0.05596495
top acc: 0.0205 ::: bot acc: 0.1057
top acc: 0.0319 ::: bot acc: 0.0976
top acc: 0.0645 ::: bot acc: 0.1887
top acc: 0.1079 ::: bot acc: 0.0421
top acc: 0.0167 ::: bot acc: 0.0920
top acc: 0.0189 ::: bot acc: 0.0987
current epoch: 11
train loss is 0.101851
average val loss: 0.058495, accuracy: 0.0576
average test loss: 0.062657, accuracy: 0.0589
case acc: 0.043429777
case acc: 0.029418008
case acc: 0.09999344
case acc: 0.09596706
case acc: 0.04028637
case acc: 0.04413486
top acc: 0.0204 ::: bot acc: 0.0759
top acc: 0.0129 ::: bot acc: 0.0552
top acc: 0.0419 ::: bot acc: 0.1575
top acc: 0.1287 ::: bot acc: 0.0612
top acc: 0.0278 ::: bot acc: 0.0660
top acc: 0.0328 ::: bot acc: 0.0742
current epoch: 12
train loss is 0.082798
average val loss: 0.053930, accuracy: 0.0536
average test loss: 0.058874, accuracy: 0.0557
case acc: 0.04005984
case acc: 0.024422381
case acc: 0.09143552
case acc: 0.094949305
case acc: 0.03935575
case acc: 0.043774102
top acc: 0.0250 ::: bot acc: 0.0687
top acc: 0.0302 ::: bot acc: 0.0364
top acc: 0.0359 ::: bot acc: 0.1476
top acc: 0.1277 ::: bot acc: 0.0602
top acc: 0.0302 ::: bot acc: 0.0634
top acc: 0.0340 ::: bot acc: 0.0730
current epoch: 13
train loss is 0.070473
average val loss: 0.052123, accuracy: 0.0520
average test loss: 0.057119, accuracy: 0.0542
case acc: 0.038994413
case acc: 0.024797602
case acc: 0.08722994
case acc: 0.09005715
case acc: 0.03963259
case acc: 0.044311166
top acc: 0.0269 ::: bot acc: 0.0661
top acc: 0.0390 ::: bot acc: 0.0276
top acc: 0.0333 ::: bot acc: 0.1427
top acc: 0.1228 ::: bot acc: 0.0555
top acc: 0.0295 ::: bot acc: 0.0643
top acc: 0.0327 ::: bot acc: 0.0744
current epoch: 14
train loss is 0.066678
average val loss: 0.049086, accuracy: 0.0487
average test loss: 0.054651, accuracy: 0.0524
case acc: 0.036571447
case acc: 0.026728416
case acc: 0.080803044
case acc: 0.089463316
case acc: 0.038143102
case acc: 0.04283696
top acc: 0.0321 ::: bot acc: 0.0600
top acc: 0.0456 ::: bot acc: 0.0213
top acc: 0.0304 ::: bot acc: 0.1346
top acc: 0.1222 ::: bot acc: 0.0548
top acc: 0.0345 ::: bot acc: 0.0594
top acc: 0.0380 ::: bot acc: 0.0693
current epoch: 15
train loss is 0.063948
average val loss: 0.046911, accuracy: 0.0464
average test loss: 0.052819, accuracy: 0.0507
case acc: 0.035883673
case acc: 0.025930565
case acc: 0.07795309
case acc: 0.08551002
case acc: 0.037344914
case acc: 0.041835703
top acc: 0.0340 ::: bot acc: 0.0580
top acc: 0.0436 ::: bot acc: 0.0231
top acc: 0.0294 ::: bot acc: 0.1308
top acc: 0.1183 ::: bot acc: 0.0510
top acc: 0.0374 ::: bot acc: 0.0564
top acc: 0.0418 ::: bot acc: 0.0654
current epoch: 16
train loss is 0.063052
average val loss: 0.046294, accuracy: 0.0458
average test loss: 0.052093, accuracy: 0.0496
case acc: 0.037318736
case acc: 0.024378752
case acc: 0.08042691
case acc: 0.07564609
case acc: 0.037630808
case acc: 0.041931015
top acc: 0.0298 ::: bot acc: 0.0622
top acc: 0.0332 ::: bot acc: 0.0337
top acc: 0.0301 ::: bot acc: 0.1342
top acc: 0.1081 ::: bot acc: 0.0417
top acc: 0.0361 ::: bot acc: 0.0577
top acc: 0.0415 ::: bot acc: 0.0659
current epoch: 17
train loss is 0.062927
average val loss: 0.050569, accuracy: 0.0497
average test loss: 0.054929, accuracy: 0.0511
case acc: 0.04355975
case acc: 0.029386086
case acc: 0.09175771
case acc: 0.056597542
case acc: 0.040898897
case acc: 0.044651005
top acc: 0.0197 ::: bot acc: 0.0766
top acc: 0.0134 ::: bot acc: 0.0549
top acc: 0.0357 ::: bot acc: 0.1483
top acc: 0.0879 ::: bot acc: 0.0251
top acc: 0.0260 ::: bot acc: 0.0680
top acc: 0.0322 ::: bot acc: 0.0752
current epoch: 18
train loss is 0.064605
average val loss: 0.063035, accuracy: 0.0615
average test loss: 0.064514, accuracy: 0.0599
case acc: 0.05884961
case acc: 0.05017574
case acc: 0.112985045
case acc: 0.033158045
case acc: 0.05079248
case acc: 0.053371523
top acc: 0.0185 ::: bot acc: 0.1001
top acc: 0.0191 ::: bot acc: 0.0832
top acc: 0.0512 ::: bot acc: 0.1724
top acc: 0.0581 ::: bot acc: 0.0143
top acc: 0.0165 ::: bot acc: 0.0874
top acc: 0.0200 ::: bot acc: 0.0943
current epoch: 19
train loss is 0.073233
average val loss: 0.067761, accuracy: 0.0662
average test loss: 0.068588, accuracy: 0.0641
case acc: 0.065780826
case acc: 0.05788772
case acc: 0.12184956
case acc: 0.02662673
case acc: 0.055018492
case acc: 0.05759896
top acc: 0.0222 ::: bot acc: 0.1086
top acc: 0.0257 ::: bot acc: 0.0915
top acc: 0.0585 ::: bot acc: 0.1819
top acc: 0.0425 ::: bot acc: 0.0259
top acc: 0.0164 ::: bot acc: 0.0938
top acc: 0.0191 ::: bot acc: 0.1010
current epoch: 20
train loss is 0.081422
average val loss: 0.051782, accuracy: 0.0504
average test loss: 0.055417, accuracy: 0.0512
case acc: 0.0492468
case acc: 0.034122072
case acc: 0.10181571
case acc: 0.032661114
case acc: 0.042725313
case acc: 0.046831027
top acc: 0.0165 ::: bot acc: 0.0869
top acc: 0.0116 ::: bot acc: 0.0630
top acc: 0.0429 ::: bot acc: 0.1597
top acc: 0.0571 ::: bot acc: 0.0147
top acc: 0.0219 ::: bot acc: 0.0727
top acc: 0.0273 ::: bot acc: 0.0808
current epoch: 21
train loss is 0.071298
average val loss: 0.042553, accuracy: 0.0419
average test loss: 0.048147, accuracy: 0.0451
case acc: 0.040579956
case acc: 0.024477122
case acc: 0.08623392
case acc: 0.038604125
case acc: 0.038080707
case acc: 0.04278271
top acc: 0.0239 ::: bot acc: 0.0703
top acc: 0.0311 ::: bot acc: 0.0360
top acc: 0.0326 ::: bot acc: 0.1415
top acc: 0.0666 ::: bot acc: 0.0134
top acc: 0.0341 ::: bot acc: 0.0595
top acc: 0.0377 ::: bot acc: 0.0693
current epoch: 22
train loss is 0.058844
average val loss: 0.040824, accuracy: 0.0403
average test loss: 0.046598, accuracy: 0.0440
case acc: 0.039014403
case acc: 0.025555022
case acc: 0.081771925
case acc: 0.036817502
case acc: 0.037941948
case acc: 0.04296806
top acc: 0.0267 ::: bot acc: 0.0665
top acc: 0.0418 ::: bot acc: 0.0253
top acc: 0.0306 ::: bot acc: 0.1358
top acc: 0.0641 ::: bot acc: 0.0130
top acc: 0.0345 ::: bot acc: 0.0591
top acc: 0.0370 ::: bot acc: 0.0700
current epoch: 23
train loss is 0.053893
average val loss: 0.038731, accuracy: 0.0380
average test loss: 0.044900, accuracy: 0.0429
case acc: 0.037110325
case acc: 0.02749152
case acc: 0.07665087
case acc: 0.03646151
case acc: 0.037181146
case acc: 0.0422674
top acc: 0.0308 ::: bot acc: 0.0617
top acc: 0.0472 ::: bot acc: 0.0206
top acc: 0.0289 ::: bot acc: 0.1290
top acc: 0.0636 ::: bot acc: 0.0130
top acc: 0.0374 ::: bot acc: 0.0562
top acc: 0.0399 ::: bot acc: 0.0672
current epoch: 24
train loss is 0.051445
average val loss: 0.037433, accuracy: 0.0366
average test loss: 0.043821, accuracy: 0.0418
case acc: 0.03679126
case acc: 0.02639288
case acc: 0.074779056
case acc: 0.034077615
case acc: 0.036958
case acc: 0.041904
top acc: 0.0317 ::: bot acc: 0.0608
top acc: 0.0443 ::: bot acc: 0.0230
top acc: 0.0284 ::: bot acc: 0.1264
top acc: 0.0597 ::: bot acc: 0.0138
top acc: 0.0383 ::: bot acc: 0.0552
top acc: 0.0414 ::: bot acc: 0.0658
current epoch: 25
train loss is 0.050525
average val loss: 0.037632, accuracy: 0.0368
average test loss: 0.043925, accuracy: 0.0415
case acc: 0.038238216
case acc: 0.024422392
case acc: 0.07677706
case acc: 0.02977896
case acc: 0.037485342
case acc: 0.042333134
top acc: 0.0282 ::: bot acc: 0.0646
top acc: 0.0342 ::: bot acc: 0.0330
top acc: 0.0289 ::: bot acc: 0.1291
top acc: 0.0512 ::: bot acc: 0.0180
top acc: 0.0358 ::: bot acc: 0.0577
top acc: 0.0397 ::: bot acc: 0.0675
current epoch: 26
train loss is 0.050546
average val loss: 0.042095, accuracy: 0.0412
average test loss: 0.047376, accuracy: 0.0442
case acc: 0.043136455
case acc: 0.027231274
case acc: 0.08479989
case acc: 0.025050763
case acc: 0.040301796
case acc: 0.04479389
top acc: 0.0205 ::: bot acc: 0.0757
top acc: 0.0172 ::: bot acc: 0.0500
top acc: 0.0317 ::: bot acc: 0.1398
top acc: 0.0353 ::: bot acc: 0.0331
top acc: 0.0271 ::: bot acc: 0.0665
top acc: 0.0316 ::: bot acc: 0.0757
current epoch: 27
train loss is 0.052487
average val loss: 0.050162, accuracy: 0.0495
average test loss: 0.053795, accuracy: 0.0507
case acc: 0.050391115
case acc: 0.036621626
case acc: 0.095759965
case acc: 0.027501417
case acc: 0.044711336
case acc: 0.049148433
top acc: 0.0165 ::: bot acc: 0.0886
top acc: 0.0118 ::: bot acc: 0.0667
top acc: 0.0384 ::: bot acc: 0.1528
top acc: 0.0179 ::: bot acc: 0.0512
top acc: 0.0195 ::: bot acc: 0.0769
top acc: 0.0240 ::: bot acc: 0.0859
current epoch: 28
train loss is 0.057206
average val loss: 0.053795, accuracy: 0.0532
average test loss: 0.056863, accuracy: 0.0543
case acc: 0.053831883
case acc: 0.04010159
case acc: 0.10047023
case acc: 0.034320068
case acc: 0.046399098
case acc: 0.05088598
top acc: 0.0168 ::: bot acc: 0.0936
top acc: 0.0128 ::: bot acc: 0.0714
top acc: 0.0418 ::: bot acc: 0.1582
top acc: 0.0169 ::: bot acc: 0.0619
top acc: 0.0181 ::: bot acc: 0.0801
top acc: 0.0217 ::: bot acc: 0.0896
current epoch: 29
train loss is 0.061356
average val loss: 0.046141, accuracy: 0.0458
average test loss: 0.050734, accuracy: 0.0483
case acc: 0.047438353
case acc: 0.030330557
case acc: 0.09180368
case acc: 0.031504724
case acc: 0.041740622
case acc: 0.046892997
top acc: 0.0170 ::: bot acc: 0.0839
top acc: 0.0127 ::: bot acc: 0.0569
top acc: 0.0358 ::: bot acc: 0.1482
top acc: 0.0162 ::: bot acc: 0.0581
top acc: 0.0234 ::: bot acc: 0.0705
top acc: 0.0270 ::: bot acc: 0.0810
current epoch: 30
train loss is 0.058302
average val loss: 0.037367, accuracy: 0.0373
average test loss: 0.043668, accuracy: 0.0419
case acc: 0.040328477
case acc: 0.024440425
case acc: 0.0794063
case acc: 0.02659469
case acc: 0.03764104
case acc: 0.042988267
top acc: 0.0244 ::: bot acc: 0.0695
top acc: 0.0321 ::: bot acc: 0.0350
top acc: 0.0298 ::: bot acc: 0.1326
top acc: 0.0193 ::: bot acc: 0.0491
top acc: 0.0348 ::: bot acc: 0.0585
top acc: 0.0364 ::: bot acc: 0.0703
current epoch: 31
train loss is 0.050418
average val loss: 0.033487, accuracy: 0.0331
average test loss: 0.040486, accuracy: 0.0396
case acc: 0.03633943
case acc: 0.027995657
case acc: 0.07075161
case acc: 0.024881987
case acc: 0.036265213
case acc: 0.041514874
top acc: 0.0332 ::: bot acc: 0.0592
top acc: 0.0481 ::: bot acc: 0.0199
top acc: 0.0279 ::: bot acc: 0.1206
top acc: 0.0250 ::: bot acc: 0.0434
top acc: 0.0416 ::: bot acc: 0.0518
top acc: 0.0422 ::: bot acc: 0.0646
current epoch: 32
train loss is 0.044955
average val loss: 0.031317, accuracy: 0.0305
average test loss: 0.038732, accuracy: 0.0386
case acc: 0.03427439
case acc: 0.033301666
case acc: 0.06383201
case acc: 0.024573274
case acc: 0.03541371
case acc: 0.040311284
top acc: 0.0416 ::: bot acc: 0.0506
top acc: 0.0573 ::: bot acc: 0.0174
top acc: 0.0286 ::: bot acc: 0.1100
top acc: 0.0303 ::: bot acc: 0.0380
top acc: 0.0471 ::: bot acc: 0.0464
top acc: 0.0475 ::: bot acc: 0.0592
current epoch: 33
train loss is 0.042671
average val loss: 0.029949, accuracy: 0.0290
average test loss: 0.037576, accuracy: 0.0378
case acc: 0.033691674
case acc: 0.034640297
case acc: 0.059116233
case acc: 0.024929859
case acc: 0.03515204
case acc: 0.039563295
top acc: 0.0477 ::: bot acc: 0.0443
top acc: 0.0593 ::: bot acc: 0.0174
top acc: 0.0315 ::: bot acc: 0.1015
top acc: 0.0346 ::: bot acc: 0.0338
top acc: 0.0512 ::: bot acc: 0.0423
top acc: 0.0525 ::: bot acc: 0.0543
current epoch: 34
train loss is 0.041761
average val loss: 0.028989, accuracy: 0.0281
average test loss: 0.036748, accuracy: 0.0370
case acc: 0.033472285
case acc: 0.032950256
case acc: 0.05608066
case acc: 0.025285382
case acc: 0.035144895
case acc: 0.039233286
top acc: 0.0514 ::: bot acc: 0.0405
top acc: 0.0569 ::: bot acc: 0.0173
top acc: 0.0344 ::: bot acc: 0.0956
top acc: 0.0370 ::: bot acc: 0.0314
top acc: 0.0539 ::: bot acc: 0.0396
top acc: 0.0565 ::: bot acc: 0.0504
current epoch: 35
train loss is 0.041142
average val loss: 0.028329, accuracy: 0.0276
average test loss: 0.036254, accuracy: 0.0361
case acc: 0.03342796
case acc: 0.02839906
case acc: 0.055736843
case acc: 0.024914477
case acc: 0.035154365
case acc: 0.03922453
top acc: 0.0500 ::: bot acc: 0.0417
top acc: 0.0492 ::: bot acc: 0.0193
top acc: 0.0348 ::: bot acc: 0.0949
top acc: 0.0346 ::: bot acc: 0.0338
top acc: 0.0528 ::: bot acc: 0.0407
top acc: 0.0568 ::: bot acc: 0.0501
current epoch: 36
train loss is 0.040693
average val loss: 0.029017, accuracy: 0.0285
average test loss: 0.037051, accuracy: 0.0363
case acc: 0.033923063
case acc: 0.024496377
case acc: 0.058989342
case acc: 0.024915064
case acc: 0.03558422
case acc: 0.039675593
top acc: 0.0421 ::: bot acc: 0.0495
top acc: 0.0364 ::: bot acc: 0.0309
top acc: 0.0318 ::: bot acc: 0.1012
top acc: 0.0253 ::: bot acc: 0.0432
top acc: 0.0463 ::: bot acc: 0.0473
top acc: 0.0515 ::: bot acc: 0.0555
current epoch: 37
train loss is 0.041018
average val loss: 0.032503, accuracy: 0.0321
average test loss: 0.039988, accuracy: 0.0391
case acc: 0.03634479
case acc: 0.025731193
case acc: 0.06425766
case acc: 0.030211702
case acc: 0.036995217
case acc: 0.041123938
top acc: 0.0320 ::: bot acc: 0.0597
top acc: 0.0232 ::: bot acc: 0.0442
top acc: 0.0284 ::: bot acc: 0.1108
top acc: 0.0163 ::: bot acc: 0.0559
top acc: 0.0382 ::: bot acc: 0.0554
top acc: 0.0440 ::: bot acc: 0.0629
current epoch: 38
train loss is 0.042176
average val loss: 0.039295, accuracy: 0.0389
average test loss: 0.045192, accuracy: 0.0446
case acc: 0.0409955
case acc: 0.03050781
case acc: 0.07181417
case acc: 0.04136022
case acc: 0.039448075
case acc: 0.04340022
top acc: 0.0228 ::: bot acc: 0.0712
top acc: 0.0127 ::: bot acc: 0.0571
top acc: 0.0278 ::: bot acc: 0.1224
top acc: 0.0199 ::: bot acc: 0.0708
top acc: 0.0295 ::: bot acc: 0.0641
top acc: 0.0353 ::: bot acc: 0.0716
current epoch: 39
train loss is 0.044644
average val loss: 0.048425, accuracy: 0.0480
average test loss: 0.052122, accuracy: 0.0518
case acc: 0.047008168
case acc: 0.03787105
case acc: 0.08102823
case acc: 0.055265542
case acc: 0.042753525
case acc: 0.04694822
top acc: 0.0171 ::: bot acc: 0.0831
top acc: 0.0120 ::: bot acc: 0.0684
top acc: 0.0300 ::: bot acc: 0.1351
top acc: 0.0293 ::: bot acc: 0.0869
top acc: 0.0214 ::: bot acc: 0.0730
top acc: 0.0270 ::: bot acc: 0.0811
current epoch: 40
train loss is 0.048747
average val loss: 0.051707, accuracy: 0.0515
average test loss: 0.054682, accuracy: 0.0546
case acc: 0.049370624
case acc: 0.03814003
case acc: 0.084792376
case acc: 0.063347526
case acc: 0.04351642
case acc: 0.048230484
top acc: 0.0165 ::: bot acc: 0.0869
top acc: 0.0121 ::: bot acc: 0.0688
top acc: 0.0314 ::: bot acc: 0.1400
top acc: 0.0359 ::: bot acc: 0.0957
top acc: 0.0205 ::: bot acc: 0.0746
top acc: 0.0251 ::: bot acc: 0.0839
current epoch: 41
train loss is 0.051745
average val loss: 0.045004, accuracy: 0.0451
average test loss: 0.049548, accuracy: 0.0497
case acc: 0.044625904
case acc: 0.029163541
case acc: 0.07884947
case acc: 0.06025404
case acc: 0.040120475
case acc: 0.045168594
top acc: 0.0188 ::: bot acc: 0.0787
top acc: 0.0138 ::: bot acc: 0.0546
top acc: 0.0294 ::: bot acc: 0.1320
top acc: 0.0334 ::: bot acc: 0.0924
top acc: 0.0275 ::: bot acc: 0.0661
top acc: 0.0300 ::: bot acc: 0.0769
current epoch: 42
train loss is 0.051752
average val loss: 0.033904, accuracy: 0.0342
average test loss: 0.040955, accuracy: 0.0421
case acc: 0.03676426
case acc: 0.024761405
case acc: 0.06585473
case acc: 0.047768213
case acc: 0.036141902
case acc: 0.04113658
top acc: 0.0316 ::: bot acc: 0.0606
top acc: 0.0381 ::: bot acc: 0.0293
top acc: 0.0282 ::: bot acc: 0.1132
top acc: 0.0239 ::: bot acc: 0.0784
top acc: 0.0427 ::: bot acc: 0.0508
top acc: 0.0435 ::: bot acc: 0.0632
current epoch: 43
train loss is 0.047540
average val loss: 0.028948, accuracy: 0.0287
average test loss: 0.036940, accuracy: 0.0390
case acc: 0.033617537
case acc: 0.03634405
case acc: 0.054876518
case acc: 0.03485677
case acc: 0.035146587
case acc: 0.03922358
top acc: 0.0495 ::: bot acc: 0.0426
top acc: 0.0619 ::: bot acc: 0.0176
top acc: 0.0357 ::: bot acc: 0.0931
top acc: 0.0170 ::: bot acc: 0.0625
top acc: 0.0551 ::: bot acc: 0.0384
top acc: 0.0550 ::: bot acc: 0.0517
current epoch: 44
train loss is 0.043811
average val loss: 0.031279, accuracy: 0.0313
average test loss: 0.038527, accuracy: 0.0402
case acc: 0.03813611
case acc: 0.055251956
case acc: 0.046622787
case acc: 0.024479922
case acc: 0.036782317
case acc: 0.03985485
top acc: 0.0721 ::: bot acc: 0.0204
top acc: 0.0859 ::: bot acc: 0.0262
top acc: 0.0593 ::: bot acc: 0.0671
top acc: 0.0293 ::: bot acc: 0.0390
top acc: 0.0703 ::: bot acc: 0.0232
top acc: 0.0705 ::: bot acc: 0.0362
current epoch: 45
train loss is 0.045162
average val loss: 0.045604, accuracy: 0.0464
average test loss: 0.051182, accuracy: 0.0517
case acc: 0.05575677
case acc: 0.0775059
case acc: 0.049411967
case acc: 0.03464707
case acc: 0.045850873
case acc: 0.047315996
top acc: 0.0995 ::: bot acc: 0.0181
top acc: 0.1103 ::: bot acc: 0.0442
top acc: 0.0916 ::: bot acc: 0.0349
top acc: 0.0609 ::: bot acc: 0.0135
top acc: 0.0898 ::: bot acc: 0.0127
top acc: 0.0918 ::: bot acc: 0.0175
current epoch: 46
train loss is 0.050625
average val loss: 0.060161, accuracy: 0.0605
average test loss: 0.064758, accuracy: 0.0648
case acc: 0.07227797
case acc: 0.08795524
case acc: 0.06093154
case acc: 0.055271436
case acc: 0.05522648
case acc: 0.056955144
top acc: 0.1186 ::: bot acc: 0.0293
top acc: 0.1210 ::: bot acc: 0.0542
top acc: 0.1158 ::: bot acc: 0.0212
top acc: 0.0864 ::: bot acc: 0.0243
top acc: 0.1023 ::: bot acc: 0.0159
top acc: 0.1072 ::: bot acc: 0.0155
current epoch: 47
train loss is 0.056793
average val loss: 0.044732, accuracy: 0.0449
average test loss: 0.050325, accuracy: 0.0500
case acc: 0.05700859
case acc: 0.05816435
case acc: 0.052879494
case acc: 0.045004208
case acc: 0.040784758
case acc: 0.04635954
top acc: 0.1010 ::: bot acc: 0.0188
top acc: 0.0893 ::: bot acc: 0.0281
top acc: 0.1011 ::: bot acc: 0.0265
top acc: 0.0749 ::: bot acc: 0.0165
top acc: 0.0816 ::: bot acc: 0.0141
top acc: 0.0899 ::: bot acc: 0.0185
current epoch: 48
train loss is 0.053296
average val loss: 0.029617, accuracy: 0.0297
average test loss: 0.037032, accuracy: 0.0362
case acc: 0.038647164
case acc: 0.027960042
case acc: 0.04613262
case acc: 0.029651787
case acc: 0.035292596
case acc: 0.039461486
top acc: 0.0733 ::: bot acc: 0.0193
top acc: 0.0483 ::: bot acc: 0.0200
top acc: 0.0742 ::: bot acc: 0.0525
top acc: 0.0513 ::: bot acc: 0.0179
top acc: 0.0567 ::: bot acc: 0.0369
top acc: 0.0680 ::: bot acc: 0.0388
current epoch: 49
train loss is 0.045413
average val loss: 0.027524, accuracy: 0.0267
average test loss: 0.035775, accuracy: 0.0346
case acc: 0.033529412
case acc: 0.026348818
case acc: 0.047810808
case acc: 0.024677929
case acc: 0.03595885
case acc: 0.039139368
top acc: 0.0536 ::: bot acc: 0.0380
top acc: 0.0211 ::: bot acc: 0.0465
top acc: 0.0535 ::: bot acc: 0.0731
top acc: 0.0322 ::: bot acc: 0.0363
top acc: 0.0443 ::: bot acc: 0.0494
top acc: 0.0573 ::: bot acc: 0.0496
current epoch: 50
train loss is 0.042114
average val loss: 0.035014, accuracy: 0.0339
average test loss: 0.041929, accuracy: 0.0415
case acc: 0.036854587
case acc: 0.038585622
case acc: 0.057748824
case acc: 0.035217375
case acc: 0.039236005
case acc: 0.04131033
top acc: 0.0304 ::: bot acc: 0.0613
top acc: 0.0123 ::: bot acc: 0.0694
top acc: 0.0330 ::: bot acc: 0.0989
top acc: 0.0173 ::: bot acc: 0.0629
top acc: 0.0302 ::: bot acc: 0.0635
top acc: 0.0431 ::: bot acc: 0.0637
LME_Co_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5376 5376 5376
1.7082474 -0.6288155 0.17559324 -0.22413088
Validation: 600 600 600
Testing: 774 774 774
pre-processing time: 0.000606536865234375
the split date is 2011-07-01
net initializing with time: 0.00437617301940918
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.274154
average val loss: 0.192574, accuracy: 0.1940
average test loss: 0.195168, accuracy: 0.1933
case acc: 0.30230916
case acc: 0.2466287
case acc: 0.2824972
case acc: 0.071480975
case acc: 0.060357854
case acc: 0.19675227
top acc: 0.2578 ::: bot acc: 0.3465
top acc: 0.2066 ::: bot acc: 0.2841
top acc: 0.2321 ::: bot acc: 0.3306
top acc: 0.0313 ::: bot acc: 0.1179
top acc: 0.1146 ::: bot acc: 0.0159
top acc: 0.1418 ::: bot acc: 0.2476
current epoch: 2
train loss is 0.221166
average val loss: 0.134367, accuracy: 0.1316
average test loss: 0.136126, accuracy: 0.1367
case acc: 0.20037907
case acc: 0.14692998
case acc: 0.18243514
case acc: 0.050799254
case acc: 0.14602433
case acc: 0.093804516
top acc: 0.1557 ::: bot acc: 0.2446
top acc: 0.1072 ::: bot acc: 0.1841
top acc: 0.1319 ::: bot acc: 0.2304
top acc: 0.0918 ::: bot acc: 0.0262
top acc: 0.2027 ::: bot acc: 0.0970
top acc: 0.0412 ::: bot acc: 0.1437
current epoch: 3
train loss is 0.154446
average val loss: 0.122240, accuracy: 0.1195
average test loss: 0.123746, accuracy: 0.1246
case acc: 0.17768373
case acc: 0.12696776
case acc: 0.15996218
case acc: 0.057161976
case acc: 0.14935288
case acc: 0.0764659
top acc: 0.1329 ::: bot acc: 0.2220
top acc: 0.0875 ::: bot acc: 0.1640
top acc: 0.1094 ::: bot acc: 0.2079
top acc: 0.1033 ::: bot acc: 0.0222
top acc: 0.2063 ::: bot acc: 0.1004
top acc: 0.0274 ::: bot acc: 0.1247
current epoch: 4
train loss is 0.152184
average val loss: 0.112789, accuracy: 0.1103
average test loss: 0.114144, accuracy: 0.1151
case acc: 0.16149132
case acc: 0.11403238
case acc: 0.14431673
case acc: 0.059237562
case acc: 0.14549702
case acc: 0.06582772
top acc: 0.1166 ::: bot acc: 0.2059
top acc: 0.0747 ::: bot acc: 0.1509
top acc: 0.0937 ::: bot acc: 0.1922
top acc: 0.1069 ::: bot acc: 0.0211
top acc: 0.2026 ::: bot acc: 0.0967
top acc: 0.0213 ::: bot acc: 0.1118
current epoch: 5
train loss is 0.135172
average val loss: 0.106726, accuracy: 0.1044
average test loss: 0.108152, accuracy: 0.1090
case acc: 0.15467705
case acc: 0.11014082
case acc: 0.13710794
case acc: 0.05519128
case acc: 0.13281043
case acc: 0.06421484
top acc: 0.1097 ::: bot acc: 0.1992
top acc: 0.0710 ::: bot acc: 0.1469
top acc: 0.0865 ::: bot acc: 0.1850
top acc: 0.1003 ::: bot acc: 0.0221
top acc: 0.1900 ::: bot acc: 0.0841
top acc: 0.0206 ::: bot acc: 0.1098
current epoch: 6
train loss is 0.131039
average val loss: 0.101180, accuracy: 0.0991
average test loss: 0.102708, accuracy: 0.1034
case acc: 0.14864527
case acc: 0.1072483
case acc: 0.13094224
case acc: 0.051089406
case acc: 0.11969422
case acc: 0.0630185
top acc: 0.1035 ::: bot acc: 0.1932
top acc: 0.0681 ::: bot acc: 0.1440
top acc: 0.0803 ::: bot acc: 0.1789
top acc: 0.0931 ::: bot acc: 0.0242
top acc: 0.1770 ::: bot acc: 0.0711
top acc: 0.0201 ::: bot acc: 0.1082
current epoch: 7
train loss is 0.123331
average val loss: 0.093745, accuracy: 0.0920
average test loss: 0.095244, accuracy: 0.0959
case acc: 0.137023
case acc: 0.09865987
case acc: 0.11888397
case acc: 0.050201084
case acc: 0.11268398
case acc: 0.05822745
top acc: 0.0919 ::: bot acc: 0.1815
top acc: 0.0596 ::: bot acc: 0.1354
top acc: 0.0683 ::: bot acc: 0.1669
top acc: 0.0915 ::: bot acc: 0.0245
top acc: 0.1700 ::: bot acc: 0.0642
top acc: 0.0195 ::: bot acc: 0.1014
current epoch: 8
train loss is 0.113072
average val loss: 0.091716, accuracy: 0.0902
average test loss: 0.093526, accuracy: 0.0935
case acc: 0.1390349
case acc: 0.10376556
case acc: 0.120456085
case acc: 0.042793095
case acc: 0.09234121
case acc: 0.06290047
top acc: 0.0939 ::: bot acc: 0.1834
top acc: 0.0647 ::: bot acc: 0.1405
top acc: 0.0698 ::: bot acc: 0.1686
top acc: 0.0765 ::: bot acc: 0.0324
top acc: 0.1497 ::: bot acc: 0.0439
top acc: 0.0199 ::: bot acc: 0.1082
current epoch: 9
train loss is 0.110861
average val loss: 0.084743, accuracy: 0.0835
average test loss: 0.086503, accuracy: 0.0865
case acc: 0.12782176
case acc: 0.095817804
case acc: 0.1093738
case acc: 0.042137492
case acc: 0.08547059
case acc: 0.0581871
top acc: 0.0826 ::: bot acc: 0.1722
top acc: 0.0568 ::: bot acc: 0.1325
top acc: 0.0591 ::: bot acc: 0.1573
top acc: 0.0749 ::: bot acc: 0.0335
top acc: 0.1428 ::: bot acc: 0.0372
top acc: 0.0193 ::: bot acc: 0.1015
current epoch: 10
train loss is 0.102173
average val loss: 0.083156, accuracy: 0.0822
average test loss: 0.085038, accuracy: 0.0845
case acc: 0.1284934
case acc: 0.09990598
case acc: 0.110205844
case acc: 0.03873497
case acc: 0.06789085
case acc: 0.061570797
top acc: 0.0832 ::: bot acc: 0.1729
top acc: 0.0608 ::: bot acc: 0.1366
top acc: 0.0599 ::: bot acc: 0.1582
top acc: 0.0616 ::: bot acc: 0.0463
top acc: 0.1242 ::: bot acc: 0.0218
top acc: 0.0194 ::: bot acc: 0.1065
current epoch: 11
train loss is 0.098795
average val loss: 0.076941, accuracy: 0.0762
average test loss: 0.078792, accuracy: 0.0782
case acc: 0.11814235
case acc: 0.09318635
case acc: 0.10059217
case acc: 0.038574744
case acc: 0.06123294
case acc: 0.05739356
top acc: 0.0728 ::: bot acc: 0.1626
top acc: 0.0542 ::: bot acc: 0.1298
top acc: 0.0508 ::: bot acc: 0.1483
top acc: 0.0597 ::: bot acc: 0.0482
top acc: 0.1168 ::: bot acc: 0.0168
top acc: 0.0192 ::: bot acc: 0.1003
current epoch: 12
train loss is 0.092927
average val loss: 0.073402, accuracy: 0.0731
average test loss: 0.075331, accuracy: 0.0746
case acc: 0.112909704
case acc: 0.0916303
case acc: 0.0959804
case acc: 0.038596947
case acc: 0.05159918
case acc: 0.056797422
top acc: 0.0675 ::: bot acc: 0.1573
top acc: 0.0526 ::: bot acc: 0.1283
top acc: 0.0466 ::: bot acc: 0.1435
top acc: 0.0527 ::: bot acc: 0.0551
top acc: 0.1045 ::: bot acc: 0.0127
top acc: 0.0192 ::: bot acc: 0.0994
current epoch: 13
train loss is 0.085328
average val loss: 0.072080, accuracy: 0.0722
average test loss: 0.074247, accuracy: 0.0736
case acc: 0.111140124
case acc: 0.093671545
case acc: 0.095148414
case acc: 0.040690962
case acc: 0.042670414
case acc: 0.05833211
top acc: 0.0657 ::: bot acc: 0.1556
top acc: 0.0548 ::: bot acc: 0.1303
top acc: 0.0458 ::: bot acc: 0.1426
top acc: 0.0426 ::: bot acc: 0.0652
top acc: 0.0888 ::: bot acc: 0.0172
top acc: 0.0191 ::: bot acc: 0.1018
current epoch: 14
train loss is 0.083031
average val loss: 0.069478, accuracy: 0.0699
average test loss: 0.071854, accuracy: 0.0716
case acc: 0.10576873
case acc: 0.092331834
case acc: 0.09157837
case acc: 0.042783715
case acc: 0.040053546
case acc: 0.057302296
top acc: 0.0603 ::: bot acc: 0.1502
top acc: 0.0534 ::: bot acc: 0.1289
top acc: 0.0426 ::: bot acc: 0.1389
top acc: 0.0365 ::: bot acc: 0.0714
top acc: 0.0768 ::: bot acc: 0.0290
top acc: 0.0192 ::: bot acc: 0.1003
current epoch: 15
train loss is 0.078615
average val loss: 0.064916, accuracy: 0.0656
average test loss: 0.067386, accuracy: 0.0675
case acc: 0.09646293
case acc: 0.08693357
case acc: 0.084152974
case acc: 0.043613166
case acc: 0.039600056
case acc: 0.053957622
top acc: 0.0513 ::: bot acc: 0.1407
top acc: 0.0480 ::: bot acc: 0.1235
top acc: 0.0359 ::: bot acc: 0.1310
top acc: 0.0349 ::: bot acc: 0.0734
top acc: 0.0691 ::: bot acc: 0.0367
top acc: 0.0201 ::: bot acc: 0.0948
current epoch: 16
train loss is 0.071725
average val loss: 0.064441, accuracy: 0.0658
average test loss: 0.067196, accuracy: 0.0677
case acc: 0.093334906
case acc: 0.08770102
case acc: 0.08271583
case acc: 0.047260605
case acc: 0.040544234
case acc: 0.054418318
top acc: 0.0483 ::: bot acc: 0.1375
top acc: 0.0488 ::: bot acc: 0.1243
top acc: 0.0346 ::: bot acc: 0.1295
top acc: 0.0301 ::: bot acc: 0.0813
top acc: 0.0556 ::: bot acc: 0.0502
top acc: 0.0198 ::: bot acc: 0.0956
current epoch: 17
train loss is 0.068645
average val loss: 0.060981, accuracy: 0.0627
average test loss: 0.063721, accuracy: 0.0643
case acc: 0.08485232
case acc: 0.082724676
case acc: 0.07641051
case acc: 0.048270676
case acc: 0.04194707
case acc: 0.051747378
top acc: 0.0404 ::: bot acc: 0.1287
top acc: 0.0440 ::: bot acc: 0.1192
top acc: 0.0295 ::: bot acc: 0.1226
top acc: 0.0291 ::: bot acc: 0.0832
top acc: 0.0480 ::: bot acc: 0.0577
top acc: 0.0213 ::: bot acc: 0.0909
current epoch: 18
train loss is 0.062655
average val loss: 0.059073, accuracy: 0.0610
average test loss: 0.061827, accuracy: 0.0626
case acc: 0.078423664
case acc: 0.07914934
case acc: 0.0723161
case acc: 0.050300498
case acc: 0.044767305
case acc: 0.050380405
top acc: 0.0346 ::: bot acc: 0.1220
top acc: 0.0407 ::: bot acc: 0.1155
top acc: 0.0270 ::: bot acc: 0.1177
top acc: 0.0282 ::: bot acc: 0.0867
top acc: 0.0393 ::: bot acc: 0.0668
top acc: 0.0223 ::: bot acc: 0.0884
current epoch: 19
train loss is 0.058795
average val loss: 0.058368, accuracy: 0.0603
average test loss: 0.061108, accuracy: 0.0619
case acc: 0.07336167
case acc: 0.0767328
case acc: 0.06971977
case acc: 0.05297471
case acc: 0.048998397
case acc: 0.049729478
top acc: 0.0302 ::: bot acc: 0.1165
top acc: 0.0384 ::: bot acc: 0.1131
top acc: 0.0255 ::: bot acc: 0.1145
top acc: 0.0275 ::: bot acc: 0.0911
top acc: 0.0326 ::: bot acc: 0.0765
top acc: 0.0228 ::: bot acc: 0.0872
current epoch: 20
train loss is 0.056337
average val loss: 0.055670, accuracy: 0.0576
average test loss: 0.058215, accuracy: 0.0590
case acc: 0.06566394
case acc: 0.07095461
case acc: 0.064744785
case acc: 0.053355783
case acc: 0.051792085
case acc: 0.047503494
top acc: 0.0244 ::: bot acc: 0.1079
top acc: 0.0330 ::: bot acc: 0.1071
top acc: 0.0231 ::: bot acc: 0.1082
top acc: 0.0275 ::: bot acc: 0.0916
top acc: 0.0295 ::: bot acc: 0.0822
top acc: 0.0251 ::: bot acc: 0.0827
current epoch: 21
train loss is 0.052170
average val loss: 0.051036, accuracy: 0.0529
average test loss: 0.053183, accuracy: 0.0538
case acc: 0.055660233
case acc: 0.061650604
case acc: 0.05763431
case acc: 0.05119167
case acc: 0.052419893
case acc: 0.044381745
top acc: 0.0183 ::: bot acc: 0.0958
top acc: 0.0243 ::: bot acc: 0.0974
top acc: 0.0210 ::: bot acc: 0.0986
top acc: 0.0281 ::: bot acc: 0.0881
top acc: 0.0288 ::: bot acc: 0.0835
top acc: 0.0312 ::: bot acc: 0.0750
current epoch: 22
train loss is 0.047624
average val loss: 0.049180, accuracy: 0.0509
average test loss: 0.051143, accuracy: 0.0516
case acc: 0.049992625
case acc: 0.056507245
case acc: 0.054227266
case acc: 0.05106902
case acc: 0.054756608
case acc: 0.043255497
top acc: 0.0161 ::: bot acc: 0.0883
top acc: 0.0199 ::: bot acc: 0.0919
top acc: 0.0209 ::: bot acc: 0.0935
top acc: 0.0281 ::: bot acc: 0.0879
top acc: 0.0274 ::: bot acc: 0.0876
top acc: 0.0345 ::: bot acc: 0.0717
current epoch: 23
train loss is 0.046025
average val loss: 0.045732, accuracy: 0.0474
average test loss: 0.047425, accuracy: 0.0477
case acc: 0.043419898
case acc: 0.048835415
case acc: 0.049283694
case acc: 0.048550915
case acc: 0.054500677
case acc: 0.041595504
top acc: 0.0168 ::: bot acc: 0.0781
top acc: 0.0147 ::: bot acc: 0.0830
top acc: 0.0218 ::: bot acc: 0.0856
top acc: 0.0291 ::: bot acc: 0.0836
top acc: 0.0275 ::: bot acc: 0.0871
top acc: 0.0407 ::: bot acc: 0.0654
current epoch: 24
train loss is 0.044387
average val loss: 0.041540, accuracy: 0.0429
average test loss: 0.042984, accuracy: 0.0431
case acc: 0.037968613
case acc: 0.040140744
case acc: 0.0437016
case acc: 0.044611342
case acc: 0.05201131
case acc: 0.040368855
top acc: 0.0247 ::: bot acc: 0.0659
top acc: 0.0118 ::: bot acc: 0.0714
top acc: 0.0259 ::: bot acc: 0.0752
top acc: 0.0332 ::: bot acc: 0.0757
top acc: 0.0289 ::: bot acc: 0.0826
top acc: 0.0493 ::: bot acc: 0.0568
current epoch: 25
train loss is 0.043149
average val loss: 0.038309, accuracy: 0.0394
average test loss: 0.039631, accuracy: 0.0397
case acc: 0.034415126
case acc: 0.03409816
case acc: 0.039621938
case acc: 0.041029193
case acc: 0.049020663
case acc: 0.039766498
top acc: 0.0348 ::: bot acc: 0.0548
top acc: 0.0167 ::: bot acc: 0.0598
top acc: 0.0348 ::: bot acc: 0.0646
top acc: 0.0407 ::: bot acc: 0.0665
top acc: 0.0321 ::: bot acc: 0.0765
top acc: 0.0574 ::: bot acc: 0.0486
current epoch: 26
train loss is 0.042336
average val loss: 0.037270, accuracy: 0.0382
average test loss: 0.038521, accuracy: 0.0385
case acc: 0.03351889
case acc: 0.031677715
case acc: 0.038368788
case acc: 0.039588068
case acc: 0.04795329
case acc: 0.039602436
top acc: 0.0393 ::: bot acc: 0.0503
top acc: 0.0212 ::: bot acc: 0.0539
top acc: 0.0400 ::: bot acc: 0.0594
top acc: 0.0454 ::: bot acc: 0.0618
top acc: 0.0333 ::: bot acc: 0.0742
top acc: 0.0598 ::: bot acc: 0.0461
current epoch: 27
train loss is 0.041422
average val loss: 0.036423, accuracy: 0.0370
average test loss: 0.037576, accuracy: 0.0374
case acc: 0.033078603
case acc: 0.029801613
case acc: 0.03745507
case acc: 0.03847919
case acc: 0.046154145
case acc: 0.039604194
top acc: 0.0432 ::: bot acc: 0.0463
top acc: 0.0270 ::: bot acc: 0.0480
top acc: 0.0454 ::: bot acc: 0.0540
top acc: 0.0512 ::: bot acc: 0.0560
top acc: 0.0357 ::: bot acc: 0.0704
top acc: 0.0624 ::: bot acc: 0.0434
current epoch: 28
train loss is 0.040399
average val loss: 0.036071, accuracy: 0.0364
average test loss: 0.037202, accuracy: 0.0370
case acc: 0.03302535
case acc: 0.029024368
case acc: 0.03723236
case acc: 0.03818425
case acc: 0.045126937
case acc: 0.039569657
top acc: 0.0437 ::: bot acc: 0.0457
top acc: 0.0298 ::: bot acc: 0.0451
top acc: 0.0477 ::: bot acc: 0.0518
top acc: 0.0548 ::: bot acc: 0.0523
top acc: 0.0374 ::: bot acc: 0.0679
top acc: 0.0623 ::: bot acc: 0.0435
current epoch: 29
train loss is 0.039683
average val loss: 0.036058, accuracy: 0.0362
average test loss: 0.037243, accuracy: 0.0371
case acc: 0.03324921
case acc: 0.02915522
case acc: 0.037372544
case acc: 0.03813063
case acc: 0.045059543
case acc: 0.039518535
top acc: 0.0407 ::: bot acc: 0.0487
top acc: 0.0292 ::: bot acc: 0.0457
top acc: 0.0462 ::: bot acc: 0.0533
top acc: 0.0556 ::: bot acc: 0.0514
top acc: 0.0374 ::: bot acc: 0.0678
top acc: 0.0592 ::: bot acc: 0.0465
current epoch: 30
train loss is 0.038312
average val loss: 0.036276, accuracy: 0.0363
average test loss: 0.037524, accuracy: 0.0374
case acc: 0.033952862
case acc: 0.029611211
case acc: 0.037834205
case acc: 0.03811662
case acc: 0.045285873
case acc: 0.039685927
top acc: 0.0367 ::: bot acc: 0.0527
top acc: 0.0274 ::: bot acc: 0.0475
top acc: 0.0429 ::: bot acc: 0.0566
top acc: 0.0554 ::: bot acc: 0.0516
top acc: 0.0369 ::: bot acc: 0.0683
top acc: 0.0553 ::: bot acc: 0.0503
current epoch: 31
train loss is 0.037498
average val loss: 0.036890, accuracy: 0.0369
average test loss: 0.038182, accuracy: 0.0381
case acc: 0.03510584
case acc: 0.03049756
case acc: 0.038854092
case acc: 0.038194012
case acc: 0.045904122
case acc: 0.03998036
top acc: 0.0318 ::: bot acc: 0.0576
top acc: 0.0242 ::: bot acc: 0.0506
top acc: 0.0379 ::: bot acc: 0.0615
top acc: 0.0539 ::: bot acc: 0.0531
top acc: 0.0357 ::: bot acc: 0.0698
top acc: 0.0508 ::: bot acc: 0.0548
current epoch: 32
train loss is 0.037051
average val loss: 0.037874, accuracy: 0.0378
average test loss: 0.039173, accuracy: 0.0391
case acc: 0.036670297
case acc: 0.031700723
case acc: 0.040688735
case acc: 0.038370952
case acc: 0.046820935
case acc: 0.040339086
top acc: 0.0271 ::: bot acc: 0.0625
top acc: 0.0206 ::: bot acc: 0.0542
top acc: 0.0321 ::: bot acc: 0.0674
top acc: 0.0515 ::: bot acc: 0.0554
top acc: 0.0344 ::: bot acc: 0.0718
top acc: 0.0463 ::: bot acc: 0.0592
current epoch: 33
train loss is 0.036943
average val loss: 0.039499, accuracy: 0.0394
average test loss: 0.040796, accuracy: 0.0408
case acc: 0.038895328
case acc: 0.03363604
case acc: 0.04364735
case acc: 0.038973004
case acc: 0.04839028
case acc: 0.04110169
top acc: 0.0228 ::: bot acc: 0.0680
top acc: 0.0170 ::: bot acc: 0.0589
top acc: 0.0262 ::: bot acc: 0.0747
top acc: 0.0477 ::: bot acc: 0.0592
top acc: 0.0325 ::: bot acc: 0.0751
top acc: 0.0413 ::: bot acc: 0.0643
current epoch: 34
train loss is 0.037317
average val loss: 0.042229, accuracy: 0.0422
average test loss: 0.043550, accuracy: 0.0436
case acc: 0.041823868
case acc: 0.036732342
case acc: 0.048552938
case acc: 0.040703084
case acc: 0.051020965
case acc: 0.042593874
top acc: 0.0185 ::: bot acc: 0.0745
top acc: 0.0133 ::: bot acc: 0.0654
top acc: 0.0221 ::: bot acc: 0.0841
top acc: 0.0414 ::: bot acc: 0.0655
top acc: 0.0295 ::: bot acc: 0.0806
top acc: 0.0350 ::: bot acc: 0.0706
current epoch: 35
train loss is 0.038086
average val loss: 0.045018, accuracy: 0.0452
average test loss: 0.046483, accuracy: 0.0466
case acc: 0.044445623
case acc: 0.039925937
case acc: 0.054125015
case acc: 0.042964928
case acc: 0.053726375
case acc: 0.044200484
top acc: 0.0165 ::: bot acc: 0.0795
top acc: 0.0115 ::: bot acc: 0.0710
top acc: 0.0210 ::: bot acc: 0.0930
top acc: 0.0358 ::: bot acc: 0.0718
top acc: 0.0275 ::: bot acc: 0.0856
top acc: 0.0301 ::: bot acc: 0.0755
current epoch: 36
train loss is 0.038951
average val loss: 0.048977, accuracy: 0.0493
average test loss: 0.050841, accuracy: 0.0510
case acc: 0.04805486
case acc: 0.045041904
case acc: 0.061359424
case acc: 0.046891753
case acc: 0.05815503
case acc: 0.04658144
top acc: 0.0159 ::: bot acc: 0.0852
top acc: 0.0127 ::: bot acc: 0.0782
top acc: 0.0218 ::: bot acc: 0.1035
top acc: 0.0306 ::: bot acc: 0.0802
top acc: 0.0265 ::: bot acc: 0.0928
top acc: 0.0252 ::: bot acc: 0.0816
current epoch: 37
train loss is 0.039813
average val loss: 0.054157, accuracy: 0.0546
average test loss: 0.056612, accuracy: 0.0568
case acc: 0.05217656
case acc: 0.051461134
case acc: 0.06991452
case acc: 0.052751046
case acc: 0.06485268
case acc: 0.049699415
top acc: 0.0171 ::: bot acc: 0.0908
top acc: 0.0162 ::: bot acc: 0.0860
top acc: 0.0254 ::: bot acc: 0.1145
top acc: 0.0277 ::: bot acc: 0.0905
top acc: 0.0275 ::: bot acc: 0.1024
top acc: 0.0214 ::: bot acc: 0.0881
current epoch: 38
train loss is 0.040835
average val loss: 0.057420, accuracy: 0.0579
average test loss: 0.060145, accuracy: 0.0603
case acc: 0.05261424
case acc: 0.054671742
case acc: 0.075219825
case acc: 0.057623673
case acc: 0.07077866
case acc: 0.050990622
top acc: 0.0173 ::: bot acc: 0.0914
top acc: 0.0186 ::: bot acc: 0.0896
top acc: 0.0286 ::: bot acc: 0.1209
top acc: 0.0277 ::: bot acc: 0.0978
top acc: 0.0296 ::: bot acc: 0.1103
top acc: 0.0204 ::: bot acc: 0.0906
current epoch: 39
train loss is 0.041442
average val loss: 0.055709, accuracy: 0.0564
average test loss: 0.058262, accuracy: 0.0583
case acc: 0.046915203
case acc: 0.050750695
case acc: 0.0730431
case acc: 0.058093343
case acc: 0.07266531
case acc: 0.048447397
top acc: 0.0158 ::: bot acc: 0.0835
top acc: 0.0157 ::: bot acc: 0.0852
top acc: 0.0272 ::: bot acc: 0.1183
top acc: 0.0277 ::: bot acc: 0.0985
top acc: 0.0302 ::: bot acc: 0.1128
top acc: 0.0226 ::: bot acc: 0.0856
current epoch: 40
train loss is 0.042261
average val loss: 0.048215, accuracy: 0.0493
average test loss: 0.050141, accuracy: 0.0503
case acc: 0.03789803
case acc: 0.039443534
case acc: 0.06150462
case acc: 0.052154362
case acc: 0.06794521
case acc: 0.042936753
top acc: 0.0246 ::: bot acc: 0.0656
top acc: 0.0118 ::: bot acc: 0.0702
top acc: 0.0218 ::: bot acc: 0.1037
top acc: 0.0278 ::: bot acc: 0.0895
top acc: 0.0286 ::: bot acc: 0.1065
top acc: 0.0340 ::: bot acc: 0.0716
current epoch: 41
train loss is 0.046609
average val loss: 0.037411, accuracy: 0.0389
average test loss: 0.038362, accuracy: 0.0386
case acc: 0.034761805
case acc: 0.027638188
case acc: 0.040211223
case acc: 0.03889548
case acc: 0.050391685
case acc: 0.039882142
top acc: 0.0610 ::: bot acc: 0.0284
top acc: 0.0406 ::: bot acc: 0.0342
top acc: 0.0335 ::: bot acc: 0.0659
top acc: 0.0482 ::: bot acc: 0.0588
top acc: 0.0302 ::: bot acc: 0.0794
top acc: 0.0672 ::: bot acc: 0.0384
current epoch: 42
train loss is 0.053208
average val loss: 0.050742, accuracy: 0.0514
average test loss: 0.048552, accuracy: 0.0496
case acc: 0.058990028
case acc: 0.048974372
case acc: 0.043086503
case acc: 0.050211333
case acc: 0.03888951
case acc: 0.057643186
top acc: 0.1032 ::: bot acc: 0.0178
top acc: 0.0852 ::: bot acc: 0.0174
top acc: 0.0814 ::: bot acc: 0.0212
top acc: 0.0915 ::: bot acc: 0.0242
top acc: 0.0645 ::: bot acc: 0.0396
top acc: 0.1080 ::: bot acc: 0.0140
current epoch: 43
train loss is 0.054538
average val loss: 0.045462, accuracy: 0.0461
average test loss: 0.043892, accuracy: 0.0446
case acc: 0.04850887
case acc: 0.041865986
case acc: 0.04058912
case acc: 0.04756378
case acc: 0.039204177
case acc: 0.050024573
top acc: 0.0899 ::: bot acc: 0.0131
top acc: 0.0767 ::: bot acc: 0.0132
top acc: 0.0765 ::: bot acc: 0.0233
top acc: 0.0867 ::: bot acc: 0.0260
top acc: 0.0597 ::: bot acc: 0.0444
top acc: 0.0964 ::: bot acc: 0.0143
current epoch: 44
train loss is 0.048941
average val loss: 0.037440, accuracy: 0.0381
average test loss: 0.037626, accuracy: 0.0377
case acc: 0.03572049
case acc: 0.030290585
case acc: 0.037027553
case acc: 0.04143224
case acc: 0.04043333
case acc: 0.0415082
top acc: 0.0652 ::: bot acc: 0.0240
top acc: 0.0570 ::: bot acc: 0.0178
top acc: 0.0587 ::: bot acc: 0.0406
top acc: 0.0737 ::: bot acc: 0.0335
top acc: 0.0512 ::: bot acc: 0.0529
top acc: 0.0754 ::: bot acc: 0.0300
current epoch: 45
train loss is 0.041080
average val loss: 0.035308, accuracy: 0.0354
average test loss: 0.036271, accuracy: 0.0362
case acc: 0.032856625
case acc: 0.027678095
case acc: 0.037451338
case acc: 0.039257564
case acc: 0.04086649
case acc: 0.039366532
top acc: 0.0472 ::: bot acc: 0.0420
top acc: 0.0432 ::: bot acc: 0.0316
top acc: 0.0451 ::: bot acc: 0.0542
top acc: 0.0669 ::: bot acc: 0.0401
top acc: 0.0489 ::: bot acc: 0.0551
top acc: 0.0615 ::: bot acc: 0.0438
current epoch: 46
train loss is 0.037225
average val loss: 0.036481, accuracy: 0.0363
average test loss: 0.037771, accuracy: 0.0377
case acc: 0.035383195
case acc: 0.0289746
case acc: 0.0411683
case acc: 0.03810418
case acc: 0.042313155
case acc: 0.040014416
top acc: 0.0308 ::: bot acc: 0.0585
top acc: 0.0295 ::: bot acc: 0.0451
top acc: 0.0311 ::: bot acc: 0.0685
top acc: 0.0587 ::: bot acc: 0.0483
top acc: 0.0433 ::: bot acc: 0.0606
top acc: 0.0483 ::: bot acc: 0.0570
current epoch: 47
train loss is 0.036547
average val loss: 0.040919, accuracy: 0.0407
average test loss: 0.042206, accuracy: 0.0421
case acc: 0.04173504
case acc: 0.034027953
case acc: 0.048751645
case acc: 0.03915146
case acc: 0.046480637
case acc: 0.042581543
top acc: 0.0186 ::: bot acc: 0.0742
top acc: 0.0163 ::: bot acc: 0.0597
top acc: 0.0221 ::: bot acc: 0.0844
top acc: 0.0470 ::: bot acc: 0.0600
top acc: 0.0347 ::: bot acc: 0.0712
top acc: 0.0345 ::: bot acc: 0.0708
current epoch: 48
train loss is 0.038195
average val loss: 0.048169, accuracy: 0.0481
average test loss: 0.049962, accuracy: 0.0500
case acc: 0.05092005
case acc: 0.0427603
case acc: 0.060096078
case acc: 0.0444131
case acc: 0.053812847
case acc: 0.047968026
top acc: 0.0166 ::: bot acc: 0.0890
top acc: 0.0118 ::: bot acc: 0.0751
top acc: 0.0214 ::: bot acc: 0.1017
top acc: 0.0336 ::: bot acc: 0.0749
top acc: 0.0274 ::: bot acc: 0.0858
top acc: 0.0228 ::: bot acc: 0.0849
current epoch: 49
train loss is 0.040770
average val loss: 0.055420, accuracy: 0.0555
average test loss: 0.057966, accuracy: 0.0581
case acc: 0.057981905
case acc: 0.0518491
case acc: 0.07080335
case acc: 0.05146643
case acc: 0.06309296
case acc: 0.053239558
top acc: 0.0196 ::: bot acc: 0.0981
top acc: 0.0165 ::: bot acc: 0.0863
top acc: 0.0258 ::: bot acc: 0.1156
top acc: 0.0280 ::: bot acc: 0.0884
top acc: 0.0272 ::: bot acc: 0.0999
top acc: 0.0190 ::: bot acc: 0.0947
current epoch: 50
train loss is 0.042147
average val loss: 0.054545, accuracy: 0.0547
average test loss: 0.057014, accuracy: 0.0571
case acc: 0.053161696
case acc: 0.049444545
case acc: 0.070571996
case acc: 0.05247029
case acc: 0.066002674
case acc: 0.05100824
top acc: 0.0175 ::: bot acc: 0.0920
top acc: 0.0149 ::: bot acc: 0.0836
top acc: 0.0257 ::: bot acc: 0.1153
top acc: 0.0278 ::: bot acc: 0.0899
top acc: 0.0279 ::: bot acc: 0.1039
top acc: 0.0202 ::: bot acc: 0.0907
LME_Co_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5406 5406 5406
1.7082474 -0.6288155 0.16982092 -0.17269358
Validation: 606 606 606
Testing: 750 750 750
pre-processing time: 0.0006005764007568359
the split date is 2012-01-01
net initializing with time: 0.005437612533569336
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.163436
average val loss: 0.204613, accuracy: 0.2047
average test loss: 0.180068, accuracy: 0.1808
case acc: 0.23214968
case acc: 0.06758334
case acc: 0.12611315
case acc: 0.14197591
case acc: 0.046166886
case acc: 0.4708279
top acc: 0.1850 ::: bot acc: 0.2811
top acc: 0.0311 ::: bot acc: 0.1091
top acc: 0.0637 ::: bot acc: 0.1902
top acc: 0.0930 ::: bot acc: 0.1914
top acc: 0.0286 ::: bot acc: 0.0762
top acc: 0.4101 ::: bot acc: 0.5261
current epoch: 2
train loss is 0.168737
average val loss: 0.273163, accuracy: 0.2732
average test loss: 0.247108, accuracy: 0.2471
case acc: 0.29655555
case acc: 0.14534305
case acc: 0.19734375
case acc: 0.2159206
case acc: 0.1090381
case acc: 0.5185608
top acc: 0.2495 ::: bot acc: 0.3451
top acc: 0.1068 ::: bot acc: 0.1879
top acc: 0.1351 ::: bot acc: 0.2612
top acc: 0.1668 ::: bot acc: 0.2655
top acc: 0.0549 ::: bot acc: 0.1571
top acc: 0.4577 ::: bot acc: 0.5739
current epoch: 3
train loss is 0.147318
average val loss: 0.227521, accuracy: 0.2276
average test loss: 0.201707, accuracy: 0.2019
case acc: 0.24326777
case acc: 0.10985762
case acc: 0.15415072
case acc: 0.17573534
case acc: 0.07770343
case acc: 0.45094198
top acc: 0.1963 ::: bot acc: 0.2917
top acc: 0.0709 ::: bot acc: 0.1527
top acc: 0.0921 ::: bot acc: 0.2178
top acc: 0.1267 ::: bot acc: 0.2255
top acc: 0.0290 ::: bot acc: 0.1232
top acc: 0.3902 ::: bot acc: 0.5062
current epoch: 4
train loss is 0.123911
average val loss: 0.169956, accuracy: 0.1701
average test loss: 0.145800, accuracy: 0.1464
case acc: 0.17718148
case acc: 0.06357002
case acc: 0.09838981
case acc: 0.12286678
case acc: 0.046460196
case acc: 0.36982703
top acc: 0.1304 ::: bot acc: 0.2255
top acc: 0.0272 ::: bot acc: 0.1053
top acc: 0.0371 ::: bot acc: 0.1616
top acc: 0.0739 ::: bot acc: 0.1727
top acc: 0.0275 ::: bot acc: 0.0776
top acc: 0.3092 ::: bot acc: 0.4251
current epoch: 5
train loss is 0.115161
average val loss: 0.176732, accuracy: 0.1768
average test loss: 0.151615, accuracy: 0.1521
case acc: 0.1779904
case acc: 0.07994156
case acc: 0.10761622
case acc: 0.13450076
case acc: 0.05765766
case acc: 0.3546569
top acc: 0.1312 ::: bot acc: 0.2263
top acc: 0.0410 ::: bot acc: 0.1229
top acc: 0.0460 ::: bot acc: 0.1709
top acc: 0.0855 ::: bot acc: 0.1844
top acc: 0.0218 ::: bot acc: 0.0971
top acc: 0.2940 ::: bot acc: 0.4100
current epoch: 6
train loss is 0.104520
average val loss: 0.152169, accuracy: 0.1522
average test loss: 0.127843, accuracy: 0.1283
case acc: 0.14677261
case acc: 0.0661132
case acc: 0.08576009
case acc: 0.11388792
case acc: 0.04977822
case acc: 0.30765033
top acc: 0.1001 ::: bot acc: 0.1949
top acc: 0.0289 ::: bot acc: 0.1083
top acc: 0.0259 ::: bot acc: 0.1481
top acc: 0.0653 ::: bot acc: 0.1636
top acc: 0.0242 ::: bot acc: 0.0843
top acc: 0.2470 ::: bot acc: 0.3630
current epoch: 7
train loss is 0.096398
average val loss: 0.142975, accuracy: 0.1429
average test loss: 0.118656, accuracy: 0.1191
case acc: 0.1321014
case acc: 0.06670638
case acc: 0.07952939
case acc: 0.10833205
case acc: 0.051158868
case acc: 0.27681583
top acc: 0.0854 ::: bot acc: 0.1802
top acc: 0.0293 ::: bot acc: 0.1089
top acc: 0.0212 ::: bot acc: 0.1411
top acc: 0.0599 ::: bot acc: 0.1580
top acc: 0.0233 ::: bot acc: 0.0867
top acc: 0.2162 ::: bot acc: 0.3322
current epoch: 8
train loss is 0.086980
average val loss: 0.118198, accuracy: 0.1181
average test loss: 0.095551, accuracy: 0.0961
case acc: 0.101779364
case acc: 0.052494146
case acc: 0.06172671
case acc: 0.08665882
case acc: 0.044367306
case acc: 0.2297586
top acc: 0.0551 ::: bot acc: 0.1499
top acc: 0.0189 ::: bot acc: 0.0929
top acc: 0.0149 ::: bot acc: 0.1177
top acc: 0.0400 ::: bot acc: 0.1356
top acc: 0.0316 ::: bot acc: 0.0726
top acc: 0.1691 ::: bot acc: 0.2851
current epoch: 9
train loss is 0.077605
average val loss: 0.090149, accuracy: 0.0896
average test loss: 0.071782, accuracy: 0.0725
case acc: 0.068921015
case acc: 0.036975395
case acc: 0.049762268
case acc: 0.06278506
case acc: 0.038373064
case acc: 0.17799436
top acc: 0.0231 ::: bot acc: 0.1165
top acc: 0.0160 ::: bot acc: 0.0713
top acc: 0.0351 ::: bot acc: 0.0893
top acc: 0.0262 ::: bot acc: 0.1068
top acc: 0.0504 ::: bot acc: 0.0531
top acc: 0.1174 ::: bot acc: 0.2334
current epoch: 10
train loss is 0.071783
average val loss: 0.066474, accuracy: 0.0656
average test loss: 0.055086, accuracy: 0.0552
case acc: 0.04593392
case acc: 0.029591057
case acc: 0.047099415
case acc: 0.04226778
case acc: 0.038583223
case acc: 0.1278574
top acc: 0.0119 ::: bot acc: 0.0877
top acc: 0.0339 ::: bot acc: 0.0496
top acc: 0.0621 ::: bot acc: 0.0621
top acc: 0.0246 ::: bot acc: 0.0771
top acc: 0.0699 ::: bot acc: 0.0339
top acc: 0.0673 ::: bot acc: 0.1832
current epoch: 11
train loss is 0.070664
average val loss: 0.058858, accuracy: 0.0577
average test loss: 0.049265, accuracy: 0.0493
case acc: 0.043359816
case acc: 0.029544005
case acc: 0.047574658
case acc: 0.037452385
case acc: 0.038724817
case acc: 0.09933669
top acc: 0.0141 ::: bot acc: 0.0828
top acc: 0.0371 ::: bot acc: 0.0466
top acc: 0.0691 ::: bot acc: 0.0551
top acc: 0.0334 ::: bot acc: 0.0653
top acc: 0.0707 ::: bot acc: 0.0333
top acc: 0.0395 ::: bot acc: 0.1544
current epoch: 12
train loss is 0.068904
average val loss: 0.062351, accuracy: 0.0615
average test loss: 0.049207, accuracy: 0.0493
case acc: 0.05188523
case acc: 0.031670574
case acc: 0.046941582
case acc: 0.038696755
case acc: 0.037809845
case acc: 0.088951886
top acc: 0.0124 ::: bot acc: 0.0963
top acc: 0.0244 ::: bot acc: 0.0594
top acc: 0.0593 ::: bot acc: 0.0648
top acc: 0.0292 ::: bot acc: 0.0695
top acc: 0.0558 ::: bot acc: 0.0482
top acc: 0.0299 ::: bot acc: 0.1436
current epoch: 13
train loss is 0.064241
average val loss: 0.069821, accuracy: 0.0693
average test loss: 0.052597, accuracy: 0.0532
case acc: 0.06470971
case acc: 0.039191414
case acc: 0.047407392
case acc: 0.042654697
case acc: 0.041788362
case acc: 0.083191186
top acc: 0.0196 ::: bot acc: 0.1119
top acc: 0.0150 ::: bot acc: 0.0753
top acc: 0.0464 ::: bot acc: 0.0777
top acc: 0.0242 ::: bot acc: 0.0778
top acc: 0.0382 ::: bot acc: 0.0657
top acc: 0.0256 ::: bot acc: 0.1372
current epoch: 14
train loss is 0.059875
average val loss: 0.075910, accuracy: 0.0756
average test loss: 0.056274, accuracy: 0.0570
case acc: 0.073545106
case acc: 0.04865087
case acc: 0.049130615
case acc: 0.047075648
case acc: 0.047631312
case acc: 0.07611065
top acc: 0.0274 ::: bot acc: 0.1213
top acc: 0.0167 ::: bot acc: 0.0885
top acc: 0.0367 ::: bot acc: 0.0875
top acc: 0.0228 ::: bot acc: 0.0850
top acc: 0.0267 ::: bot acc: 0.0799
top acc: 0.0209 ::: bot acc: 0.1289
current epoch: 15
train loss is 0.054574
average val loss: 0.077694, accuracy: 0.0775
average test loss: 0.057316, accuracy: 0.0581
case acc: 0.07460053
case acc: 0.055439837
case acc: 0.050216403
case acc: 0.04945791
case acc: 0.052247595
case acc: 0.06666339
top acc: 0.0284 ::: bot acc: 0.1223
top acc: 0.0208 ::: bot acc: 0.0967
top acc: 0.0324 ::: bot acc: 0.0917
top acc: 0.0228 ::: bot acc: 0.0887
top acc: 0.0231 ::: bot acc: 0.0886
top acc: 0.0171 ::: bot acc: 0.1168
current epoch: 16
train loss is 0.049364
average val loss: 0.077584, accuracy: 0.0775
average test loss: 0.057186, accuracy: 0.0580
case acc: 0.071443625
case acc: 0.060196884
case acc: 0.050679483
case acc: 0.051049795
case acc: 0.055595685
case acc: 0.059139583
top acc: 0.0255 ::: bot acc: 0.1190
top acc: 0.0242 ::: bot acc: 0.1021
top acc: 0.0306 ::: bot acc: 0.0933
top acc: 0.0230 ::: bot acc: 0.0910
top acc: 0.0222 ::: bot acc: 0.0941
top acc: 0.0185 ::: bot acc: 0.1049
current epoch: 17
train loss is 0.044297
average val loss: 0.075676, accuracy: 0.0758
average test loss: 0.055814, accuracy: 0.0566
case acc: 0.06538108
case acc: 0.06192538
case acc: 0.0504558
case acc: 0.051330436
case acc: 0.05655749
case acc: 0.054067224
top acc: 0.0202 ::: bot acc: 0.1126
top acc: 0.0255 ::: bot acc: 0.1041
top acc: 0.0313 ::: bot acc: 0.0926
top acc: 0.0230 ::: bot acc: 0.0914
top acc: 0.0221 ::: bot acc: 0.0956
top acc: 0.0232 ::: bot acc: 0.0950
current epoch: 18
train loss is 0.041807
average val loss: 0.076311, accuracy: 0.0764
average test loss: 0.056325, accuracy: 0.0570
case acc: 0.061662335
case acc: 0.06459675
case acc: 0.05097836
case acc: 0.053676043
case acc: 0.057891272
case acc: 0.053263582
top acc: 0.0172 ::: bot acc: 0.1085
top acc: 0.0274 ::: bot acc: 0.1071
top acc: 0.0296 ::: bot acc: 0.0943
top acc: 0.0234 ::: bot acc: 0.0947
top acc: 0.0220 ::: bot acc: 0.0976
top acc: 0.0244 ::: bot acc: 0.0932
current epoch: 19
train loss is 0.040216
average val loss: 0.074868, accuracy: 0.0749
average test loss: 0.055192, accuracy: 0.0558
case acc: 0.05603339
case acc: 0.06379378
case acc: 0.050710358
case acc: 0.05439687
case acc: 0.05646111
case acc: 0.05328975
top acc: 0.0138 ::: bot acc: 0.1018
top acc: 0.0268 ::: bot acc: 0.1061
top acc: 0.0306 ::: bot acc: 0.0933
top acc: 0.0237 ::: bot acc: 0.0957
top acc: 0.0221 ::: bot acc: 0.0954
top acc: 0.0244 ::: bot acc: 0.0933
current epoch: 20
train loss is 0.039686
average val loss: 0.071609, accuracy: 0.0716
average test loss: 0.052766, accuracy: 0.0533
case acc: 0.04956207
case acc: 0.06031827
case acc: 0.049759842
case acc: 0.053447526
case acc: 0.05325636
case acc: 0.053279553
top acc: 0.0118 ::: bot acc: 0.0931
top acc: 0.0243 ::: bot acc: 0.1022
top acc: 0.0340 ::: bot acc: 0.0899
top acc: 0.0235 ::: bot acc: 0.0944
top acc: 0.0229 ::: bot acc: 0.0901
top acc: 0.0244 ::: bot acc: 0.0933
current epoch: 21
train loss is 0.038811
average val loss: 0.066806, accuracy: 0.0668
average test loss: 0.049416, accuracy: 0.0498
case acc: 0.043573216
case acc: 0.054322015
case acc: 0.04842323
case acc: 0.050643288
case acc: 0.048809364
case acc: 0.053074125
top acc: 0.0135 ::: bot acc: 0.0834
top acc: 0.0199 ::: bot acc: 0.0954
top acc: 0.0398 ::: bot acc: 0.0843
top acc: 0.0230 ::: bot acc: 0.0904
top acc: 0.0255 ::: bot acc: 0.0822
top acc: 0.0248 ::: bot acc: 0.0928
current epoch: 22
train loss is 0.038716
average val loss: 0.062069, accuracy: 0.0620
average test loss: 0.046473, accuracy: 0.0468
case acc: 0.03977286
case acc: 0.0481309
case acc: 0.047474932
case acc: 0.04732807
case acc: 0.04501491
case acc: 0.052900083
top acc: 0.0193 ::: bot acc: 0.0749
top acc: 0.0164 ::: bot acc: 0.0880
top acc: 0.0459 ::: bot acc: 0.0783
top acc: 0.0228 ::: bot acc: 0.0856
top acc: 0.0308 ::: bot acc: 0.0739
top acc: 0.0252 ::: bot acc: 0.0924
current epoch: 23
train loss is 0.038638
average val loss: 0.054836, accuracy: 0.0547
average test loss: 0.042605, accuracy: 0.0427
case acc: 0.03652862
case acc: 0.03970182
case acc: 0.04694824
case acc: 0.041505966
case acc: 0.04048128
case acc: 0.05082317
top acc: 0.0297 ::: bot acc: 0.0644
top acc: 0.0146 ::: bot acc: 0.0763
top acc: 0.0559 ::: bot acc: 0.0683
top acc: 0.0251 ::: bot acc: 0.0758
top acc: 0.0414 ::: bot acc: 0.0620
top acc: 0.0292 ::: bot acc: 0.0873
current epoch: 24
train loss is 0.038271
average val loss: 0.048959, accuracy: 0.0486
average test loss: 0.040299, accuracy: 0.0401
case acc: 0.035047036
case acc: 0.033767972
case acc: 0.047235284
case acc: 0.037481524
case acc: 0.03803016
case acc: 0.04915258
top acc: 0.0372 ::: bot acc: 0.0569
top acc: 0.0194 ::: bot acc: 0.0652
top acc: 0.0651 ::: bot acc: 0.0590
top acc: 0.0333 ::: bot acc: 0.0656
top acc: 0.0524 ::: bot acc: 0.0510
top acc: 0.0337 ::: bot acc: 0.0825
current epoch: 25
train loss is 0.038291
average val loss: 0.044073, accuracy: 0.0436
average test loss: 0.039257, accuracy: 0.0391
case acc: 0.034690905
case acc: 0.030188434
case acc: 0.048267897
case acc: 0.036059834
case acc: 0.037605114
case acc: 0.047592957
top acc: 0.0424 ::: bot acc: 0.0517
top acc: 0.0302 ::: bot acc: 0.0537
top acc: 0.0743 ::: bot acc: 0.0499
top acc: 0.0447 ::: bot acc: 0.0542
top acc: 0.0633 ::: bot acc: 0.0398
top acc: 0.0386 ::: bot acc: 0.0777
current epoch: 26
train loss is 0.037829
average val loss: 0.040096, accuracy: 0.0394
average test loss: 0.039549, accuracy: 0.0395
case acc: 0.034759853
case acc: 0.030140143
case acc: 0.049779713
case acc: 0.03723691
case acc: 0.039236862
case acc: 0.045965977
top acc: 0.0472 ::: bot acc: 0.0469
top acc: 0.0428 ::: bot acc: 0.0412
top acc: 0.0843 ::: bot acc: 0.0398
top acc: 0.0580 ::: bot acc: 0.0409
top acc: 0.0752 ::: bot acc: 0.0280
top acc: 0.0446 ::: bot acc: 0.0717
current epoch: 27
train loss is 0.037593
average val loss: 0.037284, accuracy: 0.0363
average test loss: 0.042184, accuracy: 0.0426
case acc: 0.03521608
case acc: 0.03580761
case acc: 0.0537348
case acc: 0.042370386
case acc: 0.044614755
case acc: 0.043844257
top acc: 0.0531 ::: bot acc: 0.0411
top acc: 0.0579 ::: bot acc: 0.0287
top acc: 0.0969 ::: bot acc: 0.0276
top acc: 0.0749 ::: bot acc: 0.0255
top acc: 0.0895 ::: bot acc: 0.0160
top acc: 0.0545 ::: bot acc: 0.0617
current epoch: 28
train loss is 0.039195
average val loss: 0.037400, accuracy: 0.0365
average test loss: 0.047893, accuracy: 0.0487
case acc: 0.03612611
case acc: 0.04532458
case acc: 0.060953766
case acc: 0.05315858
case acc: 0.053998742
case acc: 0.042847
top acc: 0.0587 ::: bot acc: 0.0355
top acc: 0.0740 ::: bot acc: 0.0247
top acc: 0.1107 ::: bot acc: 0.0214
top acc: 0.0943 ::: bot acc: 0.0188
top acc: 0.1046 ::: bot acc: 0.0135
top acc: 0.0676 ::: bot acc: 0.0486
current epoch: 29
train loss is 0.043523
average val loss: 0.039586, accuracy: 0.0390
average test loss: 0.053120, accuracy: 0.0542
case acc: 0.03597177
case acc: 0.05255672
case acc: 0.06653126
case acc: 0.064827845
case acc: 0.061880175
case acc: 0.04332725
top acc: 0.0578 ::: bot acc: 0.0363
top acc: 0.0847 ::: bot acc: 0.0248
top acc: 0.1191 ::: bot acc: 0.0209
top acc: 0.1097 ::: bot acc: 0.0227
top acc: 0.1148 ::: bot acc: 0.0168
top acc: 0.0773 ::: bot acc: 0.0389
current epoch: 30
train loss is 0.052080
average val loss: 0.038681, accuracy: 0.0384
average test loss: 0.047708, accuracy: 0.0492
case acc: 0.035467636
case acc: 0.044919968
case acc: 0.05833102
case acc: 0.060735337
case acc: 0.052804127
case acc: 0.04281559
top acc: 0.0345 ::: bot acc: 0.0597
top acc: 0.0733 ::: bot acc: 0.0249
top acc: 0.1059 ::: bot acc: 0.0230
top acc: 0.1046 ::: bot acc: 0.0206
top acc: 0.1030 ::: bot acc: 0.0133
top acc: 0.0689 ::: bot acc: 0.0472
current epoch: 31
train loss is 0.063124
average val loss: 0.064960, accuracy: 0.0647
average test loss: 0.049480, accuracy: 0.0509
case acc: 0.08384175
case acc: 0.03912899
case acc: 0.048024513
case acc: 0.036058437
case acc: 0.041519355
case acc: 0.056967948
top acc: 0.0375 ::: bot acc: 0.1318
top acc: 0.0147 ::: bot acc: 0.0755
top acc: 0.0412 ::: bot acc: 0.0828
top acc: 0.0466 ::: bot acc: 0.0522
top acc: 0.0380 ::: bot acc: 0.0650
top acc: 0.0201 ::: bot acc: 0.1010
current epoch: 32
train loss is 0.071113
average val loss: 0.109428, accuracy: 0.1096
average test loss: 0.084764, accuracy: 0.0851
case acc: 0.12911944
case acc: 0.088539325
case acc: 0.07406781
case acc: 0.06141784
case acc: 0.07664326
case acc: 0.08090468
top acc: 0.0828 ::: bot acc: 0.1772
top acc: 0.0486 ::: bot acc: 0.1326
top acc: 0.0182 ::: bot acc: 0.1345
top acc: 0.0254 ::: bot acc: 0.1053
top acc: 0.0288 ::: bot acc: 0.1218
top acc: 0.0239 ::: bot acc: 0.1347
current epoch: 33
train loss is 0.057355
average val loss: 0.095881, accuracy: 0.0963
average test loss: 0.072553, accuracy: 0.0732
case acc: 0.1031532
case acc: 0.08243123
case acc: 0.064662345
case acc: 0.057074286
case acc: 0.0716887
case acc: 0.059967395
top acc: 0.0568 ::: bot acc: 0.1512
top acc: 0.0428 ::: bot acc: 0.1262
top acc: 0.0152 ::: bot acc: 0.1220
top acc: 0.0242 ::: bot acc: 0.0995
top acc: 0.0258 ::: bot acc: 0.1159
top acc: 0.0180 ::: bot acc: 0.1063
current epoch: 34
train loss is 0.052498
average val loss: 0.087468, accuracy: 0.0879
average test loss: 0.065292, accuracy: 0.0658
case acc: 0.0806372
case acc: 0.07760179
case acc: 0.0589276
case acc: 0.056814175
case acc: 0.06718416
case acc: 0.053889226
top acc: 0.0344 ::: bot acc: 0.1286
top acc: 0.0384 ::: bot acc: 0.1212
top acc: 0.0155 ::: bot acc: 0.1133
top acc: 0.0241 ::: bot acc: 0.0992
top acc: 0.0234 ::: bot acc: 0.1104
top acc: 0.0235 ::: bot acc: 0.0946
current epoch: 35
train loss is 0.050084
average val loss: 0.068806, accuracy: 0.0692
average test loss: 0.050871, accuracy: 0.0513
case acc: 0.05019549
case acc: 0.058998886
case acc: 0.050150458
case acc: 0.047464125
case acc: 0.052434787
case acc: 0.048361458
top acc: 0.0120 ::: bot acc: 0.0942
top acc: 0.0233 ::: bot acc: 0.1009
top acc: 0.0324 ::: bot acc: 0.0916
top acc: 0.0226 ::: bot acc: 0.0860
top acc: 0.0230 ::: bot acc: 0.0886
top acc: 0.0358 ::: bot acc: 0.0803
current epoch: 36
train loss is 0.043770
average val loss: 0.050975, accuracy: 0.0513
average test loss: 0.041018, accuracy: 0.0408
case acc: 0.035457723
case acc: 0.03906475
case acc: 0.046930537
case acc: 0.037993886
case acc: 0.040070105
case acc: 0.045135047
top acc: 0.0342 ::: bot acc: 0.0600
top acc: 0.0149 ::: bot acc: 0.0753
top acc: 0.0574 ::: bot acc: 0.0666
top acc: 0.0312 ::: bot acc: 0.0677
top acc: 0.0416 ::: bot acc: 0.0612
top acc: 0.0476 ::: bot acc: 0.0685
current epoch: 37
train loss is 0.041203
average val loss: 0.039506, accuracy: 0.0393
average test loss: 0.039851, accuracy: 0.0391
case acc: 0.03696981
case acc: 0.029566461
case acc: 0.049459938
case acc: 0.036724765
case acc: 0.038367577
case acc: 0.043349966
top acc: 0.0618 ::: bot acc: 0.0324
top acc: 0.0366 ::: bot acc: 0.0476
top acc: 0.0826 ::: bot acc: 0.0414
top acc: 0.0539 ::: bot acc: 0.0450
top acc: 0.0709 ::: bot acc: 0.0319
top acc: 0.0582 ::: bot acc: 0.0579
current epoch: 38
train loss is 0.040229
average val loss: 0.036796, accuracy: 0.0359
average test loss: 0.045120, accuracy: 0.0451
case acc: 0.043238763
case acc: 0.03732418
case acc: 0.05630983
case acc: 0.04290611
case acc: 0.047898117
case acc: 0.04294703
top acc: 0.0769 ::: bot acc: 0.0210
top acc: 0.0606 ::: bot acc: 0.0278
top acc: 0.1023 ::: bot acc: 0.0242
top acc: 0.0757 ::: bot acc: 0.0251
top acc: 0.0956 ::: bot acc: 0.0132
top acc: 0.0647 ::: bot acc: 0.0514
current epoch: 39
train loss is 0.040298
average val loss: 0.038118, accuracy: 0.0370
average test loss: 0.050600, accuracy: 0.0513
case acc: 0.044335894
case acc: 0.046826385
case acc: 0.062808
case acc: 0.052214347
case acc: 0.058999535
case acc: 0.042859614
top acc: 0.0790 ::: bot acc: 0.0201
top acc: 0.0762 ::: bot acc: 0.0248
top acc: 0.1136 ::: bot acc: 0.0209
top acc: 0.0927 ::: bot acc: 0.0188
top acc: 0.1110 ::: bot acc: 0.0155
top acc: 0.0683 ::: bot acc: 0.0478
current epoch: 40
train loss is 0.046018
average val loss: 0.037207, accuracy: 0.0361
average test loss: 0.047001, accuracy: 0.0481
case acc: 0.03657763
case acc: 0.043769818
case acc: 0.058289908
case acc: 0.052280407
case acc: 0.054376133
case acc: 0.043037526
top acc: 0.0603 ::: bot acc: 0.0339
top acc: 0.0713 ::: bot acc: 0.0254
top acc: 0.1059 ::: bot acc: 0.0229
top acc: 0.0928 ::: bot acc: 0.0188
top acc: 0.1050 ::: bot acc: 0.0137
top acc: 0.0626 ::: bot acc: 0.0535
current epoch: 41
train loss is 0.049769
average val loss: 0.044246, accuracy: 0.0436
average test loss: 0.039619, accuracy: 0.0405
case acc: 0.041379742
case acc: 0.029560383
case acc: 0.047813453
case acc: 0.03882258
case acc: 0.038062543
case acc: 0.047361407
top acc: 0.0163 ::: bot acc: 0.0790
top acc: 0.0366 ::: bot acc: 0.0475
top acc: 0.0705 ::: bot acc: 0.0535
top acc: 0.0653 ::: bot acc: 0.0335
top acc: 0.0688 ::: bot acc: 0.0340
top acc: 0.0388 ::: bot acc: 0.0773
current epoch: 42
train loss is 0.050090
average val loss: 0.071071, accuracy: 0.0711
average test loss: 0.052303, accuracy: 0.0531
case acc: 0.07308754
case acc: 0.050675765
case acc: 0.05094513
case acc: 0.0395081
case acc: 0.047092587
case acc: 0.05713406
top acc: 0.0272 ::: bot acc: 0.1209
top acc: 0.0177 ::: bot acc: 0.0913
top acc: 0.0296 ::: bot acc: 0.0944
top acc: 0.0274 ::: bot acc: 0.0719
top acc: 0.0269 ::: bot acc: 0.0788
top acc: 0.0198 ::: bot acc: 0.1014
current epoch: 43
train loss is 0.047037
average val loss: 0.078799, accuracy: 0.0791
average test loss: 0.058083, accuracy: 0.0588
case acc: 0.074833065
case acc: 0.06465481
case acc: 0.054839794
case acc: 0.04748323
case acc: 0.056762654
case acc: 0.054095224
top acc: 0.0289 ::: bot acc: 0.1227
top acc: 0.0275 ::: bot acc: 0.1073
top acc: 0.0201 ::: bot acc: 0.1049
top acc: 0.0226 ::: bot acc: 0.0861
top acc: 0.0224 ::: bot acc: 0.0953
top acc: 0.0232 ::: bot acc: 0.0952
current epoch: 44
train loss is 0.045493
average val loss: 0.072978, accuracy: 0.0735
average test loss: 0.053871, accuracy: 0.0543
case acc: 0.05855135
case acc: 0.06295294
case acc: 0.05241308
case acc: 0.04775771
case acc: 0.05537794
case acc: 0.048854604
top acc: 0.0153 ::: bot acc: 0.1051
top acc: 0.0262 ::: bot acc: 0.1054
top acc: 0.0251 ::: bot acc: 0.0989
top acc: 0.0225 ::: bot acc: 0.0865
top acc: 0.0225 ::: bot acc: 0.0932
top acc: 0.0342 ::: bot acc: 0.0819
current epoch: 45
train loss is 0.045847
average val loss: 0.051904, accuracy: 0.0526
average test loss: 0.041525, accuracy: 0.0412
case acc: 0.036881905
case acc: 0.041240588
case acc: 0.04691896
case acc: 0.0373739
case acc: 0.041475352
case acc: 0.04354761
top acc: 0.0280 ::: bot acc: 0.0662
top acc: 0.0146 ::: bot acc: 0.0788
top acc: 0.0531 ::: bot acc: 0.0708
top acc: 0.0331 ::: bot acc: 0.0656
top acc: 0.0375 ::: bot acc: 0.0652
top acc: 0.0560 ::: bot acc: 0.0601
current epoch: 46
train loss is 0.042172
average val loss: 0.038590, accuracy: 0.0387
average test loss: 0.040192, accuracy: 0.0391
case acc: 0.037134953
case acc: 0.029631268
case acc: 0.0494212
case acc: 0.037269313
case acc: 0.038249828
case acc: 0.042864516
top acc: 0.0624 ::: bot acc: 0.0317
top acc: 0.0365 ::: bot acc: 0.0478
top acc: 0.0824 ::: bot acc: 0.0415
top acc: 0.0583 ::: bot acc: 0.0405
top acc: 0.0703 ::: bot acc: 0.0324
top acc: 0.0707 ::: bot acc: 0.0454
current epoch: 47
train loss is 0.040347
average val loss: 0.036437, accuracy: 0.0359
average test loss: 0.045693, accuracy: 0.0454
case acc: 0.044059943
case acc: 0.037131738
case acc: 0.056033418
case acc: 0.0444052
case acc: 0.04771995
case acc: 0.043021176
top acc: 0.0784 ::: bot acc: 0.0204
top acc: 0.0602 ::: bot acc: 0.0280
top acc: 0.1017 ::: bot acc: 0.0246
top acc: 0.0788 ::: bot acc: 0.0234
top acc: 0.0954 ::: bot acc: 0.0131
top acc: 0.0739 ::: bot acc: 0.0422
current epoch: 48
train loss is 0.038995
average val loss: 0.037071, accuracy: 0.0361
average test loss: 0.048203, accuracy: 0.0488
case acc: 0.042448256
case acc: 0.0432354
case acc: 0.058998007
case acc: 0.05017523
case acc: 0.055131365
case acc: 0.042852994
top acc: 0.0753 ::: bot acc: 0.0218
top acc: 0.0705 ::: bot acc: 0.0256
top acc: 0.1071 ::: bot acc: 0.0225
top acc: 0.0894 ::: bot acc: 0.0193
top acc: 0.1060 ::: bot acc: 0.0139
top acc: 0.0687 ::: bot acc: 0.0475
current epoch: 49
train loss is 0.042499
average val loss: 0.036829, accuracy: 0.0359
average test loss: 0.043091, accuracy: 0.0440
case acc: 0.035268914
case acc: 0.037458766
case acc: 0.05282048
case acc: 0.04713624
case acc: 0.047766708
case acc: 0.043428
top acc: 0.0527 ::: bot acc: 0.0414
top acc: 0.0608 ::: bot acc: 0.0279
top acc: 0.0944 ::: bot acc: 0.0295
top acc: 0.0840 ::: bot acc: 0.0210
top acc: 0.0955 ::: bot acc: 0.0131
top acc: 0.0574 ::: bot acc: 0.0587
current epoch: 50
train loss is 0.043181
average val loss: 0.045148, accuracy: 0.0446
average test loss: 0.039386, accuracy: 0.0402
case acc: 0.041007068
case acc: 0.030025234
case acc: 0.04718256
case acc: 0.037946485
case acc: 0.037649043
case acc: 0.047152344
top acc: 0.0168 ::: bot acc: 0.0782
top acc: 0.0321 ::: bot acc: 0.0523
top acc: 0.0654 ::: bot acc: 0.0584
top acc: 0.0615 ::: bot acc: 0.0374
top acc: 0.0653 ::: bot acc: 0.0375
top acc: 0.0397 ::: bot acc: 0.0765
LME_Co_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5394 5394 5394
1.7082474 -0.6288155 0.16982092 -0.17269358
Validation: 600 600 600
Testing: 768 768 768
pre-processing time: 0.003536224365234375
the split date is 2012-07-01
net initializing with time: 0.0034220218658447266
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.163238
average val loss: 0.070599, accuracy: 0.0692
average test loss: 0.070121, accuracy: 0.0710
case acc: 0.07556543
case acc: 0.12284644
case acc: 0.087197825
case acc: 0.03945163
case acc: 0.05891297
case acc: 0.042089358
top acc: 0.0327 ::: bot acc: 0.1151
top acc: 0.0849 ::: bot acc: 0.1581
top acc: 0.0378 ::: bot acc: 0.1363
top acc: 0.0144 ::: bot acc: 0.0650
top acc: 0.0164 ::: bot acc: 0.1076
top acc: 0.0588 ::: bot acc: 0.0544
current epoch: 2
train loss is 0.089910
average val loss: 0.145404, accuracy: 0.1454
average test loss: 0.145014, accuracy: 0.1453
case acc: 0.15331106
case acc: 0.21002576
case acc: 0.17306942
case acc: 0.11898792
case acc: 0.13898279
case acc: 0.07714133
top acc: 0.0971 ::: bot acc: 0.2001
top acc: 0.1723 ::: bot acc: 0.2453
top acc: 0.1185 ::: bot acc: 0.2260
top acc: 0.0886 ::: bot acc: 0.1467
top acc: 0.0897 ::: bot acc: 0.1909
top acc: 0.0343 ::: bot acc: 0.1321
current epoch: 3
train loss is 0.121592
average val loss: 0.136757, accuracy: 0.1368
average test loss: 0.136258, accuracy: 0.1365
case acc: 0.14158708
case acc: 0.19847947
case acc: 0.16659828
case acc: 0.11134909
case acc: 0.13370526
case acc: 0.06733483
top acc: 0.0853 ::: bot acc: 0.1882
top acc: 0.1613 ::: bot acc: 0.2336
top acc: 0.1122 ::: bot acc: 0.2200
top acc: 0.0805 ::: bot acc: 0.1392
top acc: 0.0845 ::: bot acc: 0.1855
top acc: 0.0274 ::: bot acc: 0.1206
current epoch: 4
train loss is 0.140731
average val loss: 0.053826, accuracy: 0.0532
average test loss: 0.052530, accuracy: 0.0539
case acc: 0.041809916
case acc: 0.032379188
case acc: 0.039304186
case acc: 0.055232625
case acc: 0.046870288
case acc: 0.107834175
top acc: 0.0887 ::: bot acc: 0.0149
top acc: 0.0188 ::: bot acc: 0.0560
top acc: 0.0602 ::: bot acc: 0.0472
top acc: 0.0865 ::: bot acc: 0.0272
top acc: 0.0813 ::: bot acc: 0.0251
top acc: 0.1563 ::: bot acc: 0.0557
current epoch: 5
train loss is 0.070225
average val loss: 0.052454, accuracy: 0.0504
average test loss: 0.051289, accuracy: 0.0530
case acc: 0.04940754
case acc: 0.08102512
case acc: 0.063822314
case acc: 0.025875695
case acc: 0.04587059
case acc: 0.051806934
top acc: 0.0278 ::: bot acc: 0.0790
top acc: 0.0438 ::: bot acc: 0.1164
top acc: 0.0258 ::: bot acc: 0.1091
top acc: 0.0170 ::: bot acc: 0.0432
top acc: 0.0172 ::: bot acc: 0.0876
top acc: 0.0838 ::: bot acc: 0.0338
current epoch: 6
train loss is 0.067825
average val loss: 0.081198, accuracy: 0.0810
average test loss: 0.080975, accuracy: 0.0814
case acc: 0.08124707
case acc: 0.11956024
case acc: 0.098269075
case acc: 0.06380877
case acc: 0.08421144
case acc: 0.041426085
top acc: 0.0361 ::: bot acc: 0.1224
top acc: 0.0823 ::: bot acc: 0.1551
top acc: 0.0458 ::: bot acc: 0.1505
top acc: 0.0322 ::: bot acc: 0.0923
top acc: 0.0371 ::: bot acc: 0.1349
top acc: 0.0322 ::: bot acc: 0.0799
current epoch: 7
train loss is 0.081241
average val loss: 0.063979, accuracy: 0.0630
average test loss: 0.063383, accuracy: 0.0643
case acc: 0.06201242
case acc: 0.09091221
case acc: 0.07653636
case acc: 0.04837545
case acc: 0.06801628
case acc: 0.039784685
top acc: 0.0267 ::: bot acc: 0.0984
top acc: 0.0536 ::: bot acc: 0.1266
top acc: 0.0307 ::: bot acc: 0.1254
top acc: 0.0187 ::: bot acc: 0.0760
top acc: 0.0228 ::: bot acc: 0.1178
top acc: 0.0477 ::: bot acc: 0.0648
current epoch: 8
train loss is 0.075734
average val loss: 0.044855, accuracy: 0.0432
average test loss: 0.042824, accuracy: 0.0440
case acc: 0.041747622
case acc: 0.051274832
case acc: 0.049011473
case acc: 0.027640246
case acc: 0.046133384
case acc: 0.048266493
top acc: 0.0406 ::: bot acc: 0.0614
top acc: 0.0183 ::: bot acc: 0.0849
top acc: 0.0250 ::: bot acc: 0.0871
top acc: 0.0147 ::: bot acc: 0.0472
top acc: 0.0169 ::: bot acc: 0.0881
top acc: 0.0756 ::: bot acc: 0.0400
current epoch: 9
train loss is 0.065441
average val loss: 0.041148, accuracy: 0.0396
average test loss: 0.038500, accuracy: 0.0391
case acc: 0.03831184
case acc: 0.03736783
case acc: 0.041624673
case acc: 0.024850767
case acc: 0.041625097
case acc: 0.05087903
top acc: 0.0552 ::: bot acc: 0.0468
top acc: 0.0158 ::: bot acc: 0.0655
top acc: 0.0361 ::: bot acc: 0.0705
top acc: 0.0222 ::: bot acc: 0.0393
top acc: 0.0221 ::: bot acc: 0.0788
top acc: 0.0816 ::: bot acc: 0.0359
current epoch: 10
train loss is 0.060235
average val loss: 0.039854, accuracy: 0.0381
average test loss: 0.036913, accuracy: 0.0365
case acc: 0.03751172
case acc: 0.028231997
case acc: 0.03893047
case acc: 0.023321578
case acc: 0.03775858
case acc: 0.053514324
top acc: 0.0688 ::: bot acc: 0.0330
top acc: 0.0263 ::: bot acc: 0.0468
top acc: 0.0520 ::: bot acc: 0.0544
top acc: 0.0309 ::: bot acc: 0.0309
top acc: 0.0319 ::: bot acc: 0.0681
top acc: 0.0871 ::: bot acc: 0.0326
current epoch: 11
train loss is 0.056851
average val loss: 0.040500, accuracy: 0.0386
average test loss: 0.037821, accuracy: 0.0366
case acc: 0.039007533
case acc: 0.026597712
case acc: 0.039517008
case acc: 0.023163883
case acc: 0.03611674
case acc: 0.05519045
top acc: 0.0795 ::: bot acc: 0.0221
top acc: 0.0421 ::: bot acc: 0.0310
top acc: 0.0656 ::: bot acc: 0.0408
top acc: 0.0381 ::: bot acc: 0.0239
top acc: 0.0417 ::: bot acc: 0.0582
top acc: 0.0903 ::: bot acc: 0.0311
current epoch: 12
train loss is 0.055230
average val loss: 0.044716, accuracy: 0.0430
average test loss: 0.042827, accuracy: 0.0412
case acc: 0.04523363
case acc: 0.03339037
case acc: 0.04538818
case acc: 0.025869938
case acc: 0.03734586
case acc: 0.060025163
top acc: 0.0945 ::: bot acc: 0.0114
top acc: 0.0621 ::: bot acc: 0.0149
top acc: 0.0836 ::: bot acc: 0.0248
top acc: 0.0509 ::: bot acc: 0.0122
top acc: 0.0578 ::: bot acc: 0.0421
top acc: 0.0984 ::: bot acc: 0.0293
current epoch: 13
train loss is 0.055072
average val loss: 0.050690, accuracy: 0.0492
average test loss: 0.049712, accuracy: 0.0485
case acc: 0.05316115
case acc: 0.044096068
case acc: 0.054625154
case acc: 0.032667775
case acc: 0.04273926
case acc: 0.06373505
top acc: 0.1060 ::: bot acc: 0.0117
top acc: 0.0788 ::: bot acc: 0.0132
top acc: 0.0991 ::: bot acc: 0.0212
top acc: 0.0621 ::: bot acc: 0.0102
top acc: 0.0730 ::: bot acc: 0.0285
top acc: 0.1040 ::: bot acc: 0.0291
current epoch: 14
train loss is 0.055173
average val loss: 0.060298, accuracy: 0.0589
average test loss: 0.060409, accuracy: 0.0599
case acc: 0.06467711
case acc: 0.059448503
case acc: 0.068046406
case acc: 0.044580244
case acc: 0.05346557
case acc: 0.0693367
top acc: 0.1191 ::: bot acc: 0.0202
top acc: 0.0967 ::: bot acc: 0.0235
top acc: 0.1166 ::: bot acc: 0.0262
top acc: 0.0763 ::: bot acc: 0.0173
top acc: 0.0923 ::: bot acc: 0.0217
top acc: 0.1117 ::: bot acc: 0.0302
current epoch: 15
train loss is 0.056135
average val loss: 0.064155, accuracy: 0.0631
average test loss: 0.064642, accuracy: 0.0644
case acc: 0.06653398
case acc: 0.065607004
case acc: 0.074291706
case acc: 0.05006229
case acc: 0.061542578
case acc: 0.068313465
top acc: 0.1211 ::: bot acc: 0.0216
top acc: 0.1029 ::: bot acc: 0.0295
top acc: 0.1239 ::: bot acc: 0.0303
top acc: 0.0822 ::: bot acc: 0.0220
top acc: 0.1043 ::: bot acc: 0.0219
top acc: 0.1102 ::: bot acc: 0.0301
current epoch: 16
train loss is 0.056167
average val loss: 0.063863, accuracy: 0.0634
average test loss: 0.064341, accuracy: 0.0641
case acc: 0.062283788
case acc: 0.0640126
case acc: 0.07485598
case acc: 0.051692747
case acc: 0.06744517
case acc: 0.06412242
top acc: 0.1165 ::: bot acc: 0.0181
top acc: 0.1013 ::: bot acc: 0.0279
top acc: 0.1245 ::: bot acc: 0.0307
top acc: 0.0840 ::: bot acc: 0.0234
top acc: 0.1122 ::: bot acc: 0.0236
top acc: 0.1044 ::: bot acc: 0.0293
current epoch: 17
train loss is 0.056338
average val loss: 0.060415, accuracy: 0.0606
average test loss: 0.060591, accuracy: 0.0601
case acc: 0.05415843
case acc: 0.056053594
case acc: 0.07030295
case acc: 0.05066453
case acc: 0.07071844
case acc: 0.058997337
top acc: 0.1074 ::: bot acc: 0.0121
top acc: 0.0931 ::: bot acc: 0.0205
top acc: 0.1192 ::: bot acc: 0.0276
top acc: 0.0829 ::: bot acc: 0.0225
top acc: 0.1164 ::: bot acc: 0.0250
top acc: 0.0966 ::: bot acc: 0.0295
current epoch: 18
train loss is 0.056197
average val loss: 0.047047, accuracy: 0.0480
average test loss: 0.045685, accuracy: 0.0454
case acc: 0.03986297
case acc: 0.035601463
case acc: 0.05222889
case acc: 0.0358548
case acc: 0.060772784
case acc: 0.048147652
top acc: 0.0819 ::: bot acc: 0.0204
top acc: 0.0659 ::: bot acc: 0.0137
top acc: 0.0953 ::: bot acc: 0.0215
top acc: 0.0663 ::: bot acc: 0.0113
top acc: 0.1032 ::: bot acc: 0.0217
top acc: 0.0753 ::: bot acc: 0.0401
current epoch: 19
train loss is 0.050324
average val loss: 0.039033, accuracy: 0.0403
average test loss: 0.036068, accuracy: 0.0362
case acc: 0.038817666
case acc: 0.026914572
case acc: 0.03952666
case acc: 0.023937665
case acc: 0.047975563
case acc: 0.04025171
top acc: 0.0526 ::: bot acc: 0.0499
top acc: 0.0331 ::: bot acc: 0.0401
top acc: 0.0649 ::: bot acc: 0.0416
top acc: 0.0452 ::: bot acc: 0.0171
top acc: 0.0830 ::: bot acc: 0.0240
top acc: 0.0517 ::: bot acc: 0.0607
current epoch: 20
train loss is 0.048321
average val loss: 0.042514, accuracy: 0.0436
average test loss: 0.040153, accuracy: 0.0407
case acc: 0.050301522
case acc: 0.04418048
case acc: 0.043395385
case acc: 0.026077634
case acc: 0.036803186
case acc: 0.043487452
top acc: 0.0273 ::: bot acc: 0.0811
top acc: 0.0158 ::: bot acc: 0.0758
top acc: 0.0304 ::: bot acc: 0.0763
top acc: 0.0197 ::: bot acc: 0.0424
top acc: 0.0559 ::: bot acc: 0.0439
top acc: 0.0272 ::: bot acc: 0.0857
current epoch: 21
train loss is 0.053848
average val loss: 0.068816, accuracy: 0.0696
average test loss: 0.068779, accuracy: 0.0691
case acc: 0.08199869
case acc: 0.08776297
case acc: 0.07570566
case acc: 0.054293543
case acc: 0.046639234
case acc: 0.068307966
top acc: 0.0372 ::: bot acc: 0.1235
top acc: 0.0507 ::: bot acc: 0.1236
top acc: 0.0301 ::: bot acc: 0.1246
top acc: 0.0229 ::: bot acc: 0.0828
top acc: 0.0166 ::: bot acc: 0.0891
top acc: 0.0280 ::: bot acc: 0.1223
current epoch: 22
train loss is 0.062345
average val loss: 0.074363, accuracy: 0.0749
average test loss: 0.074547, accuracy: 0.0746
case acc: 0.084472135
case acc: 0.095360026
case acc: 0.08451785
case acc: 0.06029891
case acc: 0.055038117
case acc: 0.068037
top acc: 0.0388 ::: bot acc: 0.1264
top acc: 0.0583 ::: bot acc: 0.1311
top acc: 0.0354 ::: bot acc: 0.1352
top acc: 0.0282 ::: bot acc: 0.0891
top acc: 0.0153 ::: bot acc: 0.1022
top acc: 0.0278 ::: bot acc: 0.1220
current epoch: 23
train loss is 0.062412
average val loss: 0.063021, accuracy: 0.0632
average test loss: 0.062729, accuracy: 0.0629
case acc: 0.06882602
case acc: 0.07867627
case acc: 0.074449785
case acc: 0.050080016
case acc: 0.05205287
case acc: 0.053549506
top acc: 0.0297 ::: bot acc: 0.1074
top acc: 0.0417 ::: bot acc: 0.1145
top acc: 0.0296 ::: bot acc: 0.1230
top acc: 0.0195 ::: bot acc: 0.0782
top acc: 0.0148 ::: bot acc: 0.0980
top acc: 0.0213 ::: bot acc: 0.1036
current epoch: 24
train loss is 0.057831
average val loss: 0.048284, accuracy: 0.0481
average test loss: 0.046830, accuracy: 0.0470
case acc: 0.049973153
case acc: 0.052758373
case acc: 0.05726853
case acc: 0.03583326
case acc: 0.044660144
case acc: 0.041654117
top acc: 0.0273 ::: bot acc: 0.0804
top acc: 0.0192 ::: bot acc: 0.0868
top acc: 0.0241 ::: bot acc: 0.1002
top acc: 0.0115 ::: bot acc: 0.0609
top acc: 0.0177 ::: bot acc: 0.0856
top acc: 0.0317 ::: bot acc: 0.0807
current epoch: 25
train loss is 0.048777
average val loss: 0.042428, accuracy: 0.0421
average test loss: 0.040029, accuracy: 0.0401
case acc: 0.04225771
case acc: 0.038500242
case acc: 0.047450546
case acc: 0.030472558
case acc: 0.04207882
case acc: 0.03981763
top acc: 0.0395 ::: bot acc: 0.0629
top acc: 0.0157 ::: bot acc: 0.0673
top acc: 0.0254 ::: bot acc: 0.0849
top acc: 0.0122 ::: bot acc: 0.0526
top acc: 0.0206 ::: bot acc: 0.0804
top acc: 0.0436 ::: bot acc: 0.0689
current epoch: 26
train loss is 0.043045
average val loss: 0.038644, accuracy: 0.0379
average test loss: 0.035416, accuracy: 0.0350
case acc: 0.0378094
case acc: 0.027403757
case acc: 0.04032497
case acc: 0.025115715
case acc: 0.03776873
case acc: 0.041330114
top acc: 0.0600 ::: bot acc: 0.0425
top acc: 0.0284 ::: bot acc: 0.0441
top acc: 0.0415 ::: bot acc: 0.0654
top acc: 0.0225 ::: bot acc: 0.0394
top acc: 0.0311 ::: bot acc: 0.0688
top acc: 0.0580 ::: bot acc: 0.0543
current epoch: 27
train loss is 0.041483
average val loss: 0.040216, accuracy: 0.0391
average test loss: 0.037641, accuracy: 0.0365
case acc: 0.040594332
case acc: 0.030430451
case acc: 0.040095635
case acc: 0.023239726
case acc: 0.035930578
case acc: 0.048824407
top acc: 0.0847 ::: bot acc: 0.0180
top acc: 0.0563 ::: bot acc: 0.0174
top acc: 0.0669 ::: bot acc: 0.0401
top acc: 0.0421 ::: bot acc: 0.0198
top acc: 0.0511 ::: bot acc: 0.0488
top acc: 0.0770 ::: bot acc: 0.0386
current epoch: 28
train loss is 0.044534
average val loss: 0.052399, accuracy: 0.0510
average test loss: 0.051685, accuracy: 0.0510
case acc: 0.05799461
case acc: 0.051643007
case acc: 0.053487934
case acc: 0.036678035
case acc: 0.045788527
case acc: 0.060598377
top acc: 0.1123 ::: bot acc: 0.0145
top acc: 0.0878 ::: bot acc: 0.0175
top acc: 0.0972 ::: bot acc: 0.0212
top acc: 0.0674 ::: bot acc: 0.0119
top acc: 0.0790 ::: bot acc: 0.0255
top acc: 0.0992 ::: bot acc: 0.0290
current epoch: 29
train loss is 0.049655
average val loss: 0.065918, accuracy: 0.0648
average test loss: 0.066573, accuracy: 0.0666
case acc: 0.07391725
case acc: 0.07204771
case acc: 0.070355356
case acc: 0.053356938
case acc: 0.06026087
case acc: 0.06963526
top acc: 0.1293 ::: bot acc: 0.0282
top acc: 0.1089 ::: bot acc: 0.0364
top acc: 0.1193 ::: bot acc: 0.0275
top acc: 0.0859 ::: bot acc: 0.0249
top acc: 0.1024 ::: bot acc: 0.0217
top acc: 0.1121 ::: bot acc: 0.0301
current epoch: 30
train loss is 0.054237
average val loss: 0.065191, accuracy: 0.0645
average test loss: 0.065796, accuracy: 0.0658
case acc: 0.06924474
case acc: 0.06985865
case acc: 0.07117072
case acc: 0.05405858
case acc: 0.06533686
case acc: 0.06518395
top acc: 0.1243 ::: bot acc: 0.0240
top acc: 0.1067 ::: bot acc: 0.0343
top acc: 0.1203 ::: bot acc: 0.0279
top acc: 0.0866 ::: bot acc: 0.0255
top acc: 0.1093 ::: bot acc: 0.0229
top acc: 0.1060 ::: bot acc: 0.0292
current epoch: 31
train loss is 0.055352
average val loss: 0.050814, accuracy: 0.0510
average test loss: 0.049961, accuracy: 0.0496
case acc: 0.048067376
case acc: 0.046394527
case acc: 0.05469236
case acc: 0.039078206
case acc: 0.0571896
case acc: 0.05193855
top acc: 0.0994 ::: bot acc: 0.0104
top acc: 0.0816 ::: bot acc: 0.0144
top acc: 0.0991 ::: bot acc: 0.0211
top acc: 0.0701 ::: bot acc: 0.0136
top acc: 0.0979 ::: bot acc: 0.0215
top acc: 0.0837 ::: bot acc: 0.0344
current epoch: 32
train loss is 0.048197
average val loss: 0.041600, accuracy: 0.0423
average test loss: 0.039378, accuracy: 0.0391
case acc: 0.03883548
case acc: 0.030378778
case acc: 0.04284642
case acc: 0.028395643
case acc: 0.04999601
case acc: 0.044350535
top acc: 0.0767 ::: bot acc: 0.0261
top acc: 0.0562 ::: bot acc: 0.0176
top acc: 0.0773 ::: bot acc: 0.0297
top acc: 0.0561 ::: bot acc: 0.0096
top acc: 0.0865 ::: bot acc: 0.0229
top acc: 0.0663 ::: bot acc: 0.0467
current epoch: 33
train loss is 0.041417
average val loss: 0.038040, accuracy: 0.0389
average test loss: 0.034789, accuracy: 0.0351
case acc: 0.03844101
case acc: 0.026796352
case acc: 0.038991146
case acc: 0.023272455
case acc: 0.04283139
case acc: 0.04005765
top acc: 0.0563 ::: bot acc: 0.0467
top acc: 0.0319 ::: bot acc: 0.0406
top acc: 0.0557 ::: bot acc: 0.0515
top acc: 0.0425 ::: bot acc: 0.0193
top acc: 0.0732 ::: bot acc: 0.0283
top acc: 0.0518 ::: bot acc: 0.0603
current epoch: 34
train loss is 0.039897
average val loss: 0.040779, accuracy: 0.0415
average test loss: 0.038052, accuracy: 0.0385
case acc: 0.045202598
case acc: 0.03960356
case acc: 0.044381198
case acc: 0.025067363
case acc: 0.03589661
case acc: 0.04104742
top acc: 0.0330 ::: bot acc: 0.0708
top acc: 0.0155 ::: bot acc: 0.0689
top acc: 0.0287 ::: bot acc: 0.0788
top acc: 0.0223 ::: bot acc: 0.0395
top acc: 0.0509 ::: bot acc: 0.0490
top acc: 0.0331 ::: bot acc: 0.0788
current epoch: 35
train loss is 0.043321
average val loss: 0.051589, accuracy: 0.0519
average test loss: 0.050484, accuracy: 0.0508
case acc: 0.0577866
case acc: 0.06039148
case acc: 0.061401777
case acc: 0.03620448
case acc: 0.040121384
case acc: 0.048973877
top acc: 0.0258 ::: bot acc: 0.0930
top acc: 0.0243 ::: bot acc: 0.0956
top acc: 0.0245 ::: bot acc: 0.1062
top acc: 0.0115 ::: bot acc: 0.0613
top acc: 0.0247 ::: bot acc: 0.0755
top acc: 0.0218 ::: bot acc: 0.0965
current epoch: 36
train loss is 0.047751
average val loss: 0.055011, accuracy: 0.0550
average test loss: 0.054183, accuracy: 0.0544
case acc: 0.059119076
case acc: 0.065466315
case acc: 0.06747922
case acc: 0.040382244
case acc: 0.04542046
case acc: 0.048636325
top acc: 0.0260 ::: bot acc: 0.0949
top acc: 0.0289 ::: bot acc: 0.1009
top acc: 0.0266 ::: bot acc: 0.1143
top acc: 0.0130 ::: bot acc: 0.0668
top acc: 0.0171 ::: bot acc: 0.0871
top acc: 0.0220 ::: bot acc: 0.0959
current epoch: 37
train loss is 0.049988
average val loss: 0.049025, accuracy: 0.0487
average test loss: 0.047646, accuracy: 0.0478
case acc: 0.05017491
case acc: 0.053685516
case acc: 0.060842387
case acc: 0.035062518
case acc: 0.044695392
case acc: 0.042272873
top acc: 0.0274 ::: bot acc: 0.0809
top acc: 0.0196 ::: bot acc: 0.0878
top acc: 0.0243 ::: bot acc: 0.1055
top acc: 0.0112 ::: bot acc: 0.0598
top acc: 0.0176 ::: bot acc: 0.0858
top acc: 0.0291 ::: bot acc: 0.0830
current epoch: 38
train loss is 0.047384
average val loss: 0.040583, accuracy: 0.0400
average test loss: 0.037791, accuracy: 0.0380
case acc: 0.04060123
case acc: 0.035224766
case acc: 0.04676132
case acc: 0.02653151
case acc: 0.039363846
case acc: 0.039671585
top acc: 0.0466 ::: bot acc: 0.0565
top acc: 0.0164 ::: bot acc: 0.0619
top acc: 0.0257 ::: bot acc: 0.0839
top acc: 0.0181 ::: bot acc: 0.0437
top acc: 0.0265 ::: bot acc: 0.0735
top acc: 0.0491 ::: bot acc: 0.0629
current epoch: 39
train loss is 0.041740
average val loss: 0.038120, accuracy: 0.0372
average test loss: 0.034857, accuracy: 0.0345
case acc: 0.037785918
case acc: 0.02619621
case acc: 0.039965056
case acc: 0.023093063
case acc: 0.036201283
case acc: 0.04395166
top acc: 0.0686 ::: bot acc: 0.0345
top acc: 0.0353 ::: bot acc: 0.0369
top acc: 0.0452 ::: bot acc: 0.0622
top acc: 0.0327 ::: bot acc: 0.0291
top acc: 0.0395 ::: bot acc: 0.0604
top acc: 0.0655 ::: bot acc: 0.0470
current epoch: 40
train loss is 0.040756
average val loss: 0.043494, accuracy: 0.0425
average test loss: 0.041563, accuracy: 0.0407
case acc: 0.045837864
case acc: 0.03597537
case acc: 0.04204116
case acc: 0.027717035
case acc: 0.03886205
case acc: 0.053505674
top acc: 0.0959 ::: bot acc: 0.0113
top acc: 0.0665 ::: bot acc: 0.0133
top acc: 0.0744 ::: bot acc: 0.0330
top acc: 0.0551 ::: bot acc: 0.0095
top acc: 0.0631 ::: bot acc: 0.0368
top acc: 0.0869 ::: bot acc: 0.0322
current epoch: 41
train loss is 0.043425
average val loss: 0.056661, accuracy: 0.0555
average test loss: 0.056485, accuracy: 0.0563
case acc: 0.06374773
case acc: 0.057015814
case acc: 0.0562745
case acc: 0.045073338
case acc: 0.051341947
case acc: 0.06447442
top acc: 0.1189 ::: bot acc: 0.0189
top acc: 0.0937 ::: bot acc: 0.0218
top acc: 0.1015 ::: bot acc: 0.0211
top acc: 0.0769 ::: bot acc: 0.0178
top acc: 0.0886 ::: bot acc: 0.0225
top acc: 0.1047 ::: bot acc: 0.0292
current epoch: 42
train loss is 0.048028
average val loss: 0.060338, accuracy: 0.0595
average test loss: 0.060575, accuracy: 0.0606
case acc: 0.065810785
case acc: 0.062347736
case acc: 0.062182315
case acc: 0.05024558
case acc: 0.05877236
case acc: 0.06408699
top acc: 0.1210 ::: bot acc: 0.0207
top acc: 0.0991 ::: bot acc: 0.0269
top acc: 0.1095 ::: bot acc: 0.0228
top acc: 0.0824 ::: bot acc: 0.0222
top acc: 0.1001 ::: bot acc: 0.0217
top acc: 0.1042 ::: bot acc: 0.0292
current epoch: 43
train loss is 0.049125
average val loss: 0.050682, accuracy: 0.0506
average test loss: 0.049844, accuracy: 0.0495
case acc: 0.05006877
case acc: 0.04636758
case acc: 0.05241824
case acc: 0.039926093
case acc: 0.054870553
case acc: 0.05346202
top acc: 0.1026 ::: bot acc: 0.0104
top acc: 0.0815 ::: bot acc: 0.0144
top acc: 0.0958 ::: bot acc: 0.0212
top acc: 0.0710 ::: bot acc: 0.0141
top acc: 0.0943 ::: bot acc: 0.0217
top acc: 0.0868 ::: bot acc: 0.0323
current epoch: 44
train loss is 0.044514
average val loss: 0.042023, accuracy: 0.0425
average test loss: 0.039946, accuracy: 0.0397
case acc: 0.039779946
case acc: 0.03160023
case acc: 0.04272346
case acc: 0.029519109
case acc: 0.048946314
case acc: 0.04585903
top acc: 0.0816 ::: bot acc: 0.0214
top acc: 0.0587 ::: bot acc: 0.0161
top acc: 0.0769 ::: bot acc: 0.0305
top acc: 0.0579 ::: bot acc: 0.0092
top acc: 0.0846 ::: bot acc: 0.0235
top acc: 0.0700 ::: bot acc: 0.0435
current epoch: 45
train loss is 0.038546
average val loss: 0.037893, accuracy: 0.0386
average test loss: 0.034629, accuracy: 0.0348
case acc: 0.037942328
case acc: 0.026365638
case acc: 0.039053284
case acc: 0.023282353
case acc: 0.04166773
case acc: 0.040328648
top acc: 0.0604 ::: bot acc: 0.0426
top acc: 0.0343 ::: bot acc: 0.0380
top acc: 0.0553 ::: bot acc: 0.0520
top acc: 0.0431 ::: bot acc: 0.0186
top acc: 0.0706 ::: bot acc: 0.0300
top acc: 0.0540 ::: bot acc: 0.0580
current epoch: 46
train loss is 0.036845
average val loss: 0.039128, accuracy: 0.0397
average test loss: 0.035998, accuracy: 0.0365
case acc: 0.041960206
case acc: 0.034510598
case acc: 0.042552136
case acc: 0.023610955
case acc: 0.036259245
case acc: 0.039869975
top acc: 0.0414 ::: bot acc: 0.0616
top acc: 0.0167 ::: bot acc: 0.0607
top acc: 0.0338 ::: bot acc: 0.0735
top acc: 0.0275 ::: bot acc: 0.0342
top acc: 0.0531 ::: bot acc: 0.0467
top acc: 0.0397 ::: bot acc: 0.0722
current epoch: 47
train loss is 0.039226
average val loss: 0.045539, accuracy: 0.0457
average test loss: 0.043739, accuracy: 0.0441
case acc: 0.049370006
case acc: 0.049212664
case acc: 0.054136187
case acc: 0.030186245
case acc: 0.037805185
case acc: 0.04363867
top acc: 0.0278 ::: bot acc: 0.0794
top acc: 0.0177 ::: bot acc: 0.0821
top acc: 0.0238 ::: bot acc: 0.0958
top acc: 0.0121 ::: bot acc: 0.0521
top acc: 0.0310 ::: bot acc: 0.0688
top acc: 0.0265 ::: bot acc: 0.0863
current epoch: 48
train loss is 0.042587
average val loss: 0.046719, accuracy: 0.0466
average test loss: 0.045111, accuracy: 0.0453
case acc: 0.04898813
case acc: 0.050557382
case acc: 0.057203367
case acc: 0.031913288
case acc: 0.040743556
case acc: 0.04262485
top acc: 0.0281 ::: bot acc: 0.0787
top acc: 0.0182 ::: bot acc: 0.0838
top acc: 0.0239 ::: bot acc: 0.1003
top acc: 0.0115 ::: bot acc: 0.0550
top acc: 0.0232 ::: bot acc: 0.0772
top acc: 0.0282 ::: bot acc: 0.0839
current epoch: 49
train loss is 0.043203
average val loss: 0.041897, accuracy: 0.0415
average test loss: 0.039437, accuracy: 0.0397
case acc: 0.04260027
case acc: 0.039706506
case acc: 0.04990893
case acc: 0.027373118
case acc: 0.03921148
case acc: 0.039654203
top acc: 0.0395 ::: bot acc: 0.0635
top acc: 0.0156 ::: bot acc: 0.0689
top acc: 0.0243 ::: bot acc: 0.0892
top acc: 0.0158 ::: bot acc: 0.0461
top acc: 0.0268 ::: bot acc: 0.0732
top acc: 0.0416 ::: bot acc: 0.0703
current epoch: 50
train loss is 0.040620
average val loss: 0.038228, accuracy: 0.0376
average test loss: 0.034897, accuracy: 0.0348
case acc: 0.037976306
case acc: 0.02796713
case acc: 0.041545734
case acc: 0.02331674
case acc: 0.036430996
case acc: 0.04134036
top acc: 0.0601 ::: bot acc: 0.0429
top acc: 0.0257 ::: bot acc: 0.0464
top acc: 0.0371 ::: bot acc: 0.0703
top acc: 0.0293 ::: bot acc: 0.0324
top acc: 0.0376 ::: bot acc: 0.0622
top acc: 0.0582 ::: bot acc: 0.0537
