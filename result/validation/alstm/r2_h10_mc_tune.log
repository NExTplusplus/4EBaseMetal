
		{"drop_out": 0.2, "drop_out_mc": 0.05, "repeat_mc": 50, "hidden": 30, "embedding_size": 5, "batch": 512, "lag": 5}
LME_Co_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5370 5370 5370
1.7082474 -0.6288155 0.24786325 -0.22413088
Validation: 600 600 600
Testing: 774 774 774
pre-processing time: 0.00020265579223632812
the split date is 2010-07-01
train dropout: 0.2 test dropout: 0.05
net initializing with time: 0.06891250610351562
preparing training and testing date with time: 7.152557373046875e-07
current epoch: 1
train loss is 0.007855
average val loss: 0.005142, accuracy: 0.0918
average test loss: 0.006442, accuracy: 0.1055
case acc: 0.14923868
case acc: 0.08590753
case acc: 0.11038019
case acc: 0.09804665
case acc: 0.12363954
case acc: 0.06555212
top acc: 0.1288 ::: bot acc: 0.1714
top acc: 0.1272 ::: bot acc: 0.0429
top acc: 0.0656 ::: bot acc: 0.1612
top acc: 0.0646 ::: bot acc: 0.1324
top acc: 0.0910 ::: bot acc: 0.1567
top acc: 0.0369 ::: bot acc: 0.0996
current epoch: 2
train loss is 0.006721
average val loss: 0.004502, accuracy: 0.0655
average test loss: 0.003703, accuracy: 0.0588
case acc: 0.031395514
case acc: 0.18822569
case acc: 0.035919264
case acc: 0.027249092
case acc: 0.027918532
case acc: 0.04196532
top acc: 0.0127 ::: bot acc: 0.0530
top acc: 0.2301 ::: bot acc: 0.1451
top acc: 0.0514 ::: bot acc: 0.0433
top acc: 0.0458 ::: bot acc: 0.0220
top acc: 0.0167 ::: bot acc: 0.0498
top acc: 0.0651 ::: bot acc: 0.0176
current epoch: 3
train loss is 0.008213
average val loss: 0.013690, accuracy: 0.1474
average test loss: 0.011042, accuracy: 0.1285
case acc: 0.073437124
case acc: 0.27641085
case acc: 0.108785845
case acc: 0.10894085
case acc: 0.077673815
case acc: 0.12602675
top acc: 0.0931 ::: bot acc: 0.0532
top acc: 0.3181 ::: bot acc: 0.2335
top acc: 0.1533 ::: bot acc: 0.0593
top acc: 0.1423 ::: bot acc: 0.0745
top acc: 0.1112 ::: bot acc: 0.0446
top acc: 0.1549 ::: bot acc: 0.0914
current epoch: 4
train loss is 0.011868
average val loss: 0.014622, accuracy: 0.1553
average test loss: 0.011810, accuracy: 0.1363
case acc: 0.08670335
case acc: 0.27646118
case acc: 0.12015782
case acc: 0.116665065
case acc: 0.08835133
case acc: 0.12947263
top acc: 0.1067 ::: bot acc: 0.0655
top acc: 0.3184 ::: bot acc: 0.2331
top acc: 0.1649 ::: bot acc: 0.0705
top acc: 0.1501 ::: bot acc: 0.0824
top acc: 0.1223 ::: bot acc: 0.0553
top acc: 0.1586 ::: bot acc: 0.0945
current epoch: 5
train loss is 0.013254
average val loss: 0.002579, accuracy: 0.0496
average test loss: 0.002300, accuracy: 0.0503
case acc: 0.043908298
case acc: 0.13170387
case acc: 0.035238553
case acc: 0.028140586
case acc: 0.039421916
case acc: 0.023528898
top acc: 0.0230 ::: bot acc: 0.0663
top acc: 0.1730 ::: bot acc: 0.0892
top acc: 0.0310 ::: bot acc: 0.0632
top acc: 0.0170 ::: bot acc: 0.0515
top acc: 0.0092 ::: bot acc: 0.0713
top acc: 0.0231 ::: bot acc: 0.0403
current epoch: 6
train loss is 0.006426
average val loss: 0.002201, accuracy: 0.0558
average test loss: 0.002665, accuracy: 0.0646
case acc: 0.080261245
case acc: 0.08136467
case acc: 0.060064264
case acc: 0.056241278
case acc: 0.066609696
case acc: 0.042947274
top acc: 0.0597 ::: bot acc: 0.1018
top acc: 0.1230 ::: bot acc: 0.0383
top acc: 0.0229 ::: bot acc: 0.1057
top acc: 0.0240 ::: bot acc: 0.0899
top acc: 0.0326 ::: bot acc: 0.1002
top acc: 0.0151 ::: bot acc: 0.0775
current epoch: 7
train loss is 0.003244
average val loss: 0.002269, accuracy: 0.0465
average test loss: 0.001845, accuracy: 0.0440
case acc: 0.031125234
case acc: 0.11994274
case acc: 0.034813095
case acc: 0.025638381
case acc: 0.027323058
case acc: 0.025452087
top acc: 0.0120 ::: bot acc: 0.0522
top acc: 0.1618 ::: bot acc: 0.0769
top acc: 0.0334 ::: bot acc: 0.0617
top acc: 0.0240 ::: bot acc: 0.0427
top acc: 0.0215 ::: bot acc: 0.0463
top acc: 0.0344 ::: bot acc: 0.0299
current epoch: 8
train loss is 0.003580
average val loss: 0.002232, accuracy: 0.0473
average test loss: 0.001752, accuracy: 0.0424
case acc: 0.024705105
case acc: 0.11749921
case acc: 0.034808703
case acc: 0.025007235
case acc: 0.02530683
case acc: 0.02728556
top acc: 0.0075 ::: bot acc: 0.0451
top acc: 0.1597 ::: bot acc: 0.0740
top acc: 0.0374 ::: bot acc: 0.0583
top acc: 0.0286 ::: bot acc: 0.0380
top acc: 0.0309 ::: bot acc: 0.0366
top acc: 0.0398 ::: bot acc: 0.0255
current epoch: 9
train loss is 0.003601
average val loss: 0.001719, accuracy: 0.0415
average test loss: 0.001493, accuracy: 0.0413
case acc: 0.03392558
case acc: 0.0968989
case acc: 0.03779999
case acc: 0.027868167
case acc: 0.027230421
case acc: 0.023958638
top acc: 0.0147 ::: bot acc: 0.0551
top acc: 0.1392 ::: bot acc: 0.0537
top acc: 0.0236 ::: bot acc: 0.0720
top acc: 0.0167 ::: bot acc: 0.0509
top acc: 0.0230 ::: bot acc: 0.0455
top acc: 0.0267 ::: bot acc: 0.0380
current epoch: 10
train loss is 0.003046
average val loss: 0.001536, accuracy: 0.0397
average test loss: 0.001368, accuracy: 0.0400
case acc: 0.034134656
case acc: 0.08763547
case acc: 0.039241962
case acc: 0.028891025
case acc: 0.026535042
case acc: 0.023616992
top acc: 0.0147 ::: bot acc: 0.0557
top acc: 0.1291 ::: bot acc: 0.0443
top acc: 0.0207 ::: bot acc: 0.0757
top acc: 0.0156 ::: bot acc: 0.0528
top acc: 0.0248 ::: bot acc: 0.0431
top acc: 0.0243 ::: bot acc: 0.0392
current epoch: 11
train loss is 0.002749
average val loss: 0.001502, accuracy: 0.0398
average test loss: 0.001267, accuracy: 0.0380
case acc: 0.028494636
case acc: 0.084807046
case acc: 0.037995953
case acc: 0.027140593
case acc: 0.02553493
case acc: 0.024208926
top acc: 0.0105 ::: bot acc: 0.0492
top acc: 0.1268 ::: bot acc: 0.0417
top acc: 0.0228 ::: bot acc: 0.0730
top acc: 0.0192 ::: bot acc: 0.0484
top acc: 0.0315 ::: bot acc: 0.0366
top acc: 0.0290 ::: bot acc: 0.0354
current epoch: 12
train loss is 0.002594
average val loss: 0.001501, accuracy: 0.0405
average test loss: 0.001195, accuracy: 0.0366
case acc: 0.023288293
case acc: 0.082779124
case acc: 0.0367081
case acc: 0.026086448
case acc: 0.025480414
case acc: 0.025474474
top acc: 0.0074 ::: bot acc: 0.0429
top acc: 0.1245 ::: bot acc: 0.0397
top acc: 0.0255 ::: bot acc: 0.0694
top acc: 0.0236 ::: bot acc: 0.0442
top acc: 0.0376 ::: bot acc: 0.0303
top acc: 0.0339 ::: bot acc: 0.0312
current epoch: 13
train loss is 0.002509
average val loss: 0.001406, accuracy: 0.0396
average test loss: 0.001115, accuracy: 0.0356
case acc: 0.022096582
case acc: 0.07710194
case acc: 0.03683239
case acc: 0.025666364
case acc: 0.025910906
case acc: 0.025733434
top acc: 0.0070 ::: bot acc: 0.0416
top acc: 0.1194 ::: bot acc: 0.0334
top acc: 0.0251 ::: bot acc: 0.0693
top acc: 0.0236 ::: bot acc: 0.0433
top acc: 0.0398 ::: bot acc: 0.0290
top acc: 0.0338 ::: bot acc: 0.0314
current epoch: 14
train loss is 0.002345
average val loss: 0.001248, accuracy: 0.0374
average test loss: 0.001028, accuracy: 0.0344
case acc: 0.023271535
case acc: 0.06848568
case acc: 0.0378008
case acc: 0.026482321
case acc: 0.025539134
case acc: 0.0247626
top acc: 0.0075 ::: bot acc: 0.0434
top acc: 0.1105 ::: bot acc: 0.0250
top acc: 0.0216 ::: bot acc: 0.0730
top acc: 0.0207 ::: bot acc: 0.0464
top acc: 0.0379 ::: bot acc: 0.0304
top acc: 0.0308 ::: bot acc: 0.0339
current epoch: 15
train loss is 0.002178
average val loss: 0.001214, accuracy: 0.0373
average test loss: 0.000970, accuracy: 0.0334
case acc: 0.021294856
case acc: 0.064746246
case acc: 0.037669737
case acc: 0.026155626
case acc: 0.02548305
case acc: 0.025121681
top acc: 0.0075 ::: bot acc: 0.0399
top acc: 0.1063 ::: bot acc: 0.0217
top acc: 0.0232 ::: bot acc: 0.0721
top acc: 0.0233 ::: bot acc: 0.0442
top acc: 0.0394 ::: bot acc: 0.0280
top acc: 0.0322 ::: bot acc: 0.0326
current epoch: 16
train loss is 0.002132
average val loss: 0.001173, accuracy: 0.0368
average test loss: 0.000916, accuracy: 0.0324
case acc: 0.01991778
case acc: 0.060790263
case acc: 0.03735534
case acc: 0.025643833
case acc: 0.025666151
case acc: 0.025173806
top acc: 0.0076 ::: bot acc: 0.0381
top acc: 0.1023 ::: bot acc: 0.0183
top acc: 0.0236 ::: bot acc: 0.0716
top acc: 0.0242 ::: bot acc: 0.0425
top acc: 0.0407 ::: bot acc: 0.0270
top acc: 0.0332 ::: bot acc: 0.0313
current epoch: 17
train loss is 0.001993
average val loss: 0.001097, accuracy: 0.0357
average test loss: 0.000868, accuracy: 0.0316
case acc: 0.019683938
case acc: 0.05626559
case acc: 0.0377653
case acc: 0.025631018
case acc: 0.025634024
case acc: 0.024715623
top acc: 0.0074 ::: bot acc: 0.0377
top acc: 0.0971 ::: bot acc: 0.0146
top acc: 0.0234 ::: bot acc: 0.0720
top acc: 0.0231 ::: bot acc: 0.0436
top acc: 0.0399 ::: bot acc: 0.0278
top acc: 0.0312 ::: bot acc: 0.0329
current epoch: 18
train loss is 0.001955
average val loss: 0.001068, accuracy: 0.0354
average test loss: 0.000828, accuracy: 0.0309
case acc: 0.018365582
case acc: 0.053447176
case acc: 0.037480902
case acc: 0.025500497
case acc: 0.025548445
case acc: 0.024945315
top acc: 0.0080 ::: bot acc: 0.0351
top acc: 0.0939 ::: bot acc: 0.0132
top acc: 0.0244 ::: bot acc: 0.0712
top acc: 0.0245 ::: bot acc: 0.0422
top acc: 0.0404 ::: bot acc: 0.0270
top acc: 0.0325 ::: bot acc: 0.0322
current epoch: 19
train loss is 0.001906
average val loss: 0.001070, accuracy: 0.0358
average test loss: 0.000802, accuracy: 0.0304
case acc: 0.017140292
case acc: 0.052007586
case acc: 0.03660194
case acc: 0.025355687
case acc: 0.026141733
case acc: 0.02537689
top acc: 0.0103 ::: bot acc: 0.0321
top acc: 0.0918 ::: bot acc: 0.0124
top acc: 0.0264 ::: bot acc: 0.0688
top acc: 0.0268 ::: bot acc: 0.0402
top acc: 0.0424 ::: bot acc: 0.0258
top acc: 0.0339 ::: bot acc: 0.0308
current epoch: 20
train loss is 0.001847
average val loss: 0.001038, accuracy: 0.0352
average test loss: 0.000767, accuracy: 0.0298
case acc: 0.016694035
case acc: 0.049569715
case acc: 0.036199674
case acc: 0.02513388
case acc: 0.025660696
case acc: 0.025250753
top acc: 0.0118 ::: bot acc: 0.0306
top acc: 0.0890 ::: bot acc: 0.0113
top acc: 0.0271 ::: bot acc: 0.0680
top acc: 0.0276 ::: bot acc: 0.0394
top acc: 0.0415 ::: bot acc: 0.0260
top acc: 0.0334 ::: bot acc: 0.0313
current epoch: 21
train loss is 0.001824
average val loss: 0.001061, accuracy: 0.0360
average test loss: 0.000755, accuracy: 0.0296
case acc: 0.016063387
case acc: 0.049080517
case acc: 0.035556834
case acc: 0.025073163
case acc: 0.026040453
case acc: 0.025722226
top acc: 0.0150 ::: bot acc: 0.0278
top acc: 0.0884 ::: bot acc: 0.0113
top acc: 0.0302 ::: bot acc: 0.0650
top acc: 0.0303 ::: bot acc: 0.0374
top acc: 0.0429 ::: bot acc: 0.0251
top acc: 0.0348 ::: bot acc: 0.0296
current epoch: 22
train loss is 0.001806
average val loss: 0.001053, accuracy: 0.0360
average test loss: 0.000735, accuracy: 0.0293
case acc: 0.015712619
case acc: 0.047837716
case acc: 0.03523843
case acc: 0.024975425
case acc: 0.026164027
case acc: 0.025874315
top acc: 0.0167 ::: bot acc: 0.0259
top acc: 0.0865 ::: bot acc: 0.0115
top acc: 0.0322 ::: bot acc: 0.0635
top acc: 0.0311 ::: bot acc: 0.0360
top acc: 0.0431 ::: bot acc: 0.0251
top acc: 0.0349 ::: bot acc: 0.0299
current epoch: 23
train loss is 0.001793
average val loss: 0.001113, accuracy: 0.0374
average test loss: 0.000734, accuracy: 0.0294
case acc: 0.015445988
case acc: 0.04821185
case acc: 0.034633372
case acc: 0.025148032
case acc: 0.02653568
case acc: 0.0265211
top acc: 0.0207 ::: bot acc: 0.0216
top acc: 0.0869 ::: bot acc: 0.0112
top acc: 0.0358 ::: bot acc: 0.0594
top acc: 0.0348 ::: bot acc: 0.0326
top acc: 0.0449 ::: bot acc: 0.0231
top acc: 0.0369 ::: bot acc: 0.0278
current epoch: 24
train loss is 0.001781
average val loss: 0.001149, accuracy: 0.0382
average test loss: 0.000736, accuracy: 0.0295
case acc: 0.015831297
case acc: 0.04802697
case acc: 0.03465049
case acc: 0.025182776
case acc: 0.026631843
case acc: 0.02697157
top acc: 0.0238 ::: bot acc: 0.0188
top acc: 0.0866 ::: bot acc: 0.0111
top acc: 0.0391 ::: bot acc: 0.0566
top acc: 0.0371 ::: bot acc: 0.0300
top acc: 0.0460 ::: bot acc: 0.0218
top acc: 0.0389 ::: bot acc: 0.0262
current epoch: 25
train loss is 0.001780
average val loss: 0.001202, accuracy: 0.0394
average test loss: 0.000744, accuracy: 0.0299
case acc: 0.01695584
case acc: 0.048182618
case acc: 0.034312107
case acc: 0.025523309
case acc: 0.026957808
case acc: 0.027751712
top acc: 0.0268 ::: bot acc: 0.0163
top acc: 0.0871 ::: bot acc: 0.0112
top acc: 0.0418 ::: bot acc: 0.0531
top acc: 0.0395 ::: bot acc: 0.0277
top acc: 0.0475 ::: bot acc: 0.0199
top acc: 0.0403 ::: bot acc: 0.0254
current epoch: 26
train loss is 0.001764
average val loss: 0.001164, accuracy: 0.0387
average test loss: 0.000720, accuracy: 0.0295
case acc: 0.016724812
case acc: 0.046182796
case acc: 0.034388218
case acc: 0.025434222
case acc: 0.027018309
case acc: 0.02717937
top acc: 0.0269 ::: bot acc: 0.0158
top acc: 0.0844 ::: bot acc: 0.0107
top acc: 0.0422 ::: bot acc: 0.0528
top acc: 0.0391 ::: bot acc: 0.0278
top acc: 0.0469 ::: bot acc: 0.0213
top acc: 0.0391 ::: bot acc: 0.0261
current epoch: 27
train loss is 0.001748
average val loss: 0.001122, accuracy: 0.0379
average test loss: 0.000693, accuracy: 0.0290
case acc: 0.016464006
case acc: 0.044397004
case acc: 0.03439403
case acc: 0.025187338
case acc: 0.026668781
case acc: 0.02671241
top acc: 0.0263 ::: bot acc: 0.0161
top acc: 0.0809 ::: bot acc: 0.0117
top acc: 0.0422 ::: bot acc: 0.0534
top acc: 0.0383 ::: bot acc: 0.0285
top acc: 0.0456 ::: bot acc: 0.0227
top acc: 0.0376 ::: bot acc: 0.0279
current epoch: 28
train loss is 0.001706
average val loss: 0.001096, accuracy: 0.0374
average test loss: 0.000676, accuracy: 0.0288
case acc: 0.016798101
case acc: 0.043029875
case acc: 0.034522835
case acc: 0.025490724
case acc: 0.02645012
case acc: 0.0263148
top acc: 0.0262 ::: bot acc: 0.0166
top acc: 0.0786 ::: bot acc: 0.0123
top acc: 0.0422 ::: bot acc: 0.0531
top acc: 0.0386 ::: bot acc: 0.0288
top acc: 0.0449 ::: bot acc: 0.0230
top acc: 0.0364 ::: bot acc: 0.0286
current epoch: 29
train loss is 0.001683
average val loss: 0.001155, accuracy: 0.0387
average test loss: 0.000690, accuracy: 0.0292
case acc: 0.017591074
case acc: 0.043366704
case acc: 0.034914173
case acc: 0.025637053
case acc: 0.026802674
case acc: 0.026847618
top acc: 0.0287 ::: bot acc: 0.0142
top acc: 0.0795 ::: bot acc: 0.0117
top acc: 0.0450 ::: bot acc: 0.0503
top acc: 0.0408 ::: bot acc: 0.0265
top acc: 0.0465 ::: bot acc: 0.0213
top acc: 0.0377 ::: bot acc: 0.0273
current epoch: 30
train loss is 0.001699
average val loss: 0.001147, accuracy: 0.0384
average test loss: 0.000682, accuracy: 0.0291
case acc: 0.017821578
case acc: 0.042613123
case acc: 0.034908123
case acc: 0.025822364
case acc: 0.026773645
case acc: 0.026572779
top acc: 0.0294 ::: bot acc: 0.0141
top acc: 0.0782 ::: bot acc: 0.0121
top acc: 0.0459 ::: bot acc: 0.0494
top acc: 0.0412 ::: bot acc: 0.0264
top acc: 0.0463 ::: bot acc: 0.0218
top acc: 0.0371 ::: bot acc: 0.0279
current epoch: 31
train loss is 0.001671
average val loss: 0.001172, accuracy: 0.0389
average test loss: 0.000688, accuracy: 0.0293
case acc: 0.018496104
case acc: 0.042664815
case acc: 0.03509873
case acc: 0.026054427
case acc: 0.026872741
case acc: 0.02665448
top acc: 0.0302 ::: bot acc: 0.0135
top acc: 0.0779 ::: bot acc: 0.0126
top acc: 0.0476 ::: bot acc: 0.0479
top acc: 0.0426 ::: bot acc: 0.0252
top acc: 0.0468 ::: bot acc: 0.0209
top acc: 0.0377 ::: bot acc: 0.0274
current epoch: 32
train loss is 0.001666
average val loss: 0.001165, accuracy: 0.0388
average test loss: 0.000679, accuracy: 0.0292
case acc: 0.018795239
case acc: 0.041955333
case acc: 0.03526473
case acc: 0.0259724
case acc: 0.026780568
case acc: 0.026305791
top acc: 0.0309 ::: bot acc: 0.0137
top acc: 0.0767 ::: bot acc: 0.0129
top acc: 0.0483 ::: bot acc: 0.0472
top acc: 0.0425 ::: bot acc: 0.0249
top acc: 0.0467 ::: bot acc: 0.0211
top acc: 0.0370 ::: bot acc: 0.0276
current epoch: 33
train loss is 0.001665
average val loss: 0.001189, accuracy: 0.0393
average test loss: 0.000683, accuracy: 0.0294
case acc: 0.019433953
case acc: 0.04156615
case acc: 0.035481486
case acc: 0.026149081
case acc: 0.026939113
case acc: 0.026589535
top acc: 0.0320 ::: bot acc: 0.0131
top acc: 0.0760 ::: bot acc: 0.0130
top acc: 0.0498 ::: bot acc: 0.0459
top acc: 0.0433 ::: bot acc: 0.0237
top acc: 0.0474 ::: bot acc: 0.0204
top acc: 0.0379 ::: bot acc: 0.0271
current epoch: 34
train loss is 0.001646
average val loss: 0.001177, accuracy: 0.0390
average test loss: 0.000674, accuracy: 0.0292
case acc: 0.019228416
case acc: 0.040940564
case acc: 0.035533983
case acc: 0.026138283
case acc: 0.027076585
case acc: 0.026570546
top acc: 0.0318 ::: bot acc: 0.0131
top acc: 0.0749 ::: bot acc: 0.0137
top acc: 0.0500 ::: bot acc: 0.0454
top acc: 0.0432 ::: bot acc: 0.0238
top acc: 0.0474 ::: bot acc: 0.0206
top acc: 0.0374 ::: bot acc: 0.0280
current epoch: 35
train loss is 0.001616
average val loss: 0.001130, accuracy: 0.0380
average test loss: 0.000648, accuracy: 0.0286
case acc: 0.01843549
case acc: 0.039460134
case acc: 0.035325434
case acc: 0.025686368
case acc: 0.026804421
case acc: 0.025819141
top acc: 0.0302 ::: bot acc: 0.0136
top acc: 0.0720 ::: bot acc: 0.0150
top acc: 0.0489 ::: bot acc: 0.0466
top acc: 0.0419 ::: bot acc: 0.0249
top acc: 0.0463 ::: bot acc: 0.0219
top acc: 0.0353 ::: bot acc: 0.0297
current epoch: 36
train loss is 0.001608
average val loss: 0.001144, accuracy: 0.0384
average test loss: 0.000654, accuracy: 0.0288
case acc: 0.01877915
case acc: 0.039652433
case acc: 0.03550892
case acc: 0.026070021
case acc: 0.026731802
case acc: 0.02587212
top acc: 0.0311 ::: bot acc: 0.0133
top acc: 0.0722 ::: bot acc: 0.0153
top acc: 0.0500 ::: bot acc: 0.0455
top acc: 0.0426 ::: bot acc: 0.0243
top acc: 0.0469 ::: bot acc: 0.0208
top acc: 0.0356 ::: bot acc: 0.0292
current epoch: 37
train loss is 0.001595
average val loss: 0.001094, accuracy: 0.0373
average test loss: 0.000635, accuracy: 0.0283
case acc: 0.018084154
case acc: 0.03854842
case acc: 0.035292875
case acc: 0.02590366
case acc: 0.026606984
case acc: 0.02557123
top acc: 0.0295 ::: bot acc: 0.0140
top acc: 0.0696 ::: bot acc: 0.0169
top acc: 0.0488 ::: bot acc: 0.0467
top acc: 0.0419 ::: bot acc: 0.0257
top acc: 0.0456 ::: bot acc: 0.0228
top acc: 0.0343 ::: bot acc: 0.0309
current epoch: 38
train loss is 0.001574
average val loss: 0.001115, accuracy: 0.0377
average test loss: 0.000636, accuracy: 0.0284
case acc: 0.018393815
case acc: 0.038524486
case acc: 0.035256527
case acc: 0.02601199
case acc: 0.026671462
case acc: 0.02564005
top acc: 0.0302 ::: bot acc: 0.0138
top acc: 0.0694 ::: bot acc: 0.0169
top acc: 0.0494 ::: bot acc: 0.0454
top acc: 0.0427 ::: bot acc: 0.0245
top acc: 0.0461 ::: bot acc: 0.0215
top acc: 0.0348 ::: bot acc: 0.0299
current epoch: 39
train loss is 0.001572
average val loss: 0.001073, accuracy: 0.0369
average test loss: 0.000623, accuracy: 0.0281
case acc: 0.017667532
case acc: 0.0376384
case acc: 0.035372842
case acc: 0.02581638
case acc: 0.02644932
case acc: 0.025638321
top acc: 0.0287 ::: bot acc: 0.0145
top acc: 0.0675 ::: bot acc: 0.0187
top acc: 0.0491 ::: bot acc: 0.0466
top acc: 0.0415 ::: bot acc: 0.0255
top acc: 0.0450 ::: bot acc: 0.0228
top acc: 0.0341 ::: bot acc: 0.0313
current epoch: 40
train loss is 0.001553
average val loss: 0.001097, accuracy: 0.0374
average test loss: 0.000626, accuracy: 0.0282
case acc: 0.018032333
case acc: 0.03755775
case acc: 0.035362933
case acc: 0.02603063
case acc: 0.026606886
case acc: 0.025716223
top acc: 0.0297 ::: bot acc: 0.0141
top acc: 0.0674 ::: bot acc: 0.0187
top acc: 0.0497 ::: bot acc: 0.0455
top acc: 0.0427 ::: bot acc: 0.0242
top acc: 0.0459 ::: bot acc: 0.0221
top acc: 0.0346 ::: bot acc: 0.0303
current epoch: 41
train loss is 0.001547
average val loss: 0.001011, accuracy: 0.0354
average test loss: 0.000597, accuracy: 0.0274
case acc: 0.016683377
case acc: 0.036261063
case acc: 0.03511987
case acc: 0.025551094
case acc: 0.025956923
case acc: 0.024982208
top acc: 0.0262 ::: bot acc: 0.0163
top acc: 0.0638 ::: bot acc: 0.0219
top acc: 0.0473 ::: bot acc: 0.0483
top acc: 0.0402 ::: bot acc: 0.0269
top acc: 0.0428 ::: bot acc: 0.0249
top acc: 0.0321 ::: bot acc: 0.0332
current epoch: 42
train loss is 0.001507
average val loss: 0.000975, accuracy: 0.0346
average test loss: 0.000585, accuracy: 0.0270
case acc: 0.016128052
case acc: 0.035657667
case acc: 0.034866136
case acc: 0.025366755
case acc: 0.025795795
case acc: 0.024444794
top acc: 0.0247 ::: bot acc: 0.0176
top acc: 0.0616 ::: bot acc: 0.0240
top acc: 0.0462 ::: bot acc: 0.0493
top acc: 0.0387 ::: bot acc: 0.0282
top acc: 0.0412 ::: bot acc: 0.0265
top acc: 0.0301 ::: bot acc: 0.0349
current epoch: 43
train loss is 0.001493
average val loss: 0.000942, accuracy: 0.0338
average test loss: 0.000576, accuracy: 0.0268
case acc: 0.015651839
case acc: 0.035122655
case acc: 0.034858573
case acc: 0.025244646
case acc: 0.02558713
case acc: 0.024212334
top acc: 0.0233 ::: bot acc: 0.0188
top acc: 0.0601 ::: bot acc: 0.0255
top acc: 0.0456 ::: bot acc: 0.0501
top acc: 0.0377 ::: bot acc: 0.0291
top acc: 0.0401 ::: bot acc: 0.0276
top acc: 0.0290 ::: bot acc: 0.0360
current epoch: 44
train loss is 0.001482
average val loss: 0.000936, accuracy: 0.0337
average test loss: 0.000573, accuracy: 0.0267
case acc: 0.0155262835
case acc: 0.035083123
case acc: 0.034775544
case acc: 0.025155986
case acc: 0.025248654
case acc: 0.02444351
top acc: 0.0227 ::: bot acc: 0.0194
top acc: 0.0598 ::: bot acc: 0.0263
top acc: 0.0455 ::: bot acc: 0.0499
top acc: 0.0377 ::: bot acc: 0.0292
top acc: 0.0394 ::: bot acc: 0.0278
top acc: 0.0290 ::: bot acc: 0.0367
current epoch: 45
train loss is 0.001471
average val loss: 0.000935, accuracy: 0.0336
average test loss: 0.000573, accuracy: 0.0267
case acc: 0.015447397
case acc: 0.034786172
case acc: 0.034918275
case acc: 0.025105793
case acc: 0.025666688
case acc: 0.024101907
top acc: 0.0225 ::: bot acc: 0.0195
top acc: 0.0590 ::: bot acc: 0.0269
top acc: 0.0456 ::: bot acc: 0.0501
top acc: 0.0374 ::: bot acc: 0.0293
top acc: 0.0394 ::: bot acc: 0.0287
top acc: 0.0283 ::: bot acc: 0.0370
current epoch: 46
train loss is 0.001473
average val loss: 0.000978, accuracy: 0.0346
average test loss: 0.000577, accuracy: 0.0269
case acc: 0.015857022
case acc: 0.03515593
case acc: 0.034969527
case acc: 0.0253135
case acc: 0.025913449
case acc: 0.02421937
top acc: 0.0244 ::: bot acc: 0.0175
top acc: 0.0604 ::: bot acc: 0.0249
top acc: 0.0477 ::: bot acc: 0.0473
top acc: 0.0394 ::: bot acc: 0.0274
top acc: 0.0416 ::: bot acc: 0.0265
top acc: 0.0297 ::: bot acc: 0.0349
current epoch: 47
train loss is 0.001479
average val loss: 0.000972, accuracy: 0.0344
average test loss: 0.000578, accuracy: 0.0269
case acc: 0.015858576
case acc: 0.035050176
case acc: 0.035174813
case acc: 0.025356965
case acc: 0.025757398
case acc: 0.024329174
top acc: 0.0242 ::: bot acc: 0.0182
top acc: 0.0598 ::: bot acc: 0.0257
top acc: 0.0478 ::: bot acc: 0.0478
top acc: 0.0392 ::: bot acc: 0.0276
top acc: 0.0411 ::: bot acc: 0.0267
top acc: 0.0295 ::: bot acc: 0.0355
current epoch: 48
train loss is 0.001468
average val loss: 0.000979, accuracy: 0.0346
average test loss: 0.000579, accuracy: 0.0270
case acc: 0.015910894
case acc: 0.03501388
case acc: 0.035141837
case acc: 0.025587466
case acc: 0.025828512
case acc: 0.024306675
top acc: 0.0244 ::: bot acc: 0.0178
top acc: 0.0599 ::: bot acc: 0.0256
top acc: 0.0482 ::: bot acc: 0.0471
top acc: 0.0395 ::: bot acc: 0.0274
top acc: 0.0417 ::: bot acc: 0.0264
top acc: 0.0297 ::: bot acc: 0.0354
current epoch: 49
train loss is 0.001470
average val loss: 0.000983, accuracy: 0.0346
average test loss: 0.000578, accuracy: 0.0269
case acc: 0.01580048
case acc: 0.03480414
case acc: 0.035165317
case acc: 0.025481252
case acc: 0.025924025
case acc: 0.02419722
top acc: 0.0238 ::: bot acc: 0.0182
top acc: 0.0593 ::: bot acc: 0.0259
top acc: 0.0483 ::: bot acc: 0.0470
top acc: 0.0399 ::: bot acc: 0.0269
top acc: 0.0419 ::: bot acc: 0.0261
top acc: 0.0298 ::: bot acc: 0.0349
current epoch: 50
train loss is 0.001458
average val loss: 0.000955, accuracy: 0.0340
average test loss: 0.000571, accuracy: 0.0267
case acc: 0.0154859405
case acc: 0.034547467
case acc: 0.034998443
case acc: 0.025382243
case acc: 0.025767174
case acc: 0.02427894
top acc: 0.0224 ::: bot acc: 0.0196
top acc: 0.0579 ::: bot acc: 0.0276
top acc: 0.0473 ::: bot acc: 0.0479
top acc: 0.0389 ::: bot acc: 0.0279
top acc: 0.0413 ::: bot acc: 0.0267
top acc: 0.0291 ::: bot acc: 0.0362
LME_Co_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5400 5400 5400
1.7082474 -0.6288155 0.17559324 -0.22413088
Validation: 600 600 600
Testing: 744 744 744
pre-processing time: 0.0003676414489746094
the split date is 2011-01-01
train dropout: 0.2 test dropout: 0.05
net initializing with time: 0.0025680065155029297
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.007684
average val loss: 0.005517, accuracy: 0.0971
average test loss: 0.005810, accuracy: 0.0978
case acc: 0.12854287
case acc: 0.098421924
case acc: 0.10021845
case acc: 0.09891593
case acc: 0.10873015
case acc: 0.051685147
top acc: 0.0829 ::: bot acc: 0.1727
top acc: 0.1317 ::: bot acc: 0.0636
top acc: 0.0402 ::: bot acc: 0.1588
top acc: 0.0678 ::: bot acc: 0.1326
top acc: 0.0597 ::: bot acc: 0.1540
top acc: 0.0228 ::: bot acc: 0.0901
current epoch: 2
train loss is 0.006674
average val loss: 0.003967, accuracy: 0.0601
average test loss: 0.004525, accuracy: 0.0676
case acc: 0.03448856
case acc: 0.19783358
case acc: 0.047085825
case acc: 0.026560381
case acc: 0.035995074
case acc: 0.06342545
top acc: 0.0331 ::: bot acc: 0.0554
top acc: 0.2307 ::: bot acc: 0.1635
top acc: 0.0820 ::: bot acc: 0.0443
top acc: 0.0422 ::: bot acc: 0.0259
top acc: 0.0454 ::: bot acc: 0.0494
top acc: 0.1171 ::: bot acc: 0.0163
current epoch: 3
train loss is 0.008399
average val loss: 0.011727, accuracy: 0.1341
average test loss: 0.012494, accuracy: 0.1369
case acc: 0.087377064
case acc: 0.28237402
case acc: 0.11577758
case acc: 0.102086484
case acc: 0.08742428
case acc: 0.14642797
top acc: 0.1322 ::: bot acc: 0.0438
top acc: 0.3159 ::: bot acc: 0.2488
top acc: 0.1810 ::: bot acc: 0.0558
top acc: 0.1347 ::: bot acc: 0.0677
top acc: 0.1371 ::: bot acc: 0.0423
top acc: 0.2021 ::: bot acc: 0.0941
current epoch: 4
train loss is 0.011893
average val loss: 0.010641, accuracy: 0.1277
average test loss: 0.011388, accuracy: 0.1306
case acc: 0.086762734
case acc: 0.26813203
case acc: 0.112952754
case acc: 0.09552028
case acc: 0.08457976
case acc: 0.13564426
top acc: 0.1321 ::: bot acc: 0.0432
top acc: 0.3013 ::: bot acc: 0.2342
top acc: 0.1774 ::: bot acc: 0.0536
top acc: 0.1286 ::: bot acc: 0.0607
top acc: 0.1342 ::: bot acc: 0.0397
top acc: 0.1914 ::: bot acc: 0.0835
current epoch: 5
train loss is 0.012716
average val loss: 0.002251, accuracy: 0.0524
average test loss: 0.002653, accuracy: 0.0595
case acc: 0.049356617
case acc: 0.123005845
case acc: 0.051348027
case acc: 0.044465568
case acc: 0.049075812
case acc: 0.039575856
top acc: 0.0169 ::: bot acc: 0.0868
top acc: 0.1568 ::: bot acc: 0.0891
top acc: 0.0439 ::: bot acc: 0.0844
top acc: 0.0227 ::: bot acc: 0.0741
top acc: 0.0166 ::: bot acc: 0.0853
top acc: 0.0568 ::: bot acc: 0.0509
current epoch: 6
train loss is 0.005825
average val loss: 0.002571, accuracy: 0.0634
average test loss: 0.002889, accuracy: 0.0665
case acc: 0.07009745
case acc: 0.08353824
case acc: 0.06731863
case acc: 0.0698447
case acc: 0.062015004
case acc: 0.04626104
top acc: 0.0264 ::: bot acc: 0.1129
top acc: 0.1174 ::: bot acc: 0.0496
top acc: 0.0279 ::: bot acc: 0.1161
top acc: 0.0417 ::: bot acc: 0.1025
top acc: 0.0181 ::: bot acc: 0.1043
top acc: 0.0296 ::: bot acc: 0.0787
current epoch: 7
train loss is 0.003079
average val loss: 0.001797, accuracy: 0.0436
average test loss: 0.002249, accuracy: 0.0524
case acc: 0.038244016
case acc: 0.11942268
case acc: 0.04805913
case acc: 0.032003295
case acc: 0.036787953
case acc: 0.039940175
top acc: 0.0253 ::: bot acc: 0.0664
top acc: 0.1531 ::: bot acc: 0.0851
top acc: 0.0528 ::: bot acc: 0.0742
top acc: 0.0172 ::: bot acc: 0.0577
top acc: 0.0424 ::: bot acc: 0.0526
top acc: 0.0733 ::: bot acc: 0.0322
current epoch: 8
train loss is 0.003358
average val loss: 0.001661, accuracy: 0.0418
average test loss: 0.002126, accuracy: 0.0509
case acc: 0.03634776
case acc: 0.114541136
case acc: 0.04755427
case acc: 0.0303961
case acc: 0.035499364
case acc: 0.04080129
top acc: 0.0294 ::: bot acc: 0.0611
top acc: 0.1481 ::: bot acc: 0.0800
top acc: 0.0534 ::: bot acc: 0.0730
top acc: 0.0163 ::: bot acc: 0.0558
top acc: 0.0490 ::: bot acc: 0.0450
top acc: 0.0755 ::: bot acc: 0.0305
current epoch: 9
train loss is 0.003327
average val loss: 0.001449, accuracy: 0.0411
average test loss: 0.001888, accuracy: 0.0504
case acc: 0.04107782
case acc: 0.0937471
case acc: 0.052233383
case acc: 0.039723154
case acc: 0.036869735
case acc: 0.03892402
top acc: 0.0221 ::: bot acc: 0.0720
top acc: 0.1269 ::: bot acc: 0.0598
top acc: 0.0408 ::: bot acc: 0.0870
top acc: 0.0190 ::: bot acc: 0.0688
top acc: 0.0404 ::: bot acc: 0.0540
top acc: 0.0626 ::: bot acc: 0.0439
current epoch: 10
train loss is 0.002797
average val loss: 0.001326, accuracy: 0.0394
average test loss: 0.001767, accuracy: 0.0489
case acc: 0.039753597
case acc: 0.08717598
case acc: 0.05223633
case acc: 0.0392855
case acc: 0.03624202
case acc: 0.038617022
top acc: 0.0236 ::: bot acc: 0.0691
top acc: 0.1205 ::: bot acc: 0.0538
top acc: 0.0388 ::: bot acc: 0.0879
top acc: 0.0187 ::: bot acc: 0.0684
top acc: 0.0450 ::: bot acc: 0.0491
top acc: 0.0632 ::: bot acc: 0.0428
current epoch: 11
train loss is 0.002486
average val loss: 0.001231, accuracy: 0.0374
average test loss: 0.001682, accuracy: 0.0473
case acc: 0.037136786
case acc: 0.08443233
case acc: 0.051457174
case acc: 0.036270324
case acc: 0.035590306
case acc: 0.038940113
top acc: 0.0285 ::: bot acc: 0.0630
top acc: 0.1172 ::: bot acc: 0.0506
top acc: 0.0419 ::: bot acc: 0.0854
top acc: 0.0174 ::: bot acc: 0.0645
top acc: 0.0522 ::: bot acc: 0.0427
top acc: 0.0671 ::: bot acc: 0.0390
current epoch: 12
train loss is 0.002346
average val loss: 0.001152, accuracy: 0.0360
average test loss: 0.001598, accuracy: 0.0460
case acc: 0.0360548
case acc: 0.07998969
case acc: 0.05092467
case acc: 0.034836177
case acc: 0.035301026
case acc: 0.0390133
top acc: 0.0319 ::: bot acc: 0.0597
top acc: 0.1134 ::: bot acc: 0.0464
top acc: 0.0424 ::: bot acc: 0.0840
top acc: 0.0168 ::: bot acc: 0.0625
top acc: 0.0552 ::: bot acc: 0.0389
top acc: 0.0686 ::: bot acc: 0.0371
current epoch: 13
train loss is 0.002213
average val loss: 0.001075, accuracy: 0.0347
average test loss: 0.001533, accuracy: 0.0449
case acc: 0.034633268
case acc: 0.07633649
case acc: 0.049948134
case acc: 0.033499006
case acc: 0.035652004
case acc: 0.03945768
top acc: 0.0350 ::: bot acc: 0.0557
top acc: 0.1089 ::: bot acc: 0.0434
top acc: 0.0433 ::: bot acc: 0.0823
top acc: 0.0167 ::: bot acc: 0.0606
top acc: 0.0591 ::: bot acc: 0.0357
top acc: 0.0700 ::: bot acc: 0.0359
current epoch: 14
train loss is 0.002118
average val loss: 0.001008, accuracy: 0.0336
average test loss: 0.001471, accuracy: 0.0440
case acc: 0.03437451
case acc: 0.07124534
case acc: 0.050293256
case acc: 0.033200663
case acc: 0.03560262
case acc: 0.039381046
top acc: 0.0376 ::: bot acc: 0.0537
top acc: 0.1039 ::: bot acc: 0.0385
top acc: 0.0438 ::: bot acc: 0.0825
top acc: 0.0172 ::: bot acc: 0.0599
top acc: 0.0600 ::: bot acc: 0.0344
top acc: 0.0703 ::: bot acc: 0.0356
current epoch: 15
train loss is 0.002021
average val loss: 0.000956, accuracy: 0.0327
average test loss: 0.001403, accuracy: 0.0432
case acc: 0.03405202
case acc: 0.06593312
case acc: 0.050602175
case acc: 0.0334598
case acc: 0.035741087
case acc: 0.039146107
top acc: 0.0382 ::: bot acc: 0.0528
top acc: 0.0983 ::: bot acc: 0.0341
top acc: 0.0435 ::: bot acc: 0.0829
top acc: 0.0168 ::: bot acc: 0.0605
top acc: 0.0603 ::: bot acc: 0.0340
top acc: 0.0693 ::: bot acc: 0.0364
current epoch: 16
train loss is 0.001903
average val loss: 0.000898, accuracy: 0.0317
average test loss: 0.001355, accuracy: 0.0423
case acc: 0.033729788
case acc: 0.062394474
case acc: 0.05009623
case acc: 0.032537542
case acc: 0.035791185
case acc: 0.03934874
top acc: 0.0411 ::: bot acc: 0.0503
top acc: 0.0939 ::: bot acc: 0.0315
top acc: 0.0437 ::: bot acc: 0.0823
top acc: 0.0169 ::: bot acc: 0.0592
top acc: 0.0614 ::: bot acc: 0.0329
top acc: 0.0701 ::: bot acc: 0.0361
current epoch: 17
train loss is 0.001817
average val loss: 0.000862, accuracy: 0.0311
average test loss: 0.001324, accuracy: 0.0416
case acc: 0.03311985
case acc: 0.060546182
case acc: 0.049503967
case acc: 0.030762944
case acc: 0.0358925
case acc: 0.039786603
top acc: 0.0446 ::: bot acc: 0.0463
top acc: 0.0920 ::: bot acc: 0.0303
top acc: 0.0464 ::: bot acc: 0.0799
top acc: 0.0163 ::: bot acc: 0.0568
top acc: 0.0636 ::: bot acc: 0.0303
top acc: 0.0721 ::: bot acc: 0.0341
current epoch: 18
train loss is 0.001752
average val loss: 0.000832, accuracy: 0.0306
average test loss: 0.001294, accuracy: 0.0410
case acc: 0.033184614
case acc: 0.0580129
case acc: 0.04929284
case acc: 0.029503351
case acc: 0.03624543
case acc: 0.039861016
top acc: 0.0471 ::: bot acc: 0.0441
top acc: 0.0890 ::: bot acc: 0.0280
top acc: 0.0481 ::: bot acc: 0.0786
top acc: 0.0164 ::: bot acc: 0.0551
top acc: 0.0650 ::: bot acc: 0.0294
top acc: 0.0724 ::: bot acc: 0.0334
current epoch: 19
train loss is 0.001712
average val loss: 0.000793, accuracy: 0.0299
average test loss: 0.001253, accuracy: 0.0404
case acc: 0.03304256
case acc: 0.054591447
case acc: 0.04916825
case acc: 0.029726233
case acc: 0.035944648
case acc: 0.039744996
top acc: 0.0474 ::: bot acc: 0.0432
top acc: 0.0852 ::: bot acc: 0.0260
top acc: 0.0480 ::: bot acc: 0.0786
top acc: 0.0165 ::: bot acc: 0.0553
top acc: 0.0642 ::: bot acc: 0.0299
top acc: 0.0719 ::: bot acc: 0.0343
current epoch: 20
train loss is 0.001639
average val loss: 0.000763, accuracy: 0.0293
average test loss: 0.001207, accuracy: 0.0398
case acc: 0.033265967
case acc: 0.0503138
case acc: 0.04947297
case acc: 0.030513594
case acc: 0.035857487
case acc: 0.039189026
top acc: 0.0472 ::: bot acc: 0.0438
top acc: 0.0803 ::: bot acc: 0.0232
top acc: 0.0470 ::: bot acc: 0.0798
top acc: 0.0164 ::: bot acc: 0.0563
top acc: 0.0617 ::: bot acc: 0.0326
top acc: 0.0693 ::: bot acc: 0.0366
current epoch: 21
train loss is 0.001592
average val loss: 0.000735, accuracy: 0.0289
average test loss: 0.001196, accuracy: 0.0394
case acc: 0.033229746
case acc: 0.049755458
case acc: 0.048563372
case acc: 0.028723983
case acc: 0.036102217
case acc: 0.03981166
top acc: 0.0507 ::: bot acc: 0.0402
top acc: 0.0793 ::: bot acc: 0.0230
top acc: 0.0503 ::: bot acc: 0.0766
top acc: 0.0167 ::: bot acc: 0.0536
top acc: 0.0636 ::: bot acc: 0.0310
top acc: 0.0716 ::: bot acc: 0.0351
current epoch: 22
train loss is 0.001564
average val loss: 0.000729, accuracy: 0.0289
average test loss: 0.001180, accuracy: 0.0390
case acc: 0.03346984
case acc: 0.04900029
case acc: 0.047924843
case acc: 0.027610166
case acc: 0.03595674
case acc: 0.039801065
top acc: 0.0539 ::: bot acc: 0.0371
top acc: 0.0783 ::: bot acc: 0.0229
top acc: 0.0533 ::: bot acc: 0.0739
top acc: 0.0177 ::: bot acc: 0.0516
top acc: 0.0643 ::: bot acc: 0.0296
top acc: 0.0719 ::: bot acc: 0.0341
current epoch: 23
train loss is 0.001585
average val loss: 0.000734, accuracy: 0.0293
average test loss: 0.001191, accuracy: 0.0389
case acc: 0.034120366
case acc: 0.049710605
case acc: 0.04699477
case acc: 0.026116746
case acc: 0.036159955
case acc: 0.04019938
top acc: 0.0577 ::: bot acc: 0.0336
top acc: 0.0792 ::: bot acc: 0.0229
top acc: 0.0566 ::: bot acc: 0.0700
top acc: 0.0198 ::: bot acc: 0.0482
top acc: 0.0664 ::: bot acc: 0.0274
top acc: 0.0741 ::: bot acc: 0.0315
current epoch: 24
train loss is 0.001558
average val loss: 0.000719, accuracy: 0.0291
average test loss: 0.001180, accuracy: 0.0387
case acc: 0.03441925
case acc: 0.04828035
case acc: 0.04675526
case acc: 0.02578628
case acc: 0.036434084
case acc: 0.04055795
top acc: 0.0597 ::: bot acc: 0.0314
top acc: 0.0773 ::: bot acc: 0.0224
top acc: 0.0588 ::: bot acc: 0.0681
top acc: 0.0210 ::: bot acc: 0.0471
top acc: 0.0667 ::: bot acc: 0.0276
top acc: 0.0742 ::: bot acc: 0.0323
current epoch: 25
train loss is 0.001528
average val loss: 0.000696, accuracy: 0.0287
average test loss: 0.001151, accuracy: 0.0383
case acc: 0.034378804
case acc: 0.04562474
case acc: 0.047124725
case acc: 0.026178539
case acc: 0.0361813
case acc: 0.040104434
top acc: 0.0593 ::: bot acc: 0.0318
top acc: 0.0744 ::: bot acc: 0.0209
top acc: 0.0586 ::: bot acc: 0.0688
top acc: 0.0207 ::: bot acc: 0.0478
top acc: 0.0652 ::: bot acc: 0.0288
top acc: 0.0729 ::: bot acc: 0.0338
current epoch: 26
train loss is 0.001505
average val loss: 0.000693, accuracy: 0.0289
average test loss: 0.001149, accuracy: 0.0381
case acc: 0.034584563
case acc: 0.04486611
case acc: 0.046632383
case acc: 0.025919892
case acc: 0.03637059
case acc: 0.04000548
top acc: 0.0615 ::: bot acc: 0.0296
top acc: 0.0734 ::: bot acc: 0.0200
top acc: 0.0607 ::: bot acc: 0.0666
top acc: 0.0218 ::: bot acc: 0.0469
top acc: 0.0661 ::: bot acc: 0.0287
top acc: 0.0730 ::: bot acc: 0.0330
current epoch: 27
train loss is 0.001505
average val loss: 0.000692, accuracy: 0.0291
average test loss: 0.001144, accuracy: 0.0379
case acc: 0.03504494
case acc: 0.044491563
case acc: 0.04625933
case acc: 0.025268853
case acc: 0.036210634
case acc: 0.040330224
top acc: 0.0631 ::: bot acc: 0.0275
top acc: 0.0731 ::: bot acc: 0.0204
top acc: 0.0626 ::: bot acc: 0.0643
top acc: 0.0234 ::: bot acc: 0.0447
top acc: 0.0659 ::: bot acc: 0.0281
top acc: 0.0739 ::: bot acc: 0.0323
current epoch: 28
train loss is 0.001488
average val loss: 0.000695, accuracy: 0.0293
average test loss: 0.001150, accuracy: 0.0380
case acc: 0.035637178
case acc: 0.044349942
case acc: 0.045996137
case acc: 0.024907518
case acc: 0.03636725
case acc: 0.040500678
top acc: 0.0654 ::: bot acc: 0.0256
top acc: 0.0726 ::: bot acc: 0.0203
top acc: 0.0644 ::: bot acc: 0.0618
top acc: 0.0246 ::: bot acc: 0.0434
top acc: 0.0668 ::: bot acc: 0.0272
top acc: 0.0744 ::: bot acc: 0.0321
current epoch: 29
train loss is 0.001480
average val loss: 0.000706, accuracy: 0.0297
average test loss: 0.001160, accuracy: 0.0381
case acc: 0.03619066
case acc: 0.04430676
case acc: 0.0461268
case acc: 0.024785005
case acc: 0.036601946
case acc: 0.04029054
top acc: 0.0673 ::: bot acc: 0.0235
top acc: 0.0728 ::: bot acc: 0.0202
top acc: 0.0677 ::: bot acc: 0.0596
top acc: 0.0266 ::: bot acc: 0.0417
top acc: 0.0679 ::: bot acc: 0.0264
top acc: 0.0748 ::: bot acc: 0.0306
current epoch: 30
train loss is 0.001500
average val loss: 0.000719, accuracy: 0.0301
average test loss: 0.001183, accuracy: 0.0384
case acc: 0.037023637
case acc: 0.04506687
case acc: 0.045894824
case acc: 0.024712237
case acc: 0.036546692
case acc: 0.04102341
top acc: 0.0695 ::: bot acc: 0.0217
top acc: 0.0739 ::: bot acc: 0.0204
top acc: 0.0700 ::: bot acc: 0.0567
top acc: 0.0287 ::: bot acc: 0.0397
top acc: 0.0692 ::: bot acc: 0.0247
top acc: 0.0764 ::: bot acc: 0.0300
current epoch: 31
train loss is 0.001484
average val loss: 0.000731, accuracy: 0.0305
average test loss: 0.001192, accuracy: 0.0385
case acc: 0.037836056
case acc: 0.044877194
case acc: 0.046298195
case acc: 0.024655778
case acc: 0.03671742
case acc: 0.040804204
top acc: 0.0711 ::: bot acc: 0.0210
top acc: 0.0735 ::: bot acc: 0.0207
top acc: 0.0719 ::: bot acc: 0.0554
top acc: 0.0300 ::: bot acc: 0.0385
top acc: 0.0703 ::: bot acc: 0.0235
top acc: 0.0763 ::: bot acc: 0.0294
current epoch: 32
train loss is 0.001472
average val loss: 0.000724, accuracy: 0.0303
average test loss: 0.001177, accuracy: 0.0383
case acc: 0.037542515
case acc: 0.043757673
case acc: 0.045917064
case acc: 0.024894489
case acc: 0.03687572
case acc: 0.040645573
top acc: 0.0709 ::: bot acc: 0.0207
top acc: 0.0719 ::: bot acc: 0.0199
top acc: 0.0719 ::: bot acc: 0.0548
top acc: 0.0298 ::: bot acc: 0.0389
top acc: 0.0702 ::: bot acc: 0.0240
top acc: 0.0752 ::: bot acc: 0.0305
current epoch: 33
train loss is 0.001463
average val loss: 0.000731, accuracy: 0.0306
average test loss: 0.001191, accuracy: 0.0384
case acc: 0.038054183
case acc: 0.043422394
case acc: 0.046337873
case acc: 0.024916733
case acc: 0.036963996
case acc: 0.040775523
top acc: 0.0721 ::: bot acc: 0.0200
top acc: 0.0717 ::: bot acc: 0.0198
top acc: 0.0738 ::: bot acc: 0.0536
top acc: 0.0306 ::: bot acc: 0.0380
top acc: 0.0714 ::: bot acc: 0.0230
top acc: 0.0762 ::: bot acc: 0.0299
current epoch: 34
train loss is 0.001467
average val loss: 0.000734, accuracy: 0.0307
average test loss: 0.001195, accuracy: 0.0385
case acc: 0.038649265
case acc: 0.043217503
case acc: 0.04633513
case acc: 0.024959229
case acc: 0.036905017
case acc: 0.040930066
top acc: 0.0731 ::: bot acc: 0.0194
top acc: 0.0712 ::: bot acc: 0.0199
top acc: 0.0748 ::: bot acc: 0.0523
top acc: 0.0317 ::: bot acc: 0.0370
top acc: 0.0714 ::: bot acc: 0.0218
top acc: 0.0765 ::: bot acc: 0.0294
current epoch: 35
train loss is 0.001466
average val loss: 0.000751, accuracy: 0.0312
average test loss: 0.001216, accuracy: 0.0388
case acc: 0.039107725
case acc: 0.043399516
case acc: 0.046633486
case acc: 0.025151493
case acc: 0.037307777
case acc: 0.041336752
top acc: 0.0740 ::: bot acc: 0.0189
top acc: 0.0717 ::: bot acc: 0.0198
top acc: 0.0768 ::: bot acc: 0.0505
top acc: 0.0335 ::: bot acc: 0.0354
top acc: 0.0732 ::: bot acc: 0.0206
top acc: 0.0775 ::: bot acc: 0.0286
current epoch: 36
train loss is 0.001457
average val loss: 0.000721, accuracy: 0.0306
average test loss: 0.001176, accuracy: 0.0381
case acc: 0.038158998
case acc: 0.04134722
case acc: 0.046474013
case acc: 0.024811793
case acc: 0.037095148
case acc: 0.040809218
top acc: 0.0720 ::: bot acc: 0.0195
top acc: 0.0692 ::: bot acc: 0.0188
top acc: 0.0752 ::: bot acc: 0.0518
top acc: 0.0320 ::: bot acc: 0.0363
top acc: 0.0716 ::: bot acc: 0.0223
top acc: 0.0761 ::: bot acc: 0.0301
current epoch: 37
train loss is 0.001438
average val loss: 0.000705, accuracy: 0.0301
average test loss: 0.001159, accuracy: 0.0378
case acc: 0.037788767
case acc: 0.04006489
case acc: 0.046364438
case acc: 0.024896106
case acc: 0.03703837
case acc: 0.040612064
top acc: 0.0711 ::: bot acc: 0.0200
top acc: 0.0673 ::: bot acc: 0.0186
top acc: 0.0749 ::: bot acc: 0.0527
top acc: 0.0314 ::: bot acc: 0.0373
top acc: 0.0713 ::: bot acc: 0.0228
top acc: 0.0750 ::: bot acc: 0.0313
current epoch: 38
train loss is 0.001407
average val loss: 0.000680, accuracy: 0.0296
average test loss: 0.001124, accuracy: 0.0372
case acc: 0.037087202
case acc: 0.038146697
case acc: 0.0463545
case acc: 0.0247827
case acc: 0.036734205
case acc: 0.0401324
top acc: 0.0695 ::: bot acc: 0.0218
top acc: 0.0647 ::: bot acc: 0.0178
top acc: 0.0732 ::: bot acc: 0.0538
top acc: 0.0300 ::: bot acc: 0.0385
top acc: 0.0699 ::: bot acc: 0.0239
top acc: 0.0733 ::: bot acc: 0.0326
current epoch: 39
train loss is 0.001378
average val loss: 0.000656, accuracy: 0.0290
average test loss: 0.001098, accuracy: 0.0367
case acc: 0.03669755
case acc: 0.036345668
case acc: 0.046164893
case acc: 0.02471481
case acc: 0.036457375
case acc: 0.039591033
top acc: 0.0682 ::: bot acc: 0.0235
top acc: 0.0623 ::: bot acc: 0.0175
top acc: 0.0719 ::: bot acc: 0.0550
top acc: 0.0288 ::: bot acc: 0.0398
top acc: 0.0687 ::: bot acc: 0.0248
top acc: 0.0720 ::: bot acc: 0.0335
current epoch: 40
train loss is 0.001345
average val loss: 0.000627, accuracy: 0.0282
average test loss: 0.001068, accuracy: 0.0361
case acc: 0.035961796
case acc: 0.034164257
case acc: 0.046117198
case acc: 0.024793006
case acc: 0.03621566
case acc: 0.039446224
top acc: 0.0657 ::: bot acc: 0.0259
top acc: 0.0592 ::: bot acc: 0.0173
top acc: 0.0698 ::: bot acc: 0.0572
top acc: 0.0267 ::: bot acc: 0.0418
top acc: 0.0672 ::: bot acc: 0.0267
top acc: 0.0702 ::: bot acc: 0.0357
current epoch: 41
train loss is 0.001326
average val loss: 0.000611, accuracy: 0.0278
average test loss: 0.001039, accuracy: 0.0356
case acc: 0.03520297
case acc: 0.032459185
case acc: 0.046025008
case acc: 0.025019988
case acc: 0.03604772
case acc: 0.03899722
top acc: 0.0635 ::: bot acc: 0.0277
top acc: 0.0566 ::: bot acc: 0.0172
top acc: 0.0678 ::: bot acc: 0.0591
top acc: 0.0251 ::: bot acc: 0.0432
top acc: 0.0653 ::: bot acc: 0.0284
top acc: 0.0682 ::: bot acc: 0.0374
current epoch: 42
train loss is 0.001301
average val loss: 0.000591, accuracy: 0.0272
average test loss: 0.001016, accuracy: 0.0352
case acc: 0.034602135
case acc: 0.03051145
case acc: 0.046118055
case acc: 0.025489476
case acc: 0.03576982
case acc: 0.038806487
top acc: 0.0608 ::: bot acc: 0.0307
top acc: 0.0534 ::: bot acc: 0.0176
top acc: 0.0654 ::: bot acc: 0.0615
top acc: 0.0232 ::: bot acc: 0.0454
top acc: 0.0626 ::: bot acc: 0.0312
top acc: 0.0663 ::: bot acc: 0.0398
current epoch: 43
train loss is 0.001281
average val loss: 0.000582, accuracy: 0.0270
average test loss: 0.001000, accuracy: 0.0350
case acc: 0.034150857
case acc: 0.028949004
case acc: 0.046437316
case acc: 0.026034754
case acc: 0.035618193
case acc: 0.038701557
top acc: 0.0583 ::: bot acc: 0.0331
top acc: 0.0506 ::: bot acc: 0.0186
top acc: 0.0633 ::: bot acc: 0.0639
top acc: 0.0211 ::: bot acc: 0.0476
top acc: 0.0603 ::: bot acc: 0.0335
top acc: 0.0642 ::: bot acc: 0.0421
current epoch: 44
train loss is 0.001258
average val loss: 0.000574, accuracy: 0.0267
average test loss: 0.000990, accuracy: 0.0348
case acc: 0.033936173
case acc: 0.028144885
case acc: 0.046333287
case acc: 0.026086109
case acc: 0.035648573
case acc: 0.038595572
top acc: 0.0569 ::: bot acc: 0.0341
top acc: 0.0492 ::: bot acc: 0.0191
top acc: 0.0625 ::: bot acc: 0.0644
top acc: 0.0205 ::: bot acc: 0.0481
top acc: 0.0597 ::: bot acc: 0.0345
top acc: 0.0638 ::: bot acc: 0.0422
current epoch: 45
train loss is 0.001251
average val loss: 0.000573, accuracy: 0.0266
average test loss: 0.000984, accuracy: 0.0347
case acc: 0.03370114
case acc: 0.027451193
case acc: 0.046440735
case acc: 0.026549285
case acc: 0.035389114
case acc: 0.038699605
top acc: 0.0554 ::: bot acc: 0.0355
top acc: 0.0477 ::: bot acc: 0.0203
top acc: 0.0613 ::: bot acc: 0.0658
top acc: 0.0199 ::: bot acc: 0.0489
top acc: 0.0585 ::: bot acc: 0.0353
top acc: 0.0628 ::: bot acc: 0.0433
current epoch: 46
train loss is 0.001249
average val loss: 0.000574, accuracy: 0.0265
average test loss: 0.000980, accuracy: 0.0347
case acc: 0.033733945
case acc: 0.026936738
case acc: 0.046551324
case acc: 0.026744008
case acc: 0.035348438
case acc: 0.0387016
top acc: 0.0547 ::: bot acc: 0.0369
top acc: 0.0459 ::: bot acc: 0.0220
top acc: 0.0598 ::: bot acc: 0.0669
top acc: 0.0187 ::: bot acc: 0.0498
top acc: 0.0571 ::: bot acc: 0.0365
top acc: 0.0618 ::: bot acc: 0.0444
current epoch: 47
train loss is 0.001237
average val loss: 0.000576, accuracy: 0.0266
average test loss: 0.000975, accuracy: 0.0347
case acc: 0.0335208
case acc: 0.026142072
case acc: 0.0468627
case acc: 0.027616361
case acc: 0.03543363
case acc: 0.038554024
top acc: 0.0526 ::: bot acc: 0.0388
top acc: 0.0437 ::: bot acc: 0.0236
top acc: 0.0585 ::: bot acc: 0.0686
top acc: 0.0178 ::: bot acc: 0.0515
top acc: 0.0555 ::: bot acc: 0.0387
top acc: 0.0599 ::: bot acc: 0.0460
current epoch: 48
train loss is 0.001223
average val loss: 0.000574, accuracy: 0.0265
average test loss: 0.000974, accuracy: 0.0347
case acc: 0.03344489
case acc: 0.025750361
case acc: 0.046740677
case acc: 0.028180322
case acc: 0.035387866
case acc: 0.03861358
top acc: 0.0518 ::: bot acc: 0.0395
top acc: 0.0431 ::: bot acc: 0.0243
top acc: 0.0576 ::: bot acc: 0.0689
top acc: 0.0181 ::: bot acc: 0.0523
top acc: 0.0547 ::: bot acc: 0.0395
top acc: 0.0593 ::: bot acc: 0.0462
current epoch: 49
train loss is 0.001220
average val loss: 0.000571, accuracy: 0.0265
average test loss: 0.000976, accuracy: 0.0347
case acc: 0.033502433
case acc: 0.02582317
case acc: 0.046769347
case acc: 0.027740898
case acc: 0.03536626
case acc: 0.03876264
top acc: 0.0523 ::: bot acc: 0.0391
top acc: 0.0431 ::: bot acc: 0.0245
top acc: 0.0582 ::: bot acc: 0.0686
top acc: 0.0179 ::: bot acc: 0.0517
top acc: 0.0552 ::: bot acc: 0.0389
top acc: 0.0600 ::: bot acc: 0.0464
current epoch: 50
train loss is 0.001218
average val loss: 0.000575, accuracy: 0.0265
average test loss: 0.000976, accuracy: 0.0347
case acc: 0.033440012
case acc: 0.025571391
case acc: 0.047015507
case acc: 0.028114876
case acc: 0.035376746
case acc: 0.038803272
top acc: 0.0512 ::: bot acc: 0.0402
top acc: 0.0418 ::: bot acc: 0.0259
top acc: 0.0573 ::: bot acc: 0.0698
top acc: 0.0174 ::: bot acc: 0.0525
top acc: 0.0547 ::: bot acc: 0.0394
top acc: 0.0592 ::: bot acc: 0.0470
LME_Co_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5370 5370 5370
1.7082474 -0.6288155 0.17559324 -0.22413088
Validation: 600 600 600
Testing: 774 774 774
pre-processing time: 0.0001838207244873047
the split date is 2011-07-01
train dropout: 0.2 test dropout: 0.05
net initializing with time: 0.002214670181274414
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.007991
average val loss: 0.005291, accuracy: 0.0933
average test loss: 0.005889, accuracy: 0.0969
case acc: 0.13538082
case acc: 0.10360354
case acc: 0.09838875
case acc: 0.07527779
case acc: 0.12224764
case acc: 0.046660017
top acc: 0.0908 ::: bot acc: 0.1767
top acc: 0.1414 ::: bot acc: 0.0684
top acc: 0.0476 ::: bot acc: 0.1452
top acc: 0.0331 ::: bot acc: 0.1222
top acc: 0.0692 ::: bot acc: 0.1670
top acc: 0.0216 ::: bot acc: 0.0830
current epoch: 2
train loss is 0.006958
average val loss: 0.004830, accuracy: 0.0700
average test loss: 0.004971, accuracy: 0.0732
case acc: 0.035829864
case acc: 0.20359382
case acc: 0.03731177
case acc: 0.051460993
case acc: 0.042714328
case acc: 0.06836947
top acc: 0.0254 ::: bot acc: 0.0602
top acc: 0.2419 ::: bot acc: 0.1681
top acc: 0.0680 ::: bot acc: 0.0303
top acc: 0.0924 ::: bot acc: 0.0246
top acc: 0.0391 ::: bot acc: 0.0627
top acc: 0.1203 ::: bot acc: 0.0205
current epoch: 3
train loss is 0.008526
average val loss: 0.013421, accuracy: 0.1438
average test loss: 0.013141, accuracy: 0.1398
case acc: 0.07940249
case acc: 0.2874285
case acc: 0.11435549
case acc: 0.13064162
case acc: 0.07525944
case acc: 0.15148929
top acc: 0.1236 ::: bot acc: 0.0376
top acc: 0.3256 ::: bot acc: 0.2519
top acc: 0.1655 ::: bot acc: 0.0671
top acc: 0.1844 ::: bot acc: 0.0782
top acc: 0.1303 ::: bot acc: 0.0301
top acc: 0.2043 ::: bot acc: 0.1012
current epoch: 4
train loss is 0.012298
average val loss: 0.009732, accuracy: 0.1178
average test loss: 0.009541, accuracy: 0.1142
case acc: 0.059898693
case acc: 0.2527643
case acc: 0.091509335
case acc: 0.10403717
case acc: 0.0558995
case acc: 0.12106996
top acc: 0.1036 ::: bot acc: 0.0201
top acc: 0.2903 ::: bot acc: 0.2175
top acc: 0.1419 ::: bot acc: 0.0458
top acc: 0.1576 ::: bot acc: 0.0523
top acc: 0.1082 ::: bot acc: 0.0159
top acc: 0.1740 ::: bot acc: 0.0708
current epoch: 5
train loss is 0.011862
average val loss: 0.002684, accuracy: 0.0623
average test loss: 0.002998, accuracy: 0.0660
case acc: 0.07115795
case acc: 0.10933806
case acc: 0.05245846
case acc: 0.046997793
case acc: 0.075810544
case acc: 0.040081173
top acc: 0.0291 ::: bot acc: 0.1120
top acc: 0.1469 ::: bot acc: 0.0739
top acc: 0.0211 ::: bot acc: 0.0897
top acc: 0.0306 ::: bot acc: 0.0800
top acc: 0.0320 ::: bot acc: 0.1152
top acc: 0.0408 ::: bot acc: 0.0624
current epoch: 6
train loss is 0.005151
average val loss: 0.002619, accuracy: 0.0630
average test loss: 0.002913, accuracy: 0.0662
case acc: 0.077173926
case acc: 0.089141965
case acc: 0.06107207
case acc: 0.05250375
case acc: 0.075447515
case acc: 0.042127162
top acc: 0.0343 ::: bot acc: 0.1184
top acc: 0.1266 ::: bot acc: 0.0539
top acc: 0.0212 ::: bot acc: 0.1021
top acc: 0.0282 ::: bot acc: 0.0893
top acc: 0.0321 ::: bot acc: 0.1150
top acc: 0.0330 ::: bot acc: 0.0706
current epoch: 7
train loss is 0.003090
average val loss: 0.002344, accuracy: 0.0526
average test loss: 0.002436, accuracy: 0.0548
case acc: 0.040999886
case acc: 0.12512349
case acc: 0.03883952
case acc: 0.038104203
case acc: 0.043645404
case acc: 0.04210163
top acc: 0.0197 ::: bot acc: 0.0716
top acc: 0.1627 ::: bot acc: 0.0896
top acc: 0.0383 ::: bot acc: 0.0605
top acc: 0.0604 ::: bot acc: 0.0461
top acc: 0.0382 ::: bot acc: 0.0638
top acc: 0.0774 ::: bot acc: 0.0267
current epoch: 8
train loss is 0.003499
average val loss: 0.002116, accuracy: 0.0510
average test loss: 0.002204, accuracy: 0.0529
case acc: 0.041145697
case acc: 0.11388495
case acc: 0.04039187
case acc: 0.037893303
case acc: 0.043339714
case acc: 0.040777724
top acc: 0.0190 ::: bot acc: 0.0720
top acc: 0.1513 ::: bot acc: 0.0787
top acc: 0.0332 ::: bot acc: 0.0656
top acc: 0.0559 ::: bot acc: 0.0501
top acc: 0.0390 ::: bot acc: 0.0633
top acc: 0.0730 ::: bot acc: 0.0306
current epoch: 9
train loss is 0.003311
average val loss: 0.001870, accuracy: 0.0502
average test loss: 0.001976, accuracy: 0.0518
case acc: 0.045994487
case acc: 0.09475232
case acc: 0.045461632
case acc: 0.039379954
case acc: 0.046091083
case acc: 0.03892808
top acc: 0.0164 ::: bot acc: 0.0809
top acc: 0.1320 ::: bot acc: 0.0595
top acc: 0.0242 ::: bot acc: 0.0773
top acc: 0.0449 ::: bot acc: 0.0610
top acc: 0.0347 ::: bot acc: 0.0700
top acc: 0.0619 ::: bot acc: 0.0422
current epoch: 10
train loss is 0.002770
average val loss: 0.001750, accuracy: 0.0488
average test loss: 0.001846, accuracy: 0.0501
case acc: 0.044444315
case acc: 0.08767535
case acc: 0.04622651
case acc: 0.039211202
case acc: 0.04396145
case acc: 0.039124385
top acc: 0.0167 ::: bot acc: 0.0786
top acc: 0.1251 ::: bot acc: 0.0524
top acc: 0.0234 ::: bot acc: 0.0793
top acc: 0.0449 ::: bot acc: 0.0610
top acc: 0.0375 ::: bot acc: 0.0651
top acc: 0.0622 ::: bot acc: 0.0421
current epoch: 11
train loss is 0.002515
average val loss: 0.001679, accuracy: 0.0474
average test loss: 0.001730, accuracy: 0.0483
case acc: 0.041182276
case acc: 0.084158
case acc: 0.04497131
case acc: 0.038609978
case acc: 0.041721545
case acc: 0.038896613
top acc: 0.0190 ::: bot acc: 0.0723
top acc: 0.1220 ::: bot acc: 0.0490
top acc: 0.0249 ::: bot acc: 0.0764
top acc: 0.0487 ::: bot acc: 0.0577
top acc: 0.0431 ::: bot acc: 0.0589
top acc: 0.0644 ::: bot acc: 0.0386
current epoch: 12
train loss is 0.002352
average val loss: 0.001623, accuracy: 0.0461
average test loss: 0.001661, accuracy: 0.0470
case acc: 0.03853704
case acc: 0.08203709
case acc: 0.043739226
case acc: 0.03812403
case acc: 0.039935794
case acc: 0.039873634
top acc: 0.0233 ::: bot acc: 0.0661
top acc: 0.1198 ::: bot acc: 0.0465
top acc: 0.0273 ::: bot acc: 0.0735
top acc: 0.0530 ::: bot acc: 0.0534
top acc: 0.0491 ::: bot acc: 0.0532
top acc: 0.0687 ::: bot acc: 0.0346
current epoch: 13
train loss is 0.002258
average val loss: 0.001552, accuracy: 0.0452
average test loss: 0.001576, accuracy: 0.0458
case acc: 0.037223063
case acc: 0.076372825
case acc: 0.043521762
case acc: 0.03811948
case acc: 0.03967563
case acc: 0.039939165
top acc: 0.0244 ::: bot acc: 0.0639
top acc: 0.1139 ::: bot acc: 0.0414
top acc: 0.0267 ::: bot acc: 0.0738
top acc: 0.0538 ::: bot acc: 0.0527
top acc: 0.0517 ::: bot acc: 0.0506
top acc: 0.0693 ::: bot acc: 0.0349
current epoch: 14
train loss is 0.002086
average val loss: 0.001463, accuracy: 0.0442
average test loss: 0.001492, accuracy: 0.0448
case acc: 0.037785355
case acc: 0.06848376
case acc: 0.04479689
case acc: 0.038613666
case acc: 0.03987677
case acc: 0.03935511
top acc: 0.0240 ::: bot acc: 0.0647
top acc: 0.1057 ::: bot acc: 0.0338
top acc: 0.0248 ::: bot acc: 0.0767
top acc: 0.0513 ::: bot acc: 0.0557
top acc: 0.0498 ::: bot acc: 0.0523
top acc: 0.0667 ::: bot acc: 0.0374
current epoch: 15
train loss is 0.001920
average val loss: 0.001418, accuracy: 0.0434
average test loss: 0.001431, accuracy: 0.0438
case acc: 0.036651492
case acc: 0.06460166
case acc: 0.04439227
case acc: 0.03825202
case acc: 0.039423913
case acc: 0.039677907
top acc: 0.0263 ::: bot acc: 0.0619
top acc: 0.1017 ::: bot acc: 0.0304
top acc: 0.0260 ::: bot acc: 0.0753
top acc: 0.0536 ::: bot acc: 0.0535
top acc: 0.0519 ::: bot acc: 0.0501
top acc: 0.0676 ::: bot acc: 0.0365
current epoch: 16
train loss is 0.001859
average val loss: 0.001362, accuracy: 0.0425
average test loss: 0.001371, accuracy: 0.0429
case acc: 0.035861623
case acc: 0.06013738
case acc: 0.044174638
case acc: 0.03823905
case acc: 0.039066397
case acc: 0.039942853
top acc: 0.0279 ::: bot acc: 0.0601
top acc: 0.0965 ::: bot acc: 0.0268
top acc: 0.0262 ::: bot acc: 0.0748
top acc: 0.0541 ::: bot acc: 0.0527
top acc: 0.0521 ::: bot acc: 0.0493
top acc: 0.0681 ::: bot acc: 0.0368
current epoch: 17
train loss is 0.001715
average val loss: 0.001316, accuracy: 0.0418
average test loss: 0.001323, accuracy: 0.0421
case acc: 0.03549327
case acc: 0.055231266
case acc: 0.04428947
case acc: 0.03820341
case acc: 0.039573025
case acc: 0.039562147
top acc: 0.0283 ::: bot acc: 0.0591
top acc: 0.0916 ::: bot acc: 0.0227
top acc: 0.0252 ::: bot acc: 0.0759
top acc: 0.0534 ::: bot acc: 0.0535
top acc: 0.0518 ::: bot acc: 0.0506
top acc: 0.0663 ::: bot acc: 0.0384
current epoch: 18
train loss is 0.001673
average val loss: 0.001281, accuracy: 0.0412
average test loss: 0.001283, accuracy: 0.0413
case acc: 0.034950577
case acc: 0.0519428
case acc: 0.044001766
case acc: 0.03802538
case acc: 0.039498884
case acc: 0.03934682
top acc: 0.0305 ::: bot acc: 0.0575
top acc: 0.0881 ::: bot acc: 0.0199
top acc: 0.0259 ::: bot acc: 0.0748
top acc: 0.0543 ::: bot acc: 0.0523
top acc: 0.0522 ::: bot acc: 0.0500
top acc: 0.0667 ::: bot acc: 0.0374
current epoch: 19
train loss is 0.001619
average val loss: 0.001265, accuracy: 0.0407
average test loss: 0.001254, accuracy: 0.0407
case acc: 0.033859175
case acc: 0.05070846
case acc: 0.042793643
case acc: 0.0380932
case acc: 0.038898706
case acc: 0.039797854
top acc: 0.0339 ::: bot acc: 0.0533
top acc: 0.0866 ::: bot acc: 0.0188
top acc: 0.0280 ::: bot acc: 0.0722
top acc: 0.0572 ::: bot acc: 0.0498
top acc: 0.0540 ::: bot acc: 0.0477
top acc: 0.0685 ::: bot acc: 0.0357
current epoch: 20
train loss is 0.001561
average val loss: 0.001234, accuracy: 0.0401
average test loss: 0.001219, accuracy: 0.0401
case acc: 0.033427794
case acc: 0.04801226
case acc: 0.042244717
case acc: 0.038023353
case acc: 0.03905784
case acc: 0.03966882
top acc: 0.0359 ::: bot acc: 0.0517
top acc: 0.0833 ::: bot acc: 0.0170
top acc: 0.0290 ::: bot acc: 0.0707
top acc: 0.0578 ::: bot acc: 0.0488
top acc: 0.0536 ::: bot acc: 0.0485
top acc: 0.0682 ::: bot acc: 0.0360
current epoch: 21
train loss is 0.001534
average val loss: 0.001229, accuracy: 0.0398
average test loss: 0.001203, accuracy: 0.0397
case acc: 0.032855712
case acc: 0.046979107
case acc: 0.04113312
case acc: 0.038253546
case acc: 0.0388609
case acc: 0.04009604
top acc: 0.0392 ::: bot acc: 0.0485
top acc: 0.0822 ::: bot acc: 0.0164
top acc: 0.0311 ::: bot acc: 0.0676
top acc: 0.0606 ::: bot acc: 0.0463
top acc: 0.0546 ::: bot acc: 0.0475
top acc: 0.0699 ::: bot acc: 0.0349
current epoch: 22
train loss is 0.001516
average val loss: 0.001206, accuracy: 0.0394
average test loss: 0.001180, accuracy: 0.0393
case acc: 0.032674875
case acc: 0.045252062
case acc: 0.040287744
case acc: 0.038355406
case acc: 0.039016318
case acc: 0.039998163
top acc: 0.0414 ::: bot acc: 0.0464
top acc: 0.0796 ::: bot acc: 0.0155
top acc: 0.0326 ::: bot acc: 0.0658
top acc: 0.0616 ::: bot acc: 0.0450
top acc: 0.0543 ::: bot acc: 0.0477
top acc: 0.0699 ::: bot acc: 0.0348
current epoch: 23
train loss is 0.001505
average val loss: 0.001236, accuracy: 0.0396
average test loss: 0.001196, accuracy: 0.0394
case acc: 0.032152686
case acc: 0.046465676
case acc: 0.038838267
case acc: 0.03895673
case acc: 0.038776316
case acc: 0.040926814
top acc: 0.0460 ::: bot acc: 0.0412
top acc: 0.0815 ::: bot acc: 0.0162
top acc: 0.0374 ::: bot acc: 0.0613
top acc: 0.0659 ::: bot acc: 0.0405
top acc: 0.0575 ::: bot acc: 0.0454
top acc: 0.0728 ::: bot acc: 0.0323
current epoch: 24
train loss is 0.001493
average val loss: 0.001244, accuracy: 0.0397
average test loss: 0.001195, accuracy: 0.0393
case acc: 0.032537766
case acc: 0.046126496
case acc: 0.03794171
case acc: 0.039532367
case acc: 0.03844599
case acc: 0.040958308
top acc: 0.0495 ::: bot acc: 0.0388
top acc: 0.0810 ::: bot acc: 0.0159
top acc: 0.0403 ::: bot acc: 0.0581
top acc: 0.0683 ::: bot acc: 0.0384
top acc: 0.0584 ::: bot acc: 0.0439
top acc: 0.0741 ::: bot acc: 0.0304
current epoch: 25
train loss is 0.001487
average val loss: 0.001269, accuracy: 0.0399
average test loss: 0.001208, accuracy: 0.0395
case acc: 0.033033773
case acc: 0.04619126
case acc: 0.037525747
case acc: 0.040313713
case acc: 0.038394388
case acc: 0.04151366
top acc: 0.0528 ::: bot acc: 0.0356
top acc: 0.0813 ::: bot acc: 0.0159
top acc: 0.0438 ::: bot acc: 0.0550
top acc: 0.0709 ::: bot acc: 0.0358
top acc: 0.0605 ::: bot acc: 0.0420
top acc: 0.0756 ::: bot acc: 0.0290
current epoch: 26
train loss is 0.001465
average val loss: 0.001228, accuracy: 0.0393
average test loss: 0.001171, accuracy: 0.0389
case acc: 0.032893177
case acc: 0.04311507
case acc: 0.037655424
case acc: 0.040078785
case acc: 0.038594916
case acc: 0.040851466
top acc: 0.0524 ::: bot acc: 0.0359
top acc: 0.0772 ::: bot acc: 0.0143
top acc: 0.0434 ::: bot acc: 0.0555
top acc: 0.0702 ::: bot acc: 0.0367
top acc: 0.0588 ::: bot acc: 0.0435
top acc: 0.0739 ::: bot acc: 0.0306
current epoch: 27
train loss is 0.001437
average val loss: 0.001208, accuracy: 0.0389
average test loss: 0.001148, accuracy: 0.0384
case acc: 0.032627348
case acc: 0.040803548
case acc: 0.03746536
case acc: 0.039950658
case acc: 0.038853798
case acc: 0.040520187
top acc: 0.0520 ::: bot acc: 0.0356
top acc: 0.0747 ::: bot acc: 0.0127
top acc: 0.0432 ::: bot acc: 0.0556
top acc: 0.0697 ::: bot acc: 0.0372
top acc: 0.0580 ::: bot acc: 0.0449
top acc: 0.0721 ::: bot acc: 0.0323
current epoch: 28
train loss is 0.001406
average val loss: 0.001193, accuracy: 0.0386
average test loss: 0.001136, accuracy: 0.0382
case acc: 0.03299641
case acc: 0.039271284
case acc: 0.03742425
case acc: 0.039966192
case acc: 0.03890579
case acc: 0.040461194
top acc: 0.0527 ::: bot acc: 0.0356
top acc: 0.0728 ::: bot acc: 0.0120
top acc: 0.0439 ::: bot acc: 0.0549
top acc: 0.0700 ::: bot acc: 0.0369
top acc: 0.0576 ::: bot acc: 0.0452
top acc: 0.0716 ::: bot acc: 0.0332
current epoch: 29
train loss is 0.001385
average val loss: 0.001222, accuracy: 0.0390
average test loss: 0.001152, accuracy: 0.0383
case acc: 0.03329612
case acc: 0.039805878
case acc: 0.036953814
case acc: 0.0408337
case acc: 0.038500898
case acc: 0.040558334
top acc: 0.0552 ::: bot acc: 0.0325
top acc: 0.0737 ::: bot acc: 0.0121
top acc: 0.0468 ::: bot acc: 0.0516
top acc: 0.0725 ::: bot acc: 0.0348
top acc: 0.0592 ::: bot acc: 0.0430
top acc: 0.0731 ::: bot acc: 0.0311
current epoch: 30
train loss is 0.001401
average val loss: 0.001211, accuracy: 0.0389
average test loss: 0.001142, accuracy: 0.0382
case acc: 0.03339605
case acc: 0.03889854
case acc: 0.036825456
case acc: 0.04084341
case acc: 0.03844251
case acc: 0.04053674
top acc: 0.0559 ::: bot acc: 0.0321
top acc: 0.0722 ::: bot acc: 0.0121
top acc: 0.0476 ::: bot acc: 0.0510
top acc: 0.0728 ::: bot acc: 0.0344
top acc: 0.0587 ::: bot acc: 0.0437
top acc: 0.0722 ::: bot acc: 0.0322
current epoch: 31
train loss is 0.001375
average val loss: 0.001223, accuracy: 0.0390
average test loss: 0.001150, accuracy: 0.0383
case acc: 0.03355833
case acc: 0.03872303
case acc: 0.036690023
case acc: 0.041347813
case acc: 0.038362984
case acc: 0.040830746
top acc: 0.0578 ::: bot acc: 0.0304
top acc: 0.0720 ::: bot acc: 0.0121
top acc: 0.0495 ::: bot acc: 0.0492
top acc: 0.0739 ::: bot acc: 0.0329
top acc: 0.0595 ::: bot acc: 0.0426
top acc: 0.0732 ::: bot acc: 0.0316
current epoch: 32
train loss is 0.001374
average val loss: 0.001221, accuracy: 0.0389
average test loss: 0.001145, accuracy: 0.0382
case acc: 0.033689648
case acc: 0.037899062
case acc: 0.0366754
case acc: 0.041659694
case acc: 0.03844473
case acc: 0.040643334
top acc: 0.0581 ::: bot acc: 0.0301
top acc: 0.0707 ::: bot acc: 0.0115
top acc: 0.0505 ::: bot acc: 0.0481
top acc: 0.0747 ::: bot acc: 0.0328
top acc: 0.0596 ::: bot acc: 0.0426
top acc: 0.0727 ::: bot acc: 0.0319
current epoch: 33
train loss is 0.001379
average val loss: 0.001245, accuracy: 0.0392
average test loss: 0.001163, accuracy: 0.0384
case acc: 0.034149658
case acc: 0.03811234
case acc: 0.036500506
case acc: 0.0422729
case acc: 0.038396534
case acc: 0.04094168
top acc: 0.0602 ::: bot acc: 0.0281
top acc: 0.0711 ::: bot acc: 0.0119
top acc: 0.0527 ::: bot acc: 0.0459
top acc: 0.0765 ::: bot acc: 0.0309
top acc: 0.0613 ::: bot acc: 0.0411
top acc: 0.0742 ::: bot acc: 0.0303
current epoch: 34
train loss is 0.001361
average val loss: 0.001231, accuracy: 0.0391
average test loss: 0.001147, accuracy: 0.0380
case acc: 0.03382775
case acc: 0.036813077
case acc: 0.036581807
case acc: 0.042192917
case acc: 0.038287725
case acc: 0.04056202
top acc: 0.0592 ::: bot acc: 0.0286
top acc: 0.0696 ::: bot acc: 0.0112
top acc: 0.0529 ::: bot acc: 0.0461
top acc: 0.0760 ::: bot acc: 0.0316
top acc: 0.0609 ::: bot acc: 0.0417
top acc: 0.0729 ::: bot acc: 0.0314
current epoch: 35
train loss is 0.001327
average val loss: 0.001205, accuracy: 0.0385
average test loss: 0.001131, accuracy: 0.0378
case acc: 0.03398068
case acc: 0.035451215
case acc: 0.03654754
case acc: 0.041998588
case acc: 0.038523212
case acc: 0.040532373
top acc: 0.0590 ::: bot acc: 0.0297
top acc: 0.0673 ::: bot acc: 0.0114
top acc: 0.0520 ::: bot acc: 0.0465
top acc: 0.0754 ::: bot acc: 0.0323
top acc: 0.0602 ::: bot acc: 0.0427
top acc: 0.0717 ::: bot acc: 0.0333
current epoch: 36
train loss is 0.001321
average val loss: 0.001208, accuracy: 0.0386
average test loss: 0.001130, accuracy: 0.0377
case acc: 0.034010194
case acc: 0.035033576
case acc: 0.036502972
case acc: 0.042087335
case acc: 0.038329996
case acc: 0.040530354
top acc: 0.0595 ::: bot acc: 0.0291
top acc: 0.0668 ::: bot acc: 0.0111
top acc: 0.0530 ::: bot acc: 0.0456
top acc: 0.0757 ::: bot acc: 0.0316
top acc: 0.0605 ::: bot acc: 0.0420
top acc: 0.0718 ::: bot acc: 0.0330
current epoch: 37
train loss is 0.001307
average val loss: 0.001177, accuracy: 0.0380
average test loss: 0.001109, accuracy: 0.0375
case acc: 0.033753883
case acc: 0.03380861
case acc: 0.03662638
case acc: 0.041832976
case acc: 0.03871392
case acc: 0.0401178
top acc: 0.0578 ::: bot acc: 0.0307
top acc: 0.0646 ::: bot acc: 0.0122
top acc: 0.0517 ::: bot acc: 0.0469
top acc: 0.0750 ::: bot acc: 0.0327
top acc: 0.0592 ::: bot acc: 0.0434
top acc: 0.0704 ::: bot acc: 0.0342
current epoch: 38
train loss is 0.001284
average val loss: 0.001185, accuracy: 0.0381
average test loss: 0.001114, accuracy: 0.0375
case acc: 0.033732355
case acc: 0.033706218
case acc: 0.036697987
case acc: 0.042125996
case acc: 0.038426407
case acc: 0.04026289
top acc: 0.0583 ::: bot acc: 0.0298
top acc: 0.0642 ::: bot acc: 0.0122
top acc: 0.0526 ::: bot acc: 0.0464
top acc: 0.0760 ::: bot acc: 0.0316
top acc: 0.0601 ::: bot acc: 0.0423
top acc: 0.0710 ::: bot acc: 0.0335
current epoch: 39
train loss is 0.001283
average val loss: 0.001151, accuracy: 0.0375
average test loss: 0.001089, accuracy: 0.0371
case acc: 0.03361729
case acc: 0.032251477
case acc: 0.036539875
case acc: 0.041483458
case acc: 0.03873468
case acc: 0.039951287
top acc: 0.0565 ::: bot acc: 0.0319
top acc: 0.0616 ::: bot acc: 0.0135
top acc: 0.0511 ::: bot acc: 0.0476
top acc: 0.0746 ::: bot acc: 0.0328
top acc: 0.0588 ::: bot acc: 0.0439
top acc: 0.0696 ::: bot acc: 0.0348
current epoch: 40
train loss is 0.001262
average val loss: 0.001151, accuracy: 0.0374
average test loss: 0.001088, accuracy: 0.0371
case acc: 0.03356224
case acc: 0.031646952
case acc: 0.03660202
case acc: 0.041804947
case acc: 0.03858498
case acc: 0.04010215
top acc: 0.0564 ::: bot acc: 0.0321
top acc: 0.0604 ::: bot acc: 0.0138
top acc: 0.0511 ::: bot acc: 0.0477
top acc: 0.0749 ::: bot acc: 0.0326
top acc: 0.0586 ::: bot acc: 0.0439
top acc: 0.0699 ::: bot acc: 0.0350
current epoch: 41
train loss is 0.001242
average val loss: 0.001099, accuracy: 0.0366
average test loss: 0.001047, accuracy: 0.0365
case acc: 0.03307676
case acc: 0.029764846
case acc: 0.036975466
case acc: 0.040619668
case acc: 0.039088055
case acc: 0.039547123
top acc: 0.0528 ::: bot acc: 0.0357
top acc: 0.0559 ::: bot acc: 0.0167
top acc: 0.0476 ::: bot acc: 0.0510
top acc: 0.0718 ::: bot acc: 0.0354
top acc: 0.0551 ::: bot acc: 0.0477
top acc: 0.0663 ::: bot acc: 0.0383
current epoch: 42
train loss is 0.001196
average val loss: 0.001072, accuracy: 0.0362
average test loss: 0.001032, accuracy: 0.0363
case acc: 0.032742716
case acc: 0.028880656
case acc: 0.037079137
case acc: 0.04006669
case acc: 0.039530754
case acc: 0.03926091
top acc: 0.0506 ::: bot acc: 0.0379
top acc: 0.0534 ::: bot acc: 0.0194
top acc: 0.0458 ::: bot acc: 0.0529
top acc: 0.0701 ::: bot acc: 0.0373
top acc: 0.0533 ::: bot acc: 0.0497
top acc: 0.0641 ::: bot acc: 0.0406
current epoch: 43
train loss is 0.001179
average val loss: 0.001049, accuracy: 0.0358
average test loss: 0.001023, accuracy: 0.0362
case acc: 0.032671034
case acc: 0.028342253
case acc: 0.0374337
case acc: 0.039614566
case acc: 0.03975514
case acc: 0.03919889
top acc: 0.0486 ::: bot acc: 0.0402
top acc: 0.0511 ::: bot acc: 0.0220
top acc: 0.0445 ::: bot acc: 0.0544
top acc: 0.0682 ::: bot acc: 0.0387
top acc: 0.0512 ::: bot acc: 0.0515
top acc: 0.0625 ::: bot acc: 0.0423
current epoch: 44
train loss is 0.001166
average val loss: 0.001041, accuracy: 0.0357
average test loss: 0.001016, accuracy: 0.0360
case acc: 0.032675236
case acc: 0.027701559
case acc: 0.03761245
case acc: 0.03933912
case acc: 0.039930854
case acc: 0.038921524
top acc: 0.0474 ::: bot acc: 0.0414
top acc: 0.0496 ::: bot acc: 0.0229
top acc: 0.0439 ::: bot acc: 0.0552
top acc: 0.0677 ::: bot acc: 0.0392
top acc: 0.0500 ::: bot acc: 0.0527
top acc: 0.0612 ::: bot acc: 0.0430
current epoch: 45
train loss is 0.001154
average val loss: 0.001035, accuracy: 0.0356
average test loss: 0.001013, accuracy: 0.0360
case acc: 0.03257479
case acc: 0.027522253
case acc: 0.037570808
case acc: 0.039226107
case acc: 0.04036901
case acc: 0.038961038
top acc: 0.0467 ::: bot acc: 0.0419
top acc: 0.0485 ::: bot acc: 0.0244
top acc: 0.0429 ::: bot acc: 0.0559
top acc: 0.0670 ::: bot acc: 0.0400
top acc: 0.0491 ::: bot acc: 0.0537
top acc: 0.0606 ::: bot acc: 0.0442
current epoch: 46
train loss is 0.001152
average val loss: 0.001047, accuracy: 0.0357
average test loss: 0.001023, accuracy: 0.0362
case acc: 0.032685634
case acc: 0.028037192
case acc: 0.037351068
case acc: 0.039737813
case acc: 0.03996525
case acc: 0.039157875
top acc: 0.0484 ::: bot acc: 0.0400
top acc: 0.0501 ::: bot acc: 0.0230
top acc: 0.0451 ::: bot acc: 0.0540
top acc: 0.0688 ::: bot acc: 0.0383
top acc: 0.0512 ::: bot acc: 0.0519
top acc: 0.0620 ::: bot acc: 0.0428
current epoch: 47
train loss is 0.001154
average val loss: 0.001043, accuracy: 0.0357
average test loss: 0.001017, accuracy: 0.0360
case acc: 0.032474983
case acc: 0.027864797
case acc: 0.03720492
case acc: 0.03961222
case acc: 0.0399918
case acc: 0.039011054
top acc: 0.0479 ::: bot acc: 0.0404
top acc: 0.0492 ::: bot acc: 0.0237
top acc: 0.0446 ::: bot acc: 0.0541
top acc: 0.0686 ::: bot acc: 0.0385
top acc: 0.0506 ::: bot acc: 0.0523
top acc: 0.0619 ::: bot acc: 0.0430
current epoch: 48
train loss is 0.001146
average val loss: 0.001041, accuracy: 0.0356
average test loss: 0.001019, accuracy: 0.0361
case acc: 0.03264219
case acc: 0.027712878
case acc: 0.037325185
case acc: 0.039807696
case acc: 0.03984895
case acc: 0.039108735
top acc: 0.0484 ::: bot acc: 0.0402
top acc: 0.0495 ::: bot acc: 0.0234
top acc: 0.0453 ::: bot acc: 0.0537
top acc: 0.0690 ::: bot acc: 0.0381
top acc: 0.0511 ::: bot acc: 0.0516
top acc: 0.0620 ::: bot acc: 0.0427
current epoch: 49
train loss is 0.001149
average val loss: 0.001039, accuracy: 0.0355
average test loss: 0.001020, accuracy: 0.0361
case acc: 0.03260456
case acc: 0.027631879
case acc: 0.037264567
case acc: 0.039868135
case acc: 0.0398701
case acc: 0.039131742
top acc: 0.0478 ::: bot acc: 0.0408
top acc: 0.0490 ::: bot acc: 0.0242
top acc: 0.0450 ::: bot acc: 0.0538
top acc: 0.0689 ::: bot acc: 0.0382
top acc: 0.0511 ::: bot acc: 0.0518
top acc: 0.0620 ::: bot acc: 0.0430
current epoch: 50
train loss is 0.001140
average val loss: 0.001034, accuracy: 0.0355
average test loss: 0.001016, accuracy: 0.0361
case acc: 0.03268772
case acc: 0.027491689
case acc: 0.037595786
case acc: 0.039423186
case acc: 0.040114254
case acc: 0.039124377
top acc: 0.0467 ::: bot acc: 0.0420
top acc: 0.0477 ::: bot acc: 0.0254
top acc: 0.0443 ::: bot acc: 0.0547
top acc: 0.0679 ::: bot acc: 0.0387
top acc: 0.0507 ::: bot acc: 0.0525
top acc: 0.0610 ::: bot acc: 0.0439
LME_Co_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5400 5400 5400
1.7082474 -0.6288155 0.16982092 -0.17269358
Validation: 606 606 606
Testing: 750 750 750
pre-processing time: 0.0002582073211669922
the split date is 2012-01-01
train dropout: 0.2 test dropout: 0.05
net initializing with time: 0.0024085044860839844
preparing training and testing date with time: 0.0
current epoch: 1
train loss is 0.008083
average val loss: 0.005732, accuracy: 0.0953
average test loss: 0.004202, accuracy: 0.0778
case acc: 0.109830916
case acc: 0.1244106
case acc: 0.06297369
case acc: 0.05124971
case acc: 0.072769426
case acc: 0.04572042
top acc: 0.0627 ::: bot acc: 0.1596
top acc: 0.1639 ::: bot acc: 0.0791
top acc: 0.0161 ::: bot acc: 0.1196
top acc: 0.0219 ::: bot acc: 0.0910
top acc: 0.0281 ::: bot acc: 0.1148
top acc: 0.0433 ::: bot acc: 0.0717
current epoch: 2
train loss is 0.007109
average val loss: 0.005562, accuracy: 0.0774
average test loss: 0.007081, accuracy: 0.0919
case acc: 0.037169836
case acc: 0.23053402
case acc: 0.07149024
case acc: 0.07486573
case acc: 0.047376342
case acc: 0.089831755
top acc: 0.0591 ::: bot acc: 0.0369
top acc: 0.2700 ::: bot acc: 0.1863
top acc: 0.1262 ::: bot acc: 0.0222
top acc: 0.1202 ::: bot acc: 0.0306
top acc: 0.0945 ::: bot acc: 0.0137
top acc: 0.1508 ::: bot acc: 0.0339
current epoch: 3
train loss is 0.008886
average val loss: 0.014048, accuracy: 0.1456
average test loss: 0.017808, accuracy: 0.1719
case acc: 0.10586763
case acc: 0.30976304
case acc: 0.15729028
case acc: 0.15983655
case acc: 0.12782732
case acc: 0.17056903
top acc: 0.1521 ::: bot acc: 0.0565
top acc: 0.3496 ::: bot acc: 0.2646
top acc: 0.2189 ::: bot acc: 0.0938
top acc: 0.2077 ::: bot acc: 0.1108
top acc: 0.1818 ::: bot acc: 0.0807
top acc: 0.2314 ::: bot acc: 0.1148
current epoch: 4
train loss is 0.012422
average val loss: 0.007786, accuracy: 0.0991
average test loss: 0.010331, accuracy: 0.1226
case acc: 0.06370856
case acc: 0.25145197
case acc: 0.11040522
case acc: 0.10981416
case acc: 0.082861185
case acc: 0.11762091
top acc: 0.1081 ::: bot acc: 0.0193
top acc: 0.2919 ::: bot acc: 0.2059
top acc: 0.1708 ::: bot acc: 0.0482
top acc: 0.1572 ::: bot acc: 0.0612
top acc: 0.1371 ::: bot acc: 0.0356
top acc: 0.1791 ::: bot acc: 0.0618
current epoch: 5
train loss is 0.010755
average val loss: 0.003239, accuracy: 0.0690
average test loss: 0.002614, accuracy: 0.0585
case acc: 0.058763657
case acc: 0.11732793
case acc: 0.047431767
case acc: 0.036200512
case acc: 0.047133867
case acc: 0.044304572
top acc: 0.0154 ::: bot acc: 0.1059
top acc: 0.1569 ::: bot acc: 0.0723
top acc: 0.0472 ::: bot acc: 0.0773
top acc: 0.0348 ::: bot acc: 0.0617
top acc: 0.0264 ::: bot acc: 0.0785
top acc: 0.0549 ::: bot acc: 0.0619
current epoch: 6
train loss is 0.004377
average val loss: 0.002713, accuracy: 0.0619
average test loss: 0.002419, accuracy: 0.0553
case acc: 0.048569072
case acc: 0.117965885
case acc: 0.04697479
case acc: 0.035430707
case acc: 0.03940297
case acc: 0.043262433
top acc: 0.0120 ::: bot acc: 0.0923
top acc: 0.1578 ::: bot acc: 0.0727
top acc: 0.0545 ::: bot acc: 0.0692
top acc: 0.0460 ::: bot acc: 0.0517
top acc: 0.0433 ::: bot acc: 0.0585
top acc: 0.0666 ::: bot acc: 0.0499
current epoch: 7
train loss is 0.003115
average val loss: 0.002575, accuracy: 0.0547
average test loss: 0.003257, accuracy: 0.0623
case acc: 0.035094213
case acc: 0.14617914
case acc: 0.050891988
case acc: 0.04595052
case acc: 0.04280242
case acc: 0.052585736
top acc: 0.0423 ::: bot acc: 0.0524
top acc: 0.1862 ::: bot acc: 0.1010
top acc: 0.0892 ::: bot acc: 0.0346
top acc: 0.0821 ::: bot acc: 0.0210
top acc: 0.0865 ::: bot acc: 0.0173
top acc: 0.1034 ::: bot acc: 0.0175
current epoch: 8
train loss is 0.003535
average val loss: 0.002289, accuracy: 0.0525
average test loss: 0.002860, accuracy: 0.0588
case acc: 0.035031013
case acc: 0.13322407
case acc: 0.049253087
case acc: 0.042768884
case acc: 0.042870186
case acc: 0.049910426
top acc: 0.0391 ::: bot acc: 0.0551
top acc: 0.1731 ::: bot acc: 0.0880
top acc: 0.0821 ::: bot acc: 0.0412
top acc: 0.0763 ::: bot acc: 0.0238
top acc: 0.0857 ::: bot acc: 0.0186
top acc: 0.0977 ::: bot acc: 0.0217
current epoch: 9
train loss is 0.003227
average val loss: 0.002027, accuracy: 0.0510
average test loss: 0.002347, accuracy: 0.0538
case acc: 0.036074158
case acc: 0.11503402
case acc: 0.047532182
case acc: 0.038703404
case acc: 0.04028953
case acc: 0.045375034
top acc: 0.0316 ::: bot acc: 0.0626
top acc: 0.1551 ::: bot acc: 0.0699
top acc: 0.0709 ::: bot acc: 0.0524
top acc: 0.0660 ::: bot acc: 0.0312
top acc: 0.0798 ::: bot acc: 0.0226
top acc: 0.0867 ::: bot acc: 0.0297
current epoch: 10
train loss is 0.002726
average val loss: 0.001886, accuracy: 0.0493
average test loss: 0.002233, accuracy: 0.0531
case acc: 0.03554148
case acc: 0.108054176
case acc: 0.04780033
case acc: 0.039113186
case acc: 0.042356055
case acc: 0.045475096
top acc: 0.0344 ::: bot acc: 0.0598
top acc: 0.1476 ::: bot acc: 0.0634
top acc: 0.0702 ::: bot acc: 0.0540
top acc: 0.0667 ::: bot acc: 0.0314
top acc: 0.0840 ::: bot acc: 0.0198
top acc: 0.0870 ::: bot acc: 0.0296
current epoch: 11
train loss is 0.002473
average val loss: 0.001792, accuracy: 0.0477
average test loss: 0.002252, accuracy: 0.0536
case acc: 0.03523756
case acc: 0.10497303
case acc: 0.0480207
case acc: 0.040603984
case acc: 0.045338754
case acc: 0.047239862
top acc: 0.0410 ::: bot acc: 0.0540
top acc: 0.1447 ::: bot acc: 0.0602
top acc: 0.0734 ::: bot acc: 0.0512
top acc: 0.0710 ::: bot acc: 0.0280
top acc: 0.0910 ::: bot acc: 0.0149
top acc: 0.0909 ::: bot acc: 0.0271
current epoch: 12
train loss is 0.002271
average val loss: 0.001706, accuracy: 0.0464
average test loss: 0.002229, accuracy: 0.0535
case acc: 0.034742907
case acc: 0.101076834
case acc: 0.048228744
case acc: 0.041675508
case acc: 0.047790594
case acc: 0.04748769
top acc: 0.0448 ::: bot acc: 0.0493
top acc: 0.1413 ::: bot acc: 0.0563
top acc: 0.0749 ::: bot acc: 0.0494
top acc: 0.0736 ::: bot acc: 0.0263
top acc: 0.0956 ::: bot acc: 0.0134
top acc: 0.0927 ::: bot acc: 0.0243
current epoch: 13
train loss is 0.002181
average val loss: 0.001614, accuracy: 0.0452
average test loss: 0.002158, accuracy: 0.0529
case acc: 0.034955926
case acc: 0.09568727
case acc: 0.04808306
case acc: 0.042034943
case acc: 0.049032398
case acc: 0.047726624
top acc: 0.0474 ::: bot acc: 0.0470
top acc: 0.1356 ::: bot acc: 0.0510
top acc: 0.0752 ::: bot acc: 0.0487
top acc: 0.0743 ::: bot acc: 0.0259
top acc: 0.0975 ::: bot acc: 0.0132
top acc: 0.0930 ::: bot acc: 0.0250
current epoch: 14
train loss is 0.002004
average val loss: 0.001514, accuracy: 0.0441
average test loss: 0.001969, accuracy: 0.0507
case acc: 0.0348696
case acc: 0.0870272
case acc: 0.04766264
case acc: 0.04064415
case acc: 0.047630787
case acc: 0.046222825
top acc: 0.0463 ::: bot acc: 0.0477
top acc: 0.1266 ::: bot acc: 0.0429
top acc: 0.0717 ::: bot acc: 0.0520
top acc: 0.0712 ::: bot acc: 0.0278
top acc: 0.0953 ::: bot acc: 0.0134
top acc: 0.0894 ::: bot acc: 0.0274
current epoch: 15
train loss is 0.001837
average val loss: 0.001455, accuracy: 0.0432
average test loss: 0.001918, accuracy: 0.0502
case acc: 0.03503497
case acc: 0.08235492
case acc: 0.047894835
case acc: 0.041122932
case acc: 0.048382625
case acc: 0.046521913
top acc: 0.0487 ::: bot acc: 0.0453
top acc: 0.1218 ::: bot acc: 0.0387
top acc: 0.0726 ::: bot acc: 0.0514
top acc: 0.0722 ::: bot acc: 0.0270
top acc: 0.0969 ::: bot acc: 0.0130
top acc: 0.0898 ::: bot acc: 0.0273
current epoch: 16
train loss is 0.001749
average val loss: 0.001398, accuracy: 0.0423
average test loss: 0.001889, accuracy: 0.0500
case acc: 0.035291683
case acc: 0.078756236
case acc: 0.047809668
case acc: 0.041611258
case acc: 0.04955953
case acc: 0.046959866
top acc: 0.0515 ::: bot acc: 0.0427
top acc: 0.1179 ::: bot acc: 0.0356
top acc: 0.0731 ::: bot acc: 0.0505
top acc: 0.0736 ::: bot acc: 0.0257
top acc: 0.0984 ::: bot acc: 0.0134
top acc: 0.0907 ::: bot acc: 0.0268
current epoch: 17
train loss is 0.001646
average val loss: 0.001364, accuracy: 0.0417
average test loss: 0.001919, accuracy: 0.0506
case acc: 0.035977844
case acc: 0.077053525
case acc: 0.048038766
case acc: 0.043347344
case acc: 0.051278636
case acc: 0.047612116
top acc: 0.0560 ::: bot acc: 0.0385
top acc: 0.1161 ::: bot acc: 0.0345
top acc: 0.0762 ::: bot acc: 0.0472
top acc: 0.0765 ::: bot acc: 0.0246
top acc: 0.1005 ::: bot acc: 0.0137
top acc: 0.0924 ::: bot acc: 0.0249
current epoch: 18
train loss is 0.001594
average val loss: 0.001325, accuracy: 0.0410
average test loss: 0.001941, accuracy: 0.0509
case acc: 0.0366791
case acc: 0.07531044
case acc: 0.0487045
case acc: 0.04433913
case acc: 0.052249637
case acc: 0.048213024
top acc: 0.0595 ::: bot acc: 0.0352
top acc: 0.1139 ::: bot acc: 0.0335
top acc: 0.0789 ::: bot acc: 0.0451
top acc: 0.0791 ::: bot acc: 0.0226
top acc: 0.1024 ::: bot acc: 0.0136
top acc: 0.0941 ::: bot acc: 0.0238
current epoch: 19
train loss is 0.001545
average val loss: 0.001287, accuracy: 0.0406
average test loss: 0.001884, accuracy: 0.0502
case acc: 0.036734037
case acc: 0.071805
case acc: 0.048761256
case acc: 0.044630267
case acc: 0.051715974
case acc: 0.047768734
top acc: 0.0599 ::: bot acc: 0.0340
top acc: 0.1098 ::: bot acc: 0.0311
top acc: 0.0793 ::: bot acc: 0.0446
top acc: 0.0793 ::: bot acc: 0.0233
top acc: 0.1015 ::: bot acc: 0.0136
top acc: 0.0932 ::: bot acc: 0.0242
current epoch: 20
train loss is 0.001496
average val loss: 0.001267, accuracy: 0.0403
average test loss: 0.001883, accuracy: 0.0502
case acc: 0.037452985
case acc: 0.069618896
case acc: 0.048974514
case acc: 0.045408815
case acc: 0.051797472
case acc: 0.04809064
top acc: 0.0632 ::: bot acc: 0.0312
top acc: 0.1077 ::: bot acc: 0.0294
top acc: 0.0816 ::: bot acc: 0.0422
top acc: 0.0806 ::: bot acc: 0.0221
top acc: 0.1017 ::: bot acc: 0.0135
top acc: 0.0936 ::: bot acc: 0.0244
current epoch: 21
train loss is 0.001466
average val loss: 0.001239, accuracy: 0.0398
average test loss: 0.001873, accuracy: 0.0501
case acc: 0.037880033
case acc: 0.06759036
case acc: 0.049268086
case acc: 0.04611747
case acc: 0.05181475
case acc: 0.0479609
top acc: 0.0651 ::: bot acc: 0.0289
top acc: 0.1050 ::: bot acc: 0.0285
top acc: 0.0830 ::: bot acc: 0.0407
top acc: 0.0825 ::: bot acc: 0.0214
top acc: 0.1016 ::: bot acc: 0.0138
top acc: 0.0936 ::: bot acc: 0.0238
current epoch: 22
train loss is 0.001387
average val loss: 0.001229, accuracy: 0.0396
average test loss: 0.001899, accuracy: 0.0506
case acc: 0.039062355
case acc: 0.06663892
case acc: 0.049839973
case acc: 0.047149725
case acc: 0.052390236
case acc: 0.048303466
top acc: 0.0676 ::: bot acc: 0.0268
top acc: 0.1037 ::: bot acc: 0.0284
top acc: 0.0858 ::: bot acc: 0.0381
top acc: 0.0841 ::: bot acc: 0.0207
top acc: 0.1023 ::: bot acc: 0.0136
top acc: 0.0944 ::: bot acc: 0.0236
current epoch: 23
train loss is 0.001389
average val loss: 0.001242, accuracy: 0.0398
average test loss: 0.001992, accuracy: 0.0519
case acc: 0.040638447
case acc: 0.067059904
case acc: 0.05105316
case acc: 0.04915548
case acc: 0.053975724
case acc: 0.049259547
top acc: 0.0720 ::: bot acc: 0.0234
top acc: 0.1043 ::: bot acc: 0.0282
top acc: 0.0898 ::: bot acc: 0.0345
top acc: 0.0875 ::: bot acc: 0.0196
top acc: 0.1046 ::: bot acc: 0.0140
top acc: 0.0966 ::: bot acc: 0.0220
current epoch: 24
train loss is 0.001366
average val loss: 0.001236, accuracy: 0.0398
average test loss: 0.002027, accuracy: 0.0524
case acc: 0.042131398
case acc: 0.06616494
case acc: 0.051891245
case acc: 0.050371084
case acc: 0.054488637
case acc: 0.04957429
top acc: 0.0745 ::: bot acc: 0.0223
top acc: 0.1033 ::: bot acc: 0.0280
top acc: 0.0920 ::: bot acc: 0.0316
top acc: 0.0895 ::: bot acc: 0.0198
top acc: 0.1056 ::: bot acc: 0.0138
top acc: 0.0972 ::: bot acc: 0.0218
current epoch: 25
train loss is 0.001372
average val loss: 0.001247, accuracy: 0.0399
average test loss: 0.002083, accuracy: 0.0532
case acc: 0.04329286
case acc: 0.06587247
case acc: 0.052997608
case acc: 0.05170017
case acc: 0.05568338
case acc: 0.049913995
top acc: 0.0770 ::: bot acc: 0.0207
top acc: 0.1027 ::: bot acc: 0.0277
top acc: 0.0945 ::: bot acc: 0.0295
top acc: 0.0918 ::: bot acc: 0.0191
top acc: 0.1070 ::: bot acc: 0.0146
top acc: 0.0981 ::: bot acc: 0.0208
current epoch: 26
train loss is 0.001368
average val loss: 0.001237, accuracy: 0.0398
average test loss: 0.002077, accuracy: 0.0532
case acc: 0.044178776
case acc: 0.06457371
case acc: 0.053266738
case acc: 0.05197017
case acc: 0.05554436
case acc: 0.049644418
top acc: 0.0783 ::: bot acc: 0.0207
top acc: 0.1015 ::: bot acc: 0.0271
top acc: 0.0959 ::: bot acc: 0.0281
top acc: 0.0922 ::: bot acc: 0.0191
top acc: 0.1066 ::: bot acc: 0.0147
top acc: 0.0976 ::: bot acc: 0.0209
current epoch: 27
train loss is 0.001321
average val loss: 0.001200, accuracy: 0.0392
average test loss: 0.002024, accuracy: 0.0525
case acc: 0.043852955
case acc: 0.062190328
case acc: 0.053353358
case acc: 0.051854447
case acc: 0.054706465
case acc: 0.04899638
top acc: 0.0779 ::: bot acc: 0.0203
top acc: 0.0980 ::: bot acc: 0.0264
top acc: 0.0959 ::: bot acc: 0.0282
top acc: 0.0920 ::: bot acc: 0.0192
top acc: 0.1055 ::: bot acc: 0.0145
top acc: 0.0961 ::: bot acc: 0.0218
current epoch: 28
train loss is 0.001278
average val loss: 0.001170, accuracy: 0.0387
average test loss: 0.001959, accuracy: 0.0516
case acc: 0.04351308
case acc: 0.059900004
case acc: 0.05321899
case acc: 0.050926715
case acc: 0.053553768
case acc: 0.04836204
top acc: 0.0774 ::: bot acc: 0.0205
top acc: 0.0948 ::: bot acc: 0.0260
top acc: 0.0956 ::: bot acc: 0.0288
top acc: 0.0906 ::: bot acc: 0.0189
top acc: 0.1040 ::: bot acc: 0.0139
top acc: 0.0942 ::: bot acc: 0.0238
current epoch: 29
train loss is 0.001257
average val loss: 0.001167, accuracy: 0.0386
average test loss: 0.001971, accuracy: 0.0518
case acc: 0.044143595
case acc: 0.05906478
case acc: 0.05383499
case acc: 0.051673885
case acc: 0.053566203
case acc: 0.048266545
top acc: 0.0786 ::: bot acc: 0.0200
top acc: 0.0938 ::: bot acc: 0.0258
top acc: 0.0973 ::: bot acc: 0.0274
top acc: 0.0917 ::: bot acc: 0.0191
top acc: 0.1041 ::: bot acc: 0.0141
top acc: 0.0945 ::: bot acc: 0.0234
current epoch: 30
train loss is 0.001253
average val loss: 0.001181, accuracy: 0.0389
average test loss: 0.002026, accuracy: 0.0526
case acc: 0.045111917
case acc: 0.059328217
case acc: 0.054961927
case acc: 0.05272773
case acc: 0.05433663
case acc: 0.048878275
top acc: 0.0804 ::: bot acc: 0.0193
top acc: 0.0938 ::: bot acc: 0.0261
top acc: 0.0993 ::: bot acc: 0.0262
top acc: 0.0933 ::: bot acc: 0.0189
top acc: 0.1054 ::: bot acc: 0.0139
top acc: 0.0954 ::: bot acc: 0.0232
current epoch: 31
train loss is 0.001225
average val loss: 0.001146, accuracy: 0.0383
average test loss: 0.001959, accuracy: 0.0516
case acc: 0.044674896
case acc: 0.057145234
case acc: 0.054542597
case acc: 0.052188456
case acc: 0.05320805
case acc: 0.047844253
top acc: 0.0794 ::: bot acc: 0.0198
top acc: 0.0911 ::: bot acc: 0.0253
top acc: 0.0985 ::: bot acc: 0.0261
top acc: 0.0924 ::: bot acc: 0.0191
top acc: 0.1036 ::: bot acc: 0.0138
top acc: 0.0932 ::: bot acc: 0.0243
current epoch: 32
train loss is 0.001203
average val loss: 0.001152, accuracy: 0.0384
average test loss: 0.001973, accuracy: 0.0518
case acc: 0.045003776
case acc: 0.05683967
case acc: 0.055199634
case acc: 0.052662577
case acc: 0.053257667
case acc: 0.04764187
top acc: 0.0802 ::: bot acc: 0.0192
top acc: 0.0905 ::: bot acc: 0.0257
top acc: 0.1001 ::: bot acc: 0.0253
top acc: 0.0932 ::: bot acc: 0.0191
top acc: 0.1040 ::: bot acc: 0.0137
top acc: 0.0931 ::: bot acc: 0.0242
current epoch: 33
train loss is 0.001210
average val loss: 0.001180, accuracy: 0.0389
average test loss: 0.002056, accuracy: 0.0530
case acc: 0.046466373
case acc: 0.05726618
case acc: 0.05660485
case acc: 0.054137304
case acc: 0.054695755
case acc: 0.048556976
top acc: 0.0829 ::: bot acc: 0.0183
top acc: 0.0914 ::: bot acc: 0.0257
top acc: 0.1030 ::: bot acc: 0.0240
top acc: 0.0954 ::: bot acc: 0.0187
top acc: 0.1055 ::: bot acc: 0.0144
top acc: 0.0950 ::: bot acc: 0.0232
current epoch: 34
train loss is 0.001195
average val loss: 0.001200, accuracy: 0.0391
average test loss: 0.002114, accuracy: 0.0538
case acc: 0.047507353
case acc: 0.057577524
case acc: 0.05782354
case acc: 0.055415466
case acc: 0.055724677
case acc: 0.048933834
top acc: 0.0845 ::: bot acc: 0.0179
top acc: 0.0917 ::: bot acc: 0.0256
top acc: 0.1046 ::: bot acc: 0.0237
top acc: 0.0973 ::: bot acc: 0.0189
top acc: 0.1068 ::: bot acc: 0.0146
top acc: 0.0958 ::: bot acc: 0.0222
current epoch: 35
train loss is 0.001215
average val loss: 0.001213, accuracy: 0.0393
average test loss: 0.002167, accuracy: 0.0545
case acc: 0.048035447
case acc: 0.05745238
case acc: 0.058783393
case acc: 0.056544136
case acc: 0.056860454
case acc: 0.04927627
top acc: 0.0856 ::: bot acc: 0.0176
top acc: 0.0915 ::: bot acc: 0.0255
top acc: 0.1068 ::: bot acc: 0.0227
top acc: 0.0986 ::: bot acc: 0.0190
top acc: 0.1087 ::: bot acc: 0.0149
top acc: 0.0968 ::: bot acc: 0.0213
current epoch: 36
train loss is 0.001214
average val loss: 0.001200, accuracy: 0.0391
average test loss: 0.002145, accuracy: 0.0542
case acc: 0.04796021
case acc: 0.05663232
case acc: 0.058702514
case acc: 0.056433596
case acc: 0.05649679
case acc: 0.04915942
top acc: 0.0851 ::: bot acc: 0.0180
top acc: 0.0904 ::: bot acc: 0.0253
top acc: 0.1069 ::: bot acc: 0.0226
top acc: 0.0987 ::: bot acc: 0.0191
top acc: 0.1078 ::: bot acc: 0.0149
top acc: 0.0962 ::: bot acc: 0.0220
current epoch: 37
train loss is 0.001212
average val loss: 0.001183, accuracy: 0.0389
average test loss: 0.002115, accuracy: 0.0538
case acc: 0.047718354
case acc: 0.055213198
case acc: 0.05829384
case acc: 0.05639731
case acc: 0.056074478
case acc: 0.049029987
top acc: 0.0850 ::: bot acc: 0.0183
top acc: 0.0886 ::: bot acc: 0.0249
top acc: 0.1062 ::: bot acc: 0.0225
top acc: 0.0985 ::: bot acc: 0.0194
top acc: 0.1074 ::: bot acc: 0.0146
top acc: 0.0956 ::: bot acc: 0.0231
current epoch: 38
train loss is 0.001167
average val loss: 0.001164, accuracy: 0.0386
average test loss: 0.002071, accuracy: 0.0531
case acc: 0.046995685
case acc: 0.053931378
case acc: 0.058167044
case acc: 0.05590093
case acc: 0.05568847
case acc: 0.048178267
top acc: 0.0835 ::: bot acc: 0.0184
top acc: 0.0866 ::: bot acc: 0.0250
top acc: 0.1055 ::: bot acc: 0.0230
top acc: 0.0981 ::: bot acc: 0.0190
top acc: 0.1070 ::: bot acc: 0.0146
top acc: 0.0943 ::: bot acc: 0.0230
current epoch: 39
train loss is 0.001146
average val loss: 0.001131, accuracy: 0.0380
average test loss: 0.002002, accuracy: 0.0522
case acc: 0.04604975
case acc: 0.05209024
case acc: 0.057380073
case acc: 0.05476816
case acc: 0.054950256
case acc: 0.047665637
top acc: 0.0823 ::: bot acc: 0.0186
top acc: 0.0841 ::: bot acc: 0.0249
top acc: 0.1044 ::: bot acc: 0.0236
top acc: 0.0964 ::: bot acc: 0.0187
top acc: 0.1059 ::: bot acc: 0.0144
top acc: 0.0928 ::: bot acc: 0.0244
current epoch: 40
train loss is 0.001108
average val loss: 0.001087, accuracy: 0.0372
average test loss: 0.001885, accuracy: 0.0504
case acc: 0.044292282
case acc: 0.04951291
case acc: 0.05586109
case acc: 0.052873757
case acc: 0.05335004
case acc: 0.04661028
top acc: 0.0791 ::: bot acc: 0.0194
top acc: 0.0804 ::: bot acc: 0.0249
top acc: 0.1015 ::: bot acc: 0.0249
top acc: 0.0937 ::: bot acc: 0.0187
top acc: 0.1038 ::: bot acc: 0.0143
top acc: 0.0901 ::: bot acc: 0.0272
current epoch: 41
train loss is 0.001071
average val loss: 0.001052, accuracy: 0.0367
average test loss: 0.001790, accuracy: 0.0490
case acc: 0.0430551
case acc: 0.04719422
case acc: 0.054782875
case acc: 0.051447224
case acc: 0.051543925
case acc: 0.04589286
top acc: 0.0767 ::: bot acc: 0.0209
top acc: 0.0769 ::: bot acc: 0.0247
top acc: 0.0990 ::: bot acc: 0.0262
top acc: 0.0913 ::: bot acc: 0.0188
top acc: 0.1015 ::: bot acc: 0.0132
top acc: 0.0877 ::: bot acc: 0.0295
current epoch: 42
train loss is 0.001043
average val loss: 0.001034, accuracy: 0.0364
average test loss: 0.001719, accuracy: 0.0479
case acc: 0.041842703
case acc: 0.045598406
case acc: 0.053853087
case acc: 0.050649326
case acc: 0.050211936
case acc: 0.04530107
top acc: 0.0744 ::: bot acc: 0.0216
top acc: 0.0740 ::: bot acc: 0.0255
top acc: 0.0971 ::: bot acc: 0.0275
top acc: 0.0900 ::: bot acc: 0.0194
top acc: 0.0993 ::: bot acc: 0.0135
top acc: 0.0860 ::: bot acc: 0.0310
current epoch: 43
train loss is 0.001021
average val loss: 0.001024, accuracy: 0.0362
average test loss: 0.001688, accuracy: 0.0474
case acc: 0.041464094
case acc: 0.044302396
case acc: 0.053583447
case acc: 0.05034698
case acc: 0.04975106
case acc: 0.044958986
top acc: 0.0734 ::: bot acc: 0.0224
top acc: 0.0721 ::: bot acc: 0.0253
top acc: 0.0965 ::: bot acc: 0.0283
top acc: 0.0898 ::: bot acc: 0.0193
top acc: 0.0987 ::: bot acc: 0.0134
top acc: 0.0853 ::: bot acc: 0.0317
current epoch: 44
train loss is 0.001003
average val loss: 0.001006, accuracy: 0.0358
average test loss: 0.001612, accuracy: 0.0463
case acc: 0.04029313
case acc: 0.04269272
case acc: 0.052569896
case acc: 0.049130946
case acc: 0.048212335
case acc: 0.044615537
top acc: 0.0710 ::: bot acc: 0.0239
top acc: 0.0697 ::: bot acc: 0.0256
top acc: 0.0937 ::: bot acc: 0.0301
top acc: 0.0877 ::: bot acc: 0.0196
top acc: 0.0962 ::: bot acc: 0.0135
top acc: 0.0834 ::: bot acc: 0.0335
current epoch: 45
train loss is 0.000994
average val loss: 0.001007, accuracy: 0.0359
average test loss: 0.001596, accuracy: 0.0459
case acc: 0.039797496
case acc: 0.04183376
case acc: 0.052292213
case acc: 0.049037904
case acc: 0.04812538
case acc: 0.044540122
top acc: 0.0701 ::: bot acc: 0.0245
top acc: 0.0686 ::: bot acc: 0.0256
top acc: 0.0933 ::: bot acc: 0.0305
top acc: 0.0875 ::: bot acc: 0.0198
top acc: 0.0961 ::: bot acc: 0.0135
top acc: 0.0831 ::: bot acc: 0.0338
current epoch: 46
train loss is 0.000988
average val loss: 0.001002, accuracy: 0.0359
average test loss: 0.001554, accuracy: 0.0453
case acc: 0.039214715
case acc: 0.04079134
case acc: 0.05189174
case acc: 0.04851743
case acc: 0.04706024
case acc: 0.04426951
top acc: 0.0684 ::: bot acc: 0.0255
top acc: 0.0666 ::: bot acc: 0.0261
top acc: 0.0922 ::: bot acc: 0.0318
top acc: 0.0865 ::: bot acc: 0.0201
top acc: 0.0944 ::: bot acc: 0.0138
top acc: 0.0823 ::: bot acc: 0.0345
current epoch: 47
train loss is 0.000981
average val loss: 0.000995, accuracy: 0.0358
average test loss: 0.001496, accuracy: 0.0444
case acc: 0.038332965
case acc: 0.039545543
case acc: 0.051273763
case acc: 0.047283296
case acc: 0.045941208
case acc: 0.044008233
top acc: 0.0664 ::: bot acc: 0.0276
top acc: 0.0643 ::: bot acc: 0.0270
top acc: 0.0899 ::: bot acc: 0.0341
top acc: 0.0847 ::: bot acc: 0.0205
top acc: 0.0922 ::: bot acc: 0.0148
top acc: 0.0806 ::: bot acc: 0.0365
current epoch: 48
train loss is 0.000960
average val loss: 0.000994, accuracy: 0.0358
average test loss: 0.001485, accuracy: 0.0442
case acc: 0.03813287
case acc: 0.039101347
case acc: 0.05114774
case acc: 0.047256377
case acc: 0.045610823
case acc: 0.044190176
top acc: 0.0658 ::: bot acc: 0.0278
top acc: 0.0637 ::: bot acc: 0.0271
top acc: 0.0898 ::: bot acc: 0.0343
top acc: 0.0844 ::: bot acc: 0.0205
top acc: 0.0916 ::: bot acc: 0.0153
top acc: 0.0803 ::: bot acc: 0.0369
current epoch: 49
train loss is 0.000961
average val loss: 0.000992, accuracy: 0.0358
average test loss: 0.001477, accuracy: 0.0441
case acc: 0.03805114
case acc: 0.038772427
case acc: 0.051119816
case acc: 0.047090717
case acc: 0.045403033
case acc: 0.04396795
top acc: 0.0654 ::: bot acc: 0.0283
top acc: 0.0632 ::: bot acc: 0.0273
top acc: 0.0896 ::: bot acc: 0.0343
top acc: 0.0842 ::: bot acc: 0.0205
top acc: 0.0913 ::: bot acc: 0.0156
top acc: 0.0800 ::: bot acc: 0.0371
current epoch: 50
train loss is 0.000957
average val loss: 0.000995, accuracy: 0.0358
average test loss: 0.001468, accuracy: 0.0439
case acc: 0.03791489
case acc: 0.03844373
case acc: 0.05092227
case acc: 0.047168735
case acc: 0.045260906
case acc: 0.0439353
top acc: 0.0652 ::: bot acc: 0.0288
top acc: 0.0626 ::: bot acc: 0.0278
top acc: 0.0890 ::: bot acc: 0.0348
top acc: 0.0841 ::: bot acc: 0.0208
top acc: 0.0908 ::: bot acc: 0.0157
top acc: 0.0799 ::: bot acc: 0.0371
LME_Co_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5388 5388 5388
1.7082474 -0.6288155 0.16982092 -0.17269358
Validation: 600 600 600
Testing: 768 768 768
pre-processing time: 0.0001990795135498047
the split date is 2012-07-01
train dropout: 0.2 test dropout: 0.05
net initializing with time: 0.002414226531982422
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.007762
average val loss: 0.004531, accuracy: 0.0820
average test loss: 0.004379, accuracy: 0.0810
case acc: 0.1090806
case acc: 0.114871725
case acc: 0.07698344
case acc: 0.056704864
case acc: 0.08564662
case acc: 0.042442463
top acc: 0.0574 ::: bot acc: 0.1548
top acc: 0.1524 ::: bot acc: 0.0788
top acc: 0.0316 ::: bot acc: 0.1264
top acc: 0.0262 ::: bot acc: 0.0840
top acc: 0.0386 ::: bot acc: 0.1358
top acc: 0.0322 ::: bot acc: 0.0824
current epoch: 2
train loss is 0.006865
average val loss: 0.006071, accuracy: 0.0820
average test loss: 0.006043, accuracy: 0.0837
case acc: 0.039301813
case acc: 0.21912152
case acc: 0.055616904
case acc: 0.05673007
case acc: 0.041734114
case acc: 0.08985414
top acc: 0.0706 ::: bot acc: 0.0346
top acc: 0.2569 ::: bot acc: 0.1837
top acc: 0.0996 ::: bot acc: 0.0224
top acc: 0.0886 ::: bot acc: 0.0289
top acc: 0.0715 ::: bot acc: 0.0286
top acc: 0.1362 ::: bot acc: 0.0411
current epoch: 3
train loss is 0.008689
average val loss: 0.016740, accuracy: 0.1655
average test loss: 0.016772, accuracy: 0.1666
case acc: 0.113838956
case acc: 0.30421913
case acc: 0.14497453
case acc: 0.15032676
case acc: 0.11516828
case acc: 0.17093845
top acc: 0.1700 ::: bot acc: 0.0661
top acc: 0.3416 ::: bot acc: 0.2685
top acc: 0.1982 ::: bot acc: 0.0915
top acc: 0.1825 ::: bot acc: 0.1223
top acc: 0.1637 ::: bot acc: 0.0641
top acc: 0.2227 ::: bot acc: 0.1103
current epoch: 4
train loss is 0.012249
average val loss: 0.011484, accuracy: 0.1316
average test loss: 0.011498, accuracy: 0.1331
case acc: 0.08550213
case acc: 0.26212838
case acc: 0.11419357
case acc: 0.11622891
case acc: 0.085690856
case acc: 0.13465042
top acc: 0.1411 ::: bot acc: 0.0385
top acc: 0.2995 ::: bot acc: 0.2263
top acc: 0.1673 ::: bot acc: 0.0615
top acc: 0.1489 ::: bot acc: 0.0879
top acc: 0.1329 ::: bot acc: 0.0363
top acc: 0.1852 ::: bot acc: 0.0772
current epoch: 5
train loss is 0.011633
average val loss: 0.002613, accuracy: 0.0576
average test loss: 0.002477, accuracy: 0.0550
case acc: 0.054779965
case acc: 0.120288044
case acc: 0.04167421
case acc: 0.026244368
case acc: 0.046291858
case acc: 0.04059322
top acc: 0.0263 ::: bot acc: 0.0890
top acc: 0.1579 ::: bot acc: 0.0847
top acc: 0.0361 ::: bot acc: 0.0715
top acc: 0.0180 ::: bot acc: 0.0430
top acc: 0.0164 ::: bot acc: 0.0886
top acc: 0.0539 ::: bot acc: 0.0576
current epoch: 6
train loss is 0.004702
average val loss: 0.002343, accuracy: 0.0548
average test loss: 0.002215, accuracy: 0.0526
case acc: 0.053965844
case acc: 0.10833596
case acc: 0.043179873
case acc: 0.026893834
case acc: 0.042193618
case acc: 0.040749773
top acc: 0.0264 ::: bot acc: 0.0878
top acc: 0.1460 ::: bot acc: 0.0722
top acc: 0.0316 ::: bot acc: 0.0755
top acc: 0.0172 ::: bot acc: 0.0442
top acc: 0.0212 ::: bot acc: 0.0801
top acc: 0.0542 ::: bot acc: 0.0578
current epoch: 7
train loss is 0.003015
average val loss: 0.003093, accuracy: 0.0593
average test loss: 0.002999, accuracy: 0.0594
case acc: 0.03827776
case acc: 0.14386994
case acc: 0.041599583
case acc: 0.031218892
case acc: 0.04107458
case acc: 0.060416624
top acc: 0.0627 ::: bot acc: 0.0408
top acc: 0.1812 ::: bot acc: 0.1081
top acc: 0.0726 ::: bot acc: 0.0339
top acc: 0.0603 ::: bot acc: 0.0097
top acc: 0.0692 ::: bot acc: 0.0308
top acc: 0.0980 ::: bot acc: 0.0290
current epoch: 8
train loss is 0.003528
average val loss: 0.002685, accuracy: 0.0559
average test loss: 0.002613, accuracy: 0.0557
case acc: 0.03842772
case acc: 0.13069603
case acc: 0.04004361
case acc: 0.02723893
case acc: 0.040853687
case acc: 0.056887947
top acc: 0.0603 ::: bot acc: 0.0434
top acc: 0.1679 ::: bot acc: 0.0950
top acc: 0.0659 ::: bot acc: 0.0409
top acc: 0.0545 ::: bot acc: 0.0098
top acc: 0.0686 ::: bot acc: 0.0318
top acc: 0.0922 ::: bot acc: 0.0304
current epoch: 9
train loss is 0.003259
average val loss: 0.002114, accuracy: 0.0505
average test loss: 0.002035, accuracy: 0.0498
case acc: 0.040542558
case acc: 0.10890634
case acc: 0.039358422
case acc: 0.022751816
case acc: 0.03776432
case acc: 0.049510024
top acc: 0.0494 ::: bot acc: 0.0549
top acc: 0.1460 ::: bot acc: 0.0735
top acc: 0.0512 ::: bot acc: 0.0560
top acc: 0.0405 ::: bot acc: 0.0206
top acc: 0.0591 ::: bot acc: 0.0409
top acc: 0.0782 ::: bot acc: 0.0367
current epoch: 10
train loss is 0.002630
average val loss: 0.002022, accuracy: 0.0496
average test loss: 0.001948, accuracy: 0.0491
case acc: 0.03950354
case acc: 0.10313022
case acc: 0.039463386
case acc: 0.022933653
case acc: 0.039097667
case acc: 0.050259322
top acc: 0.0528 ::: bot acc: 0.0511
top acc: 0.1397 ::: bot acc: 0.0679
top acc: 0.0513 ::: bot acc: 0.0563
top acc: 0.0421 ::: bot acc: 0.0187
top acc: 0.0643 ::: bot acc: 0.0348
top acc: 0.0797 ::: bot acc: 0.0357
current epoch: 11
train loss is 0.002320
average val loss: 0.002062, accuracy: 0.0504
average test loss: 0.001996, accuracy: 0.0499
case acc: 0.03817399
case acc: 0.10188025
case acc: 0.039194085
case acc: 0.024257548
case acc: 0.04268065
case acc: 0.05302713
top acc: 0.0605 ::: bot acc: 0.0427
top acc: 0.1392 ::: bot acc: 0.0658
top acc: 0.0562 ::: bot acc: 0.0513
top acc: 0.0478 ::: bot acc: 0.0135
top acc: 0.0728 ::: bot acc: 0.0285
top acc: 0.0851 ::: bot acc: 0.0329
current epoch: 12
train loss is 0.002249
average val loss: 0.002076, accuracy: 0.0509
average test loss: 0.002017, accuracy: 0.0505
case acc: 0.038360648
case acc: 0.09933713
case acc: 0.039433822
case acc: 0.025925629
case acc: 0.045246102
case acc: 0.05480768
top acc: 0.0662 ::: bot acc: 0.0373
top acc: 0.1360 ::: bot acc: 0.0639
top acc: 0.0588 ::: bot acc: 0.0488
top acc: 0.0516 ::: bot acc: 0.0111
top acc: 0.0783 ::: bot acc: 0.0256
top acc: 0.0888 ::: bot acc: 0.0309
current epoch: 13
train loss is 0.002174
average val loss: 0.001935, accuracy: 0.0493
average test loss: 0.001881, accuracy: 0.0489
case acc: 0.03838808
case acc: 0.09177009
case acc: 0.039349005
case acc: 0.025403138
case acc: 0.04501293
case acc: 0.05370686
top acc: 0.0664 ::: bot acc: 0.0373
top acc: 0.1286 ::: bot acc: 0.0564
top acc: 0.0571 ::: bot acc: 0.0504
top acc: 0.0504 ::: bot acc: 0.0118
top acc: 0.0779 ::: bot acc: 0.0253
top acc: 0.0866 ::: bot acc: 0.0318
current epoch: 14
train loss is 0.001985
average val loss: 0.001821, accuracy: 0.0481
average test loss: 0.001778, accuracy: 0.0478
case acc: 0.038419295
case acc: 0.08511575
case acc: 0.039560955
case acc: 0.025135808
case acc: 0.045280684
case acc: 0.05328657
top acc: 0.0672 ::: bot acc: 0.0363
top acc: 0.1220 ::: bot acc: 0.0496
top acc: 0.0563 ::: bot acc: 0.0518
top acc: 0.0493 ::: bot acc: 0.0125
top acc: 0.0779 ::: bot acc: 0.0258
top acc: 0.0859 ::: bot acc: 0.0321
current epoch: 15
train loss is 0.001842
average val loss: 0.001725, accuracy: 0.0470
average test loss: 0.001684, accuracy: 0.0465
case acc: 0.038522124
case acc: 0.0787166
case acc: 0.039369877
case acc: 0.024837878
case acc: 0.044961333
case acc: 0.052762292
top acc: 0.0680 ::: bot acc: 0.0360
top acc: 0.1159 ::: bot acc: 0.0430
top acc: 0.0550 ::: bot acc: 0.0526
top acc: 0.0491 ::: bot acc: 0.0124
top acc: 0.0774 ::: bot acc: 0.0257
top acc: 0.0848 ::: bot acc: 0.0328
current epoch: 16
train loss is 0.001739
average val loss: 0.001710, accuracy: 0.0470
average test loss: 0.001674, accuracy: 0.0465
case acc: 0.038637497
case acc: 0.07544866
case acc: 0.039201282
case acc: 0.025827255
case acc: 0.04630528
case acc: 0.053666912
top acc: 0.0716 ::: bot acc: 0.0322
top acc: 0.1124 ::: bot acc: 0.0400
top acc: 0.0563 ::: bot acc: 0.0509
top acc: 0.0516 ::: bot acc: 0.0108
top acc: 0.0799 ::: bot acc: 0.0251
top acc: 0.0864 ::: bot acc: 0.0318
current epoch: 17
train loss is 0.001667
average val loss: 0.001747, accuracy: 0.0476
average test loss: 0.001712, accuracy: 0.0472
case acc: 0.039209496
case acc: 0.07389386
case acc: 0.03942895
case acc: 0.027654553
case acc: 0.04806538
case acc: 0.05483131
top acc: 0.0761 ::: bot acc: 0.0278
top acc: 0.1108 ::: bot acc: 0.0385
top acc: 0.0600 ::: bot acc: 0.0474
top acc: 0.0546 ::: bot acc: 0.0098
top acc: 0.0828 ::: bot acc: 0.0246
top acc: 0.0887 ::: bot acc: 0.0312
current epoch: 18
train loss is 0.001635
average val loss: 0.001713, accuracy: 0.0472
average test loss: 0.001681, accuracy: 0.0467
case acc: 0.039334968
case acc: 0.07029839
case acc: 0.039570518
case acc: 0.028231401
case acc: 0.047799386
case acc: 0.05506354
top acc: 0.0775 ::: bot acc: 0.0255
top acc: 0.1072 ::: bot acc: 0.0347
top acc: 0.0612 ::: bot acc: 0.0463
top acc: 0.0556 ::: bot acc: 0.0095
top acc: 0.0829 ::: bot acc: 0.0238
top acc: 0.0892 ::: bot acc: 0.0309
current epoch: 19
train loss is 0.001539
average val loss: 0.001702, accuracy: 0.0471
average test loss: 0.001674, accuracy: 0.0466
case acc: 0.0399201
case acc: 0.06752522
case acc: 0.039437085
case acc: 0.029019069
case acc: 0.04847034
case acc: 0.055208147
top acc: 0.0802 ::: bot acc: 0.0234
top acc: 0.1046 ::: bot acc: 0.0318
top acc: 0.0622 ::: bot acc: 0.0450
top acc: 0.0569 ::: bot acc: 0.0092
top acc: 0.0839 ::: bot acc: 0.0238
top acc: 0.0896 ::: bot acc: 0.0308
current epoch: 20
train loss is 0.001489
average val loss: 0.001722, accuracy: 0.0475
average test loss: 0.001703, accuracy: 0.0471
case acc: 0.040735558
case acc: 0.06592164
case acc: 0.039963577
case acc: 0.030793928
case acc: 0.049115717
case acc: 0.056197703
top acc: 0.0833 ::: bot acc: 0.0199
top acc: 0.1027 ::: bot acc: 0.0305
top acc: 0.0650 ::: bot acc: 0.0425
top acc: 0.0597 ::: bot acc: 0.0094
top acc: 0.0848 ::: bot acc: 0.0233
top acc: 0.0912 ::: bot acc: 0.0307
current epoch: 21
train loss is 0.001452
average val loss: 0.001659, accuracy: 0.0467
average test loss: 0.001631, accuracy: 0.0459
case acc: 0.040422983
case acc: 0.06161277
case acc: 0.03996111
case acc: 0.030391091
case acc: 0.048092708
case acc: 0.055164788
top acc: 0.0832 ::: bot acc: 0.0197
top acc: 0.0981 ::: bot acc: 0.0263
top acc: 0.0646 ::: bot acc: 0.0429
top acc: 0.0591 ::: bot acc: 0.0093
top acc: 0.0831 ::: bot acc: 0.0237
top acc: 0.0895 ::: bot acc: 0.0308
current epoch: 22
train loss is 0.001393
average val loss: 0.001585, accuracy: 0.0457
average test loss: 0.001564, accuracy: 0.0448
case acc: 0.04042583
case acc: 0.057338797
case acc: 0.039608143
case acc: 0.030016437
case acc: 0.04726792
case acc: 0.05410232
top acc: 0.0833 ::: bot acc: 0.0198
top acc: 0.0939 ::: bot acc: 0.0219
top acc: 0.0643 ::: bot acc: 0.0430
top acc: 0.0582 ::: bot acc: 0.0095
top acc: 0.0818 ::: bot acc: 0.0244
top acc: 0.0875 ::: bot acc: 0.0312
current epoch: 23
train loss is 0.001334
average val loss: 0.001623, accuracy: 0.0462
average test loss: 0.001606, accuracy: 0.0456
case acc: 0.041422505
case acc: 0.05709663
case acc: 0.04006241
case acc: 0.03187726
case acc: 0.048063613
case acc: 0.05488306
top acc: 0.0862 ::: bot acc: 0.0170
top acc: 0.0935 ::: bot acc: 0.0223
top acc: 0.0668 ::: bot acc: 0.0400
top acc: 0.0609 ::: bot acc: 0.0100
top acc: 0.0828 ::: bot acc: 0.0243
top acc: 0.0888 ::: bot acc: 0.0310
current epoch: 24
train loss is 0.001328
average val loss: 0.001694, accuracy: 0.0474
average test loss: 0.001678, accuracy: 0.0467
case acc: 0.042738922
case acc: 0.057288736
case acc: 0.040882755
case acc: 0.034237474
case acc: 0.048984125
case acc: 0.056085397
top acc: 0.0899 ::: bot acc: 0.0136
top acc: 0.0937 ::: bot acc: 0.0224
top acc: 0.0707 ::: bot acc: 0.0364
top acc: 0.0640 ::: bot acc: 0.0108
top acc: 0.0846 ::: bot acc: 0.0233
top acc: 0.0913 ::: bot acc: 0.0301
current epoch: 25
train loss is 0.001341
average val loss: 0.001721, accuracy: 0.0478
average test loss: 0.001706, accuracy: 0.0472
case acc: 0.04364691
case acc: 0.05651831
case acc: 0.04140901
case acc: 0.035684884
case acc: 0.04941607
case acc: 0.056406025
top acc: 0.0920 ::: bot acc: 0.0119
top acc: 0.0930 ::: bot acc: 0.0215
top acc: 0.0726 ::: bot acc: 0.0344
top acc: 0.0660 ::: bot acc: 0.0117
top acc: 0.0851 ::: bot acc: 0.0234
top acc: 0.0919 ::: bot acc: 0.0300
current epoch: 26
train loss is 0.001325
average val loss: 0.001765, accuracy: 0.0485
average test loss: 0.001751, accuracy: 0.0480
case acc: 0.044987705
case acc: 0.056371287
case acc: 0.04238279
case acc: 0.037354548
case acc: 0.04996377
case acc: 0.05693776
top acc: 0.0943 ::: bot acc: 0.0105
top acc: 0.0929 ::: bot acc: 0.0214
top acc: 0.0755 ::: bot acc: 0.0313
top acc: 0.0679 ::: bot acc: 0.0122
top acc: 0.0861 ::: bot acc: 0.0227
top acc: 0.0925 ::: bot acc: 0.0301
current epoch: 27
train loss is 0.001328
average val loss: 0.001788, accuracy: 0.0488
average test loss: 0.001771, accuracy: 0.0483
case acc: 0.046199262
case acc: 0.055501837
case acc: 0.042937353
case acc: 0.038330436
case acc: 0.050107855
case acc: 0.05687369
top acc: 0.0963 ::: bot acc: 0.0104
top acc: 0.0918 ::: bot acc: 0.0209
top acc: 0.0774 ::: bot acc: 0.0292
top acc: 0.0690 ::: bot acc: 0.0133
top acc: 0.0863 ::: bot acc: 0.0227
top acc: 0.0928 ::: bot acc: 0.0297
current epoch: 28
train loss is 0.001328
average val loss: 0.001881, accuracy: 0.0502
average test loss: 0.001868, accuracy: 0.0499
case acc: 0.048086226
case acc: 0.056316465
case acc: 0.044636417
case acc: 0.041032232
case acc: 0.051659495
case acc: 0.05796413
top acc: 0.0995 ::: bot acc: 0.0097
top acc: 0.0929 ::: bot acc: 0.0211
top acc: 0.0814 ::: bot acc: 0.0266
top acc: 0.0720 ::: bot acc: 0.0153
top acc: 0.0890 ::: bot acc: 0.0222
top acc: 0.0945 ::: bot acc: 0.0291
current epoch: 29
train loss is 0.001309
average val loss: 0.001896, accuracy: 0.0505
average test loss: 0.001882, accuracy: 0.0502
case acc: 0.049142607
case acc: 0.055476718
case acc: 0.045165762
case acc: 0.04170779
case acc: 0.05169689
case acc: 0.05808203
top acc: 0.1008 ::: bot acc: 0.0102
top acc: 0.0921 ::: bot acc: 0.0205
top acc: 0.0827 ::: bot acc: 0.0253
top acc: 0.0726 ::: bot acc: 0.0154
top acc: 0.0890 ::: bot acc: 0.0221
top acc: 0.0944 ::: bot acc: 0.0296
current epoch: 30
train loss is 0.001294
average val loss: 0.001922, accuracy: 0.0509
average test loss: 0.001908, accuracy: 0.0507
case acc: 0.049814735
case acc: 0.054809064
case acc: 0.046194255
case acc: 0.04253664
case acc: 0.052333463
case acc: 0.05822302
top acc: 0.1017 ::: bot acc: 0.0099
top acc: 0.0915 ::: bot acc: 0.0200
top acc: 0.0847 ::: bot acc: 0.0252
top acc: 0.0736 ::: bot acc: 0.0160
top acc: 0.0903 ::: bot acc: 0.0221
top acc: 0.0946 ::: bot acc: 0.0295
current epoch: 31
train loss is 0.001297
average val loss: 0.001856, accuracy: 0.0500
average test loss: 0.001843, accuracy: 0.0496
case acc: 0.049041785
case acc: 0.052087236
case acc: 0.045622393
case acc: 0.04182356
case acc: 0.051583037
case acc: 0.057272553
top acc: 0.1011 ::: bot acc: 0.0099
top acc: 0.0884 ::: bot acc: 0.0178
top acc: 0.0836 ::: bot acc: 0.0256
top acc: 0.0728 ::: bot acc: 0.0157
top acc: 0.0889 ::: bot acc: 0.0225
top acc: 0.0931 ::: bot acc: 0.0303
current epoch: 32
train loss is 0.001257
average val loss: 0.001795, accuracy: 0.0491
average test loss: 0.001777, accuracy: 0.0484
case acc: 0.04820816
case acc: 0.049581528
case acc: 0.04513274
case acc: 0.040683825
case acc: 0.050935026
case acc: 0.056148127
top acc: 0.0996 ::: bot acc: 0.0096
top acc: 0.0852 ::: bot acc: 0.0162
top acc: 0.0826 ::: bot acc: 0.0260
top acc: 0.0716 ::: bot acc: 0.0150
top acc: 0.0878 ::: bot acc: 0.0224
top acc: 0.0912 ::: bot acc: 0.0303
current epoch: 33
train loss is 0.001216
average val loss: 0.001714, accuracy: 0.0479
average test loss: 0.001699, accuracy: 0.0472
case acc: 0.047424763
case acc: 0.046729498
case acc: 0.044514246
case acc: 0.03930583
case acc: 0.049821757
case acc: 0.055127256
top acc: 0.0984 ::: bot acc: 0.0100
top acc: 0.0819 ::: bot acc: 0.0147
top acc: 0.0809 ::: bot acc: 0.0272
top acc: 0.0703 ::: bot acc: 0.0136
top acc: 0.0859 ::: bot acc: 0.0228
top acc: 0.0894 ::: bot acc: 0.0310
current epoch: 34
train loss is 0.001186
average val loss: 0.001708, accuracy: 0.0479
average test loss: 0.001690, accuracy: 0.0470
case acc: 0.047294687
case acc: 0.045603473
case acc: 0.04468415
case acc: 0.039589655
case acc: 0.0499267
case acc: 0.054955427
top acc: 0.0981 ::: bot acc: 0.0100
top acc: 0.0805 ::: bot acc: 0.0141
top acc: 0.0813 ::: bot acc: 0.0269
top acc: 0.0702 ::: bot acc: 0.0145
top acc: 0.0861 ::: bot acc: 0.0227
top acc: 0.0894 ::: bot acc: 0.0308
current epoch: 35
train loss is 0.001159
average val loss: 0.001638, accuracy: 0.0468
average test loss: 0.001616, accuracy: 0.0457
case acc: 0.046165377
case acc: 0.043061323
case acc: 0.044022925
case acc: 0.03826524
case acc: 0.04888825
case acc: 0.05402855
top acc: 0.0965 ::: bot acc: 0.0103
top acc: 0.0772 ::: bot acc: 0.0130
top acc: 0.0800 ::: bot acc: 0.0279
top acc: 0.0688 ::: bot acc: 0.0132
top acc: 0.0842 ::: bot acc: 0.0235
top acc: 0.0872 ::: bot acc: 0.0321
current epoch: 36
train loss is 0.001140
average val loss: 0.001622, accuracy: 0.0465
average test loss: 0.001596, accuracy: 0.0454
case acc: 0.045972668
case acc: 0.042384785
case acc: 0.043894857
case acc: 0.038255215
case acc: 0.04835476
case acc: 0.053622175
top acc: 0.0961 ::: bot acc: 0.0106
top acc: 0.0759 ::: bot acc: 0.0131
top acc: 0.0800 ::: bot acc: 0.0276
top acc: 0.0687 ::: bot acc: 0.0132
top acc: 0.0832 ::: bot acc: 0.0237
top acc: 0.0868 ::: bot acc: 0.0321
current epoch: 37
train loss is 0.001129
average val loss: 0.001618, accuracy: 0.0465
average test loss: 0.001594, accuracy: 0.0454
case acc: 0.04608106
case acc: 0.04169523
case acc: 0.044273213
case acc: 0.038544845
case acc: 0.04821825
case acc: 0.05358807
top acc: 0.0961 ::: bot acc: 0.0104
top acc: 0.0750 ::: bot acc: 0.0129
top acc: 0.0803 ::: bot acc: 0.0278
top acc: 0.0692 ::: bot acc: 0.0135
top acc: 0.0831 ::: bot acc: 0.0240
top acc: 0.0865 ::: bot acc: 0.0321
current epoch: 38
train loss is 0.001111
average val loss: 0.001646, accuracy: 0.0469
average test loss: 0.001620, accuracy: 0.0458
case acc: 0.046373658
case acc: 0.041968122
case acc: 0.044581994
case acc: 0.03952207
case acc: 0.04872806
case acc: 0.05373372
top acc: 0.0968 ::: bot acc: 0.0102
top acc: 0.0757 ::: bot acc: 0.0128
top acc: 0.0814 ::: bot acc: 0.0266
top acc: 0.0704 ::: bot acc: 0.0140
top acc: 0.0839 ::: bot acc: 0.0237
top acc: 0.0870 ::: bot acc: 0.0318
current epoch: 39
train loss is 0.001109
average val loss: 0.001678, accuracy: 0.0474
average test loss: 0.001653, accuracy: 0.0464
case acc: 0.0470557
case acc: 0.041926246
case acc: 0.045495026
case acc: 0.040476944
case acc: 0.049168594
case acc: 0.05420881
top acc: 0.0979 ::: bot acc: 0.0100
top acc: 0.0756 ::: bot acc: 0.0128
top acc: 0.0831 ::: bot acc: 0.0257
top acc: 0.0713 ::: bot acc: 0.0147
top acc: 0.0848 ::: bot acc: 0.0233
top acc: 0.0879 ::: bot acc: 0.0315
current epoch: 40
train loss is 0.001116
average val loss: 0.001675, accuracy: 0.0473
average test loss: 0.001643, accuracy: 0.0462
case acc: 0.04693184
case acc: 0.041258313
case acc: 0.045570944
case acc: 0.040534325
case acc: 0.04914596
case acc: 0.053931862
top acc: 0.0977 ::: bot acc: 0.0100
top acc: 0.0746 ::: bot acc: 0.0127
top acc: 0.0834 ::: bot acc: 0.0256
top acc: 0.0714 ::: bot acc: 0.0149
top acc: 0.0846 ::: bot acc: 0.0235
top acc: 0.0874 ::: bot acc: 0.0318
current epoch: 41
train loss is 0.001098
average val loss: 0.001639, accuracy: 0.0468
average test loss: 0.001613, accuracy: 0.0457
case acc: 0.046492483
case acc: 0.04038678
case acc: 0.04519738
case acc: 0.03997327
case acc: 0.048607834
case acc: 0.053448074
top acc: 0.0970 ::: bot acc: 0.0102
top acc: 0.0733 ::: bot acc: 0.0126
top acc: 0.0826 ::: bot acc: 0.0260
top acc: 0.0708 ::: bot acc: 0.0143
top acc: 0.0840 ::: bot acc: 0.0235
top acc: 0.0863 ::: bot acc: 0.0324
current epoch: 42
train loss is 0.001092
average val loss: 0.001655, accuracy: 0.0470
average test loss: 0.001621, accuracy: 0.0458
case acc: 0.04661622
case acc: 0.040140405
case acc: 0.04549703
case acc: 0.04045529
case acc: 0.048672084
case acc: 0.053599887
top acc: 0.0973 ::: bot acc: 0.0101
top acc: 0.0730 ::: bot acc: 0.0123
top acc: 0.0831 ::: bot acc: 0.0254
top acc: 0.0713 ::: bot acc: 0.0148
top acc: 0.0841 ::: bot acc: 0.0232
top acc: 0.0867 ::: bot acc: 0.0320
current epoch: 43
train loss is 0.001085
average val loss: 0.001615, accuracy: 0.0464
average test loss: 0.001581, accuracy: 0.0451
case acc: 0.045800548
case acc: 0.038957
case acc: 0.045269407
case acc: 0.03976338
case acc: 0.048249476
case acc: 0.052834664
top acc: 0.0958 ::: bot acc: 0.0107
top acc: 0.0713 ::: bot acc: 0.0124
top acc: 0.0829 ::: bot acc: 0.0259
top acc: 0.0706 ::: bot acc: 0.0142
top acc: 0.0833 ::: bot acc: 0.0237
top acc: 0.0853 ::: bot acc: 0.0325
current epoch: 44
train loss is 0.001060
average val loss: 0.001561, accuracy: 0.0455
average test loss: 0.001527, accuracy: 0.0443
case acc: 0.044744022
case acc: 0.03784436
case acc: 0.044747993
case acc: 0.038774382
case acc: 0.047322392
case acc: 0.052107874
top acc: 0.0936 ::: bot acc: 0.0114
top acc: 0.0697 ::: bot acc: 0.0124
top acc: 0.0816 ::: bot acc: 0.0269
top acc: 0.0694 ::: bot acc: 0.0134
top acc: 0.0815 ::: bot acc: 0.0245
top acc: 0.0839 ::: bot acc: 0.0334
current epoch: 45
train loss is 0.001050
average val loss: 0.001564, accuracy: 0.0456
average test loss: 0.001530, accuracy: 0.0443
case acc: 0.044809863
case acc: 0.03771827
case acc: 0.044867653
case acc: 0.038913697
case acc: 0.04739804
case acc: 0.05220046
top acc: 0.0939 ::: bot acc: 0.0114
top acc: 0.0692 ::: bot acc: 0.0127
top acc: 0.0817 ::: bot acc: 0.0269
top acc: 0.0697 ::: bot acc: 0.0135
top acc: 0.0816 ::: bot acc: 0.0246
top acc: 0.0841 ::: bot acc: 0.0333
current epoch: 46
train loss is 0.001048
average val loss: 0.001584, accuracy: 0.0459
average test loss: 0.001551, accuracy: 0.0446
case acc: 0.04511345
case acc: 0.03778652
case acc: 0.045201506
case acc: 0.03963437
case acc: 0.047633972
case acc: 0.052430913
top acc: 0.0945 ::: bot acc: 0.0112
top acc: 0.0695 ::: bot acc: 0.0125
top acc: 0.0828 ::: bot acc: 0.0258
top acc: 0.0704 ::: bot acc: 0.0141
top acc: 0.0824 ::: bot acc: 0.0240
top acc: 0.0846 ::: bot acc: 0.0329
current epoch: 47
train loss is 0.001048
average val loss: 0.001600, accuracy: 0.0462
average test loss: 0.001567, accuracy: 0.0449
case acc: 0.0452113
case acc: 0.037832472
case acc: 0.04566338
case acc: 0.040214896
case acc: 0.04811283
case acc: 0.05266411
top acc: 0.0947 ::: bot acc: 0.0110
top acc: 0.0696 ::: bot acc: 0.0124
top acc: 0.0835 ::: bot acc: 0.0256
top acc: 0.0709 ::: bot acc: 0.0147
top acc: 0.0829 ::: bot acc: 0.0239
top acc: 0.0850 ::: bot acc: 0.0329
current epoch: 48
train loss is 0.001041
average val loss: 0.001582, accuracy: 0.0459
average test loss: 0.001545, accuracy: 0.0445
case acc: 0.044862963
case acc: 0.037202932
case acc: 0.045345217
case acc: 0.03974408
case acc: 0.04780894
case acc: 0.05230054
top acc: 0.0942 ::: bot acc: 0.0111
top acc: 0.0685 ::: bot acc: 0.0125
top acc: 0.0831 ::: bot acc: 0.0257
top acc: 0.0706 ::: bot acc: 0.0141
top acc: 0.0825 ::: bot acc: 0.0239
top acc: 0.0841 ::: bot acc: 0.0335
current epoch: 49
train loss is 0.001037
average val loss: 0.001573, accuracy: 0.0457
average test loss: 0.001533, accuracy: 0.0443
case acc: 0.044518895
case acc: 0.03680581
case acc: 0.045365024
case acc: 0.03954429
case acc: 0.047820367
case acc: 0.051985905
top acc: 0.0934 ::: bot acc: 0.0114
top acc: 0.0680 ::: bot acc: 0.0126
top acc: 0.0829 ::: bot acc: 0.0257
top acc: 0.0702 ::: bot acc: 0.0141
top acc: 0.0826 ::: bot acc: 0.0241
top acc: 0.0837 ::: bot acc: 0.0335
current epoch: 50
train loss is 0.001031
average val loss: 0.001566, accuracy: 0.0456
average test loss: 0.001525, accuracy: 0.0442
case acc: 0.044331454
case acc: 0.03663007
case acc: 0.045201115
case acc: 0.039435968
case acc: 0.04783361
case acc: 0.05180848
top acc: 0.0932 ::: bot acc: 0.0115
top acc: 0.0675 ::: bot acc: 0.0129
top acc: 0.0829 ::: bot acc: 0.0254
top acc: 0.0700 ::: bot acc: 0.0139
top acc: 0.0826 ::: bot acc: 0.0241
top acc: 0.0832 ::: bot acc: 0.0339

		{"drop_out": 0.2, "drop_out_mc": 0.1, "repeat_mc": 50, "hidden": 30, "embedding_size": 5, "batch": 512, "lag": 5}
LME_Co_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5370 5370 5370
1.7082474 -0.6288155 0.24786325 -0.22413088
Validation: 600 600 600
Testing: 774 774 774
pre-processing time: 0.0002224445343017578
the split date is 2010-07-01
train dropout: 0.2 test dropout: 0.1
net initializing with time: 0.0035958290100097656
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.007855
average val loss: 0.005070, accuracy: 0.0910
average test loss: 0.006337, accuracy: 0.1045
case acc: 0.14730105
case acc: 0.087843694
case acc: 0.109109566
case acc: 0.09644524
case acc: 0.12226857
case acc: 0.06404207
top acc: 0.1263 ::: bot acc: 0.1696
top acc: 0.1298 ::: bot acc: 0.0441
top acc: 0.0642 ::: bot acc: 0.1599
top acc: 0.0627 ::: bot acc: 0.1303
top acc: 0.0891 ::: bot acc: 0.1558
top acc: 0.0355 ::: bot acc: 0.0979
current epoch: 2
train loss is 0.006721
average val loss: 0.004558, accuracy: 0.0659
average test loss: 0.003745, accuracy: 0.0590
case acc: 0.030752784
case acc: 0.18947652
case acc: 0.036767364
case acc: 0.027227702
case acc: 0.027768705
case acc: 0.042252287
top acc: 0.0117 ::: bot acc: 0.0521
top acc: 0.2313 ::: bot acc: 0.1463
top acc: 0.0526 ::: bot acc: 0.0433
top acc: 0.0461 ::: bot acc: 0.0205
top acc: 0.0175 ::: bot acc: 0.0492
top acc: 0.0658 ::: bot acc: 0.0173
current epoch: 3
train loss is 0.008213
average val loss: 0.013836, accuracy: 0.1483
average test loss: 0.011149, accuracy: 0.1294
case acc: 0.074212864
case acc: 0.27733806
case acc: 0.109539926
case acc: 0.10981789
case acc: 0.07850419
case acc: 0.12698056
top acc: 0.0940 ::: bot acc: 0.0545
top acc: 0.3187 ::: bot acc: 0.2346
top acc: 0.1541 ::: bot acc: 0.0601
top acc: 0.1426 ::: bot acc: 0.0759
top acc: 0.1121 ::: bot acc: 0.0455
top acc: 0.1556 ::: bot acc: 0.0925
current epoch: 4
train loss is 0.011868
average val loss: 0.014755, accuracy: 0.1562
average test loss: 0.011931, accuracy: 0.1372
case acc: 0.08784646
case acc: 0.27720025
case acc: 0.121011555
case acc: 0.1174244
case acc: 0.089347
case acc: 0.13058113
top acc: 0.1077 ::: bot acc: 0.0669
top acc: 0.3190 ::: bot acc: 0.2340
top acc: 0.1663 ::: bot acc: 0.0712
top acc: 0.1506 ::: bot acc: 0.0836
top acc: 0.1234 ::: bot acc: 0.0564
top acc: 0.1599 ::: bot acc: 0.0953
current epoch: 5
train loss is 0.013254
average val loss: 0.002587, accuracy: 0.0495
average test loss: 0.002315, accuracy: 0.0504
case acc: 0.044031244
case acc: 0.1322586
case acc: 0.03531395
case acc: 0.028106479
case acc: 0.03909695
case acc: 0.02386854
top acc: 0.0235 ::: bot acc: 0.0666
top acc: 0.1741 ::: bot acc: 0.0896
top acc: 0.0310 ::: bot acc: 0.0633
top acc: 0.0172 ::: bot acc: 0.0515
top acc: 0.0092 ::: bot acc: 0.0710
top acc: 0.0240 ::: bot acc: 0.0404
current epoch: 6
train loss is 0.006426
average val loss: 0.002198, accuracy: 0.0558
average test loss: 0.002662, accuracy: 0.0645
case acc: 0.07995504
case acc: 0.08160086
case acc: 0.059703827
case acc: 0.056427453
case acc: 0.06625832
case acc: 0.043065287
top acc: 0.0595 ::: bot acc: 0.1014
top acc: 0.1231 ::: bot acc: 0.0386
top acc: 0.0222 ::: bot acc: 0.1059
top acc: 0.0241 ::: bot acc: 0.0900
top acc: 0.0320 ::: bot acc: 0.0997
top acc: 0.0152 ::: bot acc: 0.0773
current epoch: 7
train loss is 0.003244
average val loss: 0.002288, accuracy: 0.0467
average test loss: 0.001846, accuracy: 0.0440
case acc: 0.03095978
case acc: 0.11997091
case acc: 0.0348402
case acc: 0.025458727
case acc: 0.02729767
case acc: 0.025661224
top acc: 0.0120 ::: bot acc: 0.0523
top acc: 0.1619 ::: bot acc: 0.0765
top acc: 0.0337 ::: bot acc: 0.0618
top acc: 0.0244 ::: bot acc: 0.0417
top acc: 0.0218 ::: bot acc: 0.0457
top acc: 0.0345 ::: bot acc: 0.0302
current epoch: 8
train loss is 0.003580
average val loss: 0.002231, accuracy: 0.0472
average test loss: 0.001756, accuracy: 0.0425
case acc: 0.024940766
case acc: 0.11770574
case acc: 0.03491114
case acc: 0.024995176
case acc: 0.02506289
case acc: 0.027136544
top acc: 0.0079 ::: bot acc: 0.0455
top acc: 0.1596 ::: bot acc: 0.0745
top acc: 0.0373 ::: bot acc: 0.0584
top acc: 0.0285 ::: bot acc: 0.0380
top acc: 0.0308 ::: bot acc: 0.0363
top acc: 0.0401 ::: bot acc: 0.0255
current epoch: 9
train loss is 0.003601
average val loss: 0.001719, accuracy: 0.0414
average test loss: 0.001494, accuracy: 0.0413
case acc: 0.03411879
case acc: 0.09699407
case acc: 0.03778672
case acc: 0.027724555
case acc: 0.027448302
case acc: 0.023571406
top acc: 0.0148 ::: bot acc: 0.0551
top acc: 0.1395 ::: bot acc: 0.0539
top acc: 0.0238 ::: bot acc: 0.0718
top acc: 0.0165 ::: bot acc: 0.0508
top acc: 0.0235 ::: bot acc: 0.0461
top acc: 0.0262 ::: bot acc: 0.0374
current epoch: 10
train loss is 0.003046
average val loss: 0.001531, accuracy: 0.0396
average test loss: 0.001369, accuracy: 0.0400
case acc: 0.034261182
case acc: 0.087544866
case acc: 0.039454393
case acc: 0.028800037
case acc: 0.026397096
case acc: 0.023603274
top acc: 0.0146 ::: bot acc: 0.0558
top acc: 0.1290 ::: bot acc: 0.0439
top acc: 0.0207 ::: bot acc: 0.0760
top acc: 0.0152 ::: bot acc: 0.0525
top acc: 0.0241 ::: bot acc: 0.0437
top acc: 0.0244 ::: bot acc: 0.0392
current epoch: 11
train loss is 0.002749
average val loss: 0.001491, accuracy: 0.0396
average test loss: 0.001272, accuracy: 0.0381
case acc: 0.028963182
case acc: 0.0847915
case acc: 0.038092732
case acc: 0.027414547
case acc: 0.025418473
case acc: 0.023958184
top acc: 0.0109 ::: bot acc: 0.0496
top acc: 0.1269 ::: bot acc: 0.0416
top acc: 0.0229 ::: bot acc: 0.0733
top acc: 0.0191 ::: bot acc: 0.0489
top acc: 0.0311 ::: bot acc: 0.0365
top acc: 0.0289 ::: bot acc: 0.0353
current epoch: 12
train loss is 0.002594
average val loss: 0.001490, accuracy: 0.0403
average test loss: 0.001197, accuracy: 0.0367
case acc: 0.023455225
case acc: 0.08260634
case acc: 0.03682522
case acc: 0.02639405
case acc: 0.025349407
case acc: 0.025408518
top acc: 0.0072 ::: bot acc: 0.0433
top acc: 0.1245 ::: bot acc: 0.0394
top acc: 0.0255 ::: bot acc: 0.0696
top acc: 0.0233 ::: bot acc: 0.0450
top acc: 0.0370 ::: bot acc: 0.0307
top acc: 0.0335 ::: bot acc: 0.0318
current epoch: 13
train loss is 0.002509
average val loss: 0.001398, accuracy: 0.0394
average test loss: 0.001112, accuracy: 0.0355
case acc: 0.022388913
case acc: 0.07653761
case acc: 0.037071377
case acc: 0.025846777
case acc: 0.025956068
case acc: 0.025491003
top acc: 0.0072 ::: bot acc: 0.0419
top acc: 0.1189 ::: bot acc: 0.0324
top acc: 0.0249 ::: bot acc: 0.0697
top acc: 0.0233 ::: bot acc: 0.0437
top acc: 0.0392 ::: bot acc: 0.0296
top acc: 0.0335 ::: bot acc: 0.0317
current epoch: 14
train loss is 0.002345
average val loss: 0.001245, accuracy: 0.0373
average test loss: 0.001027, accuracy: 0.0344
case acc: 0.023575464
case acc: 0.06818638
case acc: 0.037744943
case acc: 0.026650636
case acc: 0.025502505
case acc: 0.024864431
top acc: 0.0076 ::: bot acc: 0.0437
top acc: 0.1105 ::: bot acc: 0.0247
top acc: 0.0216 ::: bot acc: 0.0731
top acc: 0.0203 ::: bot acc: 0.0467
top acc: 0.0372 ::: bot acc: 0.0308
top acc: 0.0308 ::: bot acc: 0.0343
current epoch: 15
train loss is 0.002178
average val loss: 0.001209, accuracy: 0.0372
average test loss: 0.000968, accuracy: 0.0334
case acc: 0.02146178
case acc: 0.06429633
case acc: 0.03789456
case acc: 0.026172357
case acc: 0.025484918
case acc: 0.025085922
top acc: 0.0071 ::: bot acc: 0.0400
top acc: 0.1057 ::: bot acc: 0.0214
top acc: 0.0231 ::: bot acc: 0.0726
top acc: 0.0230 ::: bot acc: 0.0444
top acc: 0.0393 ::: bot acc: 0.0283
top acc: 0.0322 ::: bot acc: 0.0328
current epoch: 16
train loss is 0.002132
average val loss: 0.001168, accuracy: 0.0367
average test loss: 0.000915, accuracy: 0.0324
case acc: 0.020058854
case acc: 0.06040698
case acc: 0.037471175
case acc: 0.025709637
case acc: 0.025803829
case acc: 0.024987288
top acc: 0.0074 ::: bot acc: 0.0388
top acc: 0.1021 ::: bot acc: 0.0179
top acc: 0.0236 ::: bot acc: 0.0719
top acc: 0.0238 ::: bot acc: 0.0430
top acc: 0.0404 ::: bot acc: 0.0277
top acc: 0.0325 ::: bot acc: 0.0318
current epoch: 17
train loss is 0.001993
average val loss: 0.001086, accuracy: 0.0355
average test loss: 0.000862, accuracy: 0.0315
case acc: 0.019762678
case acc: 0.055679765
case acc: 0.038004786
case acc: 0.02564517
case acc: 0.025363786
case acc: 0.024740685
top acc: 0.0073 ::: bot acc: 0.0377
top acc: 0.0967 ::: bot acc: 0.0142
top acc: 0.0234 ::: bot acc: 0.0725
top acc: 0.0226 ::: bot acc: 0.0438
top acc: 0.0392 ::: bot acc: 0.0279
top acc: 0.0312 ::: bot acc: 0.0333
current epoch: 18
train loss is 0.001955
average val loss: 0.001062, accuracy: 0.0353
average test loss: 0.000829, accuracy: 0.0309
case acc: 0.018584967
case acc: 0.052969754
case acc: 0.03765333
case acc: 0.02574348
case acc: 0.025515564
case acc: 0.024808928
top acc: 0.0081 ::: bot acc: 0.0351
top acc: 0.0934 ::: bot acc: 0.0127
top acc: 0.0239 ::: bot acc: 0.0718
top acc: 0.0244 ::: bot acc: 0.0430
top acc: 0.0405 ::: bot acc: 0.0272
top acc: 0.0322 ::: bot acc: 0.0325
current epoch: 19
train loss is 0.001906
average val loss: 0.001062, accuracy: 0.0356
average test loss: 0.000800, accuracy: 0.0304
case acc: 0.01729325
case acc: 0.051608235
case acc: 0.036712278
case acc: 0.025422413
case acc: 0.026193514
case acc: 0.025043081
top acc: 0.0101 ::: bot acc: 0.0323
top acc: 0.0918 ::: bot acc: 0.0120
top acc: 0.0262 ::: bot acc: 0.0688
top acc: 0.0264 ::: bot acc: 0.0405
top acc: 0.0421 ::: bot acc: 0.0263
top acc: 0.0331 ::: bot acc: 0.0311
current epoch: 20
train loss is 0.001847
average val loss: 0.001028, accuracy: 0.0350
average test loss: 0.000770, accuracy: 0.0298
case acc: 0.016750613
case acc: 0.049375292
case acc: 0.036491297
case acc: 0.025368514
case acc: 0.02581894
case acc: 0.025177188
top acc: 0.0116 ::: bot acc: 0.0308
top acc: 0.0887 ::: bot acc: 0.0116
top acc: 0.0271 ::: bot acc: 0.0686
top acc: 0.0275 ::: bot acc: 0.0398
top acc: 0.0414 ::: bot acc: 0.0264
top acc: 0.0334 ::: bot acc: 0.0317
current epoch: 21
train loss is 0.001824
average val loss: 0.001056, accuracy: 0.0359
average test loss: 0.000751, accuracy: 0.0295
case acc: 0.016089695
case acc: 0.04874015
case acc: 0.035606235
case acc: 0.024976179
case acc: 0.026015984
case acc: 0.025749307
top acc: 0.0145 ::: bot acc: 0.0279
top acc: 0.0882 ::: bot acc: 0.0109
top acc: 0.0296 ::: bot acc: 0.0652
top acc: 0.0300 ::: bot acc: 0.0373
top acc: 0.0422 ::: bot acc: 0.0255
top acc: 0.0347 ::: bot acc: 0.0302
current epoch: 22
train loss is 0.001806
average val loss: 0.001047, accuracy: 0.0359
average test loss: 0.000733, accuracy: 0.0292
case acc: 0.015759502
case acc: 0.047463365
case acc: 0.03521642
case acc: 0.024753619
case acc: 0.026045304
case acc: 0.025741441
top acc: 0.0164 ::: bot acc: 0.0263
top acc: 0.0862 ::: bot acc: 0.0111
top acc: 0.0319 ::: bot acc: 0.0637
top acc: 0.0307 ::: bot acc: 0.0362
top acc: 0.0428 ::: bot acc: 0.0251
top acc: 0.0345 ::: bot acc: 0.0305
current epoch: 23
train loss is 0.001793
average val loss: 0.001106, accuracy: 0.0372
average test loss: 0.000736, accuracy: 0.0294
case acc: 0.015466738
case acc: 0.04797671
case acc: 0.03456257
case acc: 0.025209501
case acc: 0.026485402
case acc: 0.02658158
top acc: 0.0206 ::: bot acc: 0.0218
top acc: 0.0867 ::: bot acc: 0.0111
top acc: 0.0356 ::: bot acc: 0.0598
top acc: 0.0347 ::: bot acc: 0.0330
top acc: 0.0448 ::: bot acc: 0.0234
top acc: 0.0368 ::: bot acc: 0.0284
current epoch: 24
train loss is 0.001781
average val loss: 0.001133, accuracy: 0.0379
average test loss: 0.000735, accuracy: 0.0295
case acc: 0.015783215
case acc: 0.04790669
case acc: 0.03489112
case acc: 0.025218159
case acc: 0.026496256
case acc: 0.026746698
top acc: 0.0233 ::: bot acc: 0.0193
top acc: 0.0866 ::: bot acc: 0.0113
top acc: 0.0392 ::: bot acc: 0.0572
top acc: 0.0366 ::: bot acc: 0.0308
top acc: 0.0458 ::: bot acc: 0.0219
top acc: 0.0383 ::: bot acc: 0.0266
current epoch: 25
train loss is 0.001780
average val loss: 0.001193, accuracy: 0.0393
average test loss: 0.000741, accuracy: 0.0299
case acc: 0.016843652
case acc: 0.04808383
case acc: 0.034431424
case acc: 0.025614161
case acc: 0.026832666
case acc: 0.027388448
top acc: 0.0264 ::: bot acc: 0.0167
top acc: 0.0868 ::: bot acc: 0.0112
top acc: 0.0413 ::: bot acc: 0.0537
top acc: 0.0391 ::: bot acc: 0.0282
top acc: 0.0469 ::: bot acc: 0.0204
top acc: 0.0398 ::: bot acc: 0.0254
current epoch: 26
train loss is 0.001764
average val loss: 0.001156, accuracy: 0.0386
average test loss: 0.000720, accuracy: 0.0295
case acc: 0.01687339
case acc: 0.04609961
case acc: 0.034573518
case acc: 0.025484087
case acc: 0.026987165
case acc: 0.02678517
top acc: 0.0272 ::: bot acc: 0.0161
top acc: 0.0843 ::: bot acc: 0.0109
top acc: 0.0422 ::: bot acc: 0.0534
top acc: 0.0389 ::: bot acc: 0.0282
top acc: 0.0465 ::: bot acc: 0.0218
top acc: 0.0386 ::: bot acc: 0.0261
current epoch: 27
train loss is 0.001748
average val loss: 0.001111, accuracy: 0.0377
average test loss: 0.000688, accuracy: 0.0289
case acc: 0.016385913
case acc: 0.044328105
case acc: 0.034434702
case acc: 0.02508142
case acc: 0.026584633
case acc: 0.026464473
top acc: 0.0264 ::: bot acc: 0.0164
top acc: 0.0807 ::: bot acc: 0.0121
top acc: 0.0417 ::: bot acc: 0.0539
top acc: 0.0376 ::: bot acc: 0.0290
top acc: 0.0450 ::: bot acc: 0.0232
top acc: 0.0372 ::: bot acc: 0.0281
current epoch: 28
train loss is 0.001706
average val loss: 0.001091, accuracy: 0.0373
average test loss: 0.000671, accuracy: 0.0286
case acc: 0.016572412
case acc: 0.042797238
case acc: 0.034409188
case acc: 0.025464255
case acc: 0.02629392
case acc: 0.026140368
top acc: 0.0257 ::: bot acc: 0.0168
top acc: 0.0782 ::: bot acc: 0.0123
top acc: 0.0421 ::: bot acc: 0.0530
top acc: 0.0378 ::: bot acc: 0.0291
top acc: 0.0445 ::: bot acc: 0.0232
top acc: 0.0360 ::: bot acc: 0.0289
current epoch: 29
train loss is 0.001683
average val loss: 0.001149, accuracy: 0.0385
average test loss: 0.000690, accuracy: 0.0291
case acc: 0.017632592
case acc: 0.043295972
case acc: 0.034919042
case acc: 0.02566463
case acc: 0.02649362
case acc: 0.026778905
top acc: 0.0287 ::: bot acc: 0.0146
top acc: 0.0797 ::: bot acc: 0.0119
top acc: 0.0446 ::: bot acc: 0.0509
top acc: 0.0406 ::: bot acc: 0.0267
top acc: 0.0459 ::: bot acc: 0.0215
top acc: 0.0382 ::: bot acc: 0.0274
current epoch: 30
train loss is 0.001699
average val loss: 0.001141, accuracy: 0.0383
average test loss: 0.000682, accuracy: 0.0290
case acc: 0.017732313
case acc: 0.042661015
case acc: 0.034973003
case acc: 0.025465297
case acc: 0.026789507
case acc: 0.026398866
top acc: 0.0292 ::: bot acc: 0.0142
top acc: 0.0784 ::: bot acc: 0.0124
top acc: 0.0456 ::: bot acc: 0.0501
top acc: 0.0406 ::: bot acc: 0.0263
top acc: 0.0461 ::: bot acc: 0.0221
top acc: 0.0370 ::: bot acc: 0.0281
current epoch: 31
train loss is 0.001671
average val loss: 0.001162, accuracy: 0.0387
average test loss: 0.000688, accuracy: 0.0292
case acc: 0.01840642
case acc: 0.042489223
case acc: 0.03511122
case acc: 0.025985667
case acc: 0.026974043
case acc: 0.02644462
top acc: 0.0300 ::: bot acc: 0.0137
top acc: 0.0779 ::: bot acc: 0.0125
top acc: 0.0473 ::: bot acc: 0.0486
top acc: 0.0425 ::: bot acc: 0.0254
top acc: 0.0467 ::: bot acc: 0.0211
top acc: 0.0375 ::: bot acc: 0.0275
current epoch: 32
train loss is 0.001666
average val loss: 0.001155, accuracy: 0.0386
average test loss: 0.000677, accuracy: 0.0290
case acc: 0.018593902
case acc: 0.04179411
case acc: 0.03512302
case acc: 0.025786016
case acc: 0.026676862
case acc: 0.02612949
top acc: 0.0304 ::: bot acc: 0.0139
top acc: 0.0767 ::: bot acc: 0.0131
top acc: 0.0483 ::: bot acc: 0.0472
top acc: 0.0421 ::: bot acc: 0.0253
top acc: 0.0463 ::: bot acc: 0.0216
top acc: 0.0366 ::: bot acc: 0.0281
current epoch: 33
train loss is 0.001665
average val loss: 0.001174, accuracy: 0.0390
average test loss: 0.000679, accuracy: 0.0292
case acc: 0.01929671
case acc: 0.041241884
case acc: 0.035582513
case acc: 0.026139392
case acc: 0.026711341
case acc: 0.026502993
top acc: 0.0315 ::: bot acc: 0.0133
top acc: 0.0757 ::: bot acc: 0.0130
top acc: 0.0496 ::: bot acc: 0.0464
top acc: 0.0429 ::: bot acc: 0.0244
top acc: 0.0468 ::: bot acc: 0.0205
top acc: 0.0376 ::: bot acc: 0.0276
current epoch: 34
train loss is 0.001646
average val loss: 0.001163, accuracy: 0.0387
average test loss: 0.000670, accuracy: 0.0291
case acc: 0.019182695
case acc: 0.040714543
case acc: 0.0353838
case acc: 0.026122607
case acc: 0.027061976
case acc: 0.02621019
top acc: 0.0316 ::: bot acc: 0.0136
top acc: 0.0747 ::: bot acc: 0.0137
top acc: 0.0496 ::: bot acc: 0.0458
top acc: 0.0427 ::: bot acc: 0.0242
top acc: 0.0470 ::: bot acc: 0.0210
top acc: 0.0370 ::: bot acc: 0.0282
current epoch: 35
train loss is 0.001616
average val loss: 0.001116, accuracy: 0.0377
average test loss: 0.000645, accuracy: 0.0285
case acc: 0.018377135
case acc: 0.039281636
case acc: 0.035213105
case acc: 0.02562459
case acc: 0.02663893
case acc: 0.025777329
top acc: 0.0301 ::: bot acc: 0.0138
top acc: 0.0716 ::: bot acc: 0.0153
top acc: 0.0486 ::: bot acc: 0.0470
top acc: 0.0414 ::: bot acc: 0.0253
top acc: 0.0456 ::: bot acc: 0.0221
top acc: 0.0349 ::: bot acc: 0.0300
current epoch: 36
train loss is 0.001608
average val loss: 0.001131, accuracy: 0.0381
average test loss: 0.000649, accuracy: 0.0287
case acc: 0.018674161
case acc: 0.03944064
case acc: 0.03524862
case acc: 0.02598154
case acc: 0.026779288
case acc: 0.025799425
top acc: 0.0307 ::: bot acc: 0.0136
top acc: 0.0718 ::: bot acc: 0.0154
top acc: 0.0496 ::: bot acc: 0.0456
top acc: 0.0421 ::: bot acc: 0.0248
top acc: 0.0462 ::: bot acc: 0.0215
top acc: 0.0353 ::: bot acc: 0.0297
current epoch: 37
train loss is 0.001595
average val loss: 0.001078, accuracy: 0.0370
average test loss: 0.000633, accuracy: 0.0282
case acc: 0.017736135
case acc: 0.038297653
case acc: 0.03530838
case acc: 0.025978325
case acc: 0.026548743
case acc: 0.025592398
top acc: 0.0290 ::: bot acc: 0.0142
top acc: 0.0692 ::: bot acc: 0.0172
top acc: 0.0487 ::: bot acc: 0.0473
top acc: 0.0417 ::: bot acc: 0.0260
top acc: 0.0453 ::: bot acc: 0.0231
top acc: 0.0342 ::: bot acc: 0.0314
current epoch: 38
train loss is 0.001574
average val loss: 0.001106, accuracy: 0.0375
average test loss: 0.000635, accuracy: 0.0283
case acc: 0.018292997
case acc: 0.038481697
case acc: 0.035297114
case acc: 0.025874414
case acc: 0.026517944
case acc: 0.025520705
top acc: 0.0301 ::: bot acc: 0.0141
top acc: 0.0693 ::: bot acc: 0.0174
top acc: 0.0493 ::: bot acc: 0.0458
top acc: 0.0421 ::: bot acc: 0.0248
top acc: 0.0454 ::: bot acc: 0.0220
top acc: 0.0345 ::: bot acc: 0.0304
current epoch: 39
train loss is 0.001572
average val loss: 0.001063, accuracy: 0.0367
average test loss: 0.000619, accuracy: 0.0279
case acc: 0.017357811
case acc: 0.037360266
case acc: 0.035316702
case acc: 0.0256074
case acc: 0.02632552
case acc: 0.025566341
top acc: 0.0282 ::: bot acc: 0.0146
top acc: 0.0671 ::: bot acc: 0.0190
top acc: 0.0487 ::: bot acc: 0.0471
top acc: 0.0412 ::: bot acc: 0.0256
top acc: 0.0445 ::: bot acc: 0.0233
top acc: 0.0339 ::: bot acc: 0.0319
current epoch: 40
train loss is 0.001553
average val loss: 0.001087, accuracy: 0.0371
average test loss: 0.000622, accuracy: 0.0281
case acc: 0.0178652
case acc: 0.03731167
case acc: 0.035481475
case acc: 0.025924802
case acc: 0.02647732
case acc: 0.02553722
top acc: 0.0293 ::: bot acc: 0.0142
top acc: 0.0670 ::: bot acc: 0.0190
top acc: 0.0496 ::: bot acc: 0.0463
top acc: 0.0421 ::: bot acc: 0.0246
top acc: 0.0453 ::: bot acc: 0.0225
top acc: 0.0341 ::: bot acc: 0.0306
current epoch: 41
train loss is 0.001547
average val loss: 0.001001, accuracy: 0.0353
average test loss: 0.000595, accuracy: 0.0273
case acc: 0.016469268
case acc: 0.03614183
case acc: 0.03511497
case acc: 0.025593
case acc: 0.025758477
case acc: 0.024912907
top acc: 0.0256 ::: bot acc: 0.0167
top acc: 0.0634 ::: bot acc: 0.0223
top acc: 0.0468 ::: bot acc: 0.0491
top acc: 0.0399 ::: bot acc: 0.0274
top acc: 0.0420 ::: bot acc: 0.0253
top acc: 0.0317 ::: bot acc: 0.0335
current epoch: 42
train loss is 0.001507
average val loss: 0.000968, accuracy: 0.0345
average test loss: 0.000583, accuracy: 0.0269
case acc: 0.015981441
case acc: 0.03545747
case acc: 0.03493584
case acc: 0.025223337
case acc: 0.025652226
case acc: 0.024384618
top acc: 0.0242 ::: bot acc: 0.0181
top acc: 0.0614 ::: bot acc: 0.0243
top acc: 0.0459 ::: bot acc: 0.0499
top acc: 0.0379 ::: bot acc: 0.0286
top acc: 0.0404 ::: bot acc: 0.0270
top acc: 0.0296 ::: bot acc: 0.0354
current epoch: 43
train loss is 0.001493
average val loss: 0.000932, accuracy: 0.0336
average test loss: 0.000576, accuracy: 0.0268
case acc: 0.015669305
case acc: 0.035031974
case acc: 0.034769192
case acc: 0.02530486
case acc: 0.025597187
case acc: 0.024195487
top acc: 0.0228 ::: bot acc: 0.0193
top acc: 0.0599 ::: bot acc: 0.0261
top acc: 0.0451 ::: bot acc: 0.0506
top acc: 0.0376 ::: bot acc: 0.0296
top acc: 0.0396 ::: bot acc: 0.0282
top acc: 0.0286 ::: bot acc: 0.0365
current epoch: 44
train loss is 0.001482
average val loss: 0.000925, accuracy: 0.0334
average test loss: 0.000571, accuracy: 0.0266
case acc: 0.015399899
case acc: 0.034972522
case acc: 0.03486041
case acc: 0.025135074
case acc: 0.025229484
case acc: 0.024242472
top acc: 0.0222 ::: bot acc: 0.0199
top acc: 0.0593 ::: bot acc: 0.0270
top acc: 0.0449 ::: bot acc: 0.0504
top acc: 0.0371 ::: bot acc: 0.0299
top acc: 0.0387 ::: bot acc: 0.0282
top acc: 0.0284 ::: bot acc: 0.0369
current epoch: 45
train loss is 0.001471
average val loss: 0.000927, accuracy: 0.0334
average test loss: 0.000574, accuracy: 0.0267
case acc: 0.0156022655
case acc: 0.034747146
case acc: 0.035013933
case acc: 0.025014851
case acc: 0.02555186
case acc: 0.024229817
top acc: 0.0223 ::: bot acc: 0.0201
top acc: 0.0589 ::: bot acc: 0.0272
top acc: 0.0453 ::: bot acc: 0.0509
top acc: 0.0368 ::: bot acc: 0.0297
top acc: 0.0387 ::: bot acc: 0.0289
top acc: 0.0280 ::: bot acc: 0.0378
current epoch: 46
train loss is 0.001473
average val loss: 0.000971, accuracy: 0.0344
average test loss: 0.000577, accuracy: 0.0269
case acc: 0.015854595
case acc: 0.0350872
case acc: 0.034999993
case acc: 0.02517058
case acc: 0.025938796
case acc: 0.024155086
top acc: 0.0239 ::: bot acc: 0.0183
top acc: 0.0604 ::: bot acc: 0.0253
top acc: 0.0474 ::: bot acc: 0.0480
top acc: 0.0388 ::: bot acc: 0.0276
top acc: 0.0412 ::: bot acc: 0.0272
top acc: 0.0294 ::: bot acc: 0.0354
current epoch: 47
train loss is 0.001479
average val loss: 0.000961, accuracy: 0.0341
average test loss: 0.000576, accuracy: 0.0268
case acc: 0.015775563
case acc: 0.034879107
case acc: 0.035284672
case acc: 0.025109977
case acc: 0.025664322
case acc: 0.024276314
top acc: 0.0239 ::: bot acc: 0.0186
top acc: 0.0596 ::: bot acc: 0.0261
top acc: 0.0477 ::: bot acc: 0.0485
top acc: 0.0385 ::: bot acc: 0.0278
top acc: 0.0407 ::: bot acc: 0.0271
top acc: 0.0288 ::: bot acc: 0.0360
current epoch: 48
train loss is 0.001468
average val loss: 0.000971, accuracy: 0.0344
average test loss: 0.000579, accuracy: 0.0269
case acc: 0.015753994
case acc: 0.034874465
case acc: 0.03507099
case acc: 0.025537735
case acc: 0.025768122
case acc: 0.024341868
top acc: 0.0238 ::: bot acc: 0.0181
top acc: 0.0596 ::: bot acc: 0.0257
top acc: 0.0480 ::: bot acc: 0.0474
top acc: 0.0394 ::: bot acc: 0.0276
top acc: 0.0414 ::: bot acc: 0.0267
top acc: 0.0295 ::: bot acc: 0.0358
current epoch: 49
train loss is 0.001470
average val loss: 0.000971, accuracy: 0.0343
average test loss: 0.000577, accuracy: 0.0269
case acc: 0.015737128
case acc: 0.03478603
case acc: 0.035185516
case acc: 0.025647707
case acc: 0.025702344
case acc: 0.024220364
top acc: 0.0236 ::: bot acc: 0.0184
top acc: 0.0592 ::: bot acc: 0.0263
top acc: 0.0478 ::: bot acc: 0.0478
top acc: 0.0397 ::: bot acc: 0.0275
top acc: 0.0413 ::: bot acc: 0.0263
top acc: 0.0295 ::: bot acc: 0.0354
current epoch: 50
train loss is 0.001458
average val loss: 0.000947, accuracy: 0.0338
average test loss: 0.000572, accuracy: 0.0268
case acc: 0.015475566
case acc: 0.034533292
case acc: 0.035064563
case acc: 0.025483852
case acc: 0.025767867
case acc: 0.024278667
top acc: 0.0220 ::: bot acc: 0.0199
top acc: 0.0577 ::: bot acc: 0.0281
top acc: 0.0471 ::: bot acc: 0.0483
top acc: 0.0385 ::: bot acc: 0.0285
top acc: 0.0409 ::: bot acc: 0.0272
top acc: 0.0289 ::: bot acc: 0.0365
LME_Co_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5400 5400 5400
1.7082474 -0.6288155 0.17559324 -0.22413088
Validation: 600 600 600
Testing: 744 744 744
pre-processing time: 0.00042819976806640625
the split date is 2011-01-01
train dropout: 0.2 test dropout: 0.1
net initializing with time: 0.00420832633972168
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.007684
average val loss: 0.005437, accuracy: 0.0962
average test loss: 0.005731, accuracy: 0.0970
case acc: 0.12675591
case acc: 0.10037517
case acc: 0.09922449
case acc: 0.09719586
case acc: 0.107403114
case acc: 0.051269427
top acc: 0.0809 ::: bot acc: 0.1708
top acc: 0.1338 ::: bot acc: 0.0659
top acc: 0.0399 ::: bot acc: 0.1576
top acc: 0.0658 ::: bot acc: 0.1322
top acc: 0.0588 ::: bot acc: 0.1518
top acc: 0.0243 ::: bot acc: 0.0889
current epoch: 2
train loss is 0.006674
average val loss: 0.004008, accuracy: 0.0603
average test loss: 0.004585, accuracy: 0.0680
case acc: 0.034742873
case acc: 0.19931836
case acc: 0.047308333
case acc: 0.026752926
case acc: 0.035779167
case acc: 0.06392702
top acc: 0.0343 ::: bot acc: 0.0548
top acc: 0.2323 ::: bot acc: 0.1650
top acc: 0.0826 ::: bot acc: 0.0435
top acc: 0.0427 ::: bot acc: 0.0250
top acc: 0.0460 ::: bot acc: 0.0486
top acc: 0.1171 ::: bot acc: 0.0169
current epoch: 3
train loss is 0.008399
average val loss: 0.011865, accuracy: 0.1351
average test loss: 0.012628, accuracy: 0.1378
case acc: 0.08813767
case acc: 0.283522
case acc: 0.11642317
case acc: 0.10316612
case acc: 0.08846267
case acc: 0.14726703
top acc: 0.1335 ::: bot acc: 0.0445
top acc: 0.3170 ::: bot acc: 0.2502
top acc: 0.1820 ::: bot acc: 0.0560
top acc: 0.1354 ::: bot acc: 0.0686
top acc: 0.1380 ::: bot acc: 0.0433
top acc: 0.2029 ::: bot acc: 0.0947
current epoch: 4
train loss is 0.011893
average val loss: 0.010761, accuracy: 0.1287
average test loss: 0.011509, accuracy: 0.1315
case acc: 0.08791757
case acc: 0.2688923
case acc: 0.113722175
case acc: 0.09624552
case acc: 0.08552046
case acc: 0.13683414
top acc: 0.1335 ::: bot acc: 0.0441
top acc: 0.3026 ::: bot acc: 0.2345
top acc: 0.1782 ::: bot acc: 0.0539
top acc: 0.1294 ::: bot acc: 0.0617
top acc: 0.1354 ::: bot acc: 0.0406
top acc: 0.1925 ::: bot acc: 0.0847
current epoch: 5
train loss is 0.012716
average val loss: 0.002255, accuracy: 0.0522
average test loss: 0.002665, accuracy: 0.0596
case acc: 0.049708135
case acc: 0.12344462
case acc: 0.05136327
case acc: 0.044293936
case acc: 0.049174856
case acc: 0.039338026
top acc: 0.0173 ::: bot acc: 0.0869
top acc: 0.1575 ::: bot acc: 0.0896
top acc: 0.0438 ::: bot acc: 0.0846
top acc: 0.0232 ::: bot acc: 0.0732
top acc: 0.0170 ::: bot acc: 0.0856
top acc: 0.0567 ::: bot acc: 0.0505
current epoch: 6
train loss is 0.005825
average val loss: 0.002565, accuracy: 0.0633
average test loss: 0.002889, accuracy: 0.0665
case acc: 0.06991567
case acc: 0.08378191
case acc: 0.0672333
case acc: 0.06997847
case acc: 0.06182516
case acc: 0.04631846
top acc: 0.0261 ::: bot acc: 0.1130
top acc: 0.1180 ::: bot acc: 0.0498
top acc: 0.0281 ::: bot acc: 0.1158
top acc: 0.0416 ::: bot acc: 0.1026
top acc: 0.0185 ::: bot acc: 0.1041
top acc: 0.0298 ::: bot acc: 0.0787
current epoch: 7
train loss is 0.003079
average val loss: 0.001804, accuracy: 0.0435
average test loss: 0.002259, accuracy: 0.0524
case acc: 0.038114537
case acc: 0.11988383
case acc: 0.048141487
case acc: 0.031504564
case acc: 0.03697207
case acc: 0.03999787
top acc: 0.0255 ::: bot acc: 0.0659
top acc: 0.1539 ::: bot acc: 0.0855
top acc: 0.0530 ::: bot acc: 0.0741
top acc: 0.0169 ::: bot acc: 0.0570
top acc: 0.0430 ::: bot acc: 0.0522
top acc: 0.0734 ::: bot acc: 0.0324
current epoch: 8
train loss is 0.003358
average val loss: 0.001659, accuracy: 0.0418
average test loss: 0.002135, accuracy: 0.0510
case acc: 0.036622655
case acc: 0.1147848
case acc: 0.047582205
case acc: 0.030526776
case acc: 0.035585165
case acc: 0.041029137
top acc: 0.0299 ::: bot acc: 0.0614
top acc: 0.1484 ::: bot acc: 0.0800
top acc: 0.0533 ::: bot acc: 0.0732
top acc: 0.0168 ::: bot acc: 0.0558
top acc: 0.0490 ::: bot acc: 0.0451
top acc: 0.0757 ::: bot acc: 0.0308
current epoch: 9
train loss is 0.003327
average val loss: 0.001459, accuracy: 0.0412
average test loss: 0.001888, accuracy: 0.0504
case acc: 0.041242294
case acc: 0.09382392
case acc: 0.051955834
case acc: 0.039346132
case acc: 0.036993057
case acc: 0.038748484
top acc: 0.0218 ::: bot acc: 0.0724
top acc: 0.1271 ::: bot acc: 0.0601
top acc: 0.0409 ::: bot acc: 0.0862
top acc: 0.0187 ::: bot acc: 0.0683
top acc: 0.0407 ::: bot acc: 0.0544
top acc: 0.0624 ::: bot acc: 0.0438
current epoch: 10
train loss is 0.002797
average val loss: 0.001329, accuracy: 0.0394
average test loss: 0.001767, accuracy: 0.0489
case acc: 0.039881144
case acc: 0.08713988
case acc: 0.05248396
case acc: 0.039339617
case acc: 0.0359675
case acc: 0.038513213
top acc: 0.0238 ::: bot acc: 0.0688
top acc: 0.1203 ::: bot acc: 0.0536
top acc: 0.0396 ::: bot acc: 0.0880
top acc: 0.0190 ::: bot acc: 0.0684
top acc: 0.0443 ::: bot acc: 0.0492
top acc: 0.0633 ::: bot acc: 0.0424
current epoch: 11
train loss is 0.002486
average val loss: 0.001232, accuracy: 0.0374
average test loss: 0.001691, accuracy: 0.0474
case acc: 0.037624348
case acc: 0.08458179
case acc: 0.051549252
case acc: 0.036515936
case acc: 0.035447616
case acc: 0.038971797
top acc: 0.0290 ::: bot acc: 0.0633
top acc: 0.1175 ::: bot acc: 0.0507
top acc: 0.0424 ::: bot acc: 0.0855
top acc: 0.0170 ::: bot acc: 0.0647
top acc: 0.0517 ::: bot acc: 0.0430
top acc: 0.0668 ::: bot acc: 0.0392
current epoch: 12
train loss is 0.002346
average val loss: 0.001152, accuracy: 0.0360
average test loss: 0.001600, accuracy: 0.0460
case acc: 0.036162984
case acc: 0.079765394
case acc: 0.05086485
case acc: 0.034986798
case acc: 0.034916837
case acc: 0.039357543
top acc: 0.0315 ::: bot acc: 0.0601
top acc: 0.1133 ::: bot acc: 0.0459
top acc: 0.0423 ::: bot acc: 0.0844
top acc: 0.0168 ::: bot acc: 0.0627
top acc: 0.0544 ::: bot acc: 0.0388
top acc: 0.0689 ::: bot acc: 0.0377
current epoch: 13
train loss is 0.002213
average val loss: 0.001085, accuracy: 0.0348
average test loss: 0.001530, accuracy: 0.0449
case acc: 0.03509896
case acc: 0.07595754
case acc: 0.050051168
case acc: 0.033849046
case acc: 0.035361577
case acc: 0.039285056
top acc: 0.0351 ::: bot acc: 0.0563
top acc: 0.1087 ::: bot acc: 0.0429
top acc: 0.0433 ::: bot acc: 0.0823
top acc: 0.0169 ::: bot acc: 0.0609
top acc: 0.0581 ::: bot acc: 0.0356
top acc: 0.0700 ::: bot acc: 0.0356
current epoch: 14
train loss is 0.002118
average val loss: 0.001007, accuracy: 0.0336
average test loss: 0.001475, accuracy: 0.0441
case acc: 0.034649335
case acc: 0.071255624
case acc: 0.05028327
case acc: 0.03327985
case acc: 0.035768937
case acc: 0.03943314
top acc: 0.0370 ::: bot acc: 0.0545
top acc: 0.1039 ::: bot acc: 0.0385
top acc: 0.0436 ::: bot acc: 0.0827
top acc: 0.0174 ::: bot acc: 0.0598
top acc: 0.0598 ::: bot acc: 0.0347
top acc: 0.0704 ::: bot acc: 0.0357
current epoch: 15
train loss is 0.002021
average val loss: 0.000962, accuracy: 0.0328
average test loss: 0.001401, accuracy: 0.0432
case acc: 0.03420214
case acc: 0.06556156
case acc: 0.050753612
case acc: 0.033793
case acc: 0.035707843
case acc: 0.03929043
top acc: 0.0377 ::: bot acc: 0.0533
top acc: 0.0980 ::: bot acc: 0.0339
top acc: 0.0435 ::: bot acc: 0.0831
top acc: 0.0170 ::: bot acc: 0.0606
top acc: 0.0598 ::: bot acc: 0.0344
top acc: 0.0694 ::: bot acc: 0.0368
current epoch: 16
train loss is 0.001903
average val loss: 0.000900, accuracy: 0.0317
average test loss: 0.001355, accuracy: 0.0424
case acc: 0.033955425
case acc: 0.06220783
case acc: 0.050268926
case acc: 0.032685142
case acc: 0.035851564
case acc: 0.039527625
top acc: 0.0405 ::: bot acc: 0.0509
top acc: 0.0934 ::: bot acc: 0.0314
top acc: 0.0436 ::: bot acc: 0.0826
top acc: 0.0168 ::: bot acc: 0.0591
top acc: 0.0611 ::: bot acc: 0.0336
top acc: 0.0700 ::: bot acc: 0.0369
current epoch: 17
train loss is 0.001817
average val loss: 0.000860, accuracy: 0.0311
average test loss: 0.001322, accuracy: 0.0416
case acc: 0.033299234
case acc: 0.060412552
case acc: 0.04946315
case acc: 0.031042853
case acc: 0.035843574
case acc: 0.039726887
top acc: 0.0441 ::: bot acc: 0.0468
top acc: 0.0923 ::: bot acc: 0.0303
top acc: 0.0458 ::: bot acc: 0.0802
top acc: 0.0165 ::: bot acc: 0.0570
top acc: 0.0631 ::: bot acc: 0.0306
top acc: 0.0716 ::: bot acc: 0.0345
current epoch: 18
train loss is 0.001752
average val loss: 0.000829, accuracy: 0.0305
average test loss: 0.001289, accuracy: 0.0410
case acc: 0.03333196
case acc: 0.05771987
case acc: 0.04937845
case acc: 0.029789047
case acc: 0.03600569
case acc: 0.039728597
top acc: 0.0467 ::: bot acc: 0.0443
top acc: 0.0882 ::: bot acc: 0.0280
top acc: 0.0474 ::: bot acc: 0.0794
top acc: 0.0167 ::: bot acc: 0.0553
top acc: 0.0645 ::: bot acc: 0.0299
top acc: 0.0718 ::: bot acc: 0.0335
current epoch: 19
train loss is 0.001712
average val loss: 0.000796, accuracy: 0.0300
average test loss: 0.001250, accuracy: 0.0403
case acc: 0.033080123
case acc: 0.05421134
case acc: 0.0492143
case acc: 0.02995604
case acc: 0.036027573
case acc: 0.039606724
top acc: 0.0473 ::: bot acc: 0.0435
top acc: 0.0846 ::: bot acc: 0.0259
top acc: 0.0475 ::: bot acc: 0.0790
top acc: 0.0162 ::: bot acc: 0.0559
top acc: 0.0640 ::: bot acc: 0.0301
top acc: 0.0714 ::: bot acc: 0.0348
current epoch: 20
train loss is 0.001639
average val loss: 0.000762, accuracy: 0.0293
average test loss: 0.001208, accuracy: 0.0398
case acc: 0.03343391
case acc: 0.050243877
case acc: 0.049772277
case acc: 0.030745018
case acc: 0.035676483
case acc: 0.039153084
top acc: 0.0467 ::: bot acc: 0.0446
top acc: 0.0800 ::: bot acc: 0.0231
top acc: 0.0465 ::: bot acc: 0.0805
top acc: 0.0163 ::: bot acc: 0.0568
top acc: 0.0608 ::: bot acc: 0.0332
top acc: 0.0688 ::: bot acc: 0.0371
current epoch: 21
train loss is 0.001592
average val loss: 0.000735, accuracy: 0.0289
average test loss: 0.001197, accuracy: 0.0395
case acc: 0.03321089
case acc: 0.049665138
case acc: 0.04879268
case acc: 0.028944792
case acc: 0.036288507
case acc: 0.039829873
top acc: 0.0503 ::: bot acc: 0.0408
top acc: 0.0795 ::: bot acc: 0.0229
top acc: 0.0499 ::: bot acc: 0.0770
top acc: 0.0168 ::: bot acc: 0.0537
top acc: 0.0632 ::: bot acc: 0.0318
top acc: 0.0711 ::: bot acc: 0.0355
current epoch: 22
train loss is 0.001564
average val loss: 0.000727, accuracy: 0.0288
average test loss: 0.001177, accuracy: 0.0389
case acc: 0.03346048
case acc: 0.048557512
case acc: 0.04804558
case acc: 0.027865143
case acc: 0.03594275
case acc: 0.03966079
top acc: 0.0534 ::: bot acc: 0.0378
top acc: 0.0780 ::: bot acc: 0.0223
top acc: 0.0528 ::: bot acc: 0.0743
top acc: 0.0177 ::: bot acc: 0.0518
top acc: 0.0639 ::: bot acc: 0.0299
top acc: 0.0711 ::: bot acc: 0.0349
current epoch: 23
train loss is 0.001585
average val loss: 0.000731, accuracy: 0.0291
average test loss: 0.001186, accuracy: 0.0389
case acc: 0.034092158
case acc: 0.04939855
case acc: 0.04712345
case acc: 0.026410995
case acc: 0.036298506
case acc: 0.04007423
top acc: 0.0572 ::: bot acc: 0.0341
top acc: 0.0787 ::: bot acc: 0.0229
top acc: 0.0563 ::: bot acc: 0.0704
top acc: 0.0196 ::: bot acc: 0.0486
top acc: 0.0661 ::: bot acc: 0.0280
top acc: 0.0734 ::: bot acc: 0.0316
current epoch: 24
train loss is 0.001558
average val loss: 0.000717, accuracy: 0.0290
average test loss: 0.001177, accuracy: 0.0387
case acc: 0.034440957
case acc: 0.04794685
case acc: 0.046989176
case acc: 0.0260578
case acc: 0.036469243
case acc: 0.040522955
top acc: 0.0593 ::: bot acc: 0.0321
top acc: 0.0768 ::: bot acc: 0.0225
top acc: 0.0583 ::: bot acc: 0.0690
top acc: 0.0205 ::: bot acc: 0.0479
top acc: 0.0664 ::: bot acc: 0.0280
top acc: 0.0737 ::: bot acc: 0.0326
current epoch: 25
train loss is 0.001528
average val loss: 0.000694, accuracy: 0.0286
average test loss: 0.001144, accuracy: 0.0382
case acc: 0.034496084
case acc: 0.04535713
case acc: 0.046981223
case acc: 0.02629482
case acc: 0.036045518
case acc: 0.039861705
top acc: 0.0588 ::: bot acc: 0.0327
top acc: 0.0739 ::: bot acc: 0.0208
top acc: 0.0580 ::: bot acc: 0.0690
top acc: 0.0203 ::: bot acc: 0.0479
top acc: 0.0648 ::: bot acc: 0.0288
top acc: 0.0724 ::: bot acc: 0.0340
current epoch: 26
train loss is 0.001505
average val loss: 0.000689, accuracy: 0.0288
average test loss: 0.001146, accuracy: 0.0380
case acc: 0.034581624
case acc: 0.04469678
case acc: 0.046696436
case acc: 0.026077088
case acc: 0.036216583
case acc: 0.039991826
top acc: 0.0611 ::: bot acc: 0.0300
top acc: 0.0729 ::: bot acc: 0.0203
top acc: 0.0603 ::: bot acc: 0.0667
top acc: 0.0211 ::: bot acc: 0.0473
top acc: 0.0657 ::: bot acc: 0.0287
top acc: 0.0727 ::: bot acc: 0.0332
current epoch: 27
train loss is 0.001505
average val loss: 0.000690, accuracy: 0.0290
average test loss: 0.001140, accuracy: 0.0379
case acc: 0.035041824
case acc: 0.044155415
case acc: 0.046362538
case acc: 0.025297172
case acc: 0.036011215
case acc: 0.04036403
top acc: 0.0628 ::: bot acc: 0.0281
top acc: 0.0727 ::: bot acc: 0.0202
top acc: 0.0624 ::: bot acc: 0.0645
top acc: 0.0229 ::: bot acc: 0.0451
top acc: 0.0653 ::: bot acc: 0.0283
top acc: 0.0736 ::: bot acc: 0.0327
current epoch: 28
train loss is 0.001488
average val loss: 0.000692, accuracy: 0.0291
average test loss: 0.001145, accuracy: 0.0378
case acc: 0.03541971
case acc: 0.043863602
case acc: 0.046196993
case acc: 0.024914058
case acc: 0.036165264
case acc: 0.040411152
top acc: 0.0650 ::: bot acc: 0.0260
top acc: 0.0725 ::: bot acc: 0.0197
top acc: 0.0643 ::: bot acc: 0.0624
top acc: 0.0242 ::: bot acc: 0.0438
top acc: 0.0663 ::: bot acc: 0.0275
top acc: 0.0738 ::: bot acc: 0.0326
current epoch: 29
train loss is 0.001480
average val loss: 0.000703, accuracy: 0.0295
average test loss: 0.001154, accuracy: 0.0380
case acc: 0.03609105
case acc: 0.044126656
case acc: 0.046076655
case acc: 0.024770176
case acc: 0.03646224
case acc: 0.0402981
top acc: 0.0666 ::: bot acc: 0.0241
top acc: 0.0728 ::: bot acc: 0.0198
top acc: 0.0673 ::: bot acc: 0.0598
top acc: 0.0259 ::: bot acc: 0.0420
top acc: 0.0674 ::: bot acc: 0.0268
top acc: 0.0741 ::: bot acc: 0.0313
current epoch: 30
train loss is 0.001500
average val loss: 0.000712, accuracy: 0.0299
average test loss: 0.001177, accuracy: 0.0383
case acc: 0.036866006
case acc: 0.044530418
case acc: 0.0460792
case acc: 0.024686633
case acc: 0.03650121
case acc: 0.04105726
top acc: 0.0689 ::: bot acc: 0.0223
top acc: 0.0732 ::: bot acc: 0.0198
top acc: 0.0696 ::: bot acc: 0.0574
top acc: 0.0282 ::: bot acc: 0.0402
top acc: 0.0686 ::: bot acc: 0.0252
top acc: 0.0762 ::: bot acc: 0.0306
current epoch: 31
train loss is 0.001484
average val loss: 0.000723, accuracy: 0.0303
average test loss: 0.001182, accuracy: 0.0384
case acc: 0.03731121
case acc: 0.044442166
case acc: 0.046270132
case acc: 0.024881229
case acc: 0.03663246
case acc: 0.040780634
top acc: 0.0702 ::: bot acc: 0.0212
top acc: 0.0730 ::: bot acc: 0.0204
top acc: 0.0714 ::: bot acc: 0.0558
top acc: 0.0296 ::: bot acc: 0.0393
top acc: 0.0698 ::: bot acc: 0.0241
top acc: 0.0757 ::: bot acc: 0.0301
current epoch: 32
train loss is 0.001472
average val loss: 0.000715, accuracy: 0.0301
average test loss: 0.001171, accuracy: 0.0382
case acc: 0.03718888
case acc: 0.043463495
case acc: 0.046002757
case acc: 0.02488886
case acc: 0.03675122
case acc: 0.040717497
top acc: 0.0701 ::: bot acc: 0.0212
top acc: 0.0716 ::: bot acc: 0.0199
top acc: 0.0712 ::: bot acc: 0.0555
top acc: 0.0295 ::: bot acc: 0.0391
top acc: 0.0696 ::: bot acc: 0.0245
top acc: 0.0753 ::: bot acc: 0.0310
current epoch: 33
train loss is 0.001463
average val loss: 0.000726, accuracy: 0.0304
average test loss: 0.001183, accuracy: 0.0383
case acc: 0.037910733
case acc: 0.04289722
case acc: 0.04634992
case acc: 0.024982158
case acc: 0.036760196
case acc: 0.040806763
top acc: 0.0717 ::: bot acc: 0.0207
top acc: 0.0713 ::: bot acc: 0.0195
top acc: 0.0734 ::: bot acc: 0.0541
top acc: 0.0300 ::: bot acc: 0.0386
top acc: 0.0707 ::: bot acc: 0.0235
top acc: 0.0757 ::: bot acc: 0.0307
current epoch: 34
train loss is 0.001467
average val loss: 0.000725, accuracy: 0.0305
average test loss: 0.001186, accuracy: 0.0384
case acc: 0.038415663
case acc: 0.04265009
case acc: 0.046272583
case acc: 0.024992898
case acc: 0.036919784
case acc: 0.04089095
top acc: 0.0725 ::: bot acc: 0.0198
top acc: 0.0705 ::: bot acc: 0.0197
top acc: 0.0742 ::: bot acc: 0.0528
top acc: 0.0312 ::: bot acc: 0.0376
top acc: 0.0711 ::: bot acc: 0.0225
top acc: 0.0758 ::: bot acc: 0.0301
current epoch: 35
train loss is 0.001466
average val loss: 0.000739, accuracy: 0.0308
average test loss: 0.001204, accuracy: 0.0386
case acc: 0.03874213
case acc: 0.042945713
case acc: 0.046535634
case acc: 0.025117787
case acc: 0.037156593
case acc: 0.041328624
top acc: 0.0732 ::: bot acc: 0.0192
top acc: 0.0710 ::: bot acc: 0.0197
top acc: 0.0763 ::: bot acc: 0.0512
top acc: 0.0331 ::: bot acc: 0.0361
top acc: 0.0725 ::: bot acc: 0.0211
top acc: 0.0768 ::: bot acc: 0.0294
current epoch: 36
train loss is 0.001457
average val loss: 0.000711, accuracy: 0.0303
average test loss: 0.001166, accuracy: 0.0380
case acc: 0.038010184
case acc: 0.04087688
case acc: 0.046470467
case acc: 0.024807433
case acc: 0.036918048
case acc: 0.040721346
top acc: 0.0715 ::: bot acc: 0.0202
top acc: 0.0686 ::: bot acc: 0.0188
top acc: 0.0747 ::: bot acc: 0.0522
top acc: 0.0316 ::: bot acc: 0.0370
top acc: 0.0711 ::: bot acc: 0.0228
top acc: 0.0753 ::: bot acc: 0.0309
current epoch: 37
train loss is 0.001438
average val loss: 0.000696, accuracy: 0.0299
average test loss: 0.001148, accuracy: 0.0376
case acc: 0.03750274
case acc: 0.03973137
case acc: 0.0463043
case acc: 0.024990847
case acc: 0.036795855
case acc: 0.04054006
top acc: 0.0705 ::: bot acc: 0.0205
top acc: 0.0666 ::: bot acc: 0.0187
top acc: 0.0741 ::: bot acc: 0.0532
top acc: 0.0310 ::: bot acc: 0.0381
top acc: 0.0704 ::: bot acc: 0.0233
top acc: 0.0745 ::: bot acc: 0.0319
current epoch: 38
train loss is 0.001407
average val loss: 0.000670, accuracy: 0.0293
average test loss: 0.001116, accuracy: 0.0371
case acc: 0.036910467
case acc: 0.037719026
case acc: 0.046278365
case acc: 0.024775902
case acc: 0.036815636
case acc: 0.0400025
top acc: 0.0691 ::: bot acc: 0.0226
top acc: 0.0641 ::: bot acc: 0.0177
top acc: 0.0728 ::: bot acc: 0.0542
top acc: 0.0295 ::: bot acc: 0.0391
top acc: 0.0695 ::: bot acc: 0.0248
top acc: 0.0726 ::: bot acc: 0.0331
current epoch: 39
train loss is 0.001378
average val loss: 0.000654, accuracy: 0.0289
average test loss: 0.001093, accuracy: 0.0366
case acc: 0.036612768
case acc: 0.035976943
case acc: 0.046268277
case acc: 0.024658151
case acc: 0.036374092
case acc: 0.03964257
top acc: 0.0677 ::: bot acc: 0.0242
top acc: 0.0615 ::: bot acc: 0.0177
top acc: 0.0716 ::: bot acc: 0.0556
top acc: 0.0282 ::: bot acc: 0.0405
top acc: 0.0680 ::: bot acc: 0.0255
top acc: 0.0713 ::: bot acc: 0.0345
current epoch: 40
train loss is 0.001345
average val loss: 0.000623, accuracy: 0.0281
average test loss: 0.001060, accuracy: 0.0360
case acc: 0.035895284
case acc: 0.03380932
case acc: 0.04614714
case acc: 0.024843575
case acc: 0.036124438
case acc: 0.039329775
top acc: 0.0652 ::: bot acc: 0.0270
top acc: 0.0583 ::: bot acc: 0.0176
top acc: 0.0693 ::: bot acc: 0.0577
top acc: 0.0263 ::: bot acc: 0.0424
top acc: 0.0664 ::: bot acc: 0.0276
top acc: 0.0694 ::: bot acc: 0.0367
current epoch: 41
train loss is 0.001326
average val loss: 0.000610, accuracy: 0.0277
average test loss: 0.001034, accuracy: 0.0355
case acc: 0.035111237
case acc: 0.031997923
case acc: 0.046004087
case acc: 0.025097629
case acc: 0.03605198
case acc: 0.038958352
top acc: 0.0632 ::: bot acc: 0.0283
top acc: 0.0558 ::: bot acc: 0.0170
top acc: 0.0672 ::: bot acc: 0.0595
top acc: 0.0243 ::: bot acc: 0.0439
top acc: 0.0646 ::: bot acc: 0.0290
top acc: 0.0676 ::: bot acc: 0.0381
current epoch: 42
train loss is 0.001301
average val loss: 0.000591, accuracy: 0.0272
average test loss: 0.001010, accuracy: 0.0352
case acc: 0.03446138
case acc: 0.030312544
case acc: 0.04603415
case acc: 0.02580892
case acc: 0.03560575
case acc: 0.038744714
top acc: 0.0601 ::: bot acc: 0.0315
top acc: 0.0529 ::: bot acc: 0.0180
top acc: 0.0646 ::: bot acc: 0.0618
top acc: 0.0228 ::: bot acc: 0.0458
top acc: 0.0617 ::: bot acc: 0.0318
top acc: 0.0657 ::: bot acc: 0.0404
current epoch: 43
train loss is 0.001281
average val loss: 0.000584, accuracy: 0.0269
average test loss: 0.000998, accuracy: 0.0350
case acc: 0.034013934
case acc: 0.028769152
case acc: 0.04650964
case acc: 0.02632179
case acc: 0.035501022
case acc: 0.038741462
top acc: 0.0579 ::: bot acc: 0.0335
top acc: 0.0501 ::: bot acc: 0.0191
top acc: 0.0630 ::: bot acc: 0.0644
top acc: 0.0208 ::: bot acc: 0.0481
top acc: 0.0592 ::: bot acc: 0.0341
top acc: 0.0638 ::: bot acc: 0.0427
current epoch: 44
train loss is 0.001258
average val loss: 0.000575, accuracy: 0.0266
average test loss: 0.000990, accuracy: 0.0348
case acc: 0.033852253
case acc: 0.027974892
case acc: 0.04651355
case acc: 0.026411876
case acc: 0.03553622
case acc: 0.03879285
top acc: 0.0562 ::: bot acc: 0.0347
top acc: 0.0486 ::: bot acc: 0.0197
top acc: 0.0620 ::: bot acc: 0.0653
top acc: 0.0203 ::: bot acc: 0.0486
top acc: 0.0588 ::: bot acc: 0.0352
top acc: 0.0633 ::: bot acc: 0.0430
current epoch: 45
train loss is 0.001251
average val loss: 0.000576, accuracy: 0.0266
average test loss: 0.000984, accuracy: 0.0347
case acc: 0.033664025
case acc: 0.027257254
case acc: 0.04646787
case acc: 0.026853474
case acc: 0.035329323
case acc: 0.038745303
top acc: 0.0547 ::: bot acc: 0.0362
top acc: 0.0470 ::: bot acc: 0.0212
top acc: 0.0607 ::: bot acc: 0.0663
top acc: 0.0193 ::: bot acc: 0.0497
top acc: 0.0578 ::: bot acc: 0.0356
top acc: 0.0624 ::: bot acc: 0.0441
current epoch: 46
train loss is 0.001249
average val loss: 0.000574, accuracy: 0.0265
average test loss: 0.000980, accuracy: 0.0347
case acc: 0.033686865
case acc: 0.026820045
case acc: 0.04686884
case acc: 0.02707951
case acc: 0.035287805
case acc: 0.038752932
top acc: 0.0540 ::: bot acc: 0.0376
top acc: 0.0456 ::: bot acc: 0.0227
top acc: 0.0595 ::: bot acc: 0.0678
top acc: 0.0183 ::: bot acc: 0.0504
top acc: 0.0564 ::: bot acc: 0.0370
top acc: 0.0610 ::: bot acc: 0.0451
current epoch: 47
train loss is 0.001237
average val loss: 0.000575, accuracy: 0.0265
average test loss: 0.000974, accuracy: 0.0347
case acc: 0.033398684
case acc: 0.025946006
case acc: 0.047016326
case acc: 0.027825605
case acc: 0.03542049
case acc: 0.03852086
top acc: 0.0518 ::: bot acc: 0.0395
top acc: 0.0431 ::: bot acc: 0.0241
top acc: 0.0580 ::: bot acc: 0.0692
top acc: 0.0174 ::: bot acc: 0.0520
top acc: 0.0547 ::: bot acc: 0.0395
top acc: 0.0591 ::: bot acc: 0.0464
current epoch: 48
train loss is 0.001223
average val loss: 0.000576, accuracy: 0.0265
average test loss: 0.000973, accuracy: 0.0347
case acc: 0.033507477
case acc: 0.025556143
case acc: 0.04694782
case acc: 0.028543573
case acc: 0.035249654
case acc: 0.038600907
top acc: 0.0513 ::: bot acc: 0.0400
top acc: 0.0424 ::: bot acc: 0.0249
top acc: 0.0573 ::: bot acc: 0.0694
top acc: 0.0178 ::: bot acc: 0.0527
top acc: 0.0538 ::: bot acc: 0.0399
top acc: 0.0587 ::: bot acc: 0.0470
current epoch: 49
train loss is 0.001220
average val loss: 0.000573, accuracy: 0.0264
average test loss: 0.000972, accuracy: 0.0346
case acc: 0.033451594
case acc: 0.025666364
case acc: 0.046854608
case acc: 0.027834786
case acc: 0.035336956
case acc: 0.038700465
top acc: 0.0516 ::: bot acc: 0.0398
top acc: 0.0423 ::: bot acc: 0.0250
top acc: 0.0576 ::: bot acc: 0.0691
top acc: 0.0175 ::: bot acc: 0.0520
top acc: 0.0545 ::: bot acc: 0.0394
top acc: 0.0594 ::: bot acc: 0.0466
current epoch: 50
train loss is 0.001218
average val loss: 0.000579, accuracy: 0.0265
average test loss: 0.000977, accuracy: 0.0348
case acc: 0.033455916
case acc: 0.025658315
case acc: 0.047170036
case acc: 0.02837718
case acc: 0.035426024
case acc: 0.038792223
top acc: 0.0508 ::: bot acc: 0.0407
top acc: 0.0415 ::: bot acc: 0.0267
top acc: 0.0568 ::: bot acc: 0.0705
top acc: 0.0172 ::: bot acc: 0.0529
top acc: 0.0541 ::: bot acc: 0.0401
top acc: 0.0586 ::: bot acc: 0.0476
LME_Co_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5370 5370 5370
1.7082474 -0.6288155 0.17559324 -0.22413088
Validation: 600 600 600
Testing: 774 774 774
pre-processing time: 0.00021195411682128906
the split date is 2011-07-01
train dropout: 0.2 test dropout: 0.1
net initializing with time: 0.002368927001953125
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.007991
average val loss: 0.005219, accuracy: 0.0925
average test loss: 0.005806, accuracy: 0.0961
case acc: 0.13339254
case acc: 0.105562106
case acc: 0.097012214
case acc: 0.07399018
case acc: 0.12089522
case acc: 0.045982584
top acc: 0.0884 ::: bot acc: 0.1751
top acc: 0.1434 ::: bot acc: 0.0712
top acc: 0.0468 ::: bot acc: 0.1439
top acc: 0.0328 ::: bot acc: 0.1202
top acc: 0.0675 ::: bot acc: 0.1657
top acc: 0.0228 ::: bot acc: 0.0820
current epoch: 2
train loss is 0.006958
average val loss: 0.004889, accuracy: 0.0705
average test loss: 0.005024, accuracy: 0.0735
case acc: 0.035462372
case acc: 0.20485921
case acc: 0.037356
case acc: 0.0520748
case acc: 0.042244725
case acc: 0.06883244
top acc: 0.0259 ::: bot acc: 0.0596
top acc: 0.2430 ::: bot acc: 0.1695
top acc: 0.0686 ::: bot acc: 0.0291
top acc: 0.0931 ::: bot acc: 0.0241
top acc: 0.0397 ::: bot acc: 0.0617
top acc: 0.1212 ::: bot acc: 0.0204
current epoch: 3
train loss is 0.008526
average val loss: 0.013590, accuracy: 0.1449
average test loss: 0.013283, accuracy: 0.1407
case acc: 0.080262564
case acc: 0.28847396
case acc: 0.11527007
case acc: 0.13177688
case acc: 0.07615524
case acc: 0.15253109
top acc: 0.1248 ::: bot acc: 0.0385
top acc: 0.3271 ::: bot acc: 0.2530
top acc: 0.1666 ::: bot acc: 0.0680
top acc: 0.1852 ::: bot acc: 0.0798
top acc: 0.1314 ::: bot acc: 0.0308
top acc: 0.2056 ::: bot acc: 0.1023
current epoch: 4
train loss is 0.012298
average val loss: 0.009830, accuracy: 0.1186
average test loss: 0.009640, accuracy: 0.1150
case acc: 0.06080093
case acc: 0.25332358
case acc: 0.09246868
case acc: 0.10487699
case acc: 0.05661498
case acc: 0.122070774
top acc: 0.1052 ::: bot acc: 0.0202
top acc: 0.2909 ::: bot acc: 0.2181
top acc: 0.1430 ::: bot acc: 0.0469
top acc: 0.1588 ::: bot acc: 0.0531
top acc: 0.1094 ::: bot acc: 0.0159
top acc: 0.1751 ::: bot acc: 0.0713
current epoch: 5
train loss is 0.011862
average val loss: 0.002685, accuracy: 0.0623
average test loss: 0.003006, accuracy: 0.0660
case acc: 0.0714488
case acc: 0.10980369
case acc: 0.05262145
case acc: 0.046897672
case acc: 0.07542249
case acc: 0.040046405
top acc: 0.0290 ::: bot acc: 0.1126
top acc: 0.1471 ::: bot acc: 0.0745
top acc: 0.0212 ::: bot acc: 0.0900
top acc: 0.0303 ::: bot acc: 0.0797
top acc: 0.0313 ::: bot acc: 0.1152
top acc: 0.0406 ::: bot acc: 0.0620
current epoch: 6
train loss is 0.005151
average val loss: 0.002612, accuracy: 0.0629
average test loss: 0.002904, accuracy: 0.0661
case acc: 0.07682063
case acc: 0.08934949
case acc: 0.06082052
case acc: 0.052488957
case acc: 0.07521572
case acc: 0.041921712
top acc: 0.0341 ::: bot acc: 0.1180
top acc: 0.1267 ::: bot acc: 0.0541
top acc: 0.0213 ::: bot acc: 0.1020
top acc: 0.0281 ::: bot acc: 0.0893
top acc: 0.0322 ::: bot acc: 0.1149
top acc: 0.0327 ::: bot acc: 0.0706
current epoch: 7
train loss is 0.003090
average val loss: 0.002359, accuracy: 0.0527
average test loss: 0.002437, accuracy: 0.0548
case acc: 0.04100381
case acc: 0.12518296
case acc: 0.03864528
case acc: 0.038164314
case acc: 0.043615617
case acc: 0.042147562
top acc: 0.0202 ::: bot acc: 0.0714
top acc: 0.1628 ::: bot acc: 0.0894
top acc: 0.0387 ::: bot acc: 0.0601
top acc: 0.0604 ::: bot acc: 0.0454
top acc: 0.0386 ::: bot acc: 0.0637
top acc: 0.0775 ::: bot acc: 0.0266
current epoch: 8
train loss is 0.003499
average val loss: 0.002115, accuracy: 0.0510
average test loss: 0.002212, accuracy: 0.0530
case acc: 0.041371036
case acc: 0.11417985
case acc: 0.040153753
case acc: 0.03808213
case acc: 0.043357894
case acc: 0.040600855
top acc: 0.0191 ::: bot acc: 0.0721
top acc: 0.1514 ::: bot acc: 0.0789
top acc: 0.0329 ::: bot acc: 0.0653
top acc: 0.0560 ::: bot acc: 0.0504
top acc: 0.0393 ::: bot acc: 0.0633
top acc: 0.0731 ::: bot acc: 0.0301
current epoch: 9
train loss is 0.003311
average val loss: 0.001874, accuracy: 0.0503
average test loss: 0.001981, accuracy: 0.0518
case acc: 0.046141647
case acc: 0.09487652
case acc: 0.045293547
case acc: 0.039391402
case acc: 0.046106532
case acc: 0.03926866
top acc: 0.0166 ::: bot acc: 0.0805
top acc: 0.1321 ::: bot acc: 0.0596
top acc: 0.0242 ::: bot acc: 0.0771
top acc: 0.0451 ::: bot acc: 0.0612
top acc: 0.0350 ::: bot acc: 0.0699
top acc: 0.0621 ::: bot acc: 0.0424
current epoch: 10
train loss is 0.002770
average val loss: 0.001750, accuracy: 0.0488
average test loss: 0.001848, accuracy: 0.0501
case acc: 0.04449936
case acc: 0.08779805
case acc: 0.046002235
case acc: 0.039482858
case acc: 0.04417111
case acc: 0.03888547
top acc: 0.0168 ::: bot acc: 0.0783
top acc: 0.1254 ::: bot acc: 0.0525
top acc: 0.0235 ::: bot acc: 0.0792
top acc: 0.0452 ::: bot acc: 0.0611
top acc: 0.0376 ::: bot acc: 0.0654
top acc: 0.0618 ::: bot acc: 0.0421
current epoch: 11
train loss is 0.002515
average val loss: 0.001679, accuracy: 0.0474
average test loss: 0.001729, accuracy: 0.0483
case acc: 0.041256662
case acc: 0.08425266
case acc: 0.044941716
case acc: 0.038580447
case acc: 0.041706745
case acc: 0.03877549
top acc: 0.0186 ::: bot acc: 0.0729
top acc: 0.1224 ::: bot acc: 0.0493
top acc: 0.0251 ::: bot acc: 0.0760
top acc: 0.0484 ::: bot acc: 0.0575
top acc: 0.0430 ::: bot acc: 0.0588
top acc: 0.0641 ::: bot acc: 0.0384
current epoch: 12
train loss is 0.002352
average val loss: 0.001625, accuracy: 0.0462
average test loss: 0.001663, accuracy: 0.0471
case acc: 0.038634103
case acc: 0.082010746
case acc: 0.04393516
case acc: 0.038046844
case acc: 0.040318407
case acc: 0.03990208
top acc: 0.0233 ::: bot acc: 0.0666
top acc: 0.1200 ::: bot acc: 0.0465
top acc: 0.0279 ::: bot acc: 0.0735
top acc: 0.0525 ::: bot acc: 0.0533
top acc: 0.0492 ::: bot acc: 0.0535
top acc: 0.0685 ::: bot acc: 0.0350
current epoch: 13
train loss is 0.002258
average val loss: 0.001552, accuracy: 0.0452
average test loss: 0.001572, accuracy: 0.0458
case acc: 0.037639856
case acc: 0.075992465
case acc: 0.043574583
case acc: 0.038114667
case acc: 0.039441142
case acc: 0.03997901
top acc: 0.0244 ::: bot acc: 0.0647
top acc: 0.1135 ::: bot acc: 0.0409
top acc: 0.0267 ::: bot acc: 0.0740
top acc: 0.0534 ::: bot acc: 0.0528
top acc: 0.0515 ::: bot acc: 0.0503
top acc: 0.0691 ::: bot acc: 0.0352
current epoch: 14
train loss is 0.002086
average val loss: 0.001465, accuracy: 0.0442
average test loss: 0.001489, accuracy: 0.0448
case acc: 0.037892517
case acc: 0.06834252
case acc: 0.04470721
case acc: 0.03870931
case acc: 0.039947834
case acc: 0.03912018
top acc: 0.0239 ::: bot acc: 0.0650
top acc: 0.1057 ::: bot acc: 0.0336
top acc: 0.0246 ::: bot acc: 0.0770
top acc: 0.0511 ::: bot acc: 0.0559
top acc: 0.0495 ::: bot acc: 0.0526
top acc: 0.0661 ::: bot acc: 0.0370
current epoch: 15
train loss is 0.001920
average val loss: 0.001419, accuracy: 0.0434
average test loss: 0.001430, accuracy: 0.0438
case acc: 0.036783453
case acc: 0.064351976
case acc: 0.044655304
case acc: 0.03804571
case acc: 0.03952757
case acc: 0.039539266
top acc: 0.0260 ::: bot acc: 0.0625
top acc: 0.1012 ::: bot acc: 0.0303
top acc: 0.0263 ::: bot acc: 0.0758
top acc: 0.0531 ::: bot acc: 0.0537
top acc: 0.0519 ::: bot acc: 0.0504
top acc: 0.0673 ::: bot acc: 0.0364
current epoch: 16
train loss is 0.001859
average val loss: 0.001363, accuracy: 0.0426
average test loss: 0.001373, accuracy: 0.0429
case acc: 0.03601458
case acc: 0.059822027
case acc: 0.044075664
case acc: 0.038487405
case acc: 0.039341662
case acc: 0.03993308
top acc: 0.0278 ::: bot acc: 0.0602
top acc: 0.0963 ::: bot acc: 0.0264
top acc: 0.0262 ::: bot acc: 0.0748
top acc: 0.0543 ::: bot acc: 0.0533
top acc: 0.0519 ::: bot acc: 0.0496
top acc: 0.0681 ::: bot acc: 0.0370
current epoch: 17
train loss is 0.001715
average val loss: 0.001313, accuracy: 0.0418
average test loss: 0.001316, accuracy: 0.0419
case acc: 0.035605494
case acc: 0.054698937
case acc: 0.044317055
case acc: 0.03820982
case acc: 0.039521076
case acc: 0.039206482
top acc: 0.0283 ::: bot acc: 0.0593
top acc: 0.0909 ::: bot acc: 0.0220
top acc: 0.0250 ::: bot acc: 0.0761
top acc: 0.0532 ::: bot acc: 0.0536
top acc: 0.0514 ::: bot acc: 0.0506
top acc: 0.0660 ::: bot acc: 0.0381
current epoch: 18
train loss is 0.001673
average val loss: 0.001279, accuracy: 0.0411
average test loss: 0.001279, accuracy: 0.0413
case acc: 0.03508264
case acc: 0.051621914
case acc: 0.04430646
case acc: 0.037858613
case acc: 0.039422937
case acc: 0.03927519
top acc: 0.0307 ::: bot acc: 0.0573
top acc: 0.0878 ::: bot acc: 0.0197
top acc: 0.0258 ::: bot acc: 0.0754
top acc: 0.0540 ::: bot acc: 0.0521
top acc: 0.0521 ::: bot acc: 0.0500
top acc: 0.0665 ::: bot acc: 0.0374
current epoch: 19
train loss is 0.001619
average val loss: 0.001261, accuracy: 0.0406
average test loss: 0.001254, accuracy: 0.0407
case acc: 0.03389623
case acc: 0.05051561
case acc: 0.04263507
case acc: 0.038232226
case acc: 0.039100286
case acc: 0.039903797
top acc: 0.0338 ::: bot acc: 0.0535
top acc: 0.0862 ::: bot acc: 0.0186
top acc: 0.0277 ::: bot acc: 0.0721
top acc: 0.0570 ::: bot acc: 0.0502
top acc: 0.0542 ::: bot acc: 0.0480
top acc: 0.0684 ::: bot acc: 0.0362
current epoch: 20
train loss is 0.001561
average val loss: 0.001236, accuracy: 0.0402
average test loss: 0.001221, accuracy: 0.0401
case acc: 0.033790343
case acc: 0.047724776
case acc: 0.042332124
case acc: 0.037937142
case acc: 0.039180174
case acc: 0.039739344
top acc: 0.0363 ::: bot acc: 0.0520
top acc: 0.0833 ::: bot acc: 0.0168
top acc: 0.0290 ::: bot acc: 0.0709
top acc: 0.0575 ::: bot acc: 0.0489
top acc: 0.0537 ::: bot acc: 0.0488
top acc: 0.0681 ::: bot acc: 0.0363
current epoch: 21
train loss is 0.001534
average val loss: 0.001230, accuracy: 0.0399
average test loss: 0.001205, accuracy: 0.0397
case acc: 0.03304764
case acc: 0.04698842
case acc: 0.041261397
case acc: 0.038304377
case acc: 0.038891602
case acc: 0.03986756
top acc: 0.0395 ::: bot acc: 0.0490
top acc: 0.0822 ::: bot acc: 0.0163
top acc: 0.0310 ::: bot acc: 0.0680
top acc: 0.0608 ::: bot acc: 0.0466
top acc: 0.0542 ::: bot acc: 0.0480
top acc: 0.0692 ::: bot acc: 0.0348
current epoch: 22
train loss is 0.001516
average val loss: 0.001203, accuracy: 0.0393
average test loss: 0.001183, accuracy: 0.0394
case acc: 0.033025272
case acc: 0.045322597
case acc: 0.040379934
case acc: 0.03850616
case acc: 0.03915424
case acc: 0.039962005
top acc: 0.0415 ::: bot acc: 0.0471
top acc: 0.0798 ::: bot acc: 0.0156
top acc: 0.0327 ::: bot acc: 0.0661
top acc: 0.0614 ::: bot acc: 0.0455
top acc: 0.0545 ::: bot acc: 0.0478
top acc: 0.0693 ::: bot acc: 0.0352
current epoch: 23
train loss is 0.001505
average val loss: 0.001233, accuracy: 0.0397
average test loss: 0.001195, accuracy: 0.0394
case acc: 0.03212749
case acc: 0.046180554
case acc: 0.039112356
case acc: 0.039000843
case acc: 0.03885439
case acc: 0.040845133
top acc: 0.0462 ::: bot acc: 0.0410
top acc: 0.0811 ::: bot acc: 0.0159
top acc: 0.0377 ::: bot acc: 0.0614
top acc: 0.0657 ::: bot acc: 0.0409
top acc: 0.0576 ::: bot acc: 0.0454
top acc: 0.0729 ::: bot acc: 0.0322
current epoch: 24
train loss is 0.001493
average val loss: 0.001243, accuracy: 0.0398
average test loss: 0.001188, accuracy: 0.0392
case acc: 0.032599505
case acc: 0.045885507
case acc: 0.03780438
case acc: 0.039162893
case acc: 0.038606368
case acc: 0.040903173
top acc: 0.0493 ::: bot acc: 0.0393
top acc: 0.0809 ::: bot acc: 0.0156
top acc: 0.0400 ::: bot acc: 0.0578
top acc: 0.0676 ::: bot acc: 0.0384
top acc: 0.0584 ::: bot acc: 0.0442
top acc: 0.0736 ::: bot acc: 0.0307
current epoch: 25
train loss is 0.001487
average val loss: 0.001266, accuracy: 0.0398
average test loss: 0.001208, accuracy: 0.0395
case acc: 0.033178233
case acc: 0.046120636
case acc: 0.03754854
case acc: 0.040237907
case acc: 0.038458206
case acc: 0.041488856
top acc: 0.0529 ::: bot acc: 0.0360
top acc: 0.0811 ::: bot acc: 0.0160
top acc: 0.0433 ::: bot acc: 0.0554
top acc: 0.0705 ::: bot acc: 0.0361
top acc: 0.0601 ::: bot acc: 0.0427
top acc: 0.0754 ::: bot acc: 0.0293
current epoch: 26
train loss is 0.001465
average val loss: 0.001227, accuracy: 0.0393
average test loss: 0.001168, accuracy: 0.0388
case acc: 0.032841563
case acc: 0.04298231
case acc: 0.037669506
case acc: 0.03997157
case acc: 0.038682148
case acc: 0.040750332
top acc: 0.0524 ::: bot acc: 0.0359
top acc: 0.0771 ::: bot acc: 0.0142
top acc: 0.0430 ::: bot acc: 0.0557
top acc: 0.0699 ::: bot acc: 0.0369
top acc: 0.0584 ::: bot acc: 0.0442
top acc: 0.0737 ::: bot acc: 0.0309
current epoch: 27
train loss is 0.001437
average val loss: 0.001200, accuracy: 0.0388
average test loss: 0.001143, accuracy: 0.0383
case acc: 0.032511484
case acc: 0.04057575
case acc: 0.037522182
case acc: 0.03989289
case acc: 0.038926482
case acc: 0.040362567
top acc: 0.0516 ::: bot acc: 0.0360
top acc: 0.0743 ::: bot acc: 0.0129
top acc: 0.0428 ::: bot acc: 0.0561
top acc: 0.0695 ::: bot acc: 0.0377
top acc: 0.0574 ::: bot acc: 0.0453
top acc: 0.0716 ::: bot acc: 0.0325
current epoch: 28
train loss is 0.001406
average val loss: 0.001189, accuracy: 0.0386
average test loss: 0.001137, accuracy: 0.0382
case acc: 0.03308092
case acc: 0.039037097
case acc: 0.03764726
case acc: 0.040038172
case acc: 0.03890562
case acc: 0.040379144
top acc: 0.0525 ::: bot acc: 0.0362
top acc: 0.0727 ::: bot acc: 0.0119
top acc: 0.0442 ::: bot acc: 0.0552
top acc: 0.0695 ::: bot acc: 0.0377
top acc: 0.0574 ::: bot acc: 0.0458
top acc: 0.0716 ::: bot acc: 0.0335
current epoch: 29
train loss is 0.001385
average val loss: 0.001221, accuracy: 0.0390
average test loss: 0.001154, accuracy: 0.0383
case acc: 0.033287868
case acc: 0.039602175
case acc: 0.037096478
case acc: 0.04082072
case acc: 0.038614664
case acc: 0.040533934
top acc: 0.0548 ::: bot acc: 0.0329
top acc: 0.0737 ::: bot acc: 0.0116
top acc: 0.0464 ::: bot acc: 0.0523
top acc: 0.0723 ::: bot acc: 0.0350
top acc: 0.0588 ::: bot acc: 0.0436
top acc: 0.0731 ::: bot acc: 0.0312
current epoch: 30
train loss is 0.001401
average val loss: 0.001206, accuracy: 0.0388
average test loss: 0.001143, accuracy: 0.0381
case acc: 0.033443607
case acc: 0.038732648
case acc: 0.03681226
case acc: 0.04087683
case acc: 0.038691256
case acc: 0.040325336
top acc: 0.0559 ::: bot acc: 0.0324
top acc: 0.0720 ::: bot acc: 0.0120
top acc: 0.0472 ::: bot acc: 0.0511
top acc: 0.0725 ::: bot acc: 0.0349
top acc: 0.0589 ::: bot acc: 0.0439
top acc: 0.0720 ::: bot acc: 0.0321
current epoch: 31
train loss is 0.001375
average val loss: 0.001217, accuracy: 0.0389
average test loss: 0.001150, accuracy: 0.0382
case acc: 0.03354668
case acc: 0.038672913
case acc: 0.036781814
case acc: 0.04122364
case acc: 0.038440127
case acc: 0.04065226
top acc: 0.0578 ::: bot acc: 0.0305
top acc: 0.0720 ::: bot acc: 0.0121
top acc: 0.0492 ::: bot acc: 0.0497
top acc: 0.0737 ::: bot acc: 0.0331
top acc: 0.0595 ::: bot acc: 0.0428
top acc: 0.0727 ::: bot acc: 0.0316
current epoch: 32
train loss is 0.001374
average val loss: 0.001219, accuracy: 0.0389
average test loss: 0.001142, accuracy: 0.0381
case acc: 0.03374964
case acc: 0.03768192
case acc: 0.036789242
case acc: 0.0413507
case acc: 0.03867128
case acc: 0.04052992
top acc: 0.0583 ::: bot acc: 0.0303
top acc: 0.0705 ::: bot acc: 0.0116
top acc: 0.0507 ::: bot acc: 0.0483
top acc: 0.0741 ::: bot acc: 0.0330
top acc: 0.0592 ::: bot acc: 0.0432
top acc: 0.0721 ::: bot acc: 0.0323
current epoch: 33
train loss is 0.001379
average val loss: 0.001233, accuracy: 0.0390
average test loss: 0.001159, accuracy: 0.0383
case acc: 0.034039445
case acc: 0.03766513
case acc: 0.036667984
case acc: 0.041878633
case acc: 0.03852285
case acc: 0.040854536
top acc: 0.0598 ::: bot acc: 0.0283
top acc: 0.0705 ::: bot acc: 0.0114
top acc: 0.0526 ::: bot acc: 0.0463
top acc: 0.0762 ::: bot acc: 0.0307
top acc: 0.0612 ::: bot acc: 0.0417
top acc: 0.0737 ::: bot acc: 0.0306
current epoch: 34
train loss is 0.001361
average val loss: 0.001227, accuracy: 0.0390
average test loss: 0.001141, accuracy: 0.0380
case acc: 0.033639573
case acc: 0.036625855
case acc: 0.036659647
case acc: 0.04210133
case acc: 0.03836699
case acc: 0.04041441
top acc: 0.0590 ::: bot acc: 0.0289
top acc: 0.0693 ::: bot acc: 0.0113
top acc: 0.0526 ::: bot acc: 0.0465
top acc: 0.0754 ::: bot acc: 0.0320
top acc: 0.0605 ::: bot acc: 0.0422
top acc: 0.0723 ::: bot acc: 0.0318
current epoch: 35
train loss is 0.001327
average val loss: 0.001197, accuracy: 0.0384
average test loss: 0.001127, accuracy: 0.0378
case acc: 0.03387016
case acc: 0.035198987
case acc: 0.036564022
case acc: 0.04185785
case acc: 0.038535185
case acc: 0.040567268
top acc: 0.0586 ::: bot acc: 0.0300
top acc: 0.0669 ::: bot acc: 0.0114
top acc: 0.0514 ::: bot acc: 0.0470
top acc: 0.0750 ::: bot acc: 0.0325
top acc: 0.0598 ::: bot acc: 0.0431
top acc: 0.0717 ::: bot acc: 0.0336
current epoch: 36
train loss is 0.001321
average val loss: 0.001198, accuracy: 0.0384
average test loss: 0.001126, accuracy: 0.0377
case acc: 0.033935063
case acc: 0.03494316
case acc: 0.036702335
case acc: 0.041881103
case acc: 0.038367398
case acc: 0.040302455
top acc: 0.0590 ::: bot acc: 0.0295
top acc: 0.0666 ::: bot acc: 0.0114
top acc: 0.0528 ::: bot acc: 0.0463
top acc: 0.0753 ::: bot acc: 0.0320
top acc: 0.0600 ::: bot acc: 0.0426
top acc: 0.0714 ::: bot acc: 0.0332
current epoch: 37
train loss is 0.001307
average val loss: 0.001170, accuracy: 0.0379
average test loss: 0.001102, accuracy: 0.0374
case acc: 0.03372236
case acc: 0.03356501
case acc: 0.03663869
case acc: 0.041542124
case acc: 0.03895322
case acc: 0.040031534
top acc: 0.0573 ::: bot acc: 0.0313
top acc: 0.0639 ::: bot acc: 0.0126
top acc: 0.0512 ::: bot acc: 0.0472
top acc: 0.0747 ::: bot acc: 0.0326
top acc: 0.0589 ::: bot acc: 0.0441
top acc: 0.0699 ::: bot acc: 0.0344
current epoch: 38
train loss is 0.001284
average val loss: 0.001179, accuracy: 0.0380
average test loss: 0.001111, accuracy: 0.0375
case acc: 0.033659514
case acc: 0.033321526
case acc: 0.03679636
case acc: 0.04214566
case acc: 0.038719464
case acc: 0.040096216
top acc: 0.0580 ::: bot acc: 0.0302
top acc: 0.0636 ::: bot acc: 0.0123
top acc: 0.0523 ::: bot acc: 0.0469
top acc: 0.0756 ::: bot acc: 0.0324
top acc: 0.0599 ::: bot acc: 0.0432
top acc: 0.0704 ::: bot acc: 0.0340
current epoch: 39
train loss is 0.001283
average val loss: 0.001146, accuracy: 0.0374
average test loss: 0.001085, accuracy: 0.0371
case acc: 0.03367699
case acc: 0.03206574
case acc: 0.036673784
case acc: 0.04150033
case acc: 0.03865225
case acc: 0.039937407
top acc: 0.0559 ::: bot acc: 0.0327
top acc: 0.0612 ::: bot acc: 0.0140
top acc: 0.0506 ::: bot acc: 0.0480
top acc: 0.0743 ::: bot acc: 0.0333
top acc: 0.0579 ::: bot acc: 0.0444
top acc: 0.0691 ::: bot acc: 0.0356
current epoch: 40
train loss is 0.001262
average val loss: 0.001149, accuracy: 0.0374
average test loss: 0.001082, accuracy: 0.0370
case acc: 0.033496212
case acc: 0.031596027
case acc: 0.036666375
case acc: 0.041684736
case acc: 0.03857901
case acc: 0.04008492
top acc: 0.0560 ::: bot acc: 0.0324
top acc: 0.0601 ::: bot acc: 0.0143
top acc: 0.0505 ::: bot acc: 0.0483
top acc: 0.0743 ::: bot acc: 0.0330
top acc: 0.0580 ::: bot acc: 0.0443
top acc: 0.0695 ::: bot acc: 0.0355
current epoch: 41
train loss is 0.001242
average val loss: 0.001093, accuracy: 0.0365
average test loss: 0.001043, accuracy: 0.0365
case acc: 0.033008922
case acc: 0.029751293
case acc: 0.036799107
case acc: 0.040462557
case acc: 0.03925086
case acc: 0.039507
top acc: 0.0524 ::: bot acc: 0.0363
top acc: 0.0557 ::: bot acc: 0.0174
top acc: 0.0468 ::: bot acc: 0.0512
top acc: 0.0714 ::: bot acc: 0.0357
top acc: 0.0545 ::: bot acc: 0.0484
top acc: 0.0656 ::: bot acc: 0.0389
current epoch: 42
train loss is 0.001196
average val loss: 0.001063, accuracy: 0.0361
average test loss: 0.001028, accuracy: 0.0362
case acc: 0.032587793
case acc: 0.028803453
case acc: 0.037197504
case acc: 0.03987875
case acc: 0.0397939
case acc: 0.03908064
top acc: 0.0501 ::: bot acc: 0.0384
top acc: 0.0531 ::: bot acc: 0.0199
top acc: 0.0454 ::: bot acc: 0.0533
top acc: 0.0693 ::: bot acc: 0.0377
top acc: 0.0529 ::: bot acc: 0.0507
top acc: 0.0633 ::: bot acc: 0.0407
current epoch: 43
train loss is 0.001179
average val loss: 0.001048, accuracy: 0.0358
average test loss: 0.001023, accuracy: 0.0362
case acc: 0.03292124
case acc: 0.02833041
case acc: 0.03743773
case acc: 0.03953381
case acc: 0.039953925
case acc: 0.03898483
top acc: 0.0485 ::: bot acc: 0.0409
top acc: 0.0508 ::: bot acc: 0.0228
top acc: 0.0437 ::: bot acc: 0.0549
top acc: 0.0679 ::: bot acc: 0.0391
top acc: 0.0506 ::: bot acc: 0.0523
top acc: 0.0618 ::: bot acc: 0.0426
current epoch: 44
train loss is 0.001166
average val loss: 0.001038, accuracy: 0.0357
average test loss: 0.001017, accuracy: 0.0360
case acc: 0.03266605
case acc: 0.027511682
case acc: 0.037668526
case acc: 0.039226044
case acc: 0.040092297
case acc: 0.039040677
top acc: 0.0470 ::: bot acc: 0.0419
top acc: 0.0489 ::: bot acc: 0.0235
top acc: 0.0435 ::: bot acc: 0.0557
top acc: 0.0671 ::: bot acc: 0.0398
top acc: 0.0495 ::: bot acc: 0.0532
top acc: 0.0609 ::: bot acc: 0.0437
current epoch: 45
train loss is 0.001154
average val loss: 0.001030, accuracy: 0.0355
average test loss: 0.001009, accuracy: 0.0360
case acc: 0.032540046
case acc: 0.02756343
case acc: 0.037495777
case acc: 0.03909016
case acc: 0.04039757
case acc: 0.038827416
top acc: 0.0463 ::: bot acc: 0.0423
top acc: 0.0484 ::: bot acc: 0.0248
top acc: 0.0422 ::: bot acc: 0.0561
top acc: 0.0666 ::: bot acc: 0.0403
top acc: 0.0486 ::: bot acc: 0.0541
top acc: 0.0597 ::: bot acc: 0.0445
current epoch: 46
train loss is 0.001152
average val loss: 0.001047, accuracy: 0.0358
average test loss: 0.001021, accuracy: 0.0362
case acc: 0.032667674
case acc: 0.02828838
case acc: 0.037371036
case acc: 0.039673768
case acc: 0.04004771
case acc: 0.039201993
top acc: 0.0480 ::: bot acc: 0.0406
top acc: 0.0498 ::: bot acc: 0.0237
top acc: 0.0446 ::: bot acc: 0.0544
top acc: 0.0682 ::: bot acc: 0.0387
top acc: 0.0506 ::: bot acc: 0.0524
top acc: 0.0617 ::: bot acc: 0.0431
current epoch: 47
train loss is 0.001154
average val loss: 0.001041, accuracy: 0.0357
average test loss: 0.001014, accuracy: 0.0360
case acc: 0.032598607
case acc: 0.027774803
case acc: 0.037079453
case acc: 0.039495327
case acc: 0.04005003
case acc: 0.038956806
top acc: 0.0476 ::: bot acc: 0.0410
top acc: 0.0486 ::: bot acc: 0.0244
top acc: 0.0439 ::: bot acc: 0.0542
top acc: 0.0681 ::: bot acc: 0.0390
top acc: 0.0499 ::: bot acc: 0.0528
top acc: 0.0614 ::: bot acc: 0.0434
current epoch: 48
train loss is 0.001146
average val loss: 0.001040, accuracy: 0.0356
average test loss: 0.001019, accuracy: 0.0361
case acc: 0.0328107
case acc: 0.027751645
case acc: 0.03744913
case acc: 0.03970975
case acc: 0.039950952
case acc: 0.038994223
top acc: 0.0483 ::: bot acc: 0.0409
top acc: 0.0494 ::: bot acc: 0.0239
top acc: 0.0451 ::: bot acc: 0.0539
top acc: 0.0686 ::: bot acc: 0.0381
top acc: 0.0507 ::: bot acc: 0.0520
top acc: 0.0616 ::: bot acc: 0.0428
current epoch: 49
train loss is 0.001149
average val loss: 0.001037, accuracy: 0.0355
average test loss: 0.001018, accuracy: 0.0361
case acc: 0.032518297
case acc: 0.027773527
case acc: 0.037245236
case acc: 0.03973899
case acc: 0.04005991
case acc: 0.039170984
top acc: 0.0476 ::: bot acc: 0.0410
top acc: 0.0487 ::: bot acc: 0.0247
top acc: 0.0444 ::: bot acc: 0.0541
top acc: 0.0683 ::: bot acc: 0.0385
top acc: 0.0507 ::: bot acc: 0.0523
top acc: 0.0615 ::: bot acc: 0.0435
current epoch: 50
train loss is 0.001140
average val loss: 0.001034, accuracy: 0.0355
average test loss: 0.001014, accuracy: 0.0360
case acc: 0.03256968
case acc: 0.027453415
case acc: 0.03761401
case acc: 0.03923285
case acc: 0.04016377
case acc: 0.03911584
top acc: 0.0462 ::: bot acc: 0.0424
top acc: 0.0474 ::: bot acc: 0.0258
top acc: 0.0439 ::: bot acc: 0.0550
top acc: 0.0673 ::: bot acc: 0.0390
top acc: 0.0503 ::: bot acc: 0.0529
top acc: 0.0604 ::: bot acc: 0.0443
LME_Co_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5400 5400 5400
1.7082474 -0.6288155 0.16982092 -0.17269358
Validation: 606 606 606
Testing: 750 750 750
pre-processing time: 0.0003192424774169922
the split date is 2012-01-01
train dropout: 0.2 test dropout: 0.1
net initializing with time: 0.0033426284790039062
preparing training and testing date with time: 4.76837158203125e-07
current epoch: 1
train loss is 0.008083
average val loss: 0.005645, accuracy: 0.0944
average test loss: 0.004169, accuracy: 0.0772
case acc: 0.10795263
case acc: 0.12634368
case acc: 0.061707497
case acc: 0.05048291
case acc: 0.07160618
case acc: 0.04532223
top acc: 0.0607 ::: bot acc: 0.1580
top acc: 0.1662 ::: bot acc: 0.0812
top acc: 0.0157 ::: bot acc: 0.1180
top acc: 0.0222 ::: bot acc: 0.0900
top acc: 0.0275 ::: bot acc: 0.1136
top acc: 0.0449 ::: bot acc: 0.0704
current epoch: 2
train loss is 0.007109
average val loss: 0.005631, accuracy: 0.0778
average test loss: 0.007184, accuracy: 0.0926
case acc: 0.037295036
case acc: 0.23211083
case acc: 0.07217809
case acc: 0.076002315
case acc: 0.047835097
case acc: 0.090468116
top acc: 0.0599 ::: bot acc: 0.0362
top acc: 0.2719 ::: bot acc: 0.1881
top acc: 0.1272 ::: bot acc: 0.0219
top acc: 0.1207 ::: bot acc: 0.0318
top acc: 0.0954 ::: bot acc: 0.0131
top acc: 0.1516 ::: bot acc: 0.0347
current epoch: 3
train loss is 0.008886
average val loss: 0.014207, accuracy: 0.1467
average test loss: 0.017968, accuracy: 0.1728
case acc: 0.10663993
case acc: 0.3108101
case acc: 0.15804939
case acc: 0.16096386
case acc: 0.12867701
case acc: 0.17149284
top acc: 0.1527 ::: bot acc: 0.0576
top acc: 0.3504 ::: bot acc: 0.2658
top acc: 0.2197 ::: bot acc: 0.0945
top acc: 0.2088 ::: bot acc: 0.1112
top acc: 0.1827 ::: bot acc: 0.0812
top acc: 0.2326 ::: bot acc: 0.1161
current epoch: 4
train loss is 0.012422
average val loss: 0.007842, accuracy: 0.0996
average test loss: 0.010386, accuracy: 0.1231
case acc: 0.06435624
case acc: 0.25169128
case acc: 0.11089813
case acc: 0.11006442
case acc: 0.08322637
case acc: 0.118322514
top acc: 0.1092 ::: bot acc: 0.0195
top acc: 0.2923 ::: bot acc: 0.2057
top acc: 0.1707 ::: bot acc: 0.0486
top acc: 0.1575 ::: bot acc: 0.0610
top acc: 0.1378 ::: bot acc: 0.0355
top acc: 0.1797 ::: bot acc: 0.0622
current epoch: 5
train loss is 0.010755
average val loss: 0.003243, accuracy: 0.0690
average test loss: 0.002628, accuracy: 0.0587
case acc: 0.05913286
case acc: 0.117597185
case acc: 0.047121108
case acc: 0.03622514
case acc: 0.047299474
case acc: 0.04470999
top acc: 0.0154 ::: bot acc: 0.1061
top acc: 0.1574 ::: bot acc: 0.0725
top acc: 0.0465 ::: bot acc: 0.0770
top acc: 0.0347 ::: bot acc: 0.0614
top acc: 0.0270 ::: bot acc: 0.0786
top acc: 0.0555 ::: bot acc: 0.0622
current epoch: 6
train loss is 0.004377
average val loss: 0.002721, accuracy: 0.0620
average test loss: 0.002416, accuracy: 0.0552
case acc: 0.048444733
case acc: 0.118084356
case acc: 0.04687006
case acc: 0.03507247
case acc: 0.03929393
case acc: 0.043205213
top acc: 0.0120 ::: bot acc: 0.0922
top acc: 0.1580 ::: bot acc: 0.0728
top acc: 0.0543 ::: bot acc: 0.0691
top acc: 0.0452 ::: bot acc: 0.0516
top acc: 0.0432 ::: bot acc: 0.0582
top acc: 0.0665 ::: bot acc: 0.0502
current epoch: 7
train loss is 0.003115
average val loss: 0.002590, accuracy: 0.0548
average test loss: 0.003261, accuracy: 0.0623
case acc: 0.034926135
case acc: 0.14633793
case acc: 0.051382333
case acc: 0.04597153
case acc: 0.042865798
case acc: 0.05229545
top acc: 0.0427 ::: bot acc: 0.0519
top acc: 0.1861 ::: bot acc: 0.1012
top acc: 0.0899 ::: bot acc: 0.0352
top acc: 0.0818 ::: bot acc: 0.0207
top acc: 0.0868 ::: bot acc: 0.0173
top acc: 0.1030 ::: bot acc: 0.0173
current epoch: 8
train loss is 0.003535
average val loss: 0.002290, accuracy: 0.0526
average test loss: 0.002858, accuracy: 0.0588
case acc: 0.035051007
case acc: 0.13332722
case acc: 0.04933709
case acc: 0.04273246
case acc: 0.042489782
case acc: 0.049707353
top acc: 0.0392 ::: bot acc: 0.0550
top acc: 0.1729 ::: bot acc: 0.0883
top acc: 0.0824 ::: bot acc: 0.0413
top acc: 0.0762 ::: bot acc: 0.0238
top acc: 0.0854 ::: bot acc: 0.0180
top acc: 0.0973 ::: bot acc: 0.0213
current epoch: 9
train loss is 0.003227
average val loss: 0.002032, accuracy: 0.0511
average test loss: 0.002348, accuracy: 0.0539
case acc: 0.036213703
case acc: 0.11504527
case acc: 0.047682513
case acc: 0.038879257
case acc: 0.03998088
case acc: 0.0455295
top acc: 0.0318 ::: bot acc: 0.0631
top acc: 0.1549 ::: bot acc: 0.0700
top acc: 0.0710 ::: bot acc: 0.0524
top acc: 0.0664 ::: bot acc: 0.0314
top acc: 0.0794 ::: bot acc: 0.0222
top acc: 0.0863 ::: bot acc: 0.0302
current epoch: 10
train loss is 0.002726
average val loss: 0.001887, accuracy: 0.0494
average test loss: 0.002236, accuracy: 0.0531
case acc: 0.03557068
case acc: 0.108091824
case acc: 0.04783943
case acc: 0.039196633
case acc: 0.042220313
case acc: 0.045404285
top acc: 0.0342 ::: bot acc: 0.0603
top acc: 0.1479 ::: bot acc: 0.0635
top acc: 0.0700 ::: bot acc: 0.0542
top acc: 0.0668 ::: bot acc: 0.0315
top acc: 0.0841 ::: bot acc: 0.0198
top acc: 0.0871 ::: bot acc: 0.0291
current epoch: 11
train loss is 0.002473
average val loss: 0.001788, accuracy: 0.0476
average test loss: 0.002252, accuracy: 0.0535
case acc: 0.035034887
case acc: 0.10517943
case acc: 0.047687713
case acc: 0.040652316
case acc: 0.045386624
case acc: 0.047266476
top acc: 0.0403 ::: bot acc: 0.0539
top acc: 0.1450 ::: bot acc: 0.0604
top acc: 0.0730 ::: bot acc: 0.0504
top acc: 0.0711 ::: bot acc: 0.0282
top acc: 0.0911 ::: bot acc: 0.0149
top acc: 0.0905 ::: bot acc: 0.0273
current epoch: 12
train loss is 0.002271
average val loss: 0.001703, accuracy: 0.0464
average test loss: 0.002221, accuracy: 0.0534
case acc: 0.03472222
case acc: 0.10095854
case acc: 0.048065044
case acc: 0.04148824
case acc: 0.047675353
case acc: 0.047408983
top acc: 0.0448 ::: bot acc: 0.0493
top acc: 0.1411 ::: bot acc: 0.0562
top acc: 0.0749 ::: bot acc: 0.0489
top acc: 0.0735 ::: bot acc: 0.0263
top acc: 0.0952 ::: bot acc: 0.0134
top acc: 0.0926 ::: bot acc: 0.0244
current epoch: 13
train loss is 0.002181
average val loss: 0.001623, accuracy: 0.0454
average test loss: 0.002149, accuracy: 0.0528
case acc: 0.03451897
case acc: 0.09552589
case acc: 0.048034936
case acc: 0.041651867
case acc: 0.049095534
case acc: 0.047847692
top acc: 0.0465 ::: bot acc: 0.0466
top acc: 0.1355 ::: bot acc: 0.0508
top acc: 0.0750 ::: bot acc: 0.0488
top acc: 0.0741 ::: bot acc: 0.0255
top acc: 0.0969 ::: bot acc: 0.0137
top acc: 0.0933 ::: bot acc: 0.0248
current epoch: 14
train loss is 0.002004
average val loss: 0.001517, accuracy: 0.0442
average test loss: 0.001972, accuracy: 0.0508
case acc: 0.03487817
case acc: 0.08728681
case acc: 0.047556836
case acc: 0.04081
case acc: 0.04767114
case acc: 0.046366714
top acc: 0.0463 ::: bot acc: 0.0478
top acc: 0.1267 ::: bot acc: 0.0434
top acc: 0.0719 ::: bot acc: 0.0516
top acc: 0.0710 ::: bot acc: 0.0282
top acc: 0.0952 ::: bot acc: 0.0137
top acc: 0.0897 ::: bot acc: 0.0271
current epoch: 15
train loss is 0.001837
average val loss: 0.001456, accuracy: 0.0433
average test loss: 0.001916, accuracy: 0.0502
case acc: 0.03501107
case acc: 0.08227782
case acc: 0.047697257
case acc: 0.041136995
case acc: 0.048336074
case acc: 0.04662088
top acc: 0.0485 ::: bot acc: 0.0458
top acc: 0.1214 ::: bot acc: 0.0386
top acc: 0.0726 ::: bot acc: 0.0512
top acc: 0.0721 ::: bot acc: 0.0273
top acc: 0.0969 ::: bot acc: 0.0129
top acc: 0.0902 ::: bot acc: 0.0271
current epoch: 16
train loss is 0.001749
average val loss: 0.001400, accuracy: 0.0424
average test loss: 0.001886, accuracy: 0.0499
case acc: 0.03527187
case acc: 0.07862484
case acc: 0.04767952
case acc: 0.041286137
case acc: 0.049629007
case acc: 0.047008872
top acc: 0.0513 ::: bot acc: 0.0428
top acc: 0.1178 ::: bot acc: 0.0354
top acc: 0.0730 ::: bot acc: 0.0505
top acc: 0.0732 ::: bot acc: 0.0254
top acc: 0.0982 ::: bot acc: 0.0137
top acc: 0.0908 ::: bot acc: 0.0268
current epoch: 17
train loss is 0.001646
average val loss: 0.001362, accuracy: 0.0417
average test loss: 0.001916, accuracy: 0.0505
case acc: 0.03585902
case acc: 0.07676937
case acc: 0.047922432
case acc: 0.043331258
case acc: 0.05142559
case acc: 0.047573105
top acc: 0.0562 ::: bot acc: 0.0382
top acc: 0.1160 ::: bot acc: 0.0343
top acc: 0.0757 ::: bot acc: 0.0472
top acc: 0.0766 ::: bot acc: 0.0244
top acc: 0.1009 ::: bot acc: 0.0141
top acc: 0.0925 ::: bot acc: 0.0249
current epoch: 18
train loss is 0.001594
average val loss: 0.001326, accuracy: 0.0411
average test loss: 0.001933, accuracy: 0.0508
case acc: 0.036447044
case acc: 0.07512404
case acc: 0.048691675
case acc: 0.044234905
case acc: 0.05212208
case acc: 0.048201356
top acc: 0.0594 ::: bot acc: 0.0348
top acc: 0.1138 ::: bot acc: 0.0335
top acc: 0.0783 ::: bot acc: 0.0454
top acc: 0.0788 ::: bot acc: 0.0226
top acc: 0.1022 ::: bot acc: 0.0137
top acc: 0.0939 ::: bot acc: 0.0237
current epoch: 19
train loss is 0.001545
average val loss: 0.001286, accuracy: 0.0406
average test loss: 0.001881, accuracy: 0.0503
case acc: 0.03655548
case acc: 0.07172787
case acc: 0.04889186
case acc: 0.044583496
case acc: 0.051947642
case acc: 0.04788011
top acc: 0.0597 ::: bot acc: 0.0339
top acc: 0.1095 ::: bot acc: 0.0314
top acc: 0.0790 ::: bot acc: 0.0451
top acc: 0.0790 ::: bot acc: 0.0238
top acc: 0.1015 ::: bot acc: 0.0139
top acc: 0.0934 ::: bot acc: 0.0246
current epoch: 20
train loss is 0.001496
average val loss: 0.001271, accuracy: 0.0403
average test loss: 0.001878, accuracy: 0.0502
case acc: 0.03755595
case acc: 0.06957419
case acc: 0.0488813
case acc: 0.045241885
case acc: 0.051722553
case acc: 0.04812887
top acc: 0.0632 ::: bot acc: 0.0315
top acc: 0.1073 ::: bot acc: 0.0299
top acc: 0.0814 ::: bot acc: 0.0423
top acc: 0.0801 ::: bot acc: 0.0221
top acc: 0.1017 ::: bot acc: 0.0137
top acc: 0.0935 ::: bot acc: 0.0246
current epoch: 21
train loss is 0.001466
average val loss: 0.001238, accuracy: 0.0397
average test loss: 0.001869, accuracy: 0.0500
case acc: 0.03781616
case acc: 0.06764333
case acc: 0.049248815
case acc: 0.04612324
case acc: 0.051525787
case acc: 0.047908954
top acc: 0.0651 ::: bot acc: 0.0288
top acc: 0.1054 ::: bot acc: 0.0283
top acc: 0.0827 ::: bot acc: 0.0408
top acc: 0.0824 ::: bot acc: 0.0214
top acc: 0.1011 ::: bot acc: 0.0134
top acc: 0.0934 ::: bot acc: 0.0239
current epoch: 22
train loss is 0.001387
average val loss: 0.001221, accuracy: 0.0395
average test loss: 0.001896, accuracy: 0.0506
case acc: 0.039033636
case acc: 0.0665848
case acc: 0.050034687
case acc: 0.04710379
case acc: 0.05237062
case acc: 0.048265878
top acc: 0.0675 ::: bot acc: 0.0270
top acc: 0.1038 ::: bot acc: 0.0283
top acc: 0.0857 ::: bot acc: 0.0384
top acc: 0.0841 ::: bot acc: 0.0207
top acc: 0.1021 ::: bot acc: 0.0137
top acc: 0.0942 ::: bot acc: 0.0236
current epoch: 23
train loss is 0.001389
average val loss: 0.001242, accuracy: 0.0398
average test loss: 0.001982, accuracy: 0.0518
case acc: 0.04074759
case acc: 0.066964865
case acc: 0.0509978
case acc: 0.04898458
case acc: 0.053740583
case acc: 0.049180575
top acc: 0.0719 ::: bot acc: 0.0237
top acc: 0.1038 ::: bot acc: 0.0282
top acc: 0.0893 ::: bot acc: 0.0349
top acc: 0.0870 ::: bot acc: 0.0201
top acc: 0.1043 ::: bot acc: 0.0137
top acc: 0.0963 ::: bot acc: 0.0220
current epoch: 24
train loss is 0.001366
average val loss: 0.001240, accuracy: 0.0399
average test loss: 0.002019, accuracy: 0.0523
case acc: 0.042109475
case acc: 0.06610782
case acc: 0.0518235
case acc: 0.050210528
case acc: 0.054369066
case acc: 0.049441993
top acc: 0.0743 ::: bot acc: 0.0228
top acc: 0.1031 ::: bot acc: 0.0281
top acc: 0.0919 ::: bot acc: 0.0318
top acc: 0.0891 ::: bot acc: 0.0199
top acc: 0.1052 ::: bot acc: 0.0139
top acc: 0.0970 ::: bot acc: 0.0217
current epoch: 25
train loss is 0.001372
average val loss: 0.001245, accuracy: 0.0399
average test loss: 0.002079, accuracy: 0.0531
case acc: 0.042930257
case acc: 0.06589501
case acc: 0.053134553
case acc: 0.051508047
case acc: 0.055530768
case acc: 0.049877606
top acc: 0.0766 ::: bot acc: 0.0204
top acc: 0.1025 ::: bot acc: 0.0279
top acc: 0.0945 ::: bot acc: 0.0300
top acc: 0.0919 ::: bot acc: 0.0191
top acc: 0.1067 ::: bot acc: 0.0145
top acc: 0.0981 ::: bot acc: 0.0209
current epoch: 26
train loss is 0.001368
average val loss: 0.001235, accuracy: 0.0398
average test loss: 0.002071, accuracy: 0.0531
case acc: 0.04406973
case acc: 0.06460704
case acc: 0.053404987
case acc: 0.05166377
case acc: 0.055410597
case acc: 0.049582202
top acc: 0.0783 ::: bot acc: 0.0207
top acc: 0.1013 ::: bot acc: 0.0272
top acc: 0.0956 ::: bot acc: 0.0287
top acc: 0.0916 ::: bot acc: 0.0192
top acc: 0.1067 ::: bot acc: 0.0143
top acc: 0.0976 ::: bot acc: 0.0208
current epoch: 27
train loss is 0.001321
average val loss: 0.001196, accuracy: 0.0392
average test loss: 0.002006, accuracy: 0.0522
case acc: 0.043557495
case acc: 0.06185027
case acc: 0.053312156
case acc: 0.05157379
case acc: 0.054486495
case acc: 0.0486858
top acc: 0.0775 ::: bot acc: 0.0204
top acc: 0.0976 ::: bot acc: 0.0263
top acc: 0.0957 ::: bot acc: 0.0285
top acc: 0.0913 ::: bot acc: 0.0192
top acc: 0.1051 ::: bot acc: 0.0145
top acc: 0.0956 ::: bot acc: 0.0217
current epoch: 28
train loss is 0.001278
average val loss: 0.001168, accuracy: 0.0387
average test loss: 0.001948, accuracy: 0.0515
case acc: 0.043536756
case acc: 0.059779007
case acc: 0.053218067
case acc: 0.050884563
case acc: 0.053342223
case acc: 0.04812043
top acc: 0.0773 ::: bot acc: 0.0212
top acc: 0.0946 ::: bot acc: 0.0262
top acc: 0.0954 ::: bot acc: 0.0292
top acc: 0.0904 ::: bot acc: 0.0190
top acc: 0.1036 ::: bot acc: 0.0137
top acc: 0.0935 ::: bot acc: 0.0240
current epoch: 29
train loss is 0.001257
average val loss: 0.001165, accuracy: 0.0386
average test loss: 0.001963, accuracy: 0.0516
case acc: 0.044029687
case acc: 0.05897173
case acc: 0.05377122
case acc: 0.051452577
case acc: 0.0533588
case acc: 0.048092388
top acc: 0.0786 ::: bot acc: 0.0200
top acc: 0.0934 ::: bot acc: 0.0260
top acc: 0.0971 ::: bot acc: 0.0278
top acc: 0.0914 ::: bot acc: 0.0192
top acc: 0.1041 ::: bot acc: 0.0142
top acc: 0.0940 ::: bot acc: 0.0234
current epoch: 30
train loss is 0.001253
average val loss: 0.001183, accuracy: 0.0389
average test loss: 0.002017, accuracy: 0.0525
case acc: 0.044877186
case acc: 0.05936225
case acc: 0.054889966
case acc: 0.05250174
case acc: 0.05419158
case acc: 0.048922658
top acc: 0.0799 ::: bot acc: 0.0193
top acc: 0.0935 ::: bot acc: 0.0265
top acc: 0.0987 ::: bot acc: 0.0264
top acc: 0.0930 ::: bot acc: 0.0187
top acc: 0.1053 ::: bot acc: 0.0138
top acc: 0.0951 ::: bot acc: 0.0236
current epoch: 31
train loss is 0.001225
average val loss: 0.001145, accuracy: 0.0383
average test loss: 0.001951, accuracy: 0.0515
case acc: 0.044442344
case acc: 0.057173766
case acc: 0.0543335
case acc: 0.052150358
case acc: 0.05310504
case acc: 0.047685087
top acc: 0.0794 ::: bot acc: 0.0198
top acc: 0.0911 ::: bot acc: 0.0255
top acc: 0.0980 ::: bot acc: 0.0262
top acc: 0.0923 ::: bot acc: 0.0191
top acc: 0.1033 ::: bot acc: 0.0134
top acc: 0.0927 ::: bot acc: 0.0244
current epoch: 32
train loss is 0.001203
average val loss: 0.001153, accuracy: 0.0384
average test loss: 0.001967, accuracy: 0.0517
case acc: 0.04492007
case acc: 0.056835648
case acc: 0.055071596
case acc: 0.052628223
case acc: 0.05308169
case acc: 0.04755072
top acc: 0.0801 ::: bot acc: 0.0194
top acc: 0.0905 ::: bot acc: 0.0258
top acc: 0.0999 ::: bot acc: 0.0254
top acc: 0.0930 ::: bot acc: 0.0193
top acc: 0.1037 ::: bot acc: 0.0139
top acc: 0.0930 ::: bot acc: 0.0242
current epoch: 33
train loss is 0.001210
average val loss: 0.001175, accuracy: 0.0388
average test loss: 0.002042, accuracy: 0.0527
case acc: 0.04632743
case acc: 0.057020307
case acc: 0.05640348
case acc: 0.053930063
case acc: 0.054311175
case acc: 0.048319474
top acc: 0.0827 ::: bot acc: 0.0185
top acc: 0.0910 ::: bot acc: 0.0257
top acc: 0.1027 ::: bot acc: 0.0240
top acc: 0.0951 ::: bot acc: 0.0187
top acc: 0.1053 ::: bot acc: 0.0140
top acc: 0.0946 ::: bot acc: 0.0231
current epoch: 34
train loss is 0.001195
average val loss: 0.001196, accuracy: 0.0390
average test loss: 0.002096, accuracy: 0.0536
case acc: 0.04745988
case acc: 0.05719819
case acc: 0.05755341
case acc: 0.055304285
case acc: 0.055464845
case acc: 0.048601687
top acc: 0.0844 ::: bot acc: 0.0184
top acc: 0.0911 ::: bot acc: 0.0257
top acc: 0.1040 ::: bot acc: 0.0240
top acc: 0.0969 ::: bot acc: 0.0191
top acc: 0.1066 ::: bot acc: 0.0141
top acc: 0.0952 ::: bot acc: 0.0223
current epoch: 35
train loss is 0.001215
average val loss: 0.001208, accuracy: 0.0393
average test loss: 0.002148, accuracy: 0.0543
case acc: 0.047727738
case acc: 0.05718731
case acc: 0.058665756
case acc: 0.056187514
case acc: 0.056602433
case acc: 0.049264275
top acc: 0.0849 ::: bot acc: 0.0179
top acc: 0.0912 ::: bot acc: 0.0254
top acc: 0.1062 ::: bot acc: 0.0231
top acc: 0.0984 ::: bot acc: 0.0187
top acc: 0.1084 ::: bot acc: 0.0148
top acc: 0.0964 ::: bot acc: 0.0215
current epoch: 36
train loss is 0.001214
average val loss: 0.001201, accuracy: 0.0392
average test loss: 0.002127, accuracy: 0.0540
case acc: 0.047702402
case acc: 0.056358855
case acc: 0.058560237
case acc: 0.056090303
case acc: 0.056285124
case acc: 0.048899986
top acc: 0.0846 ::: bot acc: 0.0182
top acc: 0.0899 ::: bot acc: 0.0253
top acc: 0.1062 ::: bot acc: 0.0228
top acc: 0.0984 ::: bot acc: 0.0189
top acc: 0.1075 ::: bot acc: 0.0149
top acc: 0.0958 ::: bot acc: 0.0220
current epoch: 37
train loss is 0.001212
average val loss: 0.001179, accuracy: 0.0388
average test loss: 0.002103, accuracy: 0.0536
case acc: 0.047542896
case acc: 0.055051584
case acc: 0.058066286
case acc: 0.056281842
case acc: 0.05598128
case acc: 0.04876187
top acc: 0.0847 ::: bot acc: 0.0185
top acc: 0.0883 ::: bot acc: 0.0248
top acc: 0.1058 ::: bot acc: 0.0228
top acc: 0.0980 ::: bot acc: 0.0195
top acc: 0.1073 ::: bot acc: 0.0146
top acc: 0.0954 ::: bot acc: 0.0230
current epoch: 38
train loss is 0.001167
average val loss: 0.001158, accuracy: 0.0385
average test loss: 0.002057, accuracy: 0.0530
case acc: 0.046872035
case acc: 0.05364729
case acc: 0.058121648
case acc: 0.055674873
case acc: 0.05526384
case acc: 0.048168927
top acc: 0.0834 ::: bot acc: 0.0184
top acc: 0.0863 ::: bot acc: 0.0249
top acc: 0.1052 ::: bot acc: 0.0235
top acc: 0.0976 ::: bot acc: 0.0193
top acc: 0.1068 ::: bot acc: 0.0140
top acc: 0.0942 ::: bot acc: 0.0230
current epoch: 39
train loss is 0.001146
average val loss: 0.001130, accuracy: 0.0380
average test loss: 0.001983, accuracy: 0.0519
case acc: 0.04591373
case acc: 0.05191152
case acc: 0.05711454
case acc: 0.054368928
case acc: 0.05466012
case acc: 0.04755861
top acc: 0.0822 ::: bot acc: 0.0188
top acc: 0.0836 ::: bot acc: 0.0250
top acc: 0.1037 ::: bot acc: 0.0238
top acc: 0.0959 ::: bot acc: 0.0186
top acc: 0.1054 ::: bot acc: 0.0145
top acc: 0.0924 ::: bot acc: 0.0248
current epoch: 40
train loss is 0.001108
average val loss: 0.001080, accuracy: 0.0372
average test loss: 0.001865, accuracy: 0.0501
case acc: 0.04393728
case acc: 0.049333986
case acc: 0.055571657
case acc: 0.052649774
case acc: 0.053000018
case acc: 0.046291452
top acc: 0.0786 ::: bot acc: 0.0196
top acc: 0.0800 ::: bot acc: 0.0250
top acc: 0.1008 ::: bot acc: 0.0250
top acc: 0.0932 ::: bot acc: 0.0188
top acc: 0.1033 ::: bot acc: 0.0140
top acc: 0.0896 ::: bot acc: 0.0273
current epoch: 41
train loss is 0.001071
average val loss: 0.001046, accuracy: 0.0366
average test loss: 0.001771, accuracy: 0.0487
case acc: 0.042824317
case acc: 0.046864294
case acc: 0.054614346
case acc: 0.051309913
case acc: 0.051146872
case acc: 0.045673165
top acc: 0.0763 ::: bot acc: 0.0211
top acc: 0.0763 ::: bot acc: 0.0247
top acc: 0.0984 ::: bot acc: 0.0267
top acc: 0.0911 ::: bot acc: 0.0192
top acc: 0.1008 ::: bot acc: 0.0133
top acc: 0.0870 ::: bot acc: 0.0298
current epoch: 42
train loss is 0.001043
average val loss: 0.001032, accuracy: 0.0363
average test loss: 0.001701, accuracy: 0.0476
case acc: 0.041530002
case acc: 0.045308128
case acc: 0.053720202
case acc: 0.050191592
case acc: 0.04993521
case acc: 0.045048766
top acc: 0.0739 ::: bot acc: 0.0219
top acc: 0.0737 ::: bot acc: 0.0253
top acc: 0.0968 ::: bot acc: 0.0278
top acc: 0.0892 ::: bot acc: 0.0194
top acc: 0.0989 ::: bot acc: 0.0134
top acc: 0.0854 ::: bot acc: 0.0310
current epoch: 43
train loss is 0.001021
average val loss: 0.001022, accuracy: 0.0362
average test loss: 0.001667, accuracy: 0.0471
case acc: 0.041039128
case acc: 0.043968614
case acc: 0.05331775
case acc: 0.049820084
case acc: 0.04946774
case acc: 0.04488193
top acc: 0.0728 ::: bot acc: 0.0224
top acc: 0.0716 ::: bot acc: 0.0254
top acc: 0.0958 ::: bot acc: 0.0290
top acc: 0.0891 ::: bot acc: 0.0192
top acc: 0.0983 ::: bot acc: 0.0134
top acc: 0.0849 ::: bot acc: 0.0322
current epoch: 44
train loss is 0.001003
average val loss: 0.001007, accuracy: 0.0359
average test loss: 0.001600, accuracy: 0.0461
case acc: 0.040186692
case acc: 0.042427655
case acc: 0.052454453
case acc: 0.049039103
case acc: 0.047833506
case acc: 0.04440178
top acc: 0.0711 ::: bot acc: 0.0240
top acc: 0.0693 ::: bot acc: 0.0256
top acc: 0.0935 ::: bot acc: 0.0304
top acc: 0.0872 ::: bot acc: 0.0201
top acc: 0.0957 ::: bot acc: 0.0135
top acc: 0.0829 ::: bot acc: 0.0338
current epoch: 45
train loss is 0.000994
average val loss: 0.001010, accuracy: 0.0360
average test loss: 0.001582, accuracy: 0.0458
case acc: 0.039583538
case acc: 0.041655436
case acc: 0.052191135
case acc: 0.04880471
case acc: 0.047794044
case acc: 0.044528566
top acc: 0.0696 ::: bot acc: 0.0248
top acc: 0.0682 ::: bot acc: 0.0259
top acc: 0.0929 ::: bot acc: 0.0307
top acc: 0.0868 ::: bot acc: 0.0200
top acc: 0.0956 ::: bot acc: 0.0133
top acc: 0.0828 ::: bot acc: 0.0341
current epoch: 46
train loss is 0.000988
average val loss: 0.001005, accuracy: 0.0360
average test loss: 0.001550, accuracy: 0.0452
case acc: 0.039117828
case acc: 0.040736355
case acc: 0.051927052
case acc: 0.048491813
case acc: 0.046811268
case acc: 0.04417922
top acc: 0.0683 ::: bot acc: 0.0261
top acc: 0.0666 ::: bot acc: 0.0265
top acc: 0.0919 ::: bot acc: 0.0325
top acc: 0.0862 ::: bot acc: 0.0204
top acc: 0.0943 ::: bot acc: 0.0135
top acc: 0.0818 ::: bot acc: 0.0347
current epoch: 47
train loss is 0.000981
average val loss: 0.000996, accuracy: 0.0358
average test loss: 0.001484, accuracy: 0.0442
case acc: 0.03816169
case acc: 0.03936807
case acc: 0.051203314
case acc: 0.047157217
case acc: 0.04569158
case acc: 0.043908585
top acc: 0.0661 ::: bot acc: 0.0279
top acc: 0.0639 ::: bot acc: 0.0271
top acc: 0.0895 ::: bot acc: 0.0347
top acc: 0.0842 ::: bot acc: 0.0210
top acc: 0.0917 ::: bot acc: 0.0150
top acc: 0.0804 ::: bot acc: 0.0366
current epoch: 48
train loss is 0.000960
average val loss: 0.000997, accuracy: 0.0359
average test loss: 0.001477, accuracy: 0.0441
case acc: 0.03813621
case acc: 0.039053984
case acc: 0.0510428
case acc: 0.047113128
case acc: 0.04548119
case acc: 0.044058394
top acc: 0.0658 ::: bot acc: 0.0282
top acc: 0.0633 ::: bot acc: 0.0276
top acc: 0.0892 ::: bot acc: 0.0346
top acc: 0.0841 ::: bot acc: 0.0207
top acc: 0.0913 ::: bot acc: 0.0155
top acc: 0.0801 ::: bot acc: 0.0372
current epoch: 49
train loss is 0.000961
average val loss: 0.000995, accuracy: 0.0358
average test loss: 0.001473, accuracy: 0.0440
case acc: 0.03793364
case acc: 0.03880425
case acc: 0.05106626
case acc: 0.046911493
case acc: 0.045331195
case acc: 0.04383974
top acc: 0.0653 ::: bot acc: 0.0286
top acc: 0.0633 ::: bot acc: 0.0275
top acc: 0.0895 ::: bot acc: 0.0346
top acc: 0.0840 ::: bot acc: 0.0206
top acc: 0.0912 ::: bot acc: 0.0158
top acc: 0.0796 ::: bot acc: 0.0373
current epoch: 50
train loss is 0.000957
average val loss: 0.000994, accuracy: 0.0358
average test loss: 0.001461, accuracy: 0.0438
case acc: 0.03791524
case acc: 0.03824752
case acc: 0.05082857
case acc: 0.04708583
case acc: 0.045008652
case acc: 0.04397293
top acc: 0.0650 ::: bot acc: 0.0291
top acc: 0.0621 ::: bot acc: 0.0280
top acc: 0.0886 ::: bot acc: 0.0351
top acc: 0.0839 ::: bot acc: 0.0210
top acc: 0.0905 ::: bot acc: 0.0158
top acc: 0.0798 ::: bot acc: 0.0373
LME_Co_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5388 5388 5388
1.7082474 -0.6288155 0.16982092 -0.17269358
Validation: 600 600 600
Testing: 768 768 768
pre-processing time: 0.00019121170043945312
the split date is 2012-07-01
train dropout: 0.2 test dropout: 0.1
net initializing with time: 0.00228118896484375
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.007762
average val loss: 0.004482, accuracy: 0.0812
average test loss: 0.004316, accuracy: 0.0802
case acc: 0.107333645
case acc: 0.116649754
case acc: 0.075806275
case acc: 0.05500682
case acc: 0.08438708
case acc: 0.04205134
top acc: 0.0563 ::: bot acc: 0.1524
top acc: 0.1540 ::: bot acc: 0.0810
top acc: 0.0301 ::: bot acc: 0.1249
top acc: 0.0247 ::: bot acc: 0.0827
top acc: 0.0377 ::: bot acc: 0.1346
top acc: 0.0337 ::: bot acc: 0.0806
current epoch: 2
train loss is 0.006865
average val loss: 0.006143, accuracy: 0.0826
average test loss: 0.006122, accuracy: 0.0843
case acc: 0.03892575
case acc: 0.22044715
case acc: 0.05617009
case acc: 0.057834446
case acc: 0.042322095
case acc: 0.09015977
top acc: 0.0710 ::: bot acc: 0.0333
top acc: 0.2581 ::: bot acc: 0.1853
top acc: 0.1011 ::: bot acc: 0.0217
top acc: 0.0898 ::: bot acc: 0.0306
top acc: 0.0724 ::: bot acc: 0.0287
top acc: 0.1373 ::: bot acc: 0.0411
current epoch: 3
train loss is 0.008689
average val loss: 0.016913, accuracy: 0.1665
average test loss: 0.016918, accuracy: 0.1674
case acc: 0.1146057
case acc: 0.30527052
case acc: 0.1457222
case acc: 0.15129161
case acc: 0.11598504
case acc: 0.17180675
top acc: 0.1710 ::: bot acc: 0.0672
top acc: 0.3425 ::: bot acc: 0.2696
top acc: 0.1986 ::: bot acc: 0.0928
top acc: 0.1832 ::: bot acc: 0.1234
top acc: 0.1644 ::: bot acc: 0.0646
top acc: 0.2232 ::: bot acc: 0.1114
current epoch: 4
train loss is 0.012249
average val loss: 0.011586, accuracy: 0.1324
average test loss: 0.011599, accuracy: 0.1339
case acc: 0.08651267
case acc: 0.26265955
case acc: 0.115096085
case acc: 0.11694094
case acc: 0.08658055
case acc: 0.13551751
top acc: 0.1422 ::: bot acc: 0.0393
top acc: 0.2998 ::: bot acc: 0.2271
top acc: 0.1683 ::: bot acc: 0.0630
top acc: 0.1496 ::: bot acc: 0.0886
top acc: 0.1335 ::: bot acc: 0.0374
top acc: 0.1860 ::: bot acc: 0.0778
current epoch: 5
train loss is 0.011633
average val loss: 0.002616, accuracy: 0.0577
average test loss: 0.002481, accuracy: 0.0550
case acc: 0.05488583
case acc: 0.12053573
case acc: 0.04164241
case acc: 0.026295632
case acc: 0.046131082
case acc: 0.040676862
top acc: 0.0261 ::: bot acc: 0.0887
top acc: 0.1579 ::: bot acc: 0.0848
top acc: 0.0355 ::: bot acc: 0.0715
top acc: 0.0181 ::: bot acc: 0.0429
top acc: 0.0164 ::: bot acc: 0.0883
top acc: 0.0539 ::: bot acc: 0.0575
current epoch: 6
train loss is 0.004702
average val loss: 0.002344, accuracy: 0.0547
average test loss: 0.002215, accuracy: 0.0525
case acc: 0.053719603
case acc: 0.10842679
case acc: 0.0432595
case acc: 0.027058382
case acc: 0.042194
case acc: 0.04063166
top acc: 0.0263 ::: bot acc: 0.0873
top acc: 0.1463 ::: bot acc: 0.0725
top acc: 0.0317 ::: bot acc: 0.0755
top acc: 0.0173 ::: bot acc: 0.0443
top acc: 0.0217 ::: bot acc: 0.0800
top acc: 0.0540 ::: bot acc: 0.0580
current epoch: 7
train loss is 0.003015
average val loss: 0.003116, accuracy: 0.0596
average test loss: 0.003002, accuracy: 0.0594
case acc: 0.038191214
case acc: 0.14389223
case acc: 0.041662045
case acc: 0.03170062
case acc: 0.041027144
case acc: 0.060222656
top acc: 0.0629 ::: bot acc: 0.0404
top acc: 0.1811 ::: bot acc: 0.1080
top acc: 0.0727 ::: bot acc: 0.0336
top acc: 0.0606 ::: bot acc: 0.0099
top acc: 0.0693 ::: bot acc: 0.0305
top acc: 0.0979 ::: bot acc: 0.0290
current epoch: 8
train loss is 0.003528
average val loss: 0.002689, accuracy: 0.0560
average test loss: 0.002620, accuracy: 0.0558
case acc: 0.038420863
case acc: 0.13090526
case acc: 0.040048853
case acc: 0.027350554
case acc: 0.041031796
case acc: 0.056914736
top acc: 0.0603 ::: bot acc: 0.0435
top acc: 0.1681 ::: bot acc: 0.0953
top acc: 0.0658 ::: bot acc: 0.0409
top acc: 0.0545 ::: bot acc: 0.0102
top acc: 0.0685 ::: bot acc: 0.0322
top acc: 0.0924 ::: bot acc: 0.0305
current epoch: 9
train loss is 0.003259
average val loss: 0.002111, accuracy: 0.0505
average test loss: 0.002033, accuracy: 0.0497
case acc: 0.040310726
case acc: 0.10882478
case acc: 0.039757144
case acc: 0.022686183
case acc: 0.03757845
case acc: 0.049198534
top acc: 0.0488 ::: bot acc: 0.0550
top acc: 0.1458 ::: bot acc: 0.0736
top acc: 0.0518 ::: bot acc: 0.0560
top acc: 0.0403 ::: bot acc: 0.0204
top acc: 0.0593 ::: bot acc: 0.0407
top acc: 0.0779 ::: bot acc: 0.0363
current epoch: 10
train loss is 0.002630
average val loss: 0.002018, accuracy: 0.0496
average test loss: 0.001941, accuracy: 0.0490
case acc: 0.03956782
case acc: 0.10290894
case acc: 0.039345447
case acc: 0.022874206
case acc: 0.03884255
case acc: 0.050304115
top acc: 0.0527 ::: bot acc: 0.0510
top acc: 0.1394 ::: bot acc: 0.0678
top acc: 0.0511 ::: bot acc: 0.0560
top acc: 0.0417 ::: bot acc: 0.0187
top acc: 0.0640 ::: bot acc: 0.0350
top acc: 0.0795 ::: bot acc: 0.0362
current epoch: 11
train loss is 0.002320
average val loss: 0.002056, accuracy: 0.0503
average test loss: 0.002000, accuracy: 0.0498
case acc: 0.03826342
case acc: 0.10195527
case acc: 0.03915607
case acc: 0.024262235
case acc: 0.04267133
case acc: 0.052754194
top acc: 0.0600 ::: bot acc: 0.0431
top acc: 0.1396 ::: bot acc: 0.0655
top acc: 0.0561 ::: bot acc: 0.0515
top acc: 0.0473 ::: bot acc: 0.0138
top acc: 0.0732 ::: bot acc: 0.0286
top acc: 0.0850 ::: bot acc: 0.0328
current epoch: 12
train loss is 0.002249
average val loss: 0.002070, accuracy: 0.0509
average test loss: 0.002007, accuracy: 0.0504
case acc: 0.03834711
case acc: 0.09916578
case acc: 0.039341666
case acc: 0.02575763
case acc: 0.045077942
case acc: 0.05462555
top acc: 0.0658 ::: bot acc: 0.0378
top acc: 0.1358 ::: bot acc: 0.0638
top acc: 0.0588 ::: bot acc: 0.0485
top acc: 0.0511 ::: bot acc: 0.0114
top acc: 0.0778 ::: bot acc: 0.0257
top acc: 0.0883 ::: bot acc: 0.0313
current epoch: 13
train loss is 0.002174
average val loss: 0.001936, accuracy: 0.0494
average test loss: 0.001868, accuracy: 0.0488
case acc: 0.038211666
case acc: 0.09131448
case acc: 0.03936851
case acc: 0.025166238
case acc: 0.04467345
case acc: 0.05383504
top acc: 0.0660 ::: bot acc: 0.0374
top acc: 0.1281 ::: bot acc: 0.0558
top acc: 0.0570 ::: bot acc: 0.0507
top acc: 0.0499 ::: bot acc: 0.0119
top acc: 0.0776 ::: bot acc: 0.0251
top acc: 0.0866 ::: bot acc: 0.0322
current epoch: 14
train loss is 0.001985
average val loss: 0.001819, accuracy: 0.0482
average test loss: 0.001772, accuracy: 0.0477
case acc: 0.03813692
case acc: 0.0850075
case acc: 0.039634407
case acc: 0.024867054
case acc: 0.04509334
case acc: 0.05339454
top acc: 0.0665 ::: bot acc: 0.0364
top acc: 0.1219 ::: bot acc: 0.0495
top acc: 0.0564 ::: bot acc: 0.0517
top acc: 0.0488 ::: bot acc: 0.0123
top acc: 0.0776 ::: bot acc: 0.0260
top acc: 0.0857 ::: bot acc: 0.0327
current epoch: 15
train loss is 0.001842
average val loss: 0.001729, accuracy: 0.0471
average test loss: 0.001681, accuracy: 0.0465
case acc: 0.03830254
case acc: 0.07832573
case acc: 0.03952961
case acc: 0.025020055
case acc: 0.04487869
case acc: 0.052941546
top acc: 0.0676 ::: bot acc: 0.0360
top acc: 0.1157 ::: bot acc: 0.0427
top acc: 0.0552 ::: bot acc: 0.0528
top acc: 0.0490 ::: bot acc: 0.0128
top acc: 0.0776 ::: bot acc: 0.0255
top acc: 0.0851 ::: bot acc: 0.0329
current epoch: 16
train loss is 0.001739
average val loss: 0.001707, accuracy: 0.0470
average test loss: 0.001675, accuracy: 0.0466
case acc: 0.038532164
case acc: 0.07539101
case acc: 0.039348733
case acc: 0.025988534
case acc: 0.046634525
case acc: 0.053539462
top acc: 0.0711 ::: bot acc: 0.0323
top acc: 0.1121 ::: bot acc: 0.0400
top acc: 0.0565 ::: bot acc: 0.0509
top acc: 0.0514 ::: bot acc: 0.0111
top acc: 0.0800 ::: bot acc: 0.0260
top acc: 0.0864 ::: bot acc: 0.0317
current epoch: 17
train loss is 0.001667
average val loss: 0.001746, accuracy: 0.0476
average test loss: 0.001707, accuracy: 0.0471
case acc: 0.039140053
case acc: 0.07363927
case acc: 0.03935659
case acc: 0.02775535
case acc: 0.04794585
case acc: 0.05482544
top acc: 0.0761 ::: bot acc: 0.0277
top acc: 0.1101 ::: bot acc: 0.0385
top acc: 0.0600 ::: bot acc: 0.0472
top acc: 0.0546 ::: bot acc: 0.0101
top acc: 0.0827 ::: bot acc: 0.0245
top acc: 0.0888 ::: bot acc: 0.0309
current epoch: 18
train loss is 0.001635
average val loss: 0.001708, accuracy: 0.0472
average test loss: 0.001675, accuracy: 0.0466
case acc: 0.039083786
case acc: 0.07019646
case acc: 0.039500758
case acc: 0.028068475
case acc: 0.047763363
case acc: 0.054768257
top acc: 0.0770 ::: bot acc: 0.0252
top acc: 0.1071 ::: bot acc: 0.0349
top acc: 0.0610 ::: bot acc: 0.0464
top acc: 0.0552 ::: bot acc: 0.0096
top acc: 0.0829 ::: bot acc: 0.0238
top acc: 0.0891 ::: bot acc: 0.0306
current epoch: 19
train loss is 0.001539
average val loss: 0.001696, accuracy: 0.0471
average test loss: 0.001666, accuracy: 0.0465
case acc: 0.039856203
case acc: 0.06724116
case acc: 0.03949684
case acc: 0.028797869
case acc: 0.048341345
case acc: 0.05499043
top acc: 0.0802 ::: bot acc: 0.0233
top acc: 0.1049 ::: bot acc: 0.0317
top acc: 0.0621 ::: bot acc: 0.0451
top acc: 0.0565 ::: bot acc: 0.0092
top acc: 0.0839 ::: bot acc: 0.0236
top acc: 0.0891 ::: bot acc: 0.0309
current epoch: 20
train loss is 0.001489
average val loss: 0.001714, accuracy: 0.0473
average test loss: 0.001697, accuracy: 0.0470
case acc: 0.040681556
case acc: 0.06538418
case acc: 0.03992419
case acc: 0.030647501
case acc: 0.04904752
case acc: 0.056073744
top acc: 0.0834 ::: bot acc: 0.0201
top acc: 0.1022 ::: bot acc: 0.0299
top acc: 0.0651 ::: bot acc: 0.0425
top acc: 0.0597 ::: bot acc: 0.0094
top acc: 0.0849 ::: bot acc: 0.0235
top acc: 0.0907 ::: bot acc: 0.0308
current epoch: 21
train loss is 0.001452
average val loss: 0.001651, accuracy: 0.0466
average test loss: 0.001624, accuracy: 0.0459
case acc: 0.040469483
case acc: 0.0613331
case acc: 0.040097356
case acc: 0.0304488
case acc: 0.047800403
case acc: 0.054979604
top acc: 0.0829 ::: bot acc: 0.0202
top acc: 0.0976 ::: bot acc: 0.0264
top acc: 0.0643 ::: bot acc: 0.0433
top acc: 0.0591 ::: bot acc: 0.0094
top acc: 0.0829 ::: bot acc: 0.0236
top acc: 0.0893 ::: bot acc: 0.0309
current epoch: 22
train loss is 0.001393
average val loss: 0.001576, accuracy: 0.0455
average test loss: 0.001553, accuracy: 0.0446
case acc: 0.04047983
case acc: 0.056938693
case acc: 0.039343495
case acc: 0.029617535
case acc: 0.047070052
case acc: 0.053897187
top acc: 0.0832 ::: bot acc: 0.0206
top acc: 0.0935 ::: bot acc: 0.0217
top acc: 0.0637 ::: bot acc: 0.0430
top acc: 0.0580 ::: bot acc: 0.0093
top acc: 0.0816 ::: bot acc: 0.0244
top acc: 0.0869 ::: bot acc: 0.0317
current epoch: 23
train loss is 0.001334
average val loss: 0.001610, accuracy: 0.0460
average test loss: 0.001589, accuracy: 0.0453
case acc: 0.04104827
case acc: 0.056620896
case acc: 0.03994707
case acc: 0.031589426
case acc: 0.04774107
case acc: 0.054625664
top acc: 0.0858 ::: bot acc: 0.0169
top acc: 0.0930 ::: bot acc: 0.0218
top acc: 0.0662 ::: bot acc: 0.0406
top acc: 0.0605 ::: bot acc: 0.0101
top acc: 0.0819 ::: bot acc: 0.0246
top acc: 0.0885 ::: bot acc: 0.0309
current epoch: 24
train loss is 0.001328
average val loss: 0.001681, accuracy: 0.0472
average test loss: 0.001663, accuracy: 0.0465
case acc: 0.04263999
case acc: 0.056959286
case acc: 0.04067284
case acc: 0.033872526
case acc: 0.04875151
case acc: 0.05582361
top acc: 0.0894 ::: bot acc: 0.0141
top acc: 0.0935 ::: bot acc: 0.0218
top acc: 0.0703 ::: bot acc: 0.0364
top acc: 0.0632 ::: bot acc: 0.0109
top acc: 0.0840 ::: bot acc: 0.0234
top acc: 0.0911 ::: bot acc: 0.0303
current epoch: 25
train loss is 0.001341
average val loss: 0.001698, accuracy: 0.0474
average test loss: 0.001693, accuracy: 0.0469
case acc: 0.043299757
case acc: 0.056268387
case acc: 0.041361794
case acc: 0.03532792
case acc: 0.049393054
case acc: 0.055978157
top acc: 0.0911 ::: bot acc: 0.0123
top acc: 0.0929 ::: bot acc: 0.0211
top acc: 0.0721 ::: bot acc: 0.0350
top acc: 0.0657 ::: bot acc: 0.0115
top acc: 0.0852 ::: bot acc: 0.0236
top acc: 0.0915 ::: bot acc: 0.0299
current epoch: 26
train loss is 0.001325
average val loss: 0.001751, accuracy: 0.0483
average test loss: 0.001733, accuracy: 0.0476
case acc: 0.044642746
case acc: 0.055893432
case acc: 0.042038422
case acc: 0.03700687
case acc: 0.04963818
case acc: 0.056671962
top acc: 0.0938 ::: bot acc: 0.0104
top acc: 0.0927 ::: bot acc: 0.0209
top acc: 0.0748 ::: bot acc: 0.0315
top acc: 0.0673 ::: bot acc: 0.0123
top acc: 0.0855 ::: bot acc: 0.0230
top acc: 0.0921 ::: bot acc: 0.0303
current epoch: 27
train loss is 0.001328
average val loss: 0.001772, accuracy: 0.0486
average test loss: 0.001753, accuracy: 0.0480
case acc: 0.045931444
case acc: 0.054947454
case acc: 0.04276188
case acc: 0.038033016
case acc: 0.04987486
case acc: 0.056654364
top acc: 0.0958 ::: bot acc: 0.0107
top acc: 0.0916 ::: bot acc: 0.0203
top acc: 0.0771 ::: bot acc: 0.0298
top acc: 0.0686 ::: bot acc: 0.0131
top acc: 0.0856 ::: bot acc: 0.0229
top acc: 0.0921 ::: bot acc: 0.0300
current epoch: 28
train loss is 0.001328
average val loss: 0.001855, accuracy: 0.0498
average test loss: 0.001850, accuracy: 0.0496
case acc: 0.04764474
case acc: 0.05599223
case acc: 0.044328175
case acc: 0.04050396
case acc: 0.05151681
case acc: 0.0577505
top acc: 0.0987 ::: bot acc: 0.0099
top acc: 0.0928 ::: bot acc: 0.0207
top acc: 0.0809 ::: bot acc: 0.0266
top acc: 0.0713 ::: bot acc: 0.0148
top acc: 0.0885 ::: bot acc: 0.0225
top acc: 0.0941 ::: bot acc: 0.0293
current epoch: 29
train loss is 0.001309
average val loss: 0.001879, accuracy: 0.0503
average test loss: 0.001858, accuracy: 0.0498
case acc: 0.048649438
case acc: 0.05496728
case acc: 0.044982962
case acc: 0.041359834
case acc: 0.051329702
case acc: 0.057779834
top acc: 0.1000 ::: bot acc: 0.0102
top acc: 0.0916 ::: bot acc: 0.0201
top acc: 0.0822 ::: bot acc: 0.0260
top acc: 0.0720 ::: bot acc: 0.0154
top acc: 0.0884 ::: bot acc: 0.0226
top acc: 0.0940 ::: bot acc: 0.0296
current epoch: 30
train loss is 0.001294
average val loss: 0.001900, accuracy: 0.0505
average test loss: 0.001886, accuracy: 0.0503
case acc: 0.04940425
case acc: 0.054351278
case acc: 0.04576567
case acc: 0.042232357
case acc: 0.052058876
case acc: 0.057749867
top acc: 0.1012 ::: bot acc: 0.0099
top acc: 0.0908 ::: bot acc: 0.0200
top acc: 0.0842 ::: bot acc: 0.0250
top acc: 0.0732 ::: bot acc: 0.0159
top acc: 0.0900 ::: bot acc: 0.0221
top acc: 0.0942 ::: bot acc: 0.0291
current epoch: 31
train loss is 0.001297
average val loss: 0.001835, accuracy: 0.0497
average test loss: 0.001819, accuracy: 0.0492
case acc: 0.048614927
case acc: 0.051773913
case acc: 0.045137107
case acc: 0.04141332
case acc: 0.051379487
case acc: 0.056696594
top acc: 0.1005 ::: bot acc: 0.0098
top acc: 0.0881 ::: bot acc: 0.0179
top acc: 0.0827 ::: bot acc: 0.0258
top acc: 0.0719 ::: bot acc: 0.0156
top acc: 0.0888 ::: bot acc: 0.0223
top acc: 0.0925 ::: bot acc: 0.0299
current epoch: 32
train loss is 0.001257
average val loss: 0.001785, accuracy: 0.0490
average test loss: 0.001755, accuracy: 0.0481
case acc: 0.04801886
case acc: 0.049073957
case acc: 0.045132015
case acc: 0.03995498
case acc: 0.05047862
case acc: 0.055804037
top acc: 0.0991 ::: bot acc: 0.0102
top acc: 0.0846 ::: bot acc: 0.0158
top acc: 0.0823 ::: bot acc: 0.0265
top acc: 0.0708 ::: bot acc: 0.0146
top acc: 0.0872 ::: bot acc: 0.0225
top acc: 0.0906 ::: bot acc: 0.0305
current epoch: 33
train loss is 0.001216
average val loss: 0.001696, accuracy: 0.0476
average test loss: 0.001681, accuracy: 0.0468
case acc: 0.046953462
case acc: 0.04627411
case acc: 0.044373054
case acc: 0.03868547
case acc: 0.049614776
case acc: 0.054900087
top acc: 0.0980 ::: bot acc: 0.0100
top acc: 0.0813 ::: bot acc: 0.0146
top acc: 0.0805 ::: bot acc: 0.0277
top acc: 0.0696 ::: bot acc: 0.0132
top acc: 0.0856 ::: bot acc: 0.0230
top acc: 0.0888 ::: bot acc: 0.0312
current epoch: 34
train loss is 0.001186
average val loss: 0.001685, accuracy: 0.0476
average test loss: 0.001672, accuracy: 0.0467
case acc: 0.04687564
case acc: 0.045245975
case acc: 0.04450912
case acc: 0.039129525
case acc: 0.04963635
case acc: 0.054549903
top acc: 0.0974 ::: bot acc: 0.0102
top acc: 0.0802 ::: bot acc: 0.0140
top acc: 0.0810 ::: bot acc: 0.0273
top acc: 0.0697 ::: bot acc: 0.0139
top acc: 0.0854 ::: bot acc: 0.0231
top acc: 0.0889 ::: bot acc: 0.0311
current epoch: 35
train loss is 0.001159
average val loss: 0.001623, accuracy: 0.0466
average test loss: 0.001603, accuracy: 0.0455
case acc: 0.045888714
case acc: 0.042603165
case acc: 0.04393015
case acc: 0.038120873
case acc: 0.048659846
case acc: 0.05381617
top acc: 0.0956 ::: bot acc: 0.0106
top acc: 0.0771 ::: bot acc: 0.0125
top acc: 0.0797 ::: bot acc: 0.0285
top acc: 0.0687 ::: bot acc: 0.0132
top acc: 0.0839 ::: bot acc: 0.0237
top acc: 0.0869 ::: bot acc: 0.0324
current epoch: 36
train loss is 0.001140
average val loss: 0.001603, accuracy: 0.0463
average test loss: 0.001582, accuracy: 0.0451
case acc: 0.04566922
case acc: 0.04197736
case acc: 0.043670014
case acc: 0.037891258
case acc: 0.04819089
case acc: 0.05337361
top acc: 0.0955 ::: bot acc: 0.0106
top acc: 0.0759 ::: bot acc: 0.0128
top acc: 0.0796 ::: bot acc: 0.0280
top acc: 0.0681 ::: bot acc: 0.0130
top acc: 0.0830 ::: bot acc: 0.0239
top acc: 0.0867 ::: bot acc: 0.0322
current epoch: 37
train loss is 0.001129
average val loss: 0.001597, accuracy: 0.0462
average test loss: 0.001574, accuracy: 0.0450
case acc: 0.045707256
case acc: 0.041253053
case acc: 0.043874163
case acc: 0.038135607
case acc: 0.04796166
case acc: 0.053263374
top acc: 0.0954 ::: bot acc: 0.0105
top acc: 0.0745 ::: bot acc: 0.0125
top acc: 0.0796 ::: bot acc: 0.0281
top acc: 0.0685 ::: bot acc: 0.0134
top acc: 0.0827 ::: bot acc: 0.0243
top acc: 0.0860 ::: bot acc: 0.0324
current epoch: 38
train loss is 0.001111
average val loss: 0.001630, accuracy: 0.0466
average test loss: 0.001605, accuracy: 0.0456
case acc: 0.046019915
case acc: 0.041649543
case acc: 0.04433745
case acc: 0.039094873
case acc: 0.048563153
case acc: 0.05365701
top acc: 0.0962 ::: bot acc: 0.0102
top acc: 0.0755 ::: bot acc: 0.0128
top acc: 0.0808 ::: bot acc: 0.0270
top acc: 0.0701 ::: bot acc: 0.0137
top acc: 0.0833 ::: bot acc: 0.0244
top acc: 0.0867 ::: bot acc: 0.0322
current epoch: 39
train loss is 0.001109
average val loss: 0.001665, accuracy: 0.0473
average test loss: 0.001634, accuracy: 0.0461
case acc: 0.04666692
case acc: 0.041563712
case acc: 0.04507617
case acc: 0.04028515
case acc: 0.04896755
case acc: 0.05383514
top acc: 0.0972 ::: bot acc: 0.0101
top acc: 0.0751 ::: bot acc: 0.0126
top acc: 0.0827 ::: bot acc: 0.0257
top acc: 0.0709 ::: bot acc: 0.0146
top acc: 0.0844 ::: bot acc: 0.0235
top acc: 0.0872 ::: bot acc: 0.0316
current epoch: 40
train loss is 0.001116
average val loss: 0.001657, accuracy: 0.0471
average test loss: 0.001625, accuracy: 0.0460
case acc: 0.046656106
case acc: 0.040967677
case acc: 0.04522982
case acc: 0.040096026
case acc: 0.04898775
case acc: 0.053770624
top acc: 0.0973 ::: bot acc: 0.0100
top acc: 0.0739 ::: bot acc: 0.0128
top acc: 0.0827 ::: bot acc: 0.0259
top acc: 0.0711 ::: bot acc: 0.0144
top acc: 0.0841 ::: bot acc: 0.0236
top acc: 0.0869 ::: bot acc: 0.0321
current epoch: 41
train loss is 0.001098
average val loss: 0.001621, accuracy: 0.0465
average test loss: 0.001593, accuracy: 0.0454
case acc: 0.04613497
case acc: 0.04014849
case acc: 0.044821125
case acc: 0.039595425
case acc: 0.048265167
case acc: 0.053154096
top acc: 0.0964 ::: bot acc: 0.0103
top acc: 0.0730 ::: bot acc: 0.0125
top acc: 0.0819 ::: bot acc: 0.0263
top acc: 0.0702 ::: bot acc: 0.0141
top acc: 0.0835 ::: bot acc: 0.0237
top acc: 0.0857 ::: bot acc: 0.0326
current epoch: 42
train loss is 0.001092
average val loss: 0.001639, accuracy: 0.0467
average test loss: 0.001606, accuracy: 0.0456
case acc: 0.046214316
case acc: 0.040097233
case acc: 0.045431104
case acc: 0.039935097
case acc: 0.04847742
case acc: 0.05324082
top acc: 0.0969 ::: bot acc: 0.0101
top acc: 0.0729 ::: bot acc: 0.0125
top acc: 0.0829 ::: bot acc: 0.0256
top acc: 0.0706 ::: bot acc: 0.0143
top acc: 0.0835 ::: bot acc: 0.0236
top acc: 0.0863 ::: bot acc: 0.0320
current epoch: 43
train loss is 0.001085
average val loss: 0.001599, accuracy: 0.0461
average test loss: 0.001562, accuracy: 0.0448
case acc: 0.045586586
case acc: 0.038622264
case acc: 0.045061506
case acc: 0.039342545
case acc: 0.048016444
case acc: 0.052446395
top acc: 0.0952 ::: bot acc: 0.0112
top acc: 0.0711 ::: bot acc: 0.0123
top acc: 0.0824 ::: bot acc: 0.0264
top acc: 0.0699 ::: bot acc: 0.0138
top acc: 0.0828 ::: bot acc: 0.0241
top acc: 0.0848 ::: bot acc: 0.0328
current epoch: 44
train loss is 0.001060
average val loss: 0.001544, accuracy: 0.0452
average test loss: 0.001510, accuracy: 0.0439
case acc: 0.044370793
case acc: 0.037488915
case acc: 0.044444017
case acc: 0.038269684
case acc: 0.047058158
case acc: 0.0518274
top acc: 0.0932 ::: bot acc: 0.0116
top acc: 0.0695 ::: bot acc: 0.0123
top acc: 0.0811 ::: bot acc: 0.0270
top acc: 0.0691 ::: bot acc: 0.0128
top acc: 0.0812 ::: bot acc: 0.0246
top acc: 0.0833 ::: bot acc: 0.0338
current epoch: 45
train loss is 0.001050
average val loss: 0.001550, accuracy: 0.0454
average test loss: 0.001515, accuracy: 0.0440
case acc: 0.044473555
case acc: 0.037451126
case acc: 0.04461702
case acc: 0.038525455
case acc: 0.04703574
case acc: 0.051798575
top acc: 0.0934 ::: bot acc: 0.0115
top acc: 0.0692 ::: bot acc: 0.0126
top acc: 0.0812 ::: bot acc: 0.0273
top acc: 0.0693 ::: bot acc: 0.0132
top acc: 0.0809 ::: bot acc: 0.0246
top acc: 0.0837 ::: bot acc: 0.0332
current epoch: 46
train loss is 0.001048
average val loss: 0.001572, accuracy: 0.0458
average test loss: 0.001535, accuracy: 0.0444
case acc: 0.044791736
case acc: 0.037574973
case acc: 0.04478766
case acc: 0.03936766
case acc: 0.047463372
case acc: 0.052385643
top acc: 0.0938 ::: bot acc: 0.0114
top acc: 0.0692 ::: bot acc: 0.0126
top acc: 0.0817 ::: bot acc: 0.0258
top acc: 0.0703 ::: bot acc: 0.0140
top acc: 0.0819 ::: bot acc: 0.0245
top acc: 0.0841 ::: bot acc: 0.0335
current epoch: 47
train loss is 0.001048
average val loss: 0.001587, accuracy: 0.0460
average test loss: 0.001548, accuracy: 0.0446
case acc: 0.04499358
case acc: 0.037518647
case acc: 0.045241512
case acc: 0.0397932
case acc: 0.04791791
case acc: 0.05236762
top acc: 0.0943 ::: bot acc: 0.0114
top acc: 0.0691 ::: bot acc: 0.0125
top acc: 0.0824 ::: bot acc: 0.0256
top acc: 0.0706 ::: bot acc: 0.0144
top acc: 0.0827 ::: bot acc: 0.0240
top acc: 0.0844 ::: bot acc: 0.0332
current epoch: 48
train loss is 0.001041
average val loss: 0.001566, accuracy: 0.0456
average test loss: 0.001530, accuracy: 0.0443
case acc: 0.044504322
case acc: 0.036986824
case acc: 0.045061313
case acc: 0.03942384
case acc: 0.04771784
case acc: 0.05206589
top acc: 0.0935 ::: bot acc: 0.0113
top acc: 0.0682 ::: bot acc: 0.0125
top acc: 0.0823 ::: bot acc: 0.0260
top acc: 0.0702 ::: bot acc: 0.0138
top acc: 0.0822 ::: bot acc: 0.0243
top acc: 0.0838 ::: bot acc: 0.0334
current epoch: 49
train loss is 0.001037
average val loss: 0.001557, accuracy: 0.0455
average test loss: 0.001518, accuracy: 0.0441
case acc: 0.044163942
case acc: 0.03660731
case acc: 0.045070685
case acc: 0.039235976
case acc: 0.04748101
case acc: 0.051782172
top acc: 0.0929 ::: bot acc: 0.0114
top acc: 0.0679 ::: bot acc: 0.0124
top acc: 0.0824 ::: bot acc: 0.0261
top acc: 0.0695 ::: bot acc: 0.0139
top acc: 0.0819 ::: bot acc: 0.0242
top acc: 0.0833 ::: bot acc: 0.0339
current epoch: 50
train loss is 0.001031
average val loss: 0.001548, accuracy: 0.0453
average test loss: 0.001506, accuracy: 0.0438
case acc: 0.043866172
case acc: 0.03630895
case acc: 0.044854254
case acc: 0.03897524
case acc: 0.047583543
case acc: 0.05148965
top acc: 0.0927 ::: bot acc: 0.0115
top acc: 0.0671 ::: bot acc: 0.0131
top acc: 0.0824 ::: bot acc: 0.0253
top acc: 0.0696 ::: bot acc: 0.0134
top acc: 0.0824 ::: bot acc: 0.0243
top acc: 0.0827 ::: bot acc: 0.0340

		{"drop_out": 0.2, "drop_out_mc": 0.15, "repeat_mc": 50, "hidden": 30, "embedding_size": 5, "batch": 512, "lag": 5}
LME_Co_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5370 5370 5370
1.7082474 -0.6288155 0.24786325 -0.22413088
Validation: 600 600 600
Testing: 774 774 774
pre-processing time: 0.00024127960205078125
the split date is 2010-07-01
train dropout: 0.2 test dropout: 0.15
net initializing with time: 0.0037126541137695312
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.007855
average val loss: 0.004988, accuracy: 0.0900
average test loss: 0.006258, accuracy: 0.1038
case acc: 0.14586575
case acc: 0.08916182
case acc: 0.107934155
case acc: 0.09539759
case acc: 0.1213258
case acc: 0.063085236
top acc: 0.1252 ::: bot acc: 0.1677
top acc: 0.1312 ::: bot acc: 0.0442
top acc: 0.0630 ::: bot acc: 0.1589
top acc: 0.0624 ::: bot acc: 0.1289
top acc: 0.0882 ::: bot acc: 0.1543
top acc: 0.0340 ::: bot acc: 0.0968
current epoch: 2
train loss is 0.006721
average val loss: 0.004622, accuracy: 0.0665
average test loss: 0.003775, accuracy: 0.0592
case acc: 0.030307604
case acc: 0.19028923
case acc: 0.03681574
case acc: 0.027589051
case acc: 0.027106274
case acc: 0.043027703
top acc: 0.0117 ::: bot acc: 0.0514
top acc: 0.2325 ::: bot acc: 0.1471
top acc: 0.0528 ::: bot acc: 0.0430
top acc: 0.0472 ::: bot acc: 0.0195
top acc: 0.0173 ::: bot acc: 0.0480
top acc: 0.0671 ::: bot acc: 0.0173
current epoch: 3
train loss is 0.008213
average val loss: 0.013901, accuracy: 0.1488
average test loss: 0.011234, accuracy: 0.1300
case acc: 0.07465492
case acc: 0.27794448
case acc: 0.11046494
case acc: 0.11013902
case acc: 0.07919436
case acc: 0.12762895
top acc: 0.0945 ::: bot acc: 0.0553
top acc: 0.3197 ::: bot acc: 0.2351
top acc: 0.1551 ::: bot acc: 0.0608
top acc: 0.1428 ::: bot acc: 0.0761
top acc: 0.1127 ::: bot acc: 0.0464
top acc: 0.1566 ::: bot acc: 0.0929
current epoch: 4
train loss is 0.011868
average val loss: 0.014811, accuracy: 0.1565
average test loss: 0.011996, accuracy: 0.1377
case acc: 0.088036634
case acc: 0.27758032
case acc: 0.12140432
case acc: 0.11804977
case acc: 0.090111606
case acc: 0.13091806
top acc: 0.1085 ::: bot acc: 0.0667
top acc: 0.3196 ::: bot acc: 0.2342
top acc: 0.1677 ::: bot acc: 0.0706
top acc: 0.1513 ::: bot acc: 0.0842
top acc: 0.1248 ::: bot acc: 0.0573
top acc: 0.1598 ::: bot acc: 0.0962
current epoch: 5
train loss is 0.013254
average val loss: 0.002597, accuracy: 0.0495
average test loss: 0.002316, accuracy: 0.0504
case acc: 0.044095904
case acc: 0.13195443
case acc: 0.03545613
case acc: 0.02817992
case acc: 0.038946964
case acc: 0.023961578
top acc: 0.0237 ::: bot acc: 0.0667
top acc: 0.1740 ::: bot acc: 0.0893
top acc: 0.0306 ::: bot acc: 0.0643
top acc: 0.0167 ::: bot acc: 0.0519
top acc: 0.0093 ::: bot acc: 0.0708
top acc: 0.0243 ::: bot acc: 0.0406
current epoch: 6
train loss is 0.006426
average val loss: 0.002200, accuracy: 0.0559
average test loss: 0.002680, accuracy: 0.0647
case acc: 0.08035996
case acc: 0.081506535
case acc: 0.0600818
case acc: 0.056559082
case acc: 0.06630352
case acc: 0.04338503
top acc: 0.0602 ::: bot acc: 0.1022
top acc: 0.1232 ::: bot acc: 0.0384
top acc: 0.0222 ::: bot acc: 0.1065
top acc: 0.0242 ::: bot acc: 0.0904
top acc: 0.0319 ::: bot acc: 0.1003
top acc: 0.0157 ::: bot acc: 0.0776
current epoch: 7
train loss is 0.003244
average val loss: 0.002289, accuracy: 0.0467
average test loss: 0.001851, accuracy: 0.0441
case acc: 0.0304495
case acc: 0.11996761
case acc: 0.03528056
case acc: 0.025783584
case acc: 0.027397623
case acc: 0.025761345
top acc: 0.0116 ::: bot acc: 0.0520
top acc: 0.1620 ::: bot acc: 0.0767
top acc: 0.0339 ::: bot acc: 0.0623
top acc: 0.0242 ::: bot acc: 0.0425
top acc: 0.0217 ::: bot acc: 0.0460
top acc: 0.0349 ::: bot acc: 0.0307
current epoch: 8
train loss is 0.003580
average val loss: 0.002226, accuracy: 0.0471
average test loss: 0.001753, accuracy: 0.0424
case acc: 0.024790974
case acc: 0.11765713
case acc: 0.0348518
case acc: 0.025129221
case acc: 0.025021875
case acc: 0.026831247
top acc: 0.0080 ::: bot acc: 0.0450
top acc: 0.1593 ::: bot acc: 0.0746
top acc: 0.0367 ::: bot acc: 0.0588
top acc: 0.0283 ::: bot acc: 0.0382
top acc: 0.0303 ::: bot acc: 0.0366
top acc: 0.0400 ::: bot acc: 0.0252
current epoch: 9
train loss is 0.003601
average val loss: 0.001722, accuracy: 0.0414
average test loss: 0.001497, accuracy: 0.0413
case acc: 0.034084354
case acc: 0.0967868
case acc: 0.037913978
case acc: 0.027631963
case acc: 0.027818233
case acc: 0.023617571
top acc: 0.0141 ::: bot acc: 0.0551
top acc: 0.1391 ::: bot acc: 0.0538
top acc: 0.0235 ::: bot acc: 0.0725
top acc: 0.0161 ::: bot acc: 0.0512
top acc: 0.0239 ::: bot acc: 0.0463
top acc: 0.0258 ::: bot acc: 0.0378
current epoch: 10
train loss is 0.003046
average val loss: 0.001526, accuracy: 0.0395
average test loss: 0.001366, accuracy: 0.0400
case acc: 0.03444778
case acc: 0.087132394
case acc: 0.039504766
case acc: 0.028818239
case acc: 0.02650919
case acc: 0.023589956
top acc: 0.0150 ::: bot acc: 0.0563
top acc: 0.1284 ::: bot acc: 0.0443
top acc: 0.0205 ::: bot acc: 0.0762
top acc: 0.0148 ::: bot acc: 0.0529
top acc: 0.0238 ::: bot acc: 0.0439
top acc: 0.0240 ::: bot acc: 0.0396
current epoch: 11
train loss is 0.002749
average val loss: 0.001484, accuracy: 0.0394
average test loss: 0.001278, accuracy: 0.0383
case acc: 0.0293968
case acc: 0.08440007
case acc: 0.038544744
case acc: 0.027620751
case acc: 0.025451498
case acc: 0.024386238
top acc: 0.0115 ::: bot acc: 0.0501
top acc: 0.1268 ::: bot acc: 0.0408
top acc: 0.0229 ::: bot acc: 0.0743
top acc: 0.0192 ::: bot acc: 0.0494
top acc: 0.0305 ::: bot acc: 0.0370
top acc: 0.0291 ::: bot acc: 0.0359
current epoch: 12
train loss is 0.002594
average val loss: 0.001484, accuracy: 0.0401
average test loss: 0.001190, accuracy: 0.0367
case acc: 0.023828657
case acc: 0.08189176
case acc: 0.036779262
case acc: 0.026539797
case acc: 0.025390347
case acc: 0.025486538
top acc: 0.0077 ::: bot acc: 0.0435
top acc: 0.1236 ::: bot acc: 0.0389
top acc: 0.0253 ::: bot acc: 0.0696
top acc: 0.0231 ::: bot acc: 0.0452
top acc: 0.0370 ::: bot acc: 0.0308
top acc: 0.0331 ::: bot acc: 0.0323
current epoch: 13
train loss is 0.002509
average val loss: 0.001395, accuracy: 0.0393
average test loss: 0.001112, accuracy: 0.0356
case acc: 0.022595618
case acc: 0.07643423
case acc: 0.03694702
case acc: 0.026243435
case acc: 0.025686445
case acc: 0.025574801
top acc: 0.0074 ::: bot acc: 0.0420
top acc: 0.1188 ::: bot acc: 0.0321
top acc: 0.0248 ::: bot acc: 0.0698
top acc: 0.0235 ::: bot acc: 0.0442
top acc: 0.0385 ::: bot acc: 0.0297
top acc: 0.0333 ::: bot acc: 0.0322
current epoch: 14
train loss is 0.002345
average val loss: 0.001228, accuracy: 0.0372
average test loss: 0.001029, accuracy: 0.0346
case acc: 0.023786312
case acc: 0.06790154
case acc: 0.038234968
case acc: 0.02667297
case acc: 0.025537591
case acc: 0.025215264
top acc: 0.0073 ::: bot acc: 0.0439
top acc: 0.1097 ::: bot acc: 0.0250
top acc: 0.0219 ::: bot acc: 0.0740
top acc: 0.0196 ::: bot acc: 0.0470
top acc: 0.0365 ::: bot acc: 0.0310
top acc: 0.0309 ::: bot acc: 0.0354
current epoch: 15
train loss is 0.002178
average val loss: 0.001204, accuracy: 0.0372
average test loss: 0.000970, accuracy: 0.0335
case acc: 0.021715788
case acc: 0.06407624
case acc: 0.03799489
case acc: 0.026347404
case acc: 0.02568647
case acc: 0.024895912
top acc: 0.0073 ::: bot acc: 0.0402
top acc: 0.1059 ::: bot acc: 0.0210
top acc: 0.0229 ::: bot acc: 0.0728
top acc: 0.0227 ::: bot acc: 0.0449
top acc: 0.0391 ::: bot acc: 0.0291
top acc: 0.0313 ::: bot acc: 0.0332
current epoch: 16
train loss is 0.002132
average val loss: 0.001160, accuracy: 0.0366
average test loss: 0.000913, accuracy: 0.0324
case acc: 0.020454846
case acc: 0.05991663
case acc: 0.037757006
case acc: 0.025632162
case acc: 0.025758509
case acc: 0.024778686
top acc: 0.0073 ::: bot acc: 0.0393
top acc: 0.1017 ::: bot acc: 0.0176
top acc: 0.0232 ::: bot acc: 0.0726
top acc: 0.0233 ::: bot acc: 0.0429
top acc: 0.0398 ::: bot acc: 0.0283
top acc: 0.0320 ::: bot acc: 0.0323
current epoch: 17
train loss is 0.001993
average val loss: 0.001077, accuracy: 0.0353
average test loss: 0.000862, accuracy: 0.0315
case acc: 0.020160582
case acc: 0.055360876
case acc: 0.038186777
case acc: 0.02564289
case acc: 0.025464177
case acc: 0.024439886
top acc: 0.0075 ::: bot acc: 0.0380
top acc: 0.0962 ::: bot acc: 0.0140
top acc: 0.0230 ::: bot acc: 0.0730
top acc: 0.0223 ::: bot acc: 0.0437
top acc: 0.0392 ::: bot acc: 0.0283
top acc: 0.0306 ::: bot acc: 0.0333
current epoch: 18
train loss is 0.001955
average val loss: 0.001050, accuracy: 0.0351
average test loss: 0.000826, accuracy: 0.0308
case acc: 0.01884525
case acc: 0.05271369
case acc: 0.03789783
case acc: 0.02569348
case acc: 0.025304679
case acc: 0.024525147
top acc: 0.0083 ::: bot acc: 0.0357
top acc: 0.0929 ::: bot acc: 0.0128
top acc: 0.0240 ::: bot acc: 0.0720
top acc: 0.0238 ::: bot acc: 0.0434
top acc: 0.0398 ::: bot acc: 0.0278
top acc: 0.0318 ::: bot acc: 0.0330
current epoch: 19
train loss is 0.001906
average val loss: 0.001049, accuracy: 0.0353
average test loss: 0.000798, accuracy: 0.0304
case acc: 0.017795563
case acc: 0.051300123
case acc: 0.036856506
case acc: 0.025361268
case acc: 0.026081033
case acc: 0.024916882
top acc: 0.0103 ::: bot acc: 0.0326
top acc: 0.0913 ::: bot acc: 0.0118
top acc: 0.0258 ::: bot acc: 0.0691
top acc: 0.0258 ::: bot acc: 0.0408
top acc: 0.0416 ::: bot acc: 0.0270
top acc: 0.0328 ::: bot acc: 0.0316
current epoch: 20
train loss is 0.001847
average val loss: 0.001011, accuracy: 0.0347
average test loss: 0.000769, accuracy: 0.0299
case acc: 0.017071765
case acc: 0.048908155
case acc: 0.036957555
case acc: 0.025206866
case acc: 0.025847288
case acc: 0.0252578
top acc: 0.0115 ::: bot acc: 0.0313
top acc: 0.0880 ::: bot acc: 0.0115
top acc: 0.0271 ::: bot acc: 0.0692
top acc: 0.0267 ::: bot acc: 0.0402
top acc: 0.0411 ::: bot acc: 0.0270
top acc: 0.0330 ::: bot acc: 0.0325
current epoch: 21
train loss is 0.001824
average val loss: 0.001046, accuracy: 0.0357
average test loss: 0.000754, accuracy: 0.0295
case acc: 0.016212258
case acc: 0.0486692
case acc: 0.03584728
case acc: 0.024956197
case acc: 0.026069133
case acc: 0.02549563
top acc: 0.0140 ::: bot acc: 0.0285
top acc: 0.0883 ::: bot acc: 0.0109
top acc: 0.0290 ::: bot acc: 0.0660
top acc: 0.0294 ::: bot acc: 0.0378
top acc: 0.0419 ::: bot acc: 0.0259
top acc: 0.0342 ::: bot acc: 0.0306
current epoch: 22
train loss is 0.001806
average val loss: 0.001036, accuracy: 0.0357
average test loss: 0.000731, accuracy: 0.0291
case acc: 0.015884113
case acc: 0.047328018
case acc: 0.035475567
case acc: 0.024801059
case acc: 0.025705822
case acc: 0.025408939
top acc: 0.0162 ::: bot acc: 0.0265
top acc: 0.0859 ::: bot acc: 0.0111
top acc: 0.0318 ::: bot acc: 0.0641
top acc: 0.0301 ::: bot acc: 0.0369
top acc: 0.0423 ::: bot acc: 0.0250
top acc: 0.0338 ::: bot acc: 0.0304
current epoch: 23
train loss is 0.001793
average val loss: 0.001097, accuracy: 0.0371
average test loss: 0.000733, accuracy: 0.0293
case acc: 0.01563126
case acc: 0.04769111
case acc: 0.03469733
case acc: 0.025127776
case acc: 0.02649328
case acc: 0.026311476
top acc: 0.0204 ::: bot acc: 0.0224
top acc: 0.0863 ::: bot acc: 0.0109
top acc: 0.0354 ::: bot acc: 0.0603
top acc: 0.0343 ::: bot acc: 0.0332
top acc: 0.0444 ::: bot acc: 0.0237
top acc: 0.0363 ::: bot acc: 0.0285
current epoch: 24
train loss is 0.001781
average val loss: 0.001125, accuracy: 0.0377
average test loss: 0.000734, accuracy: 0.0295
case acc: 0.015626086
case acc: 0.0477215
case acc: 0.034857366
case acc: 0.02550107
case acc: 0.026390608
case acc: 0.026651435
top acc: 0.0231 ::: bot acc: 0.0193
top acc: 0.0866 ::: bot acc: 0.0113
top acc: 0.0385 ::: bot acc: 0.0577
top acc: 0.0366 ::: bot acc: 0.0313
top acc: 0.0456 ::: bot acc: 0.0221
top acc: 0.0380 ::: bot acc: 0.0270
current epoch: 25
train loss is 0.001780
average val loss: 0.001186, accuracy: 0.0391
average test loss: 0.000738, accuracy: 0.0298
case acc: 0.016827265
case acc: 0.047873817
case acc: 0.03427983
case acc: 0.025293581
case acc: 0.026905611
case acc: 0.027395815
top acc: 0.0262 ::: bot acc: 0.0172
top acc: 0.0864 ::: bot acc: 0.0114
top acc: 0.0409 ::: bot acc: 0.0542
top acc: 0.0384 ::: bot acc: 0.0279
top acc: 0.0468 ::: bot acc: 0.0207
top acc: 0.0398 ::: bot acc: 0.0256
current epoch: 26
train loss is 0.001764
average val loss: 0.001145, accuracy: 0.0384
average test loss: 0.000715, accuracy: 0.0293
case acc: 0.016913509
case acc: 0.045558073
case acc: 0.034445893
case acc: 0.025398841
case acc: 0.026852904
case acc: 0.02687512
top acc: 0.0270 ::: bot acc: 0.0167
top acc: 0.0839 ::: bot acc: 0.0105
top acc: 0.0416 ::: bot acc: 0.0537
top acc: 0.0385 ::: bot acc: 0.0286
top acc: 0.0459 ::: bot acc: 0.0221
top acc: 0.0384 ::: bot acc: 0.0265
current epoch: 27
train loss is 0.001748
average val loss: 0.001101, accuracy: 0.0375
average test loss: 0.000684, accuracy: 0.0288
case acc: 0.01613175
case acc: 0.044145886
case acc: 0.034424532
case acc: 0.025062872
case acc: 0.026504409
case acc: 0.026553549
top acc: 0.0259 ::: bot acc: 0.0164
top acc: 0.0803 ::: bot acc: 0.0123
top acc: 0.0415 ::: bot acc: 0.0542
top acc: 0.0375 ::: bot acc: 0.0291
top acc: 0.0445 ::: bot acc: 0.0239
top acc: 0.0371 ::: bot acc: 0.0286
current epoch: 28
train loss is 0.001706
average val loss: 0.001082, accuracy: 0.0371
average test loss: 0.000670, accuracy: 0.0285
case acc: 0.016517272
case acc: 0.042573147
case acc: 0.034455065
case acc: 0.025342675
case acc: 0.026192276
case acc: 0.026055455
top acc: 0.0253 ::: bot acc: 0.0171
top acc: 0.0782 ::: bot acc: 0.0124
top acc: 0.0417 ::: bot acc: 0.0534
top acc: 0.0376 ::: bot acc: 0.0293
top acc: 0.0441 ::: bot acc: 0.0237
top acc: 0.0355 ::: bot acc: 0.0293
current epoch: 29
train loss is 0.001683
average val loss: 0.001141, accuracy: 0.0383
average test loss: 0.000687, accuracy: 0.0291
case acc: 0.017602349
case acc: 0.043219145
case acc: 0.034715768
case acc: 0.025617493
case acc: 0.026783152
case acc: 0.02673555
top acc: 0.0286 ::: bot acc: 0.0148
top acc: 0.0794 ::: bot acc: 0.0122
top acc: 0.0441 ::: bot acc: 0.0510
top acc: 0.0408 ::: bot acc: 0.0267
top acc: 0.0458 ::: bot acc: 0.0222
top acc: 0.0377 ::: bot acc: 0.0278
current epoch: 30
train loss is 0.001699
average val loss: 0.001130, accuracy: 0.0381
average test loss: 0.000681, accuracy: 0.0290
case acc: 0.017746972
case acc: 0.042499706
case acc: 0.0348567
case acc: 0.02550971
case acc: 0.026813973
case acc: 0.026305096
top acc: 0.0292 ::: bot acc: 0.0143
top acc: 0.0782 ::: bot acc: 0.0124
top acc: 0.0452 ::: bot acc: 0.0505
top acc: 0.0403 ::: bot acc: 0.0264
top acc: 0.0460 ::: bot acc: 0.0222
top acc: 0.0368 ::: bot acc: 0.0283
current epoch: 31
train loss is 0.001671
average val loss: 0.001159, accuracy: 0.0388
average test loss: 0.000683, accuracy: 0.0291
case acc: 0.018235115
case acc: 0.042128794
case acc: 0.035004072
case acc: 0.026274875
case acc: 0.02688016
case acc: 0.026185734
top acc: 0.0295 ::: bot acc: 0.0139
top acc: 0.0774 ::: bot acc: 0.0125
top acc: 0.0470 ::: bot acc: 0.0487
top acc: 0.0423 ::: bot acc: 0.0260
top acc: 0.0464 ::: bot acc: 0.0217
top acc: 0.0370 ::: bot acc: 0.0276
current epoch: 32
train loss is 0.001666
average val loss: 0.001146, accuracy: 0.0384
average test loss: 0.000670, accuracy: 0.0289
case acc: 0.01860489
case acc: 0.041524634
case acc: 0.035086554
case acc: 0.025586467
case acc: 0.026487088
case acc: 0.025900654
top acc: 0.0300 ::: bot acc: 0.0142
top acc: 0.0762 ::: bot acc: 0.0133
top acc: 0.0480 ::: bot acc: 0.0474
top acc: 0.0415 ::: bot acc: 0.0254
top acc: 0.0458 ::: bot acc: 0.0219
top acc: 0.0359 ::: bot acc: 0.0285
current epoch: 33
train loss is 0.001665
average val loss: 0.001161, accuracy: 0.0387
average test loss: 0.000673, accuracy: 0.0291
case acc: 0.01911564
case acc: 0.04103468
case acc: 0.035582576
case acc: 0.025904821
case acc: 0.02648772
case acc: 0.026274215
top acc: 0.0312 ::: bot acc: 0.0133
top acc: 0.0754 ::: bot acc: 0.0131
top acc: 0.0492 ::: bot acc: 0.0467
top acc: 0.0425 ::: bot acc: 0.0244
top acc: 0.0464 ::: bot acc: 0.0208
top acc: 0.0371 ::: bot acc: 0.0279
current epoch: 34
train loss is 0.001646
average val loss: 0.001156, accuracy: 0.0386
average test loss: 0.000666, accuracy: 0.0290
case acc: 0.019101135
case acc: 0.040736973
case acc: 0.035181753
case acc: 0.026014311
case acc: 0.026927099
case acc: 0.026051883
top acc: 0.0313 ::: bot acc: 0.0141
top acc: 0.0746 ::: bot acc: 0.0142
top acc: 0.0491 ::: bot acc: 0.0459
top acc: 0.0423 ::: bot acc: 0.0245
top acc: 0.0466 ::: bot acc: 0.0215
top acc: 0.0363 ::: bot acc: 0.0288
current epoch: 35
train loss is 0.001616
average val loss: 0.001105, accuracy: 0.0375
average test loss: 0.000642, accuracy: 0.0284
case acc: 0.01830124
case acc: 0.03910989
case acc: 0.0350786
case acc: 0.025622638
case acc: 0.026508598
case acc: 0.025656879
top acc: 0.0299 ::: bot acc: 0.0143
top acc: 0.0712 ::: bot acc: 0.0156
top acc: 0.0481 ::: bot acc: 0.0471
top acc: 0.0412 ::: bot acc: 0.0257
top acc: 0.0453 ::: bot acc: 0.0224
top acc: 0.0346 ::: bot acc: 0.0305
current epoch: 36
train loss is 0.001608
average val loss: 0.001122, accuracy: 0.0379
average test loss: 0.000646, accuracy: 0.0285
case acc: 0.018574238
case acc: 0.03949299
case acc: 0.03523326
case acc: 0.025565075
case acc: 0.026691
case acc: 0.025633631
top acc: 0.0305 ::: bot acc: 0.0138
top acc: 0.0718 ::: bot acc: 0.0160
top acc: 0.0492 ::: bot acc: 0.0460
top acc: 0.0414 ::: bot acc: 0.0249
top acc: 0.0459 ::: bot acc: 0.0218
top acc: 0.0348 ::: bot acc: 0.0301
current epoch: 37
train loss is 0.001595
average val loss: 0.001069, accuracy: 0.0367
average test loss: 0.000627, accuracy: 0.0281
case acc: 0.0176435
case acc: 0.038060453
case acc: 0.03515353
case acc: 0.025885263
case acc: 0.026346982
case acc: 0.025445586
top acc: 0.0287 ::: bot acc: 0.0145
top acc: 0.0688 ::: bot acc: 0.0174
top acc: 0.0482 ::: bot acc: 0.0474
top acc: 0.0412 ::: bot acc: 0.0262
top acc: 0.0448 ::: bot acc: 0.0232
top acc: 0.0336 ::: bot acc: 0.0318
current epoch: 38
train loss is 0.001574
average val loss: 0.001095, accuracy: 0.0373
average test loss: 0.000632, accuracy: 0.0283
case acc: 0.018226676
case acc: 0.038391113
case acc: 0.03535653
case acc: 0.025724404
case acc: 0.026478004
case acc: 0.025546044
top acc: 0.0297 ::: bot acc: 0.0143
top acc: 0.0691 ::: bot acc: 0.0175
top acc: 0.0488 ::: bot acc: 0.0464
top acc: 0.0415 ::: bot acc: 0.0250
top acc: 0.0450 ::: bot acc: 0.0225
top acc: 0.0341 ::: bot acc: 0.0310
current epoch: 39
train loss is 0.001572
average val loss: 0.001054, accuracy: 0.0366
average test loss: 0.000615, accuracy: 0.0278
case acc: 0.017227616
case acc: 0.03742801
case acc: 0.03533539
case acc: 0.02545617
case acc: 0.026277415
case acc: 0.0251089
top acc: 0.0279 ::: bot acc: 0.0149
top acc: 0.0670 ::: bot acc: 0.0196
top acc: 0.0488 ::: bot acc: 0.0472
top acc: 0.0405 ::: bot acc: 0.0260
top acc: 0.0438 ::: bot acc: 0.0236
top acc: 0.0331 ::: bot acc: 0.0321
current epoch: 40
train loss is 0.001553
average val loss: 0.001075, accuracy: 0.0369
average test loss: 0.000620, accuracy: 0.0280
case acc: 0.017796975
case acc: 0.03709435
case acc: 0.035299152
case acc: 0.02601072
case acc: 0.026547972
case acc: 0.025400959
top acc: 0.0289 ::: bot acc: 0.0147
top acc: 0.0666 ::: bot acc: 0.0192
top acc: 0.0492 ::: bot acc: 0.0465
top acc: 0.0421 ::: bot acc: 0.0248
top acc: 0.0452 ::: bot acc: 0.0226
top acc: 0.0338 ::: bot acc: 0.0309
current epoch: 41
train loss is 0.001547
average val loss: 0.000994, accuracy: 0.0351
average test loss: 0.000592, accuracy: 0.0273
case acc: 0.01643416
case acc: 0.03606628
case acc: 0.035058744
case acc: 0.025459653
case acc: 0.025936203
case acc: 0.024653686
top acc: 0.0253 ::: bot acc: 0.0171
top acc: 0.0632 ::: bot acc: 0.0228
top acc: 0.0466 ::: bot acc: 0.0493
top acc: 0.0394 ::: bot acc: 0.0275
top acc: 0.0420 ::: bot acc: 0.0261
top acc: 0.0311 ::: bot acc: 0.0335
current epoch: 42
train loss is 0.001507
average val loss: 0.000963, accuracy: 0.0344
average test loss: 0.000580, accuracy: 0.0269
case acc: 0.016179591
case acc: 0.035358094
case acc: 0.0347469
case acc: 0.025271375
case acc: 0.025449138
case acc: 0.02423421
top acc: 0.0240 ::: bot acc: 0.0188
top acc: 0.0609 ::: bot acc: 0.0250
top acc: 0.0456 ::: bot acc: 0.0499
top acc: 0.0378 ::: bot acc: 0.0288
top acc: 0.0398 ::: bot acc: 0.0270
top acc: 0.0293 ::: bot acc: 0.0358
current epoch: 43
train loss is 0.001493
average val loss: 0.000920, accuracy: 0.0334
average test loss: 0.000577, accuracy: 0.0268
case acc: 0.01556921
case acc: 0.034903623
case acc: 0.03493254
case acc: 0.025248328
case acc: 0.025691941
case acc: 0.024176147
top acc: 0.0222 ::: bot acc: 0.0198
top acc: 0.0595 ::: bot acc: 0.0264
top acc: 0.0448 ::: bot acc: 0.0514
top acc: 0.0372 ::: bot acc: 0.0298
top acc: 0.0395 ::: bot acc: 0.0287
top acc: 0.0280 ::: bot acc: 0.0370
current epoch: 44
train loss is 0.001482
average val loss: 0.000916, accuracy: 0.0332
average test loss: 0.000572, accuracy: 0.0266
case acc: 0.015323445
case acc: 0.034879826
case acc: 0.034918133
case acc: 0.02521577
case acc: 0.02511977
case acc: 0.024190756
top acc: 0.0216 ::: bot acc: 0.0204
top acc: 0.0589 ::: bot acc: 0.0274
top acc: 0.0449 ::: bot acc: 0.0508
top acc: 0.0370 ::: bot acc: 0.0303
top acc: 0.0382 ::: bot acc: 0.0284
top acc: 0.0281 ::: bot acc: 0.0374
current epoch: 45
train loss is 0.001471
average val loss: 0.000917, accuracy: 0.0331
average test loss: 0.000571, accuracy: 0.0266
case acc: 0.015450159
case acc: 0.03457556
case acc: 0.03514959
case acc: 0.024769595
case acc: 0.02553834
case acc: 0.024170041
top acc: 0.0218 ::: bot acc: 0.0205
top acc: 0.0583 ::: bot acc: 0.0278
top acc: 0.0451 ::: bot acc: 0.0513
top acc: 0.0360 ::: bot acc: 0.0299
top acc: 0.0384 ::: bot acc: 0.0293
top acc: 0.0279 ::: bot acc: 0.0379
current epoch: 46
train loss is 0.001473
average val loss: 0.000962, accuracy: 0.0342
average test loss: 0.000577, accuracy: 0.0268
case acc: 0.01592169
case acc: 0.03503677
case acc: 0.034939934
case acc: 0.025121626
case acc: 0.025876563
case acc: 0.024074888
top acc: 0.0237 ::: bot acc: 0.0188
top acc: 0.0600 ::: bot acc: 0.0258
top acc: 0.0471 ::: bot acc: 0.0485
top acc: 0.0386 ::: bot acc: 0.0278
top acc: 0.0408 ::: bot acc: 0.0275
top acc: 0.0289 ::: bot acc: 0.0359
current epoch: 47
train loss is 0.001479
average val loss: 0.000954, accuracy: 0.0340
average test loss: 0.000574, accuracy: 0.0267
case acc: 0.015810022
case acc: 0.034791905
case acc: 0.035088368
case acc: 0.025044814
case acc: 0.025331737
case acc: 0.024215972
top acc: 0.0238 ::: bot acc: 0.0188
top acc: 0.0594 ::: bot acc: 0.0263
top acc: 0.0470 ::: bot acc: 0.0489
top acc: 0.0382 ::: bot acc: 0.0282
top acc: 0.0400 ::: bot acc: 0.0273
top acc: 0.0284 ::: bot acc: 0.0364
current epoch: 48
train loss is 0.001468
average val loss: 0.000963, accuracy: 0.0343
average test loss: 0.000579, accuracy: 0.0268
case acc: 0.015624534
case acc: 0.034847017
case acc: 0.034984346
case acc: 0.025517557
case acc: 0.025580412
case acc: 0.02426539
top acc: 0.0235 ::: bot acc: 0.0184
top acc: 0.0594 ::: bot acc: 0.0262
top acc: 0.0478 ::: bot acc: 0.0478
top acc: 0.0391 ::: bot acc: 0.0279
top acc: 0.0411 ::: bot acc: 0.0266
top acc: 0.0291 ::: bot acc: 0.0363
current epoch: 49
train loss is 0.001470
average val loss: 0.000960, accuracy: 0.0341
average test loss: 0.000575, accuracy: 0.0269
case acc: 0.015678724
case acc: 0.034660134
case acc: 0.03516482
case acc: 0.025732337
case acc: 0.025745336
case acc: 0.024240613
top acc: 0.0234 ::: bot acc: 0.0187
top acc: 0.0586 ::: bot acc: 0.0266
top acc: 0.0478 ::: bot acc: 0.0478
top acc: 0.0395 ::: bot acc: 0.0280
top acc: 0.0412 ::: bot acc: 0.0267
top acc: 0.0291 ::: bot acc: 0.0358
current epoch: 50
train loss is 0.001458
average val loss: 0.000939, accuracy: 0.0336
average test loss: 0.000569, accuracy: 0.0267
case acc: 0.015459736
case acc: 0.03440989
case acc: 0.034995023
case acc: 0.025401572
case acc: 0.02573123
case acc: 0.024182325
top acc: 0.0217 ::: bot acc: 0.0200
top acc: 0.0574 ::: bot acc: 0.0281
top acc: 0.0466 ::: bot acc: 0.0486
top acc: 0.0381 ::: bot acc: 0.0286
top acc: 0.0406 ::: bot acc: 0.0272
top acc: 0.0287 ::: bot acc: 0.0368
LME_Co_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5400 5400 5400
1.7082474 -0.6288155 0.17559324 -0.22413088
Validation: 600 600 600
Testing: 744 744 744
pre-processing time: 0.00020599365234375
the split date is 2011-01-01
train dropout: 0.2 test dropout: 0.15
net initializing with time: 0.0023212432861328125
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.007684
average val loss: 0.005352, accuracy: 0.0952
average test loss: 0.005675, accuracy: 0.0965
case acc: 0.12545341
case acc: 0.10183118
case acc: 0.09818448
case acc: 0.0960111
case acc: 0.10661297
case acc: 0.050939895
top acc: 0.0789 ::: bot acc: 0.1700
top acc: 0.1348 ::: bot acc: 0.0673
top acc: 0.0397 ::: bot acc: 0.1563
top acc: 0.0651 ::: bot acc: 0.1307
top acc: 0.0582 ::: bot acc: 0.1510
top acc: 0.0248 ::: bot acc: 0.0880
current epoch: 2
train loss is 0.006674
average val loss: 0.004056, accuracy: 0.0605
average test loss: 0.004626, accuracy: 0.0683
case acc: 0.03469041
case acc: 0.20024766
case acc: 0.04724486
case acc: 0.02732695
case acc: 0.035427935
case acc: 0.0648089
top acc: 0.0342 ::: bot acc: 0.0546
top acc: 0.2333 ::: bot acc: 0.1652
top acc: 0.0827 ::: bot acc: 0.0428
top acc: 0.0443 ::: bot acc: 0.0240
top acc: 0.0464 ::: bot acc: 0.0480
top acc: 0.1180 ::: bot acc: 0.0180
current epoch: 3
train loss is 0.008399
average val loss: 0.011950, accuracy: 0.1358
average test loss: 0.012726, accuracy: 0.1385
case acc: 0.08861772
case acc: 0.28431892
case acc: 0.11738188
case acc: 0.10380545
case acc: 0.089107886
case acc: 0.14798158
top acc: 0.1345 ::: bot acc: 0.0449
top acc: 0.3176 ::: bot acc: 0.2509
top acc: 0.1827 ::: bot acc: 0.0570
top acc: 0.1366 ::: bot acc: 0.0692
top acc: 0.1385 ::: bot acc: 0.0446
top acc: 0.2037 ::: bot acc: 0.0955
current epoch: 4
train loss is 0.011893
average val loss: 0.010816, accuracy: 0.1291
average test loss: 0.011573, accuracy: 0.1320
case acc: 0.088030115
case acc: 0.2694931
case acc: 0.11419031
case acc: 0.09690841
case acc: 0.08633818
case acc: 0.13718404
top acc: 0.1336 ::: bot acc: 0.0444
top acc: 0.3033 ::: bot acc: 0.2348
top acc: 0.1781 ::: bot acc: 0.0548
top acc: 0.1301 ::: bot acc: 0.0622
top acc: 0.1362 ::: bot acc: 0.0413
top acc: 0.1923 ::: bot acc: 0.0854
current epoch: 5
train loss is 0.012716
average val loss: 0.002257, accuracy: 0.0523
average test loss: 0.002669, accuracy: 0.0596
case acc: 0.049726143
case acc: 0.12339947
case acc: 0.05143891
case acc: 0.044468913
case acc: 0.04910753
case acc: 0.039382037
top acc: 0.0171 ::: bot acc: 0.0873
top acc: 0.1574 ::: bot acc: 0.0896
top acc: 0.0434 ::: bot acc: 0.0850
top acc: 0.0230 ::: bot acc: 0.0738
top acc: 0.0170 ::: bot acc: 0.0853
top acc: 0.0568 ::: bot acc: 0.0502
current epoch: 6
train loss is 0.005825
average val loss: 0.002567, accuracy: 0.0634
average test loss: 0.002897, accuracy: 0.0666
case acc: 0.070245124
case acc: 0.08380857
case acc: 0.067360476
case acc: 0.070011795
case acc: 0.06214829
case acc: 0.046101514
top acc: 0.0264 ::: bot acc: 0.1133
top acc: 0.1179 ::: bot acc: 0.0502
top acc: 0.0280 ::: bot acc: 0.1158
top acc: 0.0419 ::: bot acc: 0.1026
top acc: 0.0187 ::: bot acc: 0.1043
top acc: 0.0292 ::: bot acc: 0.0787
current epoch: 7
train loss is 0.003079
average val loss: 0.001805, accuracy: 0.0436
average test loss: 0.002259, accuracy: 0.0524
case acc: 0.03779235
case acc: 0.1198183
case acc: 0.04831252
case acc: 0.031776253
case acc: 0.036869757
case acc: 0.04003309
top acc: 0.0256 ::: bot acc: 0.0652
top acc: 0.1537 ::: bot acc: 0.0849
top acc: 0.0528 ::: bot acc: 0.0746
top acc: 0.0174 ::: bot acc: 0.0571
top acc: 0.0429 ::: bot acc: 0.0520
top acc: 0.0735 ::: bot acc: 0.0325
current epoch: 8
train loss is 0.003358
average val loss: 0.001660, accuracy: 0.0417
average test loss: 0.002135, accuracy: 0.0511
case acc: 0.036676746
case acc: 0.114471585
case acc: 0.047846273
case acc: 0.030655518
case acc: 0.035655107
case acc: 0.04102404
top acc: 0.0301 ::: bot acc: 0.0614
top acc: 0.1482 ::: bot acc: 0.0796
top acc: 0.0535 ::: bot acc: 0.0735
top acc: 0.0167 ::: bot acc: 0.0558
top acc: 0.0489 ::: bot acc: 0.0454
top acc: 0.0759 ::: bot acc: 0.0308
current epoch: 9
train loss is 0.003327
average val loss: 0.001462, accuracy: 0.0412
average test loss: 0.001890, accuracy: 0.0504
case acc: 0.041269027
case acc: 0.09380477
case acc: 0.052156933
case acc: 0.039489012
case acc: 0.037089404
case acc: 0.03866357
top acc: 0.0217 ::: bot acc: 0.0730
top acc: 0.1269 ::: bot acc: 0.0604
top acc: 0.0407 ::: bot acc: 0.0866
top acc: 0.0189 ::: bot acc: 0.0679
top acc: 0.0408 ::: bot acc: 0.0544
top acc: 0.0618 ::: bot acc: 0.0441
current epoch: 10
train loss is 0.002797
average val loss: 0.001335, accuracy: 0.0395
average test loss: 0.001768, accuracy: 0.0489
case acc: 0.04010396
case acc: 0.086621694
case acc: 0.052578524
case acc: 0.039466336
case acc: 0.03615605
case acc: 0.03874204
top acc: 0.0236 ::: bot acc: 0.0695
top acc: 0.1196 ::: bot acc: 0.0529
top acc: 0.0392 ::: bot acc: 0.0884
top acc: 0.0190 ::: bot acc: 0.0688
top acc: 0.0443 ::: bot acc: 0.0498
top acc: 0.0630 ::: bot acc: 0.0435
current epoch: 11
train loss is 0.002486
average val loss: 0.001236, accuracy: 0.0375
average test loss: 0.001699, accuracy: 0.0476
case acc: 0.038010594
case acc: 0.08448308
case acc: 0.051920515
case acc: 0.036678195
case acc: 0.0355088
case acc: 0.03901801
top acc: 0.0290 ::: bot acc: 0.0636
top acc: 0.1174 ::: bot acc: 0.0507
top acc: 0.0426 ::: bot acc: 0.0860
top acc: 0.0174 ::: bot acc: 0.0649
top acc: 0.0513 ::: bot acc: 0.0435
top acc: 0.0674 ::: bot acc: 0.0389
current epoch: 12
train loss is 0.002346
average val loss: 0.001154, accuracy: 0.0361
average test loss: 0.001602, accuracy: 0.0461
case acc: 0.0362359
case acc: 0.07946684
case acc: 0.051184706
case acc: 0.035167858
case acc: 0.03499106
case acc: 0.03957856
top acc: 0.0313 ::: bot acc: 0.0599
top acc: 0.1132 ::: bot acc: 0.0457
top acc: 0.0423 ::: bot acc: 0.0847
top acc: 0.0170 ::: bot acc: 0.0631
top acc: 0.0539 ::: bot acc: 0.0395
top acc: 0.0687 ::: bot acc: 0.0385
current epoch: 13
train loss is 0.002213
average val loss: 0.001078, accuracy: 0.0347
average test loss: 0.001536, accuracy: 0.0451
case acc: 0.035351325
case acc: 0.07596622
case acc: 0.050470233
case acc: 0.033923123
case acc: 0.035266533
case acc: 0.039507486
top acc: 0.0351 ::: bot acc: 0.0567
top acc: 0.1090 ::: bot acc: 0.0428
top acc: 0.0437 ::: bot acc: 0.0824
top acc: 0.0168 ::: bot acc: 0.0606
top acc: 0.0575 ::: bot acc: 0.0359
top acc: 0.0700 ::: bot acc: 0.0363
current epoch: 14
train loss is 0.002118
average val loss: 0.001009, accuracy: 0.0337
average test loss: 0.001471, accuracy: 0.0442
case acc: 0.034896657
case acc: 0.07062786
case acc: 0.050505444
case acc: 0.033757206
case acc: 0.03597184
case acc: 0.039699573
top acc: 0.0365 ::: bot acc: 0.0550
top acc: 0.1031 ::: bot acc: 0.0379
top acc: 0.0435 ::: bot acc: 0.0828
top acc: 0.0177 ::: bot acc: 0.0605
top acc: 0.0592 ::: bot acc: 0.0354
top acc: 0.0699 ::: bot acc: 0.0368
current epoch: 15
train loss is 0.002021
average val loss: 0.000962, accuracy: 0.0328
average test loss: 0.001398, accuracy: 0.0432
case acc: 0.03419334
case acc: 0.06527176
case acc: 0.050825667
case acc: 0.034189917
case acc: 0.035616312
case acc: 0.039013617
top acc: 0.0371 ::: bot acc: 0.0536
top acc: 0.0977 ::: bot acc: 0.0338
top acc: 0.0427 ::: bot acc: 0.0834
top acc: 0.0171 ::: bot acc: 0.0609
top acc: 0.0592 ::: bot acc: 0.0348
top acc: 0.0685 ::: bot acc: 0.0370
current epoch: 16
train loss is 0.001903
average val loss: 0.000901, accuracy: 0.0317
average test loss: 0.001355, accuracy: 0.0425
case acc: 0.03424818
case acc: 0.061896082
case acc: 0.050428603
case acc: 0.03290531
case acc: 0.03583569
case acc: 0.0395454
top acc: 0.0399 ::: bot acc: 0.0519
top acc: 0.0926 ::: bot acc: 0.0316
top acc: 0.0433 ::: bot acc: 0.0828
top acc: 0.0169 ::: bot acc: 0.0594
top acc: 0.0608 ::: bot acc: 0.0338
top acc: 0.0695 ::: bot acc: 0.0375
current epoch: 17
train loss is 0.001817
average val loss: 0.000856, accuracy: 0.0310
average test loss: 0.001318, accuracy: 0.0417
case acc: 0.03348791
case acc: 0.059905775
case acc: 0.04981991
case acc: 0.03148464
case acc: 0.035769723
case acc: 0.0395983
top acc: 0.0437 ::: bot acc: 0.0474
top acc: 0.0916 ::: bot acc: 0.0300
top acc: 0.0455 ::: bot acc: 0.0806
top acc: 0.0168 ::: bot acc: 0.0572
top acc: 0.0630 ::: bot acc: 0.0307
top acc: 0.0710 ::: bot acc: 0.0347
current epoch: 18
train loss is 0.001752
average val loss: 0.000825, accuracy: 0.0304
average test loss: 0.001286, accuracy: 0.0410
case acc: 0.03335473
case acc: 0.057345822
case acc: 0.04925952
case acc: 0.03015979
case acc: 0.036374003
case acc: 0.039370235
top acc: 0.0466 ::: bot acc: 0.0446
top acc: 0.0877 ::: bot acc: 0.0278
top acc: 0.0465 ::: bot acc: 0.0799
top acc: 0.0172 ::: bot acc: 0.0556
top acc: 0.0645 ::: bot acc: 0.0311
top acc: 0.0713 ::: bot acc: 0.0337
current epoch: 19
train loss is 0.001712
average val loss: 0.000795, accuracy: 0.0299
average test loss: 0.001251, accuracy: 0.0404
case acc: 0.03320087
case acc: 0.05396559
case acc: 0.049462195
case acc: 0.03040159
case acc: 0.036058217
case acc: 0.03945266
top acc: 0.0471 ::: bot acc: 0.0437
top acc: 0.0846 ::: bot acc: 0.0256
top acc: 0.0469 ::: bot acc: 0.0798
top acc: 0.0164 ::: bot acc: 0.0560
top acc: 0.0630 ::: bot acc: 0.0308
top acc: 0.0709 ::: bot acc: 0.0351
current epoch: 20
train loss is 0.001639
average val loss: 0.000765, accuracy: 0.0294
average test loss: 0.001208, accuracy: 0.0399
case acc: 0.033615816
case acc: 0.049573075
case acc: 0.050098553
case acc: 0.03129433
case acc: 0.035670627
case acc: 0.039356627
top acc: 0.0464 ::: bot acc: 0.0453
top acc: 0.0791 ::: bot acc: 0.0231
top acc: 0.0463 ::: bot acc: 0.0813
top acc: 0.0167 ::: bot acc: 0.0573
top acc: 0.0603 ::: bot acc: 0.0336
top acc: 0.0686 ::: bot acc: 0.0377
current epoch: 21
train loss is 0.001592
average val loss: 0.000735, accuracy: 0.0289
average test loss: 0.001192, accuracy: 0.0394
case acc: 0.03336448
case acc: 0.049177267
case acc: 0.04885933
case acc: 0.029203815
case acc: 0.0360365
case acc: 0.039622206
top acc: 0.0499 ::: bot acc: 0.0414
top acc: 0.0789 ::: bot acc: 0.0226
top acc: 0.0493 ::: bot acc: 0.0773
top acc: 0.0169 ::: bot acc: 0.0537
top acc: 0.0625 ::: bot acc: 0.0321
top acc: 0.0705 ::: bot acc: 0.0363
current epoch: 22
train loss is 0.001564
average val loss: 0.000728, accuracy: 0.0288
average test loss: 0.001174, accuracy: 0.0389
case acc: 0.03318965
case acc: 0.048258
case acc: 0.048267435
case acc: 0.028300922
case acc: 0.035831157
case acc: 0.03962441
top acc: 0.0528 ::: bot acc: 0.0379
top acc: 0.0776 ::: bot acc: 0.0223
top acc: 0.0525 ::: bot acc: 0.0752
top acc: 0.0176 ::: bot acc: 0.0524
top acc: 0.0631 ::: bot acc: 0.0304
top acc: 0.0703 ::: bot acc: 0.0354
current epoch: 23
train loss is 0.001585
average val loss: 0.000728, accuracy: 0.0290
average test loss: 0.001182, accuracy: 0.0389
case acc: 0.033980925
case acc: 0.04887623
case acc: 0.047428045
case acc: 0.026736272
case acc: 0.03623363
case acc: 0.03989532
top acc: 0.0570 ::: bot acc: 0.0342
top acc: 0.0784 ::: bot acc: 0.0225
top acc: 0.0566 ::: bot acc: 0.0709
top acc: 0.0196 ::: bot acc: 0.0490
top acc: 0.0656 ::: bot acc: 0.0284
top acc: 0.0725 ::: bot acc: 0.0324
current epoch: 24
train loss is 0.001558
average val loss: 0.000708, accuracy: 0.0289
average test loss: 0.001175, accuracy: 0.0387
case acc: 0.034523055
case acc: 0.04763891
case acc: 0.047067687
case acc: 0.026177738
case acc: 0.036446054
case acc: 0.040511027
top acc: 0.0592 ::: bot acc: 0.0326
top acc: 0.0764 ::: bot acc: 0.0222
top acc: 0.0578 ::: bot acc: 0.0691
top acc: 0.0199 ::: bot acc: 0.0480
top acc: 0.0658 ::: bot acc: 0.0287
top acc: 0.0736 ::: bot acc: 0.0331
current epoch: 25
train loss is 0.001528
average val loss: 0.000693, accuracy: 0.0286
average test loss: 0.001142, accuracy: 0.0381
case acc: 0.034405157
case acc: 0.044875246
case acc: 0.047243454
case acc: 0.026324922
case acc: 0.03607917
case acc: 0.03996987
top acc: 0.0586 ::: bot acc: 0.0329
top acc: 0.0733 ::: bot acc: 0.0205
top acc: 0.0576 ::: bot acc: 0.0696
top acc: 0.0200 ::: bot acc: 0.0479
top acc: 0.0647 ::: bot acc: 0.0294
top acc: 0.0720 ::: bot acc: 0.0345
current epoch: 26
train loss is 0.001505
average val loss: 0.000687, accuracy: 0.0287
average test loss: 0.001143, accuracy: 0.0381
case acc: 0.034948874
case acc: 0.044457752
case acc: 0.04665291
case acc: 0.026213115
case acc: 0.036177937
case acc: 0.040010817
top acc: 0.0612 ::: bot acc: 0.0309
top acc: 0.0726 ::: bot acc: 0.0202
top acc: 0.0603 ::: bot acc: 0.0667
top acc: 0.0211 ::: bot acc: 0.0473
top acc: 0.0653 ::: bot acc: 0.0292
top acc: 0.0725 ::: bot acc: 0.0337
current epoch: 27
train loss is 0.001505
average val loss: 0.000689, accuracy: 0.0289
average test loss: 0.001139, accuracy: 0.0378
case acc: 0.034891076
case acc: 0.043992575
case acc: 0.04647461
case acc: 0.025371766
case acc: 0.03597636
case acc: 0.040346377
top acc: 0.0626 ::: bot acc: 0.0283
top acc: 0.0725 ::: bot acc: 0.0203
top acc: 0.0621 ::: bot acc: 0.0650
top acc: 0.0227 ::: bot acc: 0.0448
top acc: 0.0650 ::: bot acc: 0.0285
top acc: 0.0735 ::: bot acc: 0.0329
current epoch: 28
train loss is 0.001488
average val loss: 0.000687, accuracy: 0.0290
average test loss: 0.001140, accuracy: 0.0378
case acc: 0.035206962
case acc: 0.043655686
case acc: 0.04620623
case acc: 0.025105609
case acc: 0.036160596
case acc: 0.040366426
top acc: 0.0646 ::: bot acc: 0.0262
top acc: 0.0722 ::: bot acc: 0.0196
top acc: 0.0640 ::: bot acc: 0.0624
top acc: 0.0240 ::: bot acc: 0.0442
top acc: 0.0657 ::: bot acc: 0.0281
top acc: 0.0735 ::: bot acc: 0.0332
current epoch: 29
train loss is 0.001480
average val loss: 0.000696, accuracy: 0.0293
average test loss: 0.001151, accuracy: 0.0379
case acc: 0.035775524
case acc: 0.043846913
case acc: 0.04618713
case acc: 0.024914045
case acc: 0.036567762
case acc: 0.040016003
top acc: 0.0658 ::: bot acc: 0.0245
top acc: 0.0726 ::: bot acc: 0.0196
top acc: 0.0671 ::: bot acc: 0.0603
top acc: 0.0257 ::: bot acc: 0.0425
top acc: 0.0673 ::: bot acc: 0.0273
top acc: 0.0737 ::: bot acc: 0.0313
current epoch: 30
train loss is 0.001500
average val loss: 0.000708, accuracy: 0.0298
average test loss: 0.001169, accuracy: 0.0381
case acc: 0.036713943
case acc: 0.044010084
case acc: 0.046170432
case acc: 0.024646608
case acc: 0.03622956
case acc: 0.040889487
top acc: 0.0685 ::: bot acc: 0.0228
top acc: 0.0724 ::: bot acc: 0.0196
top acc: 0.0690 ::: bot acc: 0.0579
top acc: 0.0279 ::: bot acc: 0.0408
top acc: 0.0682 ::: bot acc: 0.0257
top acc: 0.0758 ::: bot acc: 0.0312
current epoch: 31
train loss is 0.001484
average val loss: 0.000716, accuracy: 0.0301
average test loss: 0.001175, accuracy: 0.0383
case acc: 0.03699772
case acc: 0.044107422
case acc: 0.046419572
case acc: 0.02499938
case acc: 0.036585666
case acc: 0.040569324
top acc: 0.0697 ::: bot acc: 0.0214
top acc: 0.0728 ::: bot acc: 0.0204
top acc: 0.0712 ::: bot acc: 0.0562
top acc: 0.0294 ::: bot acc: 0.0397
top acc: 0.0694 ::: bot acc: 0.0245
top acc: 0.0753 ::: bot acc: 0.0305
current epoch: 32
train loss is 0.001472
average val loss: 0.000706, accuracy: 0.0299
average test loss: 0.001160, accuracy: 0.0380
case acc: 0.0369215
case acc: 0.042787287
case acc: 0.04604136
case acc: 0.024814954
case acc: 0.036638033
case acc: 0.040544473
top acc: 0.0694 ::: bot acc: 0.0217
top acc: 0.0711 ::: bot acc: 0.0192
top acc: 0.0707 ::: bot acc: 0.0559
top acc: 0.0289 ::: bot acc: 0.0393
top acc: 0.0693 ::: bot acc: 0.0248
top acc: 0.0747 ::: bot acc: 0.0316
current epoch: 33
train loss is 0.001463
average val loss: 0.000718, accuracy: 0.0302
average test loss: 0.001171, accuracy: 0.0381
case acc: 0.037777197
case acc: 0.042293556
case acc: 0.046253223
case acc: 0.024797333
case acc: 0.036648426
case acc: 0.04057269
top acc: 0.0711 ::: bot acc: 0.0214
top acc: 0.0706 ::: bot acc: 0.0186
top acc: 0.0730 ::: bot acc: 0.0546
top acc: 0.0292 ::: bot acc: 0.0388
top acc: 0.0700 ::: bot acc: 0.0241
top acc: 0.0752 ::: bot acc: 0.0311
current epoch: 34
train loss is 0.001467
average val loss: 0.000716, accuracy: 0.0303
average test loss: 0.001176, accuracy: 0.0382
case acc: 0.038077075
case acc: 0.04230718
case acc: 0.046094634
case acc: 0.024992278
case acc: 0.036902502
case acc: 0.040749114
top acc: 0.0722 ::: bot acc: 0.0200
top acc: 0.0700 ::: bot acc: 0.0194
top acc: 0.0737 ::: bot acc: 0.0530
top acc: 0.0307 ::: bot acc: 0.0382
top acc: 0.0704 ::: bot acc: 0.0231
top acc: 0.0753 ::: bot acc: 0.0305
current epoch: 35
train loss is 0.001466
average val loss: 0.000730, accuracy: 0.0306
average test loss: 0.001191, accuracy: 0.0384
case acc: 0.038556024
case acc: 0.042423345
case acc: 0.046459284
case acc: 0.025054492
case acc: 0.0368899
case acc: 0.041092236
top acc: 0.0728 ::: bot acc: 0.0195
top acc: 0.0705 ::: bot acc: 0.0193
top acc: 0.0758 ::: bot acc: 0.0517
top acc: 0.0326 ::: bot acc: 0.0362
top acc: 0.0715 ::: bot acc: 0.0215
top acc: 0.0762 ::: bot acc: 0.0298
current epoch: 36
train loss is 0.001457
average val loss: 0.000702, accuracy: 0.0301
average test loss: 0.001156, accuracy: 0.0378
case acc: 0.03780386
case acc: 0.040541485
case acc: 0.046377264
case acc: 0.024857955
case acc: 0.036775734
case acc: 0.04041118
top acc: 0.0706 ::: bot acc: 0.0207
top acc: 0.0682 ::: bot acc: 0.0186
top acc: 0.0741 ::: bot acc: 0.0528
top acc: 0.0312 ::: bot acc: 0.0374
top acc: 0.0705 ::: bot acc: 0.0233
top acc: 0.0747 ::: bot acc: 0.0312
current epoch: 37
train loss is 0.001438
average val loss: 0.000688, accuracy: 0.0297
average test loss: 0.001140, accuracy: 0.0375
case acc: 0.037341442
case acc: 0.039241023
case acc: 0.046243627
case acc: 0.025034288
case acc: 0.036816444
case acc: 0.04051643
top acc: 0.0702 ::: bot acc: 0.0208
top acc: 0.0658 ::: bot acc: 0.0186
top acc: 0.0736 ::: bot acc: 0.0535
top acc: 0.0306 ::: bot acc: 0.0386
top acc: 0.0700 ::: bot acc: 0.0239
top acc: 0.0740 ::: bot acc: 0.0326
current epoch: 38
train loss is 0.001407
average val loss: 0.000665, accuracy: 0.0292
average test loss: 0.001110, accuracy: 0.0369
case acc: 0.036875103
case acc: 0.037356924
case acc: 0.046320044
case acc: 0.02461923
case acc: 0.0368271
case acc: 0.03966965
top acc: 0.0686 ::: bot acc: 0.0233
top acc: 0.0635 ::: bot acc: 0.0174
top acc: 0.0723 ::: bot acc: 0.0549
top acc: 0.0289 ::: bot acc: 0.0392
top acc: 0.0692 ::: bot acc: 0.0253
top acc: 0.0719 ::: bot acc: 0.0337
current epoch: 39
train loss is 0.001378
average val loss: 0.000647, accuracy: 0.0287
average test loss: 0.001087, accuracy: 0.0365
case acc: 0.03641248
case acc: 0.035901755
case acc: 0.04607511
case acc: 0.024822053
case acc: 0.036288187
case acc: 0.03936105
top acc: 0.0669 ::: bot acc: 0.0248
top acc: 0.0615 ::: bot acc: 0.0177
top acc: 0.0711 ::: bot acc: 0.0560
top acc: 0.0278 ::: bot acc: 0.0412
top acc: 0.0674 ::: bot acc: 0.0262
top acc: 0.0706 ::: bot acc: 0.0351
current epoch: 40
train loss is 0.001345
average val loss: 0.000618, accuracy: 0.0279
average test loss: 0.001054, accuracy: 0.0359
case acc: 0.035583053
case acc: 0.033479284
case acc: 0.046220183
case acc: 0.024933552
case acc: 0.036055
case acc: 0.039146084
top acc: 0.0643 ::: bot acc: 0.0273
top acc: 0.0578 ::: bot acc: 0.0177
top acc: 0.0693 ::: bot acc: 0.0580
top acc: 0.0256 ::: bot acc: 0.0429
top acc: 0.0659 ::: bot acc: 0.0280
top acc: 0.0687 ::: bot acc: 0.0372
current epoch: 41
train loss is 0.001326
average val loss: 0.000606, accuracy: 0.0276
average test loss: 0.001030, accuracy: 0.0355
case acc: 0.035131946
case acc: 0.03156396
case acc: 0.04600924
case acc: 0.025191674
case acc: 0.035913922
case acc: 0.03899917
top acc: 0.0629 ::: bot acc: 0.0292
top acc: 0.0553 ::: bot acc: 0.0173
top acc: 0.0666 ::: bot acc: 0.0601
top acc: 0.0240 ::: bot acc: 0.0440
top acc: 0.0642 ::: bot acc: 0.0294
top acc: 0.0671 ::: bot acc: 0.0389
current epoch: 42
train loss is 0.001301
average val loss: 0.000589, accuracy: 0.0272
average test loss: 0.001005, accuracy: 0.0351
case acc: 0.034165625
case acc: 0.03006775
case acc: 0.046195697
case acc: 0.026040003
case acc: 0.035629936
case acc: 0.03868647
top acc: 0.0591 ::: bot acc: 0.0318
top acc: 0.0522 ::: bot acc: 0.0186
top acc: 0.0642 ::: bot acc: 0.0625
top acc: 0.0223 ::: bot acc: 0.0464
top acc: 0.0612 ::: bot acc: 0.0324
top acc: 0.0651 ::: bot acc: 0.0406
current epoch: 43
train loss is 0.001281
average val loss: 0.000583, accuracy: 0.0269
average test loss: 0.000998, accuracy: 0.0351
case acc: 0.033953052
case acc: 0.028701726
case acc: 0.046769537
case acc: 0.026634676
case acc: 0.035584535
case acc: 0.038744766
top acc: 0.0572 ::: bot acc: 0.0342
top acc: 0.0495 ::: bot acc: 0.0199
top acc: 0.0626 ::: bot acc: 0.0651
top acc: 0.0206 ::: bot acc: 0.0487
top acc: 0.0588 ::: bot acc: 0.0346
top acc: 0.0633 ::: bot acc: 0.0434
current epoch: 44
train loss is 0.001258
average val loss: 0.000578, accuracy: 0.0266
average test loss: 0.000988, accuracy: 0.0349
case acc: 0.033784233
case acc: 0.027885707
case acc: 0.046581797
case acc: 0.02646191
case acc: 0.035675317
case acc: 0.038821887
top acc: 0.0558 ::: bot acc: 0.0354
top acc: 0.0482 ::: bot acc: 0.0201
top acc: 0.0614 ::: bot acc: 0.0658
top acc: 0.0197 ::: bot acc: 0.0488
top acc: 0.0582 ::: bot acc: 0.0359
top acc: 0.0626 ::: bot acc: 0.0435
current epoch: 45
train loss is 0.001251
average val loss: 0.000577, accuracy: 0.0267
average test loss: 0.000980, accuracy: 0.0347
case acc: 0.03347699
case acc: 0.027038535
case acc: 0.04650986
case acc: 0.0270889
case acc: 0.035349138
case acc: 0.03870281
top acc: 0.0539 ::: bot acc: 0.0368
top acc: 0.0463 ::: bot acc: 0.0217
top acc: 0.0602 ::: bot acc: 0.0667
top acc: 0.0188 ::: bot acc: 0.0504
top acc: 0.0571 ::: bot acc: 0.0363
top acc: 0.0614 ::: bot acc: 0.0446
current epoch: 46
train loss is 0.001249
average val loss: 0.000577, accuracy: 0.0265
average test loss: 0.000982, accuracy: 0.0348
case acc: 0.03369687
case acc: 0.026683731
case acc: 0.047102325
case acc: 0.027320199
case acc: 0.035399515
case acc: 0.038790703
top acc: 0.0535 ::: bot acc: 0.0383
top acc: 0.0448 ::: bot acc: 0.0234
top acc: 0.0592 ::: bot acc: 0.0682
top acc: 0.0179 ::: bot acc: 0.0508
top acc: 0.0561 ::: bot acc: 0.0376
top acc: 0.0607 ::: bot acc: 0.0456
current epoch: 47
train loss is 0.001237
average val loss: 0.000577, accuracy: 0.0265
average test loss: 0.000973, accuracy: 0.0347
case acc: 0.033477135
case acc: 0.025839813
case acc: 0.047102306
case acc: 0.028058486
case acc: 0.0352366
case acc: 0.03857316
top acc: 0.0513 ::: bot acc: 0.0401
top acc: 0.0426 ::: bot acc: 0.0248
top acc: 0.0574 ::: bot acc: 0.0698
top acc: 0.0173 ::: bot acc: 0.0522
top acc: 0.0541 ::: bot acc: 0.0398
top acc: 0.0583 ::: bot acc: 0.0470
current epoch: 48
train loss is 0.001223
average val loss: 0.000581, accuracy: 0.0265
average test loss: 0.000972, accuracy: 0.0348
case acc: 0.033439014
case acc: 0.025489967
case acc: 0.047133572
case acc: 0.028994387
case acc: 0.035086177
case acc: 0.038545433
top acc: 0.0510 ::: bot acc: 0.0403
top acc: 0.0419 ::: bot acc: 0.0255
top acc: 0.0566 ::: bot acc: 0.0700
top acc: 0.0179 ::: bot acc: 0.0532
top acc: 0.0529 ::: bot acc: 0.0401
top acc: 0.0580 ::: bot acc: 0.0475
current epoch: 49
train loss is 0.001220
average val loss: 0.000573, accuracy: 0.0264
average test loss: 0.000972, accuracy: 0.0347
case acc: 0.03338049
case acc: 0.02560589
case acc: 0.046923544
case acc: 0.028164769
case acc: 0.035352964
case acc: 0.038655654
top acc: 0.0509 ::: bot acc: 0.0402
top acc: 0.0418 ::: bot acc: 0.0257
top acc: 0.0568 ::: bot acc: 0.0698
top acc: 0.0173 ::: bot acc: 0.0526
top acc: 0.0542 ::: bot acc: 0.0398
top acc: 0.0586 ::: bot acc: 0.0471
current epoch: 50
train loss is 0.001218
average val loss: 0.000583, accuracy: 0.0266
average test loss: 0.000980, accuracy: 0.0349
case acc: 0.03340095
case acc: 0.025549576
case acc: 0.04734356
case acc: 0.028762652
case acc: 0.035356764
case acc: 0.039016195
top acc: 0.0501 ::: bot acc: 0.0411
top acc: 0.0408 ::: bot acc: 0.0273
top acc: 0.0565 ::: bot acc: 0.0712
top acc: 0.0171 ::: bot acc: 0.0535
top acc: 0.0538 ::: bot acc: 0.0405
top acc: 0.0581 ::: bot acc: 0.0485
LME_Co_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5370 5370 5370
1.7082474 -0.6288155 0.17559324 -0.22413088
Validation: 600 600 600
Testing: 774 774 774
pre-processing time: 0.00018739700317382812
the split date is 2011-07-01
train dropout: 0.2 test dropout: 0.15
net initializing with time: 0.0022575855255126953
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.007991
average val loss: 0.005133, accuracy: 0.0917
average test loss: 0.005747, accuracy: 0.0955
case acc: 0.13188845
case acc: 0.10690855
case acc: 0.09564947
case acc: 0.07337269
case acc: 0.1199734
case acc: 0.04525035
top acc: 0.0856 ::: bot acc: 0.1749
top acc: 0.1448 ::: bot acc: 0.0726
top acc: 0.0449 ::: bot acc: 0.1419
top acc: 0.0333 ::: bot acc: 0.1188
top acc: 0.0669 ::: bot acc: 0.1651
top acc: 0.0233 ::: bot acc: 0.0810
current epoch: 2
train loss is 0.006958
average val loss: 0.004951, accuracy: 0.0711
average test loss: 0.005069, accuracy: 0.0739
case acc: 0.035365023
case acc: 0.20569953
case acc: 0.03762281
case acc: 0.052927516
case acc: 0.042119235
case acc: 0.0694635
top acc: 0.0263 ::: bot acc: 0.0589
top acc: 0.2434 ::: bot acc: 0.1706
top acc: 0.0692 ::: bot acc: 0.0287
top acc: 0.0949 ::: bot acc: 0.0239
top acc: 0.0405 ::: bot acc: 0.0613
top acc: 0.1219 ::: bot acc: 0.0206
current epoch: 3
train loss is 0.008526
average val loss: 0.013676, accuracy: 0.1455
average test loss: 0.013383, accuracy: 0.1414
case acc: 0.08077818
case acc: 0.28914857
case acc: 0.116267815
case acc: 0.13227488
case acc: 0.07700963
case acc: 0.15321234
top acc: 0.1254 ::: bot acc: 0.0385
top acc: 0.3275 ::: bot acc: 0.2542
top acc: 0.1676 ::: bot acc: 0.0692
top acc: 0.1859 ::: bot acc: 0.0799
top acc: 0.1320 ::: bot acc: 0.0312
top acc: 0.2062 ::: bot acc: 0.1031
current epoch: 4
train loss is 0.012298
average val loss: 0.009865, accuracy: 0.1188
average test loss: 0.009672, accuracy: 0.1154
case acc: 0.061123557
case acc: 0.25363326
case acc: 0.09280262
case acc: 0.10537977
case acc: 0.056945886
case acc: 0.1222239
top acc: 0.1048 ::: bot acc: 0.0212
top acc: 0.2914 ::: bot acc: 0.2180
top acc: 0.1428 ::: bot acc: 0.0475
top acc: 0.1587 ::: bot acc: 0.0542
top acc: 0.1098 ::: bot acc: 0.0159
top acc: 0.1753 ::: bot acc: 0.0713
current epoch: 5
train loss is 0.011862
average val loss: 0.002695, accuracy: 0.0625
average test loss: 0.003008, accuracy: 0.0661
case acc: 0.07167049
case acc: 0.10949447
case acc: 0.053014483
case acc: 0.047122765
case acc: 0.075304806
case acc: 0.039718103
top acc: 0.0289 ::: bot acc: 0.1129
top acc: 0.1467 ::: bot acc: 0.0742
top acc: 0.0216 ::: bot acc: 0.0903
top acc: 0.0303 ::: bot acc: 0.0798
top acc: 0.0311 ::: bot acc: 0.1155
top acc: 0.0404 ::: bot acc: 0.0613
current epoch: 6
train loss is 0.005151
average val loss: 0.002617, accuracy: 0.0629
average test loss: 0.002919, accuracy: 0.0663
case acc: 0.077199295
case acc: 0.08920908
case acc: 0.061369758
case acc: 0.05256915
case acc: 0.07530583
case acc: 0.04230721
top acc: 0.0339 ::: bot acc: 0.1188
top acc: 0.1265 ::: bot acc: 0.0538
top acc: 0.0221 ::: bot acc: 0.1022
top acc: 0.0284 ::: bot acc: 0.0893
top acc: 0.0323 ::: bot acc: 0.1152
top acc: 0.0330 ::: bot acc: 0.0707
current epoch: 7
train loss is 0.003090
average val loss: 0.002357, accuracy: 0.0527
average test loss: 0.002443, accuracy: 0.0548
case acc: 0.040899128
case acc: 0.12522294
case acc: 0.038505044
case acc: 0.038259145
case acc: 0.043925866
case acc: 0.04178145
top acc: 0.0206 ::: bot acc: 0.0710
top acc: 0.1629 ::: bot acc: 0.0888
top acc: 0.0381 ::: bot acc: 0.0603
top acc: 0.0606 ::: bot acc: 0.0459
top acc: 0.0390 ::: bot acc: 0.0643
top acc: 0.0771 ::: bot acc: 0.0263
current epoch: 8
train loss is 0.003499
average val loss: 0.002117, accuracy: 0.0510
average test loss: 0.002217, accuracy: 0.0530
case acc: 0.041249547
case acc: 0.11416755
case acc: 0.040334865
case acc: 0.037996624
case acc: 0.04363907
case acc: 0.04081832
top acc: 0.0193 ::: bot acc: 0.0720
top acc: 0.1518 ::: bot acc: 0.0781
top acc: 0.0331 ::: bot acc: 0.0652
top acc: 0.0558 ::: bot acc: 0.0502
top acc: 0.0391 ::: bot acc: 0.0642
top acc: 0.0734 ::: bot acc: 0.0304
current epoch: 9
train loss is 0.003311
average val loss: 0.001882, accuracy: 0.0504
average test loss: 0.001979, accuracy: 0.0519
case acc: 0.04607355
case acc: 0.09467946
case acc: 0.045474924
case acc: 0.03959174
case acc: 0.046320163
case acc: 0.039146844
top acc: 0.0167 ::: bot acc: 0.0802
top acc: 0.1318 ::: bot acc: 0.0597
top acc: 0.0242 ::: bot acc: 0.0772
top acc: 0.0452 ::: bot acc: 0.0615
top acc: 0.0352 ::: bot acc: 0.0702
top acc: 0.0615 ::: bot acc: 0.0427
current epoch: 10
train loss is 0.002770
average val loss: 0.001746, accuracy: 0.0487
average test loss: 0.001845, accuracy: 0.0501
case acc: 0.044775646
case acc: 0.08743075
case acc: 0.04597413
case acc: 0.039629385
case acc: 0.043974116
case acc: 0.038768567
top acc: 0.0170 ::: bot acc: 0.0785
top acc: 0.1255 ::: bot acc: 0.0522
top acc: 0.0232 ::: bot acc: 0.0793
top acc: 0.0450 ::: bot acc: 0.0616
top acc: 0.0370 ::: bot acc: 0.0653
top acc: 0.0616 ::: bot acc: 0.0421
current epoch: 11
train loss is 0.002515
average val loss: 0.001679, accuracy: 0.0474
average test loss: 0.001731, accuracy: 0.0483
case acc: 0.04168789
case acc: 0.08390679
case acc: 0.04520803
case acc: 0.038698573
case acc: 0.04184289
case acc: 0.038481686
top acc: 0.0187 ::: bot acc: 0.0733
top acc: 0.1220 ::: bot acc: 0.0488
top acc: 0.0246 ::: bot acc: 0.0767
top acc: 0.0484 ::: bot acc: 0.0575
top acc: 0.0425 ::: bot acc: 0.0593
top acc: 0.0640 ::: bot acc: 0.0381
current epoch: 12
train loss is 0.002352
average val loss: 0.001628, accuracy: 0.0463
average test loss: 0.001660, accuracy: 0.0471
case acc: 0.038588915
case acc: 0.081400394
case acc: 0.044061318
case acc: 0.038209233
case acc: 0.04047608
case acc: 0.03998658
top acc: 0.0229 ::: bot acc: 0.0666
top acc: 0.1193 ::: bot acc: 0.0461
top acc: 0.0277 ::: bot acc: 0.0740
top acc: 0.0524 ::: bot acc: 0.0535
top acc: 0.0492 ::: bot acc: 0.0540
top acc: 0.0686 ::: bot acc: 0.0354
current epoch: 13
train loss is 0.002258
average val loss: 0.001553, accuracy: 0.0453
average test loss: 0.001577, accuracy: 0.0458
case acc: 0.03778424
case acc: 0.075879216
case acc: 0.043479912
case acc: 0.038414482
case acc: 0.039712965
case acc: 0.0397446
top acc: 0.0243 ::: bot acc: 0.0651
top acc: 0.1133 ::: bot acc: 0.0406
top acc: 0.0264 ::: bot acc: 0.0741
top acc: 0.0536 ::: bot acc: 0.0533
top acc: 0.0510 ::: bot acc: 0.0510
top acc: 0.0686 ::: bot acc: 0.0352
current epoch: 14
train loss is 0.002086
average val loss: 0.001454, accuracy: 0.0442
average test loss: 0.001492, accuracy: 0.0449
case acc: 0.038202465
case acc: 0.068210535
case acc: 0.04479051
case acc: 0.038861133
case acc: 0.04009385
case acc: 0.038947053
top acc: 0.0237 ::: bot acc: 0.0655
top acc: 0.1060 ::: bot acc: 0.0336
top acc: 0.0245 ::: bot acc: 0.0769
top acc: 0.0506 ::: bot acc: 0.0565
top acc: 0.0491 ::: bot acc: 0.0531
top acc: 0.0655 ::: bot acc: 0.0369
current epoch: 15
train loss is 0.001920
average val loss: 0.001421, accuracy: 0.0435
average test loss: 0.001433, accuracy: 0.0439
case acc: 0.036778644
case acc: 0.0641734
case acc: 0.044781685
case acc: 0.038171332
case acc: 0.03959139
case acc: 0.039658155
top acc: 0.0255 ::: bot acc: 0.0626
top acc: 0.1013 ::: bot acc: 0.0295
top acc: 0.0263 ::: bot acc: 0.0759
top acc: 0.0529 ::: bot acc: 0.0539
top acc: 0.0516 ::: bot acc: 0.0507
top acc: 0.0671 ::: bot acc: 0.0370
current epoch: 16
train loss is 0.001859
average val loss: 0.001360, accuracy: 0.0426
average test loss: 0.001369, accuracy: 0.0429
case acc: 0.0363764
case acc: 0.059322897
case acc: 0.04428699
case acc: 0.038399
case acc: 0.03938996
case acc: 0.039708674
top acc: 0.0276 ::: bot acc: 0.0609
top acc: 0.0963 ::: bot acc: 0.0255
top acc: 0.0261 ::: bot acc: 0.0747
top acc: 0.0539 ::: bot acc: 0.0531
top acc: 0.0513 ::: bot acc: 0.0500
top acc: 0.0673 ::: bot acc: 0.0374
current epoch: 17
train loss is 0.001715
average val loss: 0.001314, accuracy: 0.0419
average test loss: 0.001316, accuracy: 0.0419
case acc: 0.035533145
case acc: 0.054289524
case acc: 0.0444498
case acc: 0.038284726
case acc: 0.039562754
case acc: 0.039242465
top acc: 0.0278 ::: bot acc: 0.0595
top acc: 0.0907 ::: bot acc: 0.0216
top acc: 0.0248 ::: bot acc: 0.0769
top acc: 0.0529 ::: bot acc: 0.0539
top acc: 0.0509 ::: bot acc: 0.0509
top acc: 0.0658 ::: bot acc: 0.0387
current epoch: 18
train loss is 0.001673
average val loss: 0.001281, accuracy: 0.0412
average test loss: 0.001278, accuracy: 0.0412
case acc: 0.03502866
case acc: 0.051310927
case acc: 0.04436617
case acc: 0.0379112
case acc: 0.039594837
case acc: 0.039279435
top acc: 0.0302 ::: bot acc: 0.0576
top acc: 0.0876 ::: bot acc: 0.0196
top acc: 0.0257 ::: bot acc: 0.0756
top acc: 0.0537 ::: bot acc: 0.0526
top acc: 0.0517 ::: bot acc: 0.0505
top acc: 0.0658 ::: bot acc: 0.0380
current epoch: 19
train loss is 0.001619
average val loss: 0.001261, accuracy: 0.0407
average test loss: 0.001255, accuracy: 0.0407
case acc: 0.033997566
case acc: 0.05004131
case acc: 0.04287118
case acc: 0.038241737
case acc: 0.03918078
case acc: 0.039926574
top acc: 0.0336 ::: bot acc: 0.0539
top acc: 0.0858 ::: bot acc: 0.0181
top acc: 0.0274 ::: bot acc: 0.0724
top acc: 0.0565 ::: bot acc: 0.0508
top acc: 0.0534 ::: bot acc: 0.0485
top acc: 0.0681 ::: bot acc: 0.0367
current epoch: 20
train loss is 0.001561
average val loss: 0.001234, accuracy: 0.0402
average test loss: 0.001219, accuracy: 0.0401
case acc: 0.033987563
case acc: 0.047223803
case acc: 0.042446032
case acc: 0.0378941
case acc: 0.03931614
case acc: 0.03964336
top acc: 0.0361 ::: bot acc: 0.0528
top acc: 0.0828 ::: bot acc: 0.0164
top acc: 0.0285 ::: bot acc: 0.0713
top acc: 0.0568 ::: bot acc: 0.0493
top acc: 0.0534 ::: bot acc: 0.0491
top acc: 0.0675 ::: bot acc: 0.0367
current epoch: 21
train loss is 0.001534
average val loss: 0.001224, accuracy: 0.0398
average test loss: 0.001203, accuracy: 0.0397
case acc: 0.033093512
case acc: 0.046865765
case acc: 0.041525725
case acc: 0.038264893
case acc: 0.03902968
case acc: 0.039557658
top acc: 0.0390 ::: bot acc: 0.0493
top acc: 0.0819 ::: bot acc: 0.0162
top acc: 0.0309 ::: bot acc: 0.0686
top acc: 0.0604 ::: bot acc: 0.0467
top acc: 0.0541 ::: bot acc: 0.0483
top acc: 0.0685 ::: bot acc: 0.0347
current epoch: 22
train loss is 0.001516
average val loss: 0.001202, accuracy: 0.0393
average test loss: 0.001179, accuracy: 0.0393
case acc: 0.032796424
case acc: 0.04513803
case acc: 0.04035334
case acc: 0.038389597
case acc: 0.03886929
case acc: 0.040141046
top acc: 0.0410 ::: bot acc: 0.0472
top acc: 0.0797 ::: bot acc: 0.0153
top acc: 0.0324 ::: bot acc: 0.0664
top acc: 0.0609 ::: bot acc: 0.0459
top acc: 0.0537 ::: bot acc: 0.0477
top acc: 0.0693 ::: bot acc: 0.0357
current epoch: 23
train loss is 0.001505
average val loss: 0.001229, accuracy: 0.0397
average test loss: 0.001200, accuracy: 0.0394
case acc: 0.032183565
case acc: 0.046138383
case acc: 0.03918747
case acc: 0.038902987
case acc: 0.038941476
case acc: 0.04094578
top acc: 0.0461 ::: bot acc: 0.0414
top acc: 0.0813 ::: bot acc: 0.0155
top acc: 0.0374 ::: bot acc: 0.0619
top acc: 0.0653 ::: bot acc: 0.0410
top acc: 0.0574 ::: bot acc: 0.0458
top acc: 0.0730 ::: bot acc: 0.0328
current epoch: 24
train loss is 0.001493
average val loss: 0.001239, accuracy: 0.0397
average test loss: 0.001184, accuracy: 0.0391
case acc: 0.032713868
case acc: 0.045473337
case acc: 0.037822925
case acc: 0.039175455
case acc: 0.038617548
case acc: 0.040811207
top acc: 0.0491 ::: bot acc: 0.0397
top acc: 0.0804 ::: bot acc: 0.0150
top acc: 0.0394 ::: bot acc: 0.0580
top acc: 0.0673 ::: bot acc: 0.0389
top acc: 0.0583 ::: bot acc: 0.0444
top acc: 0.0731 ::: bot acc: 0.0309
current epoch: 25
train loss is 0.001487
average val loss: 0.001260, accuracy: 0.0397
average test loss: 0.001207, accuracy: 0.0395
case acc: 0.032978885
case acc: 0.045842923
case acc: 0.03767204
case acc: 0.040271882
case acc: 0.038707998
case acc: 0.041555587
top acc: 0.0524 ::: bot acc: 0.0361
top acc: 0.0806 ::: bot acc: 0.0160
top acc: 0.0430 ::: bot acc: 0.0558
top acc: 0.0701 ::: bot acc: 0.0366
top acc: 0.0602 ::: bot acc: 0.0433
top acc: 0.0752 ::: bot acc: 0.0295
current epoch: 26
train loss is 0.001465
average val loss: 0.001220, accuracy: 0.0392
average test loss: 0.001168, accuracy: 0.0388
case acc: 0.03288751
case acc: 0.042499714
case acc: 0.037948165
case acc: 0.039896358
case acc: 0.038732875
case acc: 0.040728856
top acc: 0.0524 ::: bot acc: 0.0367
top acc: 0.0769 ::: bot acc: 0.0137
top acc: 0.0428 ::: bot acc: 0.0565
top acc: 0.0693 ::: bot acc: 0.0373
top acc: 0.0579 ::: bot acc: 0.0444
top acc: 0.0736 ::: bot acc: 0.0310
current epoch: 27
train loss is 0.001437
average val loss: 0.001198, accuracy: 0.0388
average test loss: 0.001140, accuracy: 0.0383
case acc: 0.032507356
case acc: 0.040351454
case acc: 0.03765652
case acc: 0.03998116
case acc: 0.038971703
case acc: 0.04038135
top acc: 0.0514 ::: bot acc: 0.0362
top acc: 0.0736 ::: bot acc: 0.0129
top acc: 0.0426 ::: bot acc: 0.0563
top acc: 0.0692 ::: bot acc: 0.0381
top acc: 0.0569 ::: bot acc: 0.0459
top acc: 0.0712 ::: bot acc: 0.0330
current epoch: 28
train loss is 0.001406
average val loss: 0.001190, accuracy: 0.0386
average test loss: 0.001136, accuracy: 0.0381
case acc: 0.033123173
case acc: 0.038798768
case acc: 0.03766436
case acc: 0.039901096
case acc: 0.038855147
case acc: 0.040430933
top acc: 0.0523 ::: bot acc: 0.0368
top acc: 0.0724 ::: bot acc: 0.0119
top acc: 0.0441 ::: bot acc: 0.0552
top acc: 0.0694 ::: bot acc: 0.0379
top acc: 0.0569 ::: bot acc: 0.0458
top acc: 0.0714 ::: bot acc: 0.0339
current epoch: 29
train loss is 0.001385
average val loss: 0.001216, accuracy: 0.0390
average test loss: 0.001149, accuracy: 0.0383
case acc: 0.0332685
case acc: 0.039354656
case acc: 0.037174277
case acc: 0.040775936
case acc: 0.038644016
case acc: 0.040412992
top acc: 0.0549 ::: bot acc: 0.0328
top acc: 0.0734 ::: bot acc: 0.0115
top acc: 0.0461 ::: bot acc: 0.0529
top acc: 0.0718 ::: bot acc: 0.0352
top acc: 0.0584 ::: bot acc: 0.0441
top acc: 0.0726 ::: bot acc: 0.0312
current epoch: 30
train loss is 0.001401
average val loss: 0.001198, accuracy: 0.0386
average test loss: 0.001141, accuracy: 0.0382
case acc: 0.033562794
case acc: 0.038589977
case acc: 0.036854994
case acc: 0.040840242
case acc: 0.038819786
case acc: 0.04044826
top acc: 0.0558 ::: bot acc: 0.0329
top acc: 0.0717 ::: bot acc: 0.0124
top acc: 0.0469 ::: bot acc: 0.0515
top acc: 0.0722 ::: bot acc: 0.0352
top acc: 0.0587 ::: bot acc: 0.0440
top acc: 0.0720 ::: bot acc: 0.0323
current epoch: 31
train loss is 0.001375
average val loss: 0.001214, accuracy: 0.0388
average test loss: 0.001145, accuracy: 0.0382
case acc: 0.03348514
case acc: 0.038565952
case acc: 0.036947288
case acc: 0.041035485
case acc: 0.03850636
case acc: 0.040688735
top acc: 0.0574 ::: bot acc: 0.0309
top acc: 0.0719 ::: bot acc: 0.0124
top acc: 0.0492 ::: bot acc: 0.0501
top acc: 0.0733 ::: bot acc: 0.0331
top acc: 0.0590 ::: bot acc: 0.0433
top acc: 0.0724 ::: bot acc: 0.0322
current epoch: 32
train loss is 0.001374
average val loss: 0.001214, accuracy: 0.0388
average test loss: 0.001142, accuracy: 0.0382
case acc: 0.033757187
case acc: 0.037432346
case acc: 0.036880378
case acc: 0.04146845
case acc: 0.038779356
case acc: 0.04061539
top acc: 0.0581 ::: bot acc: 0.0308
top acc: 0.0702 ::: bot acc: 0.0117
top acc: 0.0506 ::: bot acc: 0.0484
top acc: 0.0738 ::: bot acc: 0.0337
top acc: 0.0589 ::: bot acc: 0.0437
top acc: 0.0723 ::: bot acc: 0.0329
current epoch: 33
train loss is 0.001379
average val loss: 0.001225, accuracy: 0.0388
average test loss: 0.001156, accuracy: 0.0382
case acc: 0.034024145
case acc: 0.037483554
case acc: 0.036611434
case acc: 0.04201448
case acc: 0.038487893
case acc: 0.04071473
top acc: 0.0594 ::: bot acc: 0.0288
top acc: 0.0706 ::: bot acc: 0.0111
top acc: 0.0522 ::: bot acc: 0.0465
top acc: 0.0759 ::: bot acc: 0.0314
top acc: 0.0606 ::: bot acc: 0.0421
top acc: 0.0733 ::: bot acc: 0.0310
current epoch: 34
train loss is 0.001361
average val loss: 0.001224, accuracy: 0.0389
average test loss: 0.001135, accuracy: 0.0379
case acc: 0.0336449
case acc: 0.036372177
case acc: 0.036820903
case acc: 0.04179595
case acc: 0.038531236
case acc: 0.040257595
top acc: 0.0586 ::: bot acc: 0.0294
top acc: 0.0688 ::: bot acc: 0.0113
top acc: 0.0525 ::: bot acc: 0.0471
top acc: 0.0748 ::: bot acc: 0.0320
top acc: 0.0601 ::: bot acc: 0.0427
top acc: 0.0719 ::: bot acc: 0.0324
current epoch: 35
train loss is 0.001327
average val loss: 0.001194, accuracy: 0.0384
average test loss: 0.001126, accuracy: 0.0377
case acc: 0.034132395
case acc: 0.03500364
case acc: 0.03649191
case acc: 0.04181129
case acc: 0.038485955
case acc: 0.040451273
top acc: 0.0583 ::: bot acc: 0.0310
top acc: 0.0668 ::: bot acc: 0.0116
top acc: 0.0509 ::: bot acc: 0.0472
top acc: 0.0750 ::: bot acc: 0.0328
top acc: 0.0594 ::: bot acc: 0.0436
top acc: 0.0712 ::: bot acc: 0.0340
current epoch: 36
train loss is 0.001321
average val loss: 0.001188, accuracy: 0.0383
average test loss: 0.001121, accuracy: 0.0376
case acc: 0.033830043
case acc: 0.03477785
case acc: 0.036611933
case acc: 0.041742746
case acc: 0.038387947
case acc: 0.04027495
top acc: 0.0588 ::: bot acc: 0.0297
top acc: 0.0661 ::: bot acc: 0.0116
top acc: 0.0522 ::: bot acc: 0.0464
top acc: 0.0749 ::: bot acc: 0.0325
top acc: 0.0596 ::: bot acc: 0.0430
top acc: 0.0710 ::: bot acc: 0.0337
current epoch: 37
train loss is 0.001307
average val loss: 0.001168, accuracy: 0.0379
average test loss: 0.001100, accuracy: 0.0374
case acc: 0.03362488
case acc: 0.03354045
case acc: 0.03689424
case acc: 0.04142201
case acc: 0.039015237
case acc: 0.039952967
top acc: 0.0570 ::: bot acc: 0.0315
top acc: 0.0635 ::: bot acc: 0.0131
top acc: 0.0511 ::: bot acc: 0.0479
top acc: 0.0743 ::: bot acc: 0.0330
top acc: 0.0587 ::: bot acc: 0.0446
top acc: 0.0692 ::: bot acc: 0.0351
current epoch: 38
train loss is 0.001284
average val loss: 0.001169, accuracy: 0.0379
average test loss: 0.001107, accuracy: 0.0375
case acc: 0.0336982
case acc: 0.033334274
case acc: 0.036920607
case acc: 0.04202266
case acc: 0.038831696
case acc: 0.039913736
top acc: 0.0576 ::: bot acc: 0.0307
top acc: 0.0634 ::: bot acc: 0.0127
top acc: 0.0518 ::: bot acc: 0.0475
top acc: 0.0749 ::: bot acc: 0.0330
top acc: 0.0595 ::: bot acc: 0.0438
top acc: 0.0700 ::: bot acc: 0.0343
current epoch: 39
train loss is 0.001283
average val loss: 0.001142, accuracy: 0.0374
average test loss: 0.001080, accuracy: 0.0370
case acc: 0.033438157
case acc: 0.031929426
case acc: 0.036749613
case acc: 0.041234415
case acc: 0.038855843
case acc: 0.039921597
top acc: 0.0552 ::: bot acc: 0.0328
top acc: 0.0608 ::: bot acc: 0.0144
top acc: 0.0503 ::: bot acc: 0.0483
top acc: 0.0735 ::: bot acc: 0.0338
top acc: 0.0576 ::: bot acc: 0.0453
top acc: 0.0684 ::: bot acc: 0.0364
current epoch: 40
train loss is 0.001262
average val loss: 0.001141, accuracy: 0.0372
average test loss: 0.001078, accuracy: 0.0369
case acc: 0.033291467
case acc: 0.03139601
case acc: 0.03663866
case acc: 0.041541636
case acc: 0.03870368
case acc: 0.03993062
top acc: 0.0556 ::: bot acc: 0.0328
top acc: 0.0596 ::: bot acc: 0.0148
top acc: 0.0500 ::: bot acc: 0.0487
top acc: 0.0740 ::: bot acc: 0.0330
top acc: 0.0580 ::: bot acc: 0.0447
top acc: 0.0688 ::: bot acc: 0.0357
current epoch: 41
train loss is 0.001242
average val loss: 0.001088, accuracy: 0.0364
average test loss: 0.001044, accuracy: 0.0365
case acc: 0.033049725
case acc: 0.029703738
case acc: 0.036841746
case acc: 0.040297996
case acc: 0.039517872
case acc: 0.03952813
top acc: 0.0522 ::: bot acc: 0.0366
top acc: 0.0554 ::: bot acc: 0.0179
top acc: 0.0465 ::: bot acc: 0.0516
top acc: 0.0709 ::: bot acc: 0.0360
top acc: 0.0543 ::: bot acc: 0.0489
top acc: 0.0655 ::: bot acc: 0.0392
current epoch: 42
train loss is 0.001196
average val loss: 0.001063, accuracy: 0.0361
average test loss: 0.001026, accuracy: 0.0362
case acc: 0.032483518
case acc: 0.028716382
case acc: 0.03733779
case acc: 0.039833378
case acc: 0.039876316
case acc: 0.039076835
top acc: 0.0497 ::: bot acc: 0.0385
top acc: 0.0526 ::: bot acc: 0.0208
top acc: 0.0451 ::: bot acc: 0.0539
top acc: 0.0689 ::: bot acc: 0.0380
top acc: 0.0525 ::: bot acc: 0.0512
top acc: 0.0628 ::: bot acc: 0.0412
current epoch: 43
train loss is 0.001179
average val loss: 0.001044, accuracy: 0.0357
average test loss: 0.001025, accuracy: 0.0362
case acc: 0.032952253
case acc: 0.028223893
case acc: 0.037394937
case acc: 0.039715953
case acc: 0.040129017
case acc: 0.03898176
top acc: 0.0480 ::: bot acc: 0.0413
top acc: 0.0507 ::: bot acc: 0.0231
top acc: 0.0432 ::: bot acc: 0.0551
top acc: 0.0678 ::: bot acc: 0.0396
top acc: 0.0503 ::: bot acc: 0.0530
top acc: 0.0612 ::: bot acc: 0.0430
current epoch: 44
train loss is 0.001166
average val loss: 0.001030, accuracy: 0.0355
average test loss: 0.001015, accuracy: 0.0360
case acc: 0.03271268
case acc: 0.027392294
case acc: 0.037766773
case acc: 0.03910607
case acc: 0.040045287
case acc: 0.039074175
top acc: 0.0467 ::: bot acc: 0.0426
top acc: 0.0483 ::: bot acc: 0.0237
top acc: 0.0431 ::: bot acc: 0.0560
top acc: 0.0668 ::: bot acc: 0.0402
top acc: 0.0488 ::: bot acc: 0.0537
top acc: 0.0604 ::: bot acc: 0.0442
current epoch: 45
train loss is 0.001154
average val loss: 0.001031, accuracy: 0.0357
average test loss: 0.001012, accuracy: 0.0360
case acc: 0.032598916
case acc: 0.027551917
case acc: 0.037431564
case acc: 0.03917079
case acc: 0.040514886
case acc: 0.039020825
top acc: 0.0460 ::: bot acc: 0.0429
top acc: 0.0478 ::: bot acc: 0.0255
top acc: 0.0417 ::: bot acc: 0.0563
top acc: 0.0663 ::: bot acc: 0.0409
top acc: 0.0482 ::: bot acc: 0.0546
top acc: 0.0596 ::: bot acc: 0.0449
current epoch: 46
train loss is 0.001152
average val loss: 0.001043, accuracy: 0.0358
average test loss: 0.001019, accuracy: 0.0362
case acc: 0.032592237
case acc: 0.028233225
case acc: 0.037379522
case acc: 0.03963216
case acc: 0.040216573
case acc: 0.03916066
top acc: 0.0474 ::: bot acc: 0.0407
top acc: 0.0495 ::: bot acc: 0.0242
top acc: 0.0441 ::: bot acc: 0.0546
top acc: 0.0679 ::: bot acc: 0.0390
top acc: 0.0502 ::: bot acc: 0.0530
top acc: 0.0612 ::: bot acc: 0.0436
current epoch: 47
train loss is 0.001154
average val loss: 0.001039, accuracy: 0.0357
average test loss: 0.001013, accuracy: 0.0360
case acc: 0.032566134
case acc: 0.027620373
case acc: 0.037172616
case acc: 0.03955406
case acc: 0.039996266
case acc: 0.038949624
top acc: 0.0474 ::: bot acc: 0.0413
top acc: 0.0484 ::: bot acc: 0.0245
top acc: 0.0435 ::: bot acc: 0.0549
top acc: 0.0678 ::: bot acc: 0.0396
top acc: 0.0493 ::: bot acc: 0.0531
top acc: 0.0608 ::: bot acc: 0.0438
current epoch: 48
train loss is 0.001146
average val loss: 0.001034, accuracy: 0.0355
average test loss: 0.001019, accuracy: 0.0362
case acc: 0.03270515
case acc: 0.02790083
case acc: 0.037422217
case acc: 0.03966418
case acc: 0.040181857
case acc: 0.039038885
top acc: 0.0477 ::: bot acc: 0.0411
top acc: 0.0493 ::: bot acc: 0.0244
top acc: 0.0444 ::: bot acc: 0.0544
top acc: 0.0685 ::: bot acc: 0.0384
top acc: 0.0507 ::: bot acc: 0.0524
top acc: 0.0612 ::: bot acc: 0.0435
current epoch: 49
train loss is 0.001149
average val loss: 0.001036, accuracy: 0.0355
average test loss: 0.001015, accuracy: 0.0361
case acc: 0.032660853
case acc: 0.02780442
case acc: 0.037235975
case acc: 0.039592363
case acc: 0.040066563
case acc: 0.03919044
top acc: 0.0474 ::: bot acc: 0.0414
top acc: 0.0484 ::: bot acc: 0.0252
top acc: 0.0442 ::: bot acc: 0.0543
top acc: 0.0680 ::: bot acc: 0.0388
top acc: 0.0504 ::: bot acc: 0.0525
top acc: 0.0611 ::: bot acc: 0.0440
current epoch: 50
train loss is 0.001140
average val loss: 0.001030, accuracy: 0.0355
average test loss: 0.001017, accuracy: 0.0361
case acc: 0.03284245
case acc: 0.027380122
case acc: 0.03787798
case acc: 0.03910824
case acc: 0.040141746
case acc: 0.039204717
top acc: 0.0461 ::: bot acc: 0.0431
top acc: 0.0473 ::: bot acc: 0.0260
top acc: 0.0438 ::: bot acc: 0.0553
top acc: 0.0668 ::: bot acc: 0.0393
top acc: 0.0496 ::: bot acc: 0.0532
top acc: 0.0601 ::: bot acc: 0.0448
LME_Co_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5400 5400 5400
1.7082474 -0.6288155 0.16982092 -0.17269358
Validation: 606 606 606
Testing: 750 750 750
pre-processing time: 0.0001964569091796875
the split date is 2012-01-01
train dropout: 0.2 test dropout: 0.15
net initializing with time: 0.002204418182373047
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.008083
average val loss: 0.005567, accuracy: 0.0936
average test loss: 0.004144, accuracy: 0.0768
case acc: 0.10659257
case acc: 0.12757123
case acc: 0.060873006
case acc: 0.049642503
case acc: 0.07080565
case acc: 0.04501791
top acc: 0.0597 ::: bot acc: 0.1563
top acc: 0.1677 ::: bot acc: 0.0820
top acc: 0.0158 ::: bot acc: 0.1165
top acc: 0.0211 ::: bot acc: 0.0896
top acc: 0.0266 ::: bot acc: 0.1132
top acc: 0.0455 ::: bot acc: 0.0693
current epoch: 2
train loss is 0.007109
average val loss: 0.005694, accuracy: 0.0783
average test loss: 0.007267, accuracy: 0.0934
case acc: 0.03737573
case acc: 0.23300023
case acc: 0.072557874
case acc: 0.07708008
case acc: 0.048904166
case acc: 0.091482684
top acc: 0.0603 ::: bot acc: 0.0362
top acc: 0.2733 ::: bot acc: 0.1890
top acc: 0.1274 ::: bot acc: 0.0221
top acc: 0.1225 ::: bot acc: 0.0317
top acc: 0.0955 ::: bot acc: 0.0142
top acc: 0.1532 ::: bot acc: 0.0361
current epoch: 3
train loss is 0.008886
average val loss: 0.014298, accuracy: 0.1473
average test loss: 0.018075, accuracy: 0.1734
case acc: 0.107149996
case acc: 0.3115201
case acc: 0.15880255
case acc: 0.16148868
case acc: 0.12919244
case acc: 0.17221195
top acc: 0.1530 ::: bot acc: 0.0584
top acc: 0.3514 ::: bot acc: 0.2667
top acc: 0.2203 ::: bot acc: 0.0952
top acc: 0.2089 ::: bot acc: 0.1119
top acc: 0.1828 ::: bot acc: 0.0818
top acc: 0.2332 ::: bot acc: 0.1168
current epoch: 4
train loss is 0.012422
average val loss: 0.007862, accuracy: 0.0995
average test loss: 0.010414, accuracy: 0.1232
case acc: 0.064131156
case acc: 0.25192386
case acc: 0.11095524
case acc: 0.11035831
case acc: 0.08359648
case acc: 0.1184676
top acc: 0.1095 ::: bot acc: 0.0192
top acc: 0.2929 ::: bot acc: 0.2056
top acc: 0.1711 ::: bot acc: 0.0490
top acc: 0.1577 ::: bot acc: 0.0610
top acc: 0.1383 ::: bot acc: 0.0355
top acc: 0.1794 ::: bot acc: 0.0622
current epoch: 5
train loss is 0.010755
average val loss: 0.003261, accuracy: 0.0693
average test loss: 0.002629, accuracy: 0.0588
case acc: 0.059377123
case acc: 0.11721476
case acc: 0.047672894
case acc: 0.036759257
case acc: 0.047465306
case acc: 0.0444105
top acc: 0.0159 ::: bot acc: 0.1063
top acc: 0.1567 ::: bot acc: 0.0727
top acc: 0.0467 ::: bot acc: 0.0782
top acc: 0.0350 ::: bot acc: 0.0619
top acc: 0.0272 ::: bot acc: 0.0788
top acc: 0.0547 ::: bot acc: 0.0621
current epoch: 6
train loss is 0.004377
average val loss: 0.002736, accuracy: 0.0622
average test loss: 0.002419, accuracy: 0.0553
case acc: 0.048758384
case acc: 0.11802106
case acc: 0.04694437
case acc: 0.034957755
case acc: 0.039406992
case acc: 0.043420345
top acc: 0.0122 ::: bot acc: 0.0918
top acc: 0.1578 ::: bot acc: 0.0729
top acc: 0.0539 ::: bot acc: 0.0695
top acc: 0.0446 ::: bot acc: 0.0517
top acc: 0.0431 ::: bot acc: 0.0586
top acc: 0.0665 ::: bot acc: 0.0504
current epoch: 7
train loss is 0.003115
average val loss: 0.002586, accuracy: 0.0548
average test loss: 0.003258, accuracy: 0.0623
case acc: 0.035254408
case acc: 0.14622909
case acc: 0.051381048
case acc: 0.04590678
case acc: 0.042825665
case acc: 0.052326795
top acc: 0.0431 ::: bot acc: 0.0518
top acc: 0.1861 ::: bot acc: 0.1012
top acc: 0.0899 ::: bot acc: 0.0357
top acc: 0.0813 ::: bot acc: 0.0213
top acc: 0.0865 ::: bot acc: 0.0176
top acc: 0.1027 ::: bot acc: 0.0175
current epoch: 8
train loss is 0.003535
average val loss: 0.002289, accuracy: 0.0525
average test loss: 0.002849, accuracy: 0.0587
case acc: 0.03509612
case acc: 0.13308117
case acc: 0.049428005
case acc: 0.04245223
case acc: 0.04242406
case acc: 0.049716514
top acc: 0.0395 ::: bot acc: 0.0549
top acc: 0.1728 ::: bot acc: 0.0880
top acc: 0.0819 ::: bot acc: 0.0417
top acc: 0.0761 ::: bot acc: 0.0238
top acc: 0.0850 ::: bot acc: 0.0184
top acc: 0.0971 ::: bot acc: 0.0214
current epoch: 9
train loss is 0.003227
average val loss: 0.002036, accuracy: 0.0512
average test loss: 0.002347, accuracy: 0.0539
case acc: 0.03626106
case acc: 0.11491513
case acc: 0.047727402
case acc: 0.03866499
case acc: 0.040127635
case acc: 0.045469597
top acc: 0.0318 ::: bot acc: 0.0634
top acc: 0.1549 ::: bot acc: 0.0701
top acc: 0.0709 ::: bot acc: 0.0530
top acc: 0.0663 ::: bot acc: 0.0313
top acc: 0.0796 ::: bot acc: 0.0225
top acc: 0.0860 ::: bot acc: 0.0306
current epoch: 10
train loss is 0.002726
average val loss: 0.001888, accuracy: 0.0494
average test loss: 0.002230, accuracy: 0.0530
case acc: 0.03592252
case acc: 0.10753543
case acc: 0.04773968
case acc: 0.039048947
case acc: 0.042188484
case acc: 0.04544831
top acc: 0.0343 ::: bot acc: 0.0610
top acc: 0.1475 ::: bot acc: 0.0629
top acc: 0.0701 ::: bot acc: 0.0541
top acc: 0.0660 ::: bot acc: 0.0318
top acc: 0.0841 ::: bot acc: 0.0199
top acc: 0.0870 ::: bot acc: 0.0296
current epoch: 11
train loss is 0.002473
average val loss: 0.001791, accuracy: 0.0477
average test loss: 0.002250, accuracy: 0.0535
case acc: 0.035138298
case acc: 0.105214015
case acc: 0.047523428
case acc: 0.040793903
case acc: 0.045068394
case acc: 0.047000237
top acc: 0.0401 ::: bot acc: 0.0543
top acc: 0.1448 ::: bot acc: 0.0606
top acc: 0.0725 ::: bot acc: 0.0506
top acc: 0.0715 ::: bot acc: 0.0285
top acc: 0.0906 ::: bot acc: 0.0149
top acc: 0.0903 ::: bot acc: 0.0267
current epoch: 12
train loss is 0.002271
average val loss: 0.001699, accuracy: 0.0463
average test loss: 0.002205, accuracy: 0.0532
case acc: 0.03480937
case acc: 0.100557365
case acc: 0.04790215
case acc: 0.04144093
case acc: 0.047419995
case acc: 0.047220312
top acc: 0.0446 ::: bot acc: 0.0496
top acc: 0.1406 ::: bot acc: 0.0564
top acc: 0.0744 ::: bot acc: 0.0490
top acc: 0.0731 ::: bot acc: 0.0264
top acc: 0.0948 ::: bot acc: 0.0135
top acc: 0.0924 ::: bot acc: 0.0244
current epoch: 13
train loss is 0.002181
average val loss: 0.001623, accuracy: 0.0454
average test loss: 0.002143, accuracy: 0.0527
case acc: 0.034601483
case acc: 0.09552361
case acc: 0.048087876
case acc: 0.041467633
case acc: 0.048895378
case acc: 0.047650266
top acc: 0.0469 ::: bot acc: 0.0466
top acc: 0.1353 ::: bot acc: 0.0509
top acc: 0.0750 ::: bot acc: 0.0487
top acc: 0.0736 ::: bot acc: 0.0257
top acc: 0.0969 ::: bot acc: 0.0138
top acc: 0.0931 ::: bot acc: 0.0247
current epoch: 14
train loss is 0.002004
average val loss: 0.001516, accuracy: 0.0442
average test loss: 0.001960, accuracy: 0.0506
case acc: 0.034809843
case acc: 0.08705345
case acc: 0.047222313
case acc: 0.0407122
case acc: 0.047716297
case acc: 0.046357036
top acc: 0.0460 ::: bot acc: 0.0481
top acc: 0.1263 ::: bot acc: 0.0430
top acc: 0.0714 ::: bot acc: 0.0515
top acc: 0.0703 ::: bot acc: 0.0286
top acc: 0.0950 ::: bot acc: 0.0141
top acc: 0.0895 ::: bot acc: 0.0278
current epoch: 15
train loss is 0.001837
average val loss: 0.001460, accuracy: 0.0434
average test loss: 0.001910, accuracy: 0.0501
case acc: 0.034921084
case acc: 0.082275204
case acc: 0.047728587
case acc: 0.040910766
case acc: 0.04814397
case acc: 0.046713922
top acc: 0.0484 ::: bot acc: 0.0458
top acc: 0.1213 ::: bot acc: 0.0387
top acc: 0.0725 ::: bot acc: 0.0516
top acc: 0.0718 ::: bot acc: 0.0276
top acc: 0.0966 ::: bot acc: 0.0126
top acc: 0.0895 ::: bot acc: 0.0278
current epoch: 16
train loss is 0.001749
average val loss: 0.001401, accuracy: 0.0424
average test loss: 0.001886, accuracy: 0.0500
case acc: 0.035455406
case acc: 0.07846494
case acc: 0.04784287
case acc: 0.041358706
case acc: 0.04946789
case acc: 0.047144085
top acc: 0.0513 ::: bot acc: 0.0432
top acc: 0.1177 ::: bot acc: 0.0356
top acc: 0.0731 ::: bot acc: 0.0508
top acc: 0.0733 ::: bot acc: 0.0254
top acc: 0.0977 ::: bot acc: 0.0137
top acc: 0.0909 ::: bot acc: 0.0274
current epoch: 17
train loss is 0.001646
average val loss: 0.001360, accuracy: 0.0416
average test loss: 0.001906, accuracy: 0.0504
case acc: 0.035820782
case acc: 0.07662131
case acc: 0.048043188
case acc: 0.043143284
case acc: 0.051274203
case acc: 0.047396652
top acc: 0.0557 ::: bot acc: 0.0385
top acc: 0.1156 ::: bot acc: 0.0344
top acc: 0.0759 ::: bot acc: 0.0476
top acc: 0.0761 ::: bot acc: 0.0245
top acc: 0.1002 ::: bot acc: 0.0139
top acc: 0.0921 ::: bot acc: 0.0249
current epoch: 18
train loss is 0.001594
average val loss: 0.001330, accuracy: 0.0412
average test loss: 0.001926, accuracy: 0.0507
case acc: 0.03637423
case acc: 0.07492187
case acc: 0.04860865
case acc: 0.04399954
case acc: 0.05220356
case acc: 0.04836517
top acc: 0.0592 ::: bot acc: 0.0348
top acc: 0.1133 ::: bot acc: 0.0335
top acc: 0.0781 ::: bot acc: 0.0453
top acc: 0.0784 ::: bot acc: 0.0226
top acc: 0.1022 ::: bot acc: 0.0141
top acc: 0.0939 ::: bot acc: 0.0244
current epoch: 19
train loss is 0.001545
average val loss: 0.001280, accuracy: 0.0405
average test loss: 0.001877, accuracy: 0.0502
case acc: 0.03686365
case acc: 0.07142589
case acc: 0.04895083
case acc: 0.044441763
case acc: 0.051717747
case acc: 0.047781076
top acc: 0.0603 ::: bot acc: 0.0345
top acc: 0.1093 ::: bot acc: 0.0309
top acc: 0.0789 ::: bot acc: 0.0455
top acc: 0.0785 ::: bot acc: 0.0240
top acc: 0.1014 ::: bot acc: 0.0137
top acc: 0.0933 ::: bot acc: 0.0248
current epoch: 20
train loss is 0.001496
average val loss: 0.001273, accuracy: 0.0404
average test loss: 0.001869, accuracy: 0.0500
case acc: 0.03752057
case acc: 0.069000274
case acc: 0.048923183
case acc: 0.04504529
case acc: 0.05163111
case acc: 0.048069134
top acc: 0.0628 ::: bot acc: 0.0322
top acc: 0.1069 ::: bot acc: 0.0296
top acc: 0.0810 ::: bot acc: 0.0425
top acc: 0.0801 ::: bot acc: 0.0220
top acc: 0.1015 ::: bot acc: 0.0137
top acc: 0.0935 ::: bot acc: 0.0247
current epoch: 21
train loss is 0.001466
average val loss: 0.001233, accuracy: 0.0397
average test loss: 0.001863, accuracy: 0.0500
case acc: 0.037744798
case acc: 0.06768319
case acc: 0.049234
case acc: 0.04588454
case acc: 0.051314868
case acc: 0.04801759
top acc: 0.0648 ::: bot acc: 0.0293
top acc: 0.1051 ::: bot acc: 0.0289
top acc: 0.0825 ::: bot acc: 0.0410
top acc: 0.0820 ::: bot acc: 0.0215
top acc: 0.1007 ::: bot acc: 0.0134
top acc: 0.0935 ::: bot acc: 0.0246
current epoch: 22
train loss is 0.001387
average val loss: 0.001217, accuracy: 0.0395
average test loss: 0.001883, accuracy: 0.0504
case acc: 0.03865647
case acc: 0.06648545
case acc: 0.049880836
case acc: 0.046973083
case acc: 0.05218416
case acc: 0.048260055
top acc: 0.0669 ::: bot acc: 0.0268
top acc: 0.1037 ::: bot acc: 0.0285
top acc: 0.0850 ::: bot acc: 0.0389
top acc: 0.0835 ::: bot acc: 0.0208
top acc: 0.1019 ::: bot acc: 0.0139
top acc: 0.0940 ::: bot acc: 0.0241
current epoch: 23
train loss is 0.001389
average val loss: 0.001241, accuracy: 0.0398
average test loss: 0.001973, accuracy: 0.0517
case acc: 0.040613316
case acc: 0.06661831
case acc: 0.051150873
case acc: 0.049021333
case acc: 0.053608675
case acc: 0.048974324
top acc: 0.0718 ::: bot acc: 0.0239
top acc: 0.1036 ::: bot acc: 0.0282
top acc: 0.0898 ::: bot acc: 0.0350
top acc: 0.0869 ::: bot acc: 0.0205
top acc: 0.1042 ::: bot acc: 0.0135
top acc: 0.0955 ::: bot acc: 0.0226
current epoch: 24
train loss is 0.001366
average val loss: 0.001236, accuracy: 0.0398
average test loss: 0.002008, accuracy: 0.0522
case acc: 0.04202377
case acc: 0.065813154
case acc: 0.05163464
case acc: 0.05000453
case acc: 0.054033317
case acc: 0.04940945
top acc: 0.0741 ::: bot acc: 0.0231
top acc: 0.1027 ::: bot acc: 0.0280
top acc: 0.0915 ::: bot acc: 0.0320
top acc: 0.0887 ::: bot acc: 0.0200
top acc: 0.1049 ::: bot acc: 0.0137
top acc: 0.0967 ::: bot acc: 0.0217
current epoch: 25
train loss is 0.001372
average val loss: 0.001240, accuracy: 0.0399
average test loss: 0.002068, accuracy: 0.0530
case acc: 0.04291934
case acc: 0.065649085
case acc: 0.052827034
case acc: 0.051623534
case acc: 0.055279136
case acc: 0.049736135
top acc: 0.0767 ::: bot acc: 0.0206
top acc: 0.1021 ::: bot acc: 0.0277
top acc: 0.0938 ::: bot acc: 0.0301
top acc: 0.0917 ::: bot acc: 0.0196
top acc: 0.1065 ::: bot acc: 0.0144
top acc: 0.0977 ::: bot acc: 0.0210
current epoch: 26
train loss is 0.001368
average val loss: 0.001234, accuracy: 0.0398
average test loss: 0.002054, accuracy: 0.0529
case acc: 0.043882355
case acc: 0.06414637
case acc: 0.05316712
case acc: 0.051349636
case acc: 0.05522946
case acc: 0.04968377
top acc: 0.0781 ::: bot acc: 0.0207
top acc: 0.1009 ::: bot acc: 0.0271
top acc: 0.0955 ::: bot acc: 0.0285
top acc: 0.0911 ::: bot acc: 0.0192
top acc: 0.1061 ::: bot acc: 0.0146
top acc: 0.0974 ::: bot acc: 0.0213
current epoch: 27
train loss is 0.001321
average val loss: 0.001191, accuracy: 0.0391
average test loss: 0.001997, accuracy: 0.0522
case acc: 0.043747008
case acc: 0.061705593
case acc: 0.053057052
case acc: 0.051504787
case acc: 0.054238018
case acc: 0.048677757
top acc: 0.0772 ::: bot acc: 0.0209
top acc: 0.0974 ::: bot acc: 0.0265
top acc: 0.0958 ::: bot acc: 0.0285
top acc: 0.0911 ::: bot acc: 0.0194
top acc: 0.1044 ::: bot acc: 0.0142
top acc: 0.0953 ::: bot acc: 0.0218
current epoch: 28
train loss is 0.001278
average val loss: 0.001163, accuracy: 0.0386
average test loss: 0.001943, accuracy: 0.0514
case acc: 0.043518674
case acc: 0.05961286
case acc: 0.053102493
case acc: 0.05092526
case acc: 0.053062383
case acc: 0.048154574
top acc: 0.0772 ::: bot acc: 0.0216
top acc: 0.0944 ::: bot acc: 0.0262
top acc: 0.0955 ::: bot acc: 0.0290
top acc: 0.0905 ::: bot acc: 0.0195
top acc: 0.1035 ::: bot acc: 0.0139
top acc: 0.0933 ::: bot acc: 0.0245
current epoch: 29
train loss is 0.001257
average val loss: 0.001159, accuracy: 0.0385
average test loss: 0.001954, accuracy: 0.0515
case acc: 0.04392437
case acc: 0.05869154
case acc: 0.053562123
case acc: 0.051428784
case acc: 0.053408276
case acc: 0.047964856
top acc: 0.0780 ::: bot acc: 0.0203
top acc: 0.0930 ::: bot acc: 0.0259
top acc: 0.0964 ::: bot acc: 0.0280
top acc: 0.0911 ::: bot acc: 0.0194
top acc: 0.1045 ::: bot acc: 0.0139
top acc: 0.0933 ::: bot acc: 0.0235
current epoch: 30
train loss is 0.001253
average val loss: 0.001183, accuracy: 0.0389
average test loss: 0.002004, accuracy: 0.0523
case acc: 0.044562817
case acc: 0.05898772
case acc: 0.054742903
case acc: 0.05253358
case acc: 0.054113243
case acc: 0.048605647
top acc: 0.0795 ::: bot acc: 0.0191
top acc: 0.0929 ::: bot acc: 0.0264
top acc: 0.0989 ::: bot acc: 0.0262
top acc: 0.0932 ::: bot acc: 0.0188
top acc: 0.1050 ::: bot acc: 0.0138
top acc: 0.0946 ::: bot acc: 0.0238
current epoch: 31
train loss is 0.001225
average val loss: 0.001146, accuracy: 0.0383
average test loss: 0.001944, accuracy: 0.0514
case acc: 0.04450122
case acc: 0.056996804
case acc: 0.054237794
case acc: 0.05227165
case acc: 0.052926138
case acc: 0.04747487
top acc: 0.0795 ::: bot acc: 0.0200
top acc: 0.0909 ::: bot acc: 0.0255
top acc: 0.0980 ::: bot acc: 0.0260
top acc: 0.0922 ::: bot acc: 0.0193
top acc: 0.1031 ::: bot acc: 0.0134
top acc: 0.0922 ::: bot acc: 0.0243
current epoch: 32
train loss is 0.001203
average val loss: 0.001150, accuracy: 0.0384
average test loss: 0.001959, accuracy: 0.0516
case acc: 0.044821534
case acc: 0.056770377
case acc: 0.055047777
case acc: 0.052565467
case acc: 0.052955337
case acc: 0.0474762
top acc: 0.0798 ::: bot acc: 0.0195
top acc: 0.0900 ::: bot acc: 0.0263
top acc: 0.1000 ::: bot acc: 0.0255
top acc: 0.0929 ::: bot acc: 0.0192
top acc: 0.1033 ::: bot acc: 0.0141
top acc: 0.0927 ::: bot acc: 0.0247
current epoch: 33
train loss is 0.001210
average val loss: 0.001170, accuracy: 0.0388
average test loss: 0.002033, accuracy: 0.0526
case acc: 0.045859046
case acc: 0.057083298
case acc: 0.056327865
case acc: 0.053682912
case acc: 0.054098938
case acc: 0.048392564
top acc: 0.0825 ::: bot acc: 0.0181
top acc: 0.0908 ::: bot acc: 0.0262
top acc: 0.1024 ::: bot acc: 0.0242
top acc: 0.0945 ::: bot acc: 0.0187
top acc: 0.1050 ::: bot acc: 0.0139
top acc: 0.0946 ::: bot acc: 0.0236
current epoch: 34
train loss is 0.001195
average val loss: 0.001188, accuracy: 0.0389
average test loss: 0.002079, accuracy: 0.0534
case acc: 0.047218338
case acc: 0.056880407
case acc: 0.05734964
case acc: 0.05476534
case acc: 0.05549008
case acc: 0.04849691
top acc: 0.0841 ::: bot acc: 0.0185
top acc: 0.0907 ::: bot acc: 0.0256
top acc: 0.1036 ::: bot acc: 0.0242
top acc: 0.0958 ::: bot acc: 0.0191
top acc: 0.1064 ::: bot acc: 0.0144
top acc: 0.0950 ::: bot acc: 0.0223
current epoch: 35
train loss is 0.001215
average val loss: 0.001200, accuracy: 0.0391
average test loss: 0.002130, accuracy: 0.0541
case acc: 0.047538433
case acc: 0.05692681
case acc: 0.058450934
case acc: 0.0560357
case acc: 0.056288607
case acc: 0.04906272
top acc: 0.0848 ::: bot acc: 0.0182
top acc: 0.0907 ::: bot acc: 0.0254
top acc: 0.1056 ::: bot acc: 0.0233
top acc: 0.0979 ::: bot acc: 0.0189
top acc: 0.1078 ::: bot acc: 0.0150
top acc: 0.0960 ::: bot acc: 0.0217
current epoch: 36
train loss is 0.001214
average val loss: 0.001196, accuracy: 0.0391
average test loss: 0.002115, accuracy: 0.0538
case acc: 0.047418162
case acc: 0.056021247
case acc: 0.058561236
case acc: 0.05594421
case acc: 0.056051955
case acc: 0.048882265
top acc: 0.0843 ::: bot acc: 0.0183
top acc: 0.0893 ::: bot acc: 0.0252
top acc: 0.1058 ::: bot acc: 0.0231
top acc: 0.0980 ::: bot acc: 0.0186
top acc: 0.1074 ::: bot acc: 0.0147
top acc: 0.0954 ::: bot acc: 0.0226
current epoch: 37
train loss is 0.001212
average val loss: 0.001174, accuracy: 0.0387
average test loss: 0.002091, accuracy: 0.0534
case acc: 0.047648933
case acc: 0.054961376
case acc: 0.05802742
case acc: 0.055938974
case acc: 0.055564407
case acc: 0.04840234
top acc: 0.0843 ::: bot acc: 0.0190
top acc: 0.0882 ::: bot acc: 0.0251
top acc: 0.1054 ::: bot acc: 0.0230
top acc: 0.0979 ::: bot acc: 0.0195
top acc: 0.1070 ::: bot acc: 0.0143
top acc: 0.0951 ::: bot acc: 0.0230
current epoch: 38
train loss is 0.001167
average val loss: 0.001156, accuracy: 0.0385
average test loss: 0.002041, accuracy: 0.0528
case acc: 0.046608586
case acc: 0.053301997
case acc: 0.05787819
case acc: 0.05557014
case acc: 0.055091973
case acc: 0.04814934
top acc: 0.0830 ::: bot acc: 0.0186
top acc: 0.0858 ::: bot acc: 0.0249
top acc: 0.1046 ::: bot acc: 0.0237
top acc: 0.0973 ::: bot acc: 0.0194
top acc: 0.1065 ::: bot acc: 0.0139
top acc: 0.0938 ::: bot acc: 0.0239
current epoch: 39
train loss is 0.001146
average val loss: 0.001127, accuracy: 0.0380
average test loss: 0.001963, accuracy: 0.0517
case acc: 0.045708824
case acc: 0.05141946
case acc: 0.05685328
case acc: 0.05419099
case acc: 0.054558776
case acc: 0.047385093
top acc: 0.0817 ::: bot acc: 0.0191
top acc: 0.0829 ::: bot acc: 0.0249
top acc: 0.1031 ::: bot acc: 0.0240
top acc: 0.0952 ::: bot acc: 0.0189
top acc: 0.1052 ::: bot acc: 0.0143
top acc: 0.0919 ::: bot acc: 0.0252
current epoch: 40
train loss is 0.001108
average val loss: 0.001074, accuracy: 0.0371
average test loss: 0.001848, accuracy: 0.0499
case acc: 0.043813683
case acc: 0.048884522
case acc: 0.05548752
case acc: 0.052383352
case acc: 0.052639626
case acc: 0.046073064
top acc: 0.0782 ::: bot acc: 0.0200
top acc: 0.0794 ::: bot acc: 0.0252
top acc: 0.1003 ::: bot acc: 0.0256
top acc: 0.0928 ::: bot acc: 0.0187
top acc: 0.1029 ::: bot acc: 0.0141
top acc: 0.0890 ::: bot acc: 0.0274
current epoch: 41
train loss is 0.001071
average val loss: 0.001044, accuracy: 0.0366
average test loss: 0.001754, accuracy: 0.0485
case acc: 0.042461973
case acc: 0.046687454
case acc: 0.05426787
case acc: 0.051167693
case acc: 0.0506905
case acc: 0.04565758
top acc: 0.0758 ::: bot acc: 0.0213
top acc: 0.0759 ::: bot acc: 0.0250
top acc: 0.0977 ::: bot acc: 0.0268
top acc: 0.0906 ::: bot acc: 0.0193
top acc: 0.1002 ::: bot acc: 0.0132
top acc: 0.0865 ::: bot acc: 0.0302
current epoch: 42
train loss is 0.001043
average val loss: 0.001025, accuracy: 0.0362
average test loss: 0.001690, accuracy: 0.0475
case acc: 0.041533682
case acc: 0.045006577
case acc: 0.053402267
case acc: 0.050109606
case acc: 0.04978092
case acc: 0.04489939
top acc: 0.0739 ::: bot acc: 0.0221
top acc: 0.0734 ::: bot acc: 0.0254
top acc: 0.0961 ::: bot acc: 0.0281
top acc: 0.0892 ::: bot acc: 0.0199
top acc: 0.0986 ::: bot acc: 0.0138
top acc: 0.0849 ::: bot acc: 0.0313
current epoch: 43
train loss is 0.001021
average val loss: 0.001022, accuracy: 0.0361
average test loss: 0.001648, accuracy: 0.0468
case acc: 0.040988676
case acc: 0.0434912
case acc: 0.05300204
case acc: 0.049660023
case acc: 0.04902786
case acc: 0.04465492
top acc: 0.0725 ::: bot acc: 0.0230
top acc: 0.0709 ::: bot acc: 0.0252
top acc: 0.0953 ::: bot acc: 0.0293
top acc: 0.0884 ::: bot acc: 0.0195
top acc: 0.0977 ::: bot acc: 0.0134
top acc: 0.0841 ::: bot acc: 0.0324
current epoch: 44
train loss is 0.001003
average val loss: 0.001005, accuracy: 0.0359
average test loss: 0.001584, accuracy: 0.0458
case acc: 0.039991815
case acc: 0.042062234
case acc: 0.05234794
case acc: 0.04869606
case acc: 0.047550164
case acc: 0.044170577
top acc: 0.0704 ::: bot acc: 0.0245
top acc: 0.0686 ::: bot acc: 0.0256
top acc: 0.0932 ::: bot acc: 0.0307
top acc: 0.0867 ::: bot acc: 0.0201
top acc: 0.0952 ::: bot acc: 0.0135
top acc: 0.0826 ::: bot acc: 0.0341
current epoch: 45
train loss is 0.000994
average val loss: 0.001008, accuracy: 0.0360
average test loss: 0.001569, accuracy: 0.0456
case acc: 0.039479546
case acc: 0.041641887
case acc: 0.051911555
case acc: 0.048653193
case acc: 0.047534056
case acc: 0.044340674
top acc: 0.0693 ::: bot acc: 0.0252
top acc: 0.0679 ::: bot acc: 0.0261
top acc: 0.0922 ::: bot acc: 0.0311
top acc: 0.0863 ::: bot acc: 0.0203
top acc: 0.0954 ::: bot acc: 0.0134
top acc: 0.0822 ::: bot acc: 0.0344
current epoch: 46
train loss is 0.000988
average val loss: 0.001003, accuracy: 0.0359
average test loss: 0.001536, accuracy: 0.0450
case acc: 0.038903568
case acc: 0.040474147
case acc: 0.051805127
case acc: 0.048142813
case acc: 0.046653587
case acc: 0.04405234
top acc: 0.0678 ::: bot acc: 0.0266
top acc: 0.0661 ::: bot acc: 0.0269
top acc: 0.0914 ::: bot acc: 0.0330
top acc: 0.0858 ::: bot acc: 0.0204
top acc: 0.0938 ::: bot acc: 0.0140
top acc: 0.0812 ::: bot acc: 0.0352
current epoch: 47
train loss is 0.000981
average val loss: 0.000993, accuracy: 0.0358
average test loss: 0.001473, accuracy: 0.0441
case acc: 0.038013887
case acc: 0.039069105
case acc: 0.050971024
case acc: 0.04696232
case acc: 0.045661163
case acc: 0.04379577
top acc: 0.0656 ::: bot acc: 0.0285
top acc: 0.0632 ::: bot acc: 0.0272
top acc: 0.0888 ::: bot acc: 0.0351
top acc: 0.0836 ::: bot acc: 0.0212
top acc: 0.0914 ::: bot acc: 0.0151
top acc: 0.0798 ::: bot acc: 0.0370
current epoch: 48
train loss is 0.000960
average val loss: 0.000996, accuracy: 0.0358
average test loss: 0.001469, accuracy: 0.0440
case acc: 0.038119674
case acc: 0.03885701
case acc: 0.05086503
case acc: 0.047053542
case acc: 0.04540572
case acc: 0.043993156
top acc: 0.0655 ::: bot acc: 0.0288
top acc: 0.0631 ::: bot acc: 0.0276
top acc: 0.0888 ::: bot acc: 0.0351
top acc: 0.0838 ::: bot acc: 0.0211
top acc: 0.0911 ::: bot acc: 0.0156
top acc: 0.0796 ::: bot acc: 0.0376
current epoch: 49
train loss is 0.000961
average val loss: 0.000996, accuracy: 0.0358
average test loss: 0.001464, accuracy: 0.0438
case acc: 0.03784432
case acc: 0.038512494
case acc: 0.050978947
case acc: 0.04676404
case acc: 0.045260303
case acc: 0.043588284
top acc: 0.0650 ::: bot acc: 0.0290
top acc: 0.0628 ::: bot acc: 0.0276
top acc: 0.0891 ::: bot acc: 0.0353
top acc: 0.0838 ::: bot acc: 0.0206
top acc: 0.0910 ::: bot acc: 0.0161
top acc: 0.0790 ::: bot acc: 0.0376
current epoch: 50
train loss is 0.000957
average val loss: 0.000993, accuracy: 0.0358
average test loss: 0.001454, accuracy: 0.0438
case acc: 0.037850782
case acc: 0.038154483
case acc: 0.05071221
case acc: 0.04712596
case acc: 0.044792913
case acc: 0.0438698
top acc: 0.0644 ::: bot acc: 0.0297
top acc: 0.0620 ::: bot acc: 0.0282
top acc: 0.0883 ::: bot acc: 0.0354
top acc: 0.0838 ::: bot acc: 0.0215
top acc: 0.0901 ::: bot acc: 0.0159
top acc: 0.0794 ::: bot acc: 0.0377
LME_Co_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5388 5388 5388
1.7082474 -0.6288155 0.16982092 -0.17269358
Validation: 600 600 600
Testing: 768 768 768
pre-processing time: 0.0001881122589111328
the split date is 2012-07-01
train dropout: 0.2 test dropout: 0.15
net initializing with time: 0.0021932125091552734
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.007762
average val loss: 0.004408, accuracy: 0.0804
average test loss: 0.004281, accuracy: 0.0797
case acc: 0.10599314
case acc: 0.117954634
case acc: 0.074913666
case acc: 0.053770777
case acc: 0.08372772
case acc: 0.042137638
top acc: 0.0556 ::: bot acc: 0.1506
top acc: 0.1553 ::: bot acc: 0.0824
top acc: 0.0300 ::: bot acc: 0.1236
top acc: 0.0234 ::: bot acc: 0.0820
top acc: 0.0370 ::: bot acc: 0.1336
top acc: 0.0347 ::: bot acc: 0.0802
current epoch: 2
train loss is 0.006865
average val loss: 0.006226, accuracy: 0.0833
average test loss: 0.006184, accuracy: 0.0848
case acc: 0.038722515
case acc: 0.22128224
case acc: 0.056215182
case acc: 0.059083246
case acc: 0.04291954
case acc: 0.0908289
top acc: 0.0712 ::: bot acc: 0.0324
top acc: 0.2591 ::: bot acc: 0.1859
top acc: 0.1017 ::: bot acc: 0.0214
top acc: 0.0910 ::: bot acc: 0.0317
top acc: 0.0725 ::: bot acc: 0.0290
top acc: 0.1382 ::: bot acc: 0.0413
current epoch: 3
train loss is 0.008689
average val loss: 0.017011, accuracy: 0.1671
average test loss: 0.017035, accuracy: 0.1681
case acc: 0.11512752
case acc: 0.30603647
case acc: 0.14668746
case acc: 0.15180826
case acc: 0.11673328
case acc: 0.17234494
top acc: 0.1714 ::: bot acc: 0.0677
top acc: 0.3434 ::: bot acc: 0.2700
top acc: 0.1995 ::: bot acc: 0.0935
top acc: 0.1840 ::: bot acc: 0.1236
top acc: 0.1652 ::: bot acc: 0.0660
top acc: 0.2248 ::: bot acc: 0.1116
current epoch: 4
train loss is 0.012249
average val loss: 0.011626, accuracy: 0.1326
average test loss: 0.011653, accuracy: 0.1343
case acc: 0.08658312
case acc: 0.2630328
case acc: 0.11547365
case acc: 0.11741511
case acc: 0.08739432
case acc: 0.13566878
top acc: 0.1423 ::: bot acc: 0.0394
top acc: 0.2998 ::: bot acc: 0.2281
top acc: 0.1685 ::: bot acc: 0.0633
top acc: 0.1507 ::: bot acc: 0.0888
top acc: 0.1345 ::: bot acc: 0.0376
top acc: 0.1865 ::: bot acc: 0.0777
current epoch: 5
train loss is 0.011633
average val loss: 0.002631, accuracy: 0.0578
average test loss: 0.002480, accuracy: 0.0550
case acc: 0.054996565
case acc: 0.12025744
case acc: 0.04175565
case acc: 0.026274558
case acc: 0.046162613
case acc: 0.040794663
top acc: 0.0261 ::: bot acc: 0.0887
top acc: 0.1572 ::: bot acc: 0.0850
top acc: 0.0349 ::: bot acc: 0.0717
top acc: 0.0175 ::: bot acc: 0.0427
top acc: 0.0162 ::: bot acc: 0.0886
top acc: 0.0539 ::: bot acc: 0.0579
current epoch: 6
train loss is 0.004702
average val loss: 0.002344, accuracy: 0.0547
average test loss: 0.002218, accuracy: 0.0526
case acc: 0.053991657
case acc: 0.10838061
case acc: 0.043266155
case acc: 0.027397016
case acc: 0.042028774
case acc: 0.04026919
top acc: 0.0262 ::: bot acc: 0.0881
top acc: 0.1461 ::: bot acc: 0.0723
top acc: 0.0313 ::: bot acc: 0.0756
top acc: 0.0177 ::: bot acc: 0.0444
top acc: 0.0213 ::: bot acc: 0.0800
top acc: 0.0539 ::: bot acc: 0.0577
current epoch: 7
train loss is 0.003015
average val loss: 0.003122, accuracy: 0.0597
average test loss: 0.002999, accuracy: 0.0593
case acc: 0.037978463
case acc: 0.14378345
case acc: 0.041363254
case acc: 0.0314105
case acc: 0.040977444
case acc: 0.060349643
top acc: 0.0632 ::: bot acc: 0.0394
top acc: 0.1807 ::: bot acc: 0.1079
top acc: 0.0723 ::: bot acc: 0.0335
top acc: 0.0606 ::: bot acc: 0.0099
top acc: 0.0695 ::: bot acc: 0.0304
top acc: 0.0981 ::: bot acc: 0.0294
current epoch: 8
train loss is 0.003528
average val loss: 0.002686, accuracy: 0.0560
average test loss: 0.002611, accuracy: 0.0557
case acc: 0.038248383
case acc: 0.13070203
case acc: 0.040158674
case acc: 0.027211765
case acc: 0.0409099
case acc: 0.05688995
top acc: 0.0603 ::: bot acc: 0.0430
top acc: 0.1673 ::: bot acc: 0.0953
top acc: 0.0655 ::: bot acc: 0.0416
top acc: 0.0541 ::: bot acc: 0.0104
top acc: 0.0683 ::: bot acc: 0.0324
top acc: 0.0921 ::: bot acc: 0.0308
current epoch: 9
train loss is 0.003259
average val loss: 0.002111, accuracy: 0.0504
average test loss: 0.002028, accuracy: 0.0496
case acc: 0.0405469
case acc: 0.108550385
case acc: 0.03976454
case acc: 0.02265839
case acc: 0.03740013
case acc: 0.048970688
top acc: 0.0490 ::: bot acc: 0.0551
top acc: 0.1457 ::: bot acc: 0.0735
top acc: 0.0513 ::: bot acc: 0.0559
top acc: 0.0403 ::: bot acc: 0.0207
top acc: 0.0591 ::: bot acc: 0.0406
top acc: 0.0777 ::: bot acc: 0.0370
current epoch: 10
train loss is 0.002630
average val loss: 0.002015, accuracy: 0.0495
average test loss: 0.001935, accuracy: 0.0489
case acc: 0.039794594
case acc: 0.10248271
case acc: 0.039539937
case acc: 0.022873048
case acc: 0.038827673
case acc: 0.049908303
top acc: 0.0525 ::: bot acc: 0.0513
top acc: 0.1392 ::: bot acc: 0.0670
top acc: 0.0512 ::: bot acc: 0.0562
top acc: 0.0413 ::: bot acc: 0.0192
top acc: 0.0635 ::: bot acc: 0.0355
top acc: 0.0791 ::: bot acc: 0.0361
current epoch: 11
train loss is 0.002320
average val loss: 0.002052, accuracy: 0.0503
average test loss: 0.001985, accuracy: 0.0497
case acc: 0.0381499
case acc: 0.10157477
case acc: 0.038910277
case acc: 0.024341112
case acc: 0.04247685
case acc: 0.052563775
top acc: 0.0592 ::: bot acc: 0.0434
top acc: 0.1388 ::: bot acc: 0.0653
top acc: 0.0553 ::: bot acc: 0.0516
top acc: 0.0471 ::: bot acc: 0.0142
top acc: 0.0724 ::: bot acc: 0.0291
top acc: 0.0849 ::: bot acc: 0.0326
current epoch: 12
train loss is 0.002249
average val loss: 0.002057, accuracy: 0.0508
average test loss: 0.001990, accuracy: 0.0502
case acc: 0.038218673
case acc: 0.09848808
case acc: 0.03930322
case acc: 0.025655493
case acc: 0.04493984
case acc: 0.054441277
top acc: 0.0653 ::: bot acc: 0.0381
top acc: 0.1352 ::: bot acc: 0.0632
top acc: 0.0584 ::: bot acc: 0.0487
top acc: 0.0507 ::: bot acc: 0.0115
top acc: 0.0775 ::: bot acc: 0.0259
top acc: 0.0881 ::: bot acc: 0.0311
current epoch: 13
train loss is 0.002174
average val loss: 0.001927, accuracy: 0.0493
average test loss: 0.001863, accuracy: 0.0487
case acc: 0.038004115
case acc: 0.091414586
case acc: 0.039355755
case acc: 0.025144644
case acc: 0.044494595
case acc: 0.05357633
top acc: 0.0653 ::: bot acc: 0.0373
top acc: 0.1284 ::: bot acc: 0.0559
top acc: 0.0569 ::: bot acc: 0.0507
top acc: 0.0492 ::: bot acc: 0.0125
top acc: 0.0771 ::: bot acc: 0.0251
top acc: 0.0867 ::: bot acc: 0.0322
current epoch: 14
train loss is 0.001985
average val loss: 0.001811, accuracy: 0.0481
average test loss: 0.001769, accuracy: 0.0477
case acc: 0.038413078
case acc: 0.08479215
case acc: 0.039386325
case acc: 0.024873013
case acc: 0.045283783
case acc: 0.053373907
top acc: 0.0666 ::: bot acc: 0.0368
top acc: 0.1218 ::: bot acc: 0.0491
top acc: 0.0559 ::: bot acc: 0.0514
top acc: 0.0485 ::: bot acc: 0.0130
top acc: 0.0779 ::: bot acc: 0.0267
top acc: 0.0857 ::: bot acc: 0.0332
current epoch: 15
train loss is 0.001842
average val loss: 0.001723, accuracy: 0.0470
average test loss: 0.001674, accuracy: 0.0464
case acc: 0.038100038
case acc: 0.078220166
case acc: 0.039499827
case acc: 0.024966594
case acc: 0.04483153
case acc: 0.05257989
top acc: 0.0673 ::: bot acc: 0.0360
top acc: 0.1159 ::: bot acc: 0.0423
top acc: 0.0547 ::: bot acc: 0.0529
top acc: 0.0487 ::: bot acc: 0.0131
top acc: 0.0774 ::: bot acc: 0.0258
top acc: 0.0844 ::: bot acc: 0.0327
current epoch: 16
train loss is 0.001739
average val loss: 0.001713, accuracy: 0.0470
average test loss: 0.001660, accuracy: 0.0463
case acc: 0.03829647
case acc: 0.074980885
case acc: 0.038890727
case acc: 0.026048092
case acc: 0.046401322
case acc: 0.053269956
top acc: 0.0706 ::: bot acc: 0.0322
top acc: 0.1120 ::: bot acc: 0.0393
top acc: 0.0558 ::: bot acc: 0.0508
top acc: 0.0515 ::: bot acc: 0.0111
top acc: 0.0796 ::: bot acc: 0.0260
top acc: 0.0855 ::: bot acc: 0.0321
current epoch: 17
train loss is 0.001667
average val loss: 0.001741, accuracy: 0.0475
average test loss: 0.001708, accuracy: 0.0472
case acc: 0.039242137
case acc: 0.07344839
case acc: 0.039382793
case acc: 0.028043061
case acc: 0.048098415
case acc: 0.054880913
top acc: 0.0762 ::: bot acc: 0.0282
top acc: 0.1099 ::: bot acc: 0.0384
top acc: 0.0599 ::: bot acc: 0.0473
top acc: 0.0550 ::: bot acc: 0.0107
top acc: 0.0824 ::: bot acc: 0.0250
top acc: 0.0885 ::: bot acc: 0.0313
current epoch: 18
train loss is 0.001635
average val loss: 0.001710, accuracy: 0.0472
average test loss: 0.001670, accuracy: 0.0465
case acc: 0.03918393
case acc: 0.06980327
case acc: 0.039521012
case acc: 0.028076362
case acc: 0.047537237
case acc: 0.054898825
top acc: 0.0772 ::: bot acc: 0.0254
top acc: 0.1066 ::: bot acc: 0.0347
top acc: 0.0608 ::: bot acc: 0.0464
top acc: 0.0552 ::: bot acc: 0.0100
top acc: 0.0828 ::: bot acc: 0.0237
top acc: 0.0889 ::: bot acc: 0.0312
current epoch: 19
train loss is 0.001539
average val loss: 0.001692, accuracy: 0.0471
average test loss: 0.001656, accuracy: 0.0463
case acc: 0.039884098
case acc: 0.06683867
case acc: 0.03938341
case acc: 0.028450137
case acc: 0.048014358
case acc: 0.05493595
top acc: 0.0803 ::: bot acc: 0.0235
top acc: 0.1044 ::: bot acc: 0.0314
top acc: 0.0620 ::: bot acc: 0.0451
top acc: 0.0562 ::: bot acc: 0.0091
top acc: 0.0835 ::: bot acc: 0.0233
top acc: 0.0892 ::: bot acc: 0.0310
current epoch: 20
train loss is 0.001489
average val loss: 0.001704, accuracy: 0.0472
average test loss: 0.001680, accuracy: 0.0467
case acc: 0.040655747
case acc: 0.064774044
case acc: 0.039739296
case acc: 0.0302203
case acc: 0.04878781
case acc: 0.055810016
top acc: 0.0833 ::: bot acc: 0.0208
top acc: 0.1016 ::: bot acc: 0.0295
top acc: 0.0645 ::: bot acc: 0.0426
top acc: 0.0589 ::: bot acc: 0.0092
top acc: 0.0847 ::: bot acc: 0.0233
top acc: 0.0903 ::: bot acc: 0.0312
current epoch: 21
train loss is 0.001452
average val loss: 0.001638, accuracy: 0.0464
average test loss: 0.001613, accuracy: 0.0457
case acc: 0.040431798
case acc: 0.060922433
case acc: 0.04018276
case acc: 0.030124255
case acc: 0.047846675
case acc: 0.0547489
top acc: 0.0822 ::: bot acc: 0.0209
top acc: 0.0975 ::: bot acc: 0.0259
top acc: 0.0640 ::: bot acc: 0.0439
top acc: 0.0586 ::: bot acc: 0.0095
top acc: 0.0830 ::: bot acc: 0.0242
top acc: 0.0887 ::: bot acc: 0.0313
current epoch: 22
train loss is 0.001393
average val loss: 0.001561, accuracy: 0.0454
average test loss: 0.001541, accuracy: 0.0444
case acc: 0.040265527
case acc: 0.056682467
case acc: 0.039359465
case acc: 0.029277252
case acc: 0.046842173
case acc: 0.053691313
top acc: 0.0823 ::: bot acc: 0.0209
top acc: 0.0932 ::: bot acc: 0.0214
top acc: 0.0633 ::: bot acc: 0.0433
top acc: 0.0577 ::: bot acc: 0.0092
top acc: 0.0812 ::: bot acc: 0.0243
top acc: 0.0868 ::: bot acc: 0.0318
current epoch: 23
train loss is 0.001334
average val loss: 0.001595, accuracy: 0.0458
average test loss: 0.001574, accuracy: 0.0450
case acc: 0.040977266
case acc: 0.056162544
case acc: 0.039854124
case acc: 0.03127902
case acc: 0.04749417
case acc: 0.05439381
top acc: 0.0851 ::: bot acc: 0.0177
top acc: 0.0927 ::: bot acc: 0.0214
top acc: 0.0661 ::: bot acc: 0.0409
top acc: 0.0601 ::: bot acc: 0.0102
top acc: 0.0814 ::: bot acc: 0.0247
top acc: 0.0881 ::: bot acc: 0.0313
current epoch: 24
train loss is 0.001328
average val loss: 0.001669, accuracy: 0.0470
average test loss: 0.001650, accuracy: 0.0463
case acc: 0.04245718
case acc: 0.056631334
case acc: 0.04069513
case acc: 0.033638164
case acc: 0.048555113
case acc: 0.055677306
top acc: 0.0892 ::: bot acc: 0.0145
top acc: 0.0932 ::: bot acc: 0.0216
top acc: 0.0696 ::: bot acc: 0.0372
top acc: 0.0628 ::: bot acc: 0.0106
top acc: 0.0840 ::: bot acc: 0.0235
top acc: 0.0903 ::: bot acc: 0.0306
current epoch: 25
train loss is 0.001341
average val loss: 0.001688, accuracy: 0.0473
average test loss: 0.001676, accuracy: 0.0467
case acc: 0.043105185
case acc: 0.055869646
case acc: 0.04111916
case acc: 0.034946784
case acc: 0.049140304
case acc: 0.05585275
top acc: 0.0905 ::: bot acc: 0.0126
top acc: 0.0926 ::: bot acc: 0.0209
top acc: 0.0717 ::: bot acc: 0.0352
top acc: 0.0655 ::: bot acc: 0.0110
top acc: 0.0846 ::: bot acc: 0.0238
top acc: 0.0911 ::: bot acc: 0.0300
current epoch: 26
train loss is 0.001325
average val loss: 0.001738, accuracy: 0.0481
average test loss: 0.001715, accuracy: 0.0473
case acc: 0.044258706
case acc: 0.055109017
case acc: 0.04225641
case acc: 0.036524463
case acc: 0.049243875
case acc: 0.05649585
top acc: 0.0935 ::: bot acc: 0.0105
top acc: 0.0918 ::: bot acc: 0.0201
top acc: 0.0749 ::: bot acc: 0.0323
top acc: 0.0667 ::: bot acc: 0.0120
top acc: 0.0851 ::: bot acc: 0.0230
top acc: 0.0920 ::: bot acc: 0.0302
current epoch: 27
train loss is 0.001328
average val loss: 0.001759, accuracy: 0.0484
average test loss: 0.001737, accuracy: 0.0478
case acc: 0.045749344
case acc: 0.054489456
case acc: 0.042780936
case acc: 0.03781469
case acc: 0.04958789
case acc: 0.05637294
top acc: 0.0952 ::: bot acc: 0.0109
top acc: 0.0911 ::: bot acc: 0.0199
top acc: 0.0767 ::: bot acc: 0.0306
top acc: 0.0683 ::: bot acc: 0.0130
top acc: 0.0853 ::: bot acc: 0.0232
top acc: 0.0916 ::: bot acc: 0.0300
current epoch: 28
train loss is 0.001328
average val loss: 0.001843, accuracy: 0.0496
average test loss: 0.001831, accuracy: 0.0493
case acc: 0.047523536
case acc: 0.05554229
case acc: 0.044078823
case acc: 0.04015948
case acc: 0.051049657
case acc: 0.057521824
top acc: 0.0984 ::: bot acc: 0.0102
top acc: 0.0923 ::: bot acc: 0.0202
top acc: 0.0805 ::: bot acc: 0.0265
top acc: 0.0708 ::: bot acc: 0.0143
top acc: 0.0878 ::: bot acc: 0.0225
top acc: 0.0936 ::: bot acc: 0.0296
current epoch: 29
train loss is 0.001309
average val loss: 0.001866, accuracy: 0.0501
average test loss: 0.001838, accuracy: 0.0495
case acc: 0.04830745
case acc: 0.054682367
case acc: 0.04494742
case acc: 0.040967185
case acc: 0.050883885
case acc: 0.05750252
top acc: 0.0994 ::: bot acc: 0.0099
top acc: 0.0911 ::: bot acc: 0.0198
top acc: 0.0819 ::: bot acc: 0.0265
top acc: 0.0716 ::: bot acc: 0.0149
top acc: 0.0876 ::: bot acc: 0.0223
top acc: 0.0933 ::: bot acc: 0.0298
current epoch: 30
train loss is 0.001294
average val loss: 0.001881, accuracy: 0.0502
average test loss: 0.001870, accuracy: 0.0500
case acc: 0.04920156
case acc: 0.05378238
case acc: 0.045579173
case acc: 0.04206501
case acc: 0.051840615
case acc: 0.057468053
top acc: 0.1007 ::: bot acc: 0.0100
top acc: 0.0905 ::: bot acc: 0.0194
top acc: 0.0841 ::: bot acc: 0.0252
top acc: 0.0728 ::: bot acc: 0.0160
top acc: 0.0898 ::: bot acc: 0.0219
top acc: 0.0937 ::: bot acc: 0.0293
current epoch: 31
train loss is 0.001297
average val loss: 0.001822, accuracy: 0.0495
average test loss: 0.001800, accuracy: 0.0488
case acc: 0.04799688
case acc: 0.051485192
case acc: 0.04503122
case acc: 0.041142866
case acc: 0.050880924
case acc: 0.05655858
top acc: 0.0999 ::: bot acc: 0.0095
top acc: 0.0876 ::: bot acc: 0.0179
top acc: 0.0823 ::: bot acc: 0.0263
top acc: 0.0713 ::: bot acc: 0.0157
top acc: 0.0879 ::: bot acc: 0.0222
top acc: 0.0921 ::: bot acc: 0.0300
current epoch: 32
train loss is 0.001257
average val loss: 0.001770, accuracy: 0.0487
average test loss: 0.001733, accuracy: 0.0477
case acc: 0.04767739
case acc: 0.04843709
case acc: 0.044802852
case acc: 0.03957842
case acc: 0.05018769
case acc: 0.055428073
top acc: 0.0985 ::: bot acc: 0.0103
top acc: 0.0840 ::: bot acc: 0.0153
top acc: 0.0820 ::: bot acc: 0.0262
top acc: 0.0702 ::: bot acc: 0.0144
top acc: 0.0867 ::: bot acc: 0.0226
top acc: 0.0902 ::: bot acc: 0.0306
current epoch: 33
train loss is 0.001216
average val loss: 0.001676, accuracy: 0.0474
average test loss: 0.001667, accuracy: 0.0466
case acc: 0.046640083
case acc: 0.04605915
case acc: 0.04405517
case acc: 0.038278542
case acc: 0.049424216
case acc: 0.054842856
top acc: 0.0974 ::: bot acc: 0.0101
top acc: 0.0811 ::: bot acc: 0.0146
top acc: 0.0802 ::: bot acc: 0.0277
top acc: 0.0691 ::: bot acc: 0.0129
top acc: 0.0852 ::: bot acc: 0.0232
top acc: 0.0887 ::: bot acc: 0.0317
current epoch: 34
train loss is 0.001186
average val loss: 0.001670, accuracy: 0.0474
average test loss: 0.001652, accuracy: 0.0463
case acc: 0.046692275
case acc: 0.044991642
case acc: 0.044354107
case acc: 0.0385582
case acc: 0.04929786
case acc: 0.0541834
top acc: 0.0970 ::: bot acc: 0.0103
top acc: 0.0802 ::: bot acc: 0.0137
top acc: 0.0804 ::: bot acc: 0.0277
top acc: 0.0689 ::: bot acc: 0.0136
top acc: 0.0845 ::: bot acc: 0.0234
top acc: 0.0880 ::: bot acc: 0.0311
current epoch: 35
train loss is 0.001159
average val loss: 0.001605, accuracy: 0.0463
average test loss: 0.001588, accuracy: 0.0452
case acc: 0.045533977
case acc: 0.042209774
case acc: 0.043773368
case acc: 0.03791639
case acc: 0.04847516
case acc: 0.053426497
top acc: 0.0952 ::: bot acc: 0.0104
top acc: 0.0767 ::: bot acc: 0.0126
top acc: 0.0795 ::: bot acc: 0.0287
top acc: 0.0686 ::: bot acc: 0.0132
top acc: 0.0834 ::: bot acc: 0.0241
top acc: 0.0862 ::: bot acc: 0.0324
current epoch: 36
train loss is 0.001140
average val loss: 0.001586, accuracy: 0.0460
average test loss: 0.001566, accuracy: 0.0449
case acc: 0.045523256
case acc: 0.041535195
case acc: 0.043494333
case acc: 0.037446573
case acc: 0.048058372
case acc: 0.05307722
top acc: 0.0952 ::: bot acc: 0.0109
top acc: 0.0751 ::: bot acc: 0.0127
top acc: 0.0792 ::: bot acc: 0.0283
top acc: 0.0677 ::: bot acc: 0.0126
top acc: 0.0824 ::: bot acc: 0.0244
top acc: 0.0860 ::: bot acc: 0.0324
current epoch: 37
train loss is 0.001129
average val loss: 0.001582, accuracy: 0.0459
average test loss: 0.001554, accuracy: 0.0447
case acc: 0.045347996
case acc: 0.04068599
case acc: 0.043747947
case acc: 0.037694547
case acc: 0.04755188
case acc: 0.053009517
top acc: 0.0948 ::: bot acc: 0.0108
top acc: 0.0740 ::: bot acc: 0.0120
top acc: 0.0792 ::: bot acc: 0.0285
top acc: 0.0680 ::: bot acc: 0.0131
top acc: 0.0822 ::: bot acc: 0.0242
top acc: 0.0854 ::: bot acc: 0.0327
current epoch: 38
train loss is 0.001111
average val loss: 0.001610, accuracy: 0.0463
average test loss: 0.001592, accuracy: 0.0453
case acc: 0.04593175
case acc: 0.04148742
case acc: 0.044155717
case acc: 0.03883369
case acc: 0.04818383
case acc: 0.053351842
top acc: 0.0961 ::: bot acc: 0.0105
top acc: 0.0753 ::: bot acc: 0.0128
top acc: 0.0803 ::: bot acc: 0.0275
top acc: 0.0698 ::: bot acc: 0.0135
top acc: 0.0828 ::: bot acc: 0.0245
top acc: 0.0862 ::: bot acc: 0.0324
current epoch: 39
train loss is 0.001109
average val loss: 0.001649, accuracy: 0.0470
average test loss: 0.001617, accuracy: 0.0458
case acc: 0.04639832
case acc: 0.041370846
case acc: 0.044820018
case acc: 0.039766796
case acc: 0.048797484
case acc: 0.053599298
top acc: 0.0967 ::: bot acc: 0.0099
top acc: 0.0747 ::: bot acc: 0.0125
top acc: 0.0824 ::: bot acc: 0.0257
top acc: 0.0701 ::: bot acc: 0.0144
top acc: 0.0840 ::: bot acc: 0.0237
top acc: 0.0865 ::: bot acc: 0.0320
current epoch: 40
train loss is 0.001116
average val loss: 0.001642, accuracy: 0.0468
average test loss: 0.001609, accuracy: 0.0457
case acc: 0.04629102
case acc: 0.040611863
case acc: 0.044964798
case acc: 0.039739408
case acc: 0.04890235
case acc: 0.053583205
top acc: 0.0967 ::: bot acc: 0.0102
top acc: 0.0735 ::: bot acc: 0.0129
top acc: 0.0823 ::: bot acc: 0.0261
top acc: 0.0705 ::: bot acc: 0.0142
top acc: 0.0839 ::: bot acc: 0.0238
top acc: 0.0864 ::: bot acc: 0.0325
current epoch: 41
train loss is 0.001098
average val loss: 0.001606, accuracy: 0.0462
average test loss: 0.001578, accuracy: 0.0451
case acc: 0.045947835
case acc: 0.039758805
case acc: 0.044565916
case acc: 0.039358348
case acc: 0.048096612
case acc: 0.052994378
top acc: 0.0961 ::: bot acc: 0.0107
top acc: 0.0726 ::: bot acc: 0.0124
top acc: 0.0815 ::: bot acc: 0.0264
top acc: 0.0697 ::: bot acc: 0.0140
top acc: 0.0831 ::: bot acc: 0.0239
top acc: 0.0856 ::: bot acc: 0.0325
current epoch: 42
train loss is 0.001092
average val loss: 0.001619, accuracy: 0.0464
average test loss: 0.001589, accuracy: 0.0453
case acc: 0.046078514
case acc: 0.03968186
case acc: 0.04511704
case acc: 0.039665937
case acc: 0.048319273
case acc: 0.053020768
top acc: 0.0964 ::: bot acc: 0.0106
top acc: 0.0726 ::: bot acc: 0.0123
top acc: 0.0823 ::: bot acc: 0.0259
top acc: 0.0704 ::: bot acc: 0.0141
top acc: 0.0830 ::: bot acc: 0.0239
top acc: 0.0857 ::: bot acc: 0.0325
current epoch: 43
train loss is 0.001085
average val loss: 0.001576, accuracy: 0.0458
average test loss: 0.001542, accuracy: 0.0445
case acc: 0.0452352
case acc: 0.038318694
case acc: 0.0448332
case acc: 0.03900063
case acc: 0.047686026
case acc: 0.05207052
top acc: 0.0947 ::: bot acc: 0.0113
top acc: 0.0706 ::: bot acc: 0.0123
top acc: 0.0817 ::: bot acc: 0.0268
top acc: 0.0695 ::: bot acc: 0.0137
top acc: 0.0821 ::: bot acc: 0.0244
top acc: 0.0841 ::: bot acc: 0.0329
current epoch: 44
train loss is 0.001060
average val loss: 0.001527, accuracy: 0.0450
average test loss: 0.001495, accuracy: 0.0436
case acc: 0.04406044
case acc: 0.037155308
case acc: 0.04426589
case acc: 0.03788519
case acc: 0.046767347
case acc: 0.051599815
top acc: 0.0926 ::: bot acc: 0.0118
top acc: 0.0690 ::: bot acc: 0.0124
top acc: 0.0811 ::: bot acc: 0.0273
top acc: 0.0685 ::: bot acc: 0.0125
top acc: 0.0806 ::: bot acc: 0.0248
top acc: 0.0827 ::: bot acc: 0.0341
current epoch: 45
train loss is 0.001050
average val loss: 0.001541, accuracy: 0.0452
average test loss: 0.001498, accuracy: 0.0437
case acc: 0.044072025
case acc: 0.036957934
case acc: 0.04434575
case acc: 0.038307525
case acc: 0.046816606
case acc: 0.05151447
top acc: 0.0931 ::: bot acc: 0.0115
top acc: 0.0684 ::: bot acc: 0.0127
top acc: 0.0805 ::: bot acc: 0.0276
top acc: 0.0690 ::: bot acc: 0.0132
top acc: 0.0804 ::: bot acc: 0.0248
top acc: 0.0832 ::: bot acc: 0.0332
current epoch: 46
train loss is 0.001048
average val loss: 0.001552, accuracy: 0.0454
average test loss: 0.001522, accuracy: 0.0441
case acc: 0.04457311
case acc: 0.03734007
case acc: 0.044605553
case acc: 0.039000638
case acc: 0.047229435
case acc: 0.052074738
top acc: 0.0933 ::: bot acc: 0.0118
top acc: 0.0689 ::: bot acc: 0.0126
top acc: 0.0814 ::: bot acc: 0.0262
top acc: 0.0697 ::: bot acc: 0.0137
top acc: 0.0814 ::: bot acc: 0.0247
top acc: 0.0836 ::: bot acc: 0.0339
current epoch: 47
train loss is 0.001048
average val loss: 0.001573, accuracy: 0.0458
average test loss: 0.001534, accuracy: 0.0444
case acc: 0.044652358
case acc: 0.037325367
case acc: 0.045077763
case acc: 0.0394309
case acc: 0.047759175
case acc: 0.052120022
top acc: 0.0939 ::: bot acc: 0.0115
top acc: 0.0687 ::: bot acc: 0.0126
top acc: 0.0817 ::: bot acc: 0.0264
top acc: 0.0701 ::: bot acc: 0.0143
top acc: 0.0824 ::: bot acc: 0.0243
top acc: 0.0841 ::: bot acc: 0.0333
current epoch: 48
train loss is 0.001041
average val loss: 0.001559, accuracy: 0.0455
average test loss: 0.001514, accuracy: 0.0440
case acc: 0.044159345
case acc: 0.036732502
case acc: 0.044901446
case acc: 0.039053105
case acc: 0.047468767
case acc: 0.051782355
top acc: 0.0927 ::: bot acc: 0.0115
top acc: 0.0679 ::: bot acc: 0.0126
top acc: 0.0816 ::: bot acc: 0.0267
top acc: 0.0697 ::: bot acc: 0.0135
top acc: 0.0819 ::: bot acc: 0.0244
top acc: 0.0833 ::: bot acc: 0.0336
current epoch: 49
train loss is 0.001037
average val loss: 0.001543, accuracy: 0.0453
average test loss: 0.001500, accuracy: 0.0437
case acc: 0.043900177
case acc: 0.036235023
case acc: 0.044734806
case acc: 0.0388064
case acc: 0.047316305
case acc: 0.051422656
top acc: 0.0925 ::: bot acc: 0.0115
top acc: 0.0672 ::: bot acc: 0.0127
top acc: 0.0815 ::: bot acc: 0.0262
top acc: 0.0691 ::: bot acc: 0.0136
top acc: 0.0816 ::: bot acc: 0.0242
top acc: 0.0828 ::: bot acc: 0.0341
current epoch: 50
train loss is 0.001031
average val loss: 0.001538, accuracy: 0.0451
average test loss: 0.001494, accuracy: 0.0436
case acc: 0.04363224
case acc: 0.036166027
case acc: 0.044646494
case acc: 0.038829073
case acc: 0.047391176
case acc: 0.05122703
top acc: 0.0922 ::: bot acc: 0.0119
top acc: 0.0668 ::: bot acc: 0.0133
top acc: 0.0820 ::: bot acc: 0.0257
top acc: 0.0693 ::: bot acc: 0.0137
top acc: 0.0819 ::: bot acc: 0.0247
top acc: 0.0824 ::: bot acc: 0.0339
