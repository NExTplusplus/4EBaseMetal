
		{"drop_out": 0.6, "drop_out_mc": 0.05, "repeat_mc": 50, "hidden": 30, "embedding_size": 5, "batch": 512, "lag": 4}
LME_Co_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6768 6768 6768
1.8562728 -0.6288155 0.2585643 -0.19947179
Validation: 756 756 756
Testing: 774 774 774
pre-processing time: 0.00020766258239746094
the split date is 2009-07-01
train dropout: 0.6 test dropout: 0.05
net initializing with time: 0.09074807167053223
preparing training and testing date with time: 4.76837158203125e-07
current epoch: 1
train loss is 0.013014
average val loss: 0.006050, accuracy: 0.1030
average test loss: 0.006619, accuracy: 0.1085
case acc: 0.15376021
case acc: 0.072240636
case acc: 0.11227853
case acc: 0.104995385
case acc: 0.13607796
case acc: 0.07159194
top acc: 0.1364 ::: bot acc: 0.1697
top acc: 0.1001 ::: bot acc: 0.0461
top acc: 0.0745 ::: bot acc: 0.1483
top acc: 0.0804 ::: bot acc: 0.1300
top acc: 0.1125 ::: bot acc: 0.1617
top acc: 0.0512 ::: bot acc: 0.0930
current epoch: 2
train loss is 0.008574
average val loss: 0.002894, accuracy: 0.0543
average test loss: 0.002786, accuracy: 0.0543
case acc: 0.051496677
case acc: 0.1571472
case acc: 0.028741978
case acc: 0.020592183
case acc: 0.047819387
case acc: 0.019991972
top acc: 0.0339 ::: bot acc: 0.0678
top acc: 0.1848 ::: bot acc: 0.1310
top acc: 0.0263 ::: bot acc: 0.0465
top acc: 0.0149 ::: bot acc: 0.0365
top acc: 0.0238 ::: bot acc: 0.0739
top acc: 0.0337 ::: bot acc: 0.0111
current epoch: 3
train loss is 0.008840
average val loss: 0.006959, accuracy: 0.0953
average test loss: 0.006308, accuracy: 0.0880
case acc: 0.0365729
case acc: 0.2301422
case acc: 0.07632906
case acc: 0.06842448
case acc: 0.032337
case acc: 0.084045365
top acc: 0.0536 ::: bot acc: 0.0207
top acc: 0.2577 ::: bot acc: 0.2038
top acc: 0.1141 ::: bot acc: 0.0410
top acc: 0.0935 ::: bot acc: 0.0428
top acc: 0.0535 ::: bot acc: 0.0111
top acc: 0.1048 ::: bot acc: 0.0620
current epoch: 4
train loss is 0.010151
average val loss: 0.016109, accuracy: 0.1667
average test loss: 0.014901, accuracy: 0.1595
case acc: 0.11616932
case acc: 0.29611042
case acc: 0.15467894
case acc: 0.13923265
case acc: 0.10344754
case acc: 0.1474752
top acc: 0.1330 ::: bot acc: 0.0996
top acc: 0.3234 ::: bot acc: 0.2702
top acc: 0.1925 ::: bot acc: 0.1191
top acc: 0.1644 ::: bot acc: 0.1136
top acc: 0.1282 ::: bot acc: 0.0766
top acc: 0.1683 ::: bot acc: 0.1252
current epoch: 5
train loss is 0.013143
average val loss: 0.014718, accuracy: 0.1597
average test loss: 0.013556, accuracy: 0.1526
case acc: 0.11471169
case acc: 0.27992752
case acc: 0.15091904
case acc: 0.13105395
case acc: 0.10397808
case acc: 0.13500264
top acc: 0.1314 ::: bot acc: 0.0985
top acc: 0.3072 ::: bot acc: 0.2542
top acc: 0.1889 ::: bot acc: 0.1154
top acc: 0.1568 ::: bot acc: 0.1052
top acc: 0.1282 ::: bot acc: 0.0772
top acc: 0.1562 ::: bot acc: 0.1123
current epoch: 6
train loss is 0.013632
average val loss: 0.004273, accuracy: 0.0729
average test loss: 0.003732, accuracy: 0.0657
case acc: 0.031181553
case acc: 0.17883082
case acc: 0.062190823
case acc: 0.043962408
case acc: 0.030993499
case acc: 0.046991605
top acc: 0.0480 ::: bot acc: 0.0152
top acc: 0.2058 ::: bot acc: 0.1530
top acc: 0.0995 ::: bot acc: 0.0277
top acc: 0.0684 ::: bot acc: 0.0207
top acc: 0.0522 ::: bot acc: 0.0107
top acc: 0.0675 ::: bot acc: 0.0254
current epoch: 7
train loss is 0.008228
average val loss: 0.001375, accuracy: 0.0386
average test loss: 0.001325, accuracy: 0.0397
case acc: 0.033218212
case acc: 0.098127745
case acc: 0.02850614
case acc: 0.028061902
case acc: 0.028971964
case acc: 0.021413462
top acc: 0.0164 ::: bot acc: 0.0495
top acc: 0.1255 ::: bot acc: 0.0723
top acc: 0.0278 ::: bot acc: 0.0453
top acc: 0.0108 ::: bot acc: 0.0498
top acc: 0.0088 ::: bot acc: 0.0539
top acc: 0.0080 ::: bot acc: 0.0403
current epoch: 8
train loss is 0.004006
average val loss: 0.001183, accuracy: 0.0343
average test loss: 0.001103, accuracy: 0.0345
case acc: 0.02578402
case acc: 0.09274265
case acc: 0.027952489
case acc: 0.023533855
case acc: 0.020363398
case acc: 0.016757289
top acc: 0.0107 ::: bot acc: 0.0413
top acc: 0.1198 ::: bot acc: 0.0668
top acc: 0.0296 ::: bot acc: 0.0435
top acc: 0.0114 ::: bot acc: 0.0428
top acc: 0.0129 ::: bot acc: 0.0388
top acc: 0.0123 ::: bot acc: 0.0307
current epoch: 9
train loss is 0.002938
average val loss: 0.001258, accuracy: 0.0349
average test loss: 0.001096, accuracy: 0.0327
case acc: 0.0147628635
case acc: 0.09723527
case acc: 0.026811257
case acc: 0.019077182
case acc: 0.020665633
case acc: 0.017493894
top acc: 0.0094 ::: bot acc: 0.0254
top acc: 0.1243 ::: bot acc: 0.0714
top acc: 0.0405 ::: bot acc: 0.0330
top acc: 0.0228 ::: bot acc: 0.0290
top acc: 0.0320 ::: bot acc: 0.0195
top acc: 0.0279 ::: bot acc: 0.0158
current epoch: 10
train loss is 0.002743
average val loss: 0.001201, accuracy: 0.0351
average test loss: 0.001033, accuracy: 0.0322
case acc: 0.012888146
case acc: 0.09248575
case acc: 0.026804607
case acc: 0.018895762
case acc: 0.023154626
case acc: 0.019259144
top acc: 0.0125 ::: bot acc: 0.0210
top acc: 0.1198 ::: bot acc: 0.0667
top acc: 0.0417 ::: bot acc: 0.0317
top acc: 0.0257 ::: bot acc: 0.0256
top acc: 0.0388 ::: bot acc: 0.0142
top acc: 0.0317 ::: bot acc: 0.0135
current epoch: 11
train loss is 0.002594
average val loss: 0.001059, accuracy: 0.0336
average test loss: 0.000905, accuracy: 0.0308
case acc: 0.0129685495
case acc: 0.083718754
case acc: 0.026884798
case acc: 0.018651722
case acc: 0.023821956
case acc: 0.018949917
top acc: 0.0133 ::: bot acc: 0.0206
top acc: 0.1110 ::: bot acc: 0.0578
top acc: 0.0387 ::: bot acc: 0.0347
top acc: 0.0248 ::: bot acc: 0.0262
top acc: 0.0403 ::: bot acc: 0.0134
top acc: 0.0311 ::: bot acc: 0.0135
current epoch: 12
train loss is 0.002314
average val loss: 0.001018, accuracy: 0.0337
average test loss: 0.000859, accuracy: 0.0306
case acc: 0.01214271
case acc: 0.0789
case acc: 0.026907118
case acc: 0.019081112
case acc: 0.026146382
case acc: 0.020502599
top acc: 0.0164 ::: bot acc: 0.0169
top acc: 0.1061 ::: bot acc: 0.0531
top acc: 0.0394 ::: bot acc: 0.0340
top acc: 0.0276 ::: bot acc: 0.0241
top acc: 0.0446 ::: bot acc: 0.0114
top acc: 0.0344 ::: bot acc: 0.0124
current epoch: 13
train loss is 0.002115
average val loss: 0.000920, accuracy: 0.0325
average test loss: 0.000766, accuracy: 0.0294
case acc: 0.012025633
case acc: 0.07146803
case acc: 0.026975391
case acc: 0.01886743
case acc: 0.026509259
case acc: 0.020324962
top acc: 0.0170 ::: bot acc: 0.0160
top acc: 0.0989 ::: bot acc: 0.0455
top acc: 0.0376 ::: bot acc: 0.0358
top acc: 0.0271 ::: bot acc: 0.0240
top acc: 0.0452 ::: bot acc: 0.0111
top acc: 0.0336 ::: bot acc: 0.0130
current epoch: 14
train loss is 0.001897
average val loss: 0.000846, accuracy: 0.0316
average test loss: 0.000703, accuracy: 0.0285
case acc: 0.012162569
case acc: 0.06560617
case acc: 0.026830489
case acc: 0.018941691
case acc: 0.02694253
case acc: 0.020222908
top acc: 0.0186 ::: bot acc: 0.0147
top acc: 0.0929 ::: bot acc: 0.0398
top acc: 0.0369 ::: bot acc: 0.0361
top acc: 0.0275 ::: bot acc: 0.0240
top acc: 0.0459 ::: bot acc: 0.0108
top acc: 0.0338 ::: bot acc: 0.0125
current epoch: 15
train loss is 0.001756
average val loss: 0.000804, accuracy: 0.0312
average test loss: 0.000658, accuracy: 0.0279
case acc: 0.012267888
case acc: 0.06086729
case acc: 0.027142828
case acc: 0.01912646
case acc: 0.027478302
case acc: 0.020356486
top acc: 0.0202 ::: bot acc: 0.0129
top acc: 0.0882 ::: bot acc: 0.0350
top acc: 0.0372 ::: bot acc: 0.0365
top acc: 0.0283 ::: bot acc: 0.0232
top acc: 0.0469 ::: bot acc: 0.0108
top acc: 0.0341 ::: bot acc: 0.0123
current epoch: 16
train loss is 0.001668
average val loss: 0.000771, accuracy: 0.0308
average test loss: 0.000623, accuracy: 0.0274
case acc: 0.012624155
case acc: 0.056996737
case acc: 0.02702276
case acc: 0.019206025
case acc: 0.02773877
case acc: 0.020671245
top acc: 0.0219 ::: bot acc: 0.0114
top acc: 0.0842 ::: bot acc: 0.0310
top acc: 0.0379 ::: bot acc: 0.0357
top acc: 0.0293 ::: bot acc: 0.0219
top acc: 0.0473 ::: bot acc: 0.0106
top acc: 0.0346 ::: bot acc: 0.0121
current epoch: 17
train loss is 0.001586
average val loss: 0.000792, accuracy: 0.0317
average test loss: 0.000630, accuracy: 0.0278
case acc: 0.013781035
case acc: 0.05554265
case acc: 0.026709411
case acc: 0.019698981
case acc: 0.029104723
case acc: 0.021974998
top acc: 0.0254 ::: bot acc: 0.0085
top acc: 0.0829 ::: bot acc: 0.0297
top acc: 0.0403 ::: bot acc: 0.0330
top acc: 0.0318 ::: bot acc: 0.0193
top acc: 0.0496 ::: bot acc: 0.0102
top acc: 0.0367 ::: bot acc: 0.0119
current epoch: 18
train loss is 0.001525
average val loss: 0.000795, accuracy: 0.0321
average test loss: 0.000623, accuracy: 0.0279
case acc: 0.015060917
case acc: 0.05342294
case acc: 0.02673407
case acc: 0.020329054
case acc: 0.029434923
case acc: 0.022581542
top acc: 0.0278 ::: bot acc: 0.0073
top acc: 0.0807 ::: bot acc: 0.0275
top acc: 0.0425 ::: bot acc: 0.0307
top acc: 0.0340 ::: bot acc: 0.0175
top acc: 0.0499 ::: bot acc: 0.0105
top acc: 0.0375 ::: bot acc: 0.0120
current epoch: 19
train loss is 0.001492
average val loss: 0.000758, accuracy: 0.0314
average test loss: 0.000587, accuracy: 0.0272
case acc: 0.01518133
case acc: 0.050179843
case acc: 0.026782459
case acc: 0.020297466
case acc: 0.028771996
case acc: 0.021894202
top acc: 0.0284 ::: bot acc: 0.0066
top acc: 0.0772 ::: bot acc: 0.0245
top acc: 0.0427 ::: bot acc: 0.0304
top acc: 0.0336 ::: bot acc: 0.0175
top acc: 0.0488 ::: bot acc: 0.0107
top acc: 0.0366 ::: bot acc: 0.0119
current epoch: 20
train loss is 0.001427
average val loss: 0.000737, accuracy: 0.0310
average test loss: 0.000567, accuracy: 0.0268
case acc: 0.015810134
case acc: 0.047638226
case acc: 0.02691149
case acc: 0.020487217
case acc: 0.028132705
case acc: 0.021631658
top acc: 0.0296 ::: bot acc: 0.0063
top acc: 0.0750 ::: bot acc: 0.0219
top acc: 0.0439 ::: bot acc: 0.0296
top acc: 0.0344 ::: bot acc: 0.0171
top acc: 0.0479 ::: bot acc: 0.0106
top acc: 0.0360 ::: bot acc: 0.0121
current epoch: 21
train loss is 0.001413
average val loss: 0.000746, accuracy: 0.0314
average test loss: 0.000565, accuracy: 0.0268
case acc: 0.016870916
case acc: 0.046336766
case acc: 0.026934933
case acc: 0.02099428
case acc: 0.028103352
case acc: 0.021786055
top acc: 0.0313 ::: bot acc: 0.0058
top acc: 0.0734 ::: bot acc: 0.0209
top acc: 0.0458 ::: bot acc: 0.0274
top acc: 0.0360 ::: bot acc: 0.0158
top acc: 0.0476 ::: bot acc: 0.0105
top acc: 0.0364 ::: bot acc: 0.0120
current epoch: 22
train loss is 0.001383
average val loss: 0.000819, accuracy: 0.0334
average test loss: 0.000617, accuracy: 0.0285
case acc: 0.019801931
case acc: 0.04764376
case acc: 0.027960254
case acc: 0.022382835
case acc: 0.029714122
case acc: 0.023284502
top acc: 0.0356 ::: bot acc: 0.0064
top acc: 0.0747 ::: bot acc: 0.0221
top acc: 0.0501 ::: bot acc: 0.0235
top acc: 0.0393 ::: bot acc: 0.0132
top acc: 0.0502 ::: bot acc: 0.0108
top acc: 0.0387 ::: bot acc: 0.0118
current epoch: 23
train loss is 0.001389
average val loss: 0.000889, accuracy: 0.0351
average test loss: 0.000662, accuracy: 0.0299
case acc: 0.022469766
case acc: 0.04830054
case acc: 0.02906523
case acc: 0.023752822
case acc: 0.031195614
case acc: 0.024443664
top acc: 0.0386 ::: bot acc: 0.0079
top acc: 0.0755 ::: bot acc: 0.0226
top acc: 0.0537 ::: bot acc: 0.0197
top acc: 0.0420 ::: bot acc: 0.0120
top acc: 0.0523 ::: bot acc: 0.0111
top acc: 0.0402 ::: bot acc: 0.0123
current epoch: 24
train loss is 0.001346
average val loss: 0.000810, accuracy: 0.0332
average test loss: 0.000601, accuracy: 0.0282
case acc: 0.021193963
case acc: 0.04475481
case acc: 0.028712813
case acc: 0.022805575
case acc: 0.029274493
case acc: 0.02238871
top acc: 0.0370 ::: bot acc: 0.0072
top acc: 0.0718 ::: bot acc: 0.0195
top acc: 0.0524 ::: bot acc: 0.0209
top acc: 0.0402 ::: bot acc: 0.0129
top acc: 0.0495 ::: bot acc: 0.0105
top acc: 0.0376 ::: bot acc: 0.0116
current epoch: 25
train loss is 0.001313
average val loss: 0.000859, accuracy: 0.0345
average test loss: 0.000635, accuracy: 0.0292
case acc: 0.023228908
case acc: 0.045113895
case acc: 0.029681638
case acc: 0.02388866
case acc: 0.030324358
case acc: 0.02323699
top acc: 0.0396 ::: bot acc: 0.0084
top acc: 0.0722 ::: bot acc: 0.0198
top acc: 0.0554 ::: bot acc: 0.0184
top acc: 0.0423 ::: bot acc: 0.0119
top acc: 0.0508 ::: bot acc: 0.0109
top acc: 0.0386 ::: bot acc: 0.0119
current epoch: 26
train loss is 0.001299
average val loss: 0.000820, accuracy: 0.0335
average test loss: 0.000604, accuracy: 0.0284
case acc: 0.022708358
case acc: 0.042939764
case acc: 0.029601468
case acc: 0.023554374
case acc: 0.029314304
case acc: 0.022121822
top acc: 0.0390 ::: bot acc: 0.0080
top acc: 0.0700 ::: bot acc: 0.0177
top acc: 0.0553 ::: bot acc: 0.0183
top acc: 0.0416 ::: bot acc: 0.0123
top acc: 0.0493 ::: bot acc: 0.0108
top acc: 0.0370 ::: bot acc: 0.0116
current epoch: 27
train loss is 0.001287
average val loss: 0.000832, accuracy: 0.0338
average test loss: 0.000607, accuracy: 0.0285
case acc: 0.023376137
case acc: 0.04226327
case acc: 0.029993564
case acc: 0.023997648
case acc: 0.029385477
case acc: 0.022135295
top acc: 0.0396 ::: bot acc: 0.0087
top acc: 0.0693 ::: bot acc: 0.0171
top acc: 0.0566 ::: bot acc: 0.0170
top acc: 0.0423 ::: bot acc: 0.0119
top acc: 0.0495 ::: bot acc: 0.0107
top acc: 0.0370 ::: bot acc: 0.0117
current epoch: 28
train loss is 0.001280
average val loss: 0.000870, accuracy: 0.0348
average test loss: 0.000634, accuracy: 0.0293
case acc: 0.02499877
case acc: 0.042450313
case acc: 0.031034373
case acc: 0.024776211
case acc: 0.029943433
case acc: 0.022746041
top acc: 0.0414 ::: bot acc: 0.0099
top acc: 0.0695 ::: bot acc: 0.0173
top acc: 0.0589 ::: bot acc: 0.0154
top acc: 0.0438 ::: bot acc: 0.0114
top acc: 0.0505 ::: bot acc: 0.0106
top acc: 0.0378 ::: bot acc: 0.0119
current epoch: 29
train loss is 0.001265
average val loss: 0.000921, accuracy: 0.0360
average test loss: 0.000670, accuracy: 0.0304
case acc: 0.026883211
case acc: 0.04286382
case acc: 0.031884976
case acc: 0.026000233
case acc: 0.031267367
case acc: 0.02335352
top acc: 0.0433 ::: bot acc: 0.0115
top acc: 0.0700 ::: bot acc: 0.0177
top acc: 0.0612 ::: bot acc: 0.0136
top acc: 0.0455 ::: bot acc: 0.0115
top acc: 0.0523 ::: bot acc: 0.0110
top acc: 0.0387 ::: bot acc: 0.0117
current epoch: 30
train loss is 0.001274
average val loss: 0.000930, accuracy: 0.0362
average test loss: 0.000678, accuracy: 0.0306
case acc: 0.027401164
case acc: 0.042368792
case acc: 0.032489944
case acc: 0.026405346
case acc: 0.031616066
case acc: 0.023430336
top acc: 0.0439 ::: bot acc: 0.0122
top acc: 0.0695 ::: bot acc: 0.0173
top acc: 0.0623 ::: bot acc: 0.0130
top acc: 0.0464 ::: bot acc: 0.0114
top acc: 0.0527 ::: bot acc: 0.0111
top acc: 0.0387 ::: bot acc: 0.0119
current epoch: 31
train loss is 0.001255
average val loss: 0.000948, accuracy: 0.0367
average test loss: 0.000691, accuracy: 0.0310
case acc: 0.028114924
case acc: 0.042170558
case acc: 0.033141572
case acc: 0.02689017
case acc: 0.0320988
case acc: 0.023410192
top acc: 0.0448 ::: bot acc: 0.0126
top acc: 0.0693 ::: bot acc: 0.0169
top acc: 0.0636 ::: bot acc: 0.0123
top acc: 0.0469 ::: bot acc: 0.0116
top acc: 0.0533 ::: bot acc: 0.0114
top acc: 0.0389 ::: bot acc: 0.0119
current epoch: 32
train loss is 0.001264
average val loss: 0.000989, accuracy: 0.0376
average test loss: 0.000721, accuracy: 0.0318
case acc: 0.029211007
case acc: 0.042608935
case acc: 0.034109104
case acc: 0.027898902
case acc: 0.03302115
case acc: 0.023927422
top acc: 0.0458 ::: bot acc: 0.0137
top acc: 0.0697 ::: bot acc: 0.0175
top acc: 0.0655 ::: bot acc: 0.0114
top acc: 0.0484 ::: bot acc: 0.0117
top acc: 0.0546 ::: bot acc: 0.0118
top acc: 0.0396 ::: bot acc: 0.0120
current epoch: 33
train loss is 0.001262
average val loss: 0.001030, accuracy: 0.0385
average test loss: 0.000751, accuracy: 0.0326
case acc: 0.03027081
case acc: 0.04273025
case acc: 0.035158306
case acc: 0.028761672
case acc: 0.034005713
case acc: 0.024581226
top acc: 0.0470 ::: bot acc: 0.0146
top acc: 0.0697 ::: bot acc: 0.0176
top acc: 0.0673 ::: bot acc: 0.0111
top acc: 0.0497 ::: bot acc: 0.0115
top acc: 0.0559 ::: bot acc: 0.0122
top acc: 0.0405 ::: bot acc: 0.0122
current epoch: 34
train loss is 0.001257
average val loss: 0.000996, accuracy: 0.0377
average test loss: 0.000722, accuracy: 0.0318
case acc: 0.029450133
case acc: 0.04119474
case acc: 0.03490466
case acc: 0.02838927
case acc: 0.033520173
case acc: 0.023566026
top acc: 0.0460 ::: bot acc: 0.0141
top acc: 0.0680 ::: bot acc: 0.0162
top acc: 0.0668 ::: bot acc: 0.0111
top acc: 0.0490 ::: bot acc: 0.0117
top acc: 0.0552 ::: bot acc: 0.0118
top acc: 0.0390 ::: bot acc: 0.0119
current epoch: 35
train loss is 0.001241
average val loss: 0.000990, accuracy: 0.0376
average test loss: 0.000718, accuracy: 0.0317
case acc: 0.0291453
case acc: 0.04031692
case acc: 0.03510675
case acc: 0.028411627
case acc: 0.03365812
case acc: 0.023517309
top acc: 0.0456 ::: bot acc: 0.0138
top acc: 0.0672 ::: bot acc: 0.0155
top acc: 0.0672 ::: bot acc: 0.0109
top acc: 0.0492 ::: bot acc: 0.0116
top acc: 0.0553 ::: bot acc: 0.0121
top acc: 0.0390 ::: bot acc: 0.0120
current epoch: 36
train loss is 0.001228
average val loss: 0.001003, accuracy: 0.0379
average test loss: 0.000725, accuracy: 0.0319
case acc: 0.029180193
case acc: 0.039988726
case acc: 0.035527784
case acc: 0.02892473
case acc: 0.033941254
case acc: 0.023808837
top acc: 0.0457 ::: bot acc: 0.0137
top acc: 0.0668 ::: bot acc: 0.0152
top acc: 0.0679 ::: bot acc: 0.0109
top acc: 0.0499 ::: bot acc: 0.0118
top acc: 0.0557 ::: bot acc: 0.0122
top acc: 0.0394 ::: bot acc: 0.0120
current epoch: 37
train loss is 0.001205
average val loss: 0.000924, accuracy: 0.0360
average test loss: 0.000661, accuracy: 0.0301
case acc: 0.026692487
case acc: 0.03739096
case acc: 0.03440768
case acc: 0.027663616
case acc: 0.032355443
case acc: 0.022378834
top acc: 0.0432 ::: bot acc: 0.0113
top acc: 0.0640 ::: bot acc: 0.0133
top acc: 0.0660 ::: bot acc: 0.0113
top acc: 0.0479 ::: bot acc: 0.0117
top acc: 0.0536 ::: bot acc: 0.0116
top acc: 0.0374 ::: bot acc: 0.0119
current epoch: 38
train loss is 0.001177
average val loss: 0.000862, accuracy: 0.0345
average test loss: 0.000613, accuracy: 0.0287
case acc: 0.024734426
case acc: 0.03498595
case acc: 0.033534326
case acc: 0.026612917
case acc: 0.03135777
case acc: 0.021151748
top acc: 0.0412 ::: bot acc: 0.0097
top acc: 0.0612 ::: bot acc: 0.0115
top acc: 0.0642 ::: bot acc: 0.0121
top acc: 0.0465 ::: bot acc: 0.0115
top acc: 0.0523 ::: bot acc: 0.0111
top acc: 0.0355 ::: bot acc: 0.0118
current epoch: 39
train loss is 0.001156
average val loss: 0.000837, accuracy: 0.0338
average test loss: 0.000591, accuracy: 0.0281
case acc: 0.02370967
case acc: 0.033545636
case acc: 0.033086374
case acc: 0.026177669
case acc: 0.031289604
case acc: 0.020711249
top acc: 0.0399 ::: bot acc: 0.0089
top acc: 0.0595 ::: bot acc: 0.0107
top acc: 0.0634 ::: bot acc: 0.0124
top acc: 0.0459 ::: bot acc: 0.0114
top acc: 0.0523 ::: bot acc: 0.0111
top acc: 0.0347 ::: bot acc: 0.0120
current epoch: 40
train loss is 0.001137
average val loss: 0.000836, accuracy: 0.0338
average test loss: 0.000591, accuracy: 0.0281
case acc: 0.023401054
case acc: 0.033052858
case acc: 0.033042118
case acc: 0.026332738
case acc: 0.0317876
case acc: 0.020878943
top acc: 0.0396 ::: bot acc: 0.0087
top acc: 0.0589 ::: bot acc: 0.0104
top acc: 0.0635 ::: bot acc: 0.0123
top acc: 0.0462 ::: bot acc: 0.0114
top acc: 0.0529 ::: bot acc: 0.0113
top acc: 0.0349 ::: bot acc: 0.0121
current epoch: 41
train loss is 0.001119
average val loss: 0.000757, accuracy: 0.0318
average test loss: 0.000532, accuracy: 0.0263
case acc: 0.0209452
case acc: 0.030229036
case acc: 0.031877775
case acc: 0.024845641
case acc: 0.03016408
case acc: 0.019497484
top acc: 0.0368 ::: bot acc: 0.0070
top acc: 0.0555 ::: bot acc: 0.0085
top acc: 0.0610 ::: bot acc: 0.0138
top acc: 0.0439 ::: bot acc: 0.0115
top acc: 0.0508 ::: bot acc: 0.0107
top acc: 0.0328 ::: bot acc: 0.0126
current epoch: 42
train loss is 0.001088
average val loss: 0.000677, accuracy: 0.0296
average test loss: 0.000472, accuracy: 0.0244
case acc: 0.018189726
case acc: 0.027468324
case acc: 0.030541621
case acc: 0.023422042
case acc: 0.02846081
case acc: 0.018239327
top acc: 0.0333 ::: bot acc: 0.0057
top acc: 0.0519 ::: bot acc: 0.0077
top acc: 0.0580 ::: bot acc: 0.0159
top acc: 0.0414 ::: bot acc: 0.0122
top acc: 0.0483 ::: bot acc: 0.0108
top acc: 0.0303 ::: bot acc: 0.0137
current epoch: 43
train loss is 0.001063
average val loss: 0.000606, accuracy: 0.0276
average test loss: 0.000422, accuracy: 0.0229
case acc: 0.016104162
case acc: 0.025387054
case acc: 0.029369256
case acc: 0.022259967
case acc: 0.026780583
case acc: 0.017344244
top acc: 0.0301 ::: bot acc: 0.0059
top acc: 0.0484 ::: bot acc: 0.0084
top acc: 0.0548 ::: bot acc: 0.0186
top acc: 0.0390 ::: bot acc: 0.0134
top acc: 0.0456 ::: bot acc: 0.0110
top acc: 0.0278 ::: bot acc: 0.0158
current epoch: 44
train loss is 0.001044
average val loss: 0.000583, accuracy: 0.0270
average test loss: 0.000406, accuracy: 0.0224
case acc: 0.015339252
case acc: 0.024331689
case acc: 0.02906294
case acc: 0.021743646
case acc: 0.026693242
case acc: 0.017089456
top acc: 0.0287 ::: bot acc: 0.0064
top acc: 0.0464 ::: bot acc: 0.0091
top acc: 0.0536 ::: bot acc: 0.0200
top acc: 0.0382 ::: bot acc: 0.0135
top acc: 0.0454 ::: bot acc: 0.0113
top acc: 0.0273 ::: bot acc: 0.0159
current epoch: 45
train loss is 0.001028
average val loss: 0.000538, accuracy: 0.0258
average test loss: 0.000377, accuracy: 0.0215
case acc: 0.014066039
case acc: 0.023032779
case acc: 0.028192518
case acc: 0.021073135
case acc: 0.025748212
case acc: 0.016600115
top acc: 0.0260 ::: bot acc: 0.0077
top acc: 0.0437 ::: bot acc: 0.0107
top acc: 0.0512 ::: bot acc: 0.0223
top acc: 0.0365 ::: bot acc: 0.0149
top acc: 0.0437 ::: bot acc: 0.0117
top acc: 0.0260 ::: bot acc: 0.0172
current epoch: 46
train loss is 0.001012
average val loss: 0.000473, accuracy: 0.0239
average test loss: 0.000339, accuracy: 0.0202
case acc: 0.012655371
case acc: 0.021341072
case acc: 0.027309714
case acc: 0.019964159
case acc: 0.023967657
case acc: 0.016011544
top acc: 0.0222 ::: bot acc: 0.0111
top acc: 0.0394 ::: bot acc: 0.0143
top acc: 0.0473 ::: bot acc: 0.0264
top acc: 0.0333 ::: bot acc: 0.0174
top acc: 0.0404 ::: bot acc: 0.0130
top acc: 0.0233 ::: bot acc: 0.0201
current epoch: 47
train loss is 0.000991
average val loss: 0.000450, accuracy: 0.0232
average test loss: 0.000325, accuracy: 0.0198
case acc: 0.012146351
case acc: 0.020803602
case acc: 0.026977524
case acc: 0.019606883
case acc: 0.023494525
case acc: 0.015869422
top acc: 0.0201 ::: bot acc: 0.0128
top acc: 0.0369 ::: bot acc: 0.0167
top acc: 0.0451 ::: bot acc: 0.0283
top acc: 0.0321 ::: bot acc: 0.0186
top acc: 0.0393 ::: bot acc: 0.0135
top acc: 0.0223 ::: bot acc: 0.0209
current epoch: 48
train loss is 0.000985
average val loss: 0.000416, accuracy: 0.0222
average test loss: 0.000312, accuracy: 0.0195
case acc: 0.0119015975
case acc: 0.020320402
case acc: 0.026806362
case acc: 0.019209404
case acc: 0.022898698
case acc: 0.015827227
top acc: 0.0174 ::: bot acc: 0.0156
top acc: 0.0334 ::: bot acc: 0.0203
top acc: 0.0418 ::: bot acc: 0.0316
top acc: 0.0301 ::: bot acc: 0.0209
top acc: 0.0379 ::: bot acc: 0.0146
top acc: 0.0210 ::: bot acc: 0.0226
current epoch: 49
train loss is 0.000981
average val loss: 0.000381, accuracy: 0.0211
average test loss: 0.000302, accuracy: 0.0193
case acc: 0.012385428
case acc: 0.02000756
case acc: 0.027019463
case acc: 0.01862372
case acc: 0.021765353
case acc: 0.015807798
top acc: 0.0138 ::: bot acc: 0.0193
top acc: 0.0290 ::: bot acc: 0.0247
top acc: 0.0376 ::: bot acc: 0.0360
top acc: 0.0270 ::: bot acc: 0.0237
top acc: 0.0356 ::: bot acc: 0.0160
top acc: 0.0190 ::: bot acc: 0.0245
current epoch: 50
train loss is 0.000975
average val loss: 0.000351, accuracy: 0.0201
average test loss: 0.000309, accuracy: 0.0196
case acc: 0.0142097045
case acc: 0.020557242
case acc: 0.027770115
case acc: 0.018623885
case acc: 0.020421593
case acc: 0.016156824
top acc: 0.0094 ::: bot acc: 0.0244
top acc: 0.0231 ::: bot acc: 0.0306
top acc: 0.0317 ::: bot acc: 0.0419
top acc: 0.0227 ::: bot acc: 0.0281
top acc: 0.0320 ::: bot acc: 0.0192
top acc: 0.0154 ::: bot acc: 0.0279
LME_Co_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6798 6798 6798
1.8562728 -0.6288155 0.21141115 -0.19947179
Validation: 756 756 756
Testing: 744 744 744
pre-processing time: 0.0003962516784667969
the split date is 2010-01-01
train dropout: 0.6 test dropout: 0.05
net initializing with time: 0.0027468204498291016
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.013177
average val loss: 0.005838, accuracy: 0.1020
average test loss: 0.006187, accuracy: 0.1034
case acc: 0.14281696
case acc: 0.08115853
case acc: 0.10457465
case acc: 0.10306401
case acc: 0.12854327
case acc: 0.059965014
top acc: 0.1103 ::: bot acc: 0.1762
top acc: 0.1073 ::: bot acc: 0.0549
top acc: 0.0601 ::: bot acc: 0.1494
top acc: 0.0732 ::: bot acc: 0.1338
top acc: 0.0963 ::: bot acc: 0.1589
top acc: 0.0270 ::: bot acc: 0.0922
current epoch: 2
train loss is 0.008565
average val loss: 0.002957, accuracy: 0.0535
average test loss: 0.003156, accuracy: 0.0574
case acc: 0.043455865
case acc: 0.1667455
case acc: 0.03332154
case acc: 0.023219498
case acc: 0.04207596
case acc: 0.03574712
top acc: 0.0188 ::: bot acc: 0.0722
top acc: 0.1919 ::: bot acc: 0.1412
top acc: 0.0420 ::: bot acc: 0.0471
top acc: 0.0232 ::: bot acc: 0.0384
top acc: 0.0150 ::: bot acc: 0.0695
top acc: 0.0686 ::: bot acc: 0.0140
current epoch: 3
train loss is 0.008269
average val loss: 0.007642, accuracy: 0.1017
average test loss: 0.007764, accuracy: 0.1012
case acc: 0.052921876
case acc: 0.24330994
case acc: 0.08903463
case acc: 0.07589812
case acc: 0.043210644
case acc: 0.10303582
top acc: 0.0839 ::: bot acc: 0.0234
top acc: 0.2686 ::: bot acc: 0.2176
top acc: 0.1326 ::: bot acc: 0.0448
top acc: 0.1060 ::: bot acc: 0.0461
top acc: 0.0736 ::: bot acc: 0.0165
top acc: 0.1429 ::: bot acc: 0.0670
current epoch: 4
train loss is 0.010474
average val loss: 0.016254, accuracy: 0.1680
average test loss: 0.016273, accuracy: 0.1667
case acc: 0.12505935
case acc: 0.30270067
case acc: 0.16047904
case acc: 0.1408641
case acc: 0.10981005
case acc: 0.16156496
top acc: 0.1575 ::: bot acc: 0.0932
top acc: 0.3275 ::: bot acc: 0.2775
top acc: 0.2048 ::: bot acc: 0.1154
top acc: 0.1717 ::: bot acc: 0.1096
top acc: 0.1418 ::: bot acc: 0.0803
top acc: 0.2016 ::: bot acc: 0.1256
current epoch: 5
train loss is 0.013258
average val loss: 0.013762, accuracy: 0.1541
average test loss: 0.013791, accuracy: 0.1529
case acc: 0.11674438
case acc: 0.27879003
case acc: 0.14954023
case acc: 0.12587814
case acc: 0.10382072
case acc: 0.14274389
top acc: 0.1487 ::: bot acc: 0.0852
top acc: 0.3041 ::: bot acc: 0.2536
top acc: 0.1939 ::: bot acc: 0.1043
top acc: 0.1566 ::: bot acc: 0.0946
top acc: 0.1358 ::: bot acc: 0.0744
top acc: 0.1830 ::: bot acc: 0.1063
current epoch: 6
train loss is 0.013347
average val loss: 0.003242, accuracy: 0.0588
average test loss: 0.003387, accuracy: 0.0604
case acc: 0.030857837
case acc: 0.16840875
case acc: 0.05515656
case acc: 0.03389067
case acc: 0.026331639
case acc: 0.047628816
top acc: 0.0561 ::: bot acc: 0.0139
top acc: 0.1930 ::: bot acc: 0.1436
top acc: 0.0947 ::: bot acc: 0.0195
top acc: 0.0593 ::: bot acc: 0.0135
top acc: 0.0504 ::: bot acc: 0.0128
top acc: 0.0852 ::: bot acc: 0.0160
current epoch: 7
train loss is 0.007593
average val loss: 0.001309, accuracy: 0.0386
average test loss: 0.001526, accuracy: 0.0441
case acc: 0.03670371
case acc: 0.09778724
case acc: 0.03386608
case acc: 0.034321234
case acc: 0.032416962
case acc: 0.029703345
top acc: 0.0173 ::: bot acc: 0.0623
top acc: 0.1231 ::: bot acc: 0.0728
top acc: 0.0331 ::: bot acc: 0.0563
top acc: 0.0154 ::: bot acc: 0.0598
top acc: 0.0140 ::: bot acc: 0.0554
top acc: 0.0319 ::: bot acc: 0.0447
current epoch: 8
train loss is 0.003705
average val loss: 0.001123, accuracy: 0.0340
average test loss: 0.001333, accuracy: 0.0397
case acc: 0.030507602
case acc: 0.0943585
case acc: 0.03349781
case acc: 0.028498404
case acc: 0.024268653
case acc: 0.027075157
top acc: 0.0188 ::: bot acc: 0.0518
top acc: 0.1193 ::: bot acc: 0.0694
top acc: 0.0367 ::: bot acc: 0.0526
top acc: 0.0149 ::: bot acc: 0.0514
top acc: 0.0223 ::: bot acc: 0.0391
top acc: 0.0419 ::: bot acc: 0.0345
current epoch: 9
train loss is 0.002738
average val loss: 0.001238, accuracy: 0.0346
average test loss: 0.001430, accuracy: 0.0393
case acc: 0.022866754
case acc: 0.102204844
case acc: 0.03384079
case acc: 0.022257235
case acc: 0.023894016
case acc: 0.030780172
top acc: 0.0312 ::: bot acc: 0.0331
top acc: 0.1271 ::: bot acc: 0.0771
top acc: 0.0507 ::: bot acc: 0.0388
top acc: 0.0279 ::: bot acc: 0.0340
top acc: 0.0446 ::: bot acc: 0.0170
top acc: 0.0598 ::: bot acc: 0.0170
current epoch: 10
train loss is 0.002740
average val loss: 0.001185, accuracy: 0.0348
average test loss: 0.001383, accuracy: 0.0394
case acc: 0.023025673
case acc: 0.09731602
case acc: 0.033912934
case acc: 0.022602573
case acc: 0.026704792
case acc: 0.032813348
top acc: 0.0354 ::: bot acc: 0.0287
top acc: 0.1225 ::: bot acc: 0.0724
top acc: 0.0517 ::: bot acc: 0.0376
top acc: 0.0316 ::: bot acc: 0.0314
top acc: 0.0512 ::: bot acc: 0.0129
top acc: 0.0635 ::: bot acc: 0.0157
current epoch: 11
train loss is 0.002521
average val loss: 0.000971, accuracy: 0.0320
average test loss: 0.001172, accuracy: 0.0367
case acc: 0.023071848
case acc: 0.08504937
case acc: 0.0334051
case acc: 0.022513838
case acc: 0.025768701
case acc: 0.030603748
top acc: 0.0326 ::: bot acc: 0.0319
top acc: 0.1099 ::: bot acc: 0.0600
top acc: 0.0453 ::: bot acc: 0.0440
top acc: 0.0268 ::: bot acc: 0.0355
top acc: 0.0494 ::: bot acc: 0.0134
top acc: 0.0594 ::: bot acc: 0.0173
current epoch: 12
train loss is 0.002216
average val loss: 0.000883, accuracy: 0.0311
average test loss: 0.001086, accuracy: 0.0358
case acc: 0.023044474
case acc: 0.07804933
case acc: 0.033401296
case acc: 0.022511339
case acc: 0.026928738
case acc: 0.030785441
top acc: 0.0342 ::: bot acc: 0.0306
top acc: 0.1032 ::: bot acc: 0.0527
top acc: 0.0439 ::: bot acc: 0.0452
top acc: 0.0270 ::: bot acc: 0.0353
top acc: 0.0517 ::: bot acc: 0.0121
top acc: 0.0600 ::: bot acc: 0.0168
current epoch: 13
train loss is 0.001980
average val loss: 0.000791, accuracy: 0.0299
average test loss: 0.000997, accuracy: 0.0346
case acc: 0.023068406
case acc: 0.07073712
case acc: 0.03332312
case acc: 0.022564892
case acc: 0.027316596
case acc: 0.03064035
top acc: 0.0347 ::: bot acc: 0.0299
top acc: 0.0956 ::: bot acc: 0.0459
top acc: 0.0419 ::: bot acc: 0.0475
top acc: 0.0267 ::: bot acc: 0.0360
top acc: 0.0519 ::: bot acc: 0.0125
top acc: 0.0592 ::: bot acc: 0.0175
current epoch: 14
train loss is 0.001737
average val loss: 0.000732, accuracy: 0.0292
average test loss: 0.000940, accuracy: 0.0338
case acc: 0.023141982
case acc: 0.06546046
case acc: 0.033310488
case acc: 0.02235267
case acc: 0.027569758
case acc: 0.031062186
top acc: 0.0360 ::: bot acc: 0.0282
top acc: 0.0904 ::: bot acc: 0.0405
top acc: 0.0418 ::: bot acc: 0.0475
top acc: 0.0274 ::: bot acc: 0.0345
top acc: 0.0527 ::: bot acc: 0.0120
top acc: 0.0602 ::: bot acc: 0.0171
current epoch: 15
train loss is 0.001619
average val loss: 0.000677, accuracy: 0.0283
average test loss: 0.000890, accuracy: 0.0330
case acc: 0.023401488
case acc: 0.060404245
case acc: 0.033356033
case acc: 0.022379966
case acc: 0.027472777
case acc: 0.03102677
top acc: 0.0375 ::: bot acc: 0.0272
top acc: 0.0852 ::: bot acc: 0.0354
top acc: 0.0417 ::: bot acc: 0.0479
top acc: 0.0280 ::: bot acc: 0.0345
top acc: 0.0527 ::: bot acc: 0.0120
top acc: 0.0603 ::: bot acc: 0.0170
current epoch: 16
train loss is 0.001569
average val loss: 0.000675, accuracy: 0.0287
average test loss: 0.000884, accuracy: 0.0331
case acc: 0.023972025
case acc: 0.058205254
case acc: 0.033431247
case acc: 0.022367489
case acc: 0.028464874
case acc: 0.032041006
top acc: 0.0406 ::: bot acc: 0.0238
top acc: 0.0831 ::: bot acc: 0.0333
top acc: 0.0440 ::: bot acc: 0.0454
top acc: 0.0308 ::: bot acc: 0.0317
top acc: 0.0544 ::: bot acc: 0.0112
top acc: 0.0624 ::: bot acc: 0.0158
current epoch: 17
train loss is 0.001459
average val loss: 0.000717, accuracy: 0.0301
average test loss: 0.000921, accuracy: 0.0341
case acc: 0.0256264
case acc: 0.0581916
case acc: 0.033589486
case acc: 0.022846725
case acc: 0.030613147
case acc: 0.03380291
top acc: 0.0456 ::: bot acc: 0.0192
top acc: 0.0831 ::: bot acc: 0.0332
top acc: 0.0481 ::: bot acc: 0.0413
top acc: 0.0351 ::: bot acc: 0.0274
top acc: 0.0579 ::: bot acc: 0.0108
top acc: 0.0656 ::: bot acc: 0.0147
current epoch: 18
train loss is 0.001415
average val loss: 0.000654, accuracy: 0.0287
average test loss: 0.000861, accuracy: 0.0329
case acc: 0.025650205
case acc: 0.053550616
case acc: 0.033553235
case acc: 0.022719912
case acc: 0.02933361
case acc: 0.032628343
top acc: 0.0455 ::: bot acc: 0.0194
top acc: 0.0784 ::: bot acc: 0.0287
top acc: 0.0474 ::: bot acc: 0.0421
top acc: 0.0340 ::: bot acc: 0.0287
top acc: 0.0557 ::: bot acc: 0.0112
top acc: 0.0635 ::: bot acc: 0.0152
current epoch: 19
train loss is 0.001350
average val loss: 0.000605, accuracy: 0.0277
average test loss: 0.000816, accuracy: 0.0320
case acc: 0.025687952
case acc: 0.049637754
case acc: 0.0336278
case acc: 0.022654992
case acc: 0.028292906
case acc: 0.031958785
top acc: 0.0455 ::: bot acc: 0.0195
top acc: 0.0745 ::: bot acc: 0.0250
top acc: 0.0472 ::: bot acc: 0.0425
top acc: 0.0337 ::: bot acc: 0.0289
top acc: 0.0541 ::: bot acc: 0.0114
top acc: 0.0621 ::: bot acc: 0.0159
current epoch: 20
train loss is 0.001317
average val loss: 0.000626, accuracy: 0.0284
average test loss: 0.000835, accuracy: 0.0325
case acc: 0.026986416
case acc: 0.04933386
case acc: 0.03377828
case acc: 0.022965716
case acc: 0.029087856
case acc: 0.032887
top acc: 0.0488 ::: bot acc: 0.0169
top acc: 0.0739 ::: bot acc: 0.0248
top acc: 0.0500 ::: bot acc: 0.0394
top acc: 0.0361 ::: bot acc: 0.0262
top acc: 0.0555 ::: bot acc: 0.0111
top acc: 0.0639 ::: bot acc: 0.0154
current epoch: 21
train loss is 0.001297
average val loss: 0.000597, accuracy: 0.0278
average test loss: 0.000807, accuracy: 0.0319
case acc: 0.027382929
case acc: 0.046499494
case acc: 0.03382908
case acc: 0.023225894
case acc: 0.028279927
case acc: 0.032396156
top acc: 0.0495 ::: bot acc: 0.0165
top acc: 0.0711 ::: bot acc: 0.0223
top acc: 0.0507 ::: bot acc: 0.0387
top acc: 0.0368 ::: bot acc: 0.0262
top acc: 0.0540 ::: bot acc: 0.0113
top acc: 0.0631 ::: bot acc: 0.0156
current epoch: 22
train loss is 0.001274
average val loss: 0.000577, accuracy: 0.0274
average test loss: 0.000791, accuracy: 0.0316
case acc: 0.027752122
case acc: 0.04468362
case acc: 0.033939093
case acc: 0.023252016
case acc: 0.027922546
case acc: 0.03207707
top acc: 0.0502 ::: bot acc: 0.0160
top acc: 0.0693 ::: bot acc: 0.0208
top acc: 0.0518 ::: bot acc: 0.0379
top acc: 0.0370 ::: bot acc: 0.0258
top acc: 0.0535 ::: bot acc: 0.0116
top acc: 0.0624 ::: bot acc: 0.0157
current epoch: 23
train loss is 0.001227
average val loss: 0.000618, accuracy: 0.0287
average test loss: 0.000826, accuracy: 0.0324
case acc: 0.02933629
case acc: 0.045349963
case acc: 0.034453996
case acc: 0.023894954
case acc: 0.028849049
case acc: 0.03272176
top acc: 0.0535 ::: bot acc: 0.0142
top acc: 0.0700 ::: bot acc: 0.0214
top acc: 0.0554 ::: bot acc: 0.0343
top acc: 0.0397 ::: bot acc: 0.0232
top acc: 0.0551 ::: bot acc: 0.0110
top acc: 0.0638 ::: bot acc: 0.0152
current epoch: 24
train loss is 0.001244
average val loss: 0.000618, accuracy: 0.0288
average test loss: 0.000825, accuracy: 0.0325
case acc: 0.03016408
case acc: 0.044483434
case acc: 0.034782786
case acc: 0.02409999
case acc: 0.02870292
case acc: 0.03253516
top acc: 0.0546 ::: bot acc: 0.0141
top acc: 0.0690 ::: bot acc: 0.0207
top acc: 0.0572 ::: bot acc: 0.0324
top acc: 0.0405 ::: bot acc: 0.0225
top acc: 0.0546 ::: bot acc: 0.0113
top acc: 0.0636 ::: bot acc: 0.0153
current epoch: 25
train loss is 0.001220
average val loss: 0.000673, accuracy: 0.0304
average test loss: 0.000880, accuracy: 0.0337
case acc: 0.032394137
case acc: 0.045897614
case acc: 0.035865784
case acc: 0.025132738
case acc: 0.029684804
case acc: 0.033416387
top acc: 0.0582 ::: bot acc: 0.0139
top acc: 0.0706 ::: bot acc: 0.0217
top acc: 0.0615 ::: bot acc: 0.0285
top acc: 0.0435 ::: bot acc: 0.0198
top acc: 0.0563 ::: bot acc: 0.0110
top acc: 0.0649 ::: bot acc: 0.0150
current epoch: 26
train loss is 0.001246
average val loss: 0.000807, accuracy: 0.0340
average test loss: 0.001006, accuracy: 0.0365
case acc: 0.03620112
case acc: 0.04961143
case acc: 0.037913952
case acc: 0.027150992
case acc: 0.03242441
case acc: 0.035700254
top acc: 0.0637 ::: bot acc: 0.0141
top acc: 0.0746 ::: bot acc: 0.0249
top acc: 0.0677 ::: bot acc: 0.0223
top acc: 0.0485 ::: bot acc: 0.0153
top acc: 0.0603 ::: bot acc: 0.0112
top acc: 0.0689 ::: bot acc: 0.0137
current epoch: 27
train loss is 0.001248
average val loss: 0.000818, accuracy: 0.0343
average test loss: 0.001017, accuracy: 0.0368
case acc: 0.03697044
case acc: 0.049420264
case acc: 0.03860029
case acc: 0.027557746
case acc: 0.032555968
case acc: 0.03541914
top acc: 0.0650 ::: bot acc: 0.0141
top acc: 0.0743 ::: bot acc: 0.0248
top acc: 0.0694 ::: bot acc: 0.0208
top acc: 0.0491 ::: bot acc: 0.0151
top acc: 0.0604 ::: bot acc: 0.0112
top acc: 0.0682 ::: bot acc: 0.0141
current epoch: 28
train loss is 0.001285
average val loss: 0.000902, accuracy: 0.0364
average test loss: 0.001103, accuracy: 0.0386
case acc: 0.039102748
case acc: 0.051309843
case acc: 0.040705595
case acc: 0.029171083
case acc: 0.034560103
case acc: 0.03650796
top acc: 0.0681 ::: bot acc: 0.0147
top acc: 0.0762 ::: bot acc: 0.0265
top acc: 0.0735 ::: bot acc: 0.0188
top acc: 0.0523 ::: bot acc: 0.0138
top acc: 0.0634 ::: bot acc: 0.0117
top acc: 0.0701 ::: bot acc: 0.0138
current epoch: 29
train loss is 0.001282
average val loss: 0.001003, accuracy: 0.0388
average test loss: 0.001195, accuracy: 0.0406
case acc: 0.041803367
case acc: 0.053114094
case acc: 0.042743783
case acc: 0.030971322
case acc: 0.0370276
case acc: 0.037640493
top acc: 0.0713 ::: bot acc: 0.0161
top acc: 0.0782 ::: bot acc: 0.0283
top acc: 0.0773 ::: bot acc: 0.0173
top acc: 0.0550 ::: bot acc: 0.0135
top acc: 0.0664 ::: bot acc: 0.0128
top acc: 0.0719 ::: bot acc: 0.0134
current epoch: 30
train loss is 0.001306
average val loss: 0.000987, accuracy: 0.0384
average test loss: 0.001183, accuracy: 0.0403
case acc: 0.04168622
case acc: 0.05192643
case acc: 0.04326337
case acc: 0.030837068
case acc: 0.037018728
case acc: 0.037056755
top acc: 0.0713 ::: bot acc: 0.0158
top acc: 0.0768 ::: bot acc: 0.0271
top acc: 0.0781 ::: bot acc: 0.0175
top acc: 0.0550 ::: bot acc: 0.0136
top acc: 0.0663 ::: bot acc: 0.0129
top acc: 0.0711 ::: bot acc: 0.0135
current epoch: 31
train loss is 0.001294
average val loss: 0.001048, accuracy: 0.0399
average test loss: 0.001241, accuracy: 0.0415
case acc: 0.043257795
case acc: 0.052493338
case acc: 0.044750597
case acc: 0.032027647
case acc: 0.038936116
case acc: 0.03776428
top acc: 0.0731 ::: bot acc: 0.0169
top acc: 0.0775 ::: bot acc: 0.0276
top acc: 0.0804 ::: bot acc: 0.0172
top acc: 0.0567 ::: bot acc: 0.0134
top acc: 0.0688 ::: bot acc: 0.0139
top acc: 0.0721 ::: bot acc: 0.0134
current epoch: 32
train loss is 0.001290
average val loss: 0.001027, accuracy: 0.0395
average test loss: 0.001221, accuracy: 0.0412
case acc: 0.04290683
case acc: 0.050846428
case acc: 0.04477686
case acc: 0.03190737
case acc: 0.03919584
case acc: 0.037322655
top acc: 0.0727 ::: bot acc: 0.0166
top acc: 0.0757 ::: bot acc: 0.0260
top acc: 0.0806 ::: bot acc: 0.0169
top acc: 0.0566 ::: bot acc: 0.0135
top acc: 0.0690 ::: bot acc: 0.0140
top acc: 0.0713 ::: bot acc: 0.0137
current epoch: 33
train loss is 0.001283
average val loss: 0.000989, accuracy: 0.0386
average test loss: 0.001183, accuracy: 0.0404
case acc: 0.04199541
case acc: 0.048805576
case acc: 0.044357255
case acc: 0.03144875
case acc: 0.038844228
case acc: 0.036742557
top acc: 0.0716 ::: bot acc: 0.0161
top acc: 0.0736 ::: bot acc: 0.0243
top acc: 0.0798 ::: bot acc: 0.0172
top acc: 0.0560 ::: bot acc: 0.0133
top acc: 0.0687 ::: bot acc: 0.0138
top acc: 0.0704 ::: bot acc: 0.0138
current epoch: 34
train loss is 0.001228
average val loss: 0.000861, accuracy: 0.0355
average test loss: 0.001058, accuracy: 0.0377
case acc: 0.038822193
case acc: 0.044211254
case acc: 0.042324074
case acc: 0.029606676
case acc: 0.03649174
case acc: 0.034725785
top acc: 0.0677 ::: bot acc: 0.0146
top acc: 0.0687 ::: bot acc: 0.0203
top acc: 0.0764 ::: bot acc: 0.0176
top acc: 0.0531 ::: bot acc: 0.0137
top acc: 0.0657 ::: bot acc: 0.0126
top acc: 0.0673 ::: bot acc: 0.0143
current epoch: 35
train loss is 0.001173
average val loss: 0.000813, accuracy: 0.0344
average test loss: 0.001012, accuracy: 0.0367
case acc: 0.037658945
case acc: 0.041762963
case acc: 0.041453462
case acc: 0.029013414
case acc: 0.035842188
case acc: 0.034349613
top acc: 0.0660 ::: bot acc: 0.0144
top acc: 0.0661 ::: bot acc: 0.0184
top acc: 0.0751 ::: bot acc: 0.0179
top acc: 0.0522 ::: bot acc: 0.0138
top acc: 0.0650 ::: bot acc: 0.0122
top acc: 0.0665 ::: bot acc: 0.0145
current epoch: 36
train loss is 0.001151
average val loss: 0.000737, accuracy: 0.0324
average test loss: 0.000941, accuracy: 0.0351
case acc: 0.035714306
case acc: 0.038395353
case acc: 0.04014083
case acc: 0.02813017
case acc: 0.034718182
case acc: 0.033489805
top acc: 0.0633 ::: bot acc: 0.0140
top acc: 0.0623 ::: bot acc: 0.0157
top acc: 0.0727 ::: bot acc: 0.0191
top acc: 0.0504 ::: bot acc: 0.0146
top acc: 0.0635 ::: bot acc: 0.0118
top acc: 0.0651 ::: bot acc: 0.0150
current epoch: 37
train loss is 0.001089
average val loss: 0.000638, accuracy: 0.0297
average test loss: 0.000843, accuracy: 0.0328
case acc: 0.033024434
case acc: 0.033863276
case acc: 0.038325764
case acc: 0.026767528
case acc: 0.03282462
case acc: 0.031861283
top acc: 0.0593 ::: bot acc: 0.0138
top acc: 0.0574 ::: bot acc: 0.0120
top acc: 0.0685 ::: bot acc: 0.0216
top acc: 0.0475 ::: bot acc: 0.0166
top acc: 0.0610 ::: bot acc: 0.0113
top acc: 0.0621 ::: bot acc: 0.0160
current epoch: 38
train loss is 0.001035
average val loss: 0.000523, accuracy: 0.0262
average test loss: 0.000731, accuracy: 0.0300
case acc: 0.029751053
case acc: 0.02884128
case acc: 0.03652302
case acc: 0.02501413
case acc: 0.030008035
case acc: 0.029874917
top acc: 0.0540 ::: bot acc: 0.0143
top acc: 0.0513 ::: bot acc: 0.0092
top acc: 0.0632 ::: bot acc: 0.0266
top acc: 0.0429 ::: bot acc: 0.0203
top acc: 0.0570 ::: bot acc: 0.0109
top acc: 0.0579 ::: bot acc: 0.0182
current epoch: 39
train loss is 0.000999
average val loss: 0.000466, accuracy: 0.0244
average test loss: 0.000677, accuracy: 0.0286
case acc: 0.027973494
case acc: 0.025955977
case acc: 0.03557055
case acc: 0.024284208
case acc: 0.028717423
case acc: 0.028951101
top acc: 0.0511 ::: bot acc: 0.0154
top acc: 0.0471 ::: bot acc: 0.0090
top acc: 0.0601 ::: bot acc: 0.0296
top acc: 0.0407 ::: bot acc: 0.0226
top acc: 0.0550 ::: bot acc: 0.0110
top acc: 0.0558 ::: bot acc: 0.0198
current epoch: 40
train loss is 0.000962
average val loss: 0.000402, accuracy: 0.0224
average test loss: 0.000615, accuracy: 0.0270
case acc: 0.026095614
case acc: 0.02302719
case acc: 0.034528032
case acc: 0.023365932
case acc: 0.026812533
case acc: 0.02791013
top acc: 0.0468 ::: bot acc: 0.0184
top acc: 0.0420 ::: bot acc: 0.0103
top acc: 0.0558 ::: bot acc: 0.0341
top acc: 0.0371 ::: bot acc: 0.0259
top acc: 0.0515 ::: bot acc: 0.0122
top acc: 0.0527 ::: bot acc: 0.0229
current epoch: 41
train loss is 0.000935
average val loss: 0.000351, accuracy: 0.0207
average test loss: 0.000570, accuracy: 0.0257
case acc: 0.024524083
case acc: 0.020657852
case acc: 0.033922225
case acc: 0.022833712
case acc: 0.025208116
case acc: 0.0270063
top acc: 0.0424 ::: bot acc: 0.0225
top acc: 0.0368 ::: bot acc: 0.0136
top acc: 0.0510 ::: bot acc: 0.0387
top acc: 0.0336 ::: bot acc: 0.0295
top acc: 0.0482 ::: bot acc: 0.0145
top acc: 0.0497 ::: bot acc: 0.0261
current epoch: 42
train loss is 0.000909
average val loss: 0.000307, accuracy: 0.0193
average test loss: 0.000532, accuracy: 0.0246
case acc: 0.023399329
case acc: 0.018574908
case acc: 0.03340676
case acc: 0.02267751
case acc: 0.023250999
case acc: 0.02653803
top acc: 0.0364 ::: bot acc: 0.0284
top acc: 0.0302 ::: bot acc: 0.0198
top acc: 0.0450 ::: bot acc: 0.0447
top acc: 0.0286 ::: bot acc: 0.0347
top acc: 0.0427 ::: bot acc: 0.0188
top acc: 0.0450 ::: bot acc: 0.0306
current epoch: 43
train loss is 0.000898
average val loss: 0.000298, accuracy: 0.0191
average test loss: 0.000525, accuracy: 0.0244
case acc: 0.023114879
case acc: 0.01799586
case acc: 0.033380054
case acc: 0.0229028
case acc: 0.022668755
case acc: 0.02659869
top acc: 0.0326 ::: bot acc: 0.0322
top acc: 0.0260 ::: bot acc: 0.0241
top acc: 0.0409 ::: bot acc: 0.0489
top acc: 0.0255 ::: bot acc: 0.0373
top acc: 0.0398 ::: bot acc: 0.0220
top acc: 0.0428 ::: bot acc: 0.0326
current epoch: 44
train loss is 0.000898
average val loss: 0.000299, accuracy: 0.0192
average test loss: 0.000530, accuracy: 0.0246
case acc: 0.02324712
case acc: 0.018143032
case acc: 0.03358258
case acc: 0.023610963
case acc: 0.022152942
case acc: 0.026819624
top acc: 0.0296 ::: bot acc: 0.0354
top acc: 0.0223 ::: bot acc: 0.0278
top acc: 0.0371 ::: bot acc: 0.0527
top acc: 0.0235 ::: bot acc: 0.0397
top acc: 0.0376 ::: bot acc: 0.0238
top acc: 0.0412 ::: bot acc: 0.0345
current epoch: 45
train loss is 0.000894
average val loss: 0.000316, accuracy: 0.0198
average test loss: 0.000550, accuracy: 0.0253
case acc: 0.024243379
case acc: 0.019219114
case acc: 0.034240335
case acc: 0.024678368
case acc: 0.021946523
case acc: 0.027247561
top acc: 0.0254 ::: bot acc: 0.0396
top acc: 0.0172 ::: bot acc: 0.0329
top acc: 0.0319 ::: bot acc: 0.0578
top acc: 0.0196 ::: bot acc: 0.0434
top acc: 0.0344 ::: bot acc: 0.0272
top acc: 0.0385 ::: bot acc: 0.0370
current epoch: 46
train loss is 0.000904
average val loss: 0.000340, accuracy: 0.0205
average test loss: 0.000575, accuracy: 0.0261
case acc: 0.025460206
case acc: 0.02099499
case acc: 0.03505279
case acc: 0.025515018
case acc: 0.021868601
case acc: 0.02756713
top acc: 0.0229 ::: bot acc: 0.0426
top acc: 0.0143 ::: bot acc: 0.0370
top acc: 0.0278 ::: bot acc: 0.0616
top acc: 0.0175 ::: bot acc: 0.0457
top acc: 0.0322 ::: bot acc: 0.0291
top acc: 0.0372 ::: bot acc: 0.0383
current epoch: 47
train loss is 0.000922
average val loss: 0.000407, accuracy: 0.0226
average test loss: 0.000649, accuracy: 0.0284
case acc: 0.028530741
case acc: 0.024874397
case acc: 0.03740153
case acc: 0.028127838
case acc: 0.02272383
case acc: 0.028746713
top acc: 0.0202 ::: bot acc: 0.0486
top acc: 0.0127 ::: bot acc: 0.0436
top acc: 0.0214 ::: bot acc: 0.0687
top acc: 0.0146 ::: bot acc: 0.0512
top acc: 0.0278 ::: bot acc: 0.0336
top acc: 0.0331 ::: bot acc: 0.0425
current epoch: 48
train loss is 0.000955
average val loss: 0.000500, accuracy: 0.0253
average test loss: 0.000747, accuracy: 0.0312
case acc: 0.031631943
case acc: 0.02949864
case acc: 0.040673885
case acc: 0.031313747
case acc: 0.023814531
case acc: 0.030005597
top acc: 0.0187 ::: bot acc: 0.0541
top acc: 0.0134 ::: bot acc: 0.0501
top acc: 0.0172 ::: bot acc: 0.0756
top acc: 0.0141 ::: bot acc: 0.0561
top acc: 0.0240 ::: bot acc: 0.0372
top acc: 0.0300 ::: bot acc: 0.0460
current epoch: 49
train loss is 0.001006
average val loss: 0.000661, accuracy: 0.0297
average test loss: 0.000914, accuracy: 0.0352
case acc: 0.03591062
case acc: 0.035899535
case acc: 0.045485333
case acc: 0.036006745
case acc: 0.025763871
case acc: 0.031870503
top acc: 0.0177 ::: bot acc: 0.0609
top acc: 0.0160 ::: bot acc: 0.0584
top acc: 0.0142 ::: bot acc: 0.0843
top acc: 0.0157 ::: bot acc: 0.0624
top acc: 0.0193 ::: bot acc: 0.0425
top acc: 0.0264 ::: bot acc: 0.0504
current epoch: 50
train loss is 0.001080
average val loss: 0.000879, accuracy: 0.0350
average test loss: 0.001143, accuracy: 0.0400
case acc: 0.040942915
case acc: 0.043596096
case acc: 0.052071508
case acc: 0.041332792
case acc: 0.028378049
case acc: 0.033916757
top acc: 0.0184 ::: bot acc: 0.0681
top acc: 0.0213 ::: bot acc: 0.0674
top acc: 0.0150 ::: bot acc: 0.0937
top acc: 0.0181 ::: bot acc: 0.0693
top acc: 0.0161 ::: bot acc: 0.0480
top acc: 0.0237 ::: bot acc: 0.0548
LME_Co_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6768 6768 6768
1.7082474 -0.6288155 0.21141115 -0.19947179
Validation: 756 756 756
Testing: 774 774 774
pre-processing time: 0.00019979476928710938
the split date is 2010-07-01
train dropout: 0.6 test dropout: 0.05
net initializing with time: 0.0021941661834716797
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012875
average val loss: 0.006390, accuracy: 0.1049
average test loss: 0.006614, accuracy: 0.1058
case acc: 0.15127285
case acc: 0.075531274
case acc: 0.11109832
case acc: 0.094584666
case acc: 0.13974969
case acc: 0.06245352
top acc: 0.1228 ::: bot acc: 0.1794
top acc: 0.1026 ::: bot acc: 0.0478
top acc: 0.0727 ::: bot acc: 0.1493
top acc: 0.0525 ::: bot acc: 0.1347
top acc: 0.0985 ::: bot acc: 0.1736
top acc: 0.0301 ::: bot acc: 0.0936
current epoch: 2
train loss is 0.008317
average val loss: 0.003025, accuracy: 0.0577
average test loss: 0.003152, accuracy: 0.0601
case acc: 0.0520164
case acc: 0.15782133
case acc: 0.029983386
case acc: 0.03059009
case acc: 0.05914746
case acc: 0.031007778
top acc: 0.0251 ::: bot acc: 0.0793
top acc: 0.1849 ::: bot acc: 0.1299
top acc: 0.0270 ::: bot acc: 0.0504
top acc: 0.0412 ::: bot acc: 0.0428
top acc: 0.0275 ::: bot acc: 0.0885
top acc: 0.0612 ::: bot acc: 0.0129
current epoch: 3
train loss is 0.008617
average val loss: 0.006778, accuracy: 0.0919
average test loss: 0.006844, accuracy: 0.0916
case acc: 0.038053248
case acc: 0.23104064
case acc: 0.0764881
case acc: 0.07853575
case acc: 0.031547718
case acc: 0.094162256
top acc: 0.0646 ::: bot acc: 0.0133
top acc: 0.2586 ::: bot acc: 0.2030
top acc: 0.1144 ::: bot acc: 0.0396
top acc: 0.1213 ::: bot acc: 0.0392
top acc: 0.0657 ::: bot acc: 0.0126
top acc: 0.1324 ::: bot acc: 0.0603
current epoch: 4
train loss is 0.009942
average val loss: 0.015277, accuracy: 0.1608
average test loss: 0.015230, accuracy: 0.1597
case acc: 0.11399714
case acc: 0.29458597
case acc: 0.15193789
case acc: 0.1467183
case acc: 0.09555297
case acc: 0.15547003
top acc: 0.1421 ::: bot acc: 0.0862
top acc: 0.3220 ::: bot acc: 0.2669
top acc: 0.1904 ::: bot acc: 0.1136
top acc: 0.1900 ::: bot acc: 0.1056
top acc: 0.1377 ::: bot acc: 0.0607
top acc: 0.1936 ::: bot acc: 0.1212
current epoch: 5
train loss is 0.012879
average val loss: 0.013409, accuracy: 0.1505
average test loss: 0.013379, accuracy: 0.1496
case acc: 0.10924799
case acc: 0.2749425
case acc: 0.1447584
case acc: 0.13543606
case acc: 0.09319248
case acc: 0.14004818
top acc: 0.1374 ::: bot acc: 0.0812
top acc: 0.3021 ::: bot acc: 0.2473
top acc: 0.1830 ::: bot acc: 0.1068
top acc: 0.1781 ::: bot acc: 0.0945
top acc: 0.1356 ::: bot acc: 0.0582
top acc: 0.1785 ::: bot acc: 0.1055
current epoch: 6
train loss is 0.013143
average val loss: 0.003589, accuracy: 0.0631
average test loss: 0.003621, accuracy: 0.0635
case acc: 0.0279387
case acc: 0.17072128
case acc: 0.054800365
case acc: 0.04879456
case acc: 0.02839714
case acc: 0.050622437
top acc: 0.0504 ::: bot acc: 0.0114
top acc: 0.1982 ::: bot acc: 0.1430
top acc: 0.0909 ::: bot acc: 0.0216
top acc: 0.0870 ::: bot acc: 0.0180
top acc: 0.0566 ::: bot acc: 0.0211
top acc: 0.0866 ::: bot acc: 0.0207
current epoch: 7
train loss is 0.007743
average val loss: 0.001512, accuracy: 0.0444
average test loss: 0.001590, accuracy: 0.0459
case acc: 0.039416537
case acc: 0.09478264
case acc: 0.030712454
case acc: 0.03550437
case acc: 0.046074513
case acc: 0.029154211
top acc: 0.0161 ::: bot acc: 0.0650
top acc: 0.1222 ::: bot acc: 0.0671
top acc: 0.0236 ::: bot acc: 0.0528
top acc: 0.0269 ::: bot acc: 0.0592
top acc: 0.0242 ::: bot acc: 0.0706
top acc: 0.0274 ::: bot acc: 0.0453
current epoch: 8
train loss is 0.003692
average val loss: 0.001304, accuracy: 0.0396
average test loss: 0.001370, accuracy: 0.0413
case acc: 0.03154711
case acc: 0.09132114
case acc: 0.029606264
case acc: 0.032011174
case acc: 0.03601527
case acc: 0.027133543
top acc: 0.0123 ::: bot acc: 0.0553
top acc: 0.1188 ::: bot acc: 0.0639
top acc: 0.0272 ::: bot acc: 0.0492
top acc: 0.0340 ::: bot acc: 0.0506
top acc: 0.0270 ::: bot acc: 0.0541
top acc: 0.0386 ::: bot acc: 0.0342
current epoch: 9
train loss is 0.002668
average val loss: 0.001304, accuracy: 0.0380
average test loss: 0.001356, accuracy: 0.0389
case acc: 0.022537202
case acc: 0.09537098
case acc: 0.028153243
case acc: 0.030448042
case acc: 0.0288924
case acc: 0.028272938
top acc: 0.0163 ::: bot acc: 0.0393
top acc: 0.1225 ::: bot acc: 0.0678
top acc: 0.0376 ::: bot acc: 0.0391
top acc: 0.0481 ::: bot acc: 0.0370
top acc: 0.0419 ::: bot acc: 0.0354
top acc: 0.0537 ::: bot acc: 0.0192
current epoch: 10
train loss is 0.002464
average val loss: 0.001241, accuracy: 0.0373
average test loss: 0.001291, accuracy: 0.0380
case acc: 0.021398466
case acc: 0.09043778
case acc: 0.028162314
case acc: 0.03086717
case acc: 0.027720045
case acc: 0.029224565
top acc: 0.0205 ::: bot acc: 0.0355
top acc: 0.1179 ::: bot acc: 0.0625
top acc: 0.0385 ::: bot acc: 0.0382
top acc: 0.0512 ::: bot acc: 0.0339
top acc: 0.0477 ::: bot acc: 0.0293
top acc: 0.0572 ::: bot acc: 0.0153
current epoch: 11
train loss is 0.002326
average val loss: 0.001107, accuracy: 0.0358
average test loss: 0.001156, accuracy: 0.0364
case acc: 0.021401545
case acc: 0.081261806
case acc: 0.028338095
case acc: 0.03065711
case acc: 0.02774284
case acc: 0.029058652
top acc: 0.0205 ::: bot acc: 0.0355
top acc: 0.1089 ::: bot acc: 0.0535
top acc: 0.0349 ::: bot acc: 0.0416
top acc: 0.0498 ::: bot acc: 0.0351
top acc: 0.0486 ::: bot acc: 0.0284
top acc: 0.0567 ::: bot acc: 0.0163
current epoch: 12
train loss is 0.002048
average val loss: 0.001057, accuracy: 0.0353
average test loss: 0.001106, accuracy: 0.0357
case acc: 0.020905502
case acc: 0.07625308
case acc: 0.028247911
case acc: 0.031022612
case acc: 0.027804866
case acc: 0.030138174
top acc: 0.0237 ::: bot acc: 0.0320
top acc: 0.1035 ::: bot acc: 0.0484
top acc: 0.0355 ::: bot acc: 0.0412
top acc: 0.0524 ::: bot acc: 0.0326
top acc: 0.0530 ::: bot acc: 0.0242
top acc: 0.0594 ::: bot acc: 0.0139
current epoch: 13
train loss is 0.001844
average val loss: 0.000960, accuracy: 0.0340
average test loss: 0.001002, accuracy: 0.0343
case acc: 0.020773387
case acc: 0.06791436
case acc: 0.028480152
case acc: 0.030875087
case acc: 0.028062783
case acc: 0.02960397
top acc: 0.0237 ::: bot acc: 0.0320
top acc: 0.0952 ::: bot acc: 0.0401
top acc: 0.0329 ::: bot acc: 0.0439
top acc: 0.0511 ::: bot acc: 0.0338
top acc: 0.0530 ::: bot acc: 0.0245
top acc: 0.0583 ::: bot acc: 0.0150
current epoch: 14
train loss is 0.001631
average val loss: 0.000904, accuracy: 0.0332
average test loss: 0.000948, accuracy: 0.0334
case acc: 0.020751487
case acc: 0.062476344
case acc: 0.028659033
case acc: 0.03083541
case acc: 0.028137403
case acc: 0.029738216
top acc: 0.0258 ::: bot acc: 0.0302
top acc: 0.0899 ::: bot acc: 0.0347
top acc: 0.0329 ::: bot acc: 0.0441
top acc: 0.0518 ::: bot acc: 0.0327
top acc: 0.0543 ::: bot acc: 0.0233
top acc: 0.0586 ::: bot acc: 0.0143
current epoch: 15
train loss is 0.001493
average val loss: 0.000857, accuracy: 0.0324
average test loss: 0.000903, accuracy: 0.0327
case acc: 0.020800287
case acc: 0.057581466
case acc: 0.028701577
case acc: 0.030992804
case acc: 0.028150255
case acc: 0.029876947
top acc: 0.0272 ::: bot acc: 0.0290
top acc: 0.0848 ::: bot acc: 0.0297
top acc: 0.0325 ::: bot acc: 0.0444
top acc: 0.0526 ::: bot acc: 0.0323
top acc: 0.0548 ::: bot acc: 0.0230
top acc: 0.0589 ::: bot acc: 0.0145
current epoch: 16
train loss is 0.001410
average val loss: 0.000824, accuracy: 0.0318
average test loss: 0.000870, accuracy: 0.0321
case acc: 0.020609219
case acc: 0.05382465
case acc: 0.028521812
case acc: 0.031317264
case acc: 0.028289666
case acc: 0.029997885
top acc: 0.0288 ::: bot acc: 0.0272
top acc: 0.0811 ::: bot acc: 0.0261
top acc: 0.0332 ::: bot acc: 0.0436
top acc: 0.0537 ::: bot acc: 0.0312
top acc: 0.0553 ::: bot acc: 0.0224
top acc: 0.0596 ::: bot acc: 0.0137
current epoch: 17
train loss is 0.001334
average val loss: 0.000821, accuracy: 0.0319
average test loss: 0.000862, accuracy: 0.0319
case acc: 0.02064017
case acc: 0.051712457
case acc: 0.028270056
case acc: 0.031593353
case acc: 0.028354792
case acc: 0.030581465
top acc: 0.0316 ::: bot acc: 0.0244
top acc: 0.0791 ::: bot acc: 0.0240
top acc: 0.0353 ::: bot acc: 0.0415
top acc: 0.0556 ::: bot acc: 0.0290
top acc: 0.0566 ::: bot acc: 0.0210
top acc: 0.0608 ::: bot acc: 0.0128
current epoch: 18
train loss is 0.001262
average val loss: 0.000804, accuracy: 0.0316
average test loss: 0.000846, accuracy: 0.0316
case acc: 0.02100562
case acc: 0.04924859
case acc: 0.028132532
case acc: 0.0320675
case acc: 0.028212944
case acc: 0.03101266
top acc: 0.0337 ::: bot acc: 0.0222
top acc: 0.0764 ::: bot acc: 0.0218
top acc: 0.0367 ::: bot acc: 0.0399
top acc: 0.0572 ::: bot acc: 0.0277
top acc: 0.0563 ::: bot acc: 0.0210
top acc: 0.0615 ::: bot acc: 0.0129
current epoch: 19
train loss is 0.001220
average val loss: 0.000774, accuracy: 0.0310
average test loss: 0.000823, accuracy: 0.0312
case acc: 0.021307848
case acc: 0.04641517
case acc: 0.028353466
case acc: 0.03218045
case acc: 0.028057456
case acc: 0.030613162
top acc: 0.0348 ::: bot acc: 0.0216
top acc: 0.0736 ::: bot acc: 0.0194
top acc: 0.0376 ::: bot acc: 0.0398
top acc: 0.0577 ::: bot acc: 0.0274
top acc: 0.0553 ::: bot acc: 0.0221
top acc: 0.0607 ::: bot acc: 0.0131
current epoch: 20
train loss is 0.001162
average val loss: 0.000759, accuracy: 0.0308
average test loss: 0.000804, accuracy: 0.0308
case acc: 0.021563755
case acc: 0.04425916
case acc: 0.028113514
case acc: 0.032407105
case acc: 0.027983425
case acc: 0.030615104
top acc: 0.0361 ::: bot acc: 0.0201
top acc: 0.0712 ::: bot acc: 0.0176
top acc: 0.0387 ::: bot acc: 0.0381
top acc: 0.0584 ::: bot acc: 0.0265
top acc: 0.0546 ::: bot acc: 0.0226
top acc: 0.0606 ::: bot acc: 0.0133
current epoch: 21
train loss is 0.001151
average val loss: 0.000750, accuracy: 0.0306
average test loss: 0.000793, accuracy: 0.0306
case acc: 0.021902235
case acc: 0.042575907
case acc: 0.028249292
case acc: 0.03264036
case acc: 0.027969759
case acc: 0.030444603
top acc: 0.0374 ::: bot acc: 0.0188
top acc: 0.0691 ::: bot acc: 0.0164
top acc: 0.0403 ::: bot acc: 0.0367
top acc: 0.0594 ::: bot acc: 0.0254
top acc: 0.0541 ::: bot acc: 0.0232
top acc: 0.0604 ::: bot acc: 0.0132
current epoch: 22
train loss is 0.001114
average val loss: 0.000780, accuracy: 0.0313
average test loss: 0.000822, accuracy: 0.0312
case acc: 0.022987012
case acc: 0.043070477
case acc: 0.028202225
case acc: 0.033523835
case acc: 0.028230622
case acc: 0.031211974
top acc: 0.0409 ::: bot acc: 0.0156
top acc: 0.0698 ::: bot acc: 0.0166
top acc: 0.0437 ::: bot acc: 0.0332
top acc: 0.0620 ::: bot acc: 0.0229
top acc: 0.0557 ::: bot acc: 0.0218
top acc: 0.0619 ::: bot acc: 0.0125
current epoch: 23
train loss is 0.001114
average val loss: 0.000820, accuracy: 0.0323
average test loss: 0.000860, accuracy: 0.0320
case acc: 0.024497516
case acc: 0.043577068
case acc: 0.028673168
case acc: 0.03466119
case acc: 0.028618045
case acc: 0.032163963
top acc: 0.0442 ::: bot acc: 0.0133
top acc: 0.0703 ::: bot acc: 0.0170
top acc: 0.0474 ::: bot acc: 0.0297
top acc: 0.0647 ::: bot acc: 0.0211
top acc: 0.0577 ::: bot acc: 0.0199
top acc: 0.0637 ::: bot acc: 0.0120
current epoch: 24
train loss is 0.001082
average val loss: 0.000785, accuracy: 0.0315
average test loss: 0.000828, accuracy: 0.0313
case acc: 0.02437087
case acc: 0.041306674
case acc: 0.02865023
case acc: 0.03439691
case acc: 0.028164763
case acc: 0.031193564
top acc: 0.0441 ::: bot acc: 0.0135
top acc: 0.0678 ::: bot acc: 0.0153
top acc: 0.0472 ::: bot acc: 0.0298
top acc: 0.0641 ::: bot acc: 0.0216
top acc: 0.0558 ::: bot acc: 0.0215
top acc: 0.0618 ::: bot acc: 0.0126
current epoch: 25
train loss is 0.001049
average val loss: 0.000805, accuracy: 0.0319
average test loss: 0.000843, accuracy: 0.0317
case acc: 0.025145443
case acc: 0.04123021
case acc: 0.029189661
case acc: 0.034978688
case acc: 0.02834808
case acc: 0.03143898
top acc: 0.0456 ::: bot acc: 0.0124
top acc: 0.0678 ::: bot acc: 0.0153
top acc: 0.0497 ::: bot acc: 0.0275
top acc: 0.0655 ::: bot acc: 0.0204
top acc: 0.0566 ::: bot acc: 0.0209
top acc: 0.0622 ::: bot acc: 0.0123
current epoch: 26
train loss is 0.001030
average val loss: 0.000781, accuracy: 0.0314
average test loss: 0.000828, accuracy: 0.0314
case acc: 0.025292834
case acc: 0.039761387
case acc: 0.029380243
case acc: 0.03492939
case acc: 0.028248644
case acc: 0.030865803
top acc: 0.0457 ::: bot acc: 0.0126
top acc: 0.0660 ::: bot acc: 0.0145
top acc: 0.0502 ::: bot acc: 0.0271
top acc: 0.0655 ::: bot acc: 0.0206
top acc: 0.0555 ::: bot acc: 0.0222
top acc: 0.0612 ::: bot acc: 0.0128
current epoch: 27
train loss is 0.001023
average val loss: 0.000809, accuracy: 0.0321
average test loss: 0.000850, accuracy: 0.0319
case acc: 0.026121983
case acc: 0.03998395
case acc: 0.030026995
case acc: 0.035769198
case acc: 0.028342694
case acc: 0.031257786
top acc: 0.0473 ::: bot acc: 0.0118
top acc: 0.0663 ::: bot acc: 0.0145
top acc: 0.0524 ::: bot acc: 0.0248
top acc: 0.0672 ::: bot acc: 0.0197
top acc: 0.0565 ::: bot acc: 0.0211
top acc: 0.0621 ::: bot acc: 0.0124
current epoch: 28
train loss is 0.001030
average val loss: 0.000843, accuracy: 0.0329
average test loss: 0.000886, accuracy: 0.0327
case acc: 0.02738346
case acc: 0.040624425
case acc: 0.03086748
case acc: 0.036936454
case acc: 0.028667733
case acc: 0.031898774
top acc: 0.0497 ::: bot acc: 0.0111
top acc: 0.0670 ::: bot acc: 0.0148
top acc: 0.0551 ::: bot acc: 0.0219
top acc: 0.0694 ::: bot acc: 0.0189
top acc: 0.0578 ::: bot acc: 0.0198
top acc: 0.0632 ::: bot acc: 0.0119
current epoch: 29
train loss is 0.001020
average val loss: 0.000885, accuracy: 0.0339
average test loss: 0.000922, accuracy: 0.0336
case acc: 0.028785273
case acc: 0.04115411
case acc: 0.031812284
case acc: 0.03796497
case acc: 0.02911692
case acc: 0.032516725
top acc: 0.0521 ::: bot acc: 0.0107
top acc: 0.0675 ::: bot acc: 0.0153
top acc: 0.0575 ::: bot acc: 0.0197
top acc: 0.0710 ::: bot acc: 0.0183
top acc: 0.0596 ::: bot acc: 0.0179
top acc: 0.0643 ::: bot acc: 0.0115
current epoch: 30
train loss is 0.001026
average val loss: 0.000885, accuracy: 0.0338
average test loss: 0.000923, accuracy: 0.0335
case acc: 0.02893938
case acc: 0.04062018
case acc: 0.032203946
case acc: 0.03803733
case acc: 0.029222947
case acc: 0.03225938
top acc: 0.0522 ::: bot acc: 0.0106
top acc: 0.0671 ::: bot acc: 0.0148
top acc: 0.0587 ::: bot acc: 0.0189
top acc: 0.0713 ::: bot acc: 0.0181
top acc: 0.0601 ::: bot acc: 0.0174
top acc: 0.0640 ::: bot acc: 0.0117
current epoch: 31
train loss is 0.001012
average val loss: 0.000904, accuracy: 0.0343
average test loss: 0.000943, accuracy: 0.0340
case acc: 0.029781843
case acc: 0.040804848
case acc: 0.032839794
case acc: 0.038550228
case acc: 0.029491654
case acc: 0.032485347
top acc: 0.0533 ::: bot acc: 0.0109
top acc: 0.0673 ::: bot acc: 0.0150
top acc: 0.0604 ::: bot acc: 0.0173
top acc: 0.0722 ::: bot acc: 0.0179
top acc: 0.0611 ::: bot acc: 0.0165
top acc: 0.0642 ::: bot acc: 0.0117
current epoch: 32
train loss is 0.001024
average val loss: 0.000938, accuracy: 0.0350
average test loss: 0.000973, accuracy: 0.0347
case acc: 0.030579727
case acc: 0.041298416
case acc: 0.033713926
case acc: 0.039458983
case acc: 0.029962767
case acc: 0.03289288
top acc: 0.0546 ::: bot acc: 0.0108
top acc: 0.0678 ::: bot acc: 0.0154
top acc: 0.0622 ::: bot acc: 0.0161
top acc: 0.0737 ::: bot acc: 0.0174
top acc: 0.0621 ::: bot acc: 0.0156
top acc: 0.0651 ::: bot acc: 0.0115
current epoch: 33
train loss is 0.001024
average val loss: 0.000988, accuracy: 0.0362
average test loss: 0.001024, accuracy: 0.0357
case acc: 0.031868353
case acc: 0.042149775
case acc: 0.035141803
case acc: 0.04078237
case acc: 0.030667339
case acc: 0.03373403
top acc: 0.0566 ::: bot acc: 0.0107
top acc: 0.0688 ::: bot acc: 0.0159
top acc: 0.0648 ::: bot acc: 0.0152
top acc: 0.0758 ::: bot acc: 0.0172
top acc: 0.0640 ::: bot acc: 0.0138
top acc: 0.0664 ::: bot acc: 0.0112
current epoch: 34
train loss is 0.001035
average val loss: 0.000979, accuracy: 0.0360
average test loss: 0.001016, accuracy: 0.0355
case acc: 0.03166894
case acc: 0.04133902
case acc: 0.03520349
case acc: 0.040829908
case acc: 0.030785223
case acc: 0.03336091
top acc: 0.0563 ::: bot acc: 0.0108
top acc: 0.0677 ::: bot acc: 0.0155
top acc: 0.0651 ::: bot acc: 0.0150
top acc: 0.0761 ::: bot acc: 0.0173
top acc: 0.0644 ::: bot acc: 0.0137
top acc: 0.0658 ::: bot acc: 0.0112
current epoch: 35
train loss is 0.001019
average val loss: 0.000976, accuracy: 0.0359
average test loss: 0.001012, accuracy: 0.0355
case acc: 0.031569634
case acc: 0.040585276
case acc: 0.03555237
case acc: 0.04095713
case acc: 0.030740546
case acc: 0.033299737
top acc: 0.0563 ::: bot acc: 0.0108
top acc: 0.0669 ::: bot acc: 0.0149
top acc: 0.0656 ::: bot acc: 0.0150
top acc: 0.0760 ::: bot acc: 0.0174
top acc: 0.0644 ::: bot acc: 0.0136
top acc: 0.0658 ::: bot acc: 0.0115
current epoch: 36
train loss is 0.001006
average val loss: 0.000993, accuracy: 0.0362
average test loss: 0.001026, accuracy: 0.0357
case acc: 0.03168317
case acc: 0.04045542
case acc: 0.036106553
case acc: 0.041573565
case acc: 0.031032033
case acc: 0.033612695
top acc: 0.0562 ::: bot acc: 0.0108
top acc: 0.0668 ::: bot acc: 0.0147
top acc: 0.0667 ::: bot acc: 0.0146
top acc: 0.0771 ::: bot acc: 0.0173
top acc: 0.0650 ::: bot acc: 0.0131
top acc: 0.0661 ::: bot acc: 0.0114
current epoch: 37
train loss is 0.000989
average val loss: 0.000942, accuracy: 0.0351
average test loss: 0.000980, accuracy: 0.0347
case acc: 0.030344088
case acc: 0.03852375
case acc: 0.03534172
case acc: 0.040737197
case acc: 0.030401384
case acc: 0.032776788
top acc: 0.0543 ::: bot acc: 0.0107
top acc: 0.0645 ::: bot acc: 0.0136
top acc: 0.0653 ::: bot acc: 0.0150
top acc: 0.0760 ::: bot acc: 0.0172
top acc: 0.0637 ::: bot acc: 0.0139
top acc: 0.0649 ::: bot acc: 0.0115
current epoch: 38
train loss is 0.000964
average val loss: 0.000889, accuracy: 0.0338
average test loss: 0.000927, accuracy: 0.0335
case acc: 0.028999925
case acc: 0.036076576
case acc: 0.03428499
case acc: 0.039759964
case acc: 0.030084543
case acc: 0.031790107
top acc: 0.0523 ::: bot acc: 0.0108
top acc: 0.0616 ::: bot acc: 0.0121
top acc: 0.0634 ::: bot acc: 0.0156
top acc: 0.0744 ::: bot acc: 0.0174
top acc: 0.0628 ::: bot acc: 0.0149
top acc: 0.0631 ::: bot acc: 0.0120
current epoch: 39
train loss is 0.000939
average val loss: 0.000839, accuracy: 0.0326
average test loss: 0.000878, accuracy: 0.0324
case acc: 0.027646217
case acc: 0.03369369
case acc: 0.033340737
case acc: 0.038805872
case acc: 0.029795684
case acc: 0.030991783
top acc: 0.0501 ::: bot acc: 0.0110
top acc: 0.0586 ::: bot acc: 0.0109
top acc: 0.0614 ::: bot acc: 0.0167
top acc: 0.0727 ::: bot acc: 0.0179
top acc: 0.0619 ::: bot acc: 0.0159
top acc: 0.0615 ::: bot acc: 0.0128
current epoch: 40
train loss is 0.000910
average val loss: 0.000818, accuracy: 0.0321
average test loss: 0.000858, accuracy: 0.0319
case acc: 0.026933337
case acc: 0.032490175
case acc: 0.033051588
case acc: 0.0384223
case acc: 0.029669942
case acc: 0.030700255
top acc: 0.0490 ::: bot acc: 0.0113
top acc: 0.0571 ::: bot acc: 0.0103
top acc: 0.0605 ::: bot acc: 0.0174
top acc: 0.0721 ::: bot acc: 0.0179
top acc: 0.0618 ::: bot acc: 0.0158
top acc: 0.0610 ::: bot acc: 0.0130
current epoch: 41
train loss is 0.000885
average val loss: 0.000750, accuracy: 0.0304
average test loss: 0.000795, accuracy: 0.0304
case acc: 0.025237264
case acc: 0.029925395
case acc: 0.0318514
case acc: 0.03706564
case acc: 0.029031577
case acc: 0.029540595
top acc: 0.0457 ::: bot acc: 0.0125
top acc: 0.0535 ::: bot acc: 0.0100
top acc: 0.0578 ::: bot acc: 0.0197
top acc: 0.0697 ::: bot acc: 0.0187
top acc: 0.0594 ::: bot acc: 0.0181
top acc: 0.0584 ::: bot acc: 0.0146
current epoch: 42
train loss is 0.000848
average val loss: 0.000678, accuracy: 0.0286
average test loss: 0.000726, accuracy: 0.0289
case acc: 0.023276243
case acc: 0.027311603
case acc: 0.030503742
case acc: 0.03549593
case acc: 0.028393464
case acc: 0.028572598
top acc: 0.0415 ::: bot acc: 0.0150
top acc: 0.0490 ::: bot acc: 0.0111
top acc: 0.0540 ::: bot acc: 0.0233
top acc: 0.0666 ::: bot acc: 0.0201
top acc: 0.0564 ::: bot acc: 0.0215
top acc: 0.0554 ::: bot acc: 0.0173
current epoch: 43
train loss is 0.000818
average val loss: 0.000615, accuracy: 0.0270
average test loss: 0.000668, accuracy: 0.0276
case acc: 0.022011109
case acc: 0.024860704
case acc: 0.029293055
case acc: 0.033898737
case acc: 0.027901575
case acc: 0.027883824
top acc: 0.0373 ::: bot acc: 0.0192
top acc: 0.0444 ::: bot acc: 0.0131
top acc: 0.0498 ::: bot acc: 0.0277
top acc: 0.0633 ::: bot acc: 0.0220
top acc: 0.0530 ::: bot acc: 0.0245
top acc: 0.0523 ::: bot acc: 0.0205
current epoch: 44
train loss is 0.000799
average val loss: 0.000596, accuracy: 0.0265
average test loss: 0.000648, accuracy: 0.0272
case acc: 0.02150836
case acc: 0.023614343
case acc: 0.028743526
case acc: 0.033408105
case acc: 0.02794135
case acc: 0.02772263
top acc: 0.0354 ::: bot acc: 0.0212
top acc: 0.0418 ::: bot acc: 0.0145
top acc: 0.0477 ::: bot acc: 0.0295
top acc: 0.0620 ::: bot acc: 0.0231
top acc: 0.0523 ::: bot acc: 0.0252
top acc: 0.0515 ::: bot acc: 0.0212
current epoch: 45
train loss is 0.000782
average val loss: 0.000570, accuracy: 0.0257
average test loss: 0.000626, accuracy: 0.0267
case acc: 0.021056727
case acc: 0.022311354
case acc: 0.028431818
case acc: 0.03280717
case acc: 0.028003778
case acc: 0.027656747
top acc: 0.0327 ::: bot acc: 0.0240
top acc: 0.0384 ::: bot acc: 0.0172
top acc: 0.0447 ::: bot acc: 0.0327
top acc: 0.0601 ::: bot acc: 0.0249
top acc: 0.0508 ::: bot acc: 0.0269
top acc: 0.0502 ::: bot acc: 0.0226
current epoch: 46
train loss is 0.000767
average val loss: 0.000529, accuracy: 0.0247
average test loss: 0.000593, accuracy: 0.0262
case acc: 0.020916024
case acc: 0.02084829
case acc: 0.02836563
case acc: 0.031605024
case acc: 0.0281342
case acc: 0.027265193
top acc: 0.0271 ::: bot acc: 0.0295
top acc: 0.0322 ::: bot acc: 0.0235
top acc: 0.0387 ::: bot acc: 0.0386
top acc: 0.0556 ::: bot acc: 0.0297
top acc: 0.0464 ::: bot acc: 0.0314
top acc: 0.0464 ::: bot acc: 0.0266
current epoch: 47
train loss is 0.000754
average val loss: 0.000522, accuracy: 0.0245
average test loss: 0.000586, accuracy: 0.0261
case acc: 0.021146242
case acc: 0.020414328
case acc: 0.028547077
case acc: 0.031087779
case acc: 0.028314484
case acc: 0.027154949
top acc: 0.0242 ::: bot acc: 0.0325
top acc: 0.0284 ::: bot acc: 0.0272
top acc: 0.0351 ::: bot acc: 0.0421
top acc: 0.0533 ::: bot acc: 0.0317
top acc: 0.0448 ::: bot acc: 0.0330
top acc: 0.0448 ::: bot acc: 0.0280
current epoch: 48
train loss is 0.000755
average val loss: 0.000523, accuracy: 0.0246
average test loss: 0.000591, accuracy: 0.0264
case acc: 0.021888003
case acc: 0.020771008
case acc: 0.029353175
case acc: 0.030510068
case acc: 0.028804883
case acc: 0.02704415
top acc: 0.0201 ::: bot acc: 0.0367
top acc: 0.0231 ::: bot acc: 0.0326
top acc: 0.0299 ::: bot acc: 0.0473
top acc: 0.0500 ::: bot acc: 0.0348
top acc: 0.0423 ::: bot acc: 0.0353
top acc: 0.0424 ::: bot acc: 0.0302
current epoch: 49
train loss is 0.000760
average val loss: 0.000545, accuracy: 0.0254
average test loss: 0.000620, accuracy: 0.0274
case acc: 0.023756774
case acc: 0.022155028
case acc: 0.0312293
case acc: 0.03045806
case acc: 0.029703166
case acc: 0.027219
top acc: 0.0157 ::: bot acc: 0.0419
top acc: 0.0165 ::: bot acc: 0.0390
top acc: 0.0236 ::: bot acc: 0.0539
top acc: 0.0455 ::: bot acc: 0.0394
top acc: 0.0389 ::: bot acc: 0.0389
top acc: 0.0397 ::: bot acc: 0.0332
current epoch: 50
train loss is 0.000773
average val loss: 0.000614, accuracy: 0.0276
average test loss: 0.000696, accuracy: 0.0297
case acc: 0.027486162
case acc: 0.025619607
case acc: 0.03470409
case acc: 0.031264212
case acc: 0.03141362
case acc: 0.027646003
top acc: 0.0129 ::: bot acc: 0.0489
top acc: 0.0106 ::: bot acc: 0.0474
top acc: 0.0174 ::: bot acc: 0.0622
top acc: 0.0394 ::: bot acc: 0.0454
top acc: 0.0340 ::: bot acc: 0.0440
top acc: 0.0351 ::: bot acc: 0.0377
LME_Co_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6798 6798 6798
1.7082474 -0.6288155 0.21141115 -0.19947179
Validation: 756 756 756
Testing: 750 750 750
pre-processing time: 0.00021719932556152344
the split date is 2011-01-01
train dropout: 0.6 test dropout: 0.05
net initializing with time: 0.002183198928833008
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.013199
average val loss: 0.005912, accuracy: 0.0996
average test loss: 0.004904, accuracy: 0.0901
case acc: 0.13144149
case acc: 0.09339137
case acc: 0.08382895
case acc: 0.07468389
case acc: 0.10686451
case acc: 0.05056025
top acc: 0.0994 ::: bot acc: 0.1658
top acc: 0.1216 ::: bot acc: 0.0612
top acc: 0.0422 ::: bot acc: 0.1276
top acc: 0.0406 ::: bot acc: 0.1080
top acc: 0.0640 ::: bot acc: 0.1481
top acc: 0.0203 ::: bot acc: 0.0853
current epoch: 2
train loss is 0.007939
average val loss: 0.003399, accuracy: 0.0601
average test loss: 0.003578, accuracy: 0.0596
case acc: 0.032034926
case acc: 0.18053736
case acc: 0.037061594
case acc: 0.029802578
case acc: 0.033707805
case acc: 0.04475142
top acc: 0.0086 ::: bot acc: 0.0621
top acc: 0.2087 ::: bot acc: 0.1492
top acc: 0.0655 ::: bot acc: 0.0251
top acc: 0.0555 ::: bot acc: 0.0143
top acc: 0.0262 ::: bot acc: 0.0565
top acc: 0.0797 ::: bot acc: 0.0151
current epoch: 3
train loss is 0.008426
average val loss: 0.007974, accuracy: 0.1025
average test loss: 0.009154, accuracy: 0.1145
case acc: 0.058804125
case acc: 0.25208765
case acc: 0.10659352
case acc: 0.09869009
case acc: 0.060956933
case acc: 0.10961183
top acc: 0.0901 ::: bot acc: 0.0258
top acc: 0.2798 ::: bot acc: 0.2210
top acc: 0.1519 ::: bot acc: 0.0612
top acc: 0.1335 ::: bot acc: 0.0643
top acc: 0.1021 ::: bot acc: 0.0226
top acc: 0.1489 ::: bot acc: 0.0709
current epoch: 4
train loss is 0.010596
average val loss: 0.015838, accuracy: 0.1637
average test loss: 0.017714, accuracy: 0.1756
case acc: 0.12738284
case acc: 0.30672792
case acc: 0.17354728
case acc: 0.15912163
case acc: 0.12375749
case acc: 0.16325803
top acc: 0.1594 ::: bot acc: 0.0935
top acc: 0.3346 ::: bot acc: 0.2757
top acc: 0.2182 ::: bot acc: 0.1283
top acc: 0.1933 ::: bot acc: 0.1249
top acc: 0.1655 ::: bot acc: 0.0834
top acc: 0.2024 ::: bot acc: 0.1251
current epoch: 5
train loss is 0.012857
average val loss: 0.014237, accuracy: 0.1554
average test loss: 0.016049, accuracy: 0.1674
case acc: 0.1246378
case acc: 0.28870746
case acc: 0.16835205
case acc: 0.14978032
case acc: 0.12315516
case acc: 0.14993703
top acc: 0.1564 ::: bot acc: 0.0912
top acc: 0.3168 ::: bot acc: 0.2581
top acc: 0.2132 ::: bot acc: 0.1227
top acc: 0.1845 ::: bot acc: 0.1151
top acc: 0.1650 ::: bot acc: 0.0827
top acc: 0.1892 ::: bot acc: 0.1118
current epoch: 6
train loss is 0.013160
average val loss: 0.003390, accuracy: 0.0606
average test loss: 0.004027, accuracy: 0.0699
case acc: 0.03349459
case acc: 0.17459296
case acc: 0.06835874
case acc: 0.050592866
case acc: 0.040389687
case acc: 0.051823698
top acc: 0.0598 ::: bot acc: 0.0115
top acc: 0.2024 ::: bot acc: 0.1442
top acc: 0.1109 ::: bot acc: 0.0282
top acc: 0.0836 ::: bot acc: 0.0195
top acc: 0.0760 ::: bot acc: 0.0132
top acc: 0.0883 ::: bot acc: 0.0192
current epoch: 7
train loss is 0.007673
average val loss: 0.001577, accuracy: 0.0446
average test loss: 0.001610, accuracy: 0.0426
case acc: 0.028709546
case acc: 0.1083958
case acc: 0.033951655
case acc: 0.025769336
case acc: 0.031003902
case acc: 0.027941333
top acc: 0.0112 ::: bot acc: 0.0555
top acc: 0.1361 ::: bot acc: 0.0778
top acc: 0.0540 ::: bot acc: 0.0368
top acc: 0.0319 ::: bot acc: 0.0378
top acc: 0.0361 ::: bot acc: 0.0472
top acc: 0.0382 ::: bot acc: 0.0389
current epoch: 8
train loss is 0.003519
average val loss: 0.001400, accuracy: 0.0408
average test loss: 0.001529, accuracy: 0.0416
case acc: 0.025099218
case acc: 0.10451453
case acc: 0.034734473
case acc: 0.025606144
case acc: 0.030909846
case acc: 0.028891437
top acc: 0.0189 ::: bot acc: 0.0461
top acc: 0.1321 ::: bot acc: 0.0743
top acc: 0.0573 ::: bot acc: 0.0340
top acc: 0.0397 ::: bot acc: 0.0296
top acc: 0.0513 ::: bot acc: 0.0314
top acc: 0.0478 ::: bot acc: 0.0289
current epoch: 9
train loss is 0.002611
average val loss: 0.001508, accuracy: 0.0403
average test loss: 0.001825, accuracy: 0.0463
case acc: 0.024806712
case acc: 0.111136556
case acc: 0.03879449
case acc: 0.030080913
case acc: 0.037996043
case acc: 0.035051934
top acc: 0.0370 ::: bot acc: 0.0284
top acc: 0.1387 ::: bot acc: 0.0810
top acc: 0.0700 ::: bot acc: 0.0222
top acc: 0.0552 ::: bot acc: 0.0150
top acc: 0.0719 ::: bot acc: 0.0147
top acc: 0.0651 ::: bot acc: 0.0153
current epoch: 10
train loss is 0.002537
average val loss: 0.001445, accuracy: 0.0398
average test loss: 0.001799, accuracy: 0.0468
case acc: 0.025573647
case acc: 0.1059324
case acc: 0.03920446
case acc: 0.03167592
case acc: 0.041480552
case acc: 0.03700236
top acc: 0.0412 ::: bot acc: 0.0244
top acc: 0.1337 ::: bot acc: 0.0757
top acc: 0.0710 ::: bot acc: 0.0213
top acc: 0.0587 ::: bot acc: 0.0137
top acc: 0.0775 ::: bot acc: 0.0131
top acc: 0.0685 ::: bot acc: 0.0141
current epoch: 11
train loss is 0.002338
average val loss: 0.001262, accuracy: 0.0377
average test loss: 0.001580, accuracy: 0.0442
case acc: 0.025487764
case acc: 0.09544669
case acc: 0.03698857
case acc: 0.030697402
case acc: 0.040936712
case acc: 0.0357222
top acc: 0.0399 ::: bot acc: 0.0259
top acc: 0.1229 ::: bot acc: 0.0655
top acc: 0.0660 ::: bot acc: 0.0249
top acc: 0.0562 ::: bot acc: 0.0153
top acc: 0.0767 ::: bot acc: 0.0130
top acc: 0.0665 ::: bot acc: 0.0147
current epoch: 12
train loss is 0.002041
average val loss: 0.001125, accuracy: 0.0359
average test loss: 0.001401, accuracy: 0.0419
case acc: 0.025074162
case acc: 0.08578035
case acc: 0.03558908
case acc: 0.02948963
case acc: 0.040695697
case acc: 0.034652416
top acc: 0.0384 ::: bot acc: 0.0273
top acc: 0.1135 ::: bot acc: 0.0557
top acc: 0.0618 ::: bot acc: 0.0286
top acc: 0.0540 ::: bot acc: 0.0165
top acc: 0.0766 ::: bot acc: 0.0134
top acc: 0.0643 ::: bot acc: 0.0153
current epoch: 13
train loss is 0.001774
average val loss: 0.001085, accuracy: 0.0354
average test loss: 0.001377, accuracy: 0.0420
case acc: 0.02575516
case acc: 0.08117743
case acc: 0.035910476
case acc: 0.03064611
case acc: 0.042495094
case acc: 0.035909887
top acc: 0.0418 ::: bot acc: 0.0239
top acc: 0.1090 ::: bot acc: 0.0511
top acc: 0.0625 ::: bot acc: 0.0282
top acc: 0.0562 ::: bot acc: 0.0153
top acc: 0.0792 ::: bot acc: 0.0129
top acc: 0.0665 ::: bot acc: 0.0151
current epoch: 14
train loss is 0.001644
average val loss: 0.001021, accuracy: 0.0346
average test loss: 0.001314, accuracy: 0.0413
case acc: 0.026078613
case acc: 0.075666286
case acc: 0.035887603
case acc: 0.030930078
case acc: 0.04297043
case acc: 0.036067877
top acc: 0.0432 ::: bot acc: 0.0223
top acc: 0.1032 ::: bot acc: 0.0458
top acc: 0.0622 ::: bot acc: 0.0285
top acc: 0.0571 ::: bot acc: 0.0146
top acc: 0.0798 ::: bot acc: 0.0130
top acc: 0.0667 ::: bot acc: 0.0147
current epoch: 15
train loss is 0.001497
average val loss: 0.000950, accuracy: 0.0334
average test loss: 0.001219, accuracy: 0.0399
case acc: 0.026094796
case acc: 0.06937776
case acc: 0.035334006
case acc: 0.03048312
case acc: 0.0424498
case acc: 0.035469558
top acc: 0.0434 ::: bot acc: 0.0220
top acc: 0.0969 ::: bot acc: 0.0396
top acc: 0.0605 ::: bot acc: 0.0300
top acc: 0.0561 ::: bot acc: 0.0150
top acc: 0.0790 ::: bot acc: 0.0130
top acc: 0.0658 ::: bot acc: 0.0148
current epoch: 16
train loss is 0.001427
average val loss: 0.000949, accuracy: 0.0335
average test loss: 0.001247, accuracy: 0.0406
case acc: 0.027416036
case acc: 0.0672252
case acc: 0.0360581
case acc: 0.03221772
case acc: 0.043970384
case acc: 0.036973253
top acc: 0.0470 ::: bot acc: 0.0188
top acc: 0.0947 ::: bot acc: 0.0376
top acc: 0.0630 ::: bot acc: 0.0274
top acc: 0.0591 ::: bot acc: 0.0142
top acc: 0.0812 ::: bot acc: 0.0132
top acc: 0.0681 ::: bot acc: 0.0147
current epoch: 17
train loss is 0.001336
average val loss: 0.000947, accuracy: 0.0335
average test loss: 0.001262, accuracy: 0.0411
case acc: 0.02845109
case acc: 0.06496765
case acc: 0.036843367
case acc: 0.03350375
case acc: 0.044978265
case acc: 0.037826013
top acc: 0.0494 ::: bot acc: 0.0165
top acc: 0.0925 ::: bot acc: 0.0353
top acc: 0.0651 ::: bot acc: 0.0257
top acc: 0.0612 ::: bot acc: 0.0137
top acc: 0.0826 ::: bot acc: 0.0133
top acc: 0.0697 ::: bot acc: 0.0143
current epoch: 18
train loss is 0.001303
average val loss: 0.000914, accuracy: 0.0330
average test loss: 0.001228, accuracy: 0.0406
case acc: 0.029040908
case acc: 0.061502114
case acc: 0.037111677
case acc: 0.033876423
case acc: 0.04431791
case acc: 0.037669867
top acc: 0.0512 ::: bot acc: 0.0155
top acc: 0.0889 ::: bot acc: 0.0322
top acc: 0.0659 ::: bot acc: 0.0250
top acc: 0.0618 ::: bot acc: 0.0136
top acc: 0.0819 ::: bot acc: 0.0129
top acc: 0.0699 ::: bot acc: 0.0142
current epoch: 19
train loss is 0.001223
average val loss: 0.000894, accuracy: 0.0327
average test loss: 0.001205, accuracy: 0.0403
case acc: 0.029741002
case acc: 0.058794618
case acc: 0.03739532
case acc: 0.03443342
case acc: 0.043876115
case acc: 0.03745847
top acc: 0.0524 ::: bot acc: 0.0148
top acc: 0.0860 ::: bot acc: 0.0295
top acc: 0.0668 ::: bot acc: 0.0238
top acc: 0.0629 ::: bot acc: 0.0134
top acc: 0.0813 ::: bot acc: 0.0127
top acc: 0.0691 ::: bot acc: 0.0144
current epoch: 20
train loss is 0.001151
average val loss: 0.000875, accuracy: 0.0323
average test loss: 0.001189, accuracy: 0.0400
case acc: 0.030261772
case acc: 0.056536708
case acc: 0.03791872
case acc: 0.03477094
case acc: 0.04346163
case acc: 0.037199512
top acc: 0.0536 ::: bot acc: 0.0139
top acc: 0.0837 ::: bot acc: 0.0276
top acc: 0.0682 ::: bot acc: 0.0228
top acc: 0.0635 ::: bot acc: 0.0134
top acc: 0.0807 ::: bot acc: 0.0129
top acc: 0.0688 ::: bot acc: 0.0142
current epoch: 21
train loss is 0.001112
average val loss: 0.000881, accuracy: 0.0324
average test loss: 0.001204, accuracy: 0.0404
case acc: 0.031455636
case acc: 0.055182315
case acc: 0.03881149
case acc: 0.035886407
case acc: 0.043560807
case acc: 0.03743662
top acc: 0.0560 ::: bot acc: 0.0127
top acc: 0.0824 ::: bot acc: 0.0262
top acc: 0.0703 ::: bot acc: 0.0212
top acc: 0.0650 ::: bot acc: 0.0133
top acc: 0.0807 ::: bot acc: 0.0130
top acc: 0.0691 ::: bot acc: 0.0144
current epoch: 22
train loss is 0.001106
average val loss: 0.000898, accuracy: 0.0329
average test loss: 0.001241, accuracy: 0.0412
case acc: 0.032927684
case acc: 0.05485307
case acc: 0.040082153
case acc: 0.037253007
case acc: 0.04401117
case acc: 0.038053706
top acc: 0.0585 ::: bot acc: 0.0121
top acc: 0.0819 ::: bot acc: 0.0262
top acc: 0.0728 ::: bot acc: 0.0196
top acc: 0.0670 ::: bot acc: 0.0135
top acc: 0.0813 ::: bot acc: 0.0131
top acc: 0.0701 ::: bot acc: 0.0143
current epoch: 23
train loss is 0.001086
average val loss: 0.000877, accuracy: 0.0325
average test loss: 0.001214, accuracy: 0.0407
case acc: 0.033157166
case acc: 0.052732628
case acc: 0.04042407
case acc: 0.03735966
case acc: 0.04314203
case acc: 0.0373847
top acc: 0.0588 ::: bot acc: 0.0120
top acc: 0.0796 ::: bot acc: 0.0244
top acc: 0.0737 ::: bot acc: 0.0190
top acc: 0.0671 ::: bot acc: 0.0135
top acc: 0.0800 ::: bot acc: 0.0132
top acc: 0.0691 ::: bot acc: 0.0141
current epoch: 24
train loss is 0.001037
average val loss: 0.000871, accuracy: 0.0324
average test loss: 0.001214, accuracy: 0.0407
case acc: 0.033727024
case acc: 0.05156553
case acc: 0.041262127
case acc: 0.03768483
case acc: 0.042828333
case acc: 0.03713215
top acc: 0.0597 ::: bot acc: 0.0118
top acc: 0.0784 ::: bot acc: 0.0232
top acc: 0.0752 ::: bot acc: 0.0183
top acc: 0.0679 ::: bot acc: 0.0133
top acc: 0.0796 ::: bot acc: 0.0130
top acc: 0.0687 ::: bot acc: 0.0143
current epoch: 25
train loss is 0.001030
average val loss: 0.000947, accuracy: 0.0341
average test loss: 0.001328, accuracy: 0.0429
case acc: 0.036375597
case acc: 0.05328231
case acc: 0.04336608
case acc: 0.04056748
case acc: 0.044534158
case acc: 0.039026253
top acc: 0.0639 ::: bot acc: 0.0113
top acc: 0.0802 ::: bot acc: 0.0247
top acc: 0.0795 ::: bot acc: 0.0161
top acc: 0.0716 ::: bot acc: 0.0143
top acc: 0.0822 ::: bot acc: 0.0129
top acc: 0.0717 ::: bot acc: 0.0143
current epoch: 26
train loss is 0.001049
average val loss: 0.000998, accuracy: 0.0352
average test loss: 0.001406, accuracy: 0.0444
case acc: 0.03825804
case acc: 0.05407536
case acc: 0.04533527
case acc: 0.042481188
case acc: 0.04583152
case acc: 0.040203255
top acc: 0.0665 ::: bot acc: 0.0115
top acc: 0.0812 ::: bot acc: 0.0252
top acc: 0.0827 ::: bot acc: 0.0154
top acc: 0.0741 ::: bot acc: 0.0151
top acc: 0.0840 ::: bot acc: 0.0132
top acc: 0.0731 ::: bot acc: 0.0146
current epoch: 27
train loss is 0.001035
average val loss: 0.000948, accuracy: 0.0341
average test loss: 0.001337, accuracy: 0.0431
case acc: 0.037414037
case acc: 0.051473685
case acc: 0.04505092
case acc: 0.041368045
case acc: 0.044677645
case acc: 0.038459316
top acc: 0.0653 ::: bot acc: 0.0114
top acc: 0.0783 ::: bot acc: 0.0231
top acc: 0.0819 ::: bot acc: 0.0159
top acc: 0.0725 ::: bot acc: 0.0146
top acc: 0.0824 ::: bot acc: 0.0130
top acc: 0.0708 ::: bot acc: 0.0143
current epoch: 28
train loss is 0.001001
average val loss: 0.000940, accuracy: 0.0340
average test loss: 0.001333, accuracy: 0.0430
case acc: 0.037573237
case acc: 0.0504904
case acc: 0.045673277
case acc: 0.04144775
case acc: 0.044779275
case acc: 0.037983857
top acc: 0.0656 ::: bot acc: 0.0114
top acc: 0.0772 ::: bot acc: 0.0224
top acc: 0.0830 ::: bot acc: 0.0156
top acc: 0.0728 ::: bot acc: 0.0146
top acc: 0.0823 ::: bot acc: 0.0133
top acc: 0.0701 ::: bot acc: 0.0144
current epoch: 29
train loss is 0.001001
average val loss: 0.000997, accuracy: 0.0352
average test loss: 0.001416, accuracy: 0.0446
case acc: 0.039612398
case acc: 0.051612537
case acc: 0.04759334
case acc: 0.043271564
case acc: 0.04638112
case acc: 0.038860194
top acc: 0.0684 ::: bot acc: 0.0119
top acc: 0.0782 ::: bot acc: 0.0234
top acc: 0.0861 ::: bot acc: 0.0156
top acc: 0.0749 ::: bot acc: 0.0155
top acc: 0.0847 ::: bot acc: 0.0131
top acc: 0.0715 ::: bot acc: 0.0143
current epoch: 30
train loss is 0.000990
average val loss: 0.001035, accuracy: 0.0360
average test loss: 0.001478, accuracy: 0.0457
case acc: 0.040955
case acc: 0.05209435
case acc: 0.049286574
case acc: 0.044634435
case acc: 0.047554485
case acc: 0.039498735
top acc: 0.0702 ::: bot acc: 0.0124
top acc: 0.0792 ::: bot acc: 0.0235
top acc: 0.0885 ::: bot acc: 0.0159
top acc: 0.0767 ::: bot acc: 0.0163
top acc: 0.0865 ::: bot acc: 0.0135
top acc: 0.0724 ::: bot acc: 0.0143
current epoch: 31
train loss is 0.000976
average val loss: 0.000997, accuracy: 0.0352
average test loss: 0.001427, accuracy: 0.0447
case acc: 0.04014005
case acc: 0.05020149
case acc: 0.049096208
case acc: 0.043759514
case acc: 0.04676491
case acc: 0.038412612
top acc: 0.0691 ::: bot acc: 0.0122
top acc: 0.0769 ::: bot acc: 0.0223
top acc: 0.0880 ::: bot acc: 0.0160
top acc: 0.0758 ::: bot acc: 0.0157
top acc: 0.0853 ::: bot acc: 0.0133
top acc: 0.0707 ::: bot acc: 0.0143
current epoch: 32
train loss is 0.000966
average val loss: 0.001024, accuracy: 0.0357
average test loss: 0.001469, accuracy: 0.0455
case acc: 0.041020695
case acc: 0.050254777
case acc: 0.05035337
case acc: 0.044926308
case acc: 0.047723435
case acc: 0.038724072
top acc: 0.0703 ::: bot acc: 0.0124
top acc: 0.0770 ::: bot acc: 0.0222
top acc: 0.0899 ::: bot acc: 0.0165
top acc: 0.0772 ::: bot acc: 0.0164
top acc: 0.0867 ::: bot acc: 0.0136
top acc: 0.0710 ::: bot acc: 0.0144
current epoch: 33
train loss is 0.000975
average val loss: 0.001071, accuracy: 0.0367
average test loss: 0.001533, accuracy: 0.0467
case acc: 0.04237148
case acc: 0.051006064
case acc: 0.051933616
case acc: 0.046430353
case acc: 0.048806757
case acc: 0.039472137
top acc: 0.0722 ::: bot acc: 0.0129
top acc: 0.0778 ::: bot acc: 0.0228
top acc: 0.0919 ::: bot acc: 0.0170
top acc: 0.0790 ::: bot acc: 0.0172
top acc: 0.0881 ::: bot acc: 0.0141
top acc: 0.0724 ::: bot acc: 0.0142
current epoch: 34
train loss is 0.000969
average val loss: 0.001073, accuracy: 0.0367
average test loss: 0.001536, accuracy: 0.0467
case acc: 0.042132743
case acc: 0.050451152
case acc: 0.052763708
case acc: 0.046815906
case acc: 0.048746742
case acc: 0.039438818
top acc: 0.0718 ::: bot acc: 0.0128
top acc: 0.0771 ::: bot acc: 0.0224
top acc: 0.0927 ::: bot acc: 0.0174
top acc: 0.0793 ::: bot acc: 0.0174
top acc: 0.0879 ::: bot acc: 0.0141
top acc: 0.0723 ::: bot acc: 0.0142
current epoch: 35
train loss is 0.000978
average val loss: 0.001080, accuracy: 0.0369
average test loss: 0.001550, accuracy: 0.0470
case acc: 0.04240811
case acc: 0.050120186
case acc: 0.053338394
case acc: 0.047356024
case acc: 0.048904978
case acc: 0.03970715
top acc: 0.0721 ::: bot acc: 0.0130
top acc: 0.0767 ::: bot acc: 0.0223
top acc: 0.0937 ::: bot acc: 0.0174
top acc: 0.0800 ::: bot acc: 0.0179
top acc: 0.0881 ::: bot acc: 0.0142
top acc: 0.0726 ::: bot acc: 0.0145
current epoch: 36
train loss is 0.000962
average val loss: 0.001109, accuracy: 0.0376
average test loss: 0.001594, accuracy: 0.0478
case acc: 0.043024275
case acc: 0.05037707
case acc: 0.054450393
case acc: 0.04850088
case acc: 0.049998775
case acc: 0.040157657
top acc: 0.0729 ::: bot acc: 0.0133
top acc: 0.0773 ::: bot acc: 0.0224
top acc: 0.0951 ::: bot acc: 0.0180
top acc: 0.0814 ::: bot acc: 0.0185
top acc: 0.0894 ::: bot acc: 0.0147
top acc: 0.0733 ::: bot acc: 0.0143
current epoch: 37
train loss is 0.000952
average val loss: 0.001090, accuracy: 0.0371
average test loss: 0.001568, accuracy: 0.0473
case acc: 0.04241895
case acc: 0.048915457
case acc: 0.054109164
case acc: 0.048245892
case acc: 0.050079454
case acc: 0.03980613
top acc: 0.0723 ::: bot acc: 0.0127
top acc: 0.0753 ::: bot acc: 0.0213
top acc: 0.0946 ::: bot acc: 0.0178
top acc: 0.0811 ::: bot acc: 0.0182
top acc: 0.0896 ::: bot acc: 0.0147
top acc: 0.0728 ::: bot acc: 0.0145
current epoch: 38
train loss is 0.000941
average val loss: 0.001078, accuracy: 0.0368
average test loss: 0.001551, accuracy: 0.0470
case acc: 0.04202138
case acc: 0.047634453
case acc: 0.05393296
case acc: 0.048066072
case acc: 0.050538845
case acc: 0.039651547
top acc: 0.0716 ::: bot acc: 0.0128
top acc: 0.0739 ::: bot acc: 0.0205
top acc: 0.0944 ::: bot acc: 0.0177
top acc: 0.0809 ::: bot acc: 0.0181
top acc: 0.0900 ::: bot acc: 0.0149
top acc: 0.0724 ::: bot acc: 0.0145
current epoch: 39
train loss is 0.000933
average val loss: 0.001022, accuracy: 0.0356
average test loss: 0.001479, accuracy: 0.0456
case acc: 0.04033009
case acc: 0.045342214
case acc: 0.052626472
case acc: 0.04702173
case acc: 0.049693175
case acc: 0.03860579
top acc: 0.0694 ::: bot acc: 0.0121
top acc: 0.0712 ::: bot acc: 0.0189
top acc: 0.0927 ::: bot acc: 0.0172
top acc: 0.0796 ::: bot acc: 0.0176
top acc: 0.0892 ::: bot acc: 0.0145
top acc: 0.0711 ::: bot acc: 0.0141
current epoch: 40
train loss is 0.000885
average val loss: 0.000916, accuracy: 0.0332
average test loss: 0.001329, accuracy: 0.0428
case acc: 0.037345473
case acc: 0.041474503
case acc: 0.04975601
case acc: 0.044230856
case acc: 0.047208894
case acc: 0.036533933
top acc: 0.0654 ::: bot acc: 0.0115
top acc: 0.0667 ::: bot acc: 0.0164
top acc: 0.0890 ::: bot acc: 0.0160
top acc: 0.0762 ::: bot acc: 0.0160
top acc: 0.0857 ::: bot acc: 0.0138
top acc: 0.0678 ::: bot acc: 0.0145
current epoch: 41
train loss is 0.000838
average val loss: 0.000867, accuracy: 0.0320
average test loss: 0.001255, accuracy: 0.0413
case acc: 0.03560115
case acc: 0.03908648
case acc: 0.04812022
case acc: 0.04301375
case acc: 0.04615076
case acc: 0.035590153
top acc: 0.0629 ::: bot acc: 0.0112
top acc: 0.0638 ::: bot acc: 0.0151
top acc: 0.0867 ::: bot acc: 0.0156
top acc: 0.0748 ::: bot acc: 0.0151
top acc: 0.0843 ::: bot acc: 0.0134
top acc: 0.0661 ::: bot acc: 0.0149
current epoch: 42
train loss is 0.000819
average val loss: 0.000764, accuracy: 0.0298
average test loss: 0.001095, accuracy: 0.0381
case acc: 0.032483272
case acc: 0.0347471
case acc: 0.044788443
case acc: 0.03970241
case acc: 0.043428663
case acc: 0.033463117
top acc: 0.0577 ::: bot acc: 0.0121
top acc: 0.0579 ::: bot acc: 0.0138
top acc: 0.0817 ::: bot acc: 0.0157
top acc: 0.0706 ::: bot acc: 0.0139
top acc: 0.0803 ::: bot acc: 0.0132
top acc: 0.0622 ::: bot acc: 0.0166
current epoch: 43
train loss is 0.000779
average val loss: 0.000728, accuracy: 0.0289
average test loss: 0.001036, accuracy: 0.0369
case acc: 0.031081676
case acc: 0.03256536
case acc: 0.043365378
case acc: 0.03862343
case acc: 0.04260449
case acc: 0.03294205
top acc: 0.0554 ::: bot acc: 0.0132
top acc: 0.0549 ::: bot acc: 0.0133
top acc: 0.0793 ::: bot acc: 0.0163
top acc: 0.0691 ::: bot acc: 0.0137
top acc: 0.0791 ::: bot acc: 0.0132
top acc: 0.0610 ::: bot acc: 0.0172
current epoch: 44
train loss is 0.000757
average val loss: 0.000668, accuracy: 0.0275
average test loss: 0.000929, accuracy: 0.0346
case acc: 0.029017735
case acc: 0.029180923
case acc: 0.040978942
case acc: 0.0362195
case acc: 0.040662162
case acc: 0.03165431
top acc: 0.0509 ::: bot acc: 0.0157
top acc: 0.0500 ::: bot acc: 0.0133
top acc: 0.0747 ::: bot acc: 0.0186
top acc: 0.0658 ::: bot acc: 0.0131
top acc: 0.0760 ::: bot acc: 0.0137
top acc: 0.0583 ::: bot acc: 0.0191
current epoch: 45
train loss is 0.000733
average val loss: 0.000632, accuracy: 0.0267
average test loss: 0.000856, accuracy: 0.0330
case acc: 0.027587838
case acc: 0.0266571
case acc: 0.039198272
case acc: 0.034473356
case acc: 0.03916399
case acc: 0.030982258
top acc: 0.0474 ::: bot acc: 0.0185
top acc: 0.0456 ::: bot acc: 0.0146
top acc: 0.0709 ::: bot acc: 0.0210
top acc: 0.0631 ::: bot acc: 0.0132
top acc: 0.0737 ::: bot acc: 0.0140
top acc: 0.0565 ::: bot acc: 0.0207
current epoch: 46
train loss is 0.000721
average val loss: 0.000591, accuracy: 0.0261
average test loss: 0.000745, accuracy: 0.0305
case acc: 0.025591271
case acc: 0.02351403
case acc: 0.036513235
case acc: 0.031301178
case acc: 0.036566466
case acc: 0.029568857
top acc: 0.0412 ::: bot acc: 0.0244
top acc: 0.0387 ::: bot acc: 0.0190
top acc: 0.0644 ::: bot acc: 0.0262
top acc: 0.0580 ::: bot acc: 0.0141
top acc: 0.0687 ::: bot acc: 0.0161
top acc: 0.0519 ::: bot acc: 0.0252
current epoch: 47
train loss is 0.000710
average val loss: 0.000584, accuracy: 0.0260
average test loss: 0.000694, accuracy: 0.0294
case acc: 0.024825789
case acc: 0.021929933
case acc: 0.0351131
case acc: 0.029686432
case acc: 0.03544773
case acc: 0.02912833
top acc: 0.0373 ::: bot acc: 0.0285
top acc: 0.0339 ::: bot acc: 0.0238
top acc: 0.0596 ::: bot acc: 0.0310
top acc: 0.0548 ::: bot acc: 0.0157
top acc: 0.0663 ::: bot acc: 0.0176
top acc: 0.0496 ::: bot acc: 0.0273
current epoch: 48
train loss is 0.000718
average val loss: 0.000593, accuracy: 0.0265
average test loss: 0.000648, accuracy: 0.0283
case acc: 0.024102356
case acc: 0.020884112
case acc: 0.033970375
case acc: 0.0281033
case acc: 0.034222767
case acc: 0.028585393
top acc: 0.0324 ::: bot acc: 0.0333
top acc: 0.0278 ::: bot acc: 0.0297
top acc: 0.0536 ::: bot acc: 0.0370
top acc: 0.0507 ::: bot acc: 0.0194
top acc: 0.0632 ::: bot acc: 0.0202
top acc: 0.0467 ::: bot acc: 0.0304
current epoch: 49
train loss is 0.000736
average val loss: 0.000633, accuracy: 0.0278
average test loss: 0.000620, accuracy: 0.0277
case acc: 0.024263209
case acc: 0.021115897
case acc: 0.033149906
case acc: 0.026771119
case acc: 0.032635115
case acc: 0.02807948
top acc: 0.0266 ::: bot acc: 0.0392
top acc: 0.0208 ::: bot acc: 0.0367
top acc: 0.0465 ::: bot acc: 0.0443
top acc: 0.0454 ::: bot acc: 0.0246
top acc: 0.0590 ::: bot acc: 0.0238
top acc: 0.0429 ::: bot acc: 0.0341
current epoch: 50
train loss is 0.000771
average val loss: 0.000715, accuracy: 0.0302
average test loss: 0.000630, accuracy: 0.0278
case acc: 0.025232214
case acc: 0.02328207
case acc: 0.032990362
case acc: 0.02594387
case acc: 0.03167739
case acc: 0.02794243
top acc: 0.0202 ::: bot acc: 0.0456
top acc: 0.0132 ::: bot acc: 0.0446
top acc: 0.0383 ::: bot acc: 0.0525
top acc: 0.0398 ::: bot acc: 0.0302
top acc: 0.0550 ::: bot acc: 0.0279
top acc: 0.0392 ::: bot acc: 0.0378
LME_Co_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6774 6774 6774
1.7082474 -0.6288155 0.21141115 -0.19947179
Validation: 756 756 756
Testing: 768 768 768
pre-processing time: 0.000209808349609375
the split date is 2011-07-01
train dropout: 0.6 test dropout: 0.05
net initializing with time: 0.0021944046020507812
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.013074
average val loss: 0.005232, accuracy: 0.0937
average test loss: 0.005161, accuracy: 0.0935
case acc: 0.13172266
case acc: 0.087855205
case acc: 0.09203361
case acc: 0.08050734
case acc: 0.11543085
case acc: 0.053602256
top acc: 0.0976 ::: bot acc: 0.1616
top acc: 0.1154 ::: bot acc: 0.0606
top acc: 0.0502 ::: bot acc: 0.1334
top acc: 0.0561 ::: bot acc: 0.1062
top acc: 0.0822 ::: bot acc: 0.1516
top acc: 0.0226 ::: bot acc: 0.0894
current epoch: 2
train loss is 0.007998
average val loss: 0.003346, accuracy: 0.0578
average test loss: 0.003276, accuracy: 0.0571
case acc: 0.03832574
case acc: 0.17271917
case acc: 0.032288108
case acc: 0.021877382
case acc: 0.033510674
case acc: 0.04387134
top acc: 0.0212 ::: bot acc: 0.0600
top acc: 0.2000 ::: bot acc: 0.1456
top acc: 0.0538 ::: bot acc: 0.0324
top acc: 0.0373 ::: bot acc: 0.0142
top acc: 0.0133 ::: bot acc: 0.0632
top acc: 0.0768 ::: bot acc: 0.0199
current epoch: 3
train loss is 0.008218
average val loss: 0.008343, accuracy: 0.1074
average test loss: 0.008313, accuracy: 0.1076
case acc: 0.05698631
case acc: 0.24427944
case acc: 0.095955394
case acc: 0.09096238
case acc: 0.052114498
case acc: 0.1053721
top acc: 0.0915 ::: bot acc: 0.0269
top acc: 0.2712 ::: bot acc: 0.2171
top acc: 0.1397 ::: bot acc: 0.0530
top acc: 0.1150 ::: bot acc: 0.0657
top acc: 0.0837 ::: bot acc: 0.0196
top acc: 0.1461 ::: bot acc: 0.0650
current epoch: 4
train loss is 0.010033
average val loss: 0.016885, accuracy: 0.1707
average test loss: 0.016846, accuracy: 0.1709
case acc: 0.12821478
case acc: 0.3014783
case acc: 0.1654756
case acc: 0.15356454
case acc: 0.11642978
case acc: 0.16047621
top acc: 0.1627 ::: bot acc: 0.0986
top acc: 0.3281 ::: bot acc: 0.2748
top acc: 0.2095 ::: bot acc: 0.1223
top acc: 0.1778 ::: bot acc: 0.1281
top acc: 0.1505 ::: bot acc: 0.0798
top acc: 0.2017 ::: bot acc: 0.1196
current epoch: 5
train loss is 0.012621
average val loss: 0.016171, accuracy: 0.1680
average test loss: 0.016127, accuracy: 0.1683
case acc: 0.13113363
case acc: 0.28931737
case acc: 0.16604075
case acc: 0.14948526
case acc: 0.121245846
case acc: 0.15249316
top acc: 0.1660 ::: bot acc: 0.1011
top acc: 0.3160 ::: bot acc: 0.2624
top acc: 0.2105 ::: bot acc: 0.1228
top acc: 0.1736 ::: bot acc: 0.1246
top acc: 0.1549 ::: bot acc: 0.0850
top acc: 0.1939 ::: bot acc: 0.1116
current epoch: 6
train loss is 0.013133
average val loss: 0.005700, accuracy: 0.0887
average test loss: 0.005644, accuracy: 0.0890
case acc: 0.05539681
case acc: 0.1963247
case acc: 0.08515186
case acc: 0.06896015
case acc: 0.054104745
case acc: 0.07381305
top acc: 0.0902 ::: bot acc: 0.0254
top acc: 0.2236 ::: bot acc: 0.1690
top acc: 0.1297 ::: bot acc: 0.0414
top acc: 0.0930 ::: bot acc: 0.0435
top acc: 0.0862 ::: bot acc: 0.0213
top acc: 0.1132 ::: bot acc: 0.0370
current epoch: 7
train loss is 0.008337
average val loss: 0.001724, accuracy: 0.0434
average test loss: 0.001636, accuracy: 0.0412
case acc: 0.027190905
case acc: 0.11376182
case acc: 0.033137504
case acc: 0.018104129
case acc: 0.024979366
case acc: 0.030139413
top acc: 0.0264 ::: bot acc: 0.0405
top acc: 0.1408 ::: bot acc: 0.0869
top acc: 0.0560 ::: bot acc: 0.0316
top acc: 0.0246 ::: bot acc: 0.0251
top acc: 0.0292 ::: bot acc: 0.0408
top acc: 0.0464 ::: bot acc: 0.0349
current epoch: 8
train loss is 0.003913
average val loss: 0.001412, accuracy: 0.0406
average test loss: 0.001325, accuracy: 0.0385
case acc: 0.02907832
case acc: 0.096157864
case acc: 0.032210097
case acc: 0.01827373
case acc: 0.024929777
case acc: 0.030077431
top acc: 0.0243 ::: bot acc: 0.0441
top acc: 0.1227 ::: bot acc: 0.0693
top acc: 0.0463 ::: bot acc: 0.0418
top acc: 0.0190 ::: bot acc: 0.0298
top acc: 0.0324 ::: bot acc: 0.0379
top acc: 0.0440 ::: bot acc: 0.0378
current epoch: 9
train loss is 0.002625
average val loss: 0.001668, accuracy: 0.0440
average test loss: 0.001587, accuracy: 0.0422
case acc: 0.02296795
case acc: 0.1054365
case acc: 0.034477293
case acc: 0.021987004
case acc: 0.03271871
case acc: 0.035410374
top acc: 0.0416 ::: bot acc: 0.0231
top acc: 0.1321 ::: bot acc: 0.0787
top acc: 0.0615 ::: bot acc: 0.0262
top acc: 0.0380 ::: bot acc: 0.0128
top acc: 0.0561 ::: bot acc: 0.0176
top acc: 0.0640 ::: bot acc: 0.0206
current epoch: 10
train loss is 0.002470
average val loss: 0.001652, accuracy: 0.0445
average test loss: 0.001587, accuracy: 0.0432
case acc: 0.023763387
case acc: 0.10119749
case acc: 0.034971826
case acc: 0.024191834
case acc: 0.036617063
case acc: 0.03819447
top acc: 0.0462 ::: bot acc: 0.0184
top acc: 0.1277 ::: bot acc: 0.0745
top acc: 0.0632 ::: bot acc: 0.0245
top acc: 0.0418 ::: bot acc: 0.0118
top acc: 0.0629 ::: bot acc: 0.0160
top acc: 0.0685 ::: bot acc: 0.0199
current epoch: 11
train loss is 0.002271
average val loss: 0.001485, accuracy: 0.0426
average test loss: 0.001417, accuracy: 0.0412
case acc: 0.023807354
case acc: 0.091858655
case acc: 0.03400541
case acc: 0.023166604
case acc: 0.03686446
case acc: 0.037287727
top acc: 0.0462 ::: bot acc: 0.0187
top acc: 0.1188 ::: bot acc: 0.0649
top acc: 0.0595 ::: bot acc: 0.0279
top acc: 0.0404 ::: bot acc: 0.0119
top acc: 0.0631 ::: bot acc: 0.0160
top acc: 0.0669 ::: bot acc: 0.0202
current epoch: 12
train loss is 0.002021
average val loss: 0.001407, accuracy: 0.0419
average test loss: 0.001337, accuracy: 0.0405
case acc: 0.024166245
case acc: 0.08524518
case acc: 0.033807363
case acc: 0.023645641
case acc: 0.038231637
case acc: 0.03802199
top acc: 0.0476 ::: bot acc: 0.0172
top acc: 0.1120 ::: bot acc: 0.0585
top acc: 0.0585 ::: bot acc: 0.0290
top acc: 0.0409 ::: bot acc: 0.0119
top acc: 0.0653 ::: bot acc: 0.0154
top acc: 0.0677 ::: bot acc: 0.0202
current epoch: 13
train loss is 0.001881
average val loss: 0.001317, accuracy: 0.0409
average test loss: 0.001248, accuracy: 0.0395
case acc: 0.024375357
case acc: 0.078245714
case acc: 0.03333105
case acc: 0.023920711
case acc: 0.038940214
case acc: 0.037936606
top acc: 0.0487 ::: bot acc: 0.0160
top acc: 0.1052 ::: bot acc: 0.0513
top acc: 0.0574 ::: bot acc: 0.0302
top acc: 0.0414 ::: bot acc: 0.0120
top acc: 0.0663 ::: bot acc: 0.0155
top acc: 0.0677 ::: bot acc: 0.0200
current epoch: 14
train loss is 0.001605
average val loss: 0.001296, accuracy: 0.0409
average test loss: 0.001231, accuracy: 0.0397
case acc: 0.025543857
case acc: 0.07394981
case acc: 0.033889968
case acc: 0.025145993
case acc: 0.040401287
case acc: 0.039240353
top acc: 0.0518 ::: bot acc: 0.0136
top acc: 0.1007 ::: bot acc: 0.0469
top acc: 0.0584 ::: bot acc: 0.0296
top acc: 0.0429 ::: bot acc: 0.0119
top acc: 0.0685 ::: bot acc: 0.0154
top acc: 0.0699 ::: bot acc: 0.0202
current epoch: 15
train loss is 0.001506
average val loss: 0.001275, accuracy: 0.0407
average test loss: 0.001216, accuracy: 0.0397
case acc: 0.02663888
case acc: 0.07026521
case acc: 0.033974163
case acc: 0.026088424
case acc: 0.04156695
case acc: 0.039845027
top acc: 0.0541 ::: bot acc: 0.0116
top acc: 0.0970 ::: bot acc: 0.0433
top acc: 0.0594 ::: bot acc: 0.0284
top acc: 0.0450 ::: bot acc: 0.0115
top acc: 0.0704 ::: bot acc: 0.0158
top acc: 0.0705 ::: bot acc: 0.0202
current epoch: 16
train loss is 0.001402
average val loss: 0.001215, accuracy: 0.0399
average test loss: 0.001158, accuracy: 0.0389
case acc: 0.026956782
case acc: 0.06538884
case acc: 0.033943523
case acc: 0.026372759
case acc: 0.0409539
case acc: 0.0395701
top acc: 0.0549 ::: bot acc: 0.0113
top acc: 0.0919 ::: bot acc: 0.0388
top acc: 0.0593 ::: bot acc: 0.0285
top acc: 0.0453 ::: bot acc: 0.0118
top acc: 0.0697 ::: bot acc: 0.0157
top acc: 0.0704 ::: bot acc: 0.0199
current epoch: 17
train loss is 0.001332
average val loss: 0.001175, accuracy: 0.0393
average test loss: 0.001118, accuracy: 0.0383
case acc: 0.027522165
case acc: 0.061364606
case acc: 0.033930514
case acc: 0.026709199
case acc: 0.04069797
case acc: 0.039496697
top acc: 0.0559 ::: bot acc: 0.0111
top acc: 0.0884 ::: bot acc: 0.0343
top acc: 0.0594 ::: bot acc: 0.0284
top acc: 0.0455 ::: bot acc: 0.0118
top acc: 0.0689 ::: bot acc: 0.0158
top acc: 0.0705 ::: bot acc: 0.0198
current epoch: 18
train loss is 0.001276
average val loss: 0.001094, accuracy: 0.0379
average test loss: 0.001035, accuracy: 0.0367
case acc: 0.027220933
case acc: 0.055998866
case acc: 0.033688262
case acc: 0.026035283
case acc: 0.039014045
case acc: 0.038375165
top acc: 0.0555 ::: bot acc: 0.0110
top acc: 0.0828 ::: bot acc: 0.0291
top acc: 0.0584 ::: bot acc: 0.0293
top acc: 0.0446 ::: bot acc: 0.0114
top acc: 0.0663 ::: bot acc: 0.0158
top acc: 0.0687 ::: bot acc: 0.0198
current epoch: 19
train loss is 0.001169
average val loss: 0.001148, accuracy: 0.0391
average test loss: 0.001092, accuracy: 0.0381
case acc: 0.029362487
case acc: 0.055736415
case acc: 0.034562275
case acc: 0.028619155
case acc: 0.040057234
case acc: 0.040189162
top acc: 0.0594 ::: bot acc: 0.0097
top acc: 0.0826 ::: bot acc: 0.0289
top acc: 0.0619 ::: bot acc: 0.0257
top acc: 0.0480 ::: bot acc: 0.0124
top acc: 0.0682 ::: bot acc: 0.0154
top acc: 0.0712 ::: bot acc: 0.0200
current epoch: 20
train loss is 0.001150
average val loss: 0.001194, accuracy: 0.0401
average test loss: 0.001140, accuracy: 0.0392
case acc: 0.03141535
case acc: 0.05538036
case acc: 0.03552119
case acc: 0.030909501
case acc: 0.040901586
case acc: 0.041315496
top acc: 0.0624 ::: bot acc: 0.0093
top acc: 0.0821 ::: bot acc: 0.0285
top acc: 0.0651 ::: bot acc: 0.0226
top acc: 0.0511 ::: bot acc: 0.0135
top acc: 0.0692 ::: bot acc: 0.0156
top acc: 0.0731 ::: bot acc: 0.0200
current epoch: 21
train loss is 0.001109
average val loss: 0.001175, accuracy: 0.0398
average test loss: 0.001124, accuracy: 0.0389
case acc: 0.032211073
case acc: 0.052925125
case acc: 0.035967045
case acc: 0.031272545
case acc: 0.040406495
case acc: 0.04086121
top acc: 0.0634 ::: bot acc: 0.0093
top acc: 0.0798 ::: bot acc: 0.0261
top acc: 0.0664 ::: bot acc: 0.0214
top acc: 0.0517 ::: bot acc: 0.0135
top acc: 0.0688 ::: bot acc: 0.0154
top acc: 0.0723 ::: bot acc: 0.0199
current epoch: 22
train loss is 0.001089
average val loss: 0.001168, accuracy: 0.0396
average test loss: 0.001116, accuracy: 0.0388
case acc: 0.03302279
case acc: 0.05124908
case acc: 0.03644108
case acc: 0.03180137
case acc: 0.040015906
case acc: 0.040339097
top acc: 0.0648 ::: bot acc: 0.0093
top acc: 0.0781 ::: bot acc: 0.0245
top acc: 0.0677 ::: bot acc: 0.0204
top acc: 0.0524 ::: bot acc: 0.0136
top acc: 0.0682 ::: bot acc: 0.0155
top acc: 0.0714 ::: bot acc: 0.0198
current epoch: 23
train loss is 0.001045
average val loss: 0.001239, accuracy: 0.0411
average test loss: 0.001185, accuracy: 0.0403
case acc: 0.035494544
case acc: 0.05210492
case acc: 0.03817513
case acc: 0.033755906
case acc: 0.041280653
case acc: 0.04115872
top acc: 0.0682 ::: bot acc: 0.0098
top acc: 0.0787 ::: bot acc: 0.0254
top acc: 0.0716 ::: bot acc: 0.0179
top acc: 0.0547 ::: bot acc: 0.0148
top acc: 0.0699 ::: bot acc: 0.0156
top acc: 0.0727 ::: bot acc: 0.0197
current epoch: 24
train loss is 0.001046
average val loss: 0.001285, accuracy: 0.0420
average test loss: 0.001236, accuracy: 0.0414
case acc: 0.03720367
case acc: 0.052451015
case acc: 0.039648898
case acc: 0.035487972
case acc: 0.041971743
case acc: 0.04178205
top acc: 0.0706 ::: bot acc: 0.0102
top acc: 0.0792 ::: bot acc: 0.0259
top acc: 0.0743 ::: bot acc: 0.0164
top acc: 0.0565 ::: bot acc: 0.0160
top acc: 0.0708 ::: bot acc: 0.0159
top acc: 0.0736 ::: bot acc: 0.0199
current epoch: 25
train loss is 0.001043
average val loss: 0.001356, accuracy: 0.0434
average test loss: 0.001308, accuracy: 0.0429
case acc: 0.039512277
case acc: 0.05324284
case acc: 0.041649163
case acc: 0.037502855
case acc: 0.042985804
case acc: 0.04256907
top acc: 0.0734 ::: bot acc: 0.0116
top acc: 0.0802 ::: bot acc: 0.0266
top acc: 0.0778 ::: bot acc: 0.0153
top acc: 0.0589 ::: bot acc: 0.0171
top acc: 0.0725 ::: bot acc: 0.0159
top acc: 0.0748 ::: bot acc: 0.0199
current epoch: 26
train loss is 0.001045
average val loss: 0.001394, accuracy: 0.0441
average test loss: 0.001344, accuracy: 0.0437
case acc: 0.04090646
case acc: 0.053098142
case acc: 0.04294417
case acc: 0.038708024
case acc: 0.043349985
case acc: 0.042901788
top acc: 0.0750 ::: bot acc: 0.0124
top acc: 0.0798 ::: bot acc: 0.0263
top acc: 0.0800 ::: bot acc: 0.0149
top acc: 0.0604 ::: bot acc: 0.0181
top acc: 0.0728 ::: bot acc: 0.0160
top acc: 0.0751 ::: bot acc: 0.0201
current epoch: 27
train loss is 0.001049
average val loss: 0.001472, accuracy: 0.0456
average test loss: 0.001424, accuracy: 0.0452
case acc: 0.043211795
case acc: 0.053856283
case acc: 0.044732615
case acc: 0.04077098
case acc: 0.044942047
case acc: 0.04382538
top acc: 0.0776 ::: bot acc: 0.0142
top acc: 0.0806 ::: bot acc: 0.0271
top acc: 0.0829 ::: bot acc: 0.0144
top acc: 0.0627 ::: bot acc: 0.0195
top acc: 0.0750 ::: bot acc: 0.0162
top acc: 0.0767 ::: bot acc: 0.0199
current epoch: 28
train loss is 0.001064
average val loss: 0.001593, accuracy: 0.0478
average test loss: 0.001546, accuracy: 0.0476
case acc: 0.046350442
case acc: 0.055413354
case acc: 0.047284782
case acc: 0.043578714
case acc: 0.047141433
case acc: 0.045564935
top acc: 0.0810 ::: bot acc: 0.0170
top acc: 0.0821 ::: bot acc: 0.0286
top acc: 0.0867 ::: bot acc: 0.0144
top acc: 0.0660 ::: bot acc: 0.0216
top acc: 0.0779 ::: bot acc: 0.0169
top acc: 0.0790 ::: bot acc: 0.0204
current epoch: 29
train loss is 0.001043
average val loss: 0.001547, accuracy: 0.0469
average test loss: 0.001499, accuracy: 0.0467
case acc: 0.04566729
case acc: 0.053408965
case acc: 0.047181554
case acc: 0.04294156
case acc: 0.046458073
case acc: 0.04447465
top acc: 0.0802 ::: bot acc: 0.0163
top acc: 0.0802 ::: bot acc: 0.0267
top acc: 0.0864 ::: bot acc: 0.0144
top acc: 0.0652 ::: bot acc: 0.0211
top acc: 0.0771 ::: bot acc: 0.0165
top acc: 0.0774 ::: bot acc: 0.0201
current epoch: 30
train loss is 0.001037
average val loss: 0.001539, accuracy: 0.0468
average test loss: 0.001494, accuracy: 0.0465
case acc: 0.045817193
case acc: 0.052201755
case acc: 0.047589667
case acc: 0.043021485
case acc: 0.046596378
case acc: 0.044059344
top acc: 0.0805 ::: bot acc: 0.0163
top acc: 0.0791 ::: bot acc: 0.0254
top acc: 0.0870 ::: bot acc: 0.0143
top acc: 0.0654 ::: bot acc: 0.0211
top acc: 0.0774 ::: bot acc: 0.0165
top acc: 0.0768 ::: bot acc: 0.0202
current epoch: 31
train loss is 0.001013
average val loss: 0.001509, accuracy: 0.0463
average test loss: 0.001466, accuracy: 0.0460
case acc: 0.045244794
case acc: 0.05074554
case acc: 0.047757782
case acc: 0.042788506
case acc: 0.04614339
case acc: 0.043411903
top acc: 0.0799 ::: bot acc: 0.0159
top acc: 0.0775 ::: bot acc: 0.0240
top acc: 0.0873 ::: bot acc: 0.0144
top acc: 0.0649 ::: bot acc: 0.0210
top acc: 0.0766 ::: bot acc: 0.0165
top acc: 0.0759 ::: bot acc: 0.0200
current epoch: 32
train loss is 0.000978
average val loss: 0.001537, accuracy: 0.0468
average test loss: 0.001489, accuracy: 0.0464
case acc: 0.045763016
case acc: 0.05063419
case acc: 0.048613377
case acc: 0.043670967
case acc: 0.04654408
case acc: 0.043381404
top acc: 0.0803 ::: bot acc: 0.0166
top acc: 0.0775 ::: bot acc: 0.0239
top acc: 0.0886 ::: bot acc: 0.0143
top acc: 0.0660 ::: bot acc: 0.0218
top acc: 0.0773 ::: bot acc: 0.0166
top acc: 0.0758 ::: bot acc: 0.0199
current epoch: 33
train loss is 0.000958
average val loss: 0.001591, accuracy: 0.0477
average test loss: 0.001539, accuracy: 0.0474
case acc: 0.04696521
case acc: 0.051053666
case acc: 0.049828716
case acc: 0.044833556
case acc: 0.047550946
case acc: 0.04406621
top acc: 0.0815 ::: bot acc: 0.0177
top acc: 0.0779 ::: bot acc: 0.0242
top acc: 0.0902 ::: bot acc: 0.0146
top acc: 0.0672 ::: bot acc: 0.0226
top acc: 0.0787 ::: bot acc: 0.0169
top acc: 0.0767 ::: bot acc: 0.0203
current epoch: 34
train loss is 0.000976
average val loss: 0.001573, accuracy: 0.0474
average test loss: 0.001524, accuracy: 0.0471
case acc: 0.046537668
case acc: 0.04988598
case acc: 0.049889125
case acc: 0.04482056
case acc: 0.047544677
case acc: 0.04370368
top acc: 0.0810 ::: bot acc: 0.0174
top acc: 0.0767 ::: bot acc: 0.0232
top acc: 0.0904 ::: bot acc: 0.0145
top acc: 0.0673 ::: bot acc: 0.0226
top acc: 0.0787 ::: bot acc: 0.0167
top acc: 0.0764 ::: bot acc: 0.0200
current epoch: 35
train loss is 0.000990
average val loss: 0.001665, accuracy: 0.0490
average test loss: 0.001617, accuracy: 0.0488
case acc: 0.048352934
case acc: 0.050780058
case acc: 0.051666595
case acc: 0.047036503
case acc: 0.049653776
case acc: 0.04529102
top acc: 0.0831 ::: bot acc: 0.0188
top acc: 0.0775 ::: bot acc: 0.0240
top acc: 0.0926 ::: bot acc: 0.0151
top acc: 0.0696 ::: bot acc: 0.0244
top acc: 0.0812 ::: bot acc: 0.0178
top acc: 0.0786 ::: bot acc: 0.0203
current epoch: 36
train loss is 0.000948
average val loss: 0.001538, accuracy: 0.0467
average test loss: 0.001486, accuracy: 0.0463
case acc: 0.045265313
case acc: 0.04689478
case acc: 0.04964573
case acc: 0.04483389
case acc: 0.04779262
case acc: 0.043320805
top acc: 0.0797 ::: bot acc: 0.0160
top acc: 0.0736 ::: bot acc: 0.0205
top acc: 0.0900 ::: bot acc: 0.0145
top acc: 0.0673 ::: bot acc: 0.0226
top acc: 0.0789 ::: bot acc: 0.0171
top acc: 0.0759 ::: bot acc: 0.0197
current epoch: 37
train loss is 0.000909
average val loss: 0.001442, accuracy: 0.0449
average test loss: 0.001390, accuracy: 0.0444
case acc: 0.042832658
case acc: 0.043760788
case acc: 0.0480984
case acc: 0.043091267
case acc: 0.046522513
case acc: 0.04201758
top acc: 0.0773 ::: bot acc: 0.0138
top acc: 0.0704 ::: bot acc: 0.0175
top acc: 0.0878 ::: bot acc: 0.0144
top acc: 0.0654 ::: bot acc: 0.0211
top acc: 0.0773 ::: bot acc: 0.0165
top acc: 0.0738 ::: bot acc: 0.0198
current epoch: 38
train loss is 0.000867
average val loss: 0.001346, accuracy: 0.0432
average test loss: 0.001292, accuracy: 0.0424
case acc: 0.040479556
case acc: 0.04055919
case acc: 0.046420682
case acc: 0.041199423
case acc: 0.045110017
case acc: 0.04049011
top acc: 0.0746 ::: bot acc: 0.0121
top acc: 0.0670 ::: bot acc: 0.0146
top acc: 0.0855 ::: bot acc: 0.0141
top acc: 0.0633 ::: bot acc: 0.0196
top acc: 0.0754 ::: bot acc: 0.0162
top acc: 0.0716 ::: bot acc: 0.0197
current epoch: 39
train loss is 0.000842
average val loss: 0.001264, accuracy: 0.0415
average test loss: 0.001210, accuracy: 0.0407
case acc: 0.038534317
case acc: 0.037844054
case acc: 0.045148175
case acc: 0.039645445
case acc: 0.043857303
case acc: 0.039180614
top acc: 0.0721 ::: bot acc: 0.0111
top acc: 0.0641 ::: bot acc: 0.0124
top acc: 0.0834 ::: bot acc: 0.0144
top acc: 0.0614 ::: bot acc: 0.0186
top acc: 0.0736 ::: bot acc: 0.0159
top acc: 0.0697 ::: bot acc: 0.0195
current epoch: 40
train loss is 0.000812
average val loss: 0.001180, accuracy: 0.0398
average test loss: 0.001126, accuracy: 0.0389
case acc: 0.03637734
case acc: 0.03510414
case acc: 0.043506198
case acc: 0.038046163
case acc: 0.042362478
case acc: 0.03791117
top acc: 0.0696 ::: bot acc: 0.0101
top acc: 0.0610 ::: bot acc: 0.0105
top acc: 0.0807 ::: bot acc: 0.0146
top acc: 0.0595 ::: bot acc: 0.0176
top acc: 0.0714 ::: bot acc: 0.0159
top acc: 0.0680 ::: bot acc: 0.0194
current epoch: 41
train loss is 0.000797
average val loss: 0.001177, accuracy: 0.0398
average test loss: 0.001125, accuracy: 0.0388
case acc: 0.036005452
case acc: 0.03441567
case acc: 0.043510675
case acc: 0.038274005
case acc: 0.04263911
case acc: 0.038215566
top acc: 0.0691 ::: bot acc: 0.0097
top acc: 0.0601 ::: bot acc: 0.0102
top acc: 0.0808 ::: bot acc: 0.0147
top acc: 0.0600 ::: bot acc: 0.0175
top acc: 0.0718 ::: bot acc: 0.0158
top acc: 0.0684 ::: bot acc: 0.0196
current epoch: 42
train loss is 0.000780
average val loss: 0.001088, accuracy: 0.0380
average test loss: 0.001034, accuracy: 0.0368
case acc: 0.033804476
case acc: 0.0314338
case acc: 0.041593872
case acc: 0.03635496
case acc: 0.041010443
case acc: 0.036816336
top acc: 0.0659 ::: bot acc: 0.0095
top acc: 0.0565 ::: bot acc: 0.0087
top acc: 0.0778 ::: bot acc: 0.0152
top acc: 0.0577 ::: bot acc: 0.0164
top acc: 0.0695 ::: bot acc: 0.0157
top acc: 0.0661 ::: bot acc: 0.0197
current epoch: 43
train loss is 0.000747
average val loss: 0.001006, accuracy: 0.0363
average test loss: 0.000952, accuracy: 0.0350
case acc: 0.03180526
case acc: 0.028922189
case acc: 0.039972253
case acc: 0.034328487
case acc: 0.039401338
case acc: 0.035511103
top acc: 0.0629 ::: bot acc: 0.0096
top acc: 0.0527 ::: bot acc: 0.0083
top acc: 0.0749 ::: bot acc: 0.0162
top acc: 0.0553 ::: bot acc: 0.0151
top acc: 0.0672 ::: bot acc: 0.0155
top acc: 0.0639 ::: bot acc: 0.0205
current epoch: 44
train loss is 0.000726
average val loss: 0.000908, accuracy: 0.0343
average test loss: 0.000852, accuracy: 0.0326
case acc: 0.02912428
case acc: 0.026051821
case acc: 0.03798133
case acc: 0.03151276
case acc: 0.03732434
case acc: 0.03384774
top acc: 0.0588 ::: bot acc: 0.0097
top acc: 0.0483 ::: bot acc: 0.0089
top acc: 0.0710 ::: bot acc: 0.0183
top acc: 0.0519 ::: bot acc: 0.0136
top acc: 0.0640 ::: bot acc: 0.0159
top acc: 0.0607 ::: bot acc: 0.0219
current epoch: 45
train loss is 0.000709
average val loss: 0.000897, accuracy: 0.0340
average test loss: 0.000837, accuracy: 0.0323
case acc: 0.02864396
case acc: 0.025292344
case acc: 0.03739477
case acc: 0.031183966
case acc: 0.037321262
case acc: 0.033811696
top acc: 0.0579 ::: bot acc: 0.0101
top acc: 0.0471 ::: bot acc: 0.0093
top acc: 0.0698 ::: bot acc: 0.0188
top acc: 0.0515 ::: bot acc: 0.0133
top acc: 0.0640 ::: bot acc: 0.0158
top acc: 0.0608 ::: bot acc: 0.0217
current epoch: 46
train loss is 0.000697
average val loss: 0.000799, accuracy: 0.0319
average test loss: 0.000739, accuracy: 0.0300
case acc: 0.026089117
case acc: 0.022842921
case acc: 0.035433494
case acc: 0.02816663
case acc: 0.03484775
case acc: 0.03237409
top acc: 0.0531 ::: bot acc: 0.0125
top acc: 0.0416 ::: bot acc: 0.0127
top acc: 0.0646 ::: bot acc: 0.0232
top acc: 0.0475 ::: bot acc: 0.0122
top acc: 0.0599 ::: bot acc: 0.0166
top acc: 0.0572 ::: bot acc: 0.0244
current epoch: 47
train loss is 0.000684
average val loss: 0.000731, accuracy: 0.0303
average test loss: 0.000669, accuracy: 0.0282
case acc: 0.024298731
case acc: 0.02122979
case acc: 0.034236327
case acc: 0.025531366
case acc: 0.0326587
case acc: 0.03138968
top acc: 0.0486 ::: bot acc: 0.0159
top acc: 0.0371 ::: bot acc: 0.0170
top acc: 0.0601 ::: bot acc: 0.0278
top acc: 0.0439 ::: bot acc: 0.0114
top acc: 0.0562 ::: bot acc: 0.0176
top acc: 0.0543 ::: bot acc: 0.0272
current epoch: 48
train loss is 0.000679
average val loss: 0.000673, accuracy: 0.0290
average test loss: 0.000609, accuracy: 0.0267
case acc: 0.02318921
case acc: 0.01996459
case acc: 0.03280928
case acc: 0.023012789
case acc: 0.03078428
case acc: 0.030475914
top acc: 0.0438 ::: bot acc: 0.0205
top acc: 0.0317 ::: bot acc: 0.0221
top acc: 0.0547 ::: bot acc: 0.0329
top acc: 0.0400 ::: bot acc: 0.0119
top acc: 0.0524 ::: bot acc: 0.0197
top acc: 0.0508 ::: bot acc: 0.0305
current epoch: 49
train loss is 0.000688
average val loss: 0.000664, accuracy: 0.0288
average test loss: 0.000601, accuracy: 0.0265
case acc: 0.02303074
case acc: 0.019732986
case acc: 0.032610394
case acc: 0.022417523
case acc: 0.030673184
case acc: 0.030526947
top acc: 0.0424 ::: bot acc: 0.0222
top acc: 0.0297 ::: bot acc: 0.0244
top acc: 0.0525 ::: bot acc: 0.0355
top acc: 0.0389 ::: bot acc: 0.0122
top acc: 0.0522 ::: bot acc: 0.0198
top acc: 0.0509 ::: bot acc: 0.0305
current epoch: 50
train loss is 0.000692
average val loss: 0.000637, accuracy: 0.0282
average test loss: 0.000573, accuracy: 0.0258
case acc: 0.022721142
case acc: 0.019578576
case acc: 0.032136098
case acc: 0.020745218
case acc: 0.02950667
case acc: 0.030259738
top acc: 0.0382 ::: bot acc: 0.0261
top acc: 0.0247 ::: bot acc: 0.0290
top acc: 0.0474 ::: bot acc: 0.0405
top acc: 0.0356 ::: bot acc: 0.0141
top acc: 0.0497 ::: bot acc: 0.0213
top acc: 0.0488 ::: bot acc: 0.0326

		{"drop_out": 0.6, "drop_out_mc": 0.1, "repeat_mc": 50, "hidden": 30, "embedding_size": 5, "batch": 512, "lag": 4}
LME_Co_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6768 6768 6768
1.8562728 -0.6288155 0.2585643 -0.19947179
Validation: 756 756 756
Testing: 774 774 774
pre-processing time: 0.0002117156982421875
the split date is 2009-07-01
train dropout: 0.6 test dropout: 0.1
net initializing with time: 0.003643512725830078
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.013014
average val loss: 0.005983, accuracy: 0.1024
average test loss: 0.006548, accuracy: 0.1079
case acc: 0.1526473
case acc: 0.07342347
case acc: 0.111501046
case acc: 0.10404726
case acc: 0.13537301
case acc: 0.0706548
top acc: 0.1359 ::: bot acc: 0.1681
top acc: 0.1014 ::: bot acc: 0.0473
top acc: 0.0738 ::: bot acc: 0.1473
top acc: 0.0793 ::: bot acc: 0.1288
top acc: 0.1119 ::: bot acc: 0.1615
top acc: 0.0499 ::: bot acc: 0.0923
current epoch: 2
train loss is 0.008574
average val loss: 0.002901, accuracy: 0.0544
average test loss: 0.002790, accuracy: 0.0543
case acc: 0.051330153
case acc: 0.1574787
case acc: 0.028539078
case acc: 0.021047415
case acc: 0.047535863
case acc: 0.019852431
top acc: 0.0339 ::: bot acc: 0.0671
top acc: 0.1853 ::: bot acc: 0.1310
top acc: 0.0263 ::: bot acc: 0.0458
top acc: 0.0156 ::: bot acc: 0.0366
top acc: 0.0238 ::: bot acc: 0.0734
top acc: 0.0336 ::: bot acc: 0.0110
current epoch: 3
train loss is 0.008840
average val loss: 0.006931, accuracy: 0.0949
average test loss: 0.006285, accuracy: 0.0877
case acc: 0.03626903
case acc: 0.22994418
case acc: 0.075991035
case acc: 0.06819775
case acc: 0.03209763
case acc: 0.08373315
top acc: 0.0535 ::: bot acc: 0.0207
top acc: 0.2577 ::: bot acc: 0.2038
top acc: 0.1140 ::: bot acc: 0.0409
top acc: 0.0933 ::: bot acc: 0.0430
top acc: 0.0533 ::: bot acc: 0.0114
top acc: 0.1046 ::: bot acc: 0.0616
current epoch: 4
train loss is 0.010151
average val loss: 0.016069, accuracy: 0.1665
average test loss: 0.014879, accuracy: 0.1594
case acc: 0.11600348
case acc: 0.29592788
case acc: 0.15465082
case acc: 0.1389714
case acc: 0.10345218
case acc: 0.14731929
top acc: 0.1329 ::: bot acc: 0.0991
top acc: 0.3233 ::: bot acc: 0.2696
top acc: 0.1922 ::: bot acc: 0.1194
top acc: 0.1643 ::: bot acc: 0.1135
top acc: 0.1281 ::: bot acc: 0.0765
top acc: 0.1682 ::: bot acc: 0.1252
current epoch: 5
train loss is 0.013143
average val loss: 0.014690, accuracy: 0.1596
average test loss: 0.013544, accuracy: 0.1525
case acc: 0.1146452
case acc: 0.2797808
case acc: 0.15096398
case acc: 0.13099174
case acc: 0.103769004
case acc: 0.13499358
top acc: 0.1310 ::: bot acc: 0.0986
top acc: 0.3073 ::: bot acc: 0.2539
top acc: 0.1887 ::: bot acc: 0.1156
top acc: 0.1562 ::: bot acc: 0.1053
top acc: 0.1279 ::: bot acc: 0.0769
top acc: 0.1563 ::: bot acc: 0.1123
current epoch: 6
train loss is 0.013632
average val loss: 0.004254, accuracy: 0.0727
average test loss: 0.003717, accuracy: 0.0654
case acc: 0.03095633
case acc: 0.17865685
case acc: 0.06205742
case acc: 0.043844525
case acc: 0.030553693
case acc: 0.046536833
top acc: 0.0476 ::: bot acc: 0.0154
top acc: 0.2058 ::: bot acc: 0.1527
top acc: 0.0994 ::: bot acc: 0.0276
top acc: 0.0681 ::: bot acc: 0.0208
top acc: 0.0515 ::: bot acc: 0.0106
top acc: 0.0668 ::: bot acc: 0.0251
current epoch: 7
train loss is 0.008228
average val loss: 0.001370, accuracy: 0.0388
average test loss: 0.001325, accuracy: 0.0399
case acc: 0.03358388
case acc: 0.09751679
case acc: 0.028671958
case acc: 0.028317668
case acc: 0.029382048
case acc: 0.021907918
top acc: 0.0167 ::: bot acc: 0.0499
top acc: 0.1252 ::: bot acc: 0.0712
top acc: 0.0273 ::: bot acc: 0.0459
top acc: 0.0109 ::: bot acc: 0.0499
top acc: 0.0093 ::: bot acc: 0.0541
top acc: 0.0080 ::: bot acc: 0.0411
current epoch: 8
train loss is 0.004006
average val loss: 0.001177, accuracy: 0.0343
average test loss: 0.001103, accuracy: 0.0347
case acc: 0.02637175
case acc: 0.092189886
case acc: 0.028183633
case acc: 0.023866469
case acc: 0.020543786
case acc: 0.016982507
top acc: 0.0112 ::: bot acc: 0.0418
top acc: 0.1192 ::: bot acc: 0.0662
top acc: 0.0293 ::: bot acc: 0.0441
top acc: 0.0112 ::: bot acc: 0.0433
top acc: 0.0126 ::: bot acc: 0.0391
top acc: 0.0118 ::: bot acc: 0.0312
current epoch: 9
train loss is 0.002938
average val loss: 0.001241, accuracy: 0.0347
average test loss: 0.001087, accuracy: 0.0325
case acc: 0.015142059
case acc: 0.096695736
case acc: 0.026707558
case acc: 0.019009497
case acc: 0.020509697
case acc: 0.017173199
top acc: 0.0091 ::: bot acc: 0.0261
top acc: 0.1240 ::: bot acc: 0.0708
top acc: 0.0398 ::: bot acc: 0.0337
top acc: 0.0219 ::: bot acc: 0.0296
top acc: 0.0315 ::: bot acc: 0.0202
top acc: 0.0274 ::: bot acc: 0.0159
current epoch: 10
train loss is 0.002743
average val loss: 0.001183, accuracy: 0.0347
average test loss: 0.001021, accuracy: 0.0321
case acc: 0.013235879
case acc: 0.09180874
case acc: 0.026780354
case acc: 0.018744765
case acc: 0.022933558
case acc: 0.018812383
top acc: 0.0122 ::: bot acc: 0.0218
top acc: 0.1190 ::: bot acc: 0.0661
top acc: 0.0410 ::: bot acc: 0.0326
top acc: 0.0247 ::: bot acc: 0.0263
top acc: 0.0384 ::: bot acc: 0.0145
top acc: 0.0311 ::: bot acc: 0.0136
current epoch: 11
train loss is 0.002594
average val loss: 0.001041, accuracy: 0.0332
average test loss: 0.000890, accuracy: 0.0306
case acc: 0.013146165
case acc: 0.08283604
case acc: 0.027076738
case acc: 0.018593656
case acc: 0.023369312
case acc: 0.018531496
top acc: 0.0125 ::: bot acc: 0.0216
top acc: 0.1102 ::: bot acc: 0.0568
top acc: 0.0382 ::: bot acc: 0.0352
top acc: 0.0238 ::: bot acc: 0.0269
top acc: 0.0392 ::: bot acc: 0.0140
top acc: 0.0304 ::: bot acc: 0.0137
current epoch: 12
train loss is 0.002314
average val loss: 0.000994, accuracy: 0.0332
average test loss: 0.000839, accuracy: 0.0302
case acc: 0.012055709
case acc: 0.07791225
case acc: 0.02704476
case acc: 0.019025868
case acc: 0.025585555
case acc: 0.019828837
top acc: 0.0152 ::: bot acc: 0.0176
top acc: 0.1052 ::: bot acc: 0.0522
top acc: 0.0386 ::: bot acc: 0.0349
top acc: 0.0265 ::: bot acc: 0.0250
top acc: 0.0436 ::: bot acc: 0.0115
top acc: 0.0332 ::: bot acc: 0.0126
current epoch: 13
train loss is 0.002115
average val loss: 0.000895, accuracy: 0.0320
average test loss: 0.000750, accuracy: 0.0290
case acc: 0.012155595
case acc: 0.070550844
case acc: 0.027101511
case acc: 0.01874608
case acc: 0.026030898
case acc: 0.019544339
top acc: 0.0159 ::: bot acc: 0.0172
top acc: 0.0981 ::: bot acc: 0.0447
top acc: 0.0366 ::: bot acc: 0.0369
top acc: 0.0258 ::: bot acc: 0.0252
top acc: 0.0442 ::: bot acc: 0.0116
top acc: 0.0325 ::: bot acc: 0.0129
current epoch: 14
train loss is 0.001897
average val loss: 0.000822, accuracy: 0.0310
average test loss: 0.000683, accuracy: 0.0280
case acc: 0.012056435
case acc: 0.064440824
case acc: 0.026997225
case acc: 0.018736783
case acc: 0.026239287
case acc: 0.019652832
top acc: 0.0174 ::: bot acc: 0.0156
top acc: 0.0917 ::: bot acc: 0.0388
top acc: 0.0359 ::: bot acc: 0.0374
top acc: 0.0262 ::: bot acc: 0.0251
top acc: 0.0448 ::: bot acc: 0.0110
top acc: 0.0330 ::: bot acc: 0.0126
current epoch: 15
train loss is 0.001756
average val loss: 0.000781, accuracy: 0.0306
average test loss: 0.000641, accuracy: 0.0275
case acc: 0.012226939
case acc: 0.059817668
case acc: 0.027279193
case acc: 0.019020287
case acc: 0.026704991
case acc: 0.019794557
top acc: 0.0191 ::: bot acc: 0.0139
top acc: 0.0873 ::: bot acc: 0.0338
top acc: 0.0361 ::: bot acc: 0.0377
top acc: 0.0274 ::: bot acc: 0.0243
top acc: 0.0457 ::: bot acc: 0.0109
top acc: 0.0334 ::: bot acc: 0.0124
current epoch: 16
train loss is 0.001668
average val loss: 0.000744, accuracy: 0.0302
average test loss: 0.000606, accuracy: 0.0269
case acc: 0.012288315
case acc: 0.05599094
case acc: 0.027071392
case acc: 0.018954322
case acc: 0.026933705
case acc: 0.020103857
top acc: 0.0206 ::: bot acc: 0.0124
top acc: 0.0833 ::: bot acc: 0.0300
top acc: 0.0367 ::: bot acc: 0.0369
top acc: 0.0282 ::: bot acc: 0.0230
top acc: 0.0460 ::: bot acc: 0.0106
top acc: 0.0335 ::: bot acc: 0.0125
current epoch: 17
train loss is 0.001586
average val loss: 0.000764, accuracy: 0.0310
average test loss: 0.000610, accuracy: 0.0273
case acc: 0.013317886
case acc: 0.054469317
case acc: 0.026812308
case acc: 0.019432068
case acc: 0.028489253
case acc: 0.021154555
top acc: 0.0242 ::: bot acc: 0.0091
top acc: 0.0819 ::: bot acc: 0.0287
top acc: 0.0392 ::: bot acc: 0.0342
top acc: 0.0309 ::: bot acc: 0.0201
top acc: 0.0486 ::: bot acc: 0.0103
top acc: 0.0355 ::: bot acc: 0.0119
current epoch: 18
train loss is 0.001525
average val loss: 0.000768, accuracy: 0.0314
average test loss: 0.000599, accuracy: 0.0273
case acc: 0.014474573
case acc: 0.052258965
case acc: 0.026671829
case acc: 0.0198251
case acc: 0.028568877
case acc: 0.021705266
top acc: 0.0266 ::: bot acc: 0.0077
top acc: 0.0794 ::: bot acc: 0.0265
top acc: 0.0413 ::: bot acc: 0.0320
top acc: 0.0326 ::: bot acc: 0.0184
top acc: 0.0488 ::: bot acc: 0.0104
top acc: 0.0363 ::: bot acc: 0.0120
current epoch: 19
train loss is 0.001492
average val loss: 0.000729, accuracy: 0.0307
average test loss: 0.000569, accuracy: 0.0266
case acc: 0.014742994
case acc: 0.049172405
case acc: 0.026848273
case acc: 0.019971604
case acc: 0.027929781
case acc: 0.021094762
top acc: 0.0275 ::: bot acc: 0.0072
top acc: 0.0763 ::: bot acc: 0.0235
top acc: 0.0419 ::: bot acc: 0.0315
top acc: 0.0326 ::: bot acc: 0.0183
top acc: 0.0476 ::: bot acc: 0.0105
top acc: 0.0356 ::: bot acc: 0.0118
current epoch: 20
train loss is 0.001427
average val loss: 0.000712, accuracy: 0.0303
average test loss: 0.000548, accuracy: 0.0262
case acc: 0.015243451
case acc: 0.04665808
case acc: 0.026795557
case acc: 0.020094346
case acc: 0.027434751
case acc: 0.020853963
top acc: 0.0287 ::: bot acc: 0.0067
top acc: 0.0740 ::: bot acc: 0.0210
top acc: 0.0428 ::: bot acc: 0.0305
top acc: 0.0334 ::: bot acc: 0.0179
top acc: 0.0467 ::: bot acc: 0.0107
top acc: 0.0350 ::: bot acc: 0.0120
current epoch: 21
train loss is 0.001413
average val loss: 0.000721, accuracy: 0.0307
average test loss: 0.000546, accuracy: 0.0263
case acc: 0.016400851
case acc: 0.045380447
case acc: 0.026784407
case acc: 0.020663707
case acc: 0.027487593
case acc: 0.021109367
top acc: 0.0305 ::: bot acc: 0.0062
top acc: 0.0724 ::: bot acc: 0.0198
top acc: 0.0447 ::: bot acc: 0.0286
top acc: 0.0350 ::: bot acc: 0.0165
top acc: 0.0467 ::: bot acc: 0.0108
top acc: 0.0354 ::: bot acc: 0.0121
current epoch: 22
train loss is 0.001383
average val loss: 0.000791, accuracy: 0.0326
average test loss: 0.000596, accuracy: 0.0278
case acc: 0.019137457
case acc: 0.04675021
case acc: 0.02765057
case acc: 0.021845667
case acc: 0.029048052
case acc: 0.022612177
top acc: 0.0344 ::: bot acc: 0.0062
top acc: 0.0741 ::: bot acc: 0.0212
top acc: 0.0492 ::: bot acc: 0.0245
top acc: 0.0384 ::: bot acc: 0.0135
top acc: 0.0492 ::: bot acc: 0.0107
top acc: 0.0378 ::: bot acc: 0.0117
current epoch: 23
train loss is 0.001389
average val loss: 0.000861, accuracy: 0.0344
average test loss: 0.000639, accuracy: 0.0292
case acc: 0.021645283
case acc: 0.04742789
case acc: 0.02878696
case acc: 0.02325462
case acc: 0.030449975
case acc: 0.023679739
top acc: 0.0377 ::: bot acc: 0.0073
top acc: 0.0745 ::: bot acc: 0.0219
top acc: 0.0528 ::: bot acc: 0.0207
top acc: 0.0412 ::: bot acc: 0.0122
top acc: 0.0514 ::: bot acc: 0.0107
top acc: 0.0394 ::: bot acc: 0.0120
current epoch: 24
train loss is 0.001346
average val loss: 0.000785, accuracy: 0.0325
average test loss: 0.000580, accuracy: 0.0276
case acc: 0.020376425
case acc: 0.043862574
case acc: 0.028562408
case acc: 0.022272136
case acc: 0.028605694
case acc: 0.021787481
top acc: 0.0359 ::: bot acc: 0.0068
top acc: 0.0710 ::: bot acc: 0.0187
top acc: 0.0517 ::: bot acc: 0.0221
top acc: 0.0394 ::: bot acc: 0.0131
top acc: 0.0486 ::: bot acc: 0.0105
top acc: 0.0367 ::: bot acc: 0.0116
current epoch: 25
train loss is 0.001313
average val loss: 0.000836, accuracy: 0.0339
average test loss: 0.000614, accuracy: 0.0286
case acc: 0.02253855
case acc: 0.04425621
case acc: 0.029314186
case acc: 0.023410441
case acc: 0.029678019
case acc: 0.022562252
top acc: 0.0387 ::: bot acc: 0.0078
top acc: 0.0713 ::: bot acc: 0.0191
top acc: 0.0546 ::: bot acc: 0.0192
top acc: 0.0415 ::: bot acc: 0.0121
top acc: 0.0501 ::: bot acc: 0.0106
top acc: 0.0378 ::: bot acc: 0.0117
current epoch: 26
train loss is 0.001299
average val loss: 0.000798, accuracy: 0.0329
average test loss: 0.000586, accuracy: 0.0278
case acc: 0.022001049
case acc: 0.04221045
case acc: 0.029296309
case acc: 0.02319096
case acc: 0.028663797
case acc: 0.021614943
top acc: 0.0382 ::: bot acc: 0.0074
top acc: 0.0693 ::: bot acc: 0.0168
top acc: 0.0545 ::: bot acc: 0.0192
top acc: 0.0409 ::: bot acc: 0.0125
top acc: 0.0486 ::: bot acc: 0.0106
top acc: 0.0365 ::: bot acc: 0.0115
current epoch: 27
train loss is 0.001287
average val loss: 0.000814, accuracy: 0.0333
average test loss: 0.000591, accuracy: 0.0281
case acc: 0.022770159
case acc: 0.041517116
case acc: 0.029902855
case acc: 0.023598367
case acc: 0.028833238
case acc: 0.02169208
top acc: 0.0388 ::: bot acc: 0.0082
top acc: 0.0686 ::: bot acc: 0.0164
top acc: 0.0560 ::: bot acc: 0.0179
top acc: 0.0417 ::: bot acc: 0.0120
top acc: 0.0486 ::: bot acc: 0.0106
top acc: 0.0364 ::: bot acc: 0.0118
current epoch: 28
train loss is 0.001280
average val loss: 0.000851, accuracy: 0.0343
average test loss: 0.000617, accuracy: 0.0288
case acc: 0.024455082
case acc: 0.041727383
case acc: 0.03071549
case acc: 0.02428431
case acc: 0.029450113
case acc: 0.022321379
top acc: 0.0408 ::: bot acc: 0.0093
top acc: 0.0687 ::: bot acc: 0.0165
top acc: 0.0583 ::: bot acc: 0.0158
top acc: 0.0431 ::: bot acc: 0.0113
top acc: 0.0499 ::: bot acc: 0.0104
top acc: 0.0374 ::: bot acc: 0.0119
current epoch: 29
train loss is 0.001265
average val loss: 0.000896, accuracy: 0.0355
average test loss: 0.000652, accuracy: 0.0299
case acc: 0.026241034
case acc: 0.042125087
case acc: 0.031613983
case acc: 0.025556073
case acc: 0.03073564
case acc: 0.022867568
top acc: 0.0425 ::: bot acc: 0.0108
top acc: 0.0693 ::: bot acc: 0.0168
top acc: 0.0606 ::: bot acc: 0.0141
top acc: 0.0448 ::: bot acc: 0.0115
top acc: 0.0516 ::: bot acc: 0.0110
top acc: 0.0382 ::: bot acc: 0.0115
current epoch: 30
train loss is 0.001274
average val loss: 0.000908, accuracy: 0.0357
average test loss: 0.000660, accuracy: 0.0301
case acc: 0.02672289
case acc: 0.04176627
case acc: 0.032142505
case acc: 0.025930665
case acc: 0.031179432
case acc: 0.022931857
top acc: 0.0434 ::: bot acc: 0.0115
top acc: 0.0689 ::: bot acc: 0.0168
top acc: 0.0615 ::: bot acc: 0.0136
top acc: 0.0458 ::: bot acc: 0.0113
top acc: 0.0520 ::: bot acc: 0.0109
top acc: 0.0381 ::: bot acc: 0.0118
current epoch: 31
train loss is 0.001255
average val loss: 0.000927, accuracy: 0.0361
average test loss: 0.000676, accuracy: 0.0305
case acc: 0.027544787
case acc: 0.041602936
case acc: 0.03283985
case acc: 0.02647394
case acc: 0.031697888
case acc: 0.022895679
top acc: 0.0443 ::: bot acc: 0.0119
top acc: 0.0688 ::: bot acc: 0.0164
top acc: 0.0630 ::: bot acc: 0.0126
top acc: 0.0464 ::: bot acc: 0.0115
top acc: 0.0527 ::: bot acc: 0.0114
top acc: 0.0385 ::: bot acc: 0.0116
current epoch: 32
train loss is 0.001264
average val loss: 0.000967, accuracy: 0.0371
average test loss: 0.000703, accuracy: 0.0313
case acc: 0.028619856
case acc: 0.041944742
case acc: 0.033811573
case acc: 0.027434647
case acc: 0.03248316
case acc: 0.023397462
top acc: 0.0452 ::: bot acc: 0.0131
top acc: 0.0690 ::: bot acc: 0.0170
top acc: 0.0651 ::: bot acc: 0.0117
top acc: 0.0478 ::: bot acc: 0.0117
top acc: 0.0539 ::: bot acc: 0.0117
top acc: 0.0390 ::: bot acc: 0.0117
current epoch: 33
train loss is 0.001262
average val loss: 0.001009, accuracy: 0.0380
average test loss: 0.000734, accuracy: 0.0321
case acc: 0.02967451
case acc: 0.042131446
case acc: 0.03490237
case acc: 0.028420752
case acc: 0.033516824
case acc: 0.024157267
top acc: 0.0463 ::: bot acc: 0.0142
top acc: 0.0692 ::: bot acc: 0.0170
top acc: 0.0668 ::: bot acc: 0.0114
top acc: 0.0493 ::: bot acc: 0.0114
top acc: 0.0554 ::: bot acc: 0.0118
top acc: 0.0400 ::: bot acc: 0.0120
current epoch: 34
train loss is 0.001257
average val loss: 0.000977, accuracy: 0.0372
average test loss: 0.000707, accuracy: 0.0314
case acc: 0.028901968
case acc: 0.04063043
case acc: 0.034646228
case acc: 0.027983133
case acc: 0.033216923
case acc: 0.023142667
top acc: 0.0453 ::: bot acc: 0.0136
top acc: 0.0675 ::: bot acc: 0.0157
top acc: 0.0663 ::: bot acc: 0.0113
top acc: 0.0486 ::: bot acc: 0.0115
top acc: 0.0546 ::: bot acc: 0.0118
top acc: 0.0383 ::: bot acc: 0.0120
current epoch: 35
train loss is 0.001241
average val loss: 0.000972, accuracy: 0.0372
average test loss: 0.000702, accuracy: 0.0312
case acc: 0.028619582
case acc: 0.03972206
case acc: 0.034876484
case acc: 0.027979989
case acc: 0.033174533
case acc: 0.023106977
top acc: 0.0449 ::: bot acc: 0.0135
top acc: 0.0667 ::: bot acc: 0.0148
top acc: 0.0668 ::: bot acc: 0.0111
top acc: 0.0486 ::: bot acc: 0.0113
top acc: 0.0547 ::: bot acc: 0.0118
top acc: 0.0385 ::: bot acc: 0.0118
current epoch: 36
train loss is 0.001228
average val loss: 0.000985, accuracy: 0.0375
average test loss: 0.000710, accuracy: 0.0315
case acc: 0.028641615
case acc: 0.039486606
case acc: 0.035207044
case acc: 0.028583398
case acc: 0.033521358
case acc: 0.023430638
top acc: 0.0451 ::: bot acc: 0.0132
top acc: 0.0664 ::: bot acc: 0.0147
top acc: 0.0674 ::: bot acc: 0.0110
top acc: 0.0494 ::: bot acc: 0.0115
top acc: 0.0551 ::: bot acc: 0.0119
top acc: 0.0389 ::: bot acc: 0.0119
current epoch: 37
train loss is 0.001205
average val loss: 0.000912, accuracy: 0.0357
average test loss: 0.000651, accuracy: 0.0298
case acc: 0.026176935
case acc: 0.037039727
case acc: 0.034301717
case acc: 0.027239904
case acc: 0.03206226
case acc: 0.022149581
top acc: 0.0426 ::: bot acc: 0.0109
top acc: 0.0636 ::: bot acc: 0.0130
top acc: 0.0658 ::: bot acc: 0.0115
top acc: 0.0474 ::: bot acc: 0.0114
top acc: 0.0532 ::: bot acc: 0.0115
top acc: 0.0371 ::: bot acc: 0.0119
current epoch: 38
train loss is 0.001177
average val loss: 0.000852, accuracy: 0.0343
average test loss: 0.000605, accuracy: 0.0285
case acc: 0.02445958
case acc: 0.034658648
case acc: 0.033282984
case acc: 0.02637984
case acc: 0.031141724
case acc: 0.02101369
top acc: 0.0408 ::: bot acc: 0.0096
top acc: 0.0610 ::: bot acc: 0.0111
top acc: 0.0638 ::: bot acc: 0.0122
top acc: 0.0461 ::: bot acc: 0.0114
top acc: 0.0520 ::: bot acc: 0.0110
top acc: 0.0352 ::: bot acc: 0.0121
current epoch: 39
train loss is 0.001156
average val loss: 0.000826, accuracy: 0.0336
average test loss: 0.000582, accuracy: 0.0278
case acc: 0.023334665
case acc: 0.033172384
case acc: 0.032860365
case acc: 0.02584115
case acc: 0.030941134
case acc: 0.020457245
top acc: 0.0394 ::: bot acc: 0.0086
top acc: 0.0594 ::: bot acc: 0.0102
top acc: 0.0631 ::: bot acc: 0.0125
top acc: 0.0456 ::: bot acc: 0.0110
top acc: 0.0520 ::: bot acc: 0.0108
top acc: 0.0344 ::: bot acc: 0.0120
current epoch: 40
train loss is 0.001137
average val loss: 0.000823, accuracy: 0.0335
average test loss: 0.000583, accuracy: 0.0278
case acc: 0.02306423
case acc: 0.03272869
case acc: 0.032929428
case acc: 0.026039686
case acc: 0.031427182
case acc: 0.020693066
top acc: 0.0393 ::: bot acc: 0.0084
top acc: 0.0586 ::: bot acc: 0.0101
top acc: 0.0634 ::: bot acc: 0.0125
top acc: 0.0459 ::: bot acc: 0.0112
top acc: 0.0524 ::: bot acc: 0.0111
top acc: 0.0347 ::: bot acc: 0.0123
current epoch: 41
train loss is 0.001119
average val loss: 0.000748, accuracy: 0.0315
average test loss: 0.000524, accuracy: 0.0260
case acc: 0.020636352
case acc: 0.029865952
case acc: 0.031732738
case acc: 0.02467466
case acc: 0.029830303
case acc: 0.019261774
top acc: 0.0363 ::: bot acc: 0.0068
top acc: 0.0553 ::: bot acc: 0.0083
top acc: 0.0607 ::: bot acc: 0.0141
top acc: 0.0436 ::: bot acc: 0.0114
top acc: 0.0504 ::: bot acc: 0.0105
top acc: 0.0324 ::: bot acc: 0.0126
current epoch: 42
train loss is 0.001088
average val loss: 0.000671, accuracy: 0.0294
average test loss: 0.000467, accuracy: 0.0242
case acc: 0.018020777
case acc: 0.02729505
case acc: 0.030438688
case acc: 0.023183776
case acc: 0.02831014
case acc: 0.018059565
top acc: 0.0329 ::: bot acc: 0.0058
top acc: 0.0516 ::: bot acc: 0.0078
top acc: 0.0579 ::: bot acc: 0.0161
top acc: 0.0411 ::: bot acc: 0.0122
top acc: 0.0481 ::: bot acc: 0.0108
top acc: 0.0299 ::: bot acc: 0.0139
current epoch: 43
train loss is 0.001063
average val loss: 0.000600, accuracy: 0.0274
average test loss: 0.000418, accuracy: 0.0227
case acc: 0.015847536
case acc: 0.025235087
case acc: 0.029304026
case acc: 0.022108266
case acc: 0.026607668
case acc: 0.017203268
top acc: 0.0298 ::: bot acc: 0.0058
top acc: 0.0481 ::: bot acc: 0.0087
top acc: 0.0545 ::: bot acc: 0.0190
top acc: 0.0388 ::: bot acc: 0.0134
top acc: 0.0454 ::: bot acc: 0.0109
top acc: 0.0277 ::: bot acc: 0.0159
current epoch: 44
train loss is 0.001044
average val loss: 0.000578, accuracy: 0.0269
average test loss: 0.000403, accuracy: 0.0223
case acc: 0.015187402
case acc: 0.024232337
case acc: 0.028994014
case acc: 0.021642722
case acc: 0.026481107
case acc: 0.017009698
top acc: 0.0283 ::: bot acc: 0.0065
top acc: 0.0462 ::: bot acc: 0.0094
top acc: 0.0534 ::: bot acc: 0.0203
top acc: 0.0379 ::: bot acc: 0.0137
top acc: 0.0452 ::: bot acc: 0.0112
top acc: 0.0270 ::: bot acc: 0.0162
current epoch: 45
train loss is 0.001028
average val loss: 0.000534, accuracy: 0.0257
average test loss: 0.000375, accuracy: 0.0214
case acc: 0.013999758
case acc: 0.022871148
case acc: 0.028159384
case acc: 0.021066034
case acc: 0.02560349
case acc: 0.016620621
top acc: 0.0258 ::: bot acc: 0.0080
top acc: 0.0435 ::: bot acc: 0.0108
top acc: 0.0510 ::: bot acc: 0.0225
top acc: 0.0365 ::: bot acc: 0.0150
top acc: 0.0435 ::: bot acc: 0.0117
top acc: 0.0259 ::: bot acc: 0.0175
current epoch: 46
train loss is 0.001012
average val loss: 0.000469, accuracy: 0.0237
average test loss: 0.000337, accuracy: 0.0201
case acc: 0.012525475
case acc: 0.021263227
case acc: 0.027216937
case acc: 0.019875135
case acc: 0.023904452
case acc: 0.016016921
top acc: 0.0219 ::: bot acc: 0.0110
top acc: 0.0390 ::: bot acc: 0.0145
top acc: 0.0471 ::: bot acc: 0.0265
top acc: 0.0331 ::: bot acc: 0.0177
top acc: 0.0402 ::: bot acc: 0.0131
top acc: 0.0232 ::: bot acc: 0.0204
current epoch: 47
train loss is 0.000991
average val loss: 0.000447, accuracy: 0.0230
average test loss: 0.000323, accuracy: 0.0198
case acc: 0.012017837
case acc: 0.020736743
case acc: 0.027012004
case acc: 0.019542048
case acc: 0.023385774
case acc: 0.015836043
top acc: 0.0198 ::: bot acc: 0.0130
top acc: 0.0368 ::: bot acc: 0.0168
top acc: 0.0449 ::: bot acc: 0.0287
top acc: 0.0318 ::: bot acc: 0.0189
top acc: 0.0390 ::: bot acc: 0.0137
top acc: 0.0221 ::: bot acc: 0.0212
current epoch: 48
train loss is 0.000985
average val loss: 0.000413, accuracy: 0.0220
average test loss: 0.000312, accuracy: 0.0195
case acc: 0.011843728
case acc: 0.020324973
case acc: 0.026890188
case acc: 0.019230895
case acc: 0.022781117
case acc: 0.015818182
top acc: 0.0170 ::: bot acc: 0.0157
top acc: 0.0332 ::: bot acc: 0.0206
top acc: 0.0417 ::: bot acc: 0.0321
top acc: 0.0299 ::: bot acc: 0.0213
top acc: 0.0377 ::: bot acc: 0.0148
top acc: 0.0206 ::: bot acc: 0.0229
current epoch: 49
train loss is 0.000981
average val loss: 0.000380, accuracy: 0.0210
average test loss: 0.000302, accuracy: 0.0193
case acc: 0.012424772
case acc: 0.019996058
case acc: 0.027090577
case acc: 0.018569374
case acc: 0.021725085
case acc: 0.015756441
top acc: 0.0135 ::: bot acc: 0.0195
top acc: 0.0288 ::: bot acc: 0.0249
top acc: 0.0375 ::: bot acc: 0.0363
top acc: 0.0267 ::: bot acc: 0.0239
top acc: 0.0356 ::: bot acc: 0.0162
top acc: 0.0187 ::: bot acc: 0.0246
current epoch: 50
train loss is 0.000975
average val loss: 0.000350, accuracy: 0.0201
average test loss: 0.000310, accuracy: 0.0197
case acc: 0.014289653
case acc: 0.020628791
case acc: 0.027840473
case acc: 0.018564554
case acc: 0.020402271
case acc: 0.016274843
top acc: 0.0092 ::: bot acc: 0.0246
top acc: 0.0230 ::: bot acc: 0.0309
top acc: 0.0314 ::: bot acc: 0.0423
top acc: 0.0224 ::: bot acc: 0.0283
top acc: 0.0318 ::: bot acc: 0.0194
top acc: 0.0153 ::: bot acc: 0.0281
LME_Co_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6798 6798 6798
1.8562728 -0.6288155 0.21141115 -0.19947179
Validation: 756 756 756
Testing: 744 744 744
pre-processing time: 0.00021886825561523438
the split date is 2010-01-01
train dropout: 0.6 test dropout: 0.1
net initializing with time: 0.0023212432861328125
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.013177
average val loss: 0.005783, accuracy: 0.1015
average test loss: 0.006137, accuracy: 0.1029
case acc: 0.14189996
case acc: 0.08225473
case acc: 0.10388595
case acc: 0.102273196
case acc: 0.12795812
case acc: 0.059245013
top acc: 0.1095 ::: bot acc: 0.1751
top acc: 0.1084 ::: bot acc: 0.0561
top acc: 0.0593 ::: bot acc: 0.1488
top acc: 0.0724 ::: bot acc: 0.1327
top acc: 0.0958 ::: bot acc: 0.1584
top acc: 0.0269 ::: bot acc: 0.0915
current epoch: 2
train loss is 0.008565
average val loss: 0.002958, accuracy: 0.0537
average test loss: 0.003163, accuracy: 0.0575
case acc: 0.043530364
case acc: 0.16694064
case acc: 0.033512447
case acc: 0.023398463
case acc: 0.042117212
case acc: 0.035600293
top acc: 0.0191 ::: bot acc: 0.0718
top acc: 0.1920 ::: bot acc: 0.1412
top acc: 0.0421 ::: bot acc: 0.0471
top acc: 0.0228 ::: bot acc: 0.0388
top acc: 0.0152 ::: bot acc: 0.0688
top acc: 0.0681 ::: bot acc: 0.0142
current epoch: 3
train loss is 0.008269
average val loss: 0.007582, accuracy: 0.1012
average test loss: 0.007715, accuracy: 0.1007
case acc: 0.05253919
case acc: 0.24290314
case acc: 0.08829087
case acc: 0.0754697
case acc: 0.042709753
case acc: 0.10252517
top acc: 0.0835 ::: bot acc: 0.0233
top acc: 0.2678 ::: bot acc: 0.2176
top acc: 0.1318 ::: bot acc: 0.0441
top acc: 0.1056 ::: bot acc: 0.0455
top acc: 0.0728 ::: bot acc: 0.0162
top acc: 0.1425 ::: bot acc: 0.0667
current epoch: 4
train loss is 0.010474
average val loss: 0.016151, accuracy: 0.1673
average test loss: 0.016169, accuracy: 0.1661
case acc: 0.12434299
case acc: 0.3020631
case acc: 0.15988581
case acc: 0.14003234
case acc: 0.109295204
case acc: 0.1610401
top acc: 0.1567 ::: bot acc: 0.0926
top acc: 0.3267 ::: bot acc: 0.2771
top acc: 0.2044 ::: bot acc: 0.1144
top acc: 0.1711 ::: bot acc: 0.1087
top acc: 0.1411 ::: bot acc: 0.0802
top acc: 0.2010 ::: bot acc: 0.1254
current epoch: 5
train loss is 0.013258
average val loss: 0.013626, accuracy: 0.1533
average test loss: 0.013670, accuracy: 0.1521
case acc: 0.115889944
case acc: 0.27800235
case acc: 0.14890532
case acc: 0.125081
case acc: 0.1029553
case acc: 0.14184104
top acc: 0.1478 ::: bot acc: 0.0846
top acc: 0.3030 ::: bot acc: 0.2525
top acc: 0.1931 ::: bot acc: 0.1039
top acc: 0.1560 ::: bot acc: 0.0934
top acc: 0.1350 ::: bot acc: 0.0736
top acc: 0.1817 ::: bot acc: 0.1058
current epoch: 6
train loss is 0.013347
average val loss: 0.003192, accuracy: 0.0581
average test loss: 0.003341, accuracy: 0.0597
case acc: 0.030288067
case acc: 0.16762933
case acc: 0.054539703
case acc: 0.033348784
case acc: 0.02580321
case acc: 0.046800293
top acc: 0.0552 ::: bot acc: 0.0140
top acc: 0.1922 ::: bot acc: 0.1431
top acc: 0.0936 ::: bot acc: 0.0194
top acc: 0.0586 ::: bot acc: 0.0134
top acc: 0.0492 ::: bot acc: 0.0133
top acc: 0.0845 ::: bot acc: 0.0158
current epoch: 7
train loss is 0.007593
average val loss: 0.001306, accuracy: 0.0389
average test loss: 0.001529, accuracy: 0.0444
case acc: 0.03741271
case acc: 0.09691542
case acc: 0.03414074
case acc: 0.03512524
case acc: 0.03299407
case acc: 0.029883256
top acc: 0.0175 ::: bot acc: 0.0632
top acc: 0.1219 ::: bot acc: 0.0718
top acc: 0.0323 ::: bot acc: 0.0576
top acc: 0.0158 ::: bot acc: 0.0609
top acc: 0.0137 ::: bot acc: 0.0563
top acc: 0.0307 ::: bot acc: 0.0454
current epoch: 8
train loss is 0.003705
average val loss: 0.001115, accuracy: 0.0341
average test loss: 0.001326, accuracy: 0.0398
case acc: 0.031233843
case acc: 0.093330696
case acc: 0.033572163
case acc: 0.029185072
case acc: 0.024502264
case acc: 0.02695435
top acc: 0.0185 ::: bot acc: 0.0530
top acc: 0.1183 ::: bot acc: 0.0683
top acc: 0.0358 ::: bot acc: 0.0535
top acc: 0.0147 ::: bot acc: 0.0527
top acc: 0.0214 ::: bot acc: 0.0399
top acc: 0.0403 ::: bot acc: 0.0354
current epoch: 9
train loss is 0.002738
average val loss: 0.001207, accuracy: 0.0342
average test loss: 0.001403, accuracy: 0.0390
case acc: 0.023103513
case acc: 0.10091952
case acc: 0.03367037
case acc: 0.022353733
case acc: 0.023647862
case acc: 0.03023549
top acc: 0.0301 ::: bot acc: 0.0346
top acc: 0.1257 ::: bot acc: 0.0759
top acc: 0.0496 ::: bot acc: 0.0401
top acc: 0.0267 ::: bot acc: 0.0351
top acc: 0.0435 ::: bot acc: 0.0182
top acc: 0.0587 ::: bot acc: 0.0177
current epoch: 10
train loss is 0.002740
average val loss: 0.001152, accuracy: 0.0342
average test loss: 0.001353, accuracy: 0.0389
case acc: 0.022949085
case acc: 0.09603916
case acc: 0.033884067
case acc: 0.022670908
case acc: 0.025978174
case acc: 0.032014873
top acc: 0.0341 ::: bot acc: 0.0299
top acc: 0.1212 ::: bot acc: 0.0710
top acc: 0.0505 ::: bot acc: 0.0390
top acc: 0.0303 ::: bot acc: 0.0330
top acc: 0.0498 ::: bot acc: 0.0133
top acc: 0.0618 ::: bot acc: 0.0162
current epoch: 11
train loss is 0.002521
average val loss: 0.000944, accuracy: 0.0316
average test loss: 0.001146, accuracy: 0.0363
case acc: 0.02314821
case acc: 0.08374591
case acc: 0.033329267
case acc: 0.022753585
case acc: 0.025067821
case acc: 0.029975945
top acc: 0.0314 ::: bot acc: 0.0334
top acc: 0.1086 ::: bot acc: 0.0587
top acc: 0.0441 ::: bot acc: 0.0453
top acc: 0.0258 ::: bot acc: 0.0368
top acc: 0.0478 ::: bot acc: 0.0143
top acc: 0.0580 ::: bot acc: 0.0184
current epoch: 12
train loss is 0.002216
average val loss: 0.000858, accuracy: 0.0306
average test loss: 0.001062, accuracy: 0.0354
case acc: 0.02315749
case acc: 0.07670422
case acc: 0.033225495
case acc: 0.022709029
case acc: 0.026247876
case acc: 0.030269265
top acc: 0.0331 ::: bot acc: 0.0323
top acc: 0.1017 ::: bot acc: 0.0513
top acc: 0.0422 ::: bot acc: 0.0466
top acc: 0.0258 ::: bot acc: 0.0366
top acc: 0.0506 ::: bot acc: 0.0126
top acc: 0.0586 ::: bot acc: 0.0180
current epoch: 13
train loss is 0.001980
average val loss: 0.000768, accuracy: 0.0295
average test loss: 0.000976, accuracy: 0.0342
case acc: 0.022882981
case acc: 0.06947451
case acc: 0.03343137
case acc: 0.022724474
case acc: 0.0265174
case acc: 0.030110063
top acc: 0.0332 ::: bot acc: 0.0310
top acc: 0.0941 ::: bot acc: 0.0446
top acc: 0.0407 ::: bot acc: 0.0492
top acc: 0.0250 ::: bot acc: 0.0374
top acc: 0.0504 ::: bot acc: 0.0131
top acc: 0.0580 ::: bot acc: 0.0187
current epoch: 14
train loss is 0.001737
average val loss: 0.000709, accuracy: 0.0286
average test loss: 0.000919, accuracy: 0.0334
case acc: 0.02294921
case acc: 0.06399498
case acc: 0.03339745
case acc: 0.022545306
case acc: 0.0270239
case acc: 0.030513603
top acc: 0.0348 ::: bot acc: 0.0294
top acc: 0.0892 ::: bot acc: 0.0389
top acc: 0.0407 ::: bot acc: 0.0489
top acc: 0.0262 ::: bot acc: 0.0358
top acc: 0.0516 ::: bot acc: 0.0128
top acc: 0.0590 ::: bot acc: 0.0181
current epoch: 15
train loss is 0.001619
average val loss: 0.000655, accuracy: 0.0278
average test loss: 0.000869, accuracy: 0.0326
case acc: 0.023277603
case acc: 0.059104588
case acc: 0.03356098
case acc: 0.022489524
case acc: 0.02674613
case acc: 0.030248648
top acc: 0.0361 ::: bot acc: 0.0287
top acc: 0.0839 ::: bot acc: 0.0340
top acc: 0.0404 ::: bot acc: 0.0494
top acc: 0.0266 ::: bot acc: 0.0357
top acc: 0.0511 ::: bot acc: 0.0128
top acc: 0.0588 ::: bot acc: 0.0178
current epoch: 16
train loss is 0.001569
average val loss: 0.000650, accuracy: 0.0281
average test loss: 0.000862, accuracy: 0.0326
case acc: 0.023759406
case acc: 0.056925483
case acc: 0.033448443
case acc: 0.022365533
case acc: 0.02776158
case acc: 0.031313606
top acc: 0.0394 ::: bot acc: 0.0253
top acc: 0.0818 ::: bot acc: 0.0320
top acc: 0.0425 ::: bot acc: 0.0471
top acc: 0.0295 ::: bot acc: 0.0330
top acc: 0.0529 ::: bot acc: 0.0120
top acc: 0.0610 ::: bot acc: 0.0163
current epoch: 17
train loss is 0.001459
average val loss: 0.000688, accuracy: 0.0293
average test loss: 0.000891, accuracy: 0.0334
case acc: 0.024975551
case acc: 0.05695301
case acc: 0.033399064
case acc: 0.022599498
case acc: 0.02972023
case acc: 0.032887053
top acc: 0.0442 ::: bot acc: 0.0201
top acc: 0.0817 ::: bot acc: 0.0320
top acc: 0.0467 ::: bot acc: 0.0425
top acc: 0.0339 ::: bot acc: 0.0286
top acc: 0.0565 ::: bot acc: 0.0107
top acc: 0.0639 ::: bot acc: 0.0149
current epoch: 18
train loss is 0.001415
average val loss: 0.000629, accuracy: 0.0280
average test loss: 0.000835, accuracy: 0.0323
case acc: 0.025157293
case acc: 0.05231719
case acc: 0.03345485
case acc: 0.022477569
case acc: 0.028474202
case acc: 0.03177758
top acc: 0.0443 ::: bot acc: 0.0206
top acc: 0.0773 ::: bot acc: 0.0274
top acc: 0.0462 ::: bot acc: 0.0432
top acc: 0.0326 ::: bot acc: 0.0299
top acc: 0.0542 ::: bot acc: 0.0115
top acc: 0.0620 ::: bot acc: 0.0156
current epoch: 19
train loss is 0.001350
average val loss: 0.000579, accuracy: 0.0270
average test loss: 0.000793, accuracy: 0.0314
case acc: 0.025107063
case acc: 0.048537724
case acc: 0.03360926
case acc: 0.022518167
case acc: 0.027495746
case acc: 0.031136433
top acc: 0.0443 ::: bot acc: 0.0204
top acc: 0.0734 ::: bot acc: 0.0240
top acc: 0.0460 ::: bot acc: 0.0437
top acc: 0.0324 ::: bot acc: 0.0302
top acc: 0.0528 ::: bot acc: 0.0116
top acc: 0.0607 ::: bot acc: 0.0164
current epoch: 20
train loss is 0.001317
average val loss: 0.000602, accuracy: 0.0277
average test loss: 0.000816, accuracy: 0.0320
case acc: 0.026491955
case acc: 0.048403505
case acc: 0.033689737
case acc: 0.022789184
case acc: 0.028449442
case acc: 0.03232322
top acc: 0.0477 ::: bot acc: 0.0180
top acc: 0.0730 ::: bot acc: 0.0239
top acc: 0.0490 ::: bot acc: 0.0405
top acc: 0.0351 ::: bot acc: 0.0273
top acc: 0.0545 ::: bot acc: 0.0115
top acc: 0.0630 ::: bot acc: 0.0158
current epoch: 21
train loss is 0.001297
average val loss: 0.000574, accuracy: 0.0272
average test loss: 0.000786, accuracy: 0.0314
case acc: 0.026804604
case acc: 0.045496862
case acc: 0.033850197
case acc: 0.02295576
case acc: 0.027526949
case acc: 0.031766728
top acc: 0.0484 ::: bot acc: 0.0171
top acc: 0.0700 ::: bot acc: 0.0214
top acc: 0.0496 ::: bot acc: 0.0399
top acc: 0.0356 ::: bot acc: 0.0273
top acc: 0.0528 ::: bot acc: 0.0116
top acc: 0.0622 ::: bot acc: 0.0158
current epoch: 22
train loss is 0.001274
average val loss: 0.000557, accuracy: 0.0268
average test loss: 0.000769, accuracy: 0.0311
case acc: 0.027130947
case acc: 0.043635156
case acc: 0.033925947
case acc: 0.022984155
case acc: 0.027202409
case acc: 0.031453487
top acc: 0.0491 ::: bot acc: 0.0165
top acc: 0.0680 ::: bot acc: 0.0200
top acc: 0.0510 ::: bot acc: 0.0391
top acc: 0.0356 ::: bot acc: 0.0269
top acc: 0.0521 ::: bot acc: 0.0120
top acc: 0.0614 ::: bot acc: 0.0162
current epoch: 23
train loss is 0.001227
average val loss: 0.000595, accuracy: 0.0280
average test loss: 0.000803, accuracy: 0.0319
case acc: 0.028819423
case acc: 0.04430148
case acc: 0.03429754
case acc: 0.023604516
case acc: 0.028342767
case acc: 0.03215751
top acc: 0.0523 ::: bot acc: 0.0148
top acc: 0.0689 ::: bot acc: 0.0205
top acc: 0.0542 ::: bot acc: 0.0354
top acc: 0.0386 ::: bot acc: 0.0242
top acc: 0.0542 ::: bot acc: 0.0113
top acc: 0.0626 ::: bot acc: 0.0155
current epoch: 24
train loss is 0.001244
average val loss: 0.000597, accuracy: 0.0282
average test loss: 0.000805, accuracy: 0.0320
case acc: 0.029689074
case acc: 0.04356755
case acc: 0.034559257
case acc: 0.023865659
case acc: 0.028086485
case acc: 0.03205108
top acc: 0.0540 ::: bot acc: 0.0143
top acc: 0.0680 ::: bot acc: 0.0199
top acc: 0.0563 ::: bot acc: 0.0332
top acc: 0.0397 ::: bot acc: 0.0234
top acc: 0.0534 ::: bot acc: 0.0116
top acc: 0.0627 ::: bot acc: 0.0157
current epoch: 25
train loss is 0.001220
average val loss: 0.000649, accuracy: 0.0297
average test loss: 0.000857, accuracy: 0.0332
case acc: 0.031762294
case acc: 0.045053996
case acc: 0.035653286
case acc: 0.024732865
case acc: 0.029149521
case acc: 0.032870956
top acc: 0.0573 ::: bot acc: 0.0139
top acc: 0.0697 ::: bot acc: 0.0211
top acc: 0.0604 ::: bot acc: 0.0295
top acc: 0.0424 ::: bot acc: 0.0206
top acc: 0.0557 ::: bot acc: 0.0109
top acc: 0.0639 ::: bot acc: 0.0151
current epoch: 26
train loss is 0.001246
average val loss: 0.000781, accuracy: 0.0333
average test loss: 0.000981, accuracy: 0.0360
case acc: 0.035571806
case acc: 0.04880391
case acc: 0.03754565
case acc: 0.026825635
case acc: 0.031795237
case acc: 0.035333727
top acc: 0.0632 ::: bot acc: 0.0138
top acc: 0.0735 ::: bot acc: 0.0243
top acc: 0.0669 ::: bot acc: 0.0230
top acc: 0.0476 ::: bot acc: 0.0160
top acc: 0.0594 ::: bot acc: 0.0111
top acc: 0.0681 ::: bot acc: 0.0141
current epoch: 27
train loss is 0.001248
average val loss: 0.000793, accuracy: 0.0336
average test loss: 0.000993, accuracy: 0.0362
case acc: 0.036296643
case acc: 0.04870945
case acc: 0.038338583
case acc: 0.027234707
case acc: 0.031990513
case acc: 0.034892183
top acc: 0.0643 ::: bot acc: 0.0139
top acc: 0.0734 ::: bot acc: 0.0241
top acc: 0.0687 ::: bot acc: 0.0215
top acc: 0.0485 ::: bot acc: 0.0156
top acc: 0.0595 ::: bot acc: 0.0112
top acc: 0.0673 ::: bot acc: 0.0142
current epoch: 28
train loss is 0.001285
average val loss: 0.000876, accuracy: 0.0357
average test loss: 0.001077, accuracy: 0.0380
case acc: 0.03858539
case acc: 0.050622
case acc: 0.040261455
case acc: 0.028592112
case acc: 0.033943128
case acc: 0.035964146
top acc: 0.0673 ::: bot acc: 0.0146
top acc: 0.0753 ::: bot acc: 0.0258
top acc: 0.0727 ::: bot acc: 0.0191
top acc: 0.0514 ::: bot acc: 0.0139
top acc: 0.0629 ::: bot acc: 0.0114
top acc: 0.0693 ::: bot acc: 0.0138
current epoch: 29
train loss is 0.001282
average val loss: 0.000975, accuracy: 0.0381
average test loss: 0.001164, accuracy: 0.0399
case acc: 0.041109215
case acc: 0.052235547
case acc: 0.042332556
case acc: 0.030349799
case acc: 0.03626892
case acc: 0.036965746
top acc: 0.0707 ::: bot acc: 0.0157
top acc: 0.0771 ::: bot acc: 0.0277
top acc: 0.0766 ::: bot acc: 0.0176
top acc: 0.0542 ::: bot acc: 0.0134
top acc: 0.0655 ::: bot acc: 0.0123
top acc: 0.0709 ::: bot acc: 0.0132
current epoch: 30
train loss is 0.001306
average val loss: 0.000960, accuracy: 0.0378
average test loss: 0.001157, accuracy: 0.0397
case acc: 0.041146155
case acc: 0.051113047
case acc: 0.04275813
case acc: 0.030377455
case acc: 0.036436204
case acc: 0.036581606
top acc: 0.0711 ::: bot acc: 0.0154
top acc: 0.0759 ::: bot acc: 0.0262
top acc: 0.0772 ::: bot acc: 0.0176
top acc: 0.0542 ::: bot acc: 0.0137
top acc: 0.0655 ::: bot acc: 0.0126
top acc: 0.0704 ::: bot acc: 0.0134
current epoch: 31
train loss is 0.001294
average val loss: 0.001019, accuracy: 0.0392
average test loss: 0.001214, accuracy: 0.0409
case acc: 0.042428184
case acc: 0.051878728
case acc: 0.04437069
case acc: 0.031454686
case acc: 0.03829043
case acc: 0.03720922
top acc: 0.0722 ::: bot acc: 0.0163
top acc: 0.0768 ::: bot acc: 0.0269
top acc: 0.0798 ::: bot acc: 0.0172
top acc: 0.0559 ::: bot acc: 0.0135
top acc: 0.0682 ::: bot acc: 0.0133
top acc: 0.0713 ::: bot acc: 0.0135
current epoch: 32
train loss is 0.001290
average val loss: 0.001002, accuracy: 0.0388
average test loss: 0.001194, accuracy: 0.0406
case acc: 0.04229114
case acc: 0.0501607
case acc: 0.044209477
case acc: 0.031395264
case acc: 0.038541697
case acc: 0.036893975
top acc: 0.0720 ::: bot acc: 0.0162
top acc: 0.0750 ::: bot acc: 0.0253
top acc: 0.0795 ::: bot acc: 0.0170
top acc: 0.0556 ::: bot acc: 0.0135
top acc: 0.0683 ::: bot acc: 0.0137
top acc: 0.0706 ::: bot acc: 0.0139
current epoch: 33
train loss is 0.001283
average val loss: 0.000965, accuracy: 0.0380
average test loss: 0.001161, accuracy: 0.0399
case acc: 0.041442543
case acc: 0.048365217
case acc: 0.04391473
case acc: 0.031045232
case acc: 0.0383382
case acc: 0.036327597
top acc: 0.0711 ::: bot acc: 0.0157
top acc: 0.0733 ::: bot acc: 0.0239
top acc: 0.0791 ::: bot acc: 0.0173
top acc: 0.0552 ::: bot acc: 0.0135
top acc: 0.0682 ::: bot acc: 0.0135
top acc: 0.0697 ::: bot acc: 0.0139
current epoch: 34
train loss is 0.001228
average val loss: 0.000840, accuracy: 0.0350
average test loss: 0.001036, accuracy: 0.0372
case acc: 0.038294736
case acc: 0.043621343
case acc: 0.041979317
case acc: 0.029278843
case acc: 0.03588229
case acc: 0.034348685
top acc: 0.0671 ::: bot acc: 0.0145
top acc: 0.0681 ::: bot acc: 0.0197
top acc: 0.0758 ::: bot acc: 0.0179
top acc: 0.0523 ::: bot acc: 0.0140
top acc: 0.0650 ::: bot acc: 0.0123
top acc: 0.0666 ::: bot acc: 0.0145
current epoch: 35
train loss is 0.001173
average val loss: 0.000794, accuracy: 0.0339
average test loss: 0.000992, accuracy: 0.0362
case acc: 0.037194032
case acc: 0.04119125
case acc: 0.041132715
case acc: 0.028674755
case acc: 0.035333484
case acc: 0.0339269
top acc: 0.0656 ::: bot acc: 0.0143
top acc: 0.0655 ::: bot acc: 0.0178
top acc: 0.0745 ::: bot acc: 0.0182
top acc: 0.0515 ::: bot acc: 0.0142
top acc: 0.0644 ::: bot acc: 0.0119
top acc: 0.0656 ::: bot acc: 0.0148
current epoch: 36
train loss is 0.001151
average val loss: 0.000717, accuracy: 0.0319
average test loss: 0.000919, accuracy: 0.0346
case acc: 0.03515585
case acc: 0.037841767
case acc: 0.039805934
case acc: 0.027803138
case acc: 0.034022424
case acc: 0.03299597
top acc: 0.0626 ::: bot acc: 0.0139
top acc: 0.0616 ::: bot acc: 0.0151
top acc: 0.0720 ::: bot acc: 0.0193
top acc: 0.0496 ::: bot acc: 0.0150
top acc: 0.0624 ::: bot acc: 0.0115
top acc: 0.0643 ::: bot acc: 0.0152
current epoch: 37
train loss is 0.001089
average val loss: 0.000622, accuracy: 0.0292
average test loss: 0.000828, accuracy: 0.0324
case acc: 0.032681365
case acc: 0.033355232
case acc: 0.038003836
case acc: 0.026483648
case acc: 0.032369584
case acc: 0.03166256
top acc: 0.0587 ::: bot acc: 0.0138
top acc: 0.0568 ::: bot acc: 0.0116
top acc: 0.0677 ::: bot acc: 0.0221
top acc: 0.0467 ::: bot acc: 0.0170
top acc: 0.0603 ::: bot acc: 0.0112
top acc: 0.0615 ::: bot acc: 0.0163
current epoch: 38
train loss is 0.001035
average val loss: 0.000512, accuracy: 0.0259
average test loss: 0.000722, accuracy: 0.0297
case acc: 0.029434968
case acc: 0.028418664
case acc: 0.03632263
case acc: 0.02471737
case acc: 0.029675176
case acc: 0.029733893
top acc: 0.0537 ::: bot acc: 0.0145
top acc: 0.0506 ::: bot acc: 0.0090
top acc: 0.0628 ::: bot acc: 0.0270
top acc: 0.0422 ::: bot acc: 0.0207
top acc: 0.0566 ::: bot acc: 0.0109
top acc: 0.0575 ::: bot acc: 0.0186
current epoch: 39
train loss is 0.000999
average val loss: 0.000458, accuracy: 0.0242
average test loss: 0.000668, accuracy: 0.0284
case acc: 0.027753422
case acc: 0.02572
case acc: 0.035459086
case acc: 0.0242446
case acc: 0.028458508
case acc: 0.028724687
top acc: 0.0507 ::: bot acc: 0.0157
top acc: 0.0467 ::: bot acc: 0.0090
top acc: 0.0595 ::: bot acc: 0.0302
top acc: 0.0403 ::: bot acc: 0.0232
top acc: 0.0545 ::: bot acc: 0.0111
top acc: 0.0551 ::: bot acc: 0.0202
current epoch: 40
train loss is 0.000962
average val loss: 0.000396, accuracy: 0.0222
average test loss: 0.000609, accuracy: 0.0268
case acc: 0.025804657
case acc: 0.022756057
case acc: 0.034513216
case acc: 0.023236433
case acc: 0.026578182
case acc: 0.02775339
top acc: 0.0463 ::: bot acc: 0.0187
top acc: 0.0415 ::: bot acc: 0.0105
top acc: 0.0555 ::: bot acc: 0.0346
top acc: 0.0366 ::: bot acc: 0.0262
top acc: 0.0510 ::: bot acc: 0.0125
top acc: 0.0522 ::: bot acc: 0.0234
current epoch: 41
train loss is 0.000935
average val loss: 0.000347, accuracy: 0.0206
average test loss: 0.000567, accuracy: 0.0256
case acc: 0.024410138
case acc: 0.020475058
case acc: 0.033894196
case acc: 0.022814866
case acc: 0.024994694
case acc: 0.026893742
top acc: 0.0420 ::: bot acc: 0.0228
top acc: 0.0363 ::: bot acc: 0.0137
top acc: 0.0507 ::: bot acc: 0.0392
top acc: 0.0332 ::: bot acc: 0.0300
top acc: 0.0478 ::: bot acc: 0.0145
top acc: 0.0493 ::: bot acc: 0.0263
current epoch: 42
train loss is 0.000909
average val loss: 0.000306, accuracy: 0.0193
average test loss: 0.000530, accuracy: 0.0246
case acc: 0.023460252
case acc: 0.018563727
case acc: 0.033404674
case acc: 0.022658395
case acc: 0.023083312
case acc: 0.026537906
top acc: 0.0362 ::: bot acc: 0.0288
top acc: 0.0299 ::: bot acc: 0.0202
top acc: 0.0445 ::: bot acc: 0.0451
top acc: 0.0282 ::: bot acc: 0.0350
top acc: 0.0423 ::: bot acc: 0.0190
top acc: 0.0446 ::: bot acc: 0.0309
current epoch: 43
train loss is 0.000898
average val loss: 0.000297, accuracy: 0.0190
average test loss: 0.000527, accuracy: 0.0245
case acc: 0.0231982
case acc: 0.017993448
case acc: 0.03341015
case acc: 0.022976534
case acc: 0.022600269
case acc: 0.026676323
top acc: 0.0324 ::: bot acc: 0.0327
top acc: 0.0258 ::: bot acc: 0.0242
top acc: 0.0408 ::: bot acc: 0.0491
top acc: 0.0252 ::: bot acc: 0.0376
top acc: 0.0395 ::: bot acc: 0.0222
top acc: 0.0426 ::: bot acc: 0.0331
current epoch: 44
train loss is 0.000898
average val loss: 0.000300, accuracy: 0.0192
average test loss: 0.000530, accuracy: 0.0246
case acc: 0.023321042
case acc: 0.018055838
case acc: 0.033613615
case acc: 0.023634076
case acc: 0.022060899
case acc: 0.026868995
top acc: 0.0294 ::: bot acc: 0.0357
top acc: 0.0219 ::: bot acc: 0.0279
top acc: 0.0368 ::: bot acc: 0.0530
top acc: 0.0232 ::: bot acc: 0.0399
top acc: 0.0373 ::: bot acc: 0.0240
top acc: 0.0410 ::: bot acc: 0.0347
current epoch: 45
train loss is 0.000894
average val loss: 0.000318, accuracy: 0.0199
average test loss: 0.000552, accuracy: 0.0253
case acc: 0.024410574
case acc: 0.019251328
case acc: 0.03422786
case acc: 0.024741774
case acc: 0.021976732
case acc: 0.027383503
top acc: 0.0252 ::: bot acc: 0.0399
top acc: 0.0169 ::: bot acc: 0.0332
top acc: 0.0315 ::: bot acc: 0.0581
top acc: 0.0193 ::: bot acc: 0.0437
top acc: 0.0342 ::: bot acc: 0.0275
top acc: 0.0383 ::: bot acc: 0.0373
current epoch: 46
train loss is 0.000904
average val loss: 0.000343, accuracy: 0.0206
average test loss: 0.000577, accuracy: 0.0262
case acc: 0.025695477
case acc: 0.021120917
case acc: 0.0351358
case acc: 0.025544452
case acc: 0.02184206
case acc: 0.027693931
top acc: 0.0227 ::: bot acc: 0.0430
top acc: 0.0142 ::: bot acc: 0.0373
top acc: 0.0277 ::: bot acc: 0.0618
top acc: 0.0173 ::: bot acc: 0.0459
top acc: 0.0319 ::: bot acc: 0.0293
top acc: 0.0371 ::: bot acc: 0.0387
current epoch: 47
train loss is 0.000922
average val loss: 0.000411, accuracy: 0.0227
average test loss: 0.000652, accuracy: 0.0285
case acc: 0.028653972
case acc: 0.024945227
case acc: 0.0375171
case acc: 0.028187132
case acc: 0.02272393
case acc: 0.028817806
top acc: 0.0202 ::: bot acc: 0.0487
top acc: 0.0126 ::: bot acc: 0.0439
top acc: 0.0213 ::: bot acc: 0.0690
top acc: 0.0144 ::: bot acc: 0.0514
top acc: 0.0275 ::: bot acc: 0.0337
top acc: 0.0329 ::: bot acc: 0.0427
current epoch: 48
train loss is 0.000955
average val loss: 0.000506, accuracy: 0.0255
average test loss: 0.000750, accuracy: 0.0313
case acc: 0.031840753
case acc: 0.029574838
case acc: 0.040788338
case acc: 0.031404585
case acc: 0.02387584
case acc: 0.0301056
top acc: 0.0186 ::: bot acc: 0.0543
top acc: 0.0134 ::: bot acc: 0.0503
top acc: 0.0172 ::: bot acc: 0.0756
top acc: 0.0139 ::: bot acc: 0.0563
top acc: 0.0238 ::: bot acc: 0.0374
top acc: 0.0298 ::: bot acc: 0.0461
current epoch: 49
train loss is 0.001006
average val loss: 0.000666, accuracy: 0.0298
average test loss: 0.000919, accuracy: 0.0353
case acc: 0.03610676
case acc: 0.03599885
case acc: 0.04566955
case acc: 0.036110926
case acc: 0.025831364
case acc: 0.031976588
top acc: 0.0178 ::: bot acc: 0.0612
top acc: 0.0161 ::: bot acc: 0.0586
top acc: 0.0142 ::: bot acc: 0.0845
top acc: 0.0157 ::: bot acc: 0.0626
top acc: 0.0191 ::: bot acc: 0.0427
top acc: 0.0262 ::: bot acc: 0.0506
current epoch: 50
train loss is 0.001080
average val loss: 0.000886, accuracy: 0.0352
average test loss: 0.001150, accuracy: 0.0402
case acc: 0.04111073
case acc: 0.043767642
case acc: 0.05220439
case acc: 0.041414067
case acc: 0.028510615
case acc: 0.03399133
top acc: 0.0184 ::: bot acc: 0.0683
top acc: 0.0216 ::: bot acc: 0.0675
top acc: 0.0150 ::: bot acc: 0.0940
top acc: 0.0180 ::: bot acc: 0.0695
top acc: 0.0161 ::: bot acc: 0.0482
top acc: 0.0236 ::: bot acc: 0.0551
LME_Co_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6768 6768 6768
1.7082474 -0.6288155 0.21141115 -0.19947179
Validation: 756 756 756
Testing: 774 774 774
pre-processing time: 0.00025153160095214844
the split date is 2010-07-01
train dropout: 0.6 test dropout: 0.1
net initializing with time: 0.002946615219116211
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012875
average val loss: 0.006325, accuracy: 0.1044
average test loss: 0.006553, accuracy: 0.1052
case acc: 0.15022615
case acc: 0.07672301
case acc: 0.110234566
case acc: 0.093579635
case acc: 0.13904144
case acc: 0.061539464
top acc: 0.1216 ::: bot acc: 0.1783
top acc: 0.1038 ::: bot acc: 0.0489
top acc: 0.0716 ::: bot acc: 0.1490
top acc: 0.0520 ::: bot acc: 0.1337
top acc: 0.0971 ::: bot acc: 0.1738
top acc: 0.0288 ::: bot acc: 0.0931
current epoch: 2
train loss is 0.008317
average val loss: 0.003029, accuracy: 0.0577
average test loss: 0.003154, accuracy: 0.0601
case acc: 0.051820055
case acc: 0.15821049
case acc: 0.030000811
case acc: 0.030274875
case acc: 0.059031524
case acc: 0.031090492
top acc: 0.0254 ::: bot acc: 0.0787
top acc: 0.1849 ::: bot acc: 0.1303
top acc: 0.0273 ::: bot acc: 0.0500
top acc: 0.0409 ::: bot acc: 0.0423
top acc: 0.0281 ::: bot acc: 0.0882
top acc: 0.0611 ::: bot acc: 0.0131
current epoch: 3
train loss is 0.008617
average val loss: 0.006760, accuracy: 0.0917
average test loss: 0.006836, accuracy: 0.0915
case acc: 0.037952393
case acc: 0.2310193
case acc: 0.07618902
case acc: 0.07841253
case acc: 0.031698544
case acc: 0.09391451
top acc: 0.0643 ::: bot acc: 0.0134
top acc: 0.2586 ::: bot acc: 0.2025
top acc: 0.1142 ::: bot acc: 0.0391
top acc: 0.1208 ::: bot acc: 0.0396
top acc: 0.0660 ::: bot acc: 0.0129
top acc: 0.1325 ::: bot acc: 0.0596
current epoch: 4
train loss is 0.009942
average val loss: 0.015243, accuracy: 0.1606
average test loss: 0.015195, accuracy: 0.1595
case acc: 0.11379487
case acc: 0.29429302
case acc: 0.15191431
case acc: 0.14638808
case acc: 0.095552534
case acc: 0.1552661
top acc: 0.1421 ::: bot acc: 0.0857
top acc: 0.3213 ::: bot acc: 0.2666
top acc: 0.1903 ::: bot acc: 0.1140
top acc: 0.1896 ::: bot acc: 0.1054
top acc: 0.1378 ::: bot acc: 0.0604
top acc: 0.1932 ::: bot acc: 0.1213
current epoch: 5
train loss is 0.012879
average val loss: 0.013377, accuracy: 0.1503
average test loss: 0.013358, accuracy: 0.1495
case acc: 0.10916307
case acc: 0.27475402
case acc: 0.14478798
case acc: 0.13525905
case acc: 0.09289079
case acc: 0.13991085
top acc: 0.1377 ::: bot acc: 0.0810
top acc: 0.3020 ::: bot acc: 0.2475
top acc: 0.1833 ::: bot acc: 0.1068
top acc: 0.1779 ::: bot acc: 0.0945
top acc: 0.1352 ::: bot acc: 0.0581
top acc: 0.1784 ::: bot acc: 0.1056
current epoch: 6
train loss is 0.013143
average val loss: 0.003573, accuracy: 0.0629
average test loss: 0.003609, accuracy: 0.0634
case acc: 0.027951004
case acc: 0.17056197
case acc: 0.05455283
case acc: 0.048646573
case acc: 0.028230028
case acc: 0.05027564
top acc: 0.0505 ::: bot acc: 0.0113
top acc: 0.1980 ::: bot acc: 0.1430
top acc: 0.0907 ::: bot acc: 0.0216
top acc: 0.0870 ::: bot acc: 0.0181
top acc: 0.0559 ::: bot acc: 0.0215
top acc: 0.0863 ::: bot acc: 0.0205
current epoch: 7
train loss is 0.007743
average val loss: 0.001509, accuracy: 0.0445
average test loss: 0.001594, accuracy: 0.0461
case acc: 0.039812014
case acc: 0.09421714
case acc: 0.031180533
case acc: 0.035772637
case acc: 0.04643645
case acc: 0.029383907
top acc: 0.0162 ::: bot acc: 0.0654
top acc: 0.1213 ::: bot acc: 0.0666
top acc: 0.0235 ::: bot acc: 0.0536
top acc: 0.0266 ::: bot acc: 0.0600
top acc: 0.0244 ::: bot acc: 0.0712
top acc: 0.0268 ::: bot acc: 0.0460
current epoch: 8
train loss is 0.003692
average val loss: 0.001304, accuracy: 0.0397
average test loss: 0.001369, accuracy: 0.0414
case acc: 0.032112002
case acc: 0.0907409
case acc: 0.02955772
case acc: 0.03231044
case acc: 0.036148775
case acc: 0.027338114
top acc: 0.0125 ::: bot acc: 0.0560
top acc: 0.1181 ::: bot acc: 0.0635
top acc: 0.0267 ::: bot acc: 0.0495
top acc: 0.0336 ::: bot acc: 0.0512
top acc: 0.0266 ::: bot acc: 0.0547
top acc: 0.0382 ::: bot acc: 0.0350
current epoch: 9
train loss is 0.002668
average val loss: 0.001293, accuracy: 0.0379
average test loss: 0.001349, accuracy: 0.0389
case acc: 0.023073955
case acc: 0.09479874
case acc: 0.028138416
case acc: 0.03053056
case acc: 0.02916471
case acc: 0.027896337
top acc: 0.0161 ::: bot acc: 0.0404
top acc: 0.1219 ::: bot acc: 0.0672
top acc: 0.0368 ::: bot acc: 0.0397
top acc: 0.0476 ::: bot acc: 0.0379
top acc: 0.0413 ::: bot acc: 0.0361
top acc: 0.0531 ::: bot acc: 0.0192
current epoch: 10
train loss is 0.002464
average val loss: 0.001227, accuracy: 0.0372
average test loss: 0.001282, accuracy: 0.0379
case acc: 0.02178842
case acc: 0.0897734
case acc: 0.028374042
case acc: 0.030804338
case acc: 0.027799757
case acc: 0.028973771
top acc: 0.0201 ::: bot acc: 0.0365
top acc: 0.1174 ::: bot acc: 0.0616
top acc: 0.0379 ::: bot acc: 0.0390
top acc: 0.0504 ::: bot acc: 0.0349
top acc: 0.0471 ::: bot acc: 0.0300
top acc: 0.0565 ::: bot acc: 0.0158
current epoch: 11
train loss is 0.002326
average val loss: 0.001095, accuracy: 0.0356
average test loss: 0.001139, accuracy: 0.0362
case acc: 0.02167274
case acc: 0.08031123
case acc: 0.028316231
case acc: 0.030536123
case acc: 0.02784726
case acc: 0.028600315
top acc: 0.0198 ::: bot acc: 0.0362
top acc: 0.1078 ::: bot acc: 0.0527
top acc: 0.0343 ::: bot acc: 0.0422
top acc: 0.0490 ::: bot acc: 0.0359
top acc: 0.0478 ::: bot acc: 0.0294
top acc: 0.0558 ::: bot acc: 0.0167
current epoch: 12
train loss is 0.002048
average val loss: 0.001038, accuracy: 0.0350
average test loss: 0.001087, accuracy: 0.0355
case acc: 0.021079421
case acc: 0.0752587
case acc: 0.02832663
case acc: 0.03067981
case acc: 0.02782476
case acc: 0.029688355
top acc: 0.0228 ::: bot acc: 0.0333
top acc: 0.1026 ::: bot acc: 0.0473
top acc: 0.0345 ::: bot acc: 0.0420
top acc: 0.0514 ::: bot acc: 0.0333
top acc: 0.0518 ::: bot acc: 0.0251
top acc: 0.0583 ::: bot acc: 0.0147
current epoch: 13
train loss is 0.001844
average val loss: 0.000946, accuracy: 0.0337
average test loss: 0.000986, accuracy: 0.0340
case acc: 0.020889422
case acc: 0.06701073
case acc: 0.028614882
case acc: 0.030571712
case acc: 0.028070712
case acc: 0.029060258
top acc: 0.0225 ::: bot acc: 0.0331
top acc: 0.0940 ::: bot acc: 0.0394
top acc: 0.0319 ::: bot acc: 0.0448
top acc: 0.0498 ::: bot acc: 0.0348
top acc: 0.0520 ::: bot acc: 0.0256
top acc: 0.0570 ::: bot acc: 0.0158
current epoch: 14
train loss is 0.001631
average val loss: 0.000886, accuracy: 0.0328
average test loss: 0.000932, accuracy: 0.0332
case acc: 0.020839797
case acc: 0.061375648
case acc: 0.029048637
case acc: 0.03058679
case acc: 0.028007166
case acc: 0.0294115
top acc: 0.0251 ::: bot acc: 0.0311
top acc: 0.0886 ::: bot acc: 0.0338
top acc: 0.0320 ::: bot acc: 0.0453
top acc: 0.0506 ::: bot acc: 0.0338
top acc: 0.0530 ::: bot acc: 0.0244
top acc: 0.0578 ::: bot acc: 0.0151
current epoch: 15
train loss is 0.001493
average val loss: 0.000841, accuracy: 0.0321
average test loss: 0.000889, accuracy: 0.0325
case acc: 0.02095982
case acc: 0.05658686
case acc: 0.028986631
case acc: 0.03081378
case acc: 0.028167773
case acc: 0.029466778
top acc: 0.0263 ::: bot acc: 0.0300
top acc: 0.0836 ::: bot acc: 0.0289
top acc: 0.0316 ::: bot acc: 0.0454
top acc: 0.0516 ::: bot acc: 0.0334
top acc: 0.0536 ::: bot acc: 0.0242
top acc: 0.0580 ::: bot acc: 0.0151
current epoch: 16
train loss is 0.001410
average val loss: 0.000807, accuracy: 0.0315
average test loss: 0.000856, accuracy: 0.0319
case acc: 0.0206538
case acc: 0.05285365
case acc: 0.028613301
case acc: 0.031157294
case acc: 0.02830481
case acc: 0.02957263
top acc: 0.0278 ::: bot acc: 0.0282
top acc: 0.0800 ::: bot acc: 0.0252
top acc: 0.0319 ::: bot acc: 0.0447
top acc: 0.0529 ::: bot acc: 0.0324
top acc: 0.0544 ::: bot acc: 0.0237
top acc: 0.0587 ::: bot acc: 0.0142
current epoch: 17
train loss is 0.001334
average val loss: 0.000803, accuracy: 0.0315
average test loss: 0.000846, accuracy: 0.0316
case acc: 0.020685853
case acc: 0.050810747
case acc: 0.028401645
case acc: 0.03123868
case acc: 0.028190129
case acc: 0.030018091
top acc: 0.0308 ::: bot acc: 0.0254
top acc: 0.0781 ::: bot acc: 0.0232
top acc: 0.0343 ::: bot acc: 0.0426
top acc: 0.0547 ::: bot acc: 0.0296
top acc: 0.0557 ::: bot acc: 0.0221
top acc: 0.0598 ::: bot acc: 0.0133
current epoch: 18
train loss is 0.001262
average val loss: 0.000789, accuracy: 0.0313
average test loss: 0.000828, accuracy: 0.0312
case acc: 0.020951653
case acc: 0.04827395
case acc: 0.0281529
case acc: 0.03168915
case acc: 0.027907964
case acc: 0.03047037
top acc: 0.0330 ::: bot acc: 0.0232
top acc: 0.0753 ::: bot acc: 0.0212
top acc: 0.0357 ::: bot acc: 0.0409
top acc: 0.0562 ::: bot acc: 0.0287
top acc: 0.0551 ::: bot acc: 0.0220
top acc: 0.0607 ::: bot acc: 0.0136
current epoch: 19
train loss is 0.001220
average val loss: 0.000758, accuracy: 0.0307
average test loss: 0.000808, accuracy: 0.0309
case acc: 0.021223484
case acc: 0.045621034
case acc: 0.028336309
case acc: 0.03186382
case acc: 0.02804701
case acc: 0.03016619
top acc: 0.0341 ::: bot acc: 0.0226
top acc: 0.0729 ::: bot acc: 0.0186
top acc: 0.0367 ::: bot acc: 0.0406
top acc: 0.0568 ::: bot acc: 0.0280
top acc: 0.0543 ::: bot acc: 0.0232
top acc: 0.0598 ::: bot acc: 0.0137
current epoch: 20
train loss is 0.001162
average val loss: 0.000746, accuracy: 0.0305
average test loss: 0.000790, accuracy: 0.0306
case acc: 0.021403985
case acc: 0.04354851
case acc: 0.028116263
case acc: 0.032164056
case acc: 0.027955012
case acc: 0.030200789
top acc: 0.0353 ::: bot acc: 0.0211
top acc: 0.0703 ::: bot acc: 0.0170
top acc: 0.0379 ::: bot acc: 0.0388
top acc: 0.0577 ::: bot acc: 0.0274
top acc: 0.0537 ::: bot acc: 0.0236
top acc: 0.0596 ::: bot acc: 0.0141
current epoch: 21
train loss is 0.001151
average val loss: 0.000735, accuracy: 0.0303
average test loss: 0.000779, accuracy: 0.0304
case acc: 0.021622425
case acc: 0.041790307
case acc: 0.028368337
case acc: 0.03239257
case acc: 0.02794571
case acc: 0.030064851
top acc: 0.0365 ::: bot acc: 0.0194
top acc: 0.0682 ::: bot acc: 0.0159
top acc: 0.0395 ::: bot acc: 0.0378
top acc: 0.0585 ::: bot acc: 0.0262
top acc: 0.0532 ::: bot acc: 0.0243
top acc: 0.0596 ::: bot acc: 0.0138
current epoch: 22
train loss is 0.001114
average val loss: 0.000763, accuracy: 0.0309
average test loss: 0.000806, accuracy: 0.0309
case acc: 0.022771629
case acc: 0.04231428
case acc: 0.028129566
case acc: 0.03323211
case acc: 0.028156923
case acc: 0.030787045
top acc: 0.0402 ::: bot acc: 0.0164
top acc: 0.0690 ::: bot acc: 0.0159
top acc: 0.0427 ::: bot acc: 0.0342
top acc: 0.0612 ::: bot acc: 0.0235
top acc: 0.0549 ::: bot acc: 0.0227
top acc: 0.0612 ::: bot acc: 0.0127
current epoch: 23
train loss is 0.001114
average val loss: 0.000802, accuracy: 0.0319
average test loss: 0.000844, accuracy: 0.0317
case acc: 0.024191706
case acc: 0.042950887
case acc: 0.028434126
case acc: 0.03427356
case acc: 0.028442413
case acc: 0.031738058
top acc: 0.0436 ::: bot acc: 0.0138
top acc: 0.0696 ::: bot acc: 0.0165
top acc: 0.0466 ::: bot acc: 0.0303
top acc: 0.0638 ::: bot acc: 0.0214
top acc: 0.0569 ::: bot acc: 0.0208
top acc: 0.0629 ::: bot acc: 0.0124
current epoch: 24
train loss is 0.001082
average val loss: 0.000768, accuracy: 0.0311
average test loss: 0.000811, accuracy: 0.0310
case acc: 0.02392978
case acc: 0.040616207
case acc: 0.028433962
case acc: 0.033958226
case acc: 0.028055297
case acc: 0.030726995
top acc: 0.0433 ::: bot acc: 0.0138
top acc: 0.0671 ::: bot acc: 0.0148
top acc: 0.0464 ::: bot acc: 0.0305
top acc: 0.0633 ::: bot acc: 0.0220
top acc: 0.0550 ::: bot acc: 0.0224
top acc: 0.0613 ::: bot acc: 0.0128
current epoch: 25
train loss is 0.001049
average val loss: 0.000788, accuracy: 0.0316
average test loss: 0.000826, accuracy: 0.0314
case acc: 0.024823982
case acc: 0.040530287
case acc: 0.028920557
case acc: 0.03461916
case acc: 0.028241446
case acc: 0.030992793
top acc: 0.0450 ::: bot acc: 0.0128
top acc: 0.0670 ::: bot acc: 0.0149
top acc: 0.0489 ::: bot acc: 0.0282
top acc: 0.0648 ::: bot acc: 0.0207
top acc: 0.0558 ::: bot acc: 0.0216
top acc: 0.0612 ::: bot acc: 0.0126
current epoch: 26
train loss is 0.001030
average val loss: 0.000768, accuracy: 0.0310
average test loss: 0.000813, accuracy: 0.0311
case acc: 0.024973078
case acc: 0.03918498
case acc: 0.029196577
case acc: 0.03447851
case acc: 0.028213715
case acc: 0.030523976
top acc: 0.0451 ::: bot acc: 0.0130
top acc: 0.0653 ::: bot acc: 0.0141
top acc: 0.0495 ::: bot acc: 0.0279
top acc: 0.0648 ::: bot acc: 0.0208
top acc: 0.0547 ::: bot acc: 0.0229
top acc: 0.0606 ::: bot acc: 0.0131
current epoch: 27
train loss is 0.001023
average val loss: 0.000793, accuracy: 0.0317
average test loss: 0.000837, accuracy: 0.0316
case acc: 0.02591754
case acc: 0.039405514
case acc: 0.02992217
case acc: 0.03546462
case acc: 0.028256584
case acc: 0.030865215
top acc: 0.0470 ::: bot acc: 0.0122
top acc: 0.0657 ::: bot acc: 0.0141
top acc: 0.0519 ::: bot acc: 0.0254
top acc: 0.0665 ::: bot acc: 0.0199
top acc: 0.0559 ::: bot acc: 0.0217
top acc: 0.0614 ::: bot acc: 0.0125
current epoch: 28
train loss is 0.001030
average val loss: 0.000830, accuracy: 0.0326
average test loss: 0.000871, accuracy: 0.0324
case acc: 0.027142547
case acc: 0.04008009
case acc: 0.03070735
case acc: 0.036609475
case acc: 0.028414818
case acc: 0.031542193
top acc: 0.0493 ::: bot acc: 0.0112
top acc: 0.0662 ::: bot acc: 0.0146
top acc: 0.0546 ::: bot acc: 0.0225
top acc: 0.0687 ::: bot acc: 0.0192
top acc: 0.0571 ::: bot acc: 0.0202
top acc: 0.0626 ::: bot acc: 0.0120
current epoch: 29
train loss is 0.001020
average val loss: 0.000870, accuracy: 0.0335
average test loss: 0.000908, accuracy: 0.0333
case acc: 0.028419355
case acc: 0.04061537
case acc: 0.031711947
case acc: 0.037550174
case acc: 0.028898085
case acc: 0.0323118
top acc: 0.0515 ::: bot acc: 0.0107
top acc: 0.0670 ::: bot acc: 0.0151
top acc: 0.0569 ::: bot acc: 0.0204
top acc: 0.0704 ::: bot acc: 0.0182
top acc: 0.0590 ::: bot acc: 0.0185
top acc: 0.0638 ::: bot acc: 0.0119
current epoch: 30
train loss is 0.001026
average val loss: 0.000873, accuracy: 0.0336
average test loss: 0.000909, accuracy: 0.0332
case acc: 0.028586365
case acc: 0.04018374
case acc: 0.031964038
case acc: 0.037633214
case acc: 0.0291039
case acc: 0.031872205
top acc: 0.0517 ::: bot acc: 0.0108
top acc: 0.0667 ::: bot acc: 0.0144
top acc: 0.0580 ::: bot acc: 0.0194
top acc: 0.0706 ::: bot acc: 0.0181
top acc: 0.0597 ::: bot acc: 0.0180
top acc: 0.0634 ::: bot acc: 0.0119
current epoch: 31
train loss is 0.001012
average val loss: 0.000887, accuracy: 0.0339
average test loss: 0.000930, accuracy: 0.0337
case acc: 0.029479658
case acc: 0.040368892
case acc: 0.032588724
case acc: 0.03822067
case acc: 0.029462304
case acc: 0.03221817
top acc: 0.0529 ::: bot acc: 0.0109
top acc: 0.0668 ::: bot acc: 0.0147
top acc: 0.0599 ::: bot acc: 0.0176
top acc: 0.0717 ::: bot acc: 0.0178
top acc: 0.0608 ::: bot acc: 0.0171
top acc: 0.0638 ::: bot acc: 0.0119
current epoch: 32
train loss is 0.001024
average val loss: 0.000924, accuracy: 0.0347
average test loss: 0.000959, accuracy: 0.0344
case acc: 0.030312119
case acc: 0.040851116
case acc: 0.033505943
case acc: 0.039092008
case acc: 0.029889695
case acc: 0.032545876
top acc: 0.0542 ::: bot acc: 0.0109
top acc: 0.0672 ::: bot acc: 0.0153
top acc: 0.0617 ::: bot acc: 0.0165
top acc: 0.0729 ::: bot acc: 0.0176
top acc: 0.0617 ::: bot acc: 0.0161
top acc: 0.0646 ::: bot acc: 0.0116
current epoch: 33
train loss is 0.001024
average val loss: 0.000976, accuracy: 0.0359
average test loss: 0.001012, accuracy: 0.0355
case acc: 0.031601276
case acc: 0.041812085
case acc: 0.03487027
case acc: 0.04054981
case acc: 0.030506302
case acc: 0.03341107
top acc: 0.0561 ::: bot acc: 0.0108
top acc: 0.0684 ::: bot acc: 0.0157
top acc: 0.0643 ::: bot acc: 0.0153
top acc: 0.0755 ::: bot acc: 0.0171
top acc: 0.0637 ::: bot acc: 0.0141
top acc: 0.0660 ::: bot acc: 0.0112
current epoch: 34
train loss is 0.001035
average val loss: 0.000966, accuracy: 0.0357
average test loss: 0.001005, accuracy: 0.0353
case acc: 0.03143507
case acc: 0.040892016
case acc: 0.035032947
case acc: 0.04066132
case acc: 0.030776024
case acc: 0.033108674
top acc: 0.0560 ::: bot acc: 0.0108
top acc: 0.0673 ::: bot acc: 0.0152
top acc: 0.0646 ::: bot acc: 0.0153
top acc: 0.0758 ::: bot acc: 0.0174
top acc: 0.0643 ::: bot acc: 0.0142
top acc: 0.0653 ::: bot acc: 0.0113
current epoch: 35
train loss is 0.001019
average val loss: 0.000964, accuracy: 0.0356
average test loss: 0.001000, accuracy: 0.0352
case acc: 0.031326078
case acc: 0.040178914
case acc: 0.03533199
case acc: 0.04068233
case acc: 0.030615974
case acc: 0.033056356
top acc: 0.0560 ::: bot acc: 0.0109
top acc: 0.0665 ::: bot acc: 0.0147
top acc: 0.0652 ::: bot acc: 0.0151
top acc: 0.0756 ::: bot acc: 0.0174
top acc: 0.0638 ::: bot acc: 0.0140
top acc: 0.0655 ::: bot acc: 0.0115
current epoch: 36
train loss is 0.001006
average val loss: 0.000981, accuracy: 0.0360
average test loss: 0.001017, accuracy: 0.0355
case acc: 0.031426482
case acc: 0.040104
case acc: 0.03590667
case acc: 0.041371766
case acc: 0.030911416
case acc: 0.033345036
top acc: 0.0558 ::: bot acc: 0.0108
top acc: 0.0663 ::: bot acc: 0.0145
top acc: 0.0664 ::: bot acc: 0.0148
top acc: 0.0768 ::: bot acc: 0.0173
top acc: 0.0647 ::: bot acc: 0.0135
top acc: 0.0658 ::: bot acc: 0.0114
current epoch: 37
train loss is 0.000989
average val loss: 0.000934, accuracy: 0.0349
average test loss: 0.000970, accuracy: 0.0345
case acc: 0.030063959
case acc: 0.03833951
case acc: 0.035145495
case acc: 0.040548455
case acc: 0.030295826
case acc: 0.032603875
top acc: 0.0538 ::: bot acc: 0.0108
top acc: 0.0643 ::: bot acc: 0.0136
top acc: 0.0650 ::: bot acc: 0.0149
top acc: 0.0756 ::: bot acc: 0.0172
top acc: 0.0634 ::: bot acc: 0.0141
top acc: 0.0646 ::: bot acc: 0.0118
current epoch: 38
train loss is 0.000964
average val loss: 0.000882, accuracy: 0.0337
average test loss: 0.000922, accuracy: 0.0334
case acc: 0.028858718
case acc: 0.035886474
case acc: 0.034112044
case acc: 0.039647374
case acc: 0.030104797
case acc: 0.03169511
top acc: 0.0520 ::: bot acc: 0.0109
top acc: 0.0613 ::: bot acc: 0.0121
top acc: 0.0631 ::: bot acc: 0.0158
top acc: 0.0743 ::: bot acc: 0.0175
top acc: 0.0626 ::: bot acc: 0.0153
top acc: 0.0629 ::: bot acc: 0.0123
current epoch: 39
train loss is 0.000939
average val loss: 0.000833, accuracy: 0.0325
average test loss: 0.000871, accuracy: 0.0322
case acc: 0.02750532
case acc: 0.03339729
case acc: 0.033258703
case acc: 0.038660124
case acc: 0.029746544
case acc: 0.030779783
top acc: 0.0497 ::: bot acc: 0.0112
top acc: 0.0584 ::: bot acc: 0.0106
top acc: 0.0611 ::: bot acc: 0.0169
top acc: 0.0725 ::: bot acc: 0.0178
top acc: 0.0616 ::: bot acc: 0.0162
top acc: 0.0612 ::: bot acc: 0.0128
current epoch: 40
train loss is 0.000910
average val loss: 0.000811, accuracy: 0.0319
average test loss: 0.000850, accuracy: 0.0317
case acc: 0.026770636
case acc: 0.03234177
case acc: 0.03288904
case acc: 0.03816018
case acc: 0.029592665
case acc: 0.030450925
top acc: 0.0486 ::: bot acc: 0.0115
top acc: 0.0567 ::: bot acc: 0.0105
top acc: 0.0602 ::: bot acc: 0.0176
top acc: 0.0717 ::: bot acc: 0.0178
top acc: 0.0614 ::: bot acc: 0.0161
top acc: 0.0607 ::: bot acc: 0.0130
current epoch: 41
train loss is 0.000885
average val loss: 0.000743, accuracy: 0.0302
average test loss: 0.000789, accuracy: 0.0303
case acc: 0.025116963
case acc: 0.029687641
case acc: 0.031757083
case acc: 0.036927283
case acc: 0.02892309
case acc: 0.029462695
top acc: 0.0455 ::: bot acc: 0.0127
top acc: 0.0530 ::: bot acc: 0.0102
top acc: 0.0575 ::: bot acc: 0.0200
top acc: 0.0696 ::: bot acc: 0.0187
top acc: 0.0591 ::: bot acc: 0.0183
top acc: 0.0582 ::: bot acc: 0.0149
current epoch: 42
train loss is 0.000848
average val loss: 0.000674, accuracy: 0.0285
average test loss: 0.000722, accuracy: 0.0289
case acc: 0.023201931
case acc: 0.027215356
case acc: 0.03046039
case acc: 0.035358645
case acc: 0.028358929
case acc: 0.028583178
top acc: 0.0413 ::: bot acc: 0.0152
top acc: 0.0489 ::: bot acc: 0.0113
top acc: 0.0538 ::: bot acc: 0.0234
top acc: 0.0663 ::: bot acc: 0.0202
top acc: 0.0563 ::: bot acc: 0.0216
top acc: 0.0553 ::: bot acc: 0.0177
current epoch: 43
train loss is 0.000818
average val loss: 0.000612, accuracy: 0.0269
average test loss: 0.000666, accuracy: 0.0276
case acc: 0.02200896
case acc: 0.024747623
case acc: 0.029227756
case acc: 0.033866577
case acc: 0.027948804
case acc: 0.027810901
top acc: 0.0372 ::: bot acc: 0.0195
top acc: 0.0441 ::: bot acc: 0.0133
top acc: 0.0496 ::: bot acc: 0.0279
top acc: 0.0631 ::: bot acc: 0.0221
top acc: 0.0528 ::: bot acc: 0.0247
top acc: 0.0522 ::: bot acc: 0.0207
current epoch: 44
train loss is 0.000799
average val loss: 0.000592, accuracy: 0.0264
average test loss: 0.000646, accuracy: 0.0271
case acc: 0.0214761
case acc: 0.023531882
case acc: 0.028644398
case acc: 0.03334813
case acc: 0.02799799
case acc: 0.027658414
top acc: 0.0353 ::: bot acc: 0.0213
top acc: 0.0417 ::: bot acc: 0.0147
top acc: 0.0475 ::: bot acc: 0.0295
top acc: 0.0619 ::: bot acc: 0.0232
top acc: 0.0522 ::: bot acc: 0.0255
top acc: 0.0512 ::: bot acc: 0.0214
current epoch: 45
train loss is 0.000782
average val loss: 0.000566, accuracy: 0.0257
average test loss: 0.000625, accuracy: 0.0267
case acc: 0.021023389
case acc: 0.022253169
case acc: 0.028344534
case acc: 0.032720577
case acc: 0.028149668
case acc: 0.02757858
top acc: 0.0326 ::: bot acc: 0.0240
top acc: 0.0382 ::: bot acc: 0.0173
top acc: 0.0446 ::: bot acc: 0.0326
top acc: 0.0599 ::: bot acc: 0.0248
top acc: 0.0510 ::: bot acc: 0.0271
top acc: 0.0501 ::: bot acc: 0.0228
current epoch: 46
train loss is 0.000767
average val loss: 0.000528, accuracy: 0.0246
average test loss: 0.000593, accuracy: 0.0262
case acc: 0.020962859
case acc: 0.020844115
case acc: 0.028357398
case acc: 0.031541333
case acc: 0.028120829
case acc: 0.0272081
top acc: 0.0272 ::: bot acc: 0.0295
top acc: 0.0320 ::: bot acc: 0.0237
top acc: 0.0385 ::: bot acc: 0.0387
top acc: 0.0555 ::: bot acc: 0.0297
top acc: 0.0462 ::: bot acc: 0.0314
top acc: 0.0464 ::: bot acc: 0.0267
current epoch: 47
train loss is 0.000754
average val loss: 0.000521, accuracy: 0.0245
average test loss: 0.000585, accuracy: 0.0261
case acc: 0.02116902
case acc: 0.02039668
case acc: 0.028480247
case acc: 0.031136118
case acc: 0.028335785
case acc: 0.027123092
top acc: 0.0240 ::: bot acc: 0.0327
top acc: 0.0283 ::: bot acc: 0.0272
top acc: 0.0348 ::: bot acc: 0.0422
top acc: 0.0532 ::: bot acc: 0.0320
top acc: 0.0447 ::: bot acc: 0.0331
top acc: 0.0447 ::: bot acc: 0.0281
current epoch: 48
train loss is 0.000755
average val loss: 0.000522, accuracy: 0.0245
average test loss: 0.000591, accuracy: 0.0264
case acc: 0.021931764
case acc: 0.020785233
case acc: 0.029348621
case acc: 0.030443894
case acc: 0.028848255
case acc: 0.027073001
top acc: 0.0201 ::: bot acc: 0.0369
top acc: 0.0230 ::: bot acc: 0.0327
top acc: 0.0298 ::: bot acc: 0.0473
top acc: 0.0498 ::: bot acc: 0.0349
top acc: 0.0422 ::: bot acc: 0.0353
top acc: 0.0424 ::: bot acc: 0.0304
current epoch: 49
train loss is 0.000760
average val loss: 0.000546, accuracy: 0.0254
average test loss: 0.000622, accuracy: 0.0275
case acc: 0.023768226
case acc: 0.022231007
case acc: 0.031235106
case acc: 0.030498153
case acc: 0.029775377
case acc: 0.027298175
top acc: 0.0156 ::: bot acc: 0.0420
top acc: 0.0165 ::: bot acc: 0.0392
top acc: 0.0235 ::: bot acc: 0.0539
top acc: 0.0454 ::: bot acc: 0.0396
top acc: 0.0389 ::: bot acc: 0.0390
top acc: 0.0398 ::: bot acc: 0.0333
current epoch: 50
train loss is 0.000773
average val loss: 0.000614, accuracy: 0.0276
average test loss: 0.000697, accuracy: 0.0297
case acc: 0.027593931
case acc: 0.02563228
case acc: 0.03474372
case acc: 0.031244315
case acc: 0.03145788
case acc: 0.027747199
top acc: 0.0129 ::: bot acc: 0.0490
top acc: 0.0106 ::: bot acc: 0.0474
top acc: 0.0173 ::: bot acc: 0.0623
top acc: 0.0393 ::: bot acc: 0.0454
top acc: 0.0340 ::: bot acc: 0.0440
top acc: 0.0352 ::: bot acc: 0.0378
LME_Co_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6798 6798 6798
1.7082474 -0.6288155 0.21141115 -0.19947179
Validation: 756 756 756
Testing: 750 750 750
pre-processing time: 0.0002951622009277344
the split date is 2011-01-01
train dropout: 0.6 test dropout: 0.1
net initializing with time: 0.0025300979614257812
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.013199
average val loss: 0.005849, accuracy: 0.0990
average test loss: 0.004869, accuracy: 0.0897
case acc: 0.13043535
case acc: 0.09467686
case acc: 0.0832001
case acc: 0.07361687
case acc: 0.10617988
case acc: 0.05014521
top acc: 0.0981 ::: bot acc: 0.1647
top acc: 0.1236 ::: bot acc: 0.0620
top acc: 0.0416 ::: bot acc: 0.1272
top acc: 0.0396 ::: bot acc: 0.1070
top acc: 0.0631 ::: bot acc: 0.1477
top acc: 0.0201 ::: bot acc: 0.0845
current epoch: 2
train loss is 0.007939
average val loss: 0.003399, accuracy: 0.0602
average test loss: 0.003583, accuracy: 0.0597
case acc: 0.032247324
case acc: 0.18074867
case acc: 0.037062556
case acc: 0.029969359
case acc: 0.033667002
case acc: 0.044434793
top acc: 0.0094 ::: bot acc: 0.0622
top acc: 0.2091 ::: bot acc: 0.1491
top acc: 0.0658 ::: bot acc: 0.0247
top acc: 0.0553 ::: bot acc: 0.0151
top acc: 0.0265 ::: bot acc: 0.0561
top acc: 0.0794 ::: bot acc: 0.0150
current epoch: 3
train loss is 0.008426
average val loss: 0.007910, accuracy: 0.1019
average test loss: 0.009091, accuracy: 0.1139
case acc: 0.0581813
case acc: 0.25162834
case acc: 0.10580785
case acc: 0.098301835
case acc: 0.060436174
case acc: 0.10905336
top acc: 0.0890 ::: bot acc: 0.0257
top acc: 0.2794 ::: bot acc: 0.2206
top acc: 0.1511 ::: bot acc: 0.0605
top acc: 0.1330 ::: bot acc: 0.0643
top acc: 0.1015 ::: bot acc: 0.0218
top acc: 0.1480 ::: bot acc: 0.0703
current epoch: 4
train loss is 0.010596
average val loss: 0.015820, accuracy: 0.1636
average test loss: 0.017707, accuracy: 0.1756
case acc: 0.1272201
case acc: 0.30674776
case acc: 0.17342605
case acc: 0.15894027
case acc: 0.12380495
case acc: 0.16327615
top acc: 0.1593 ::: bot acc: 0.0932
top acc: 0.3348 ::: bot acc: 0.2756
top acc: 0.2181 ::: bot acc: 0.1282
top acc: 0.1929 ::: bot acc: 0.1248
top acc: 0.1657 ::: bot acc: 0.0837
top acc: 0.2023 ::: bot acc: 0.1249
current epoch: 5
train loss is 0.012857
average val loss: 0.014107, accuracy: 0.1546
average test loss: 0.015910, accuracy: 0.1666
case acc: 0.12365094
case acc: 0.28784457
case acc: 0.16770732
case acc: 0.14900301
case acc: 0.12225246
case acc: 0.14895292
top acc: 0.1553 ::: bot acc: 0.0900
top acc: 0.3158 ::: bot acc: 0.2572
top acc: 0.2126 ::: bot acc: 0.1222
top acc: 0.1834 ::: bot acc: 0.1144
top acc: 0.1644 ::: bot acc: 0.0815
top acc: 0.1881 ::: bot acc: 0.1108
current epoch: 6
train loss is 0.013160
average val loss: 0.003369, accuracy: 0.0603
average test loss: 0.004006, accuracy: 0.0696
case acc: 0.03303891
case acc: 0.17446081
case acc: 0.06793208
case acc: 0.050362304
case acc: 0.0399443
case acc: 0.051578235
top acc: 0.0589 ::: bot acc: 0.0114
top acc: 0.2020 ::: bot acc: 0.1442
top acc: 0.1103 ::: bot acc: 0.0278
top acc: 0.0836 ::: bot acc: 0.0194
top acc: 0.0752 ::: bot acc: 0.0131
top acc: 0.0881 ::: bot acc: 0.0194
current epoch: 7
train loss is 0.007673
average val loss: 0.001574, accuracy: 0.0446
average test loss: 0.001607, accuracy: 0.0426
case acc: 0.0290149
case acc: 0.108093046
case acc: 0.03393018
case acc: 0.025739871
case acc: 0.031054027
case acc: 0.027936803
top acc: 0.0113 ::: bot acc: 0.0557
top acc: 0.1361 ::: bot acc: 0.0775
top acc: 0.0535 ::: bot acc: 0.0371
top acc: 0.0313 ::: bot acc: 0.0381
top acc: 0.0355 ::: bot acc: 0.0479
top acc: 0.0378 ::: bot acc: 0.0391
current epoch: 8
train loss is 0.003519
average val loss: 0.001396, accuracy: 0.0408
average test loss: 0.001518, accuracy: 0.0415
case acc: 0.025207339
case acc: 0.10410285
case acc: 0.034635004
case acc: 0.02545817
case acc: 0.030608665
case acc: 0.028932624
top acc: 0.0183 ::: bot acc: 0.0468
top acc: 0.1317 ::: bot acc: 0.0739
top acc: 0.0565 ::: bot acc: 0.0344
top acc: 0.0391 ::: bot acc: 0.0300
top acc: 0.0506 ::: bot acc: 0.0317
top acc: 0.0474 ::: bot acc: 0.0295
current epoch: 9
train loss is 0.002611
average val loss: 0.001495, accuracy: 0.0402
average test loss: 0.001803, accuracy: 0.0460
case acc: 0.02464893
case acc: 0.11049474
case acc: 0.038481236
case acc: 0.029904144
case acc: 0.03743391
case acc: 0.0348309
top acc: 0.0364 ::: bot acc: 0.0289
top acc: 0.1384 ::: bot acc: 0.0802
top acc: 0.0692 ::: bot acc: 0.0226
top acc: 0.0548 ::: bot acc: 0.0157
top acc: 0.0709 ::: bot acc: 0.0147
top acc: 0.0645 ::: bot acc: 0.0156
current epoch: 10
train loss is 0.002537
average val loss: 0.001430, accuracy: 0.0396
average test loss: 0.001774, accuracy: 0.0465
case acc: 0.025464881
case acc: 0.10527079
case acc: 0.03885704
case acc: 0.03131472
case acc: 0.041257244
case acc: 0.036591046
top acc: 0.0405 ::: bot acc: 0.0251
top acc: 0.1330 ::: bot acc: 0.0750
top acc: 0.0703 ::: bot acc: 0.0218
top acc: 0.0579 ::: bot acc: 0.0141
top acc: 0.0771 ::: bot acc: 0.0136
top acc: 0.0679 ::: bot acc: 0.0142
current epoch: 11
train loss is 0.002338
average val loss: 0.001248, accuracy: 0.0375
average test loss: 0.001558, accuracy: 0.0438
case acc: 0.025124695
case acc: 0.09478335
case acc: 0.036932003
case acc: 0.030390397
case acc: 0.040402208
case acc: 0.035318334
top acc: 0.0390 ::: bot acc: 0.0264
top acc: 0.1222 ::: bot acc: 0.0647
top acc: 0.0656 ::: bot acc: 0.0257
top acc: 0.0555 ::: bot acc: 0.0158
top acc: 0.0760 ::: bot acc: 0.0131
top acc: 0.0658 ::: bot acc: 0.0150
current epoch: 12
train loss is 0.002041
average val loss: 0.001114, accuracy: 0.0358
average test loss: 0.001378, accuracy: 0.0416
case acc: 0.024898674
case acc: 0.085013114
case acc: 0.035583917
case acc: 0.029360166
case acc: 0.04027863
case acc: 0.034219358
top acc: 0.0378 ::: bot acc: 0.0279
top acc: 0.1128 ::: bot acc: 0.0550
top acc: 0.0610 ::: bot acc: 0.0295
top acc: 0.0533 ::: bot acc: 0.0174
top acc: 0.0761 ::: bot acc: 0.0135
top acc: 0.0635 ::: bot acc: 0.0156
current epoch: 13
train loss is 0.001774
average val loss: 0.001071, accuracy: 0.0352
average test loss: 0.001353, accuracy: 0.0416
case acc: 0.025504777
case acc: 0.0805585
case acc: 0.03572568
case acc: 0.0300783
case acc: 0.04213933
case acc: 0.035293646
top acc: 0.0410 ::: bot acc: 0.0248
top acc: 0.1083 ::: bot acc: 0.0508
top acc: 0.0619 ::: bot acc: 0.0290
top acc: 0.0550 ::: bot acc: 0.0156
top acc: 0.0784 ::: bot acc: 0.0134
top acc: 0.0656 ::: bot acc: 0.0151
current epoch: 14
train loss is 0.001644
average val loss: 0.001004, accuracy: 0.0343
average test loss: 0.001291, accuracy: 0.0408
case acc: 0.025748106
case acc: 0.07477755
case acc: 0.035656318
case acc: 0.03059874
case acc: 0.04254664
case acc: 0.035694238
top acc: 0.0423 ::: bot acc: 0.0228
top acc: 0.1025 ::: bot acc: 0.0448
top acc: 0.0614 ::: bot acc: 0.0291
top acc: 0.0564 ::: bot acc: 0.0151
top acc: 0.0792 ::: bot acc: 0.0130
top acc: 0.0662 ::: bot acc: 0.0147
current epoch: 15
train loss is 0.001497
average val loss: 0.000940, accuracy: 0.0332
average test loss: 0.001198, accuracy: 0.0395
case acc: 0.025878299
case acc: 0.06869585
case acc: 0.035118237
case acc: 0.03023034
case acc: 0.041906424
case acc: 0.03507726
top acc: 0.0428 ::: bot acc: 0.0227
top acc: 0.0963 ::: bot acc: 0.0389
top acc: 0.0596 ::: bot acc: 0.0309
top acc: 0.0557 ::: bot acc: 0.0155
top acc: 0.0779 ::: bot acc: 0.0133
top acc: 0.0651 ::: bot acc: 0.0152
current epoch: 16
train loss is 0.001427
average val loss: 0.000931, accuracy: 0.0331
average test loss: 0.001220, accuracy: 0.0401
case acc: 0.027116805
case acc: 0.066525675
case acc: 0.03556144
case acc: 0.031715028
case acc: 0.04339045
case acc: 0.036468953
top acc: 0.0463 ::: bot acc: 0.0194
top acc: 0.0940 ::: bot acc: 0.0370
top acc: 0.0619 ::: bot acc: 0.0281
top acc: 0.0583 ::: bot acc: 0.0144
top acc: 0.0803 ::: bot acc: 0.0134
top acc: 0.0672 ::: bot acc: 0.0147
current epoch: 17
train loss is 0.001336
average val loss: 0.000932, accuracy: 0.0332
average test loss: 0.001236, accuracy: 0.0406
case acc: 0.028082866
case acc: 0.0642589
case acc: 0.036581285
case acc: 0.033040524
case acc: 0.044457987
case acc: 0.03720062
top acc: 0.0486 ::: bot acc: 0.0172
top acc: 0.0917 ::: bot acc: 0.0348
top acc: 0.0643 ::: bot acc: 0.0265
top acc: 0.0605 ::: bot acc: 0.0138
top acc: 0.0817 ::: bot acc: 0.0132
top acc: 0.0690 ::: bot acc: 0.0141
current epoch: 18
train loss is 0.001303
average val loss: 0.000900, accuracy: 0.0327
average test loss: 0.001201, accuracy: 0.0401
case acc: 0.028597938
case acc: 0.060648385
case acc: 0.03688859
case acc: 0.033427488
case acc: 0.043784153
case acc: 0.03701507
top acc: 0.0503 ::: bot acc: 0.0161
top acc: 0.0879 ::: bot acc: 0.0315
top acc: 0.0652 ::: bot acc: 0.0258
top acc: 0.0610 ::: bot acc: 0.0138
top acc: 0.0809 ::: bot acc: 0.0130
top acc: 0.0691 ::: bot acc: 0.0142
current epoch: 19
train loss is 0.001223
average val loss: 0.000876, accuracy: 0.0323
average test loss: 0.001180, accuracy: 0.0398
case acc: 0.029268747
case acc: 0.05809784
case acc: 0.037032533
case acc: 0.0340986
case acc: 0.043262754
case acc: 0.03695273
top acc: 0.0515 ::: bot acc: 0.0151
top acc: 0.0853 ::: bot acc: 0.0291
top acc: 0.0659 ::: bot acc: 0.0245
top acc: 0.0621 ::: bot acc: 0.0137
top acc: 0.0805 ::: bot acc: 0.0125
top acc: 0.0683 ::: bot acc: 0.0144
current epoch: 20
train loss is 0.001151
average val loss: 0.000857, accuracy: 0.0319
average test loss: 0.001163, accuracy: 0.0395
case acc: 0.029828118
case acc: 0.05584552
case acc: 0.037575174
case acc: 0.034212895
case acc: 0.04293319
case acc: 0.03673689
top acc: 0.0526 ::: bot acc: 0.0145
top acc: 0.0830 ::: bot acc: 0.0270
top acc: 0.0673 ::: bot acc: 0.0235
top acc: 0.0626 ::: bot acc: 0.0134
top acc: 0.0799 ::: bot acc: 0.0129
top acc: 0.0682 ::: bot acc: 0.0143
current epoch: 21
train loss is 0.001112
average val loss: 0.000862, accuracy: 0.0321
average test loss: 0.001177, accuracy: 0.0399
case acc: 0.030961351
case acc: 0.054461658
case acc: 0.03829952
case acc: 0.035488702
case acc: 0.043073986
case acc: 0.03687889
top acc: 0.0552 ::: bot acc: 0.0130
top acc: 0.0817 ::: bot acc: 0.0256
top acc: 0.0693 ::: bot acc: 0.0217
top acc: 0.0642 ::: bot acc: 0.0135
top acc: 0.0801 ::: bot acc: 0.0131
top acc: 0.0684 ::: bot acc: 0.0144
current epoch: 22
train loss is 0.001106
average val loss: 0.000880, accuracy: 0.0325
average test loss: 0.001215, accuracy: 0.0407
case acc: 0.03247355
case acc: 0.05415981
case acc: 0.039743435
case acc: 0.036717124
case acc: 0.043448072
case acc: 0.03747549
top acc: 0.0578 ::: bot acc: 0.0122
top acc: 0.0812 ::: bot acc: 0.0256
top acc: 0.0721 ::: bot acc: 0.0200
top acc: 0.0663 ::: bot acc: 0.0134
top acc: 0.0804 ::: bot acc: 0.0130
top acc: 0.0693 ::: bot acc: 0.0142
current epoch: 23
train loss is 0.001086
average val loss: 0.000863, accuracy: 0.0322
average test loss: 0.001189, accuracy: 0.0402
case acc: 0.032744173
case acc: 0.052050713
case acc: 0.040015582
case acc: 0.036845133
case acc: 0.042782966
case acc: 0.036890943
top acc: 0.0583 ::: bot acc: 0.0121
top acc: 0.0788 ::: bot acc: 0.0239
top acc: 0.0730 ::: bot acc: 0.0194
top acc: 0.0664 ::: bot acc: 0.0134
top acc: 0.0794 ::: bot acc: 0.0132
top acc: 0.0682 ::: bot acc: 0.0143
current epoch: 24
train loss is 0.001037
average val loss: 0.000855, accuracy: 0.0321
average test loss: 0.001191, accuracy: 0.0402
case acc: 0.033288892
case acc: 0.050847873
case acc: 0.04095263
case acc: 0.037333053
case acc: 0.042350795
case acc: 0.036660697
top acc: 0.0591 ::: bot acc: 0.0119
top acc: 0.0777 ::: bot acc: 0.0225
top acc: 0.0747 ::: bot acc: 0.0187
top acc: 0.0674 ::: bot acc: 0.0133
top acc: 0.0789 ::: bot acc: 0.0131
top acc: 0.0681 ::: bot acc: 0.0141
current epoch: 25
train loss is 0.001030
average val loss: 0.000931, accuracy: 0.0337
average test loss: 0.001301, accuracy: 0.0424
case acc: 0.035865076
case acc: 0.052666623
case acc: 0.04285841
case acc: 0.040059976
case acc: 0.044168703
case acc: 0.03853491
top acc: 0.0634 ::: bot acc: 0.0112
top acc: 0.0794 ::: bot acc: 0.0243
top acc: 0.0788 ::: bot acc: 0.0162
top acc: 0.0709 ::: bot acc: 0.0143
top acc: 0.0816 ::: bot acc: 0.0130
top acc: 0.0710 ::: bot acc: 0.0141
current epoch: 26
train loss is 0.001049
average val loss: 0.000981, accuracy: 0.0348
average test loss: 0.001380, accuracy: 0.0439
case acc: 0.03764519
case acc: 0.053516127
case acc: 0.044861306
case acc: 0.04202667
case acc: 0.04531409
case acc: 0.03984364
top acc: 0.0658 ::: bot acc: 0.0113
top acc: 0.0806 ::: bot acc: 0.0248
top acc: 0.0818 ::: bot acc: 0.0156
top acc: 0.0736 ::: bot acc: 0.0149
top acc: 0.0833 ::: bot acc: 0.0131
top acc: 0.0725 ::: bot acc: 0.0146
current epoch: 27
train loss is 0.001035
average val loss: 0.000930, accuracy: 0.0338
average test loss: 0.001313, accuracy: 0.0426
case acc: 0.03700627
case acc: 0.050949264
case acc: 0.044562824
case acc: 0.04095663
case acc: 0.044132553
case acc: 0.037935216
top acc: 0.0648 ::: bot acc: 0.0114
top acc: 0.0778 ::: bot acc: 0.0227
top acc: 0.0812 ::: bot acc: 0.0159
top acc: 0.0719 ::: bot acc: 0.0143
top acc: 0.0817 ::: bot acc: 0.0127
top acc: 0.0700 ::: bot acc: 0.0142
current epoch: 28
train loss is 0.001001
average val loss: 0.000925, accuracy: 0.0336
average test loss: 0.001313, accuracy: 0.0426
case acc: 0.03719637
case acc: 0.0500608
case acc: 0.04525364
case acc: 0.041016527
case acc: 0.044566263
case acc: 0.037615836
top acc: 0.0652 ::: bot acc: 0.0113
top acc: 0.0767 ::: bot acc: 0.0221
top acc: 0.0823 ::: bot acc: 0.0156
top acc: 0.0722 ::: bot acc: 0.0144
top acc: 0.0817 ::: bot acc: 0.0136
top acc: 0.0694 ::: bot acc: 0.0145
current epoch: 29
train loss is 0.001001
average val loss: 0.000981, accuracy: 0.0348
average test loss: 0.001394, accuracy: 0.0441
case acc: 0.039188676
case acc: 0.051122766
case acc: 0.04714921
case acc: 0.042884763
case acc: 0.04590687
case acc: 0.03837898
top acc: 0.0679 ::: bot acc: 0.0118
top acc: 0.0778 ::: bot acc: 0.0231
top acc: 0.0856 ::: bot acc: 0.0155
top acc: 0.0744 ::: bot acc: 0.0153
top acc: 0.0842 ::: bot acc: 0.0130
top acc: 0.0707 ::: bot acc: 0.0142
current epoch: 30
train loss is 0.000990
average val loss: 0.001017, accuracy: 0.0356
average test loss: 0.001451, accuracy: 0.0452
case acc: 0.040449128
case acc: 0.051517047
case acc: 0.048666988
case acc: 0.04413743
case acc: 0.04717811
case acc: 0.039042834
top acc: 0.0696 ::: bot acc: 0.0120
top acc: 0.0785 ::: bot acc: 0.0231
top acc: 0.0876 ::: bot acc: 0.0158
top acc: 0.0762 ::: bot acc: 0.0160
top acc: 0.0857 ::: bot acc: 0.0136
top acc: 0.0717 ::: bot acc: 0.0141
current epoch: 31
train loss is 0.000976
average val loss: 0.000978, accuracy: 0.0348
average test loss: 0.001399, accuracy: 0.0442
case acc: 0.039449777
case acc: 0.04967726
case acc: 0.04863609
case acc: 0.043115962
case acc: 0.04633099
case acc: 0.037979472
top acc: 0.0684 ::: bot acc: 0.0119
top acc: 0.0762 ::: bot acc: 0.0221
top acc: 0.0872 ::: bot acc: 0.0158
top acc: 0.0749 ::: bot acc: 0.0153
top acc: 0.0846 ::: bot acc: 0.0133
top acc: 0.0701 ::: bot acc: 0.0142
current epoch: 32
train loss is 0.000966
average val loss: 0.001008, accuracy: 0.0354
average test loss: 0.001445, accuracy: 0.0450
case acc: 0.040575027
case acc: 0.04974645
case acc: 0.04990436
case acc: 0.044469245
case acc: 0.0472766
case acc: 0.038257588
top acc: 0.0699 ::: bot acc: 0.0123
top acc: 0.0762 ::: bot acc: 0.0219
top acc: 0.0891 ::: bot acc: 0.0165
top acc: 0.0766 ::: bot acc: 0.0160
top acc: 0.0861 ::: bot acc: 0.0135
top acc: 0.0705 ::: bot acc: 0.0143
current epoch: 33
train loss is 0.000975
average val loss: 0.001053, accuracy: 0.0363
average test loss: 0.001510, accuracy: 0.0462
case acc: 0.041905712
case acc: 0.050615583
case acc: 0.051495206
case acc: 0.045915566
case acc: 0.048407126
case acc: 0.039128087
top acc: 0.0718 ::: bot acc: 0.0127
top acc: 0.0774 ::: bot acc: 0.0226
top acc: 0.0912 ::: bot acc: 0.0169
top acc: 0.0784 ::: bot acc: 0.0168
top acc: 0.0875 ::: bot acc: 0.0139
top acc: 0.0719 ::: bot acc: 0.0143
current epoch: 34
train loss is 0.000969
average val loss: 0.001060, accuracy: 0.0365
average test loss: 0.001514, accuracy: 0.0463
case acc: 0.041740038
case acc: 0.049922794
case acc: 0.052306786
case acc: 0.046474688
case acc: 0.048298318
case acc: 0.039080136
top acc: 0.0713 ::: bot acc: 0.0127
top acc: 0.0766 ::: bot acc: 0.0219
top acc: 0.0921 ::: bot acc: 0.0171
top acc: 0.0790 ::: bot acc: 0.0171
top acc: 0.0873 ::: bot acc: 0.0138
top acc: 0.0719 ::: bot acc: 0.0141
current epoch: 35
train loss is 0.000978
average val loss: 0.001067, accuracy: 0.0366
average test loss: 0.001529, accuracy: 0.0466
case acc: 0.04197924
case acc: 0.049706534
case acc: 0.052972827
case acc: 0.04694652
case acc: 0.048656367
case acc: 0.039305154
top acc: 0.0715 ::: bot acc: 0.0129
top acc: 0.0763 ::: bot acc: 0.0220
top acc: 0.0931 ::: bot acc: 0.0173
top acc: 0.0796 ::: bot acc: 0.0174
top acc: 0.0876 ::: bot acc: 0.0140
top acc: 0.0720 ::: bot acc: 0.0144
current epoch: 36
train loss is 0.000962
average val loss: 0.001095, accuracy: 0.0373
average test loss: 0.001577, accuracy: 0.0475
case acc: 0.04267519
case acc: 0.050088402
case acc: 0.054193564
case acc: 0.048231434
case acc: 0.049612314
case acc: 0.039903633
top acc: 0.0725 ::: bot acc: 0.0132
top acc: 0.0769 ::: bot acc: 0.0222
top acc: 0.0948 ::: bot acc: 0.0177
top acc: 0.0812 ::: bot acc: 0.0182
top acc: 0.0888 ::: bot acc: 0.0146
top acc: 0.0728 ::: bot acc: 0.0144
current epoch: 37
train loss is 0.000952
average val loss: 0.001080, accuracy: 0.0369
average test loss: 0.001555, accuracy: 0.0470
case acc: 0.042221
case acc: 0.048698533
case acc: 0.053797234
case acc: 0.047998488
case acc: 0.04984748
case acc: 0.039567683
top acc: 0.0721 ::: bot acc: 0.0127
top acc: 0.0751 ::: bot acc: 0.0211
top acc: 0.0941 ::: bot acc: 0.0177
top acc: 0.0807 ::: bot acc: 0.0181
top acc: 0.0894 ::: bot acc: 0.0146
top acc: 0.0725 ::: bot acc: 0.0143
current epoch: 38
train loss is 0.000941
average val loss: 0.001069, accuracy: 0.0366
average test loss: 0.001541, accuracy: 0.0468
case acc: 0.041795623
case acc: 0.04743207
case acc: 0.0537323
case acc: 0.047814876
case acc: 0.050395712
case acc: 0.039494425
top acc: 0.0714 ::: bot acc: 0.0127
top acc: 0.0737 ::: bot acc: 0.0204
top acc: 0.0941 ::: bot acc: 0.0177
top acc: 0.0806 ::: bot acc: 0.0179
top acc: 0.0898 ::: bot acc: 0.0150
top acc: 0.0725 ::: bot acc: 0.0143
current epoch: 39
train loss is 0.000933
average val loss: 0.001015, accuracy: 0.0354
average test loss: 0.001470, accuracy: 0.0454
case acc: 0.04013714
case acc: 0.045145594
case acc: 0.052496437
case acc: 0.046858974
case acc: 0.049575932
case acc: 0.038465943
top acc: 0.0693 ::: bot acc: 0.0120
top acc: 0.0709 ::: bot acc: 0.0188
top acc: 0.0925 ::: bot acc: 0.0171
top acc: 0.0793 ::: bot acc: 0.0176
top acc: 0.0889 ::: bot acc: 0.0145
top acc: 0.0708 ::: bot acc: 0.0141
current epoch: 40
train loss is 0.000885
average val loss: 0.000912, accuracy: 0.0331
average test loss: 0.001320, accuracy: 0.0426
case acc: 0.037198715
case acc: 0.041169178
case acc: 0.04954933
case acc: 0.044119455
case acc: 0.04706708
case acc: 0.036352836
top acc: 0.0653 ::: bot acc: 0.0115
top acc: 0.0662 ::: bot acc: 0.0163
top acc: 0.0887 ::: bot acc: 0.0160
top acc: 0.0762 ::: bot acc: 0.0159
top acc: 0.0856 ::: bot acc: 0.0137
top acc: 0.0674 ::: bot acc: 0.0145
current epoch: 41
train loss is 0.000838
average val loss: 0.000862, accuracy: 0.0320
average test loss: 0.001250, accuracy: 0.0412
case acc: 0.035537463
case acc: 0.038958307
case acc: 0.048016723
case acc: 0.04291784
case acc: 0.04616136
case acc: 0.03551531
top acc: 0.0629 ::: bot acc: 0.0113
top acc: 0.0637 ::: bot acc: 0.0149
top acc: 0.0866 ::: bot acc: 0.0156
top acc: 0.0746 ::: bot acc: 0.0151
top acc: 0.0842 ::: bot acc: 0.0135
top acc: 0.0660 ::: bot acc: 0.0150
current epoch: 42
train loss is 0.000819
average val loss: 0.000761, accuracy: 0.0297
average test loss: 0.001092, accuracy: 0.0380
case acc: 0.032429032
case acc: 0.034636825
case acc: 0.044748817
case acc: 0.039667994
case acc: 0.043335952
case acc: 0.03344861
top acc: 0.0576 ::: bot acc: 0.0122
top acc: 0.0580 ::: bot acc: 0.0137
top acc: 0.0816 ::: bot acc: 0.0158
top acc: 0.0704 ::: bot acc: 0.0139
top acc: 0.0802 ::: bot acc: 0.0131
top acc: 0.0620 ::: bot acc: 0.0167
current epoch: 43
train loss is 0.000779
average val loss: 0.000726, accuracy: 0.0289
average test loss: 0.001034, accuracy: 0.0368
case acc: 0.030965135
case acc: 0.032525454
case acc: 0.043289963
case acc: 0.038586333
case acc: 0.0425921
case acc: 0.032877598
top acc: 0.0553 ::: bot acc: 0.0131
top acc: 0.0549 ::: bot acc: 0.0134
top acc: 0.0792 ::: bot acc: 0.0163
top acc: 0.0690 ::: bot acc: 0.0136
top acc: 0.0790 ::: bot acc: 0.0133
top acc: 0.0610 ::: bot acc: 0.0173
current epoch: 44
train loss is 0.000757
average val loss: 0.000668, accuracy: 0.0275
average test loss: 0.000928, accuracy: 0.0346
case acc: 0.028877359
case acc: 0.029061874
case acc: 0.040896088
case acc: 0.036238685
case acc: 0.04057349
case acc: 0.031680834
top acc: 0.0508 ::: bot acc: 0.0157
top acc: 0.0498 ::: bot acc: 0.0132
top acc: 0.0746 ::: bot acc: 0.0186
top acc: 0.0658 ::: bot acc: 0.0132
top acc: 0.0759 ::: bot acc: 0.0136
top acc: 0.0583 ::: bot acc: 0.0192
current epoch: 45
train loss is 0.000733
average val loss: 0.000632, accuracy: 0.0267
average test loss: 0.000854, accuracy: 0.0330
case acc: 0.027524188
case acc: 0.02653344
case acc: 0.039094683
case acc: 0.034582697
case acc: 0.039128378
case acc: 0.030974368
top acc: 0.0474 ::: bot acc: 0.0185
top acc: 0.0454 ::: bot acc: 0.0145
top acc: 0.0707 ::: bot acc: 0.0210
top acc: 0.0632 ::: bot acc: 0.0135
top acc: 0.0736 ::: bot acc: 0.0141
top acc: 0.0563 ::: bot acc: 0.0208
current epoch: 46
train loss is 0.000721
average val loss: 0.000590, accuracy: 0.0261
average test loss: 0.000745, accuracy: 0.0305
case acc: 0.025507001
case acc: 0.02345027
case acc: 0.0365057
case acc: 0.03137731
case acc: 0.03652714
case acc: 0.029574173
top acc: 0.0412 ::: bot acc: 0.0244
top acc: 0.0386 ::: bot acc: 0.0189
top acc: 0.0643 ::: bot acc: 0.0262
top acc: 0.0582 ::: bot acc: 0.0142
top acc: 0.0686 ::: bot acc: 0.0160
top acc: 0.0519 ::: bot acc: 0.0252
current epoch: 47
train loss is 0.000710
average val loss: 0.000584, accuracy: 0.0260
average test loss: 0.000694, accuracy: 0.0294
case acc: 0.024852257
case acc: 0.021925116
case acc: 0.03504336
case acc: 0.029742552
case acc: 0.035483617
case acc: 0.029105904
top acc: 0.0374 ::: bot acc: 0.0284
top acc: 0.0339 ::: bot acc: 0.0237
top acc: 0.0597 ::: bot acc: 0.0309
top acc: 0.0548 ::: bot acc: 0.0158
top acc: 0.0663 ::: bot acc: 0.0177
top acc: 0.0496 ::: bot acc: 0.0273
current epoch: 48
train loss is 0.000718
average val loss: 0.000593, accuracy: 0.0265
average test loss: 0.000647, accuracy: 0.0283
case acc: 0.024156542
case acc: 0.020851035
case acc: 0.03393414
case acc: 0.02809745
case acc: 0.03422051
case acc: 0.028546715
top acc: 0.0324 ::: bot acc: 0.0335
top acc: 0.0277 ::: bot acc: 0.0296
top acc: 0.0534 ::: bot acc: 0.0370
top acc: 0.0508 ::: bot acc: 0.0194
top acc: 0.0630 ::: bot acc: 0.0203
top acc: 0.0466 ::: bot acc: 0.0304
current epoch: 49
train loss is 0.000736
average val loss: 0.000633, accuracy: 0.0278
average test loss: 0.000621, accuracy: 0.0277
case acc: 0.02429566
case acc: 0.021037534
case acc: 0.0331199
case acc: 0.026855769
case acc: 0.03264475
case acc: 0.02805645
top acc: 0.0267 ::: bot acc: 0.0393
top acc: 0.0208 ::: bot acc: 0.0367
top acc: 0.0464 ::: bot acc: 0.0444
top acc: 0.0454 ::: bot acc: 0.0247
top acc: 0.0591 ::: bot acc: 0.0238
top acc: 0.0429 ::: bot acc: 0.0340
current epoch: 50
train loss is 0.000771
average val loss: 0.000715, accuracy: 0.0302
average test loss: 0.000629, accuracy: 0.0278
case acc: 0.025180636
case acc: 0.023224281
case acc: 0.032943454
case acc: 0.02595145
case acc: 0.031719297
case acc: 0.027929569
top acc: 0.0202 ::: bot acc: 0.0454
top acc: 0.0133 ::: bot acc: 0.0444
top acc: 0.0382 ::: bot acc: 0.0525
top acc: 0.0398 ::: bot acc: 0.0302
top acc: 0.0551 ::: bot acc: 0.0280
top acc: 0.0392 ::: bot acc: 0.0377
LME_Co_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6774 6774 6774
1.7082474 -0.6288155 0.21141115 -0.19947179
Validation: 756 756 756
Testing: 768 768 768
pre-processing time: 0.00023746490478515625
the split date is 2011-07-01
train dropout: 0.6 test dropout: 0.1
net initializing with time: 0.0022735595703125
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.013074
average val loss: 0.005182, accuracy: 0.0932
average test loss: 0.005122, accuracy: 0.0931
case acc: 0.13084258
case acc: 0.08892536
case acc: 0.09137852
case acc: 0.079641625
case acc: 0.11498471
case acc: 0.05300232
top acc: 0.0967 ::: bot acc: 0.1604
top acc: 0.1164 ::: bot acc: 0.0616
top acc: 0.0492 ::: bot acc: 0.1323
top acc: 0.0554 ::: bot acc: 0.1051
top acc: 0.0812 ::: bot acc: 0.1514
top acc: 0.0224 ::: bot acc: 0.0888
current epoch: 2
train loss is 0.007998
average val loss: 0.003342, accuracy: 0.0578
average test loss: 0.003274, accuracy: 0.0572
case acc: 0.038567454
case acc: 0.17269461
case acc: 0.032441884
case acc: 0.02210091
case acc: 0.033757214
case acc: 0.043473396
top acc: 0.0214 ::: bot acc: 0.0600
top acc: 0.1999 ::: bot acc: 0.1458
top acc: 0.0538 ::: bot acc: 0.0327
top acc: 0.0371 ::: bot acc: 0.0149
top acc: 0.0140 ::: bot acc: 0.0633
top acc: 0.0758 ::: bot acc: 0.0200
current epoch: 3
train loss is 0.008218
average val loss: 0.008292, accuracy: 0.1069
average test loss: 0.008274, accuracy: 0.1072
case acc: 0.056587413
case acc: 0.24401686
case acc: 0.09545988
case acc: 0.090594456
case acc: 0.051797114
case acc: 0.10497541
top acc: 0.0914 ::: bot acc: 0.0260
top acc: 0.2703 ::: bot acc: 0.2172
top acc: 0.1386 ::: bot acc: 0.0527
top acc: 0.1144 ::: bot acc: 0.0659
top acc: 0.0835 ::: bot acc: 0.0194
top acc: 0.1458 ::: bot acc: 0.0650
current epoch: 4
train loss is 0.010033
average val loss: 0.016777, accuracy: 0.1701
average test loss: 0.016753, accuracy: 0.1704
case acc: 0.12763306
case acc: 0.3008749
case acc: 0.16506964
case acc: 0.1528162
case acc: 0.11594565
case acc: 0.15987392
top acc: 0.1625 ::: bot acc: 0.0983
top acc: 0.3274 ::: bot acc: 0.2739
top acc: 0.2096 ::: bot acc: 0.1216
top acc: 0.1773 ::: bot acc: 0.1272
top acc: 0.1502 ::: bot acc: 0.0793
top acc: 0.2010 ::: bot acc: 0.1188
current epoch: 5
train loss is 0.012621
average val loss: 0.016078, accuracy: 0.1675
average test loss: 0.016055, accuracy: 0.1678
case acc: 0.13064931
case acc: 0.28889662
case acc: 0.16578655
case acc: 0.14902781
case acc: 0.12067589
case acc: 0.15195486
top acc: 0.1651 ::: bot acc: 0.1007
top acc: 0.3155 ::: bot acc: 0.2622
top acc: 0.2102 ::: bot acc: 0.1221
top acc: 0.1731 ::: bot acc: 0.1238
top acc: 0.1544 ::: bot acc: 0.0846
top acc: 0.1933 ::: bot acc: 0.1107
current epoch: 6
train loss is 0.013133
average val loss: 0.005694, accuracy: 0.0886
average test loss: 0.005644, accuracy: 0.0889
case acc: 0.055480815
case acc: 0.1964809
case acc: 0.08517283
case acc: 0.069064274
case acc: 0.05368542
case acc: 0.073551565
top acc: 0.0898 ::: bot acc: 0.0256
top acc: 0.2235 ::: bot acc: 0.1693
top acc: 0.1298 ::: bot acc: 0.0417
top acc: 0.0929 ::: bot acc: 0.0438
top acc: 0.0858 ::: bot acc: 0.0209
top acc: 0.1131 ::: bot acc: 0.0365
current epoch: 7
train loss is 0.008337
average val loss: 0.001717, accuracy: 0.0434
average test loss: 0.001629, accuracy: 0.0411
case acc: 0.027242865
case acc: 0.11335297
case acc: 0.033014715
case acc: 0.018012844
case acc: 0.025008872
case acc: 0.030250035
top acc: 0.0259 ::: bot acc: 0.0408
top acc: 0.1405 ::: bot acc: 0.0863
top acc: 0.0554 ::: bot acc: 0.0320
top acc: 0.0242 ::: bot acc: 0.0253
top acc: 0.0289 ::: bot acc: 0.0411
top acc: 0.0463 ::: bot acc: 0.0357
current epoch: 8
train loss is 0.003913
average val loss: 0.001406, accuracy: 0.0405
average test loss: 0.001321, accuracy: 0.0384
case acc: 0.029219002
case acc: 0.09568988
case acc: 0.032226175
case acc: 0.018366478
case acc: 0.024773091
case acc: 0.030275796
top acc: 0.0238 ::: bot acc: 0.0447
top acc: 0.1222 ::: bot acc: 0.0689
top acc: 0.0461 ::: bot acc: 0.0420
top acc: 0.0187 ::: bot acc: 0.0305
top acc: 0.0320 ::: bot acc: 0.0385
top acc: 0.0434 ::: bot acc: 0.0388
current epoch: 9
train loss is 0.002625
average val loss: 0.001654, accuracy: 0.0439
average test loss: 0.001578, accuracy: 0.0420
case acc: 0.022736246
case acc: 0.10513969
case acc: 0.03438223
case acc: 0.021849677
case acc: 0.0327421
case acc: 0.035268057
top acc: 0.0410 ::: bot acc: 0.0233
top acc: 0.1317 ::: bot acc: 0.0784
top acc: 0.0611 ::: bot acc: 0.0268
top acc: 0.0377 ::: bot acc: 0.0132
top acc: 0.0559 ::: bot acc: 0.0181
top acc: 0.0636 ::: bot acc: 0.0208
current epoch: 10
train loss is 0.002470
average val loss: 0.001635, accuracy: 0.0442
average test loss: 0.001573, accuracy: 0.0429
case acc: 0.023683775
case acc: 0.10079473
case acc: 0.03471236
case acc: 0.023927828
case acc: 0.036404237
case acc: 0.037984684
top acc: 0.0457 ::: bot acc: 0.0189
top acc: 0.1272 ::: bot acc: 0.0741
top acc: 0.0625 ::: bot acc: 0.0249
top acc: 0.0413 ::: bot acc: 0.0121
top acc: 0.0626 ::: bot acc: 0.0163
top acc: 0.0683 ::: bot acc: 0.0202
current epoch: 11
train loss is 0.002271
average val loss: 0.001464, accuracy: 0.0423
average test loss: 0.001399, accuracy: 0.0409
case acc: 0.02370262
case acc: 0.09132792
case acc: 0.0338434
case acc: 0.02293605
case acc: 0.036347378
case acc: 0.03694371
top acc: 0.0452 ::: bot acc: 0.0192
top acc: 0.1185 ::: bot acc: 0.0643
top acc: 0.0591 ::: bot acc: 0.0282
top acc: 0.0401 ::: bot acc: 0.0119
top acc: 0.0622 ::: bot acc: 0.0160
top acc: 0.0664 ::: bot acc: 0.0201
current epoch: 12
train loss is 0.002021
average val loss: 0.001386, accuracy: 0.0416
average test loss: 0.001314, accuracy: 0.0401
case acc: 0.02386008
case acc: 0.084515855
case acc: 0.033745583
case acc: 0.023261467
case acc: 0.037882864
case acc: 0.037311397
top acc: 0.0467 ::: bot acc: 0.0179
top acc: 0.1114 ::: bot acc: 0.0577
top acc: 0.0577 ::: bot acc: 0.0298
top acc: 0.0402 ::: bot acc: 0.0120
top acc: 0.0647 ::: bot acc: 0.0159
top acc: 0.0668 ::: bot acc: 0.0200
current epoch: 13
train loss is 0.001881
average val loss: 0.001296, accuracy: 0.0405
average test loss: 0.001223, accuracy: 0.0390
case acc: 0.023983695
case acc: 0.07765181
case acc: 0.033198617
case acc: 0.023325581
case acc: 0.038346432
case acc: 0.037392523
top acc: 0.0475 ::: bot acc: 0.0166
top acc: 0.1046 ::: bot acc: 0.0509
top acc: 0.0565 ::: bot acc: 0.0309
top acc: 0.0406 ::: bot acc: 0.0121
top acc: 0.0650 ::: bot acc: 0.0157
top acc: 0.0668 ::: bot acc: 0.0201
current epoch: 14
train loss is 0.001605
average val loss: 0.001271, accuracy: 0.0404
average test loss: 0.001202, accuracy: 0.0391
case acc: 0.025069589
case acc: 0.072985485
case acc: 0.033600263
case acc: 0.024510086
case acc: 0.0399443
case acc: 0.038784627
top acc: 0.0508 ::: bot acc: 0.0140
top acc: 0.0997 ::: bot acc: 0.0461
top acc: 0.0573 ::: bot acc: 0.0304
top acc: 0.0419 ::: bot acc: 0.0120
top acc: 0.0676 ::: bot acc: 0.0157
top acc: 0.0692 ::: bot acc: 0.0202
current epoch: 15
train loss is 0.001506
average val loss: 0.001249, accuracy: 0.0402
average test loss: 0.001187, accuracy: 0.0391
case acc: 0.026209813
case acc: 0.06936307
case acc: 0.033673923
case acc: 0.025473772
case acc: 0.040757984
case acc: 0.03927755
top acc: 0.0531 ::: bot acc: 0.0121
top acc: 0.0960 ::: bot acc: 0.0425
top acc: 0.0582 ::: bot acc: 0.0292
top acc: 0.0441 ::: bot acc: 0.0114
top acc: 0.0693 ::: bot acc: 0.0156
top acc: 0.0698 ::: bot acc: 0.0200
current epoch: 16
train loss is 0.001402
average val loss: 0.001185, accuracy: 0.0393
average test loss: 0.001128, accuracy: 0.0382
case acc: 0.02648158
case acc: 0.06452678
case acc: 0.03361743
case acc: 0.025753198
case acc: 0.040215828
case acc: 0.03890264
top acc: 0.0538 ::: bot acc: 0.0118
top acc: 0.0911 ::: bot acc: 0.0378
top acc: 0.0584 ::: bot acc: 0.0294
top acc: 0.0442 ::: bot acc: 0.0119
top acc: 0.0684 ::: bot acc: 0.0156
top acc: 0.0695 ::: bot acc: 0.0198
current epoch: 17
train loss is 0.001332
average val loss: 0.001149, accuracy: 0.0389
average test loss: 0.001091, accuracy: 0.0377
case acc: 0.027068045
case acc: 0.060480163
case acc: 0.03373386
case acc: 0.02622937
case acc: 0.040053293
case acc: 0.0387095
top acc: 0.0550 ::: bot acc: 0.0115
top acc: 0.0877 ::: bot acc: 0.0335
top acc: 0.0584 ::: bot acc: 0.0295
top acc: 0.0447 ::: bot acc: 0.0117
top acc: 0.0680 ::: bot acc: 0.0159
top acc: 0.0695 ::: bot acc: 0.0196
current epoch: 18
train loss is 0.001276
average val loss: 0.001070, accuracy: 0.0375
average test loss: 0.001006, accuracy: 0.0361
case acc: 0.026779767
case acc: 0.05490343
case acc: 0.03339228
case acc: 0.025340054
case acc: 0.038398374
case acc: 0.037549876
top acc: 0.0545 ::: bot acc: 0.0117
top acc: 0.0816 ::: bot acc: 0.0279
top acc: 0.0574 ::: bot acc: 0.0301
top acc: 0.0438 ::: bot acc: 0.0115
top acc: 0.0653 ::: bot acc: 0.0160
top acc: 0.0677 ::: bot acc: 0.0196
current epoch: 19
train loss is 0.001169
average val loss: 0.001117, accuracy: 0.0385
average test loss: 0.001063, accuracy: 0.0375
case acc: 0.028802902
case acc: 0.05484888
case acc: 0.034277394
case acc: 0.027984645
case acc: 0.039407194
case acc: 0.039419986
top acc: 0.0586 ::: bot acc: 0.0099
top acc: 0.0817 ::: bot acc: 0.0280
top acc: 0.0608 ::: bot acc: 0.0266
top acc: 0.0469 ::: bot acc: 0.0122
top acc: 0.0672 ::: bot acc: 0.0156
top acc: 0.0702 ::: bot acc: 0.0198
current epoch: 20
train loss is 0.001150
average val loss: 0.001158, accuracy: 0.0394
average test loss: 0.001106, accuracy: 0.0385
case acc: 0.030594777
case acc: 0.054341048
case acc: 0.03528805
case acc: 0.030089183
case acc: 0.040220223
case acc: 0.04051517
top acc: 0.0615 ::: bot acc: 0.0092
top acc: 0.0813 ::: bot acc: 0.0275
top acc: 0.0642 ::: bot acc: 0.0237
top acc: 0.0501 ::: bot acc: 0.0131
top acc: 0.0681 ::: bot acc: 0.0156
top acc: 0.0716 ::: bot acc: 0.0201
current epoch: 21
train loss is 0.001109
average val loss: 0.001140, accuracy: 0.0391
average test loss: 0.001087, accuracy: 0.0382
case acc: 0.031577475
case acc: 0.051876634
case acc: 0.03557431
case acc: 0.030499935
case acc: 0.03965766
case acc: 0.04001376
top acc: 0.0625 ::: bot acc: 0.0094
top acc: 0.0789 ::: bot acc: 0.0253
top acc: 0.0649 ::: bot acc: 0.0227
top acc: 0.0506 ::: bot acc: 0.0132
top acc: 0.0677 ::: bot acc: 0.0155
top acc: 0.0712 ::: bot acc: 0.0197
current epoch: 22
train loss is 0.001089
average val loss: 0.001133, accuracy: 0.0389
average test loss: 0.001081, accuracy: 0.0381
case acc: 0.032338984
case acc: 0.0502228
case acc: 0.036120236
case acc: 0.030928873
case acc: 0.039339714
case acc: 0.039574392
top acc: 0.0637 ::: bot acc: 0.0094
top acc: 0.0772 ::: bot acc: 0.0235
top acc: 0.0665 ::: bot acc: 0.0216
top acc: 0.0514 ::: bot acc: 0.0133
top acc: 0.0669 ::: bot acc: 0.0156
top acc: 0.0704 ::: bot acc: 0.0197
current epoch: 23
train loss is 0.001045
average val loss: 0.001204, accuracy: 0.0404
average test loss: 0.001149, accuracy: 0.0396
case acc: 0.034668915
case acc: 0.051155753
case acc: 0.03766098
case acc: 0.032953
case acc: 0.040625304
case acc: 0.0404137
top acc: 0.0672 ::: bot acc: 0.0094
top acc: 0.0780 ::: bot acc: 0.0244
top acc: 0.0706 ::: bot acc: 0.0185
top acc: 0.0536 ::: bot acc: 0.0145
top acc: 0.0688 ::: bot acc: 0.0157
top acc: 0.0717 ::: bot acc: 0.0197
current epoch: 24
train loss is 0.001046
average val loss: 0.001251, accuracy: 0.0413
average test loss: 0.001199, accuracy: 0.0407
case acc: 0.03643517
case acc: 0.05141031
case acc: 0.039086394
case acc: 0.03462196
case acc: 0.04130435
case acc: 0.0411684
top acc: 0.0698 ::: bot acc: 0.0098
top acc: 0.0780 ::: bot acc: 0.0250
top acc: 0.0733 ::: bot acc: 0.0168
top acc: 0.0554 ::: bot acc: 0.0154
top acc: 0.0699 ::: bot acc: 0.0159
top acc: 0.0726 ::: bot acc: 0.0199
current epoch: 25
train loss is 0.001043
average val loss: 0.001318, accuracy: 0.0427
average test loss: 0.001270, accuracy: 0.0421
case acc: 0.038697865
case acc: 0.052185103
case acc: 0.04104554
case acc: 0.036651872
case acc: 0.042325027
case acc: 0.041860204
top acc: 0.0723 ::: bot acc: 0.0112
top acc: 0.0790 ::: bot acc: 0.0257
top acc: 0.0767 ::: bot acc: 0.0156
top acc: 0.0580 ::: bot acc: 0.0165
top acc: 0.0716 ::: bot acc: 0.0158
top acc: 0.0738 ::: bot acc: 0.0197
current epoch: 26
train loss is 0.001045
average val loss: 0.001360, accuracy: 0.0434
average test loss: 0.001305, accuracy: 0.0429
case acc: 0.040107198
case acc: 0.05216983
case acc: 0.04232988
case acc: 0.037964094
case acc: 0.04262077
case acc: 0.042193618
top acc: 0.0740 ::: bot acc: 0.0119
top acc: 0.0788 ::: bot acc: 0.0255
top acc: 0.0789 ::: bot acc: 0.0152
top acc: 0.0594 ::: bot acc: 0.0178
top acc: 0.0717 ::: bot acc: 0.0158
top acc: 0.0742 ::: bot acc: 0.0198
current epoch: 27
train loss is 0.001049
average val loss: 0.001437, accuracy: 0.0449
average test loss: 0.001385, accuracy: 0.0445
case acc: 0.042368263
case acc: 0.052930318
case acc: 0.044039287
case acc: 0.040008493
case acc: 0.04425324
case acc: 0.04319094
top acc: 0.0766 ::: bot acc: 0.0135
top acc: 0.0795 ::: bot acc: 0.0261
top acc: 0.0818 ::: bot acc: 0.0143
top acc: 0.0620 ::: bot acc: 0.0190
top acc: 0.0739 ::: bot acc: 0.0160
top acc: 0.0759 ::: bot acc: 0.0197
current epoch: 28
train loss is 0.001064
average val loss: 0.001550, accuracy: 0.0470
average test loss: 0.001504, accuracy: 0.0468
case acc: 0.04547974
case acc: 0.054536607
case acc: 0.0466552
case acc: 0.04265292
case acc: 0.04643822
case acc: 0.044912603
top acc: 0.0803 ::: bot acc: 0.0160
top acc: 0.0813 ::: bot acc: 0.0278
top acc: 0.0856 ::: bot acc: 0.0145
top acc: 0.0651 ::: bot acc: 0.0208
top acc: 0.0769 ::: bot acc: 0.0167
top acc: 0.0780 ::: bot acc: 0.0204
current epoch: 29
train loss is 0.001043
average val loss: 0.001505, accuracy: 0.0462
average test loss: 0.001456, accuracy: 0.0459
case acc: 0.044730637
case acc: 0.052435935
case acc: 0.04648941
case acc: 0.04204925
case acc: 0.045673873
case acc: 0.0437866
top acc: 0.0793 ::: bot acc: 0.0155
top acc: 0.0790 ::: bot acc: 0.0261
top acc: 0.0855 ::: bot acc: 0.0142
top acc: 0.0642 ::: bot acc: 0.0205
top acc: 0.0760 ::: bot acc: 0.0163
top acc: 0.0763 ::: bot acc: 0.0199
current epoch: 30
train loss is 0.001037
average val loss: 0.001499, accuracy: 0.0461
average test loss: 0.001453, accuracy: 0.0458
case acc: 0.044975564
case acc: 0.051366895
case acc: 0.046953727
case acc: 0.04215763
case acc: 0.045843642
case acc: 0.04321255
top acc: 0.0796 ::: bot acc: 0.0154
top acc: 0.0783 ::: bot acc: 0.0246
top acc: 0.0861 ::: bot acc: 0.0142
top acc: 0.0644 ::: bot acc: 0.0206
top acc: 0.0764 ::: bot acc: 0.0163
top acc: 0.0756 ::: bot acc: 0.0199
current epoch: 31
train loss is 0.001013
average val loss: 0.001469, accuracy: 0.0455
average test loss: 0.001427, accuracy: 0.0452
case acc: 0.0443722
case acc: 0.049882796
case acc: 0.047078557
case acc: 0.04193487
case acc: 0.0455019
case acc: 0.04268661
top acc: 0.0790 ::: bot acc: 0.0152
top acc: 0.0767 ::: bot acc: 0.0231
top acc: 0.0863 ::: bot acc: 0.0144
top acc: 0.0640 ::: bot acc: 0.0204
top acc: 0.0756 ::: bot acc: 0.0164
top acc: 0.0751 ::: bot acc: 0.0198
current epoch: 32
train loss is 0.000978
average val loss: 0.001498, accuracy: 0.0460
average test loss: 0.001448, accuracy: 0.0457
case acc: 0.044874184
case acc: 0.049662817
case acc: 0.04805503
case acc: 0.04283007
case acc: 0.045918725
case acc: 0.04265912
top acc: 0.0794 ::: bot acc: 0.0156
top acc: 0.0765 ::: bot acc: 0.0230
top acc: 0.0875 ::: bot acc: 0.0144
top acc: 0.0651 ::: bot acc: 0.0211
top acc: 0.0764 ::: bot acc: 0.0166
top acc: 0.0749 ::: bot acc: 0.0198
current epoch: 33
train loss is 0.000958
average val loss: 0.001553, accuracy: 0.0471
average test loss: 0.001501, accuracy: 0.0467
case acc: 0.046037745
case acc: 0.050233506
case acc: 0.049192604
case acc: 0.044200554
case acc: 0.04693948
case acc: 0.04337123
top acc: 0.0807 ::: bot acc: 0.0167
top acc: 0.0772 ::: bot acc: 0.0234
top acc: 0.0893 ::: bot acc: 0.0146
top acc: 0.0666 ::: bot acc: 0.0221
top acc: 0.0779 ::: bot acc: 0.0167
top acc: 0.0758 ::: bot acc: 0.0201
current epoch: 34
train loss is 0.000976
average val loss: 0.001537, accuracy: 0.0467
average test loss: 0.001490, accuracy: 0.0464
case acc: 0.04578068
case acc: 0.04903489
case acc: 0.049364492
case acc: 0.044214956
case acc: 0.047013514
case acc: 0.043234065
top acc: 0.0802 ::: bot acc: 0.0166
top acc: 0.0758 ::: bot acc: 0.0225
top acc: 0.0896 ::: bot acc: 0.0146
top acc: 0.0665 ::: bot acc: 0.0222
top acc: 0.0779 ::: bot acc: 0.0166
top acc: 0.0757 ::: bot acc: 0.0200
current epoch: 35
train loss is 0.000990
average val loss: 0.001627, accuracy: 0.0483
average test loss: 0.001577, accuracy: 0.0481
case acc: 0.047530696
case acc: 0.049855284
case acc: 0.05099513
case acc: 0.046269428
case acc: 0.04895321
case acc: 0.044779815
top acc: 0.0822 ::: bot acc: 0.0181
top acc: 0.0764 ::: bot acc: 0.0231
top acc: 0.0919 ::: bot acc: 0.0148
top acc: 0.0688 ::: bot acc: 0.0238
top acc: 0.0803 ::: bot acc: 0.0175
top acc: 0.0777 ::: bot acc: 0.0203
current epoch: 36
train loss is 0.000948
average val loss: 0.001502, accuracy: 0.0461
average test loss: 0.001450, accuracy: 0.0456
case acc: 0.044461273
case acc: 0.046100028
case acc: 0.04907497
case acc: 0.044193864
case acc: 0.04715998
case acc: 0.042696185
top acc: 0.0789 ::: bot acc: 0.0154
top acc: 0.0727 ::: bot acc: 0.0198
top acc: 0.0893 ::: bot acc: 0.0144
top acc: 0.0666 ::: bot acc: 0.0220
top acc: 0.0782 ::: bot acc: 0.0168
top acc: 0.0751 ::: bot acc: 0.0195
current epoch: 37
train loss is 0.000909
average val loss: 0.001413, accuracy: 0.0444
average test loss: 0.001358, accuracy: 0.0437
case acc: 0.042105
case acc: 0.04299996
case acc: 0.047549926
case acc: 0.04231455
case acc: 0.045944992
case acc: 0.041414604
top acc: 0.0765 ::: bot acc: 0.0134
top acc: 0.0696 ::: bot acc: 0.0168
top acc: 0.0871 ::: bot acc: 0.0143
top acc: 0.0646 ::: bot acc: 0.0206
top acc: 0.0767 ::: bot acc: 0.0162
top acc: 0.0731 ::: bot acc: 0.0195
current epoch: 38
train loss is 0.000867
average val loss: 0.001317, accuracy: 0.0426
average test loss: 0.001262, accuracy: 0.0418
case acc: 0.03985903
case acc: 0.039796047
case acc: 0.045861945
case acc: 0.04059136
case acc: 0.044539552
case acc: 0.03987936
top acc: 0.0738 ::: bot acc: 0.0116
top acc: 0.0662 ::: bot acc: 0.0140
top acc: 0.0848 ::: bot acc: 0.0141
top acc: 0.0626 ::: bot acc: 0.0191
top acc: 0.0747 ::: bot acc: 0.0162
top acc: 0.0707 ::: bot acc: 0.0194
current epoch: 39
train loss is 0.000842
average val loss: 0.001241, accuracy: 0.0411
average test loss: 0.001184, accuracy: 0.0402
case acc: 0.037916083
case acc: 0.03722044
case acc: 0.044741727
case acc: 0.039083257
case acc: 0.043377552
case acc: 0.038713913
top acc: 0.0712 ::: bot acc: 0.0106
top acc: 0.0632 ::: bot acc: 0.0121
top acc: 0.0827 ::: bot acc: 0.0146
top acc: 0.0605 ::: bot acc: 0.0184
top acc: 0.0729 ::: bot acc: 0.0160
top acc: 0.0690 ::: bot acc: 0.0196
current epoch: 40
train loss is 0.000812
average val loss: 0.001156, accuracy: 0.0394
average test loss: 0.001104, accuracy: 0.0384
case acc: 0.035892777
case acc: 0.034568828
case acc: 0.04315277
case acc: 0.037519515
case acc: 0.041933227
case acc: 0.03746113
top acc: 0.0690 ::: bot acc: 0.0099
top acc: 0.0603 ::: bot acc: 0.0102
top acc: 0.0802 ::: bot acc: 0.0148
top acc: 0.0588 ::: bot acc: 0.0173
top acc: 0.0708 ::: bot acc: 0.0159
top acc: 0.0673 ::: bot acc: 0.0195
current epoch: 41
train loss is 0.000797
average val loss: 0.001156, accuracy: 0.0393
average test loss: 0.001102, accuracy: 0.0383
case acc: 0.03547513
case acc: 0.03373398
case acc: 0.043117326
case acc: 0.037705347
case acc: 0.042178698
case acc: 0.037784822
top acc: 0.0683 ::: bot acc: 0.0096
top acc: 0.0592 ::: bot acc: 0.0098
top acc: 0.0802 ::: bot acc: 0.0149
top acc: 0.0593 ::: bot acc: 0.0172
top acc: 0.0712 ::: bot acc: 0.0158
top acc: 0.0677 ::: bot acc: 0.0196
current epoch: 42
train loss is 0.000780
average val loss: 0.001069, accuracy: 0.0376
average test loss: 0.001015, accuracy: 0.0364
case acc: 0.033360153
case acc: 0.030959306
case acc: 0.04133981
case acc: 0.03575097
case acc: 0.040679358
case acc: 0.036443476
top acc: 0.0653 ::: bot acc: 0.0094
top acc: 0.0558 ::: bot acc: 0.0085
top acc: 0.0773 ::: bot acc: 0.0154
top acc: 0.0570 ::: bot acc: 0.0161
top acc: 0.0690 ::: bot acc: 0.0157
top acc: 0.0654 ::: bot acc: 0.0200
current epoch: 43
train loss is 0.000747
average val loss: 0.000990, accuracy: 0.0360
average test loss: 0.000936, accuracy: 0.0346
case acc: 0.0314288
case acc: 0.028498989
case acc: 0.039676655
case acc: 0.033866353
case acc: 0.03909439
case acc: 0.0351732
top acc: 0.0624 ::: bot acc: 0.0096
top acc: 0.0521 ::: bot acc: 0.0083
top acc: 0.0744 ::: bot acc: 0.0165
top acc: 0.0547 ::: bot acc: 0.0148
top acc: 0.0667 ::: bot acc: 0.0157
top acc: 0.0634 ::: bot acc: 0.0207
current epoch: 44
train loss is 0.000726
average val loss: 0.000893, accuracy: 0.0339
average test loss: 0.000838, accuracy: 0.0323
case acc: 0.028719533
case acc: 0.02579084
case acc: 0.037716903
case acc: 0.031101089
case acc: 0.03695489
case acc: 0.033659097
top acc: 0.0583 ::: bot acc: 0.0099
top acc: 0.0477 ::: bot acc: 0.0093
top acc: 0.0705 ::: bot acc: 0.0186
top acc: 0.0514 ::: bot acc: 0.0133
top acc: 0.0635 ::: bot acc: 0.0160
top acc: 0.0601 ::: bot acc: 0.0225
current epoch: 45
train loss is 0.000709
average val loss: 0.000882, accuracy: 0.0336
average test loss: 0.000823, accuracy: 0.0319
case acc: 0.028264903
case acc: 0.024933264
case acc: 0.03713322
case acc: 0.030780667
case acc: 0.036901586
case acc: 0.03362697
top acc: 0.0573 ::: bot acc: 0.0103
top acc: 0.0464 ::: bot acc: 0.0095
top acc: 0.0693 ::: bot acc: 0.0192
top acc: 0.0509 ::: bot acc: 0.0133
top acc: 0.0634 ::: bot acc: 0.0158
top acc: 0.0602 ::: bot acc: 0.0222
current epoch: 46
train loss is 0.000697
average val loss: 0.000789, accuracy: 0.0317
average test loss: 0.000730, accuracy: 0.0297
case acc: 0.02582296
case acc: 0.02260641
case acc: 0.03535636
case acc: 0.027799606
case acc: 0.034557436
case acc: 0.032247066
top acc: 0.0526 ::: bot acc: 0.0126
top acc: 0.0409 ::: bot acc: 0.0131
top acc: 0.0642 ::: bot acc: 0.0239
top acc: 0.0470 ::: bot acc: 0.0121
top acc: 0.0594 ::: bot acc: 0.0167
top acc: 0.0569 ::: bot acc: 0.0249
current epoch: 47
train loss is 0.000684
average val loss: 0.000725, accuracy: 0.0302
average test loss: 0.000662, accuracy: 0.0280
case acc: 0.024063028
case acc: 0.021032142
case acc: 0.034098282
case acc: 0.025254339
case acc: 0.0324415
case acc: 0.03131505
top acc: 0.0481 ::: bot acc: 0.0163
top acc: 0.0366 ::: bot acc: 0.0175
top acc: 0.0598 ::: bot acc: 0.0282
top acc: 0.0434 ::: bot acc: 0.0116
top acc: 0.0558 ::: bot acc: 0.0178
top acc: 0.0538 ::: bot acc: 0.0278
current epoch: 48
train loss is 0.000679
average val loss: 0.000668, accuracy: 0.0289
average test loss: 0.000605, accuracy: 0.0266
case acc: 0.023076251
case acc: 0.019865146
case acc: 0.032873854
case acc: 0.022820696
case acc: 0.030532487
case acc: 0.030485298
top acc: 0.0434 ::: bot acc: 0.0208
top acc: 0.0312 ::: bot acc: 0.0226
top acc: 0.0544 ::: bot acc: 0.0334
top acc: 0.0397 ::: bot acc: 0.0121
top acc: 0.0519 ::: bot acc: 0.0199
top acc: 0.0504 ::: bot acc: 0.0310
current epoch: 49
train loss is 0.000688
average val loss: 0.000660, accuracy: 0.0287
average test loss: 0.000595, accuracy: 0.0263
case acc: 0.022917021
case acc: 0.01956971
case acc: 0.032477655
case acc: 0.02216418
case acc: 0.030532354
case acc: 0.030411916
top acc: 0.0418 ::: bot acc: 0.0225
top acc: 0.0291 ::: bot acc: 0.0248
top acc: 0.0520 ::: bot acc: 0.0358
top acc: 0.0384 ::: bot acc: 0.0123
top acc: 0.0519 ::: bot acc: 0.0201
top acc: 0.0504 ::: bot acc: 0.0308
current epoch: 50
train loss is 0.000692
average val loss: 0.000634, accuracy: 0.0282
average test loss: 0.000570, accuracy: 0.0257
case acc: 0.022657651
case acc: 0.019497255
case acc: 0.032138094
case acc: 0.02056577
case acc: 0.029365845
case acc: 0.030251207
top acc: 0.0378 ::: bot acc: 0.0263
top acc: 0.0241 ::: bot acc: 0.0292
top acc: 0.0472 ::: bot acc: 0.0408
top acc: 0.0353 ::: bot acc: 0.0142
top acc: 0.0494 ::: bot acc: 0.0216
top acc: 0.0485 ::: bot acc: 0.0329

		{"drop_out": 0.6, "drop_out_mc": 0.15, "repeat_mc": 50, "hidden": 30, "embedding_size": 5, "batch": 512, "lag": 4}
LME_Co_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6768 6768 6768
1.8562728 -0.6288155 0.2585643 -0.19947179
Validation: 756 756 756
Testing: 774 774 774
pre-processing time: 0.0002193450927734375
the split date is 2009-07-01
train dropout: 0.6 test dropout: 0.15
net initializing with time: 0.003664255142211914
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.013014
average val loss: 0.005927, accuracy: 0.1019
average test loss: 0.006495, accuracy: 0.1075
case acc: 0.15179232
case acc: 0.07400286
case acc: 0.11058825
case acc: 0.103428856
case acc: 0.13491845
case acc: 0.0700191
top acc: 0.1354 ::: bot acc: 0.1670
top acc: 0.1021 ::: bot acc: 0.0475
top acc: 0.0722 ::: bot acc: 0.1467
top acc: 0.0786 ::: bot acc: 0.1276
top acc: 0.1112 ::: bot acc: 0.1612
top acc: 0.0488 ::: bot acc: 0.0917
current epoch: 2
train loss is 0.008574
average val loss: 0.002905, accuracy: 0.0543
average test loss: 0.002797, accuracy: 0.0543
case acc: 0.050981987
case acc: 0.15788811
case acc: 0.028635161
case acc: 0.02107718
case acc: 0.047346335
case acc: 0.01968297
top acc: 0.0337 ::: bot acc: 0.0669
top acc: 0.1859 ::: bot acc: 0.1312
top acc: 0.0264 ::: bot acc: 0.0464
top acc: 0.0159 ::: bot acc: 0.0367
top acc: 0.0237 ::: bot acc: 0.0731
top acc: 0.0335 ::: bot acc: 0.0111
current epoch: 3
train loss is 0.008840
average val loss: 0.006953, accuracy: 0.0952
average test loss: 0.006299, accuracy: 0.0879
case acc: 0.036465336
case acc: 0.2302065
case acc: 0.076069236
case acc: 0.068072684
case acc: 0.03234354
case acc: 0.084031604
top acc: 0.0536 ::: bot acc: 0.0208
top acc: 0.2579 ::: bot acc: 0.2047
top acc: 0.1140 ::: bot acc: 0.0410
top acc: 0.0928 ::: bot acc: 0.0434
top acc: 0.0530 ::: bot acc: 0.0118
top acc: 0.1051 ::: bot acc: 0.0615
current epoch: 4
train loss is 0.010151
average val loss: 0.016096, accuracy: 0.1667
average test loss: 0.014911, accuracy: 0.1596
case acc: 0.11614187
case acc: 0.29622087
case acc: 0.1546775
case acc: 0.13891184
case acc: 0.10397034
case acc: 0.14752272
top acc: 0.1329 ::: bot acc: 0.0993
top acc: 0.3237 ::: bot acc: 0.2699
top acc: 0.1927 ::: bot acc: 0.1190
top acc: 0.1642 ::: bot acc: 0.1134
top acc: 0.1285 ::: bot acc: 0.0771
top acc: 0.1685 ::: bot acc: 0.1253
current epoch: 5
train loss is 0.013143
average val loss: 0.014696, accuracy: 0.1596
average test loss: 0.013549, accuracy: 0.1526
case acc: 0.114638016
case acc: 0.27978432
case acc: 0.15099034
case acc: 0.13133678
case acc: 0.10382815
case acc: 0.13475762
top acc: 0.1308 ::: bot acc: 0.0992
top acc: 0.3077 ::: bot acc: 0.2536
top acc: 0.1888 ::: bot acc: 0.1157
top acc: 0.1564 ::: bot acc: 0.1058
top acc: 0.1276 ::: bot acc: 0.0771
top acc: 0.1566 ::: bot acc: 0.1119
current epoch: 6
train loss is 0.013632
average val loss: 0.004231, accuracy: 0.0723
average test loss: 0.003709, accuracy: 0.0653
case acc: 0.030843904
case acc: 0.17849994
case acc: 0.061971385
case acc: 0.04357757
case acc: 0.030537572
case acc: 0.04649902
top acc: 0.0472 ::: bot acc: 0.0155
top acc: 0.2058 ::: bot acc: 0.1523
top acc: 0.0993 ::: bot acc: 0.0276
top acc: 0.0676 ::: bot acc: 0.0206
top acc: 0.0514 ::: bot acc: 0.0106
top acc: 0.0669 ::: bot acc: 0.0248
current epoch: 7
train loss is 0.008228
average val loss: 0.001364, accuracy: 0.0388
average test loss: 0.001328, accuracy: 0.0401
case acc: 0.03398632
case acc: 0.097105585
case acc: 0.02880497
case acc: 0.028550174
case acc: 0.029804144
case acc: 0.022137173
top acc: 0.0167 ::: bot acc: 0.0505
top acc: 0.1250 ::: bot acc: 0.0707
top acc: 0.0271 ::: bot acc: 0.0463
top acc: 0.0109 ::: bot acc: 0.0501
top acc: 0.0096 ::: bot acc: 0.0546
top acc: 0.0080 ::: bot acc: 0.0414
current epoch: 8
train loss is 0.004006
average val loss: 0.001178, accuracy: 0.0345
average test loss: 0.001097, accuracy: 0.0347
case acc: 0.026634295
case acc: 0.091465645
case acc: 0.028315559
case acc: 0.02389416
case acc: 0.02081155
case acc: 0.016892226
top acc: 0.0113 ::: bot acc: 0.0420
top acc: 0.1187 ::: bot acc: 0.0653
top acc: 0.0288 ::: bot acc: 0.0447
top acc: 0.0108 ::: bot acc: 0.0433
top acc: 0.0124 ::: bot acc: 0.0395
top acc: 0.0111 ::: bot acc: 0.0316
current epoch: 9
train loss is 0.002938
average val loss: 0.001229, accuracy: 0.0345
average test loss: 0.001074, accuracy: 0.0324
case acc: 0.015573777
case acc: 0.095955424
case acc: 0.026771478
case acc: 0.01902044
case acc: 0.020283012
case acc: 0.016961144
top acc: 0.0092 ::: bot acc: 0.0265
top acc: 0.1231 ::: bot acc: 0.0703
top acc: 0.0396 ::: bot acc: 0.0339
top acc: 0.0212 ::: bot acc: 0.0301
top acc: 0.0309 ::: bot acc: 0.0207
top acc: 0.0272 ::: bot acc: 0.0164
current epoch: 10
train loss is 0.002743
average val loss: 0.001170, accuracy: 0.0345
average test loss: 0.001011, accuracy: 0.0319
case acc: 0.013421675
case acc: 0.0913486
case acc: 0.026845884
case acc: 0.018508693
case acc: 0.022846084
case acc: 0.018342772
top acc: 0.0117 ::: bot acc: 0.0222
top acc: 0.1187 ::: bot acc: 0.0654
top acc: 0.0405 ::: bot acc: 0.0332
top acc: 0.0244 ::: bot acc: 0.0263
top acc: 0.0379 ::: bot acc: 0.0152
top acc: 0.0302 ::: bot acc: 0.0138
current epoch: 11
train loss is 0.002594
average val loss: 0.001022, accuracy: 0.0328
average test loss: 0.000879, accuracy: 0.0304
case acc: 0.01333754
case acc: 0.08208172
case acc: 0.027275935
case acc: 0.018575706
case acc: 0.023070654
case acc: 0.018193629
top acc: 0.0117 ::: bot acc: 0.0223
top acc: 0.1095 ::: bot acc: 0.0560
top acc: 0.0378 ::: bot acc: 0.0361
top acc: 0.0233 ::: bot acc: 0.0273
top acc: 0.0388 ::: bot acc: 0.0139
top acc: 0.0296 ::: bot acc: 0.0143
current epoch: 12
train loss is 0.002314
average val loss: 0.000974, accuracy: 0.0328
average test loss: 0.000822, accuracy: 0.0299
case acc: 0.012132962
case acc: 0.07690071
case acc: 0.02707677
case acc: 0.018921686
case acc: 0.0250764
case acc: 0.019242467
top acc: 0.0141 ::: bot acc: 0.0184
top acc: 0.1043 ::: bot acc: 0.0511
top acc: 0.0378 ::: bot acc: 0.0358
top acc: 0.0257 ::: bot acc: 0.0257
top acc: 0.0426 ::: bot acc: 0.0117
top acc: 0.0321 ::: bot acc: 0.0127
current epoch: 13
train loss is 0.002115
average val loss: 0.000874, accuracy: 0.0315
average test loss: 0.000736, accuracy: 0.0287
case acc: 0.012301455
case acc: 0.0696259
case acc: 0.027215606
case acc: 0.018630615
case acc: 0.025501609
case acc: 0.019100608
top acc: 0.0151 ::: bot acc: 0.0178
top acc: 0.0971 ::: bot acc: 0.0437
top acc: 0.0358 ::: bot acc: 0.0377
top acc: 0.0249 ::: bot acc: 0.0262
top acc: 0.0434 ::: bot acc: 0.0118
top acc: 0.0315 ::: bot acc: 0.0133
current epoch: 14
train loss is 0.001897
average val loss: 0.000802, accuracy: 0.0305
average test loss: 0.000672, accuracy: 0.0278
case acc: 0.0119266175
case acc: 0.063634835
case acc: 0.02737089
case acc: 0.01863059
case acc: 0.025778515
case acc: 0.019259997
top acc: 0.0163 ::: bot acc: 0.0163
top acc: 0.0910 ::: bot acc: 0.0380
top acc: 0.0354 ::: bot acc: 0.0384
top acc: 0.0253 ::: bot acc: 0.0259
top acc: 0.0439 ::: bot acc: 0.0113
top acc: 0.0322 ::: bot acc: 0.0130
current epoch: 15
train loss is 0.001756
average val loss: 0.000765, accuracy: 0.0302
average test loss: 0.000628, accuracy: 0.0272
case acc: 0.012241443
case acc: 0.058911912
case acc: 0.027345313
case acc: 0.018990975
case acc: 0.026319463
case acc: 0.01921193
top acc: 0.0181 ::: bot acc: 0.0150
top acc: 0.0864 ::: bot acc: 0.0329
top acc: 0.0351 ::: bot acc: 0.0384
top acc: 0.0266 ::: bot acc: 0.0250
top acc: 0.0450 ::: bot acc: 0.0113
top acc: 0.0327 ::: bot acc: 0.0124
current epoch: 16
train loss is 0.001668
average val loss: 0.000724, accuracy: 0.0297
average test loss: 0.000593, accuracy: 0.0266
case acc: 0.012104158
case acc: 0.05502128
case acc: 0.027366616
case acc: 0.019046957
case acc: 0.026530247
case acc: 0.019486353
top acc: 0.0197 ::: bot acc: 0.0130
top acc: 0.0825 ::: bot acc: 0.0289
top acc: 0.0361 ::: bot acc: 0.0378
top acc: 0.0273 ::: bot acc: 0.0239
top acc: 0.0451 ::: bot acc: 0.0110
top acc: 0.0325 ::: bot acc: 0.0127
current epoch: 17
train loss is 0.001586
average val loss: 0.000742, accuracy: 0.0304
average test loss: 0.000595, accuracy: 0.0269
case acc: 0.013129946
case acc: 0.05363034
case acc: 0.026899425
case acc: 0.019065997
case acc: 0.027924677
case acc: 0.020760037
top acc: 0.0236 ::: bot acc: 0.0099
top acc: 0.0810 ::: bot acc: 0.0278
top acc: 0.0383 ::: bot acc: 0.0351
top acc: 0.0297 ::: bot acc: 0.0210
top acc: 0.0477 ::: bot acc: 0.0103
top acc: 0.0348 ::: bot acc: 0.0122
current epoch: 18
train loss is 0.001525
average val loss: 0.000743, accuracy: 0.0308
average test loss: 0.000584, accuracy: 0.0268
case acc: 0.014115978
case acc: 0.051462617
case acc: 0.026821608
case acc: 0.019664638
case acc: 0.027928146
case acc: 0.021006081
top acc: 0.0259 ::: bot acc: 0.0082
top acc: 0.0787 ::: bot acc: 0.0258
top acc: 0.0406 ::: bot acc: 0.0330
top acc: 0.0319 ::: bot acc: 0.0192
top acc: 0.0478 ::: bot acc: 0.0104
top acc: 0.0353 ::: bot acc: 0.0119
current epoch: 19
train loss is 0.001492
average val loss: 0.000708, accuracy: 0.0301
average test loss: 0.000554, accuracy: 0.0262
case acc: 0.014331815
case acc: 0.048388824
case acc: 0.026839778
case acc: 0.019705176
case acc: 0.027404293
case acc: 0.020507727
top acc: 0.0266 ::: bot acc: 0.0077
top acc: 0.0757 ::: bot acc: 0.0227
top acc: 0.0409 ::: bot acc: 0.0326
top acc: 0.0318 ::: bot acc: 0.0192
top acc: 0.0468 ::: bot acc: 0.0107
top acc: 0.0347 ::: bot acc: 0.0118
current epoch: 20
train loss is 0.001427
average val loss: 0.000693, accuracy: 0.0298
average test loss: 0.000535, accuracy: 0.0258
case acc: 0.014812471
case acc: 0.045872897
case acc: 0.026776647
case acc: 0.019971168
case acc: 0.026964724
case acc: 0.020425353
top acc: 0.0280 ::: bot acc: 0.0071
top acc: 0.0734 ::: bot acc: 0.0203
top acc: 0.0420 ::: bot acc: 0.0313
top acc: 0.0327 ::: bot acc: 0.0188
top acc: 0.0460 ::: bot acc: 0.0108
top acc: 0.0343 ::: bot acc: 0.0121
current epoch: 21
train loss is 0.001413
average val loss: 0.000701, accuracy: 0.0301
average test loss: 0.000533, accuracy: 0.0259
case acc: 0.01583758
case acc: 0.044841744
case acc: 0.026833678
case acc: 0.020466449
case acc: 0.027057497
case acc: 0.020599283
top acc: 0.0296 ::: bot acc: 0.0063
top acc: 0.0719 ::: bot acc: 0.0193
top acc: 0.0439 ::: bot acc: 0.0294
top acc: 0.0341 ::: bot acc: 0.0175
top acc: 0.0460 ::: bot acc: 0.0109
top acc: 0.0346 ::: bot acc: 0.0123
current epoch: 22
train loss is 0.001383
average val loss: 0.000770, accuracy: 0.0321
average test loss: 0.000580, accuracy: 0.0274
case acc: 0.01867254
case acc: 0.045981586
case acc: 0.027533647
case acc: 0.021436304
case acc: 0.028617911
case acc: 0.022004511
top acc: 0.0337 ::: bot acc: 0.0060
top acc: 0.0734 ::: bot acc: 0.0204
top acc: 0.0486 ::: bot acc: 0.0253
top acc: 0.0376 ::: bot acc: 0.0139
top acc: 0.0486 ::: bot acc: 0.0108
top acc: 0.0368 ::: bot acc: 0.0119
current epoch: 23
train loss is 0.001389
average val loss: 0.000838, accuracy: 0.0338
average test loss: 0.000621, accuracy: 0.0287
case acc: 0.020993643
case acc: 0.04662858
case acc: 0.028436176
case acc: 0.022918086
case acc: 0.029955609
case acc: 0.023185616
top acc: 0.0368 ::: bot acc: 0.0067
top acc: 0.0737 ::: bot acc: 0.0211
top acc: 0.0520 ::: bot acc: 0.0215
top acc: 0.0405 ::: bot acc: 0.0125
top acc: 0.0507 ::: bot acc: 0.0107
top acc: 0.0389 ::: bot acc: 0.0120
current epoch: 24
train loss is 0.001346
average val loss: 0.000765, accuracy: 0.0320
average test loss: 0.000565, accuracy: 0.0271
case acc: 0.01983134
case acc: 0.043165028
case acc: 0.028425336
case acc: 0.021912012
case acc: 0.028189888
case acc: 0.021290565
top acc: 0.0355 ::: bot acc: 0.0063
top acc: 0.0704 ::: bot acc: 0.0181
top acc: 0.0509 ::: bot acc: 0.0230
top acc: 0.0385 ::: bot acc: 0.0133
top acc: 0.0479 ::: bot acc: 0.0106
top acc: 0.0359 ::: bot acc: 0.0117
current epoch: 25
train loss is 0.001313
average val loss: 0.000817, accuracy: 0.0334
average test loss: 0.000599, accuracy: 0.0282
case acc: 0.02201087
case acc: 0.043530177
case acc: 0.02912629
case acc: 0.023056112
case acc: 0.029158926
case acc: 0.022170123
top acc: 0.0382 ::: bot acc: 0.0073
top acc: 0.0706 ::: bot acc: 0.0183
top acc: 0.0541 ::: bot acc: 0.0197
top acc: 0.0407 ::: bot acc: 0.0125
top acc: 0.0493 ::: bot acc: 0.0107
top acc: 0.0373 ::: bot acc: 0.0118
current epoch: 26
train loss is 0.001299
average val loss: 0.000779, accuracy: 0.0324
average test loss: 0.000571, accuracy: 0.0274
case acc: 0.021458697
case acc: 0.041498244
case acc: 0.02909577
case acc: 0.022889167
case acc: 0.028137352
case acc: 0.021240523
top acc: 0.0377 ::: bot acc: 0.0072
top acc: 0.0687 ::: bot acc: 0.0161
top acc: 0.0539 ::: bot acc: 0.0198
top acc: 0.0403 ::: bot acc: 0.0128
top acc: 0.0479 ::: bot acc: 0.0103
top acc: 0.0356 ::: bot acc: 0.0118
current epoch: 27
train loss is 0.001287
average val loss: 0.000793, accuracy: 0.0328
average test loss: 0.000576, accuracy: 0.0276
case acc: 0.022239573
case acc: 0.040818427
case acc: 0.029568499
case acc: 0.023206038
case acc: 0.028332707
case acc: 0.021363465
top acc: 0.0382 ::: bot acc: 0.0079
top acc: 0.0680 ::: bot acc: 0.0158
top acc: 0.0554 ::: bot acc: 0.0183
top acc: 0.0412 ::: bot acc: 0.0122
top acc: 0.0480 ::: bot acc: 0.0103
top acc: 0.0358 ::: bot acc: 0.0121
current epoch: 28
train loss is 0.001280
average val loss: 0.000832, accuracy: 0.0338
average test loss: 0.000603, accuracy: 0.0284
case acc: 0.02395665
case acc: 0.04113844
case acc: 0.03045984
case acc: 0.023880886
case acc: 0.029034164
case acc: 0.022073455
top acc: 0.0403 ::: bot acc: 0.0091
top acc: 0.0681 ::: bot acc: 0.0161
top acc: 0.0576 ::: bot acc: 0.0165
top acc: 0.0424 ::: bot acc: 0.0114
top acc: 0.0493 ::: bot acc: 0.0104
top acc: 0.0369 ::: bot acc: 0.0119
current epoch: 29
train loss is 0.001265
average val loss: 0.000879, accuracy: 0.0350
average test loss: 0.000639, accuracy: 0.0295
case acc: 0.025736498
case acc: 0.041619375
case acc: 0.03148126
case acc: 0.025179213
case acc: 0.030216826
case acc: 0.022588758
top acc: 0.0419 ::: bot acc: 0.0104
top acc: 0.0690 ::: bot acc: 0.0161
top acc: 0.0600 ::: bot acc: 0.0148
top acc: 0.0439 ::: bot acc: 0.0117
top acc: 0.0510 ::: bot acc: 0.0107
top acc: 0.0377 ::: bot acc: 0.0116
current epoch: 30
train loss is 0.001274
average val loss: 0.000890, accuracy: 0.0353
average test loss: 0.000645, accuracy: 0.0297
case acc: 0.026220784
case acc: 0.04119771
case acc: 0.03192487
case acc: 0.025467636
case acc: 0.030704863
case acc: 0.022573842
top acc: 0.0429 ::: bot acc: 0.0110
top acc: 0.0683 ::: bot acc: 0.0164
top acc: 0.0609 ::: bot acc: 0.0140
top acc: 0.0451 ::: bot acc: 0.0111
top acc: 0.0514 ::: bot acc: 0.0107
top acc: 0.0376 ::: bot acc: 0.0119
current epoch: 31
train loss is 0.001255
average val loss: 0.000909, accuracy: 0.0357
average test loss: 0.000663, accuracy: 0.0301
case acc: 0.02712766
case acc: 0.041023154
case acc: 0.03269086
case acc: 0.026069183
case acc: 0.031354617
case acc: 0.022529451
top acc: 0.0437 ::: bot acc: 0.0116
top acc: 0.0684 ::: bot acc: 0.0156
top acc: 0.0626 ::: bot acc: 0.0129
top acc: 0.0459 ::: bot acc: 0.0113
top acc: 0.0523 ::: bot acc: 0.0111
top acc: 0.0380 ::: bot acc: 0.0116
current epoch: 32
train loss is 0.001264
average val loss: 0.000950, accuracy: 0.0367
average test loss: 0.000691, accuracy: 0.0309
case acc: 0.028223647
case acc: 0.04142202
case acc: 0.033629004
case acc: 0.027019095
case acc: 0.032204296
case acc: 0.023055432
top acc: 0.0448 ::: bot acc: 0.0128
top acc: 0.0687 ::: bot acc: 0.0165
top acc: 0.0648 ::: bot acc: 0.0120
top acc: 0.0471 ::: bot acc: 0.0116
top acc: 0.0535 ::: bot acc: 0.0116
top acc: 0.0386 ::: bot acc: 0.0117
current epoch: 33
train loss is 0.001262
average val loss: 0.000990, accuracy: 0.0376
average test loss: 0.000719, accuracy: 0.0317
case acc: 0.029212872
case acc: 0.041566685
case acc: 0.034623705
case acc: 0.027954819
case acc: 0.03300295
case acc: 0.023793908
top acc: 0.0458 ::: bot acc: 0.0138
top acc: 0.0686 ::: bot acc: 0.0164
top acc: 0.0664 ::: bot acc: 0.0116
top acc: 0.0487 ::: bot acc: 0.0111
top acc: 0.0548 ::: bot acc: 0.0115
top acc: 0.0396 ::: bot acc: 0.0119
current epoch: 34
train loss is 0.001257
average val loss: 0.000961, accuracy: 0.0369
average test loss: 0.000694, accuracy: 0.0310
case acc: 0.028520757
case acc: 0.040245853
case acc: 0.034280185
case acc: 0.027573155
case acc: 0.032793142
case acc: 0.022831222
top acc: 0.0450 ::: bot acc: 0.0131
top acc: 0.0669 ::: bot acc: 0.0155
top acc: 0.0659 ::: bot acc: 0.0113
top acc: 0.0479 ::: bot acc: 0.0113
top acc: 0.0541 ::: bot acc: 0.0117
top acc: 0.0378 ::: bot acc: 0.0118
current epoch: 35
train loss is 0.001241
average val loss: 0.000957, accuracy: 0.0368
average test loss: 0.000689, accuracy: 0.0309
case acc: 0.028163282
case acc: 0.039301038
case acc: 0.034670047
case acc: 0.027596856
case acc: 0.03278285
case acc: 0.022701517
top acc: 0.0444 ::: bot acc: 0.0130
top acc: 0.0662 ::: bot acc: 0.0146
top acc: 0.0665 ::: bot acc: 0.0113
top acc: 0.0482 ::: bot acc: 0.0111
top acc: 0.0541 ::: bot acc: 0.0117
top acc: 0.0379 ::: bot acc: 0.0117
current epoch: 36
train loss is 0.001228
average val loss: 0.000970, accuracy: 0.0371
average test loss: 0.000698, accuracy: 0.0311
case acc: 0.028221887
case acc: 0.039052017
case acc: 0.035024356
case acc: 0.02823031
case acc: 0.033223715
case acc: 0.022986203
top acc: 0.0446 ::: bot acc: 0.0129
top acc: 0.0660 ::: bot acc: 0.0143
top acc: 0.0671 ::: bot acc: 0.0111
top acc: 0.0489 ::: bot acc: 0.0114
top acc: 0.0548 ::: bot acc: 0.0117
top acc: 0.0383 ::: bot acc: 0.0118
current epoch: 37
train loss is 0.001205
average val loss: 0.000900, accuracy: 0.0354
average test loss: 0.000642, accuracy: 0.0296
case acc: 0.025935324
case acc: 0.03666608
case acc: 0.034092482
case acc: 0.027048647
case acc: 0.031739026
case acc: 0.021963257
top acc: 0.0423 ::: bot acc: 0.0108
top acc: 0.0631 ::: bot acc: 0.0127
top acc: 0.0653 ::: bot acc: 0.0117
top acc: 0.0470 ::: bot acc: 0.0114
top acc: 0.0529 ::: bot acc: 0.0113
top acc: 0.0369 ::: bot acc: 0.0118
current epoch: 38
train loss is 0.001177
average val loss: 0.000842, accuracy: 0.0340
average test loss: 0.000598, accuracy: 0.0282
case acc: 0.024187613
case acc: 0.03432057
case acc: 0.033123434
case acc: 0.026129553
case acc: 0.030844552
case acc: 0.020835618
top acc: 0.0405 ::: bot acc: 0.0094
top acc: 0.0608 ::: bot acc: 0.0108
top acc: 0.0636 ::: bot acc: 0.0123
top acc: 0.0459 ::: bot acc: 0.0112
top acc: 0.0517 ::: bot acc: 0.0108
top acc: 0.0350 ::: bot acc: 0.0120
current epoch: 39
train loss is 0.001156
average val loss: 0.000817, accuracy: 0.0333
average test loss: 0.000575, accuracy: 0.0275
case acc: 0.022977082
case acc: 0.032868955
case acc: 0.032665305
case acc: 0.025657173
case acc: 0.030721882
case acc: 0.020218497
top acc: 0.0390 ::: bot acc: 0.0081
top acc: 0.0591 ::: bot acc: 0.0101
top acc: 0.0627 ::: bot acc: 0.0126
top acc: 0.0453 ::: bot acc: 0.0109
top acc: 0.0519 ::: bot acc: 0.0106
top acc: 0.0341 ::: bot acc: 0.0121
current epoch: 40
train loss is 0.001137
average val loss: 0.000811, accuracy: 0.0332
average test loss: 0.000575, accuracy: 0.0276
case acc: 0.022731056
case acc: 0.032466557
case acc: 0.032738153
case acc: 0.02571076
case acc: 0.031138
case acc: 0.02056463
top acc: 0.0387 ::: bot acc: 0.0083
top acc: 0.0584 ::: bot acc: 0.0097
top acc: 0.0630 ::: bot acc: 0.0127
top acc: 0.0454 ::: bot acc: 0.0111
top acc: 0.0520 ::: bot acc: 0.0111
top acc: 0.0343 ::: bot acc: 0.0125
current epoch: 41
train loss is 0.001119
average val loss: 0.000739, accuracy: 0.0313
average test loss: 0.000517, accuracy: 0.0258
case acc: 0.020429026
case acc: 0.0296049
case acc: 0.03160861
case acc: 0.024402896
case acc: 0.02954842
case acc: 0.019130211
top acc: 0.0361 ::: bot acc: 0.0066
top acc: 0.0550 ::: bot acc: 0.0082
top acc: 0.0604 ::: bot acc: 0.0144
top acc: 0.0432 ::: bot acc: 0.0113
top acc: 0.0501 ::: bot acc: 0.0104
top acc: 0.0323 ::: bot acc: 0.0127
current epoch: 42
train loss is 0.001088
average val loss: 0.000664, accuracy: 0.0292
average test loss: 0.000463, accuracy: 0.0241
case acc: 0.017866649
case acc: 0.027166419
case acc: 0.030417932
case acc: 0.022965154
case acc: 0.028154379
case acc: 0.017991731
top acc: 0.0326 ::: bot acc: 0.0058
top acc: 0.0514 ::: bot acc: 0.0080
top acc: 0.0578 ::: bot acc: 0.0163
top acc: 0.0408 ::: bot acc: 0.0121
top acc: 0.0479 ::: bot acc: 0.0107
top acc: 0.0298 ::: bot acc: 0.0140
current epoch: 43
train loss is 0.001063
average val loss: 0.000595, accuracy: 0.0273
average test loss: 0.000415, accuracy: 0.0226
case acc: 0.01569802
case acc: 0.025192268
case acc: 0.02923003
case acc: 0.02196533
case acc: 0.026430624
case acc: 0.017092543
top acc: 0.0295 ::: bot acc: 0.0058
top acc: 0.0480 ::: bot acc: 0.0089
top acc: 0.0544 ::: bot acc: 0.0191
top acc: 0.0385 ::: bot acc: 0.0136
top acc: 0.0452 ::: bot acc: 0.0109
top acc: 0.0273 ::: bot acc: 0.0162
current epoch: 44
train loss is 0.001044
average val loss: 0.000572, accuracy: 0.0267
average test loss: 0.000400, accuracy: 0.0221
case acc: 0.015041121
case acc: 0.024063805
case acc: 0.029068964
case acc: 0.021489395
case acc: 0.02628447
case acc: 0.016898103
top acc: 0.0279 ::: bot acc: 0.0066
top acc: 0.0459 ::: bot acc: 0.0094
top acc: 0.0534 ::: bot acc: 0.0206
top acc: 0.0377 ::: bot acc: 0.0137
top acc: 0.0450 ::: bot acc: 0.0111
top acc: 0.0270 ::: bot acc: 0.0162
current epoch: 45
train loss is 0.001028
average val loss: 0.000531, accuracy: 0.0255
average test loss: 0.000372, accuracy: 0.0213
case acc: 0.013963548
case acc: 0.02276235
case acc: 0.028179063
case acc: 0.020979928
case acc: 0.025400804
case acc: 0.016517524
top acc: 0.0256 ::: bot acc: 0.0082
top acc: 0.0432 ::: bot acc: 0.0109
top acc: 0.0508 ::: bot acc: 0.0228
top acc: 0.0363 ::: bot acc: 0.0153
top acc: 0.0431 ::: bot acc: 0.0118
top acc: 0.0258 ::: bot acc: 0.0175
current epoch: 46
train loss is 0.001012
average val loss: 0.000466, accuracy: 0.0236
average test loss: 0.000336, accuracy: 0.0201
case acc: 0.012427377
case acc: 0.021131324
case acc: 0.027325165
case acc: 0.019788882
case acc: 0.02380269
case acc: 0.016134296
top acc: 0.0216 ::: bot acc: 0.0111
top acc: 0.0387 ::: bot acc: 0.0148
top acc: 0.0471 ::: bot acc: 0.0269
top acc: 0.0328 ::: bot acc: 0.0179
top acc: 0.0400 ::: bot acc: 0.0133
top acc: 0.0231 ::: bot acc: 0.0208
current epoch: 47
train loss is 0.000991
average val loss: 0.000443, accuracy: 0.0229
average test loss: 0.000322, accuracy: 0.0197
case acc: 0.012026846
case acc: 0.020680312
case acc: 0.026949832
case acc: 0.019534757
case acc: 0.023302613
case acc: 0.01587239
top acc: 0.0195 ::: bot acc: 0.0134
top acc: 0.0366 ::: bot acc: 0.0171
top acc: 0.0446 ::: bot acc: 0.0290
top acc: 0.0317 ::: bot acc: 0.0192
top acc: 0.0389 ::: bot acc: 0.0140
top acc: 0.0220 ::: bot acc: 0.0215
current epoch: 48
train loss is 0.000985
average val loss: 0.000411, accuracy: 0.0220
average test loss: 0.000311, accuracy: 0.0195
case acc: 0.01174446
case acc: 0.020375015
case acc: 0.026887529
case acc: 0.019163964
case acc: 0.022762878
case acc: 0.015861345
top acc: 0.0168 ::: bot acc: 0.0158
top acc: 0.0332 ::: bot acc: 0.0209
top acc: 0.0415 ::: bot acc: 0.0325
top acc: 0.0296 ::: bot acc: 0.0215
top acc: 0.0376 ::: bot acc: 0.0150
top acc: 0.0206 ::: bot acc: 0.0232
current epoch: 49
train loss is 0.000981
average val loss: 0.000379, accuracy: 0.0210
average test loss: 0.000302, accuracy: 0.0193
case acc: 0.0124370055
case acc: 0.01991031
case acc: 0.027150447
case acc: 0.018569194
case acc: 0.02167154
case acc: 0.015782015
top acc: 0.0132 ::: bot acc: 0.0197
top acc: 0.0286 ::: bot acc: 0.0251
top acc: 0.0374 ::: bot acc: 0.0366
top acc: 0.0264 ::: bot acc: 0.0242
top acc: 0.0354 ::: bot acc: 0.0164
top acc: 0.0185 ::: bot acc: 0.0249
current epoch: 50
train loss is 0.000975
average val loss: 0.000349, accuracy: 0.0200
average test loss: 0.000310, accuracy: 0.0197
case acc: 0.014312663
case acc: 0.020719439
case acc: 0.027849527
case acc: 0.018608874
case acc: 0.02037174
case acc: 0.016321618
top acc: 0.0090 ::: bot acc: 0.0247
top acc: 0.0229 ::: bot acc: 0.0311
top acc: 0.0313 ::: bot acc: 0.0423
top acc: 0.0222 ::: bot acc: 0.0286
top acc: 0.0316 ::: bot acc: 0.0197
top acc: 0.0152 ::: bot acc: 0.0283
LME_Co_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6798 6798 6798
1.8562728 -0.6288155 0.21141115 -0.19947179
Validation: 756 756 756
Testing: 744 744 744
pre-processing time: 0.00028133392333984375
the split date is 2010-01-01
train dropout: 0.6 test dropout: 0.15
net initializing with time: 0.002576112747192383
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.013177
average val loss: 0.005739, accuracy: 0.1010
average test loss: 0.006083, accuracy: 0.1025
case acc: 0.14110221
case acc: 0.08283136
case acc: 0.10303336
case acc: 0.10147597
case acc: 0.12758838
case acc: 0.058826495
top acc: 0.1088 ::: bot acc: 0.1746
top acc: 0.1080 ::: bot acc: 0.0564
top acc: 0.0583 ::: bot acc: 0.1477
top acc: 0.0722 ::: bot acc: 0.1316
top acc: 0.0955 ::: bot acc: 0.1576
top acc: 0.0265 ::: bot acc: 0.0909
current epoch: 2
train loss is 0.008565
average val loss: 0.002966, accuracy: 0.0537
average test loss: 0.003166, accuracy: 0.0574
case acc: 0.043475486
case acc: 0.16720352
case acc: 0.03325569
case acc: 0.02311922
case acc: 0.04224519
case acc: 0.03520022
top acc: 0.0188 ::: bot acc: 0.0718
top acc: 0.1923 ::: bot acc: 0.1412
top acc: 0.0416 ::: bot acc: 0.0470
top acc: 0.0226 ::: bot acc: 0.0388
top acc: 0.0158 ::: bot acc: 0.0684
top acc: 0.0678 ::: bot acc: 0.0139
current epoch: 3
train loss is 0.008269
average val loss: 0.007585, accuracy: 0.1012
average test loss: 0.007709, accuracy: 0.1007
case acc: 0.05248819
case acc: 0.24287945
case acc: 0.088304095
case acc: 0.07521989
case acc: 0.042657323
case acc: 0.10258484
top acc: 0.0831 ::: bot acc: 0.0233
top acc: 0.2678 ::: bot acc: 0.2178
top acc: 0.1316 ::: bot acc: 0.0442
top acc: 0.1055 ::: bot acc: 0.0447
top acc: 0.0730 ::: bot acc: 0.0161
top acc: 0.1425 ::: bot acc: 0.0668
current epoch: 4
train loss is 0.010474
average val loss: 0.016108, accuracy: 0.1671
average test loss: 0.016138, accuracy: 0.1659
case acc: 0.12399961
case acc: 0.3019442
case acc: 0.15956931
case acc: 0.13965504
case acc: 0.10934382
case acc: 0.16088846
top acc: 0.1567 ::: bot acc: 0.0917
top acc: 0.3267 ::: bot acc: 0.2771
top acc: 0.2043 ::: bot acc: 0.1142
top acc: 0.1706 ::: bot acc: 0.1084
top acc: 0.1411 ::: bot acc: 0.0806
top acc: 0.2007 ::: bot acc: 0.1254
current epoch: 5
train loss is 0.013258
average val loss: 0.013548, accuracy: 0.1527
average test loss: 0.013598, accuracy: 0.1516
case acc: 0.11548923
case acc: 0.27753893
case acc: 0.14830963
case acc: 0.12487038
case acc: 0.10248634
case acc: 0.14111178
top acc: 0.1477 ::: bot acc: 0.0840
top acc: 0.3020 ::: bot acc: 0.2524
top acc: 0.1926 ::: bot acc: 0.1031
top acc: 0.1563 ::: bot acc: 0.0930
top acc: 0.1345 ::: bot acc: 0.0732
top acc: 0.1806 ::: bot acc: 0.1053
current epoch: 6
train loss is 0.013347
average val loss: 0.003151, accuracy: 0.0574
average test loss: 0.003306, accuracy: 0.0592
case acc: 0.02974839
case acc: 0.16712002
case acc: 0.054027043
case acc: 0.032966416
case acc: 0.02530085
case acc: 0.04626496
top acc: 0.0543 ::: bot acc: 0.0139
top acc: 0.1917 ::: bot acc: 0.1425
top acc: 0.0931 ::: bot acc: 0.0191
top acc: 0.0580 ::: bot acc: 0.0134
top acc: 0.0485 ::: bot acc: 0.0133
top acc: 0.0835 ::: bot acc: 0.0156
current epoch: 7
train loss is 0.007593
average val loss: 0.001299, accuracy: 0.0390
average test loss: 0.001533, accuracy: 0.0447
case acc: 0.03787993
case acc: 0.09630415
case acc: 0.03433738
case acc: 0.035726625
case acc: 0.033521067
case acc: 0.030164754
top acc: 0.0175 ::: bot acc: 0.0637
top acc: 0.1214 ::: bot acc: 0.0713
top acc: 0.0317 ::: bot acc: 0.0582
top acc: 0.0163 ::: bot acc: 0.0613
top acc: 0.0136 ::: bot acc: 0.0575
top acc: 0.0302 ::: bot acc: 0.0463
current epoch: 8
train loss is 0.003705
average val loss: 0.001113, accuracy: 0.0342
average test loss: 0.001319, accuracy: 0.0398
case acc: 0.03173507
case acc: 0.092469074
case acc: 0.03376728
case acc: 0.029530687
case acc: 0.024544233
case acc: 0.026965542
top acc: 0.0184 ::: bot acc: 0.0536
top acc: 0.1175 ::: bot acc: 0.0672
top acc: 0.0352 ::: bot acc: 0.0544
top acc: 0.0144 ::: bot acc: 0.0533
top acc: 0.0206 ::: bot acc: 0.0403
top acc: 0.0393 ::: bot acc: 0.0360
current epoch: 9
train loss is 0.002738
average val loss: 0.001184, accuracy: 0.0338
average test loss: 0.001385, accuracy: 0.0388
case acc: 0.023418956
case acc: 0.0997686
case acc: 0.033771835
case acc: 0.022809586
case acc: 0.023398718
case acc: 0.029846497
top acc: 0.0294 ::: bot acc: 0.0358
top acc: 0.1247 ::: bot acc: 0.0748
top acc: 0.0489 ::: bot acc: 0.0412
top acc: 0.0260 ::: bot acc: 0.0368
top acc: 0.0425 ::: bot acc: 0.0192
top acc: 0.0577 ::: bot acc: 0.0186
current epoch: 10
train loss is 0.002740
average val loss: 0.001129, accuracy: 0.0338
average test loss: 0.001329, accuracy: 0.0385
case acc: 0.023109583
case acc: 0.095055975
case acc: 0.03373284
case acc: 0.02255729
case acc: 0.025389673
case acc: 0.031452496
top acc: 0.0335 ::: bot acc: 0.0308
top acc: 0.1200 ::: bot acc: 0.0701
top acc: 0.0496 ::: bot acc: 0.0397
top acc: 0.0292 ::: bot acc: 0.0339
top acc: 0.0484 ::: bot acc: 0.0140
top acc: 0.0609 ::: bot acc: 0.0166
current epoch: 11
train loss is 0.002521
average val loss: 0.000927, accuracy: 0.0312
average test loss: 0.001124, accuracy: 0.0360
case acc: 0.023312222
case acc: 0.082604386
case acc: 0.033210207
case acc: 0.022912081
case acc: 0.024701925
case acc: 0.029370079
top acc: 0.0305 ::: bot acc: 0.0347
top acc: 0.1075 ::: bot acc: 0.0576
top acc: 0.0429 ::: bot acc: 0.0463
top acc: 0.0248 ::: bot acc: 0.0376
top acc: 0.0468 ::: bot acc: 0.0149
top acc: 0.0566 ::: bot acc: 0.0193
current epoch: 12
train loss is 0.002216
average val loss: 0.000838, accuracy: 0.0303
average test loss: 0.001046, accuracy: 0.0352
case acc: 0.02322405
case acc: 0.07559928
case acc: 0.03323257
case acc: 0.022927262
case acc: 0.025938775
case acc: 0.03003959
top acc: 0.0320 ::: bot acc: 0.0333
top acc: 0.1004 ::: bot acc: 0.0502
top acc: 0.0412 ::: bot acc: 0.0479
top acc: 0.0249 ::: bot acc: 0.0377
top acc: 0.0497 ::: bot acc: 0.0135
top acc: 0.0575 ::: bot acc: 0.0191
current epoch: 13
train loss is 0.001980
average val loss: 0.000749, accuracy: 0.0291
average test loss: 0.000963, accuracy: 0.0340
case acc: 0.022795172
case acc: 0.06842284
case acc: 0.03377394
case acc: 0.023113616
case acc: 0.02598955
case acc: 0.029835263
top acc: 0.0321 ::: bot acc: 0.0318
top acc: 0.0930 ::: bot acc: 0.0435
top acc: 0.0401 ::: bot acc: 0.0505
top acc: 0.0240 ::: bot acc: 0.0388
top acc: 0.0493 ::: bot acc: 0.0137
top acc: 0.0571 ::: bot acc: 0.0198
current epoch: 14
train loss is 0.001737
average val loss: 0.000691, accuracy: 0.0282
average test loss: 0.000903, accuracy: 0.0331
case acc: 0.023052538
case acc: 0.062954575
case acc: 0.033289578
case acc: 0.022642506
case acc: 0.026371012
case acc: 0.030065838
top acc: 0.0340 ::: bot acc: 0.0306
top acc: 0.0881 ::: bot acc: 0.0377
top acc: 0.0394 ::: bot acc: 0.0498
top acc: 0.0250 ::: bot acc: 0.0371
top acc: 0.0503 ::: bot acc: 0.0131
top acc: 0.0580 ::: bot acc: 0.0189
current epoch: 15
train loss is 0.001619
average val loss: 0.000641, accuracy: 0.0275
average test loss: 0.000855, accuracy: 0.0323
case acc: 0.02325059
case acc: 0.057966344
case acc: 0.03371842
case acc: 0.022778936
case acc: 0.026249148
case acc: 0.029897284
top acc: 0.0350 ::: bot acc: 0.0298
top acc: 0.0827 ::: bot acc: 0.0328
top acc: 0.0393 ::: bot acc: 0.0509
top acc: 0.0255 ::: bot acc: 0.0370
top acc: 0.0500 ::: bot acc: 0.0135
top acc: 0.0579 ::: bot acc: 0.0187
current epoch: 16
train loss is 0.001569
average val loss: 0.000631, accuracy: 0.0275
average test loss: 0.000845, accuracy: 0.0322
case acc: 0.023608107
case acc: 0.055944078
case acc: 0.03342501
case acc: 0.022401672
case acc: 0.027182754
case acc: 0.030665847
top acc: 0.0384 ::: bot acc: 0.0262
top acc: 0.0807 ::: bot acc: 0.0310
top acc: 0.0415 ::: bot acc: 0.0484
top acc: 0.0286 ::: bot acc: 0.0339
top acc: 0.0521 ::: bot acc: 0.0124
top acc: 0.0598 ::: bot acc: 0.0167
current epoch: 17
train loss is 0.001459
average val loss: 0.000668, accuracy: 0.0288
average test loss: 0.000871, accuracy: 0.0330
case acc: 0.024752302
case acc: 0.055991154
case acc: 0.033239644
case acc: 0.022511736
case acc: 0.029074388
case acc: 0.032285433
top acc: 0.0432 ::: bot acc: 0.0216
top acc: 0.0808 ::: bot acc: 0.0310
top acc: 0.0456 ::: bot acc: 0.0435
top acc: 0.0326 ::: bot acc: 0.0298
top acc: 0.0556 ::: bot acc: 0.0109
top acc: 0.0626 ::: bot acc: 0.0153
current epoch: 18
train loss is 0.001415
average val loss: 0.000608, accuracy: 0.0275
average test loss: 0.000815, accuracy: 0.0318
case acc: 0.02466961
case acc: 0.05124217
case acc: 0.033466235
case acc: 0.022343138
case acc: 0.027802208
case acc: 0.031188108
top acc: 0.0433 ::: bot acc: 0.0214
top acc: 0.0763 ::: bot acc: 0.0265
top acc: 0.0451 ::: bot acc: 0.0444
top acc: 0.0315 ::: bot acc: 0.0308
top acc: 0.0532 ::: bot acc: 0.0117
top acc: 0.0609 ::: bot acc: 0.0159
current epoch: 19
train loss is 0.001350
average val loss: 0.000565, accuracy: 0.0265
average test loss: 0.000776, accuracy: 0.0310
case acc: 0.02468655
case acc: 0.047629867
case acc: 0.033595253
case acc: 0.022345608
case acc: 0.0270887
case acc: 0.03075161
top acc: 0.0433 ::: bot acc: 0.0213
top acc: 0.0723 ::: bot acc: 0.0232
top acc: 0.0449 ::: bot acc: 0.0450
top acc: 0.0313 ::: bot acc: 0.0311
top acc: 0.0518 ::: bot acc: 0.0122
top acc: 0.0598 ::: bot acc: 0.0173
current epoch: 20
train loss is 0.001317
average val loss: 0.000584, accuracy: 0.0273
average test loss: 0.000799, accuracy: 0.0316
case acc: 0.026058327
case acc: 0.04761637
case acc: 0.033619482
case acc: 0.022607353
case acc: 0.02788108
case acc: 0.031815417
top acc: 0.0469 ::: bot acc: 0.0187
top acc: 0.0721 ::: bot acc: 0.0233
top acc: 0.0482 ::: bot acc: 0.0416
top acc: 0.0343 ::: bot acc: 0.0281
top acc: 0.0538 ::: bot acc: 0.0116
top acc: 0.0619 ::: bot acc: 0.0160
current epoch: 21
train loss is 0.001297
average val loss: 0.000557, accuracy: 0.0266
average test loss: 0.000770, accuracy: 0.0310
case acc: 0.026473472
case acc: 0.0448037
case acc: 0.03373023
case acc: 0.022796856
case acc: 0.026961884
case acc: 0.031340707
top acc: 0.0476 ::: bot acc: 0.0180
top acc: 0.0690 ::: bot acc: 0.0208
top acc: 0.0489 ::: bot acc: 0.0406
top acc: 0.0343 ::: bot acc: 0.0283
top acc: 0.0520 ::: bot acc: 0.0116
top acc: 0.0613 ::: bot acc: 0.0162
current epoch: 22
train loss is 0.001274
average val loss: 0.000540, accuracy: 0.0263
average test loss: 0.000753, accuracy: 0.0306
case acc: 0.02676638
case acc: 0.04286402
case acc: 0.033800356
case acc: 0.022705194
case acc: 0.026665488
case acc: 0.030976068
top acc: 0.0484 ::: bot acc: 0.0174
top acc: 0.0672 ::: bot acc: 0.0194
top acc: 0.0500 ::: bot acc: 0.0399
top acc: 0.0346 ::: bot acc: 0.0278
top acc: 0.0513 ::: bot acc: 0.0121
top acc: 0.0606 ::: bot acc: 0.0166
current epoch: 23
train loss is 0.001227
average val loss: 0.000578, accuracy: 0.0275
average test loss: 0.000786, accuracy: 0.0315
case acc: 0.028376563
case acc: 0.043509804
case acc: 0.03428878
case acc: 0.02341667
case acc: 0.027834425
case acc: 0.031870242
top acc: 0.0518 ::: bot acc: 0.0150
top acc: 0.0679 ::: bot acc: 0.0201
top acc: 0.0536 ::: bot acc: 0.0363
top acc: 0.0377 ::: bot acc: 0.0250
top acc: 0.0532 ::: bot acc: 0.0115
top acc: 0.0618 ::: bot acc: 0.0161
current epoch: 24
train loss is 0.001244
average val loss: 0.000578, accuracy: 0.0276
average test loss: 0.000788, accuracy: 0.0316
case acc: 0.029258905
case acc: 0.04283873
case acc: 0.03456002
case acc: 0.023684422
case acc: 0.027547661
case acc: 0.031576715
top acc: 0.0532 ::: bot acc: 0.0147
top acc: 0.0670 ::: bot acc: 0.0194
top acc: 0.0557 ::: bot acc: 0.0342
top acc: 0.0389 ::: bot acc: 0.0242
top acc: 0.0525 ::: bot acc: 0.0117
top acc: 0.0617 ::: bot acc: 0.0160
current epoch: 25
train loss is 0.001220
average val loss: 0.000630, accuracy: 0.0291
average test loss: 0.000839, accuracy: 0.0328
case acc: 0.031191979
case acc: 0.04441122
case acc: 0.03550823
case acc: 0.024600372
case acc: 0.028600682
case acc: 0.032371663
top acc: 0.0565 ::: bot acc: 0.0140
top acc: 0.0691 ::: bot acc: 0.0205
top acc: 0.0597 ::: bot acc: 0.0302
top acc: 0.0417 ::: bot acc: 0.0212
top acc: 0.0548 ::: bot acc: 0.0110
top acc: 0.0633 ::: bot acc: 0.0151
current epoch: 26
train loss is 0.001246
average val loss: 0.000760, accuracy: 0.0328
average test loss: 0.000959, accuracy: 0.0355
case acc: 0.035026327
case acc: 0.048066992
case acc: 0.03726666
case acc: 0.026634568
case acc: 0.031282183
case acc: 0.03498914
top acc: 0.0624 ::: bot acc: 0.0138
top acc: 0.0727 ::: bot acc: 0.0236
top acc: 0.0662 ::: bot acc: 0.0236
top acc: 0.0469 ::: bot acc: 0.0167
top acc: 0.0587 ::: bot acc: 0.0112
top acc: 0.0671 ::: bot acc: 0.0145
current epoch: 27
train loss is 0.001248
average val loss: 0.000773, accuracy: 0.0331
average test loss: 0.000974, accuracy: 0.0358
case acc: 0.035909247
case acc: 0.04812173
case acc: 0.038061105
case acc: 0.02690405
case acc: 0.03143385
case acc: 0.03445389
top acc: 0.0635 ::: bot acc: 0.0141
top acc: 0.0729 ::: bot acc: 0.0237
top acc: 0.0680 ::: bot acc: 0.0220
top acc: 0.0476 ::: bot acc: 0.0162
top acc: 0.0589 ::: bot acc: 0.0110
top acc: 0.0666 ::: bot acc: 0.0144
current epoch: 28
train loss is 0.001285
average val loss: 0.000854, accuracy: 0.0352
average test loss: 0.001054, accuracy: 0.0375
case acc: 0.037996963
case acc: 0.049848054
case acc: 0.03992741
case acc: 0.028273825
case acc: 0.03352243
case acc: 0.03557538
top acc: 0.0667 ::: bot acc: 0.0144
top acc: 0.0744 ::: bot acc: 0.0251
top acc: 0.0720 ::: bot acc: 0.0193
top acc: 0.0507 ::: bot acc: 0.0142
top acc: 0.0622 ::: bot acc: 0.0114
top acc: 0.0686 ::: bot acc: 0.0141
current epoch: 29
train loss is 0.001282
average val loss: 0.000951, accuracy: 0.0376
average test loss: 0.001142, accuracy: 0.0394
case acc: 0.040629625
case acc: 0.051607993
case acc: 0.04199543
case acc: 0.029987538
case acc: 0.035745125
case acc: 0.036592003
top acc: 0.0701 ::: bot acc: 0.0154
top acc: 0.0763 ::: bot acc: 0.0271
top acc: 0.0761 ::: bot acc: 0.0178
top acc: 0.0535 ::: bot acc: 0.0136
top acc: 0.0648 ::: bot acc: 0.0120
top acc: 0.0703 ::: bot acc: 0.0133
current epoch: 30
train loss is 0.001306
average val loss: 0.000940, accuracy: 0.0373
average test loss: 0.001135, accuracy: 0.0393
case acc: 0.040664867
case acc: 0.05071063
case acc: 0.042385116
case acc: 0.029960487
case acc: 0.0357657
case acc: 0.03606112
top acc: 0.0706 ::: bot acc: 0.0152
top acc: 0.0754 ::: bot acc: 0.0260
top acc: 0.0765 ::: bot acc: 0.0178
top acc: 0.0535 ::: bot acc: 0.0138
top acc: 0.0646 ::: bot acc: 0.0122
top acc: 0.0696 ::: bot acc: 0.0134
current epoch: 31
train loss is 0.001294
average val loss: 0.000997, accuracy: 0.0387
average test loss: 0.001188, accuracy: 0.0404
case acc: 0.041883484
case acc: 0.051190156
case acc: 0.043939393
case acc: 0.030844882
case acc: 0.037666887
case acc: 0.036828298
top acc: 0.0715 ::: bot acc: 0.0161
top acc: 0.0761 ::: bot acc: 0.0262
top acc: 0.0794 ::: bot acc: 0.0172
top acc: 0.0551 ::: bot acc: 0.0133
top acc: 0.0673 ::: bot acc: 0.0130
top acc: 0.0707 ::: bot acc: 0.0136
current epoch: 32
train loss is 0.001290
average val loss: 0.000980, accuracy: 0.0383
average test loss: 0.001173, accuracy: 0.0401
case acc: 0.041766506
case acc: 0.04957971
case acc: 0.04393237
case acc: 0.031069685
case acc: 0.038100425
case acc: 0.036414064
top acc: 0.0715 ::: bot acc: 0.0159
top acc: 0.0743 ::: bot acc: 0.0246
top acc: 0.0790 ::: bot acc: 0.0171
top acc: 0.0551 ::: bot acc: 0.0135
top acc: 0.0677 ::: bot acc: 0.0136
top acc: 0.0700 ::: bot acc: 0.0139
current epoch: 33
train loss is 0.001283
average val loss: 0.000947, accuracy: 0.0376
average test loss: 0.001141, accuracy: 0.0395
case acc: 0.04089554
case acc: 0.04792205
case acc: 0.04352611
case acc: 0.030623663
case acc: 0.03790939
case acc: 0.035933208
top acc: 0.0706 ::: bot acc: 0.0152
top acc: 0.0728 ::: bot acc: 0.0235
top acc: 0.0785 ::: bot acc: 0.0173
top acc: 0.0545 ::: bot acc: 0.0136
top acc: 0.0677 ::: bot acc: 0.0132
top acc: 0.0690 ::: bot acc: 0.0139
current epoch: 34
train loss is 0.001228
average val loss: 0.000822, accuracy: 0.0345
average test loss: 0.001018, accuracy: 0.0368
case acc: 0.037871424
case acc: 0.043107707
case acc: 0.04167342
case acc: 0.028843762
case acc: 0.035468202
case acc: 0.033912275
top acc: 0.0666 ::: bot acc: 0.0143
top acc: 0.0675 ::: bot acc: 0.0192
top acc: 0.0754 ::: bot acc: 0.0180
top acc: 0.0516 ::: bot acc: 0.0139
top acc: 0.0645 ::: bot acc: 0.0121
top acc: 0.0659 ::: bot acc: 0.0144
current epoch: 35
train loss is 0.001173
average val loss: 0.000778, accuracy: 0.0335
average test loss: 0.000975, accuracy: 0.0359
case acc: 0.036724262
case acc: 0.04073714
case acc: 0.040810496
case acc: 0.02830058
case acc: 0.034933213
case acc: 0.033632092
top acc: 0.0650 ::: bot acc: 0.0141
top acc: 0.0649 ::: bot acc: 0.0173
top acc: 0.0739 ::: bot acc: 0.0186
top acc: 0.0508 ::: bot acc: 0.0143
top acc: 0.0638 ::: bot acc: 0.0119
top acc: 0.0650 ::: bot acc: 0.0149
current epoch: 36
train loss is 0.001151
average val loss: 0.000702, accuracy: 0.0314
average test loss: 0.000903, accuracy: 0.0342
case acc: 0.03474304
case acc: 0.03720236
case acc: 0.03963581
case acc: 0.027602585
case acc: 0.03355476
case acc: 0.03269364
top acc: 0.0621 ::: bot acc: 0.0138
top acc: 0.0608 ::: bot acc: 0.0146
top acc: 0.0714 ::: bot acc: 0.0198
top acc: 0.0492 ::: bot acc: 0.0154
top acc: 0.0618 ::: bot acc: 0.0114
top acc: 0.0637 ::: bot acc: 0.0154
current epoch: 37
train loss is 0.001089
average val loss: 0.000609, accuracy: 0.0288
average test loss: 0.000812, accuracy: 0.0320
case acc: 0.032299872
case acc: 0.032783892
case acc: 0.03768935
case acc: 0.026255183
case acc: 0.031966496
case acc: 0.031206213
top acc: 0.0581 ::: bot acc: 0.0140
top acc: 0.0562 ::: bot acc: 0.0112
top acc: 0.0671 ::: bot acc: 0.0224
top acc: 0.0462 ::: bot acc: 0.0172
top acc: 0.0597 ::: bot acc: 0.0111
top acc: 0.0608 ::: bot acc: 0.0164
current epoch: 38
train loss is 0.001035
average val loss: 0.000503, accuracy: 0.0256
average test loss: 0.000713, accuracy: 0.0295
case acc: 0.029120456
case acc: 0.02807107
case acc: 0.036354326
case acc: 0.024608485
case acc: 0.029330786
case acc: 0.029542847
top acc: 0.0534 ::: bot acc: 0.0146
top acc: 0.0501 ::: bot acc: 0.0089
top acc: 0.0624 ::: bot acc: 0.0276
top acc: 0.0416 ::: bot acc: 0.0213
top acc: 0.0560 ::: bot acc: 0.0109
top acc: 0.0570 ::: bot acc: 0.0189
current epoch: 39
train loss is 0.000999
average val loss: 0.000450, accuracy: 0.0240
average test loss: 0.000662, accuracy: 0.0282
case acc: 0.027535634
case acc: 0.025474366
case acc: 0.035326313
case acc: 0.024096617
case acc: 0.028173786
case acc: 0.02852983
top acc: 0.0503 ::: bot acc: 0.0160
top acc: 0.0463 ::: bot acc: 0.0090
top acc: 0.0592 ::: bot acc: 0.0305
top acc: 0.0398 ::: bot acc: 0.0237
top acc: 0.0541 ::: bot acc: 0.0111
top acc: 0.0549 ::: bot acc: 0.0204
current epoch: 40
train loss is 0.000962
average val loss: 0.000391, accuracy: 0.0220
average test loss: 0.000603, accuracy: 0.0266
case acc: 0.02565737
case acc: 0.022508409
case acc: 0.03434725
case acc: 0.023141298
case acc: 0.026301822
case acc: 0.027650084
top acc: 0.0460 ::: bot acc: 0.0190
top acc: 0.0411 ::: bot acc: 0.0106
top acc: 0.0549 ::: bot acc: 0.0349
top acc: 0.0363 ::: bot acc: 0.0265
top acc: 0.0505 ::: bot acc: 0.0126
top acc: 0.0518 ::: bot acc: 0.0238
current epoch: 41
train loss is 0.000935
average val loss: 0.000344, accuracy: 0.0205
average test loss: 0.000563, accuracy: 0.0255
case acc: 0.024261307
case acc: 0.020328276
case acc: 0.03380132
case acc: 0.022753786
case acc: 0.02492156
case acc: 0.026797336
top acc: 0.0417 ::: bot acc: 0.0231
top acc: 0.0359 ::: bot acc: 0.0140
top acc: 0.0503 ::: bot acc: 0.0395
top acc: 0.0327 ::: bot acc: 0.0303
top acc: 0.0475 ::: bot acc: 0.0148
top acc: 0.0490 ::: bot acc: 0.0266
current epoch: 42
train loss is 0.000909
average val loss: 0.000305, accuracy: 0.0193
average test loss: 0.000530, accuracy: 0.0246
case acc: 0.02359166
case acc: 0.018474933
case acc: 0.03346366
case acc: 0.022699142
case acc: 0.022924883
case acc: 0.02652476
top acc: 0.0361 ::: bot acc: 0.0293
top acc: 0.0296 ::: bot acc: 0.0204
top acc: 0.0442 ::: bot acc: 0.0457
top acc: 0.0280 ::: bot acc: 0.0353
top acc: 0.0418 ::: bot acc: 0.0192
top acc: 0.0443 ::: bot acc: 0.0312
current epoch: 43
train loss is 0.000898
average val loss: 0.000297, accuracy: 0.0190
average test loss: 0.000527, accuracy: 0.0245
case acc: 0.023215886
case acc: 0.018012526
case acc: 0.03342256
case acc: 0.023073114
case acc: 0.022559194
case acc: 0.02670053
top acc: 0.0323 ::: bot acc: 0.0328
top acc: 0.0257 ::: bot acc: 0.0244
top acc: 0.0405 ::: bot acc: 0.0493
top acc: 0.0251 ::: bot acc: 0.0378
top acc: 0.0393 ::: bot acc: 0.0224
top acc: 0.0424 ::: bot acc: 0.0333
current epoch: 44
train loss is 0.000898
average val loss: 0.000301, accuracy: 0.0192
average test loss: 0.000530, accuracy: 0.0246
case acc: 0.023282636
case acc: 0.01806064
case acc: 0.03365383
case acc: 0.023664169
case acc: 0.02194869
case acc: 0.026943812
top acc: 0.0291 ::: bot acc: 0.0358
top acc: 0.0217 ::: bot acc: 0.0280
top acc: 0.0366 ::: bot acc: 0.0532
top acc: 0.0229 ::: bot acc: 0.0401
top acc: 0.0370 ::: bot acc: 0.0241
top acc: 0.0408 ::: bot acc: 0.0351
current epoch: 45
train loss is 0.000894
average val loss: 0.000319, accuracy: 0.0199
average test loss: 0.000553, accuracy: 0.0254
case acc: 0.024517454
case acc: 0.019232221
case acc: 0.03431387
case acc: 0.024818644
case acc: 0.02204392
case acc: 0.027525434
top acc: 0.0251 ::: bot acc: 0.0401
top acc: 0.0165 ::: bot acc: 0.0333
top acc: 0.0313 ::: bot acc: 0.0582
top acc: 0.0191 ::: bot acc: 0.0439
top acc: 0.0340 ::: bot acc: 0.0278
top acc: 0.0383 ::: bot acc: 0.0375
current epoch: 46
train loss is 0.000904
average val loss: 0.000344, accuracy: 0.0207
average test loss: 0.000579, accuracy: 0.0262
case acc: 0.025783058
case acc: 0.021221863
case acc: 0.03510076
case acc: 0.02567799
case acc: 0.021810127
case acc: 0.027682403
top acc: 0.0226 ::: bot acc: 0.0432
top acc: 0.0141 ::: bot acc: 0.0374
top acc: 0.0274 ::: bot acc: 0.0620
top acc: 0.0172 ::: bot acc: 0.0462
top acc: 0.0316 ::: bot acc: 0.0295
top acc: 0.0367 ::: bot acc: 0.0389
current epoch: 47
train loss is 0.000922
average val loss: 0.000415, accuracy: 0.0228
average test loss: 0.000655, accuracy: 0.0286
case acc: 0.028816951
case acc: 0.025088098
case acc: 0.037673302
case acc: 0.028290043
case acc: 0.022684472
case acc: 0.028872924
top acc: 0.0203 ::: bot acc: 0.0489
top acc: 0.0126 ::: bot acc: 0.0441
top acc: 0.0211 ::: bot acc: 0.0693
top acc: 0.0142 ::: bot acc: 0.0517
top acc: 0.0272 ::: bot acc: 0.0337
top acc: 0.0328 ::: bot acc: 0.0428
current epoch: 48
train loss is 0.000955
average val loss: 0.000511, accuracy: 0.0256
average test loss: 0.000755, accuracy: 0.0314
case acc: 0.032034818
case acc: 0.029785762
case acc: 0.040887408
case acc: 0.031584788
case acc: 0.024014154
case acc: 0.030095508
top acc: 0.0186 ::: bot acc: 0.0546
top acc: 0.0136 ::: bot acc: 0.0505
top acc: 0.0171 ::: bot acc: 0.0757
top acc: 0.0139 ::: bot acc: 0.0566
top acc: 0.0235 ::: bot acc: 0.0377
top acc: 0.0296 ::: bot acc: 0.0462
current epoch: 49
train loss is 0.001006
average val loss: 0.000671, accuracy: 0.0300
average test loss: 0.000924, accuracy: 0.0354
case acc: 0.036265254
case acc: 0.036098264
case acc: 0.04594687
case acc: 0.036241204
case acc: 0.02591743
case acc: 0.031984672
top acc: 0.0179 ::: bot acc: 0.0613
top acc: 0.0160 ::: bot acc: 0.0588
top acc: 0.0145 ::: bot acc: 0.0848
top acc: 0.0156 ::: bot acc: 0.0628
top acc: 0.0191 ::: bot acc: 0.0428
top acc: 0.0262 ::: bot acc: 0.0507
current epoch: 50
train loss is 0.001080
average val loss: 0.000892, accuracy: 0.0353
average test loss: 0.001155, accuracy: 0.0403
case acc: 0.041310392
case acc: 0.043865476
case acc: 0.052296493
case acc: 0.04152482
case acc: 0.028521081
case acc: 0.034098737
top acc: 0.0184 ::: bot acc: 0.0685
top acc: 0.0216 ::: bot acc: 0.0677
top acc: 0.0150 ::: bot acc: 0.0941
top acc: 0.0180 ::: bot acc: 0.0696
top acc: 0.0158 ::: bot acc: 0.0483
top acc: 0.0236 ::: bot acc: 0.0553
LME_Co_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6768 6768 6768
1.7082474 -0.6288155 0.21141115 -0.19947179
Validation: 756 756 756
Testing: 774 774 774
pre-processing time: 0.00019812583923339844
the split date is 2010-07-01
train dropout: 0.6 test dropout: 0.15
net initializing with time: 0.0022318363189697266
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.012875
average val loss: 0.006270, accuracy: 0.1039
average test loss: 0.006493, accuracy: 0.1047
case acc: 0.14934747
case acc: 0.07732151
case acc: 0.1091191
case acc: 0.09275073
case acc: 0.13860783
case acc: 0.06121163
top acc: 0.1210 ::: bot acc: 0.1772
top acc: 0.1044 ::: bot acc: 0.0493
top acc: 0.0705 ::: bot acc: 0.1479
top acc: 0.0517 ::: bot acc: 0.1323
top acc: 0.0963 ::: bot acc: 0.1741
top acc: 0.0288 ::: bot acc: 0.0925
current epoch: 2
train loss is 0.008317
average val loss: 0.003031, accuracy: 0.0577
average test loss: 0.003163, accuracy: 0.0600
case acc: 0.051360432
case acc: 0.15869087
case acc: 0.029856607
case acc: 0.030302914
case acc: 0.058830194
case acc: 0.031226724
top acc: 0.0245 ::: bot acc: 0.0786
top acc: 0.1858 ::: bot acc: 0.1308
top acc: 0.0273 ::: bot acc: 0.0495
top acc: 0.0411 ::: bot acc: 0.0421
top acc: 0.0280 ::: bot acc: 0.0883
top acc: 0.0617 ::: bot acc: 0.0135
current epoch: 3
train loss is 0.008617
average val loss: 0.006784, accuracy: 0.0920
average test loss: 0.006862, accuracy: 0.0917
case acc: 0.038312785
case acc: 0.23137839
case acc: 0.07631027
case acc: 0.07837206
case acc: 0.03178299
case acc: 0.09420748
top acc: 0.0648 ::: bot acc: 0.0139
top acc: 0.2589 ::: bot acc: 0.2028
top acc: 0.1141 ::: bot acc: 0.0391
top acc: 0.1207 ::: bot acc: 0.0396
top acc: 0.0664 ::: bot acc: 0.0128
top acc: 0.1328 ::: bot acc: 0.0596
current epoch: 4
train loss is 0.009942
average val loss: 0.015275, accuracy: 0.1607
average test loss: 0.015226, accuracy: 0.1597
case acc: 0.11402754
case acc: 0.2945918
case acc: 0.15194005
case acc: 0.14633693
case acc: 0.09599145
case acc: 0.1554702
top acc: 0.1425 ::: bot acc: 0.0859
top acc: 0.3216 ::: bot acc: 0.2671
top acc: 0.1903 ::: bot acc: 0.1144
top acc: 0.1895 ::: bot acc: 0.1056
top acc: 0.1384 ::: bot acc: 0.0609
top acc: 0.1935 ::: bot acc: 0.1217
current epoch: 5
train loss is 0.012879
average val loss: 0.013374, accuracy: 0.1503
average test loss: 0.013352, accuracy: 0.1494
case acc: 0.10900313
case acc: 0.27464855
case acc: 0.14479716
case acc: 0.13557683
case acc: 0.0928404
case acc: 0.13960089
top acc: 0.1376 ::: bot acc: 0.0807
top acc: 0.3024 ::: bot acc: 0.2471
top acc: 0.1831 ::: bot acc: 0.1069
top acc: 0.1780 ::: bot acc: 0.0952
top acc: 0.1350 ::: bot acc: 0.0581
top acc: 0.1782 ::: bot acc: 0.1052
current epoch: 6
train loss is 0.013143
average val loss: 0.003557, accuracy: 0.0627
average test loss: 0.003602, accuracy: 0.0633
case acc: 0.027943615
case acc: 0.17041545
case acc: 0.05451703
case acc: 0.048363376
case acc: 0.028425712
case acc: 0.05031933
top acc: 0.0507 ::: bot acc: 0.0112
top acc: 0.1977 ::: bot acc: 0.1426
top acc: 0.0907 ::: bot acc: 0.0218
top acc: 0.0864 ::: bot acc: 0.0183
top acc: 0.0559 ::: bot acc: 0.0219
top acc: 0.0865 ::: bot acc: 0.0206
current epoch: 7
train loss is 0.007743
average val loss: 0.001506, accuracy: 0.0445
average test loss: 0.001598, accuracy: 0.0463
case acc: 0.0404121
case acc: 0.09374502
case acc: 0.031237802
case acc: 0.036042508
case acc: 0.04679163
case acc: 0.029710952
top acc: 0.0166 ::: bot acc: 0.0663
top acc: 0.1206 ::: bot acc: 0.0663
top acc: 0.0232 ::: bot acc: 0.0541
top acc: 0.0264 ::: bot acc: 0.0601
top acc: 0.0246 ::: bot acc: 0.0718
top acc: 0.0268 ::: bot acc: 0.0464
current epoch: 8
train loss is 0.003692
average val loss: 0.001304, accuracy: 0.0398
average test loss: 0.001364, accuracy: 0.0414
case acc: 0.032514323
case acc: 0.09001638
case acc: 0.029745303
case acc: 0.032339245
case acc: 0.036483712
case acc: 0.027522033
top acc: 0.0127 ::: bot acc: 0.0564
top acc: 0.1173 ::: bot acc: 0.0630
top acc: 0.0263 ::: bot acc: 0.0500
top acc: 0.0330 ::: bot acc: 0.0517
top acc: 0.0268 ::: bot acc: 0.0547
top acc: 0.0378 ::: bot acc: 0.0355
current epoch: 9
train loss is 0.002668
average val loss: 0.001284, accuracy: 0.0378
average test loss: 0.001336, accuracy: 0.0388
case acc: 0.023134701
case acc: 0.09402708
case acc: 0.028253445
case acc: 0.030272402
case acc: 0.02930761
case acc: 0.027975563
top acc: 0.0157 ::: bot acc: 0.0408
top acc: 0.1212 ::: bot acc: 0.0660
top acc: 0.0364 ::: bot acc: 0.0404
top acc: 0.0464 ::: bot acc: 0.0383
top acc: 0.0409 ::: bot acc: 0.0368
top acc: 0.0528 ::: bot acc: 0.0199
current epoch: 10
train loss is 0.002464
average val loss: 0.001218, accuracy: 0.0370
average test loss: 0.001273, accuracy: 0.0378
case acc: 0.022004928
case acc: 0.089273125
case acc: 0.02839673
case acc: 0.030622827
case acc: 0.02789319
case acc: 0.028675353
top acc: 0.0196 ::: bot acc: 0.0371
top acc: 0.1168 ::: bot acc: 0.0611
top acc: 0.0374 ::: bot acc: 0.0399
top acc: 0.0499 ::: bot acc: 0.0351
top acc: 0.0467 ::: bot acc: 0.0306
top acc: 0.0558 ::: bot acc: 0.0164
current epoch: 11
train loss is 0.002326
average val loss: 0.001084, accuracy: 0.0354
average test loss: 0.001128, accuracy: 0.0361
case acc: 0.022082513
case acc: 0.07956802
case acc: 0.0284472
case acc: 0.030563535
case acc: 0.027852463
case acc: 0.028256098
top acc: 0.0192 ::: bot acc: 0.0371
top acc: 0.1070 ::: bot acc: 0.0520
top acc: 0.0336 ::: bot acc: 0.0431
top acc: 0.0485 ::: bot acc: 0.0365
top acc: 0.0474 ::: bot acc: 0.0299
top acc: 0.0549 ::: bot acc: 0.0175
current epoch: 12
train loss is 0.002048
average val loss: 0.001023, accuracy: 0.0347
average test loss: 0.001072, accuracy: 0.0353
case acc: 0.021150807
case acc: 0.074229054
case acc: 0.028383352
case acc: 0.030579714
case acc: 0.027808648
case acc: 0.029373191
top acc: 0.0220 ::: bot acc: 0.0342
top acc: 0.1017 ::: bot acc: 0.0463
top acc: 0.0337 ::: bot acc: 0.0429
top acc: 0.0507 ::: bot acc: 0.0340
top acc: 0.0510 ::: bot acc: 0.0258
top acc: 0.0575 ::: bot acc: 0.0155
current epoch: 13
train loss is 0.001844
average val loss: 0.000932, accuracy: 0.0335
average test loss: 0.000974, accuracy: 0.0338
case acc: 0.021146853
case acc: 0.06610898
case acc: 0.028630076
case acc: 0.030490365
case acc: 0.028074376
case acc: 0.028608993
top acc: 0.0218 ::: bot acc: 0.0340
top acc: 0.0933 ::: bot acc: 0.0383
top acc: 0.0308 ::: bot acc: 0.0456
top acc: 0.0491 ::: bot acc: 0.0357
top acc: 0.0512 ::: bot acc: 0.0265
top acc: 0.0558 ::: bot acc: 0.0165
current epoch: 14
train loss is 0.001631
average val loss: 0.000875, accuracy: 0.0326
average test loss: 0.000922, accuracy: 0.0331
case acc: 0.020823393
case acc: 0.06060749
case acc: 0.029180598
case acc: 0.030490108
case acc: 0.02811949
case acc: 0.029130822
top acc: 0.0239 ::: bot acc: 0.0318
top acc: 0.0881 ::: bot acc: 0.0329
top acc: 0.0312 ::: bot acc: 0.0460
top acc: 0.0498 ::: bot acc: 0.0346
top acc: 0.0526 ::: bot acc: 0.0253
top acc: 0.0570 ::: bot acc: 0.0158
current epoch: 15
train loss is 0.001493
average val loss: 0.000832, accuracy: 0.0319
average test loss: 0.000879, accuracy: 0.0323
case acc: 0.020872043
case acc: 0.055750523
case acc: 0.029239116
case acc: 0.030719033
case acc: 0.02810861
case acc: 0.029227456
top acc: 0.0254 ::: bot acc: 0.0307
top acc: 0.0826 ::: bot acc: 0.0282
top acc: 0.0310 ::: bot acc: 0.0463
top acc: 0.0510 ::: bot acc: 0.0340
top acc: 0.0529 ::: bot acc: 0.0250
top acc: 0.0572 ::: bot acc: 0.0159
current epoch: 16
train loss is 0.001410
average val loss: 0.000797, accuracy: 0.0313
average test loss: 0.000842, accuracy: 0.0316
case acc: 0.02078227
case acc: 0.051995847
case acc: 0.028641263
case acc: 0.030970363
case acc: 0.02828273
case acc: 0.029038057
top acc: 0.0272 ::: bot acc: 0.0291
top acc: 0.0792 ::: bot acc: 0.0247
top acc: 0.0308 ::: bot acc: 0.0455
top acc: 0.0521 ::: bot acc: 0.0331
top acc: 0.0535 ::: bot acc: 0.0245
top acc: 0.0575 ::: bot acc: 0.0147
current epoch: 17
train loss is 0.001334
average val loss: 0.000791, accuracy: 0.0312
average test loss: 0.000834, accuracy: 0.0314
case acc: 0.020627925
case acc: 0.050068937
case acc: 0.028557511
case acc: 0.03104728
case acc: 0.028175939
case acc: 0.02969155
top acc: 0.0303 ::: bot acc: 0.0261
top acc: 0.0774 ::: bot acc: 0.0225
top acc: 0.0334 ::: bot acc: 0.0432
top acc: 0.0537 ::: bot acc: 0.0307
top acc: 0.0548 ::: bot acc: 0.0230
top acc: 0.0590 ::: bot acc: 0.0137
current epoch: 18
train loss is 0.001262
average val loss: 0.000773, accuracy: 0.0310
average test loss: 0.000819, accuracy: 0.0311
case acc: 0.0207941
case acc: 0.04768115
case acc: 0.028190257
case acc: 0.03150945
case acc: 0.027936405
case acc: 0.030261647
top acc: 0.0325 ::: bot acc: 0.0238
top acc: 0.0747 ::: bot acc: 0.0206
top acc: 0.0350 ::: bot acc: 0.0417
top acc: 0.0554 ::: bot acc: 0.0292
top acc: 0.0543 ::: bot acc: 0.0230
top acc: 0.0600 ::: bot acc: 0.0142
current epoch: 19
train loss is 0.001220
average val loss: 0.000746, accuracy: 0.0304
average test loss: 0.000798, accuracy: 0.0306
case acc: 0.021089118
case acc: 0.044940364
case acc: 0.028483814
case acc: 0.031666163
case acc: 0.027937874
case acc: 0.02976457
top acc: 0.0335 ::: bot acc: 0.0234
top acc: 0.0720 ::: bot acc: 0.0180
top acc: 0.0360 ::: bot acc: 0.0416
top acc: 0.0562 ::: bot acc: 0.0288
top acc: 0.0532 ::: bot acc: 0.0241
top acc: 0.0589 ::: bot acc: 0.0142
current epoch: 20
train loss is 0.001162
average val loss: 0.000734, accuracy: 0.0301
average test loss: 0.000779, accuracy: 0.0304
case acc: 0.021212222
case acc: 0.042897
case acc: 0.028330633
case acc: 0.031814452
case acc: 0.02799067
case acc: 0.030004907
top acc: 0.0346 ::: bot acc: 0.0217
top acc: 0.0694 ::: bot acc: 0.0165
top acc: 0.0374 ::: bot acc: 0.0395
top acc: 0.0567 ::: bot acc: 0.0279
top acc: 0.0532 ::: bot acc: 0.0243
top acc: 0.0590 ::: bot acc: 0.0147
current epoch: 21
train loss is 0.001151
average val loss: 0.000725, accuracy: 0.0300
average test loss: 0.000771, accuracy: 0.0302
case acc: 0.02157018
case acc: 0.041396465
case acc: 0.028306672
case acc: 0.03207927
case acc: 0.027982043
case acc: 0.02987515
top acc: 0.0359 ::: bot acc: 0.0203
top acc: 0.0680 ::: bot acc: 0.0156
top acc: 0.0388 ::: bot acc: 0.0383
top acc: 0.0578 ::: bot acc: 0.0269
top acc: 0.0526 ::: bot acc: 0.0249
top acc: 0.0590 ::: bot acc: 0.0145
current epoch: 22
train loss is 0.001114
average val loss: 0.000753, accuracy: 0.0307
average test loss: 0.000794, accuracy: 0.0307
case acc: 0.022565456
case acc: 0.04173637
case acc: 0.028205581
case acc: 0.032963756
case acc: 0.028116316
case acc: 0.030439286
top acc: 0.0395 ::: bot acc: 0.0167
top acc: 0.0682 ::: bot acc: 0.0157
top acc: 0.0423 ::: bot acc: 0.0348
top acc: 0.0605 ::: bot acc: 0.0240
top acc: 0.0543 ::: bot acc: 0.0234
top acc: 0.0603 ::: bot acc: 0.0133
current epoch: 23
train loss is 0.001114
average val loss: 0.000786, accuracy: 0.0315
average test loss: 0.000831, accuracy: 0.0314
case acc: 0.023818016
case acc: 0.042274673
case acc: 0.028391253
case acc: 0.03411248
case acc: 0.028372355
case acc: 0.03140018
top acc: 0.0428 ::: bot acc: 0.0141
top acc: 0.0685 ::: bot acc: 0.0162
top acc: 0.0460 ::: bot acc: 0.0310
top acc: 0.0634 ::: bot acc: 0.0219
top acc: 0.0561 ::: bot acc: 0.0216
top acc: 0.0623 ::: bot acc: 0.0126
current epoch: 24
train loss is 0.001082
average val loss: 0.000754, accuracy: 0.0307
average test loss: 0.000800, accuracy: 0.0307
case acc: 0.02380634
case acc: 0.040064566
case acc: 0.028238213
case acc: 0.033841483
case acc: 0.027919197
case acc: 0.030393668
top acc: 0.0427 ::: bot acc: 0.0146
top acc: 0.0664 ::: bot acc: 0.0145
top acc: 0.0458 ::: bot acc: 0.0309
top acc: 0.0628 ::: bot acc: 0.0226
top acc: 0.0545 ::: bot acc: 0.0230
top acc: 0.0606 ::: bot acc: 0.0133
current epoch: 25
train loss is 0.001049
average val loss: 0.000776, accuracy: 0.0312
average test loss: 0.000812, accuracy: 0.0310
case acc: 0.024430275
case acc: 0.039961793
case acc: 0.028712934
case acc: 0.034241006
case acc: 0.028153574
case acc: 0.030677963
top acc: 0.0444 ::: bot acc: 0.0128
top acc: 0.0663 ::: bot acc: 0.0146
top acc: 0.0483 ::: bot acc: 0.0285
top acc: 0.0642 ::: bot acc: 0.0210
top acc: 0.0549 ::: bot acc: 0.0225
top acc: 0.0606 ::: bot acc: 0.0130
current epoch: 26
train loss is 0.001030
average val loss: 0.000756, accuracy: 0.0308
average test loss: 0.000801, accuracy: 0.0309
case acc: 0.024719933
case acc: 0.038574688
case acc: 0.02905548
case acc: 0.034359124
case acc: 0.028280172
case acc: 0.030267922
top acc: 0.0446 ::: bot acc: 0.0133
top acc: 0.0647 ::: bot acc: 0.0137
top acc: 0.0490 ::: bot acc: 0.0284
top acc: 0.0643 ::: bot acc: 0.0214
top acc: 0.0542 ::: bot acc: 0.0236
top acc: 0.0601 ::: bot acc: 0.0136
current epoch: 27
train loss is 0.001023
average val loss: 0.000782, accuracy: 0.0314
average test loss: 0.000824, accuracy: 0.0314
case acc: 0.025707047
case acc: 0.03885973
case acc: 0.02971737
case acc: 0.035187032
case acc: 0.028184988
case acc: 0.030536802
top acc: 0.0465 ::: bot acc: 0.0124
top acc: 0.0650 ::: bot acc: 0.0139
top acc: 0.0514 ::: bot acc: 0.0259
top acc: 0.0659 ::: bot acc: 0.0200
top acc: 0.0552 ::: bot acc: 0.0223
top acc: 0.0607 ::: bot acc: 0.0128
current epoch: 28
train loss is 0.001030
average val loss: 0.000817, accuracy: 0.0323
average test loss: 0.000859, accuracy: 0.0322
case acc: 0.026905555
case acc: 0.039620113
case acc: 0.030484717
case acc: 0.03635227
case acc: 0.028321994
case acc: 0.031385303
top acc: 0.0490 ::: bot acc: 0.0115
top acc: 0.0657 ::: bot acc: 0.0144
top acc: 0.0538 ::: bot acc: 0.0232
top acc: 0.0680 ::: bot acc: 0.0194
top acc: 0.0565 ::: bot acc: 0.0208
top acc: 0.0621 ::: bot acc: 0.0125
current epoch: 29
train loss is 0.001020
average val loss: 0.000858, accuracy: 0.0332
average test loss: 0.000895, accuracy: 0.0330
case acc: 0.028242452
case acc: 0.040257785
case acc: 0.03140174
case acc: 0.03711968
case acc: 0.028780581
case acc: 0.032026976
top acc: 0.0511 ::: bot acc: 0.0110
top acc: 0.0667 ::: bot acc: 0.0148
top acc: 0.0564 ::: bot acc: 0.0208
top acc: 0.0699 ::: bot acc: 0.0182
top acc: 0.0584 ::: bot acc: 0.0192
top acc: 0.0633 ::: bot acc: 0.0121
current epoch: 30
train loss is 0.001026
average val loss: 0.000858, accuracy: 0.0332
average test loss: 0.000898, accuracy: 0.0330
case acc: 0.028390013
case acc: 0.039732326
case acc: 0.031791944
case acc: 0.037325494
case acc: 0.029011903
case acc: 0.031618446
top acc: 0.0515 ::: bot acc: 0.0109
top acc: 0.0664 ::: bot acc: 0.0140
top acc: 0.0575 ::: bot acc: 0.0198
top acc: 0.0700 ::: bot acc: 0.0182
top acc: 0.0592 ::: bot acc: 0.0187
top acc: 0.0629 ::: bot acc: 0.0121
current epoch: 31
train loss is 0.001012
average val loss: 0.000874, accuracy: 0.0336
average test loss: 0.000919, accuracy: 0.0335
case acc: 0.029396998
case acc: 0.039994597
case acc: 0.03231019
case acc: 0.038010206
case acc: 0.02946311
case acc: 0.03200347
top acc: 0.0525 ::: bot acc: 0.0111
top acc: 0.0662 ::: bot acc: 0.0146
top acc: 0.0595 ::: bot acc: 0.0177
top acc: 0.0714 ::: bot acc: 0.0178
top acc: 0.0605 ::: bot acc: 0.0176
top acc: 0.0632 ::: bot acc: 0.0122
current epoch: 32
train loss is 0.001024
average val loss: 0.000914, accuracy: 0.0345
average test loss: 0.000948, accuracy: 0.0341
case acc: 0.030107273
case acc: 0.040510856
case acc: 0.03335587
case acc: 0.038864955
case acc: 0.02965816
case acc: 0.03224626
top acc: 0.0538 ::: bot acc: 0.0110
top acc: 0.0669 ::: bot acc: 0.0151
top acc: 0.0613 ::: bot acc: 0.0168
top acc: 0.0725 ::: bot acc: 0.0177
top acc: 0.0612 ::: bot acc: 0.0163
top acc: 0.0641 ::: bot acc: 0.0117
current epoch: 33
train loss is 0.001024
average val loss: 0.000965, accuracy: 0.0356
average test loss: 0.001000, accuracy: 0.0352
case acc: 0.031345066
case acc: 0.041407324
case acc: 0.034588564
case acc: 0.04028404
case acc: 0.030374601
case acc: 0.033162966
top acc: 0.0558 ::: bot acc: 0.0109
top acc: 0.0679 ::: bot acc: 0.0155
top acc: 0.0640 ::: bot acc: 0.0152
top acc: 0.0751 ::: bot acc: 0.0171
top acc: 0.0632 ::: bot acc: 0.0147
top acc: 0.0656 ::: bot acc: 0.0113
current epoch: 34
train loss is 0.001035
average val loss: 0.000955, accuracy: 0.0354
average test loss: 0.000995, accuracy: 0.0351
case acc: 0.031291265
case acc: 0.040608604
case acc: 0.034801077
case acc: 0.04043498
case acc: 0.030613802
case acc: 0.0329467
top acc: 0.0558 ::: bot acc: 0.0108
top acc: 0.0670 ::: bot acc: 0.0150
top acc: 0.0642 ::: bot acc: 0.0155
top acc: 0.0755 ::: bot acc: 0.0174
top acc: 0.0638 ::: bot acc: 0.0146
top acc: 0.0651 ::: bot acc: 0.0114
current epoch: 35
train loss is 0.001019
average val loss: 0.000954, accuracy: 0.0354
average test loss: 0.000989, accuracy: 0.0350
case acc: 0.031121938
case acc: 0.03986542
case acc: 0.035228163
case acc: 0.04041957
case acc: 0.03042265
case acc: 0.032857012
top acc: 0.0556 ::: bot acc: 0.0109
top acc: 0.0661 ::: bot acc: 0.0145
top acc: 0.0649 ::: bot acc: 0.0151
top acc: 0.0753 ::: bot acc: 0.0173
top acc: 0.0633 ::: bot acc: 0.0143
top acc: 0.0650 ::: bot acc: 0.0117
current epoch: 36
train loss is 0.001006
average val loss: 0.000969, accuracy: 0.0357
average test loss: 0.001009, accuracy: 0.0353
case acc: 0.03120428
case acc: 0.039784666
case acc: 0.035856403
case acc: 0.041178275
case acc: 0.030958531
case acc: 0.033025637
top acc: 0.0555 ::: bot acc: 0.0108
top acc: 0.0659 ::: bot acc: 0.0144
top acc: 0.0663 ::: bot acc: 0.0151
top acc: 0.0767 ::: bot acc: 0.0172
top acc: 0.0647 ::: bot acc: 0.0140
top acc: 0.0653 ::: bot acc: 0.0114
current epoch: 37
train loss is 0.000989
average val loss: 0.000925, accuracy: 0.0347
average test loss: 0.000964, accuracy: 0.0343
case acc: 0.029971315
case acc: 0.038092043
case acc: 0.0349163
case acc: 0.040435757
case acc: 0.030134272
case acc: 0.032479502
top acc: 0.0536 ::: bot acc: 0.0108
top acc: 0.0640 ::: bot acc: 0.0136
top acc: 0.0646 ::: bot acc: 0.0150
top acc: 0.0756 ::: bot acc: 0.0173
top acc: 0.0631 ::: bot acc: 0.0144
top acc: 0.0645 ::: bot acc: 0.0118
current epoch: 38
train loss is 0.000964
average val loss: 0.000875, accuracy: 0.0335
average test loss: 0.000916, accuracy: 0.0332
case acc: 0.028714545
case acc: 0.03565225
case acc: 0.034117203
case acc: 0.039328087
case acc: 0.030067699
case acc: 0.031585664
top acc: 0.0520 ::: bot acc: 0.0108
top acc: 0.0610 ::: bot acc: 0.0120
top acc: 0.0629 ::: bot acc: 0.0161
top acc: 0.0739 ::: bot acc: 0.0173
top acc: 0.0623 ::: bot acc: 0.0158
top acc: 0.0628 ::: bot acc: 0.0123
current epoch: 39
train loss is 0.000939
average val loss: 0.000825, accuracy: 0.0323
average test loss: 0.000865, accuracy: 0.0321
case acc: 0.027293833
case acc: 0.033141572
case acc: 0.033111244
case acc: 0.038561083
case acc: 0.02971552
case acc: 0.030561248
top acc: 0.0495 ::: bot acc: 0.0112
top acc: 0.0581 ::: bot acc: 0.0106
top acc: 0.0608 ::: bot acc: 0.0171
top acc: 0.0724 ::: bot acc: 0.0178
top acc: 0.0615 ::: bot acc: 0.0164
top acc: 0.0609 ::: bot acc: 0.0129
current epoch: 40
train loss is 0.000910
average val loss: 0.000803, accuracy: 0.0317
average test loss: 0.000844, accuracy: 0.0316
case acc: 0.026597874
case acc: 0.03215066
case acc: 0.032762483
case acc: 0.037947923
case acc: 0.029522365
case acc: 0.03037551
top acc: 0.0483 ::: bot acc: 0.0117
top acc: 0.0565 ::: bot acc: 0.0105
top acc: 0.0599 ::: bot acc: 0.0178
top acc: 0.0713 ::: bot acc: 0.0178
top acc: 0.0612 ::: bot acc: 0.0164
top acc: 0.0607 ::: bot acc: 0.0130
current epoch: 41
train loss is 0.000885
average val loss: 0.000738, accuracy: 0.0301
average test loss: 0.000783, accuracy: 0.0302
case acc: 0.02499963
case acc: 0.02944273
case acc: 0.03170046
case acc: 0.036754116
case acc: 0.028840508
case acc: 0.029389951
top acc: 0.0453 ::: bot acc: 0.0129
top acc: 0.0526 ::: bot acc: 0.0101
top acc: 0.0572 ::: bot acc: 0.0203
top acc: 0.0693 ::: bot acc: 0.0186
top acc: 0.0587 ::: bot acc: 0.0186
top acc: 0.0580 ::: bot acc: 0.0152
current epoch: 42
train loss is 0.000848
average val loss: 0.000670, accuracy: 0.0284
average test loss: 0.000720, accuracy: 0.0288
case acc: 0.023216957
case acc: 0.027176145
case acc: 0.030343112
case acc: 0.03525871
case acc: 0.0283593
case acc: 0.028553786
top acc: 0.0411 ::: bot acc: 0.0155
top acc: 0.0488 ::: bot acc: 0.0114
top acc: 0.0535 ::: bot acc: 0.0234
top acc: 0.0661 ::: bot acc: 0.0203
top acc: 0.0562 ::: bot acc: 0.0218
top acc: 0.0552 ::: bot acc: 0.0179
current epoch: 43
train loss is 0.000818
average val loss: 0.000609, accuracy: 0.0268
average test loss: 0.000663, accuracy: 0.0276
case acc: 0.021996979
case acc: 0.024728952
case acc: 0.029156655
case acc: 0.03365622
case acc: 0.028007755
case acc: 0.027771408
top acc: 0.0371 ::: bot acc: 0.0197
top acc: 0.0440 ::: bot acc: 0.0134
top acc: 0.0496 ::: bot acc: 0.0279
top acc: 0.0626 ::: bot acc: 0.0221
top acc: 0.0527 ::: bot acc: 0.0249
top acc: 0.0518 ::: bot acc: 0.0210
current epoch: 44
train loss is 0.000799
average val loss: 0.000590, accuracy: 0.0263
average test loss: 0.000644, accuracy: 0.0271
case acc: 0.021374699
case acc: 0.023447426
case acc: 0.028620983
case acc: 0.033325363
case acc: 0.02800942
case acc: 0.027647942
top acc: 0.0351 ::: bot acc: 0.0214
top acc: 0.0413 ::: bot acc: 0.0149
top acc: 0.0473 ::: bot acc: 0.0295
top acc: 0.0618 ::: bot acc: 0.0233
top acc: 0.0520 ::: bot acc: 0.0256
top acc: 0.0512 ::: bot acc: 0.0214
current epoch: 45
train loss is 0.000782
average val loss: 0.000565, accuracy: 0.0256
average test loss: 0.000623, accuracy: 0.0266
case acc: 0.021007305
case acc: 0.0221257
case acc: 0.02831729
case acc: 0.032720923
case acc: 0.02814736
case acc: 0.027510399
top acc: 0.0324 ::: bot acc: 0.0242
top acc: 0.0381 ::: bot acc: 0.0173
top acc: 0.0444 ::: bot acc: 0.0327
top acc: 0.0598 ::: bot acc: 0.0251
top acc: 0.0508 ::: bot acc: 0.0274
top acc: 0.0500 ::: bot acc: 0.0227
current epoch: 46
train loss is 0.000767
average val loss: 0.000527, accuracy: 0.0246
average test loss: 0.000593, accuracy: 0.0262
case acc: 0.021027282
case acc: 0.020847125
case acc: 0.02828063
case acc: 0.0315159
case acc: 0.0281563
case acc: 0.027195925
top acc: 0.0272 ::: bot acc: 0.0296
top acc: 0.0317 ::: bot acc: 0.0240
top acc: 0.0383 ::: bot acc: 0.0387
top acc: 0.0553 ::: bot acc: 0.0299
top acc: 0.0461 ::: bot acc: 0.0317
top acc: 0.0464 ::: bot acc: 0.0268
current epoch: 47
train loss is 0.000754
average val loss: 0.000520, accuracy: 0.0244
average test loss: 0.000585, accuracy: 0.0261
case acc: 0.021103809
case acc: 0.02041636
case acc: 0.028470058
case acc: 0.031134617
case acc: 0.028352426
case acc: 0.027208261
top acc: 0.0238 ::: bot acc: 0.0329
top acc: 0.0280 ::: bot acc: 0.0274
top acc: 0.0347 ::: bot acc: 0.0423
top acc: 0.0530 ::: bot acc: 0.0321
top acc: 0.0444 ::: bot acc: 0.0332
top acc: 0.0448 ::: bot acc: 0.0283
current epoch: 48
train loss is 0.000755
average val loss: 0.000522, accuracy: 0.0246
average test loss: 0.000592, accuracy: 0.0265
case acc: 0.022000493
case acc: 0.020845909
case acc: 0.02942284
case acc: 0.030446405
case acc: 0.028932832
case acc: 0.02707098
top acc: 0.0202 ::: bot acc: 0.0371
top acc: 0.0230 ::: bot acc: 0.0328
top acc: 0.0297 ::: bot acc: 0.0474
top acc: 0.0496 ::: bot acc: 0.0350
top acc: 0.0422 ::: bot acc: 0.0355
top acc: 0.0424 ::: bot acc: 0.0305
current epoch: 49
train loss is 0.000760
average val loss: 0.000545, accuracy: 0.0254
average test loss: 0.000621, accuracy: 0.0275
case acc: 0.023822915
case acc: 0.022134045
case acc: 0.03122994
case acc: 0.030496286
case acc: 0.029832873
case acc: 0.027262839
top acc: 0.0155 ::: bot acc: 0.0421
top acc: 0.0162 ::: bot acc: 0.0391
top acc: 0.0235 ::: bot acc: 0.0539
top acc: 0.0453 ::: bot acc: 0.0397
top acc: 0.0389 ::: bot acc: 0.0391
top acc: 0.0397 ::: bot acc: 0.0333
current epoch: 50
train loss is 0.000773
average val loss: 0.000615, accuracy: 0.0276
average test loss: 0.000699, accuracy: 0.0298
case acc: 0.027648656
case acc: 0.025730371
case acc: 0.034817386
case acc: 0.031215142
case acc: 0.03142798
case acc: 0.027762841
top acc: 0.0128 ::: bot acc: 0.0491
top acc: 0.0106 ::: bot acc: 0.0476
top acc: 0.0174 ::: bot acc: 0.0624
top acc: 0.0390 ::: bot acc: 0.0456
top acc: 0.0338 ::: bot acc: 0.0442
top acc: 0.0352 ::: bot acc: 0.0378
LME_Co_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6798 6798 6798
1.7082474 -0.6288155 0.21141115 -0.19947179
Validation: 756 756 756
Testing: 750 750 750
pre-processing time: 0.00020384788513183594
the split date is 2011-01-01
train dropout: 0.6 test dropout: 0.15
net initializing with time: 0.002279520034790039
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.013199
average val loss: 0.005802, accuracy: 0.0986
average test loss: 0.004830, accuracy: 0.0892
case acc: 0.129679
case acc: 0.09520941
case acc: 0.0823659
case acc: 0.0728335
case acc: 0.10574652
case acc: 0.04961386
top acc: 0.0969 ::: bot acc: 0.1637
top acc: 0.1239 ::: bot acc: 0.0626
top acc: 0.0414 ::: bot acc: 0.1256
top acc: 0.0393 ::: bot acc: 0.1062
top acc: 0.0625 ::: bot acc: 0.1480
top acc: 0.0197 ::: bot acc: 0.0839
current epoch: 2
train loss is 0.007939
average val loss: 0.003405, accuracy: 0.0602
average test loss: 0.003590, accuracy: 0.0597
case acc: 0.03201732
case acc: 0.18105036
case acc: 0.037127458
case acc: 0.029839607
case acc: 0.033487543
case acc: 0.044542216
top acc: 0.0093 ::: bot acc: 0.0620
top acc: 0.2094 ::: bot acc: 0.1495
top acc: 0.0655 ::: bot acc: 0.0254
top acc: 0.0554 ::: bot acc: 0.0146
top acc: 0.0260 ::: bot acc: 0.0564
top acc: 0.0784 ::: bot acc: 0.0153
current epoch: 3
train loss is 0.008426
average val loss: 0.007908, accuracy: 0.1019
average test loss: 0.009075, accuracy: 0.1138
case acc: 0.05807253
case acc: 0.25160185
case acc: 0.10552734
case acc: 0.09792777
case acc: 0.06050291
case acc: 0.108986475
top acc: 0.0889 ::: bot acc: 0.0257
top acc: 0.2791 ::: bot acc: 0.2208
top acc: 0.1502 ::: bot acc: 0.0605
top acc: 0.1330 ::: bot acc: 0.0636
top acc: 0.1015 ::: bot acc: 0.0219
top acc: 0.1480 ::: bot acc: 0.0701
current epoch: 4
train loss is 0.010596
average val loss: 0.015848, accuracy: 0.1637
average test loss: 0.017739, accuracy: 0.1758
case acc: 0.12730977
case acc: 0.30699003
case acc: 0.17348777
case acc: 0.1589325
case acc: 0.124264434
case acc: 0.16352154
top acc: 0.1595 ::: bot acc: 0.0931
top acc: 0.3349 ::: bot acc: 0.2764
top acc: 0.2181 ::: bot acc: 0.1283
top acc: 0.1931 ::: bot acc: 0.1247
top acc: 0.1660 ::: bot acc: 0.0842
top acc: 0.2026 ::: bot acc: 0.1251
current epoch: 5
train loss is 0.012857
average val loss: 0.014015, accuracy: 0.1540
average test loss: 0.015815, accuracy: 0.1660
case acc: 0.12303542
case acc: 0.28729436
case acc: 0.16701588
case acc: 0.14865337
case acc: 0.12169085
case acc: 0.14821419
top acc: 0.1548 ::: bot acc: 0.0892
top acc: 0.3150 ::: bot acc: 0.2570
top acc: 0.2112 ::: bot acc: 0.1216
top acc: 0.1828 ::: bot acc: 0.1139
top acc: 0.1638 ::: bot acc: 0.0808
top acc: 0.1876 ::: bot acc: 0.1100
current epoch: 6
train loss is 0.013160
average val loss: 0.003360, accuracy: 0.0602
average test loss: 0.003998, accuracy: 0.0694
case acc: 0.032789554
case acc: 0.17436142
case acc: 0.06770346
case acc: 0.050244804
case acc: 0.039905086
case acc: 0.05147708
top acc: 0.0587 ::: bot acc: 0.0111
top acc: 0.2021 ::: bot acc: 0.1440
top acc: 0.1103 ::: bot acc: 0.0273
top acc: 0.0835 ::: bot acc: 0.0193
top acc: 0.0748 ::: bot acc: 0.0132
top acc: 0.0879 ::: bot acc: 0.0190
current epoch: 7
train loss is 0.007673
average val loss: 0.001572, accuracy: 0.0447
average test loss: 0.001603, accuracy: 0.0427
case acc: 0.029521003
case acc: 0.10771355
case acc: 0.034011755
case acc: 0.02588651
case acc: 0.031214848
case acc: 0.02788284
top acc: 0.0117 ::: bot acc: 0.0561
top acc: 0.1357 ::: bot acc: 0.0773
top acc: 0.0531 ::: bot acc: 0.0377
top acc: 0.0311 ::: bot acc: 0.0386
top acc: 0.0349 ::: bot acc: 0.0481
top acc: 0.0374 ::: bot acc: 0.0391
current epoch: 8
train loss is 0.003519
average val loss: 0.001401, accuracy: 0.0410
average test loss: 0.001505, accuracy: 0.0413
case acc: 0.025332188
case acc: 0.10343086
case acc: 0.034363594
case acc: 0.025329368
case acc: 0.03066954
case acc: 0.028857404
top acc: 0.0177 ::: bot acc: 0.0472
top acc: 0.1312 ::: bot acc: 0.0730
top acc: 0.0559 ::: bot acc: 0.0345
top acc: 0.0386 ::: bot acc: 0.0302
top acc: 0.0504 ::: bot acc: 0.0322
top acc: 0.0468 ::: bot acc: 0.0300
current epoch: 9
train loss is 0.002611
average val loss: 0.001479, accuracy: 0.0400
average test loss: 0.001777, accuracy: 0.0456
case acc: 0.02471075
case acc: 0.10962517
case acc: 0.038071863
case acc: 0.029632311
case acc: 0.037063736
case acc: 0.034436904
top acc: 0.0359 ::: bot acc: 0.0296
top acc: 0.1377 ::: bot acc: 0.0794
top acc: 0.0688 ::: bot acc: 0.0227
top acc: 0.0542 ::: bot acc: 0.0163
top acc: 0.0703 ::: bot acc: 0.0149
top acc: 0.0637 ::: bot acc: 0.0157
current epoch: 10
train loss is 0.002537
average val loss: 0.001413, accuracy: 0.0394
average test loss: 0.001755, accuracy: 0.0462
case acc: 0.025319224
case acc: 0.104833096
case acc: 0.038557135
case acc: 0.031195743
case acc: 0.040862903
case acc: 0.03625848
top acc: 0.0400 ::: bot acc: 0.0256
top acc: 0.1325 ::: bot acc: 0.0748
top acc: 0.0699 ::: bot acc: 0.0219
top acc: 0.0571 ::: bot acc: 0.0146
top acc: 0.0765 ::: bot acc: 0.0137
top acc: 0.0671 ::: bot acc: 0.0145
current epoch: 11
train loss is 0.002338
average val loss: 0.001237, accuracy: 0.0374
average test loss: 0.001531, accuracy: 0.0434
case acc: 0.024939219
case acc: 0.093952395
case acc: 0.036684804
case acc: 0.029969713
case acc: 0.04004884
case acc: 0.0347048
top acc: 0.0382 ::: bot acc: 0.0271
top acc: 0.1215 ::: bot acc: 0.0639
top acc: 0.0647 ::: bot acc: 0.0266
top acc: 0.0545 ::: bot acc: 0.0161
top acc: 0.0756 ::: bot acc: 0.0131
top acc: 0.0648 ::: bot acc: 0.0152
current epoch: 12
train loss is 0.002041
average val loss: 0.001102, accuracy: 0.0356
average test loss: 0.001357, accuracy: 0.0412
case acc: 0.024813818
case acc: 0.08415719
case acc: 0.035382967
case acc: 0.029036125
case acc: 0.03994807
case acc: 0.034098294
top acc: 0.0372 ::: bot acc: 0.0287
top acc: 0.1118 ::: bot acc: 0.0543
top acc: 0.0602 ::: bot acc: 0.0304
top acc: 0.0526 ::: bot acc: 0.0177
top acc: 0.0756 ::: bot acc: 0.0139
top acc: 0.0630 ::: bot acc: 0.0162
current epoch: 13
train loss is 0.001774
average val loss: 0.001058, accuracy: 0.0350
average test loss: 0.001331, accuracy: 0.0412
case acc: 0.025341896
case acc: 0.07975733
case acc: 0.035391424
case acc: 0.029852068
case acc: 0.04161785
case acc: 0.035129756
top acc: 0.0400 ::: bot acc: 0.0255
top acc: 0.1075 ::: bot acc: 0.0500
top acc: 0.0611 ::: bot acc: 0.0296
top acc: 0.0545 ::: bot acc: 0.0163
top acc: 0.0775 ::: bot acc: 0.0137
top acc: 0.0651 ::: bot acc: 0.0157
current epoch: 14
train loss is 0.001644
average val loss: 0.000991, accuracy: 0.0340
average test loss: 0.001267, accuracy: 0.0404
case acc: 0.025546059
case acc: 0.074064605
case acc: 0.03533728
case acc: 0.030276546
case acc: 0.04214507
case acc: 0.035133112
top acc: 0.0414 ::: bot acc: 0.0236
top acc: 0.1017 ::: bot acc: 0.0442
top acc: 0.0607 ::: bot acc: 0.0296
top acc: 0.0557 ::: bot acc: 0.0156
top acc: 0.0784 ::: bot acc: 0.0132
top acc: 0.0654 ::: bot acc: 0.0147
current epoch: 15
train loss is 0.001497
average val loss: 0.000929, accuracy: 0.0330
average test loss: 0.001178, accuracy: 0.0391
case acc: 0.025557403
case acc: 0.0679739
case acc: 0.035026513
case acc: 0.029906757
case acc: 0.041345924
case acc: 0.03485731
top acc: 0.0418 ::: bot acc: 0.0234
top acc: 0.0957 ::: bot acc: 0.0382
top acc: 0.0589 ::: bot acc: 0.0318
top acc: 0.0548 ::: bot acc: 0.0161
top acc: 0.0772 ::: bot acc: 0.0133
top acc: 0.0646 ::: bot acc: 0.0155
current epoch: 16
train loss is 0.001427
average val loss: 0.000919, accuracy: 0.0329
average test loss: 0.001197, accuracy: 0.0397
case acc: 0.026815122
case acc: 0.06577438
case acc: 0.035307672
case acc: 0.03140751
case acc: 0.04295764
case acc: 0.036042813
top acc: 0.0456 ::: bot acc: 0.0199
top acc: 0.0931 ::: bot acc: 0.0363
top acc: 0.0612 ::: bot acc: 0.0289
top acc: 0.0579 ::: bot acc: 0.0147
top acc: 0.0795 ::: bot acc: 0.0137
top acc: 0.0663 ::: bot acc: 0.0148
current epoch: 17
train loss is 0.001336
average val loss: 0.000916, accuracy: 0.0329
average test loss: 0.001214, accuracy: 0.0402
case acc: 0.0278555
case acc: 0.063640974
case acc: 0.036293685
case acc: 0.032572474
case acc: 0.043964017
case acc: 0.036941066
top acc: 0.0480 ::: bot acc: 0.0180
top acc: 0.0910 ::: bot acc: 0.0342
top acc: 0.0632 ::: bot acc: 0.0270
top acc: 0.0598 ::: bot acc: 0.0139
top acc: 0.0811 ::: bot acc: 0.0132
top acc: 0.0685 ::: bot acc: 0.0144
current epoch: 18
train loss is 0.001303
average val loss: 0.000886, accuracy: 0.0324
average test loss: 0.001177, accuracy: 0.0396
case acc: 0.02821466
case acc: 0.05987781
case acc: 0.036527734
case acc: 0.033032123
case acc: 0.04311722
case acc: 0.03661541
top acc: 0.0496 ::: bot acc: 0.0163
top acc: 0.0872 ::: bot acc: 0.0305
top acc: 0.0642 ::: bot acc: 0.0265
top acc: 0.0604 ::: bot acc: 0.0139
top acc: 0.0801 ::: bot acc: 0.0128
top acc: 0.0683 ::: bot acc: 0.0143
current epoch: 19
train loss is 0.001223
average val loss: 0.000864, accuracy: 0.0321
average test loss: 0.001158, accuracy: 0.0394
case acc: 0.028915446
case acc: 0.057573758
case acc: 0.036567267
case acc: 0.03378455
case acc: 0.042950418
case acc: 0.036686953
top acc: 0.0508 ::: bot acc: 0.0156
top acc: 0.0846 ::: bot acc: 0.0287
top acc: 0.0647 ::: bot acc: 0.0252
top acc: 0.0614 ::: bot acc: 0.0140
top acc: 0.0799 ::: bot acc: 0.0127
top acc: 0.0675 ::: bot acc: 0.0146
current epoch: 20
train loss is 0.001151
average val loss: 0.000843, accuracy: 0.0316
average test loss: 0.001144, accuracy: 0.0391
case acc: 0.029524324
case acc: 0.055192176
case acc: 0.037342597
case acc: 0.033875383
case acc: 0.042541824
case acc: 0.036293894
top acc: 0.0519 ::: bot acc: 0.0151
top acc: 0.0822 ::: bot acc: 0.0264
top acc: 0.0668 ::: bot acc: 0.0242
top acc: 0.0621 ::: bot acc: 0.0137
top acc: 0.0793 ::: bot acc: 0.0128
top acc: 0.0675 ::: bot acc: 0.0143
current epoch: 21
train loss is 0.001112
average val loss: 0.000848, accuracy: 0.0318
average test loss: 0.001158, accuracy: 0.0395
case acc: 0.030578757
case acc: 0.05412296
case acc: 0.037995055
case acc: 0.035040967
case acc: 0.042744543
case acc: 0.03656553
top acc: 0.0547 ::: bot acc: 0.0134
top acc: 0.0811 ::: bot acc: 0.0257
top acc: 0.0685 ::: bot acc: 0.0221
top acc: 0.0635 ::: bot acc: 0.0136
top acc: 0.0795 ::: bot acc: 0.0134
top acc: 0.0679 ::: bot acc: 0.0146
current epoch: 22
train loss is 0.001106
average val loss: 0.000866, accuracy: 0.0322
average test loss: 0.001195, accuracy: 0.0403
case acc: 0.032083523
case acc: 0.05371916
case acc: 0.03929607
case acc: 0.036255173
case acc: 0.04311337
case acc: 0.03727407
top acc: 0.0570 ::: bot acc: 0.0126
top acc: 0.0806 ::: bot acc: 0.0253
top acc: 0.0713 ::: bot acc: 0.0204
top acc: 0.0657 ::: bot acc: 0.0133
top acc: 0.0800 ::: bot acc: 0.0131
top acc: 0.0688 ::: bot acc: 0.0146
current epoch: 23
train loss is 0.001086
average val loss: 0.000851, accuracy: 0.0319
average test loss: 0.001171, accuracy: 0.0399
case acc: 0.032378804
case acc: 0.051505502
case acc: 0.03972061
case acc: 0.036565624
case acc: 0.042404976
case acc: 0.036576368
top acc: 0.0578 ::: bot acc: 0.0122
top acc: 0.0783 ::: bot acc: 0.0235
top acc: 0.0724 ::: bot acc: 0.0197
top acc: 0.0661 ::: bot acc: 0.0134
top acc: 0.0789 ::: bot acc: 0.0132
top acc: 0.0675 ::: bot acc: 0.0144
current epoch: 24
train loss is 0.001037
average val loss: 0.000842, accuracy: 0.0318
average test loss: 0.001173, accuracy: 0.0399
case acc: 0.032885432
case acc: 0.05036113
case acc: 0.04064177
case acc: 0.036934257
case acc: 0.041949656
case acc: 0.036438446
top acc: 0.0587 ::: bot acc: 0.0120
top acc: 0.0771 ::: bot acc: 0.0224
top acc: 0.0741 ::: bot acc: 0.0190
top acc: 0.0670 ::: bot acc: 0.0132
top acc: 0.0782 ::: bot acc: 0.0130
top acc: 0.0677 ::: bot acc: 0.0144
current epoch: 25
train loss is 0.001030
average val loss: 0.000917, accuracy: 0.0334
average test loss: 0.001285, accuracy: 0.0421
case acc: 0.035379995
case acc: 0.05225238
case acc: 0.042696457
case acc: 0.03985557
case acc: 0.043791756
case acc: 0.03832992
top acc: 0.0626 ::: bot acc: 0.0111
top acc: 0.0789 ::: bot acc: 0.0240
top acc: 0.0785 ::: bot acc: 0.0164
top acc: 0.0706 ::: bot acc: 0.0142
top acc: 0.0810 ::: bot acc: 0.0130
top acc: 0.0707 ::: bot acc: 0.0142
current epoch: 26
train loss is 0.001049
average val loss: 0.000966, accuracy: 0.0345
average test loss: 0.001361, accuracy: 0.0435
case acc: 0.037334498
case acc: 0.05314347
case acc: 0.044600103
case acc: 0.04155942
case acc: 0.045010857
case acc: 0.039455358
top acc: 0.0654 ::: bot acc: 0.0113
top acc: 0.0800 ::: bot acc: 0.0248
top acc: 0.0813 ::: bot acc: 0.0157
top acc: 0.0729 ::: bot acc: 0.0145
top acc: 0.0829 ::: bot acc: 0.0131
top acc: 0.0721 ::: bot acc: 0.0145
current epoch: 27
train loss is 0.001035
average val loss: 0.000917, accuracy: 0.0334
average test loss: 0.001291, accuracy: 0.0422
case acc: 0.03675619
case acc: 0.05048173
case acc: 0.04426412
case acc: 0.040384952
case acc: 0.043912213
case acc: 0.037548598
top acc: 0.0644 ::: bot acc: 0.0116
top acc: 0.0771 ::: bot acc: 0.0224
top acc: 0.0807 ::: bot acc: 0.0162
top acc: 0.0711 ::: bot acc: 0.0141
top acc: 0.0810 ::: bot acc: 0.0130
top acc: 0.0693 ::: bot acc: 0.0143
current epoch: 28
train loss is 0.001001
average val loss: 0.000912, accuracy: 0.0333
average test loss: 0.001293, accuracy: 0.0422
case acc: 0.03675366
case acc: 0.04952612
case acc: 0.044802103
case acc: 0.040664505
case acc: 0.044138487
case acc: 0.037396435
top acc: 0.0647 ::: bot acc: 0.0112
top acc: 0.0761 ::: bot acc: 0.0218
top acc: 0.0820 ::: bot acc: 0.0153
top acc: 0.0717 ::: bot acc: 0.0142
top acc: 0.0812 ::: bot acc: 0.0135
top acc: 0.0689 ::: bot acc: 0.0147
current epoch: 29
train loss is 0.001001
average val loss: 0.000970, accuracy: 0.0346
average test loss: 0.001375, accuracy: 0.0438
case acc: 0.038781807
case acc: 0.05068765
case acc: 0.046855472
case acc: 0.04248422
case acc: 0.045621037
case acc: 0.03809976
top acc: 0.0676 ::: bot acc: 0.0116
top acc: 0.0773 ::: bot acc: 0.0227
top acc: 0.0850 ::: bot acc: 0.0156
top acc: 0.0738 ::: bot acc: 0.0151
top acc: 0.0837 ::: bot acc: 0.0131
top acc: 0.0703 ::: bot acc: 0.0143
current epoch: 30
train loss is 0.000990
average val loss: 0.001002, accuracy: 0.0353
average test loss: 0.001430, accuracy: 0.0448
case acc: 0.040083993
case acc: 0.051036842
case acc: 0.048309352
case acc: 0.043761607
case acc: 0.04676085
case acc: 0.038738314
top acc: 0.0692 ::: bot acc: 0.0120
top acc: 0.0780 ::: bot acc: 0.0229
top acc: 0.0872 ::: bot acc: 0.0158
top acc: 0.0757 ::: bot acc: 0.0159
top acc: 0.0852 ::: bot acc: 0.0135
top acc: 0.0712 ::: bot acc: 0.0142
current epoch: 31
train loss is 0.000976
average val loss: 0.000964, accuracy: 0.0345
average test loss: 0.001376, accuracy: 0.0438
case acc: 0.039054375
case acc: 0.04921909
case acc: 0.04815393
case acc: 0.042728744
case acc: 0.04586866
case acc: 0.03765868
top acc: 0.0678 ::: bot acc: 0.0116
top acc: 0.0755 ::: bot acc: 0.0218
top acc: 0.0867 ::: bot acc: 0.0157
top acc: 0.0745 ::: bot acc: 0.0151
top acc: 0.0840 ::: bot acc: 0.0132
top acc: 0.0695 ::: bot acc: 0.0144
current epoch: 32
train loss is 0.000966
average val loss: 0.000993, accuracy: 0.0351
average test loss: 0.001422, accuracy: 0.0446
case acc: 0.040191688
case acc: 0.04914628
case acc: 0.049552854
case acc: 0.04404856
case acc: 0.046941582
case acc: 0.037873883
top acc: 0.0694 ::: bot acc: 0.0120
top acc: 0.0755 ::: bot acc: 0.0217
top acc: 0.0887 ::: bot acc: 0.0164
top acc: 0.0760 ::: bot acc: 0.0158
top acc: 0.0855 ::: bot acc: 0.0135
top acc: 0.0699 ::: bot acc: 0.0141
current epoch: 33
train loss is 0.000975
average val loss: 0.001038, accuracy: 0.0360
average test loss: 0.001490, accuracy: 0.0459
case acc: 0.041549657
case acc: 0.05018495
case acc: 0.051027797
case acc: 0.045562014
case acc: 0.048176255
case acc: 0.038842056
top acc: 0.0714 ::: bot acc: 0.0126
top acc: 0.0770 ::: bot acc: 0.0223
top acc: 0.0907 ::: bot acc: 0.0166
top acc: 0.0779 ::: bot acc: 0.0166
top acc: 0.0871 ::: bot acc: 0.0139
top acc: 0.0713 ::: bot acc: 0.0144
current epoch: 34
train loss is 0.000969
average val loss: 0.001045, accuracy: 0.0361
average test loss: 0.001496, accuracy: 0.0460
case acc: 0.04140901
case acc: 0.049655944
case acc: 0.051998064
case acc: 0.046079602
case acc: 0.047971744
case acc: 0.038777582
top acc: 0.0709 ::: bot acc: 0.0126
top acc: 0.0764 ::: bot acc: 0.0218
top acc: 0.0917 ::: bot acc: 0.0170
top acc: 0.0785 ::: bot acc: 0.0167
top acc: 0.0868 ::: bot acc: 0.0139
top acc: 0.0713 ::: bot acc: 0.0141
current epoch: 35
train loss is 0.000978
average val loss: 0.001055, accuracy: 0.0364
average test loss: 0.001512, accuracy: 0.0463
case acc: 0.041660536
case acc: 0.049429458
case acc: 0.05256879
case acc: 0.04653348
case acc: 0.048424914
case acc: 0.03898622
top acc: 0.0711 ::: bot acc: 0.0128
top acc: 0.0761 ::: bot acc: 0.0218
top acc: 0.0927 ::: bot acc: 0.0171
top acc: 0.0793 ::: bot acc: 0.0169
top acc: 0.0874 ::: bot acc: 0.0139
top acc: 0.0717 ::: bot acc: 0.0144
current epoch: 36
train loss is 0.000962
average val loss: 0.001083, accuracy: 0.0370
average test loss: 0.001557, accuracy: 0.0471
case acc: 0.04230832
case acc: 0.04958784
case acc: 0.053941254
case acc: 0.04787674
case acc: 0.049283862
case acc: 0.039527126
top acc: 0.0721 ::: bot acc: 0.0129
top acc: 0.0764 ::: bot acc: 0.0219
top acc: 0.0945 ::: bot acc: 0.0176
top acc: 0.0808 ::: bot acc: 0.0179
top acc: 0.0883 ::: bot acc: 0.0145
top acc: 0.0724 ::: bot acc: 0.0140
current epoch: 37
train loss is 0.000952
average val loss: 0.001068, accuracy: 0.0367
average test loss: 0.001540, accuracy: 0.0467
case acc: 0.041981176
case acc: 0.048317265
case acc: 0.05349624
case acc: 0.0477681
case acc: 0.049566578
case acc: 0.039352566
top acc: 0.0717 ::: bot acc: 0.0127
top acc: 0.0747 ::: bot acc: 0.0210
top acc: 0.0937 ::: bot acc: 0.0176
top acc: 0.0805 ::: bot acc: 0.0178
top acc: 0.0893 ::: bot acc: 0.0144
top acc: 0.0722 ::: bot acc: 0.0144
current epoch: 38
train loss is 0.000941
average val loss: 0.001060, accuracy: 0.0365
average test loss: 0.001526, accuracy: 0.0465
case acc: 0.041446302
case acc: 0.04714261
case acc: 0.053575683
case acc: 0.04747995
case acc: 0.05010957
case acc: 0.03933325
top acc: 0.0711 ::: bot acc: 0.0124
top acc: 0.0735 ::: bot acc: 0.0200
top acc: 0.0939 ::: bot acc: 0.0177
top acc: 0.0802 ::: bot acc: 0.0176
top acc: 0.0892 ::: bot acc: 0.0149
top acc: 0.0720 ::: bot acc: 0.0145
current epoch: 39
train loss is 0.000933
average val loss: 0.001007, accuracy: 0.0353
average test loss: 0.001459, accuracy: 0.0452
case acc: 0.03989657
case acc: 0.044884197
case acc: 0.05227067
case acc: 0.04660355
case acc: 0.049389128
case acc: 0.038243633
top acc: 0.0692 ::: bot acc: 0.0119
top acc: 0.0706 ::: bot acc: 0.0187
top acc: 0.0922 ::: bot acc: 0.0171
top acc: 0.0790 ::: bot acc: 0.0173
top acc: 0.0887 ::: bot acc: 0.0144
top acc: 0.0705 ::: bot acc: 0.0141
current epoch: 40
train loss is 0.000885
average val loss: 0.000905, accuracy: 0.0329
average test loss: 0.001311, accuracy: 0.0424
case acc: 0.036974475
case acc: 0.041014057
case acc: 0.049277164
case acc: 0.044007234
case acc: 0.046893854
case acc: 0.036287233
top acc: 0.0650 ::: bot acc: 0.0115
top acc: 0.0659 ::: bot acc: 0.0163
top acc: 0.0883 ::: bot acc: 0.0160
top acc: 0.0760 ::: bot acc: 0.0159
top acc: 0.0853 ::: bot acc: 0.0138
top acc: 0.0673 ::: bot acc: 0.0146
current epoch: 41
train loss is 0.000838
average val loss: 0.000855, accuracy: 0.0318
average test loss: 0.001240, accuracy: 0.0410
case acc: 0.035299245
case acc: 0.038701996
case acc: 0.047740024
case acc: 0.042783137
case acc: 0.04589014
case acc: 0.035413496
top acc: 0.0626 ::: bot acc: 0.0112
top acc: 0.0634 ::: bot acc: 0.0148
top acc: 0.0862 ::: bot acc: 0.0155
top acc: 0.0743 ::: bot acc: 0.0151
top acc: 0.0839 ::: bot acc: 0.0133
top acc: 0.0657 ::: bot acc: 0.0151
current epoch: 42
train loss is 0.000819
average val loss: 0.000757, accuracy: 0.0297
average test loss: 0.001086, accuracy: 0.0379
case acc: 0.032302227
case acc: 0.03437876
case acc: 0.044627387
case acc: 0.039546695
case acc: 0.04325776
case acc: 0.033365063
top acc: 0.0575 ::: bot acc: 0.0123
top acc: 0.0577 ::: bot acc: 0.0136
top acc: 0.0814 ::: bot acc: 0.0159
top acc: 0.0702 ::: bot acc: 0.0138
top acc: 0.0801 ::: bot acc: 0.0132
top acc: 0.0618 ::: bot acc: 0.0167
current epoch: 43
train loss is 0.000779
average val loss: 0.000725, accuracy: 0.0288
average test loss: 0.001030, accuracy: 0.0367
case acc: 0.030788615
case acc: 0.032408156
case acc: 0.043273382
case acc: 0.038485344
case acc: 0.042498056
case acc: 0.03280173
top acc: 0.0550 ::: bot acc: 0.0130
top acc: 0.0548 ::: bot acc: 0.0134
top acc: 0.0791 ::: bot acc: 0.0165
top acc: 0.0689 ::: bot acc: 0.0135
top acc: 0.0788 ::: bot acc: 0.0134
top acc: 0.0607 ::: bot acc: 0.0175
current epoch: 44
train loss is 0.000757
average val loss: 0.000666, accuracy: 0.0275
average test loss: 0.000924, accuracy: 0.0345
case acc: 0.028830169
case acc: 0.028978378
case acc: 0.040839896
case acc: 0.03618661
case acc: 0.040447272
case acc: 0.03156522
top acc: 0.0507 ::: bot acc: 0.0157
top acc: 0.0497 ::: bot acc: 0.0134
top acc: 0.0745 ::: bot acc: 0.0188
top acc: 0.0658 ::: bot acc: 0.0133
top acc: 0.0757 ::: bot acc: 0.0137
top acc: 0.0581 ::: bot acc: 0.0194
current epoch: 45
train loss is 0.000733
average val loss: 0.000631, accuracy: 0.0267
average test loss: 0.000852, accuracy: 0.0329
case acc: 0.027461993
case acc: 0.026570147
case acc: 0.038915083
case acc: 0.034594037
case acc: 0.039067913
case acc: 0.030952781
top acc: 0.0472 ::: bot acc: 0.0186
top acc: 0.0454 ::: bot acc: 0.0147
top acc: 0.0705 ::: bot acc: 0.0209
top acc: 0.0632 ::: bot acc: 0.0135
top acc: 0.0735 ::: bot acc: 0.0142
top acc: 0.0564 ::: bot acc: 0.0209
current epoch: 46
train loss is 0.000721
average val loss: 0.000590, accuracy: 0.0261
average test loss: 0.000742, accuracy: 0.0304
case acc: 0.025404194
case acc: 0.023289822
case acc: 0.036439218
case acc: 0.031386185
case acc: 0.03650315
case acc: 0.029464336
top acc: 0.0410 ::: bot acc: 0.0245
top acc: 0.0384 ::: bot acc: 0.0189
top acc: 0.0643 ::: bot acc: 0.0263
top acc: 0.0581 ::: bot acc: 0.0144
top acc: 0.0686 ::: bot acc: 0.0160
top acc: 0.0517 ::: bot acc: 0.0252
current epoch: 47
train loss is 0.000710
average val loss: 0.000584, accuracy: 0.0260
average test loss: 0.000693, accuracy: 0.0293
case acc: 0.02493708
case acc: 0.02184776
case acc: 0.03494313
case acc: 0.029768351
case acc: 0.035395686
case acc: 0.029168947
top acc: 0.0374 ::: bot acc: 0.0285
top acc: 0.0337 ::: bot acc: 0.0238
top acc: 0.0595 ::: bot acc: 0.0309
top acc: 0.0548 ::: bot acc: 0.0161
top acc: 0.0662 ::: bot acc: 0.0177
top acc: 0.0496 ::: bot acc: 0.0274
current epoch: 48
train loss is 0.000718
average val loss: 0.000594, accuracy: 0.0265
average test loss: 0.000645, accuracy: 0.0283
case acc: 0.024193084
case acc: 0.020745806
case acc: 0.03387885
case acc: 0.028039115
case acc: 0.03425313
case acc: 0.0284828
top acc: 0.0323 ::: bot acc: 0.0335
top acc: 0.0274 ::: bot acc: 0.0297
top acc: 0.0532 ::: bot acc: 0.0371
top acc: 0.0507 ::: bot acc: 0.0194
top acc: 0.0630 ::: bot acc: 0.0205
top acc: 0.0465 ::: bot acc: 0.0304
current epoch: 49
train loss is 0.000736
average val loss: 0.000635, accuracy: 0.0279
average test loss: 0.000619, accuracy: 0.0276
case acc: 0.024234463
case acc: 0.02097643
case acc: 0.033088602
case acc: 0.02684225
case acc: 0.032628264
case acc: 0.028046556
top acc: 0.0266 ::: bot acc: 0.0392
top acc: 0.0206 ::: bot acc: 0.0368
top acc: 0.0462 ::: bot acc: 0.0444
top acc: 0.0453 ::: bot acc: 0.0249
top acc: 0.0589 ::: bot acc: 0.0240
top acc: 0.0429 ::: bot acc: 0.0340
current epoch: 50
train loss is 0.000771
average val loss: 0.000716, accuracy: 0.0302
average test loss: 0.000629, accuracy: 0.0278
case acc: 0.025223576
case acc: 0.023250729
case acc: 0.0328597
case acc: 0.02594931
case acc: 0.03172341
case acc: 0.027891483
top acc: 0.0202 ::: bot acc: 0.0455
top acc: 0.0133 ::: bot acc: 0.0444
top acc: 0.0380 ::: bot acc: 0.0524
top acc: 0.0398 ::: bot acc: 0.0302
top acc: 0.0550 ::: bot acc: 0.0281
top acc: 0.0391 ::: bot acc: 0.0378
LME_Co_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6774 6774 6774
1.7082474 -0.6288155 0.21141115 -0.19947179
Validation: 756 756 756
Testing: 768 768 768
pre-processing time: 0.00019502639770507812
the split date is 2011-07-01
train dropout: 0.6 test dropout: 0.15
net initializing with time: 0.0022339820861816406
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.013074
average val loss: 0.005151, accuracy: 0.0928
average test loss: 0.005091, accuracy: 0.0928
case acc: 0.13016155
case acc: 0.08937988
case acc: 0.090692386
case acc: 0.079051554
case acc: 0.11474654
case acc: 0.052679934
top acc: 0.0960 ::: bot acc: 0.1599
top acc: 0.1169 ::: bot acc: 0.0624
top acc: 0.0485 ::: bot acc: 0.1317
top acc: 0.0552 ::: bot acc: 0.1047
top acc: 0.0813 ::: bot acc: 0.1511
top acc: 0.0225 ::: bot acc: 0.0883
current epoch: 2
train loss is 0.007998
average val loss: 0.003347, accuracy: 0.0579
average test loss: 0.003279, accuracy: 0.0572
case acc: 0.038519666
case acc: 0.17282666
case acc: 0.032478273
case acc: 0.022116832
case acc: 0.033704363
case acc: 0.043338366
top acc: 0.0216 ::: bot acc: 0.0594
top acc: 0.1999 ::: bot acc: 0.1459
top acc: 0.0538 ::: bot acc: 0.0333
top acc: 0.0371 ::: bot acc: 0.0152
top acc: 0.0137 ::: bot acc: 0.0633
top acc: 0.0754 ::: bot acc: 0.0199
current epoch: 3
train loss is 0.008218
average val loss: 0.008298, accuracy: 0.1069
average test loss: 0.008274, accuracy: 0.1072
case acc: 0.05663322
case acc: 0.24412678
case acc: 0.095292255
case acc: 0.090310834
case acc: 0.051993884
case acc: 0.10498765
top acc: 0.0916 ::: bot acc: 0.0260
top acc: 0.2705 ::: bot acc: 0.2173
top acc: 0.1383 ::: bot acc: 0.0527
top acc: 0.1141 ::: bot acc: 0.0659
top acc: 0.0832 ::: bot acc: 0.0197
top acc: 0.1457 ::: bot acc: 0.0649
current epoch: 4
train loss is 0.010033
average val loss: 0.016736, accuracy: 0.1699
average test loss: 0.016727, accuracy: 0.1702
case acc: 0.12740225
case acc: 0.30086127
case acc: 0.16469686
case acc: 0.15245003
case acc: 0.11599297
case acc: 0.15973772
top acc: 0.1621 ::: bot acc: 0.0983
top acc: 0.3270 ::: bot acc: 0.2741
top acc: 0.2093 ::: bot acc: 0.1210
top acc: 0.1770 ::: bot acc: 0.1266
top acc: 0.1502 ::: bot acc: 0.0794
top acc: 0.2006 ::: bot acc: 0.1189
current epoch: 5
train loss is 0.012621
average val loss: 0.016048, accuracy: 0.1673
average test loss: 0.016032, accuracy: 0.1677
case acc: 0.13048741
case acc: 0.28876695
case acc: 0.16557637
case acc: 0.14917287
case acc: 0.120485544
case acc: 0.15156375
top acc: 0.1652 ::: bot acc: 0.1005
top acc: 0.3152 ::: bot acc: 0.2623
top acc: 0.2101 ::: bot acc: 0.1221
top acc: 0.1733 ::: bot acc: 0.1235
top acc: 0.1544 ::: bot acc: 0.0842
top acc: 0.1930 ::: bot acc: 0.1102
current epoch: 6
train loss is 0.013133
average val loss: 0.005674, accuracy: 0.0883
average test loss: 0.005634, accuracy: 0.0888
case acc: 0.055384606
case acc: 0.19638582
case acc: 0.085042916
case acc: 0.06880132
case acc: 0.0537258
case acc: 0.073565245
top acc: 0.0899 ::: bot acc: 0.0256
top acc: 0.2235 ::: bot acc: 0.1691
top acc: 0.1296 ::: bot acc: 0.0418
top acc: 0.0930 ::: bot acc: 0.0434
top acc: 0.0856 ::: bot acc: 0.0212
top acc: 0.1132 ::: bot acc: 0.0368
current epoch: 7
train loss is 0.008337
average val loss: 0.001704, accuracy: 0.0432
average test loss: 0.001623, accuracy: 0.0412
case acc: 0.027501918
case acc: 0.112968385
case acc: 0.033054225
case acc: 0.018235654
case acc: 0.025135402
case acc: 0.030094873
top acc: 0.0256 ::: bot acc: 0.0413
top acc: 0.1400 ::: bot acc: 0.0860
top acc: 0.0552 ::: bot acc: 0.0323
top acc: 0.0241 ::: bot acc: 0.0258
top acc: 0.0283 ::: bot acc: 0.0417
top acc: 0.0458 ::: bot acc: 0.0358
current epoch: 8
train loss is 0.003913
average val loss: 0.001406, accuracy: 0.0405
average test loss: 0.001313, accuracy: 0.0383
case acc: 0.029279564
case acc: 0.09503686
case acc: 0.03222238
case acc: 0.018402133
case acc: 0.025007963
case acc: 0.030049115
top acc: 0.0233 ::: bot acc: 0.0451
top acc: 0.1214 ::: bot acc: 0.0683
top acc: 0.0454 ::: bot acc: 0.0428
top acc: 0.0184 ::: bot acc: 0.0309
top acc: 0.0319 ::: bot acc: 0.0392
top acc: 0.0428 ::: bot acc: 0.0393
current epoch: 9
train loss is 0.002625
average val loss: 0.001642, accuracy: 0.0437
average test loss: 0.001559, accuracy: 0.0417
case acc: 0.022731211
case acc: 0.104357585
case acc: 0.034531258
case acc: 0.021546498
case acc: 0.032347765
case acc: 0.034954518
top acc: 0.0404 ::: bot acc: 0.0238
top acc: 0.1311 ::: bot acc: 0.0775
top acc: 0.0608 ::: bot acc: 0.0274
top acc: 0.0372 ::: bot acc: 0.0133
top acc: 0.0554 ::: bot acc: 0.0180
top acc: 0.0630 ::: bot acc: 0.0211
current epoch: 10
train loss is 0.002470
average val loss: 0.001622, accuracy: 0.0440
average test loss: 0.001559, accuracy: 0.0427
case acc: 0.023608372
case acc: 0.10043682
case acc: 0.034676872
case acc: 0.023665337
case acc: 0.035961784
case acc: 0.037552144
top acc: 0.0455 ::: bot acc: 0.0194
top acc: 0.1269 ::: bot acc: 0.0738
top acc: 0.0620 ::: bot acc: 0.0252
top acc: 0.0410 ::: bot acc: 0.0119
top acc: 0.0619 ::: bot acc: 0.0161
top acc: 0.0677 ::: bot acc: 0.0201
current epoch: 11
train loss is 0.002271
average val loss: 0.001446, accuracy: 0.0421
average test loss: 0.001381, accuracy: 0.0406
case acc: 0.023508282
case acc: 0.09071115
case acc: 0.0337211
case acc: 0.022818014
case acc: 0.036271356
case acc: 0.036448672
top acc: 0.0443 ::: bot acc: 0.0200
top acc: 0.1181 ::: bot acc: 0.0636
top acc: 0.0583 ::: bot acc: 0.0291
top acc: 0.0395 ::: bot acc: 0.0125
top acc: 0.0620 ::: bot acc: 0.0165
top acc: 0.0658 ::: bot acc: 0.0204
current epoch: 12
train loss is 0.002021
average val loss: 0.001366, accuracy: 0.0412
average test loss: 0.001293, accuracy: 0.0397
case acc: 0.023738025
case acc: 0.083710626
case acc: 0.03350412
case acc: 0.022970915
case acc: 0.03749679
case acc: 0.03693059
top acc: 0.0463 ::: bot acc: 0.0186
top acc: 0.1104 ::: bot acc: 0.0570
top acc: 0.0571 ::: bot acc: 0.0303
top acc: 0.0396 ::: bot acc: 0.0120
top acc: 0.0639 ::: bot acc: 0.0160
top acc: 0.0660 ::: bot acc: 0.0201
current epoch: 13
train loss is 0.001881
average val loss: 0.001275, accuracy: 0.0402
average test loss: 0.001203, accuracy: 0.0386
case acc: 0.023717992
case acc: 0.076917425
case acc: 0.03317032
case acc: 0.023017384
case acc: 0.03785569
case acc: 0.03682008
top acc: 0.0468 ::: bot acc: 0.0168
top acc: 0.1040 ::: bot acc: 0.0504
top acc: 0.0561 ::: bot acc: 0.0318
top acc: 0.0399 ::: bot acc: 0.0125
top acc: 0.0645 ::: bot acc: 0.0157
top acc: 0.0660 ::: bot acc: 0.0201
current epoch: 14
train loss is 0.001605
average val loss: 0.001250, accuracy: 0.0401
average test loss: 0.001186, accuracy: 0.0388
case acc: 0.024741456
case acc: 0.07238766
case acc: 0.033518177
case acc: 0.024058325
case acc: 0.039459094
case acc: 0.038489237
top acc: 0.0498 ::: bot acc: 0.0146
top acc: 0.0993 ::: bot acc: 0.0454
top acc: 0.0569 ::: bot acc: 0.0311
top acc: 0.0410 ::: bot acc: 0.0121
top acc: 0.0672 ::: bot acc: 0.0156
top acc: 0.0686 ::: bot acc: 0.0204
current epoch: 15
train loss is 0.001506
average val loss: 0.001226, accuracy: 0.0399
average test loss: 0.001164, accuracy: 0.0387
case acc: 0.025803164
case acc: 0.06860578
case acc: 0.033669785
case acc: 0.024961408
case acc: 0.040275164
case acc: 0.038756378
top acc: 0.0524 ::: bot acc: 0.0126
top acc: 0.0952 ::: bot acc: 0.0420
top acc: 0.0575 ::: bot acc: 0.0303
top acc: 0.0434 ::: bot acc: 0.0114
top acc: 0.0685 ::: bot acc: 0.0156
top acc: 0.0689 ::: bot acc: 0.0199
current epoch: 16
train loss is 0.001402
average val loss: 0.001164, accuracy: 0.0389
average test loss: 0.001104, accuracy: 0.0378
case acc: 0.026111923
case acc: 0.063666545
case acc: 0.033557966
case acc: 0.025291666
case acc: 0.039638937
case acc: 0.038330555
top acc: 0.0530 ::: bot acc: 0.0123
top acc: 0.0905 ::: bot acc: 0.0368
top acc: 0.0574 ::: bot acc: 0.0305
top acc: 0.0433 ::: bot acc: 0.0121
top acc: 0.0674 ::: bot acc: 0.0157
top acc: 0.0687 ::: bot acc: 0.0198
current epoch: 17
train loss is 0.001332
average val loss: 0.001124, accuracy: 0.0384
average test loss: 0.001067, accuracy: 0.0372
case acc: 0.026694149
case acc: 0.05970722
case acc: 0.03357846
case acc: 0.025522478
case acc: 0.03950499
case acc: 0.038112648
top acc: 0.0543 ::: bot acc: 0.0119
top acc: 0.0867 ::: bot acc: 0.0331
top acc: 0.0576 ::: bot acc: 0.0304
top acc: 0.0438 ::: bot acc: 0.0116
top acc: 0.0673 ::: bot acc: 0.0158
top acc: 0.0685 ::: bot acc: 0.0195
current epoch: 18
train loss is 0.001276
average val loss: 0.001047, accuracy: 0.0371
average test loss: 0.000986, accuracy: 0.0356
case acc: 0.026559932
case acc: 0.054142073
case acc: 0.033243544
case acc: 0.024839874
case acc: 0.037888885
case acc: 0.037045795
top acc: 0.0538 ::: bot acc: 0.0120
top acc: 0.0810 ::: bot acc: 0.0272
top acc: 0.0566 ::: bot acc: 0.0310
top acc: 0.0429 ::: bot acc: 0.0116
top acc: 0.0646 ::: bot acc: 0.0158
top acc: 0.0668 ::: bot acc: 0.0197
current epoch: 19
train loss is 0.001169
average val loss: 0.001095, accuracy: 0.0380
average test loss: 0.001041, accuracy: 0.0370
case acc: 0.028329687
case acc: 0.054154746
case acc: 0.03413366
case acc: 0.027446419
case acc: 0.039000377
case acc: 0.03898356
top acc: 0.0577 ::: bot acc: 0.0102
top acc: 0.0809 ::: bot acc: 0.0273
top acc: 0.0598 ::: bot acc: 0.0278
top acc: 0.0463 ::: bot acc: 0.0119
top acc: 0.0664 ::: bot acc: 0.0160
top acc: 0.0693 ::: bot acc: 0.0201
current epoch: 20
train loss is 0.001150
average val loss: 0.001131, accuracy: 0.0389
average test loss: 0.001082, accuracy: 0.0380
case acc: 0.030015815
case acc: 0.053542778
case acc: 0.035057127
case acc: 0.029438663
case acc: 0.039802838
case acc: 0.039956495
top acc: 0.0606 ::: bot acc: 0.0093
top acc: 0.0805 ::: bot acc: 0.0267
top acc: 0.0635 ::: bot acc: 0.0245
top acc: 0.0491 ::: bot acc: 0.0129
top acc: 0.0675 ::: bot acc: 0.0157
top acc: 0.0709 ::: bot acc: 0.0200
current epoch: 21
train loss is 0.001109
average val loss: 0.001112, accuracy: 0.0385
average test loss: 0.001062, accuracy: 0.0376
case acc: 0.03092525
case acc: 0.051212583
case acc: 0.03528434
case acc: 0.029731508
case acc: 0.039073154
case acc: 0.039480846
top acc: 0.0617 ::: bot acc: 0.0094
top acc: 0.0782 ::: bot acc: 0.0245
top acc: 0.0640 ::: bot acc: 0.0235
top acc: 0.0496 ::: bot acc: 0.0126
top acc: 0.0669 ::: bot acc: 0.0155
top acc: 0.0705 ::: bot acc: 0.0197
current epoch: 22
train loss is 0.001089
average val loss: 0.001110, accuracy: 0.0385
average test loss: 0.001054, accuracy: 0.0375
case acc: 0.031696104
case acc: 0.049345866
case acc: 0.035788745
case acc: 0.030366536
case acc: 0.03890457
case acc: 0.039026126
top acc: 0.0628 ::: bot acc: 0.0093
top acc: 0.0764 ::: bot acc: 0.0226
top acc: 0.0656 ::: bot acc: 0.0223
top acc: 0.0505 ::: bot acc: 0.0133
top acc: 0.0662 ::: bot acc: 0.0159
top acc: 0.0694 ::: bot acc: 0.0198
current epoch: 23
train loss is 0.001045
average val loss: 0.001176, accuracy: 0.0398
average test loss: 0.001122, accuracy: 0.0390
case acc: 0.03407481
case acc: 0.050363407
case acc: 0.03736925
case acc: 0.03235369
case acc: 0.04001508
case acc: 0.03986556
top acc: 0.0663 ::: bot acc: 0.0092
top acc: 0.0772 ::: bot acc: 0.0236
top acc: 0.0698 ::: bot acc: 0.0191
top acc: 0.0527 ::: bot acc: 0.0142
top acc: 0.0679 ::: bot acc: 0.0157
top acc: 0.0709 ::: bot acc: 0.0195
current epoch: 24
train loss is 0.001046
average val loss: 0.001223, accuracy: 0.0408
average test loss: 0.001172, accuracy: 0.0401
case acc: 0.03586425
case acc: 0.05072745
case acc: 0.038641814
case acc: 0.033945553
case acc: 0.04072208
case acc: 0.0405866
top acc: 0.0690 ::: bot acc: 0.0099
top acc: 0.0773 ::: bot acc: 0.0243
top acc: 0.0726 ::: bot acc: 0.0168
top acc: 0.0547 ::: bot acc: 0.0149
top acc: 0.0692 ::: bot acc: 0.0158
top acc: 0.0718 ::: bot acc: 0.0198
current epoch: 25
train loss is 0.001043
average val loss: 0.001292, accuracy: 0.0421
average test loss: 0.001242, accuracy: 0.0415
case acc: 0.037930526
case acc: 0.05143982
case acc: 0.04061082
case acc: 0.035986498
case acc: 0.041798368
case acc: 0.04124005
top acc: 0.0716 ::: bot acc: 0.0105
top acc: 0.0783 ::: bot acc: 0.0249
top acc: 0.0761 ::: bot acc: 0.0156
top acc: 0.0573 ::: bot acc: 0.0160
top acc: 0.0708 ::: bot acc: 0.0158
top acc: 0.0732 ::: bot acc: 0.0194
current epoch: 26
train loss is 0.001045
average val loss: 0.001332, accuracy: 0.0429
average test loss: 0.001278, accuracy: 0.0423
case acc: 0.039574638
case acc: 0.051418494
case acc: 0.041911863
case acc: 0.03727188
case acc: 0.042071592
case acc: 0.041716658
top acc: 0.0734 ::: bot acc: 0.0118
top acc: 0.0779 ::: bot acc: 0.0248
top acc: 0.0783 ::: bot acc: 0.0154
top acc: 0.0589 ::: bot acc: 0.0173
top acc: 0.0708 ::: bot acc: 0.0159
top acc: 0.0734 ::: bot acc: 0.0197
current epoch: 27
train loss is 0.001049
average val loss: 0.001408, accuracy: 0.0444
average test loss: 0.001358, accuracy: 0.0439
case acc: 0.041741766
case acc: 0.05214084
case acc: 0.04368876
case acc: 0.039488386
case acc: 0.04374787
case acc: 0.042573016
top acc: 0.0760 ::: bot acc: 0.0129
top acc: 0.0787 ::: bot acc: 0.0253
top acc: 0.0812 ::: bot acc: 0.0146
top acc: 0.0614 ::: bot acc: 0.0187
top acc: 0.0733 ::: bot acc: 0.0158
top acc: 0.0749 ::: bot acc: 0.0197
current epoch: 28
train loss is 0.001064
average val loss: 0.001517, accuracy: 0.0464
average test loss: 0.001473, accuracy: 0.0462
case acc: 0.044796687
case acc: 0.05374133
case acc: 0.046221316
case acc: 0.042091023
case acc: 0.045896195
case acc: 0.04444105
top acc: 0.0795 ::: bot acc: 0.0154
top acc: 0.0803 ::: bot acc: 0.0271
top acc: 0.0851 ::: bot acc: 0.0146
top acc: 0.0645 ::: bot acc: 0.0206
top acc: 0.0760 ::: bot acc: 0.0168
top acc: 0.0774 ::: bot acc: 0.0201
current epoch: 29
train loss is 0.001043
average val loss: 0.001472, accuracy: 0.0456
average test loss: 0.001423, accuracy: 0.0452
case acc: 0.044102684
case acc: 0.051758975
case acc: 0.045837257
case acc: 0.04138288
case acc: 0.045101494
case acc: 0.043210365
top acc: 0.0786 ::: bot acc: 0.0151
top acc: 0.0784 ::: bot acc: 0.0253
top acc: 0.0846 ::: bot acc: 0.0141
top acc: 0.0635 ::: bot acc: 0.0199
top acc: 0.0752 ::: bot acc: 0.0162
top acc: 0.0756 ::: bot acc: 0.0198
current epoch: 30
train loss is 0.001037
average val loss: 0.001466, accuracy: 0.0455
average test loss: 0.001424, accuracy: 0.0452
case acc: 0.04430446
case acc: 0.050579593
case acc: 0.0465342
case acc: 0.04160118
case acc: 0.045418974
case acc: 0.042740613
top acc: 0.0788 ::: bot acc: 0.0149
top acc: 0.0774 ::: bot acc: 0.0238
top acc: 0.0855 ::: bot acc: 0.0143
top acc: 0.0637 ::: bot acc: 0.0202
top acc: 0.0756 ::: bot acc: 0.0164
top acc: 0.0749 ::: bot acc: 0.0199
current epoch: 31
train loss is 0.001013
average val loss: 0.001434, accuracy: 0.0449
average test loss: 0.001396, accuracy: 0.0446
case acc: 0.04371828
case acc: 0.049247716
case acc: 0.046564117
case acc: 0.041291922
case acc: 0.04488185
case acc: 0.04217746
top acc: 0.0781 ::: bot acc: 0.0147
top acc: 0.0760 ::: bot acc: 0.0226
top acc: 0.0855 ::: bot acc: 0.0144
top acc: 0.0632 ::: bot acc: 0.0199
top acc: 0.0750 ::: bot acc: 0.0162
top acc: 0.0743 ::: bot acc: 0.0196
current epoch: 32
train loss is 0.000978
average val loss: 0.001466, accuracy: 0.0454
average test loss: 0.001418, accuracy: 0.0451
case acc: 0.044316344
case acc: 0.048788503
case acc: 0.047545753
case acc: 0.042136006
case acc: 0.045495663
case acc: 0.042124987
top acc: 0.0790 ::: bot acc: 0.0150
top acc: 0.0756 ::: bot acc: 0.0222
top acc: 0.0869 ::: bot acc: 0.0143
top acc: 0.0644 ::: bot acc: 0.0205
top acc: 0.0759 ::: bot acc: 0.0166
top acc: 0.0741 ::: bot acc: 0.0198
current epoch: 33
train loss is 0.000958
average val loss: 0.001521, accuracy: 0.0465
average test loss: 0.001470, accuracy: 0.0461
case acc: 0.045353144
case acc: 0.049488563
case acc: 0.048704326
case acc: 0.043621533
case acc: 0.04638155
case acc: 0.042850368
top acc: 0.0799 ::: bot acc: 0.0161
top acc: 0.0764 ::: bot acc: 0.0228
top acc: 0.0886 ::: bot acc: 0.0144
top acc: 0.0658 ::: bot acc: 0.0217
top acc: 0.0772 ::: bot acc: 0.0166
top acc: 0.0751 ::: bot acc: 0.0200
current epoch: 34
train loss is 0.000976
average val loss: 0.001509, accuracy: 0.0462
average test loss: 0.001464, accuracy: 0.0459
case acc: 0.04522898
case acc: 0.048515063
case acc: 0.04892616
case acc: 0.04374545
case acc: 0.04652202
case acc: 0.04272363
top acc: 0.0795 ::: bot acc: 0.0163
top acc: 0.0753 ::: bot acc: 0.0218
top acc: 0.0889 ::: bot acc: 0.0146
top acc: 0.0659 ::: bot acc: 0.0218
top acc: 0.0772 ::: bot acc: 0.0167
top acc: 0.0751 ::: bot acc: 0.0198
current epoch: 35
train loss is 0.000990
average val loss: 0.001596, accuracy: 0.0478
average test loss: 0.001546, accuracy: 0.0475
case acc: 0.046816245
case acc: 0.04922398
case acc: 0.050563075
case acc: 0.045581475
case acc: 0.04834361
case acc: 0.04418617
top acc: 0.0815 ::: bot acc: 0.0174
top acc: 0.0759 ::: bot acc: 0.0224
top acc: 0.0914 ::: bot acc: 0.0147
top acc: 0.0681 ::: bot acc: 0.0232
top acc: 0.0796 ::: bot acc: 0.0173
top acc: 0.0770 ::: bot acc: 0.0201
current epoch: 36
train loss is 0.000948
average val loss: 0.001475, accuracy: 0.0456
average test loss: 0.001423, accuracy: 0.0451
case acc: 0.04386334
case acc: 0.04539794
case acc: 0.048694413
case acc: 0.043663487
case acc: 0.0467399
case acc: 0.042224355
top acc: 0.0783 ::: bot acc: 0.0149
top acc: 0.0719 ::: bot acc: 0.0192
top acc: 0.0887 ::: bot acc: 0.0146
top acc: 0.0661 ::: bot acc: 0.0216
top acc: 0.0776 ::: bot acc: 0.0166
top acc: 0.0741 ::: bot acc: 0.0198
current epoch: 37
train loss is 0.000909
average val loss: 0.001386, accuracy: 0.0439
average test loss: 0.001332, accuracy: 0.0432
case acc: 0.04166131
case acc: 0.04235079
case acc: 0.047095038
case acc: 0.041813206
case acc: 0.045449883
case acc: 0.04097944
top acc: 0.0760 ::: bot acc: 0.0131
top acc: 0.0689 ::: bot acc: 0.0162
top acc: 0.0865 ::: bot acc: 0.0144
top acc: 0.0641 ::: bot acc: 0.0202
top acc: 0.0760 ::: bot acc: 0.0161
top acc: 0.0724 ::: bot acc: 0.0194
current epoch: 38
train loss is 0.000867
average val loss: 0.001295, accuracy: 0.0422
average test loss: 0.001240, accuracy: 0.0413
case acc: 0.03936155
case acc: 0.039247803
case acc: 0.04553251
case acc: 0.04001811
case acc: 0.044087637
case acc: 0.039473176
top acc: 0.0734 ::: bot acc: 0.0113
top acc: 0.0655 ::: bot acc: 0.0138
top acc: 0.0843 ::: bot acc: 0.0142
top acc: 0.0618 ::: bot acc: 0.0187
top acc: 0.0740 ::: bot acc: 0.0161
top acc: 0.0702 ::: bot acc: 0.0193
current epoch: 39
train loss is 0.000842
average val loss: 0.001222, accuracy: 0.0407
average test loss: 0.001163, accuracy: 0.0397
case acc: 0.03740191
case acc: 0.03657966
case acc: 0.044410247
case acc: 0.03861299
case acc: 0.04299561
case acc: 0.038352493
top acc: 0.0705 ::: bot acc: 0.0104
top acc: 0.0624 ::: bot acc: 0.0116
top acc: 0.0822 ::: bot acc: 0.0147
top acc: 0.0601 ::: bot acc: 0.0180
top acc: 0.0724 ::: bot acc: 0.0159
top acc: 0.0683 ::: bot acc: 0.0196
current epoch: 40
train loss is 0.000812
average val loss: 0.001138, accuracy: 0.0390
average test loss: 0.001088, accuracy: 0.0381
case acc: 0.03542515
case acc: 0.03414832
case acc: 0.042841226
case acc: 0.037085764
case acc: 0.041684233
case acc: 0.037253253
top acc: 0.0685 ::: bot acc: 0.0098
top acc: 0.0598 ::: bot acc: 0.0100
top acc: 0.0798 ::: bot acc: 0.0150
top acc: 0.0583 ::: bot acc: 0.0172
top acc: 0.0703 ::: bot acc: 0.0160
top acc: 0.0670 ::: bot acc: 0.0197
current epoch: 41
train loss is 0.000797
average val loss: 0.001138, accuracy: 0.0390
average test loss: 0.001084, accuracy: 0.0379
case acc: 0.035072643
case acc: 0.033205133
case acc: 0.042708203
case acc: 0.03724999
case acc: 0.041789237
case acc: 0.037434142
top acc: 0.0679 ::: bot acc: 0.0094
top acc: 0.0587 ::: bot acc: 0.0093
top acc: 0.0796 ::: bot acc: 0.0148
top acc: 0.0588 ::: bot acc: 0.0169
top acc: 0.0708 ::: bot acc: 0.0157
top acc: 0.0671 ::: bot acc: 0.0197
current epoch: 42
train loss is 0.000780
average val loss: 0.001053, accuracy: 0.0373
average test loss: 0.001002, accuracy: 0.0361
case acc: 0.033085935
case acc: 0.030570708
case acc: 0.041197352
case acc: 0.035316765
case acc: 0.040357236
case acc: 0.03626187
top acc: 0.0649 ::: bot acc: 0.0093
top acc: 0.0553 ::: bot acc: 0.0084
top acc: 0.0769 ::: bot acc: 0.0157
top acc: 0.0564 ::: bot acc: 0.0158
top acc: 0.0686 ::: bot acc: 0.0155
top acc: 0.0652 ::: bot acc: 0.0203
current epoch: 43
train loss is 0.000747
average val loss: 0.000977, accuracy: 0.0357
average test loss: 0.000924, accuracy: 0.0344
case acc: 0.031180087
case acc: 0.028288275
case acc: 0.03949262
case acc: 0.033430494
case acc: 0.038800817
case acc: 0.03496053
top acc: 0.0620 ::: bot acc: 0.0097
top acc: 0.0518 ::: bot acc: 0.0083
top acc: 0.0740 ::: bot acc: 0.0168
top acc: 0.0541 ::: bot acc: 0.0145
top acc: 0.0664 ::: bot acc: 0.0157
top acc: 0.0628 ::: bot acc: 0.0212
current epoch: 44
train loss is 0.000726
average val loss: 0.000881, accuracy: 0.0337
average test loss: 0.000828, accuracy: 0.0321
case acc: 0.028515408
case acc: 0.025538111
case acc: 0.037516654
case acc: 0.030732345
case acc: 0.036713958
case acc: 0.03349971
top acc: 0.0578 ::: bot acc: 0.0102
top acc: 0.0472 ::: bot acc: 0.0095
top acc: 0.0701 ::: bot acc: 0.0188
top acc: 0.0509 ::: bot acc: 0.0132
top acc: 0.0632 ::: bot acc: 0.0161
top acc: 0.0597 ::: bot acc: 0.0228
current epoch: 45
train loss is 0.000709
average val loss: 0.000871, accuracy: 0.0334
average test loss: 0.000811, accuracy: 0.0317
case acc: 0.02800235
case acc: 0.02463967
case acc: 0.036967598
case acc: 0.030433986
case acc: 0.036589563
case acc: 0.033390224
top acc: 0.0569 ::: bot acc: 0.0105
top acc: 0.0459 ::: bot acc: 0.0096
top acc: 0.0689 ::: bot acc: 0.0195
top acc: 0.0503 ::: bot acc: 0.0132
top acc: 0.0628 ::: bot acc: 0.0158
top acc: 0.0597 ::: bot acc: 0.0224
current epoch: 46
train loss is 0.000697
average val loss: 0.000782, accuracy: 0.0315
average test loss: 0.000723, accuracy: 0.0295
case acc: 0.02567015
case acc: 0.022395495
case acc: 0.035328545
case acc: 0.02743725
case acc: 0.034368925
case acc: 0.03209631
top acc: 0.0524 ::: bot acc: 0.0128
top acc: 0.0404 ::: bot acc: 0.0135
top acc: 0.0639 ::: bot acc: 0.0245
top acc: 0.0465 ::: bot acc: 0.0119
top acc: 0.0592 ::: bot acc: 0.0170
top acc: 0.0564 ::: bot acc: 0.0253
current epoch: 47
train loss is 0.000684
average val loss: 0.000719, accuracy: 0.0300
average test loss: 0.000656, accuracy: 0.0278
case acc: 0.023854386
case acc: 0.02077787
case acc: 0.033978753
case acc: 0.024942596
case acc: 0.03227225
case acc: 0.03124086
top acc: 0.0475 ::: bot acc: 0.0166
top acc: 0.0361 ::: bot acc: 0.0176
top acc: 0.0594 ::: bot acc: 0.0284
top acc: 0.0429 ::: bot acc: 0.0115
top acc: 0.0554 ::: bot acc: 0.0180
top acc: 0.0535 ::: bot acc: 0.0281
current epoch: 48
train loss is 0.000679
average val loss: 0.000665, accuracy: 0.0288
average test loss: 0.000603, accuracy: 0.0266
case acc: 0.022983659
case acc: 0.019899588
case acc: 0.03288267
case acc: 0.02268118
case acc: 0.030396309
case acc: 0.030549865
top acc: 0.0431 ::: bot acc: 0.0209
top acc: 0.0308 ::: bot acc: 0.0231
top acc: 0.0541 ::: bot acc: 0.0338
top acc: 0.0396 ::: bot acc: 0.0123
top acc: 0.0516 ::: bot acc: 0.0201
top acc: 0.0502 ::: bot acc: 0.0314
current epoch: 49
train loss is 0.000688
average val loss: 0.000657, accuracy: 0.0287
average test loss: 0.000592, accuracy: 0.0263
case acc: 0.022777464
case acc: 0.019397857
case acc: 0.032550044
case acc: 0.022002358
case acc: 0.030338384
case acc: 0.030459516
top acc: 0.0415 ::: bot acc: 0.0226
top acc: 0.0286 ::: bot acc: 0.0250
top acc: 0.0518 ::: bot acc: 0.0363
top acc: 0.0381 ::: bot acc: 0.0125
top acc: 0.0515 ::: bot acc: 0.0202
top acc: 0.0502 ::: bot acc: 0.0313
current epoch: 50
train loss is 0.000692
average val loss: 0.000633, accuracy: 0.0281
average test loss: 0.000570, accuracy: 0.0257
case acc: 0.022657767
case acc: 0.019515501
case acc: 0.032169774
case acc: 0.020458255
case acc: 0.029373541
case acc: 0.030246647
top acc: 0.0376 ::: bot acc: 0.0266
top acc: 0.0238 ::: bot acc: 0.0295
top acc: 0.0469 ::: bot acc: 0.0412
top acc: 0.0351 ::: bot acc: 0.0143
top acc: 0.0493 ::: bot acc: 0.0219
top acc: 0.0483 ::: bot acc: 0.0332
