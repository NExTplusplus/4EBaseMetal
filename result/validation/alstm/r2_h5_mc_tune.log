
		{"drop_out": 0.2, "drop_out_mc": 0.05, "repeat_mc": 50, "hidden": 20, "embedding_size": 5, "batch": 512, "lag": 4}
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
LME_Co_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6768 6768 6768
1.8562728 -0.6288155 0.2585643 -0.19947179
Validation: 756 756 756
Testing: 774 774 774
pre-processing time: 0.0005269050598144531
the split date is 2009-07-01
net initializing with time: 0.006316661834716797
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.404522
average val loss: 0.191110, accuracy: 0.1908
average test loss: 0.184970, accuracy: 0.1851
case acc: 0.12305199
case acc: 0.2678261
case acc: 0.15785517
case acc: 0.024398653
case acc: 0.33928204
case acc: 0.19818376
top acc: 0.1400 ::: bot acc: 0.1063
top acc: 0.2938 ::: bot acc: 0.2435
top acc: 0.1950 ::: bot acc: 0.1239
top acc: 0.0439 ::: bot acc: 0.0133
top acc: 0.3643 ::: bot acc: 0.3112
top acc: 0.2189 ::: bot acc: 0.1751
current epoch: 2
train loss is 0.141032
average val loss: 0.091625, accuracy: 0.0932
average test loss: 0.089410, accuracy: 0.0902
case acc: 0.02239996
case acc: 0.12707722
case acc: 0.028668854
case acc: 0.12732421
case acc: 0.18442746
case acc: 0.051358055
top acc: 0.0091 ::: bot acc: 0.0373
top acc: 0.1536 ::: bot acc: 0.1025
top acc: 0.0539 ::: bot acc: 0.0181
top acc: 0.1005 ::: bot acc: 0.1535
top acc: 0.2092 ::: bot acc: 0.1566
top acc: 0.0720 ::: bot acc: 0.0290
current epoch: 3
train loss is 0.116686
average val loss: 0.087717, accuracy: 0.0878
average test loss: 0.089695, accuracy: 0.0913
case acc: 0.109791234
case acc: 0.030799154
case acc: 0.07054165
case acc: 0.20638041
case acc: 0.096151836
case acc: 0.034208823
top acc: 0.0929 ::: bot acc: 0.1264
top acc: 0.0554 ::: bot acc: 0.0101
top acc: 0.0334 ::: bot acc: 0.1051
top acc: 0.1797 ::: bot acc: 0.2326
top acc: 0.1208 ::: bot acc: 0.0687
top acc: 0.0143 ::: bot acc: 0.0567
current epoch: 4
train loss is 0.117574
average val loss: 0.088757, accuracy: 0.0899
average test loss: 0.085897, accuracy: 0.0860
case acc: 0.016228085
case acc: 0.11957032
case acc: 0.033754144
case acc: 0.10477626
case acc: 0.17893651
case acc: 0.062437907
top acc: 0.0078 ::: bot acc: 0.0284
top acc: 0.1466 ::: bot acc: 0.0942
top acc: 0.0648 ::: bot acc: 0.0115
top acc: 0.0783 ::: bot acc: 0.1309
top acc: 0.2034 ::: bot acc: 0.1517
top acc: 0.0832 ::: bot acc: 0.0398
current epoch: 5
train loss is 0.093461
average val loss: 0.070788, accuracy: 0.0725
average test loss: 0.071022, accuracy: 0.0731
case acc: 0.0695524
case acc: 0.05557446
case acc: 0.03590413
case acc: 0.15831287
case acc: 0.102893695
case acc: 0.016271325
top acc: 0.0524 ::: bot acc: 0.0861
top acc: 0.0826 ::: bot acc: 0.0302
top acc: 0.0152 ::: bot acc: 0.0623
top acc: 0.1319 ::: bot acc: 0.1844
top acc: 0.1272 ::: bot acc: 0.0757
top acc: 0.0224 ::: bot acc: 0.0211
current epoch: 6
train loss is 0.090090
average val loss: 0.065596, accuracy: 0.0676
average test loss: 0.065755, accuracy: 0.0678
case acc: 0.06765465
case acc: 0.048903707
case acc: 0.033946376
case acc: 0.15271907
case acc: 0.08757238
case acc: 0.016073838
top acc: 0.0506 ::: bot acc: 0.0841
top acc: 0.0758 ::: bot acc: 0.0238
top acc: 0.0163 ::: bot acc: 0.0590
top acc: 0.1265 ::: bot acc: 0.1787
top acc: 0.1118 ::: bot acc: 0.0605
top acc: 0.0220 ::: bot acc: 0.0213
current epoch: 7
train loss is 0.092213
average val loss: 0.064629, accuracy: 0.0667
average test loss: 0.062775, accuracy: 0.0646
case acc: 0.037752893
case acc: 0.07362243
case acc: 0.026822858
case acc: 0.12113846
case acc: 0.09796282
case acc: 0.030143013
top acc: 0.0208 ::: bot acc: 0.0541
top acc: 0.1009 ::: bot acc: 0.0479
top acc: 0.0443 ::: bot acc: 0.0283
top acc: 0.0950 ::: bot acc: 0.1470
top acc: 0.1222 ::: bot acc: 0.0711
top acc: 0.0475 ::: bot acc: 0.0144
current epoch: 8
train loss is 0.081919
average val loss: 0.057528, accuracy: 0.0600
average test loss: 0.056450, accuracy: 0.0585
case acc: 0.049593028
case acc: 0.057509948
case acc: 0.02718168
case acc: 0.13109072
case acc: 0.066824876
case acc: 0.019097734
top acc: 0.0326 ::: bot acc: 0.0660
top acc: 0.0848 ::: bot acc: 0.0318
top acc: 0.0341 ::: bot acc: 0.0385
top acc: 0.1051 ::: bot acc: 0.1570
top acc: 0.0910 ::: bot acc: 0.0399
top acc: 0.0311 ::: bot acc: 0.0140
current epoch: 9
train loss is 0.075938
average val loss: 0.052774, accuracy: 0.0553
average test loss: 0.052053, accuracy: 0.0540
case acc: 0.05238883
case acc: 0.04919679
case acc: 0.027413454
case acc: 0.13134357
case acc: 0.047185544
case acc: 0.016550116
top acc: 0.0354 ::: bot acc: 0.0688
top acc: 0.0763 ::: bot acc: 0.0239
top acc: 0.0331 ::: bot acc: 0.0397
top acc: 0.1054 ::: bot acc: 0.1572
top acc: 0.0707 ::: bot acc: 0.0216
top acc: 0.0248 ::: bot acc: 0.0186
current epoch: 10
train loss is 0.073939
average val loss: 0.049944, accuracy: 0.0523
average test loss: 0.048655, accuracy: 0.0502
case acc: 0.04405364
case acc: 0.052409586
case acc: 0.026742013
case acc: 0.12066822
case acc: 0.039289217
case acc: 0.018334625
top acc: 0.0270 ::: bot acc: 0.0605
top acc: 0.0797 ::: bot acc: 0.0269
top acc: 0.0428 ::: bot acc: 0.0300
top acc: 0.0948 ::: bot acc: 0.1465
top acc: 0.0620 ::: bot acc: 0.0154
top acc: 0.0295 ::: bot acc: 0.0147
current epoch: 11
train loss is 0.068584
average val loss: 0.047239, accuracy: 0.0493
average test loss: 0.045790, accuracy: 0.0470
case acc: 0.03924865
case acc: 0.05237035
case acc: 0.027517451
case acc: 0.11310156
case acc: 0.030803075
case acc: 0.019091327
top acc: 0.0222 ::: bot acc: 0.0557
top acc: 0.0797 ::: bot acc: 0.0267
top acc: 0.0488 ::: bot acc: 0.0241
top acc: 0.0874 ::: bot acc: 0.1388
top acc: 0.0512 ::: bot acc: 0.0114
top acc: 0.0311 ::: bot acc: 0.0139
current epoch: 12
train loss is 0.064884
average val loss: 0.045469, accuracy: 0.0471
average test loss: 0.043767, accuracy: 0.0446
case acc: 0.033014335
case acc: 0.05326857
case acc: 0.029975675
case acc: 0.102791026
case acc: 0.027043143
case acc: 0.021468524
top acc: 0.0164 ::: bot acc: 0.0492
top acc: 0.0807 ::: bot acc: 0.0275
top acc: 0.0565 ::: bot acc: 0.0169
top acc: 0.0772 ::: bot acc: 0.1285
top acc: 0.0455 ::: bot acc: 0.0117
top acc: 0.0352 ::: bot acc: 0.0127
current epoch: 13
train loss is 0.062108
average val loss: 0.044360, accuracy: 0.0457
average test loss: 0.042337, accuracy: 0.0427
case acc: 0.02713748
case acc: 0.053241532
case acc: 0.033476446
case acc: 0.09089854
case acc: 0.026307482
case acc: 0.025029378
top acc: 0.0114 ::: bot acc: 0.0429
top acc: 0.0807 ::: bot acc: 0.0274
top acc: 0.0643 ::: bot acc: 0.0118
top acc: 0.0653 ::: bot acc: 0.1166
top acc: 0.0442 ::: bot acc: 0.0121
top acc: 0.0405 ::: bot acc: 0.0128
current epoch: 14
train loss is 0.059945
average val loss: 0.043533, accuracy: 0.0446
average test loss: 0.041184, accuracy: 0.0412
case acc: 0.022987902
case acc: 0.05170142
case acc: 0.03787876
case acc: 0.07886369
case acc: 0.02720585
case acc: 0.028720878
top acc: 0.0089 ::: bot acc: 0.0379
top acc: 0.0791 ::: bot acc: 0.0260
top acc: 0.0711 ::: bot acc: 0.0115
top acc: 0.0532 ::: bot acc: 0.1046
top acc: 0.0458 ::: bot acc: 0.0117
top acc: 0.0455 ::: bot acc: 0.0141
current epoch: 15
train loss is 0.057257
average val loss: 0.041267, accuracy: 0.0422
average test loss: 0.038939, accuracy: 0.0389
case acc: 0.023610724
case acc: 0.044342197
case acc: 0.038672697
case acc: 0.07153604
case acc: 0.026836181
case acc: 0.028672755
top acc: 0.0092 ::: bot acc: 0.0388
top acc: 0.0714 ::: bot acc: 0.0192
top acc: 0.0722 ::: bot acc: 0.0117
top acc: 0.0460 ::: bot acc: 0.0972
top acc: 0.0451 ::: bot acc: 0.0119
top acc: 0.0453 ::: bot acc: 0.0141
current epoch: 16
train loss is 0.053774
average val loss: 0.037680, accuracy: 0.0388
average test loss: 0.035804, accuracy: 0.0359
case acc: 0.02923622
case acc: 0.03269396
case acc: 0.035280738
case acc: 0.06959099
case acc: 0.024299227
case acc: 0.024489084
top acc: 0.0129 ::: bot acc: 0.0453
top acc: 0.0584 ::: bot acc: 0.0102
top acc: 0.0674 ::: bot acc: 0.0112
top acc: 0.0442 ::: bot acc: 0.0952
top acc: 0.0404 ::: bot acc: 0.0138
top acc: 0.0396 ::: bot acc: 0.0130
current epoch: 17
train loss is 0.050687
average val loss: 0.034562, accuracy: 0.0361
average test loss: 0.033533, accuracy: 0.0338
case acc: 0.03702138
case acc: 0.023669483
case acc: 0.03139037
case acc: 0.070384406
case acc: 0.020760218
case acc: 0.01935767
top acc: 0.0200 ::: bot acc: 0.0535
top acc: 0.0450 ::: bot acc: 0.0099
top acc: 0.0602 ::: bot acc: 0.0140
top acc: 0.0450 ::: bot acc: 0.0960
top acc: 0.0326 ::: bot acc: 0.0188
top acc: 0.0315 ::: bot acc: 0.0140
current epoch: 18
train loss is 0.048795
average val loss: 0.032836, accuracy: 0.0345
average test loss: 0.032703, accuracy: 0.0330
case acc: 0.042257905
case acc: 0.020772183
case acc: 0.029422602
case acc: 0.06979388
case acc: 0.019014813
case acc: 0.01673234
top acc: 0.0253 ::: bot acc: 0.0587
top acc: 0.0362 ::: bot acc: 0.0176
top acc: 0.0550 ::: bot acc: 0.0186
top acc: 0.0444 ::: bot acc: 0.0954
top acc: 0.0253 ::: bot acc: 0.0260
top acc: 0.0254 ::: bot acc: 0.0183
current epoch: 19
train loss is 0.047225
average val loss: 0.031678, accuracy: 0.0333
average test loss: 0.031823, accuracy: 0.0320
case acc: 0.04199354
case acc: 0.020480026
case acc: 0.029289989
case acc: 0.06520879
case acc: 0.018696345
case acc: 0.0162894
top acc: 0.0251 ::: bot acc: 0.0584
top acc: 0.0342 ::: bot acc: 0.0197
top acc: 0.0545 ::: bot acc: 0.0193
top acc: 0.0400 ::: bot acc: 0.0907
top acc: 0.0218 ::: bot acc: 0.0296
top acc: 0.0239 ::: bot acc: 0.0198
current epoch: 20
train loss is 0.046307
average val loss: 0.030656, accuracy: 0.0322
average test loss: 0.031001, accuracy: 0.0310
case acc: 0.04131439
case acc: 0.020321392
case acc: 0.029156601
case acc: 0.060535166
case acc: 0.018862724
case acc: 0.01606549
top acc: 0.0245 ::: bot acc: 0.0577
top acc: 0.0330 ::: bot acc: 0.0209
top acc: 0.0541 ::: bot acc: 0.0197
top acc: 0.0354 ::: bot acc: 0.0860
top acc: 0.0187 ::: bot acc: 0.0328
top acc: 0.0227 ::: bot acc: 0.0210
current epoch: 21
train loss is 0.045267
average val loss: 0.029599, accuracy: 0.0309
average test loss: 0.029599, accuracy: 0.0295
case acc: 0.037243497
case acc: 0.02095383
case acc: 0.030229505
case acc: 0.05309074
case acc: 0.018892337
case acc: 0.01644227
top acc: 0.0204 ::: bot acc: 0.0537
top acc: 0.0367 ::: bot acc: 0.0173
top acc: 0.0570 ::: bot acc: 0.0173
top acc: 0.0281 ::: bot acc: 0.0785
top acc: 0.0188 ::: bot acc: 0.0328
top acc: 0.0244 ::: bot acc: 0.0194
current epoch: 22
train loss is 0.044410
average val loss: 0.028776, accuracy: 0.0298
average test loss: 0.028140, accuracy: 0.0277
case acc: 0.03153616
case acc: 0.022552717
case acc: 0.032205373
case acc: 0.043689337
case acc: 0.018736893
case acc: 0.017584722
top acc: 0.0151 ::: bot acc: 0.0478
top acc: 0.0424 ::: bot acc: 0.0120
top acc: 0.0618 ::: bot acc: 0.0136
top acc: 0.0192 ::: bot acc: 0.0689
top acc: 0.0217 ::: bot acc: 0.0298
top acc: 0.0281 ::: bot acc: 0.0159
current epoch: 23
train loss is 0.042828
average val loss: 0.028045, accuracy: 0.0288
average test loss: 0.027183, accuracy: 0.0266
case acc: 0.02924412
case acc: 0.023404438
case acc: 0.0330747
case acc: 0.037530772
case acc: 0.018757295
case acc: 0.017797278
top acc: 0.0132 ::: bot acc: 0.0453
top acc: 0.0444 ::: bot acc: 0.0107
top acc: 0.0636 ::: bot acc: 0.0126
top acc: 0.0141 ::: bot acc: 0.0623
top acc: 0.0226 ::: bot acc: 0.0289
top acc: 0.0287 ::: bot acc: 0.0154
current epoch: 24
train loss is 0.042338
average val loss: 0.027877, accuracy: 0.0282
average test loss: 0.026367, accuracy: 0.0258
case acc: 0.024951318
case acc: 0.025300499
case acc: 0.035759304
case acc: 0.029755566
case acc: 0.019323446
case acc: 0.019559637
top acc: 0.0100 ::: bot acc: 0.0405
top acc: 0.0481 ::: bot acc: 0.0089
top acc: 0.0683 ::: bot acc: 0.0115
top acc: 0.0102 ::: bot acc: 0.0525
top acc: 0.0276 ::: bot acc: 0.0239
top acc: 0.0323 ::: bot acc: 0.0136
current epoch: 25
train loss is 0.041436
average val loss: 0.027580, accuracy: 0.0276
average test loss: 0.025656, accuracy: 0.0251
case acc: 0.023880117
case acc: 0.02490521
case acc: 0.036741428
case acc: 0.024769032
case acc: 0.020438004
case acc: 0.020083182
top acc: 0.0093 ::: bot acc: 0.0392
top acc: 0.0473 ::: bot acc: 0.0092
top acc: 0.0697 ::: bot acc: 0.0115
top acc: 0.0103 ::: bot acc: 0.0450
top acc: 0.0319 ::: bot acc: 0.0196
top acc: 0.0332 ::: bot acc: 0.0134
current epoch: 26
train loss is 0.039742
average val loss: 0.026261, accuracy: 0.0265
average test loss: 0.024627, accuracy: 0.0243
case acc: 0.027286507
case acc: 0.021961948
case acc: 0.03444682
case acc: 0.023332682
case acc: 0.020620298
case acc: 0.018053273
top acc: 0.0116 ::: bot acc: 0.0432
top acc: 0.0409 ::: bot acc: 0.0131
top acc: 0.0661 ::: bot acc: 0.0119
top acc: 0.0114 ::: bot acc: 0.0423
top acc: 0.0326 ::: bot acc: 0.0189
top acc: 0.0292 ::: bot acc: 0.0152
current epoch: 27
train loss is 0.038782
average val loss: 0.025872, accuracy: 0.0261
average test loss: 0.024154, accuracy: 0.0240
case acc: 0.02887737
case acc: 0.020692792
case acc: 0.03356879
case acc: 0.02090739
case acc: 0.022499412
case acc: 0.01765484
top acc: 0.0129 ::: bot acc: 0.0450
top acc: 0.0352 ::: bot acc: 0.0187
top acc: 0.0646 ::: bot acc: 0.0124
top acc: 0.0149 ::: bot acc: 0.0368
top acc: 0.0376 ::: bot acc: 0.0146
top acc: 0.0283 ::: bot acc: 0.0160
current epoch: 28
train loss is 0.038409
average val loss: 0.025956, accuracy: 0.0263
average test loss: 0.024483, accuracy: 0.0246
case acc: 0.032599207
case acc: 0.020497924
case acc: 0.03187408
case acc: 0.019473681
case acc: 0.026013274
case acc: 0.017163176
top acc: 0.0162 ::: bot acc: 0.0489
top acc: 0.0246 ::: bot acc: 0.0293
top acc: 0.0610 ::: bot acc: 0.0145
top acc: 0.0193 ::: bot acc: 0.0322
top acc: 0.0448 ::: bot acc: 0.0109
top acc: 0.0270 ::: bot acc: 0.0173
current epoch: 29
train loss is 0.039658
average val loss: 0.027789, accuracy: 0.0281
average test loss: 0.026700, accuracy: 0.0269
case acc: 0.037804145
case acc: 0.025105877
case acc: 0.029899608
case acc: 0.018899348
case acc: 0.033035588
case acc: 0.016901545
top acc: 0.0212 ::: bot acc: 0.0542
top acc: 0.0119 ::: bot acc: 0.0440
top acc: 0.0560 ::: bot acc: 0.0186
top acc: 0.0235 ::: bot acc: 0.0279
top acc: 0.0550 ::: bot acc: 0.0114
top acc: 0.0262 ::: bot acc: 0.0180
current epoch: 30
train loss is 0.045995
average val loss: 0.033121, accuracy: 0.0345
average test loss: 0.034387, accuracy: 0.0354
case acc: 0.059900727
case acc: 0.054108635
case acc: 0.027565893
case acc: 0.022065707
case acc: 0.031410903
case acc: 0.017520815
top acc: 0.0433 ::: bot acc: 0.0763
top acc: 0.0267 ::: bot acc: 0.0801
top acc: 0.0343 ::: bot acc: 0.0400
top acc: 0.0128 ::: bot acc: 0.0396
top acc: 0.0529 ::: bot acc: 0.0109
top acc: 0.0115 ::: bot acc: 0.0326
current epoch: 31
train loss is 0.065439
average val loss: 0.106487, accuracy: 0.1064
average test loss: 0.113154, accuracy: 0.1131
case acc: 0.15555999
case acc: 0.16657981
case acc: 0.09996108
case acc: 0.10477046
case acc: 0.054162912
case acc: 0.097698584
top acc: 0.1386 ::: bot acc: 0.1720
top acc: 0.1389 ::: bot acc: 0.1929
top acc: 0.0619 ::: bot acc: 0.1357
top acc: 0.0793 ::: bot acc: 0.1304
top acc: 0.0298 ::: bot acc: 0.0811
top acc: 0.0767 ::: bot acc: 0.1203
current epoch: 32
train loss is 0.103988
average val loss: 0.079637, accuracy: 0.0797
average test loss: 0.086027, accuracy: 0.0859
case acc: 0.12183552
case acc: 0.1251853
case acc: 0.07350358
case acc: 0.08680738
case acc: 0.03934469
case acc: 0.06901339
top acc: 0.1048 ::: bot acc: 0.1383
top acc: 0.0974 ::: bot acc: 0.1515
top acc: 0.0356 ::: bot acc: 0.1092
top acc: 0.0614 ::: bot acc: 0.1125
top acc: 0.0158 ::: bot acc: 0.0658
top acc: 0.0482 ::: bot acc: 0.0915
current epoch: 33
train loss is 0.069707
average val loss: 0.049980, accuracy: 0.0510
average test loss: 0.055749, accuracy: 0.0560
case acc: 0.079902604
case acc: 0.0640138
case acc: 0.04411081
case acc: 0.0693829
case acc: 0.037894182
case acc: 0.0407274
top acc: 0.0630 ::: bot acc: 0.0962
top acc: 0.0362 ::: bot acc: 0.0904
top acc: 0.0149 ::: bot acc: 0.0755
top acc: 0.0442 ::: bot acc: 0.0949
top acc: 0.0145 ::: bot acc: 0.0642
top acc: 0.0203 ::: bot acc: 0.0630
current epoch: 34
train loss is 0.045720
average val loss: 0.036004, accuracy: 0.0380
average test loss: 0.040670, accuracy: 0.0413
case acc: 0.053755496
case acc: 0.026996676
case acc: 0.03256468
case acc: 0.06266957
case acc: 0.044316683
case acc: 0.02734666
top acc: 0.0369 ::: bot acc: 0.0701
top acc: 0.0104 ::: bot acc: 0.0478
top acc: 0.0186 ::: bot acc: 0.0563
top acc: 0.0377 ::: bot acc: 0.0881
top acc: 0.0202 ::: bot acc: 0.0710
top acc: 0.0095 ::: bot acc: 0.0483
current epoch: 35
train loss is 0.040225
average val loss: 0.025754, accuracy: 0.0260
average test loss: 0.025119, accuracy: 0.0243
case acc: 0.01883799
case acc: 0.025902392
case acc: 0.028622713
case acc: 0.032187574
case acc: 0.022582669
case acc: 0.01786394
top acc: 0.0081 ::: bot acc: 0.0321
top acc: 0.0493 ::: bot acc: 0.0084
top acc: 0.0523 ::: bot acc: 0.0217
top acc: 0.0112 ::: bot acc: 0.0556
top acc: 0.0092 ::: bot acc: 0.0439
top acc: 0.0291 ::: bot acc: 0.0148
current epoch: 36
train loss is 0.040834
average val loss: 0.032993, accuracy: 0.0322
average test loss: 0.028999, accuracy: 0.0278
case acc: 0.012769889
case acc: 0.045257546
case acc: 0.039423347
case acc: 0.019120298
case acc: 0.019399306
case acc: 0.03091458
top acc: 0.0224 ::: bot acc: 0.0107
top acc: 0.0728 ::: bot acc: 0.0195
top acc: 0.0734 ::: bot acc: 0.0119
top acc: 0.0193 ::: bot acc: 0.0316
top acc: 0.0282 ::: bot acc: 0.0230
top acc: 0.0487 ::: bot acc: 0.0149
current epoch: 37
train loss is 0.043838
average val loss: 0.044528, accuracy: 0.0442
average test loss: 0.038642, accuracy: 0.0379
case acc: 0.02117567
case acc: 0.061111886
case acc: 0.05301968
case acc: 0.022586469
case acc: 0.026979089
case acc: 0.042432327
top acc: 0.0370 ::: bot acc: 0.0068
top acc: 0.0890 ::: bot acc: 0.0347
top acc: 0.0898 ::: bot acc: 0.0200
top acc: 0.0402 ::: bot acc: 0.0124
top acc: 0.0460 ::: bot acc: 0.0108
top acc: 0.0623 ::: bot acc: 0.0222
current epoch: 38
train loss is 0.045869
average val loss: 0.050187, accuracy: 0.0501
average test loss: 0.043815, accuracy: 0.0431
case acc: 0.023502382
case acc: 0.062509306
case acc: 0.05859392
case acc: 0.03132554
case acc: 0.036731128
case acc: 0.04593044
top acc: 0.0397 ::: bot acc: 0.0084
top acc: 0.0903 ::: bot acc: 0.0361
top acc: 0.0959 ::: bot acc: 0.0244
top acc: 0.0534 ::: bot acc: 0.0121
top acc: 0.0594 ::: bot acc: 0.0134
top acc: 0.0662 ::: bot acc: 0.0249
current epoch: 39
train loss is 0.042969
average val loss: 0.041183, accuracy: 0.0408
average test loss: 0.035415, accuracy: 0.0344
case acc: 0.013084502
case acc: 0.041710377
case acc: 0.04828122
case acc: 0.029755088
case acc: 0.038437333
case acc: 0.035042673
top acc: 0.0235 ::: bot acc: 0.0097
top acc: 0.0691 ::: bot acc: 0.0163
top acc: 0.0843 ::: bot acc: 0.0167
top acc: 0.0513 ::: bot acc: 0.0116
top acc: 0.0614 ::: bot acc: 0.0145
top acc: 0.0536 ::: bot acc: 0.0173
current epoch: 40
train loss is 0.036006
average val loss: 0.028009, accuracy: 0.0281
average test loss: 0.024771, accuracy: 0.0246
case acc: 0.021606594
case acc: 0.02087634
case acc: 0.03229614
case acc: 0.021809416
case acc: 0.03213591
case acc: 0.019024417
top acc: 0.0085 ::: bot acc: 0.0360
top acc: 0.0358 ::: bot acc: 0.0184
top acc: 0.0620 ::: bot acc: 0.0134
top acc: 0.0386 ::: bot acc: 0.0132
top acc: 0.0535 ::: bot acc: 0.0114
top acc: 0.0316 ::: bot acc: 0.0133
current epoch: 41
train loss is 0.036922
average val loss: 0.026051, accuracy: 0.0269
average test loss: 0.026061, accuracy: 0.0268
case acc: 0.042596012
case acc: 0.028461602
case acc: 0.026785968
case acc: 0.018577704
case acc: 0.027930144
case acc: 0.016538886
top acc: 0.0258 ::: bot acc: 0.0589
top acc: 0.0101 ::: bot acc: 0.0501
top acc: 0.0411 ::: bot acc: 0.0329
top acc: 0.0261 ::: bot acc: 0.0248
top acc: 0.0476 ::: bot acc: 0.0107
top acc: 0.0137 ::: bot acc: 0.0298
current epoch: 42
train loss is 0.050544
average val loss: 0.058802, accuracy: 0.0591
average test loss: 0.064624, accuracy: 0.0648
case acc: 0.1011738
case acc: 0.096017115
case acc: 0.05660554
case acc: 0.051766396
case acc: 0.024223013
case acc: 0.058965024
top acc: 0.0843 ::: bot acc: 0.1174
top acc: 0.0682 ::: bot acc: 0.1224
top acc: 0.0215 ::: bot acc: 0.0909
top acc: 0.0273 ::: bot acc: 0.0770
top acc: 0.0083 ::: bot acc: 0.0468
top acc: 0.0380 ::: bot acc: 0.0815
current epoch: 43
train loss is 0.076725
average val loss: 0.086446, accuracy: 0.0865
average test loss: 0.093020, accuracy: 0.0930
case acc: 0.12607634
case acc: 0.12353562
case acc: 0.084067196
case acc: 0.08550075
case acc: 0.0546525
case acc: 0.08410769
top acc: 0.1092 ::: bot acc: 0.1423
top acc: 0.0957 ::: bot acc: 0.1499
top acc: 0.0459 ::: bot acc: 0.1199
top acc: 0.0603 ::: bot acc: 0.1111
top acc: 0.0303 ::: bot acc: 0.0814
top acc: 0.0632 ::: bot acc: 0.1066
current epoch: 44
train loss is 0.067603
average val loss: 0.043987, accuracy: 0.0450
average test loss: 0.049554, accuracy: 0.0499
case acc: 0.071313255
case acc: 0.05372477
case acc: 0.04222983
case acc: 0.05421257
case acc: 0.03753747
case acc: 0.04008308
top acc: 0.0545 ::: bot acc: 0.0875
top acc: 0.0262 ::: bot acc: 0.0800
top acc: 0.0145 ::: bot acc: 0.0729
top acc: 0.0297 ::: bot acc: 0.0795
top acc: 0.0143 ::: bot acc: 0.0638
top acc: 0.0196 ::: bot acc: 0.0623
current epoch: 45
train loss is 0.042598
average val loss: 0.027232, accuracy: 0.0291
average test loss: 0.030410, accuracy: 0.0310
case acc: 0.038532004
case acc: 0.021144424
case acc: 0.02910872
case acc: 0.04085472
case acc: 0.035417043
case acc: 0.0209292
top acc: 0.0217 ::: bot acc: 0.0547
top acc: 0.0217 ::: bot acc: 0.0326
top acc: 0.0273 ::: bot acc: 0.0468
top acc: 0.0172 ::: bot acc: 0.0657
top acc: 0.0128 ::: bot acc: 0.0613
top acc: 0.0079 ::: bot acc: 0.0394
current epoch: 46
train loss is 0.037356
average val loss: 0.027739, accuracy: 0.0269
average test loss: 0.024684, accuracy: 0.0235
case acc: 0.012136376
case acc: 0.03518759
case acc: 0.032387987
case acc: 0.01910077
case acc: 0.018656122
case acc: 0.02351307
top acc: 0.0155 ::: bot acc: 0.0175
top acc: 0.0618 ::: bot acc: 0.0113
top acc: 0.0621 ::: bot acc: 0.0134
top acc: 0.0186 ::: bot acc: 0.0321
top acc: 0.0219 ::: bot acc: 0.0292
top acc: 0.0389 ::: bot acc: 0.0123
current epoch: 47
train loss is 0.038631
average val loss: 0.036564, accuracy: 0.0360
average test loss: 0.031203, accuracy: 0.0304
case acc: 0.015009442
case acc: 0.04560796
case acc: 0.04121612
case acc: 0.02136927
case acc: 0.025197618
case acc: 0.033789553
top acc: 0.0280 ::: bot acc: 0.0064
top acc: 0.0732 ::: bot acc: 0.0197
top acc: 0.0758 ::: bot acc: 0.0125
top acc: 0.0374 ::: bot acc: 0.0139
top acc: 0.0427 ::: bot acc: 0.0121
top acc: 0.0522 ::: bot acc: 0.0164
current epoch: 48
train loss is 0.038608
average val loss: 0.034759, accuracy: 0.0343
average test loss: 0.029495, accuracy: 0.0286
case acc: 0.012590465
case acc: 0.037378687
case acc: 0.038785715
case acc: 0.023357973
case acc: 0.02882081
case acc: 0.030449653
top acc: 0.0220 ::: bot acc: 0.0109
top acc: 0.0643 ::: bot acc: 0.0128
top acc: 0.0727 ::: bot acc: 0.0115
top acc: 0.0414 ::: bot acc: 0.0118
top acc: 0.0489 ::: bot acc: 0.0107
top acc: 0.0481 ::: bot acc: 0.0146
current epoch: 49
train loss is 0.034780
average val loss: 0.026575, accuracy: 0.0264
average test loss: 0.023034, accuracy: 0.0228
case acc: 0.016093966
case acc: 0.022997282
case acc: 0.030940989
case acc: 0.02031481
case acc: 0.026226228
case acc: 0.020056248
top acc: 0.0083 ::: bot acc: 0.0278
top acc: 0.0436 ::: bot acc: 0.0111
top acc: 0.0587 ::: bot acc: 0.0160
top acc: 0.0347 ::: bot acc: 0.0160
top acc: 0.0447 ::: bot acc: 0.0114
top acc: 0.0336 ::: bot acc: 0.0126
current epoch: 50
train loss is 0.033431
average val loss: 0.022794, accuracy: 0.0234
average test loss: 0.021820, accuracy: 0.0225
case acc: 0.028552618
case acc: 0.021088323
case acc: 0.026992919
case acc: 0.018593496
case acc: 0.02391701
case acc: 0.015797917
top acc: 0.0127 ::: bot acc: 0.0443
top acc: 0.0218 ::: bot acc: 0.0324
top acc: 0.0440 ::: bot acc: 0.0301
top acc: 0.0265 ::: bot acc: 0.0243
top acc: 0.0403 ::: bot acc: 0.0133
top acc: 0.0199 ::: bot acc: 0.0237
LME_Co_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6798 6798 6798
1.8562728 -0.6288155 0.21141115 -0.19947179
Validation: 756 756 756
Testing: 744 744 744
pre-processing time: 0.0005125999450683594
the split date is 2010-01-01
net initializing with time: 0.0033385753631591797
preparing training and testing date with time: 4.76837158203125e-07
current epoch: 1
train loss is 0.155670
average val loss: 0.038509, accuracy: 0.0389
average test loss: 0.042187, accuracy: 0.0431
case acc: 0.047841385
case acc: 0.020571487
case acc: 0.035113297
case acc: 0.042977776
case acc: 0.024058377
case acc: 0.0882221
top acc: 0.0228 ::: bot acc: 0.0759
top acc: 0.0164 ::: bot acc: 0.0332
top acc: 0.0547 ::: bot acc: 0.0361
top acc: 0.0210 ::: bot acc: 0.0698
top acc: 0.0406 ::: bot acc: 0.0244
top acc: 0.1325 ::: bot acc: 0.0489
current epoch: 2
train loss is 0.055237
average val loss: 0.114448, accuracy: 0.1143
average test loss: 0.116578, accuracy: 0.1173
case acc: 0.16043906
case acc: 0.12606487
case acc: 0.10881529
case acc: 0.15056504
case acc: 0.11105053
case acc: 0.046903487
top acc: 0.1292 ::: bot acc: 0.1918
top acc: 0.1017 ::: bot acc: 0.1498
top acc: 0.0643 ::: bot acc: 0.1546
top acc: 0.1175 ::: bot acc: 0.1831
top acc: 0.0783 ::: bot acc: 0.1419
top acc: 0.0208 ::: bot acc: 0.0766
current epoch: 3
train loss is 0.088819
average val loss: 0.120602, accuracy: 0.1206
average test loss: 0.122412, accuracy: 0.1228
case acc: 0.16152833
case acc: 0.12885363
case acc: 0.116154686
case acc: 0.15623713
case acc: 0.11599475
case acc: 0.058098942
top acc: 0.1295 ::: bot acc: 0.1934
top acc: 0.1044 ::: bot acc: 0.1533
top acc: 0.0717 ::: bot acc: 0.1615
top acc: 0.1239 ::: bot acc: 0.1884
top acc: 0.0836 ::: bot acc: 0.1463
top acc: 0.0246 ::: bot acc: 0.0910
current epoch: 4
train loss is 0.106103
average val loss: 0.050977, accuracy: 0.0506
average test loss: 0.054234, accuracy: 0.0553
case acc: 0.08087032
case acc: 0.05106218
case acc: 0.04687287
case acc: 0.082234554
case acc: 0.042101834
case acc: 0.028622067
top acc: 0.0488 ::: bot acc: 0.1128
top acc: 0.0281 ::: bot acc: 0.0748
top acc: 0.0140 ::: bot acc: 0.0865
top acc: 0.0520 ::: bot acc: 0.1134
top acc: 0.0158 ::: bot acc: 0.0695
top acc: 0.0542 ::: bot acc: 0.0233
current epoch: 5
train loss is 0.065942
average val loss: 0.078775, accuracy: 0.0785
average test loss: 0.081115, accuracy: 0.0819
case acc: 0.1088622
case acc: 0.08002411
case acc: 0.07589281
case acc: 0.11517796
case acc: 0.07087668
case acc: 0.04035325
top acc: 0.0767 ::: bot acc: 0.1408
top acc: 0.0551 ::: bot acc: 0.1048
top acc: 0.0324 ::: bot acc: 0.1208
top acc: 0.0837 ::: bot acc: 0.1470
top acc: 0.0387 ::: bot acc: 0.1011
top acc: 0.0213 ::: bot acc: 0.0658
current epoch: 6
train loss is 0.071591
average val loss: 0.078586, accuracy: 0.0784
average test loss: 0.080722, accuracy: 0.0813
case acc: 0.10375404
case acc: 0.076621436
case acc: 0.07680324
case acc: 0.11585728
case acc: 0.069052175
case acc: 0.045688987
top acc: 0.0715 ::: bot acc: 0.1356
top acc: 0.0519 ::: bot acc: 0.1014
top acc: 0.0334 ::: bot acc: 0.1216
top acc: 0.0846 ::: bot acc: 0.1476
top acc: 0.0370 ::: bot acc: 0.0990
top acc: 0.0205 ::: bot acc: 0.0742
current epoch: 7
train loss is 0.071236
average val loss: 0.063814, accuracy: 0.0636
average test loss: 0.066468, accuracy: 0.0670
case acc: 0.083799064
case acc: 0.05879524
case acc: 0.06345008
case acc: 0.10212723
case acc: 0.052933544
case acc: 0.041040175
top acc: 0.0515 ::: bot acc: 0.1155
top acc: 0.0351 ::: bot acc: 0.0833
top acc: 0.0216 ::: bot acc: 0.1075
top acc: 0.0714 ::: bot acc: 0.1335
top acc: 0.0225 ::: bot acc: 0.0820
top acc: 0.0208 ::: bot acc: 0.0673
current epoch: 8
train loss is 0.064245
average val loss: 0.057766, accuracy: 0.0576
average test loss: 0.060720, accuracy: 0.0612
case acc: 0.07315132
case acc: 0.04997706
case acc: 0.059050128
case acc: 0.0967963
case acc: 0.046199426
case acc: 0.042078573
top acc: 0.0411 ::: bot acc: 0.1047
top acc: 0.0272 ::: bot acc: 0.0742
top acc: 0.0188 ::: bot acc: 0.1022
top acc: 0.0664 ::: bot acc: 0.1280
top acc: 0.0182 ::: bot acc: 0.0739
top acc: 0.0206 ::: bot acc: 0.0691
current epoch: 9
train loss is 0.055834
average val loss: 0.056826, accuracy: 0.0568
average test loss: 0.059748, accuracy: 0.0601
case acc: 0.068191
case acc: 0.04647296
case acc: 0.059134
case acc: 0.09636832
case acc: 0.044159755
case acc: 0.046029896
top acc: 0.0363 ::: bot acc: 0.0995
top acc: 0.0242 ::: bot acc: 0.0706
top acc: 0.0189 ::: bot acc: 0.1022
top acc: 0.0662 ::: bot acc: 0.1274
top acc: 0.0170 ::: bot acc: 0.0713
top acc: 0.0203 ::: bot acc: 0.0752
current epoch: 10
train loss is 0.048832
average val loss: 0.056019, accuracy: 0.0561
average test loss: 0.058941, accuracy: 0.0591
case acc: 0.064074114
case acc: 0.04366613
case acc: 0.059204753
case acc: 0.095965154
case acc: 0.042520724
case acc: 0.049207974
top acc: 0.0326 ::: bot acc: 0.0952
top acc: 0.0217 ::: bot acc: 0.0678
top acc: 0.0189 ::: bot acc: 0.1023
top acc: 0.0659 ::: bot acc: 0.1269
top acc: 0.0163 ::: bot acc: 0.0691
top acc: 0.0206 ::: bot acc: 0.0797
current epoch: 11
train loss is 0.042986
average val loss: 0.049540, accuracy: 0.0497
average test loss: 0.053007, accuracy: 0.0532
case acc: 0.055618867
case acc: 0.03671849
case acc: 0.054258417
case acc: 0.089090474
case acc: 0.036994096
case acc: 0.04663527
top acc: 0.0260 ::: bot acc: 0.0859
top acc: 0.0167 ::: bot acc: 0.0600
top acc: 0.0162 ::: bot acc: 0.0962
top acc: 0.0594 ::: bot acc: 0.1199
top acc: 0.0146 ::: bot acc: 0.0617
top acc: 0.0203 ::: bot acc: 0.0759
current epoch: 12
train loss is 0.039224
average val loss: 0.041298, accuracy: 0.0416
average test loss: 0.045565, accuracy: 0.0459
case acc: 0.046913616
case acc: 0.02997924
case acc: 0.04799577
case acc: 0.077695355
case acc: 0.03136175
case acc: 0.041293453
top acc: 0.0203 ::: bot acc: 0.0757
top acc: 0.0138 ::: bot acc: 0.0516
top acc: 0.0141 ::: bot acc: 0.0879
top acc: 0.0487 ::: bot acc: 0.1081
top acc: 0.0147 ::: bot acc: 0.0531
top acc: 0.0204 ::: bot acc: 0.0679
current epoch: 13
train loss is 0.037262
average val loss: 0.036330, accuracy: 0.0368
average test loss: 0.041093, accuracy: 0.0415
case acc: 0.042148527
case acc: 0.026569994
case acc: 0.044901364
case acc: 0.06818606
case acc: 0.028716017
case acc: 0.03820957
top acc: 0.0184 ::: bot acc: 0.0696
top acc: 0.0132 ::: bot acc: 0.0469
top acc: 0.0146 ::: bot acc: 0.0830
top acc: 0.0399 ::: bot acc: 0.0982
top acc: 0.0159 ::: bot acc: 0.0484
top acc: 0.0210 ::: bot acc: 0.0629
current epoch: 14
train loss is 0.036056
average val loss: 0.034210, accuracy: 0.0346
average test loss: 0.039193, accuracy: 0.0395
case acc: 0.040492076
case acc: 0.026029356
case acc: 0.04389779
case acc: 0.061450593
case acc: 0.028316805
case acc: 0.03701876
top acc: 0.0179 ::: bot acc: 0.0674
top acc: 0.0131 ::: bot acc: 0.0461
top acc: 0.0150 ::: bot acc: 0.0813
top acc: 0.0336 ::: bot acc: 0.0912
top acc: 0.0161 ::: bot acc: 0.0477
top acc: 0.0215 ::: bot acc: 0.0608
current epoch: 15
train loss is 0.034950
average val loss: 0.030673, accuracy: 0.0311
average test loss: 0.036034, accuracy: 0.0364
case acc: 0.03744348
case acc: 0.023981843
case acc: 0.04179764
case acc: 0.053282842
case acc: 0.026853822
case acc: 0.03480639
top acc: 0.0175 ::: bot acc: 0.0631
top acc: 0.0129 ::: bot acc: 0.0431
top acc: 0.0164 ::: bot acc: 0.0775
top acc: 0.0263 ::: bot acc: 0.0826
top acc: 0.0175 ::: bot acc: 0.0449
top acc: 0.0230 ::: bot acc: 0.0566
current epoch: 16
train loss is 0.034243
average val loss: 0.028167, accuracy: 0.0286
average test loss: 0.033809, accuracy: 0.0341
case acc: 0.03551737
case acc: 0.022885226
case acc: 0.040357225
case acc: 0.04662477
case acc: 0.026046453
case acc: 0.033242516
top acc: 0.0177 ::: bot acc: 0.0602
top acc: 0.0132 ::: bot acc: 0.0412
top acc: 0.0177 ::: bot acc: 0.0748
top acc: 0.0210 ::: bot acc: 0.0753
top acc: 0.0187 ::: bot acc: 0.0432
top acc: 0.0248 ::: bot acc: 0.0534
current epoch: 17
train loss is 0.033213
average val loss: 0.026147, accuracy: 0.0266
average test loss: 0.031976, accuracy: 0.0322
case acc: 0.03375173
case acc: 0.02179502
case acc: 0.039185476
case acc: 0.041339133
case acc: 0.025281662
case acc: 0.032048617
top acc: 0.0181 ::: bot acc: 0.0573
top acc: 0.0137 ::: bot acc: 0.0393
top acc: 0.0190 ::: bot acc: 0.0724
top acc: 0.0177 ::: bot acc: 0.0690
top acc: 0.0199 ::: bot acc: 0.0414
top acc: 0.0262 ::: bot acc: 0.0509
current epoch: 18
train loss is 0.032998
average val loss: 0.024436, accuracy: 0.0249
average test loss: 0.030398, accuracy: 0.0306
case acc: 0.03223067
case acc: 0.020952303
case acc: 0.038093925
case acc: 0.03676029
case acc: 0.024660287
case acc: 0.030843515
top acc: 0.0186 ::: bot acc: 0.0547
top acc: 0.0145 ::: bot acc: 0.0376
top acc: 0.0206 ::: bot acc: 0.0700
top acc: 0.0155 ::: bot acc: 0.0633
top acc: 0.0212 ::: bot acc: 0.0398
top acc: 0.0280 ::: bot acc: 0.0483
current epoch: 19
train loss is 0.032490
average val loss: 0.024175, accuracy: 0.0245
average test loss: 0.030169, accuracy: 0.0303
case acc: 0.03212563
case acc: 0.02129951
case acc: 0.038125105
case acc: 0.034395434
case acc: 0.024927413
case acc: 0.03086726
top acc: 0.0186 ::: bot acc: 0.0545
top acc: 0.0142 ::: bot acc: 0.0381
top acc: 0.0205 ::: bot acc: 0.0701
top acc: 0.0147 ::: bot acc: 0.0602
top acc: 0.0206 ::: bot acc: 0.0405
top acc: 0.0280 ::: bot acc: 0.0482
current epoch: 20
train loss is 0.032152
average val loss: 0.021757, accuracy: 0.0221
average test loss: 0.027772, accuracy: 0.0278
case acc: 0.029282933
case acc: 0.01936853
case acc: 0.03628946
case acc: 0.029319987
case acc: 0.023395998
case acc: 0.029168848
top acc: 0.0200 ::: bot acc: 0.0496
top acc: 0.0170 ::: bot acc: 0.0339
top acc: 0.0239 ::: bot acc: 0.0657
top acc: 0.0137 ::: bot acc: 0.0531
top acc: 0.0245 ::: bot acc: 0.0363
top acc: 0.0315 ::: bot acc: 0.0438
current epoch: 21
train loss is 0.031695
average val loss: 0.021590, accuracy: 0.0219
average test loss: 0.027588, accuracy: 0.0276
case acc: 0.029051071
case acc: 0.019371998
case acc: 0.036238696
case acc: 0.028174518
case acc: 0.023435293
case acc: 0.029196648
top acc: 0.0202 ::: bot acc: 0.0491
top acc: 0.0169 ::: bot acc: 0.0338
top acc: 0.0240 ::: bot acc: 0.0656
top acc: 0.0141 ::: bot acc: 0.0513
top acc: 0.0244 ::: bot acc: 0.0364
top acc: 0.0316 ::: bot acc: 0.0439
current epoch: 22
train loss is 0.031608
average val loss: 0.020396, accuracy: 0.0207
average test loss: 0.026303, accuracy: 0.0263
case acc: 0.027221357
case acc: 0.018464899
case acc: 0.03530624
case acc: 0.02595484
case acc: 0.02249499
case acc: 0.028237797
top acc: 0.0216 ::: bot acc: 0.0456
top acc: 0.0198 ::: bot acc: 0.0307
top acc: 0.0270 ::: bot acc: 0.0625
top acc: 0.0163 ::: bot acc: 0.0469
top acc: 0.0274 ::: bot acc: 0.0334
top acc: 0.0344 ::: bot acc: 0.0410
current epoch: 23
train loss is 0.031276
average val loss: 0.020841, accuracy: 0.0211
average test loss: 0.026773, accuracy: 0.0267
case acc: 0.02785427
case acc: 0.018755626
case acc: 0.035706215
case acc: 0.026179258
case acc: 0.02298761
case acc: 0.028766686
top acc: 0.0210 ::: bot acc: 0.0468
top acc: 0.0182 ::: bot acc: 0.0321
top acc: 0.0256 ::: bot acc: 0.0640
top acc: 0.0160 ::: bot acc: 0.0474
top acc: 0.0258 ::: bot acc: 0.0350
top acc: 0.0329 ::: bot acc: 0.0427
current epoch: 24
train loss is 0.031266
average val loss: 0.021447, accuracy: 0.0217
average test loss: 0.027396, accuracy: 0.0273
case acc: 0.0287197
case acc: 0.019443378
case acc: 0.036205176
case acc: 0.02656383
case acc: 0.023635553
case acc: 0.029291583
top acc: 0.0204 ::: bot acc: 0.0484
top acc: 0.0165 ::: bot acc: 0.0340
top acc: 0.0241 ::: bot acc: 0.0655
top acc: 0.0156 ::: bot acc: 0.0482
top acc: 0.0239 ::: bot acc: 0.0369
top acc: 0.0314 ::: bot acc: 0.0442
current epoch: 25
train loss is 0.031272
average val loss: 0.019572, accuracy: 0.0199
average test loss: 0.025346, accuracy: 0.0252
case acc: 0.025378194
case acc: 0.01813284
case acc: 0.034563538
case acc: 0.02380275
case acc: 0.02190908
case acc: 0.02763962
top acc: 0.0237 ::: bot acc: 0.0418
top acc: 0.0226 ::: bot acc: 0.0277
top acc: 0.0300 ::: bot acc: 0.0596
top acc: 0.0209 ::: bot acc: 0.0414
top acc: 0.0301 ::: bot acc: 0.0307
top acc: 0.0365 ::: bot acc: 0.0388
current epoch: 26
train loss is 0.030920
average val loss: 0.019420, accuracy: 0.0197
average test loss: 0.025157, accuracy: 0.0250
case acc: 0.024867851
case acc: 0.01807721
case acc: 0.034398835
case acc: 0.023577359
case acc: 0.0217843
case acc: 0.027572513
top acc: 0.0246 ::: bot acc: 0.0406
top acc: 0.0238 ::: bot acc: 0.0266
top acc: 0.0306 ::: bot acc: 0.0589
top acc: 0.0216 ::: bot acc: 0.0407
top acc: 0.0311 ::: bot acc: 0.0297
top acc: 0.0367 ::: bot acc: 0.0386
current epoch: 27
train loss is 0.030893
average val loss: 0.019762, accuracy: 0.0200
average test loss: 0.025572, accuracy: 0.0255
case acc: 0.02563505
case acc: 0.018150555
case acc: 0.03470295
case acc: 0.024346655
case acc: 0.02204345
case acc: 0.0279218
top acc: 0.0232 ::: bot acc: 0.0424
top acc: 0.0219 ::: bot acc: 0.0284
top acc: 0.0291 ::: bot acc: 0.0604
top acc: 0.0193 ::: bot acc: 0.0430
top acc: 0.0294 ::: bot acc: 0.0315
top acc: 0.0353 ::: bot acc: 0.0400
current epoch: 28
train loss is 0.030806
average val loss: 0.020474, accuracy: 0.0207
average test loss: 0.026352, accuracy: 0.0263
case acc: 0.027025996
case acc: 0.018569248
case acc: 0.035255197
case acc: 0.025416782
case acc: 0.022805361
case acc: 0.028429475
top acc: 0.0217 ::: bot acc: 0.0453
top acc: 0.0190 ::: bot acc: 0.0312
top acc: 0.0268 ::: bot acc: 0.0626
top acc: 0.0169 ::: bot acc: 0.0458
top acc: 0.0265 ::: bot acc: 0.0344
top acc: 0.0335 ::: bot acc: 0.0417
current epoch: 29
train loss is 0.030757
average val loss: 0.021481, accuracy: 0.0216
average test loss: 0.027406, accuracy: 0.0273
case acc: 0.028624874
case acc: 0.019648215
case acc: 0.0360209
case acc: 0.026403723
case acc: 0.023892883
case acc: 0.029186664
top acc: 0.0204 ::: bot acc: 0.0483
top acc: 0.0160 ::: bot acc: 0.0345
top acc: 0.0244 ::: bot acc: 0.0651
top acc: 0.0156 ::: bot acc: 0.0479
top acc: 0.0233 ::: bot acc: 0.0377
top acc: 0.0314 ::: bot acc: 0.0439
current epoch: 30
train loss is 0.030796
average val loss: 0.019894, accuracy: 0.0201
average test loss: 0.025694, accuracy: 0.0256
case acc: 0.025895638
case acc: 0.018305603
case acc: 0.034728326
case acc: 0.02406541
case acc: 0.022477737
case acc: 0.02793299
top acc: 0.0227 ::: bot acc: 0.0431
top acc: 0.0205 ::: bot acc: 0.0297
top acc: 0.0290 ::: bot acc: 0.0605
top acc: 0.0203 ::: bot acc: 0.0421
top acc: 0.0280 ::: bot acc: 0.0331
top acc: 0.0355 ::: bot acc: 0.0399
current epoch: 31
train loss is 0.030529
average val loss: 0.019317, accuracy: 0.0195
average test loss: 0.025001, accuracy: 0.0248
case acc: 0.02441783
case acc: 0.017999848
case acc: 0.03417381
case acc: 0.023069022
case acc: 0.021883119
case acc: 0.027463548
top acc: 0.0253 ::: bot acc: 0.0396
top acc: 0.0238 ::: bot acc: 0.0264
top acc: 0.0316 ::: bot acc: 0.0579
top acc: 0.0236 ::: bot acc: 0.0388
top acc: 0.0312 ::: bot acc: 0.0300
top acc: 0.0374 ::: bot acc: 0.0380
current epoch: 32
train loss is 0.030630
average val loss: 0.018998, accuracy: 0.0192
average test loss: 0.024481, accuracy: 0.0244
case acc: 0.023190044
case acc: 0.018301511
case acc: 0.033657264
case acc: 0.022498166
case acc: 0.021967502
case acc: 0.026883136
top acc: 0.0297 ::: bot acc: 0.0349
top acc: 0.0284 ::: bot acc: 0.0217
top acc: 0.0355 ::: bot acc: 0.0541
top acc: 0.0267 ::: bot acc: 0.0358
top acc: 0.0358 ::: bot acc: 0.0254
top acc: 0.0405 ::: bot acc: 0.0349
current epoch: 33
train loss is 0.030543
average val loss: 0.018972, accuracy: 0.0192
average test loss: 0.024428, accuracy: 0.0244
case acc: 0.023106596
case acc: 0.018516343
case acc: 0.033562854
case acc: 0.022657521
case acc: 0.022047011
case acc: 0.026790954
top acc: 0.0307 ::: bot acc: 0.0340
top acc: 0.0296 ::: bot acc: 0.0204
top acc: 0.0362 ::: bot acc: 0.0533
top acc: 0.0254 ::: bot acc: 0.0371
top acc: 0.0370 ::: bot acc: 0.0241
top acc: 0.0410 ::: bot acc: 0.0344
current epoch: 34
train loss is 0.030533
average val loss: 0.019121, accuracy: 0.0194
average test loss: 0.024770, accuracy: 0.0248
case acc: 0.023842577
case acc: 0.01793407
case acc: 0.033867095
case acc: 0.024192154
case acc: 0.021821467
case acc: 0.027124865
top acc: 0.0267 ::: bot acc: 0.0379
top acc: 0.0259 ::: bot acc: 0.0240
top acc: 0.0334 ::: bot acc: 0.0561
top acc: 0.0199 ::: bot acc: 0.0426
top acc: 0.0335 ::: bot acc: 0.0276
top acc: 0.0390 ::: bot acc: 0.0365
current epoch: 35
train loss is 0.030519
average val loss: 0.021052, accuracy: 0.0213
average test loss: 0.026939, accuracy: 0.0270
case acc: 0.02784663
case acc: 0.019006783
case acc: 0.03531349
case acc: 0.028011912
case acc: 0.023393506
case acc: 0.02836318
top acc: 0.0209 ::: bot acc: 0.0468
top acc: 0.0173 ::: bot acc: 0.0327
top acc: 0.0267 ::: bot acc: 0.0628
top acc: 0.0141 ::: bot acc: 0.0511
top acc: 0.0251 ::: bot acc: 0.0361
top acc: 0.0340 ::: bot acc: 0.0414
current epoch: 36
train loss is 0.030876
average val loss: 0.027586, accuracy: 0.0276
average test loss: 0.033190, accuracy: 0.0332
case acc: 0.035845473
case acc: 0.026920188
case acc: 0.039922338
case acc: 0.03536273
case acc: 0.029254345
case acc: 0.032041624
top acc: 0.0177 ::: bot acc: 0.0604
top acc: 0.0131 ::: bot acc: 0.0466
top acc: 0.0181 ::: bot acc: 0.0741
top acc: 0.0149 ::: bot acc: 0.0617
top acc: 0.0150 ::: bot acc: 0.0498
top acc: 0.0261 ::: bot acc: 0.0508
current epoch: 37
train loss is 0.032572
average val loss: 0.028611, accuracy: 0.0285
average test loss: 0.034091, accuracy: 0.0340
case acc: 0.036997743
case acc: 0.028940748
case acc: 0.04086618
case acc: 0.033126954
case acc: 0.031017765
case acc: 0.032956604
top acc: 0.0176 ::: bot acc: 0.0622
top acc: 0.0134 ::: bot acc: 0.0495
top acc: 0.0172 ::: bot acc: 0.0759
top acc: 0.0142 ::: bot acc: 0.0587
top acc: 0.0142 ::: bot acc: 0.0528
top acc: 0.0249 ::: bot acc: 0.0527
current epoch: 38
train loss is 0.033657
average val loss: 0.019182, accuracy: 0.0192
average test loss: 0.024569, accuracy: 0.0243
case acc: 0.023151549
case acc: 0.01820618
case acc: 0.03346366
case acc: 0.022387298
case acc: 0.021811204
case acc: 0.026542187
top acc: 0.0308 ::: bot acc: 0.0340
top acc: 0.0278 ::: bot acc: 0.0224
top acc: 0.0369 ::: bot acc: 0.0525
top acc: 0.0328 ::: bot acc: 0.0294
top acc: 0.0348 ::: bot acc: 0.0263
top acc: 0.0418 ::: bot acc: 0.0332
current epoch: 39
train loss is 0.032184
average val loss: 0.027182, accuracy: 0.0273
average test loss: 0.030476, accuracy: 0.0306
case acc: 0.029992614
case acc: 0.029585978
case acc: 0.034432445
case acc: 0.029155953
case acc: 0.03115151
case acc: 0.029252104
top acc: 0.0551 ::: bot acc: 0.0142
top acc: 0.0519 ::: bot acc: 0.0095
top acc: 0.0559 ::: bot acc: 0.0335
top acc: 0.0520 ::: bot acc: 0.0138
top acc: 0.0584 ::: bot acc: 0.0110
top acc: 0.0565 ::: bot acc: 0.0190
current epoch: 40
train loss is 0.034527
average val loss: 0.019299, accuracy: 0.0195
average test loss: 0.024567, accuracy: 0.0247
case acc: 0.02312801
case acc: 0.01974323
case acc: 0.033352327
case acc: 0.022297593
case acc: 0.022556264
case acc: 0.026926348
top acc: 0.0361 ::: bot acc: 0.0287
top acc: 0.0341 ::: bot acc: 0.0158
top acc: 0.0382 ::: bot acc: 0.0511
top acc: 0.0280 ::: bot acc: 0.0343
top acc: 0.0409 ::: bot acc: 0.0200
top acc: 0.0396 ::: bot acc: 0.0356
current epoch: 41
train loss is 0.032143
average val loss: 0.023865, accuracy: 0.0241
average test loss: 0.029761, accuracy: 0.0298
case acc: 0.030954031
case acc: 0.021779133
case acc: 0.037471306
case acc: 0.03174494
case acc: 0.025620254
case acc: 0.030941715
top acc: 0.0191 ::: bot acc: 0.0523
top acc: 0.0135 ::: bot acc: 0.0387
top acc: 0.0214 ::: bot acc: 0.0688
top acc: 0.0138 ::: bot acc: 0.0569
top acc: 0.0191 ::: bot acc: 0.0423
top acc: 0.0278 ::: bot acc: 0.0483
current epoch: 42
train loss is 0.031258
average val loss: 0.025119, accuracy: 0.0251
average test loss: 0.030935, accuracy: 0.0309
case acc: 0.03321874
case acc: 0.0244789
case acc: 0.03803639
case acc: 0.031627387
case acc: 0.02758158
case acc: 0.03055446
top acc: 0.0182 ::: bot acc: 0.0561
top acc: 0.0128 ::: bot acc: 0.0431
top acc: 0.0205 ::: bot acc: 0.0700
top acc: 0.0137 ::: bot acc: 0.0567
top acc: 0.0165 ::: bot acc: 0.0465
top acc: 0.0284 ::: bot acc: 0.0474
current epoch: 43
train loss is 0.032096
average val loss: 0.019264, accuracy: 0.0194
average test loss: 0.024946, accuracy: 0.0247
case acc: 0.024434173
case acc: 0.017980676
case acc: 0.033908736
case acc: 0.022939218
case acc: 0.021968897
case acc: 0.027092468
top acc: 0.0254 ::: bot acc: 0.0395
top acc: 0.0226 ::: bot acc: 0.0274
top acc: 0.0329 ::: bot acc: 0.0565
top acc: 0.0236 ::: bot acc: 0.0386
top acc: 0.0298 ::: bot acc: 0.0311
top acc: 0.0388 ::: bot acc: 0.0365
current epoch: 44
train loss is 0.030737
average val loss: 0.020577, accuracy: 0.0206
average test loss: 0.025399, accuracy: 0.0254
case acc: 0.023923514
case acc: 0.02139026
case acc: 0.033261526
case acc: 0.023118813
case acc: 0.023866912
case acc: 0.026594881
top acc: 0.0413 ::: bot acc: 0.0235
top acc: 0.0383 ::: bot acc: 0.0122
top acc: 0.0449 ::: bot acc: 0.0445
top acc: 0.0370 ::: bot acc: 0.0252
top acc: 0.0452 ::: bot acc: 0.0160
top acc: 0.0473 ::: bot acc: 0.0280
current epoch: 45
train loss is 0.031291
average val loss: 0.019447, accuracy: 0.0196
average test loss: 0.024606, accuracy: 0.0247
case acc: 0.023215951
case acc: 0.020058895
case acc: 0.03320968
case acc: 0.022253338
case acc: 0.022793759
case acc: 0.026575739
top acc: 0.0373 ::: bot acc: 0.0275
top acc: 0.0349 ::: bot acc: 0.0150
top acc: 0.0409 ::: bot acc: 0.0485
top acc: 0.0295 ::: bot acc: 0.0328
top acc: 0.0419 ::: bot acc: 0.0190
top acc: 0.0432 ::: bot acc: 0.0321
current epoch: 46
train loss is 0.031194
average val loss: 0.021129, accuracy: 0.0214
average test loss: 0.027012, accuracy: 0.0270
case acc: 0.027368626
case acc: 0.018927123
case acc: 0.035327915
case acc: 0.028031807
case acc: 0.023456346
case acc: 0.028972685
top acc: 0.0213 ::: bot acc: 0.0458
top acc: 0.0172 ::: bot acc: 0.0326
top acc: 0.0266 ::: bot acc: 0.0629
top acc: 0.0141 ::: bot acc: 0.0512
top acc: 0.0246 ::: bot acc: 0.0363
top acc: 0.0321 ::: bot acc: 0.0433
current epoch: 47
train loss is 0.030689
average val loss: 0.026045, accuracy: 0.0261
average test loss: 0.031791, accuracy: 0.0318
case acc: 0.03391066
case acc: 0.025350003
case acc: 0.03861884
case acc: 0.033463445
case acc: 0.028259197
case acc: 0.031254597
top acc: 0.0180 ::: bot acc: 0.0573
top acc: 0.0129 ::: bot acc: 0.0444
top acc: 0.0197 ::: bot acc: 0.0713
top acc: 0.0143 ::: bot acc: 0.0591
top acc: 0.0159 ::: bot acc: 0.0478
top acc: 0.0273 ::: bot acc: 0.0491
current epoch: 48
train loss is 0.031922
average val loss: 0.020924, accuracy: 0.0210
average test loss: 0.026787, accuracy: 0.0267
case acc: 0.027590366
case acc: 0.019659104
case acc: 0.035116673
case acc: 0.025288045
case acc: 0.024085704
case acc: 0.028357014
top acc: 0.0211 ::: bot acc: 0.0463
top acc: 0.0158 ::: bot acc: 0.0344
top acc: 0.0273 ::: bot acc: 0.0622
top acc: 0.0171 ::: bot acc: 0.0455
top acc: 0.0228 ::: bot acc: 0.0382
top acc: 0.0340 ::: bot acc: 0.0414
current epoch: 49
train loss is 0.031395
average val loss: 0.020828, accuracy: 0.0208
average test loss: 0.025583, accuracy: 0.0255
case acc: 0.02408118
case acc: 0.021438174
case acc: 0.033290554
case acc: 0.023483356
case acc: 0.023975076
case acc: 0.026805503
top acc: 0.0420 ::: bot acc: 0.0227
top acc: 0.0385 ::: bot acc: 0.0119
top acc: 0.0460 ::: bot acc: 0.0434
top acc: 0.0384 ::: bot acc: 0.0239
top acc: 0.0454 ::: bot acc: 0.0159
top acc: 0.0486 ::: bot acc: 0.0268
current epoch: 50
train loss is 0.031353
average val loss: 0.020274, accuracy: 0.0203
average test loss: 0.025191, accuracy: 0.0252
case acc: 0.023908297
case acc: 0.021409048
case acc: 0.0332128
case acc: 0.02250105
case acc: 0.023908343
case acc: 0.026497524
top acc: 0.0413 ::: bot acc: 0.0234
top acc: 0.0384 ::: bot acc: 0.0119
top acc: 0.0441 ::: bot acc: 0.0453
top acc: 0.0335 ::: bot acc: 0.0288
top acc: 0.0452 ::: bot acc: 0.0159
top acc: 0.0455 ::: bot acc: 0.0299
LME_Co_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6768 6768 6768
1.7082474 -0.6288155 0.21141115 -0.19947179
Validation: 756 756 756
Testing: 774 774 774
pre-processing time: 0.0005605220794677734
the split date is 2010-07-01
net initializing with time: 0.004019737243652344
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.319601
average val loss: 0.164273, accuracy: 0.1646
average test loss: 0.165699, accuracy: 0.1659
case acc: 0.22289208
case acc: 0.36785468
case acc: 0.08202596
case acc: 0.14949138
case acc: 0.11697389
case acc: 0.055911716
top acc: 0.1942 ::: bot acc: 0.2518
top acc: 0.3409 ::: bot acc: 0.3950
top acc: 0.0449 ::: bot acc: 0.1176
top acc: 0.1055 ::: bot acc: 0.1911
top acc: 0.0740 ::: bot acc: 0.1539
top acc: 0.0243 ::: bot acc: 0.0868
current epoch: 2
train loss is 0.207701
average val loss: 0.105817, accuracy: 0.1051
average test loss: 0.105643, accuracy: 0.1048
case acc: 0.02226312
case acc: 0.13160543
case acc: 0.14053017
case acc: 0.07330315
case acc: 0.09853608
case acc: 0.16273664
top acc: 0.0342 ::: bot acc: 0.0232
top acc: 0.1047 ::: bot acc: 0.1588
top acc: 0.1788 ::: bot acc: 0.1044
top acc: 0.1158 ::: bot acc: 0.0343
top acc: 0.1416 ::: bot acc: 0.0615
top acc: 0.2024 ::: bot acc: 0.1277
current epoch: 3
train loss is 0.078527
average val loss: 0.139421, accuracy: 0.1401
average test loss: 0.141023, accuracy: 0.1412
case acc: 0.19238429
case acc: 0.3171947
case acc: 0.061215147
case acc: 0.12570263
case acc: 0.10384805
case acc: 0.046807457
top acc: 0.1637 ::: bot acc: 0.2214
top acc: 0.2902 ::: bot acc: 0.3445
top acc: 0.0262 ::: bot acc: 0.0960
top acc: 0.0816 ::: bot acc: 0.1672
top acc: 0.0621 ::: bot acc: 0.1399
top acc: 0.0198 ::: bot acc: 0.0753
current epoch: 4
train loss is 0.115021
average val loss: 0.139073, accuracy: 0.1396
average test loss: 0.140699, accuracy: 0.1408
case acc: 0.18713664
case acc: 0.30795187
case acc: 0.06416611
case acc: 0.1272358
case acc: 0.10757082
case acc: 0.050913524
top acc: 0.1586 ::: bot acc: 0.2160
top acc: 0.2809 ::: bot acc: 0.3354
top acc: 0.0286 ::: bot acc: 0.0994
top acc: 0.0830 ::: bot acc: 0.1688
top acc: 0.0653 ::: bot acc: 0.1437
top acc: 0.0217 ::: bot acc: 0.0806
current epoch: 5
train loss is 0.114980
average val loss: 0.102466, accuracy: 0.1040
average test loss: 0.104241, accuracy: 0.1049
case acc: 0.14094736
case acc: 0.25782165
case acc: 0.035655655
case acc: 0.0892084
case acc: 0.075325854
case acc: 0.030329349
top acc: 0.1125 ::: bot acc: 0.1698
top acc: 0.2307 ::: bot acc: 0.2854
top acc: 0.0178 ::: bot acc: 0.0621
top acc: 0.0461 ::: bot acc: 0.1301
top acc: 0.0384 ::: bot acc: 0.1086
top acc: 0.0276 ::: bot acc: 0.0466
current epoch: 6
train loss is 0.103015
average val loss: 0.088629, accuracy: 0.0904
average test loss: 0.090478, accuracy: 0.0914
case acc: 0.11870375
case acc: 0.22983877
case acc: 0.029999046
case acc: 0.07493683
case acc: 0.066487044
case acc: 0.028151942
top acc: 0.0902 ::: bot acc: 0.1476
top acc: 0.2026 ::: bot acc: 0.2575
top acc: 0.0282 ::: bot acc: 0.0480
top acc: 0.0342 ::: bot acc: 0.1145
top acc: 0.0326 ::: bot acc: 0.0982
top acc: 0.0363 ::: bot acc: 0.0375
current epoch: 7
train loss is 0.089883
average val loss: 0.085566, accuracy: 0.0874
average test loss: 0.087437, accuracy: 0.0882
case acc: 0.111047104
case acc: 0.21577582
case acc: 0.029983455
case acc: 0.07368705
case acc: 0.069434434
case acc: 0.029098578
top acc: 0.0826 ::: bot acc: 0.1399
top acc: 0.1885 ::: bot acc: 0.2435
top acc: 0.0279 ::: bot acc: 0.0483
top acc: 0.0333 ::: bot acc: 0.1130
top acc: 0.0342 ::: bot acc: 0.1017
top acc: 0.0310 ::: bot acc: 0.0427
current epoch: 8
train loss is 0.083166
average val loss: 0.087121, accuracy: 0.0887
average test loss: 0.088913, accuracy: 0.0894
case acc: 0.109227926
case acc: 0.2082338
case acc: 0.032046836
case acc: 0.07760036
case acc: 0.07653224
case acc: 0.03273201
top acc: 0.0807 ::: bot acc: 0.1380
top acc: 0.1809 ::: bot acc: 0.2360
top acc: 0.0220 ::: bot acc: 0.0548
top acc: 0.0364 ::: bot acc: 0.1172
top acc: 0.0387 ::: bot acc: 0.1099
top acc: 0.0221 ::: bot acc: 0.0528
current epoch: 9
train loss is 0.081253
average val loss: 0.088554, accuracy: 0.0898
average test loss: 0.090230, accuracy: 0.0905
case acc: 0.10641518
case acc: 0.20147847
case acc: 0.034658168
case acc: 0.081025705
case acc: 0.08232091
case acc: 0.03723215
top acc: 0.0778 ::: bot acc: 0.1352
top acc: 0.1742 ::: bot acc: 0.2292
top acc: 0.0183 ::: bot acc: 0.0608
top acc: 0.0391 ::: bot acc: 0.1210
top acc: 0.0431 ::: bot acc: 0.1164
top acc: 0.0186 ::: bot acc: 0.0613
current epoch: 10
train loss is 0.084338
average val loss: 0.100950, accuracy: 0.1016
average test loss: 0.102497, accuracy: 0.1025
case acc: 0.1150203
case acc: 0.20838906
case acc: 0.04568262
case acc: 0.09602027
case acc: 0.09873313
case acc: 0.051187206
top acc: 0.0863 ::: bot acc: 0.1439
top acc: 0.1810 ::: bot acc: 0.2362
top acc: 0.0165 ::: bot acc: 0.0783
top acc: 0.0521 ::: bot acc: 0.1370
top acc: 0.0571 ::: bot acc: 0.1340
top acc: 0.0217 ::: bot acc: 0.0807
current epoch: 11
train loss is 0.079979
average val loss: 0.084651, accuracy: 0.0858
average test loss: 0.086342, accuracy: 0.0865
case acc: 0.09151466
case acc: 0.18366265
case acc: 0.036024515
case acc: 0.08105353
case acc: 0.0851672
case acc: 0.04152913
top acc: 0.0627 ::: bot acc: 0.1204
top acc: 0.1562 ::: bot acc: 0.2115
top acc: 0.0171 ::: bot acc: 0.0636
top acc: 0.0392 ::: bot acc: 0.1209
top acc: 0.0453 ::: bot acc: 0.1195
top acc: 0.0184 ::: bot acc: 0.0678
current epoch: 12
train loss is 0.074608
average val loss: 0.080834, accuracy: 0.0820
average test loss: 0.082566, accuracy: 0.0827
case acc: 0.0820159
case acc: 0.17283367
case acc: 0.035451476
case acc: 0.07901366
case acc: 0.084455356
case acc: 0.042158917
top acc: 0.0532 ::: bot acc: 0.1110
top acc: 0.1454 ::: bot acc: 0.2006
top acc: 0.0174 ::: bot acc: 0.0625
top acc: 0.0376 ::: bot acc: 0.1187
top acc: 0.0447 ::: bot acc: 0.1187
top acc: 0.0185 ::: bot acc: 0.0687
current epoch: 13
train loss is 0.072496
average val loss: 0.079271, accuracy: 0.0803
average test loss: 0.081026, accuracy: 0.0810
case acc: 0.075153865
case acc: 0.16492538
case acc: 0.03621236
case acc: 0.07933776
case acc: 0.08597296
case acc: 0.044529058
top acc: 0.0463 ::: bot acc: 0.1041
top acc: 0.1374 ::: bot acc: 0.1927
top acc: 0.0168 ::: bot acc: 0.0640
top acc: 0.0379 ::: bot acc: 0.1191
top acc: 0.0460 ::: bot acc: 0.1203
top acc: 0.0191 ::: bot acc: 0.0720
current epoch: 14
train loss is 0.067730
average val loss: 0.070285, accuracy: 0.0716
average test loss: 0.072166, accuracy: 0.0721
case acc: 0.059866488
case acc: 0.14833842
case acc: 0.032526664
case acc: 0.07167593
case acc: 0.07949156
case acc: 0.040535044
top acc: 0.0314 ::: bot acc: 0.0887
top acc: 0.1208 ::: bot acc: 0.1762
top acc: 0.0204 ::: bot acc: 0.0568
top acc: 0.0319 ::: bot acc: 0.1106
top acc: 0.0408 ::: bot acc: 0.1131
top acc: 0.0182 ::: bot acc: 0.0665
current epoch: 15
train loss is 0.062700
average val loss: 0.065305, accuracy: 0.0667
average test loss: 0.067245, accuracy: 0.0670
case acc: 0.049932614
case acc: 0.13617384
case acc: 0.031395786
case acc: 0.067936294
case acc: 0.077014334
case acc: 0.03974847
top acc: 0.0225 ::: bot acc: 0.0781
top acc: 0.1086 ::: bot acc: 0.1641
top acc: 0.0227 ::: bot acc: 0.0539
top acc: 0.0290 ::: bot acc: 0.1065
top acc: 0.0389 ::: bot acc: 0.1103
top acc: 0.0181 ::: bot acc: 0.0654
current epoch: 16
train loss is 0.058614
average val loss: 0.057818, accuracy: 0.0594
average test loss: 0.059798, accuracy: 0.0596
case acc: 0.038257856
case acc: 0.12036106
case acc: 0.029604571
case acc: 0.0612876
case acc: 0.07130623
case acc: 0.036714215
top acc: 0.0146 ::: bot acc: 0.0646
top acc: 0.0928 ::: bot acc: 0.1483
top acc: 0.0292 ::: bot acc: 0.0475
top acc: 0.0249 ::: bot acc: 0.0986
top acc: 0.0350 ::: bot acc: 0.1037
top acc: 0.0188 ::: bot acc: 0.0605
current epoch: 17
train loss is 0.053230
average val loss: 0.050432, accuracy: 0.0520
average test loss: 0.052380, accuracy: 0.0522
case acc: 0.028678453
case acc: 0.103438176
case acc: 0.028471617
case acc: 0.05428358
case acc: 0.06483839
case acc: 0.033432998
top acc: 0.0124 ::: bot acc: 0.0513
top acc: 0.0759 ::: bot acc: 0.1313
top acc: 0.0365 ::: bot acc: 0.0401
top acc: 0.0221 ::: bot acc: 0.0895
top acc: 0.0312 ::: bot acc: 0.0958
top acc: 0.0208 ::: bot acc: 0.0545
current epoch: 18
train loss is 0.048136
average val loss: 0.041448, accuracy: 0.0424
average test loss: 0.043362, accuracy: 0.0433
case acc: 0.021786608
case acc: 0.080896325
case acc: 0.028857317
case acc: 0.044355262
case acc: 0.05472469
case acc: 0.029148757
top acc: 0.0229 ::: bot acc: 0.0348
top acc: 0.0533 ::: bot acc: 0.1087
top acc: 0.0487 ::: bot acc: 0.0279
top acc: 0.0217 ::: bot acc: 0.0748
top acc: 0.0272 ::: bot acc: 0.0827
top acc: 0.0301 ::: bot acc: 0.0433
current epoch: 19
train loss is 0.042979
average val loss: 0.035384, accuracy: 0.0355
average test loss: 0.036985, accuracy: 0.0370
case acc: 0.021817755
case acc: 0.05832071
case acc: 0.032201137
case acc: 0.03644202
case acc: 0.04557827
case acc: 0.027516268
top acc: 0.0355 ::: bot acc: 0.0223
top acc: 0.0313 ::: bot acc: 0.0859
top acc: 0.0597 ::: bot acc: 0.0174
top acc: 0.0271 ::: bot acc: 0.0602
top acc: 0.0246 ::: bot acc: 0.0702
top acc: 0.0403 ::: bot acc: 0.0331
current epoch: 20
train loss is 0.040885
average val loss: 0.031652, accuracy: 0.0308
average test loss: 0.032939, accuracy: 0.0326
case acc: 0.02551749
case acc: 0.034593094
case acc: 0.038962785
case acc: 0.031149935
case acc: 0.037437905
case acc: 0.027677104
top acc: 0.0465 ::: bot acc: 0.0125
top acc: 0.0114 ::: bot acc: 0.0602
top acc: 0.0713 ::: bot acc: 0.0144
top acc: 0.0415 ::: bot acc: 0.0439
top acc: 0.0262 ::: bot acc: 0.0572
top acc: 0.0508 ::: bot acc: 0.0226
current epoch: 21
train loss is 0.039656
average val loss: 0.030495, accuracy: 0.0291
average test loss: 0.031553, accuracy: 0.0311
case acc: 0.027651576
case acc: 0.021875652
case acc: 0.043822058
case acc: 0.031039098
case acc: 0.033863757
case acc: 0.028407138
top acc: 0.0505 ::: bot acc: 0.0111
top acc: 0.0171 ::: bot acc: 0.0382
top acc: 0.0777 ::: bot acc: 0.0160
top acc: 0.0528 ::: bot acc: 0.0326
top acc: 0.0298 ::: bot acc: 0.0500
top acc: 0.0553 ::: bot acc: 0.0181
current epoch: 22
train loss is 0.037602
average val loss: 0.032512, accuracy: 0.0311
average test loss: 0.032943, accuracy: 0.0329
case acc: 0.030045023
case acc: 0.022601094
case acc: 0.049464677
case acc: 0.034401298
case acc: 0.031043574
case acc: 0.029924858
top acc: 0.0542 ::: bot acc: 0.0109
top acc: 0.0399 ::: bot acc: 0.0154
top acc: 0.0848 ::: bot acc: 0.0186
top acc: 0.0645 ::: bot acc: 0.0211
top acc: 0.0355 ::: bot acc: 0.0429
top acc: 0.0598 ::: bot acc: 0.0141
current epoch: 23
train loss is 0.035971
average val loss: 0.039826, accuracy: 0.0388
average test loss: 0.039521, accuracy: 0.0399
case acc: 0.0347448
case acc: 0.039296947
case acc: 0.058903445
case acc: 0.043111164
case acc: 0.028455919
case acc: 0.03461446
top acc: 0.0608 ::: bot acc: 0.0116
top acc: 0.0653 ::: bot acc: 0.0146
top acc: 0.0957 ::: bot acc: 0.0250
top acc: 0.0797 ::: bot acc: 0.0167
top acc: 0.0461 ::: bot acc: 0.0322
top acc: 0.0681 ::: bot acc: 0.0115
current epoch: 24
train loss is 0.037446
average val loss: 0.054346, accuracy: 0.0538
average test loss: 0.053478, accuracy: 0.0536
case acc: 0.044002727
case acc: 0.06704264
case acc: 0.074843355
case acc: 0.05920921
case acc: 0.030311126
case acc: 0.046442408
top acc: 0.0721 ::: bot acc: 0.0169
top acc: 0.0945 ::: bot acc: 0.0394
top acc: 0.1127 ::: bot acc: 0.0387
top acc: 0.1006 ::: bot acc: 0.0232
top acc: 0.0626 ::: bot acc: 0.0158
top acc: 0.0828 ::: bot acc: 0.0175
current epoch: 25
train loss is 0.044987
average val loss: 0.075950, accuracy: 0.0756
average test loss: 0.074904, accuracy: 0.0748
case acc: 0.05873743
case acc: 0.09821525
case acc: 0.097421795
case acc: 0.08350105
case acc: 0.044621646
case acc: 0.06612118
top acc: 0.0876 ::: bot acc: 0.0301
top acc: 0.1257 ::: bot acc: 0.0706
top acc: 0.1360 ::: bot acc: 0.0598
top acc: 0.1269 ::: bot acc: 0.0433
top acc: 0.0855 ::: bot acc: 0.0131
top acc: 0.1044 ::: bot acc: 0.0332
current epoch: 26
train loss is 0.059697
average val loss: 0.067678, accuracy: 0.0673
average test loss: 0.066739, accuracy: 0.0666
case acc: 0.04266444
case acc: 0.09048251
case acc: 0.08955524
case acc: 0.0780714
case acc: 0.03949235
case acc: 0.059496555
top acc: 0.0705 ::: bot acc: 0.0160
top acc: 0.1180 ::: bot acc: 0.0628
top acc: 0.1280 ::: bot acc: 0.0523
top acc: 0.1211 ::: bot acc: 0.0386
top acc: 0.0784 ::: bot acc: 0.0117
top acc: 0.0973 ::: bot acc: 0.0274
current epoch: 27
train loss is 0.065158
average val loss: 0.042854, accuracy: 0.0418
average test loss: 0.042663, accuracy: 0.0427
case acc: 0.021193214
case acc: 0.05478651
case acc: 0.060892712
case acc: 0.053624745
case acc: 0.028002435
case acc: 0.037610304
top acc: 0.0328 ::: bot acc: 0.0247
top acc: 0.0822 ::: bot acc: 0.0274
top acc: 0.0979 ::: bot acc: 0.0265
top acc: 0.0938 ::: bot acc: 0.0199
top acc: 0.0527 ::: bot acc: 0.0253
top acc: 0.0722 ::: bot acc: 0.0118
current epoch: 28
train loss is 0.063779
average val loss: 0.029666, accuracy: 0.0286
average test loss: 0.030934, accuracy: 0.0315
case acc: 0.03709056
case acc: 0.021877507
case acc: 0.034324557
case acc: 0.033572003
case acc: 0.03497952
case acc: 0.027346414
top acc: 0.0140 ::: bot acc: 0.0628
top acc: 0.0378 ::: bot acc: 0.0174
top acc: 0.0636 ::: bot acc: 0.0152
top acc: 0.0622 ::: bot acc: 0.0231
top acc: 0.0276 ::: bot acc: 0.0525
top acc: 0.0462 ::: bot acc: 0.0269
current epoch: 29
train loss is 0.060790
average val loss: 0.037443, accuracy: 0.0378
average test loss: 0.039473, accuracy: 0.0402
case acc: 0.061912507
case acc: 0.0365621
case acc: 0.028568963
case acc: 0.033483956
case acc: 0.049691256
case acc: 0.030844225
top acc: 0.0333 ::: bot acc: 0.0904
top acc: 0.0126 ::: bot acc: 0.0625
top acc: 0.0331 ::: bot acc: 0.0433
top acc: 0.0320 ::: bot acc: 0.0534
top acc: 0.0248 ::: bot acc: 0.0760
top acc: 0.0246 ::: bot acc: 0.0488
current epoch: 30
train loss is 0.052671
average val loss: 0.043527, accuracy: 0.0443
average test loss: 0.045517, accuracy: 0.0464
case acc: 0.06312938
case acc: 0.06055513
case acc: 0.031049818
case acc: 0.038806297
case acc: 0.053104747
case acc: 0.032031346
top acc: 0.0345 ::: bot acc: 0.0916
top acc: 0.0334 ::: bot acc: 0.0881
top acc: 0.0229 ::: bot acc: 0.0536
top acc: 0.0240 ::: bot acc: 0.0654
top acc: 0.0260 ::: bot acc: 0.0805
top acc: 0.0219 ::: bot acc: 0.0520
current epoch: 31
train loss is 0.051939
average val loss: 0.059016, accuracy: 0.0596
average test loss: 0.060776, accuracy: 0.0613
case acc: 0.07356421
case acc: 0.09110657
case acc: 0.04218362
case acc: 0.052906837
case acc: 0.066385075
case acc: 0.04163712
top acc: 0.0448 ::: bot acc: 0.1021
top acc: 0.0636 ::: bot acc: 0.1188
top acc: 0.0149 ::: bot acc: 0.0743
top acc: 0.0218 ::: bot acc: 0.0877
top acc: 0.0320 ::: bot acc: 0.0974
top acc: 0.0184 ::: bot acc: 0.0682
current epoch: 32
train loss is 0.056420
average val loss: 0.063429, accuracy: 0.0639
average test loss: 0.065181, accuracy: 0.0655
case acc: 0.070497856
case acc: 0.09877073
case acc: 0.04755363
case acc: 0.058541484
case acc: 0.07123423
case acc: 0.04619467
top acc: 0.0417 ::: bot acc: 0.0990
top acc: 0.0713 ::: bot acc: 0.1265
top acc: 0.0171 ::: bot acc: 0.0813
top acc: 0.0238 ::: bot acc: 0.0952
top acc: 0.0349 ::: bot acc: 0.1033
top acc: 0.0195 ::: bot acc: 0.0745
current epoch: 33
train loss is 0.049209
average val loss: 0.044862, accuracy: 0.0457
average test loss: 0.046875, accuracy: 0.0474
case acc: 0.042674754
case acc: 0.073203795
case acc: 0.03431443
case acc: 0.0442818
case acc: 0.055658467
case acc: 0.034110513
top acc: 0.0174 ::: bot acc: 0.0695
top acc: 0.0457 ::: bot acc: 0.1009
top acc: 0.0175 ::: bot acc: 0.0612
top acc: 0.0217 ::: bot acc: 0.0748
top acc: 0.0269 ::: bot acc: 0.0839
top acc: 0.0196 ::: bot acc: 0.0563
current epoch: 34
train loss is 0.041788
average val loss: 0.039606, accuracy: 0.0405
average test loss: 0.041663, accuracy: 0.0420
case acc: 0.032471187
case acc: 0.061889738
case acc: 0.032015607
case acc: 0.04084333
case acc: 0.05231739
case acc: 0.032560542
top acc: 0.0123 ::: bot acc: 0.0567
top acc: 0.0347 ::: bot acc: 0.0894
top acc: 0.0203 ::: bot acc: 0.0563
top acc: 0.0229 ::: bot acc: 0.0691
top acc: 0.0257 ::: bot acc: 0.0795
top acc: 0.0211 ::: bot acc: 0.0532
current epoch: 35
train loss is 0.037906
average val loss: 0.034980, accuracy: 0.0358
average test loss: 0.037108, accuracy: 0.0373
case acc: 0.025711125
case acc: 0.051259495
case acc: 0.030393953
case acc: 0.03744958
case acc: 0.048300438
case acc: 0.030903943
top acc: 0.0138 ::: bot acc: 0.0458
top acc: 0.0247 ::: bot acc: 0.0785
top acc: 0.0252 ::: bot acc: 0.0513
top acc: 0.0253 ::: bot acc: 0.0628
top acc: 0.0244 ::: bot acc: 0.0741
top acc: 0.0242 ::: bot acc: 0.0492
current epoch: 36
train loss is 0.034642
average val loss: 0.030212, accuracy: 0.0307
average test loss: 0.032358, accuracy: 0.0324
case acc: 0.021542385
case acc: 0.03920873
case acc: 0.028714739
case acc: 0.033569224
case acc: 0.04265253
case acc: 0.028626254
top acc: 0.0222 ::: bot acc: 0.0350
top acc: 0.0146 ::: bot acc: 0.0655
top acc: 0.0324 ::: bot acc: 0.0442
top acc: 0.0316 ::: bot acc: 0.0539
top acc: 0.0239 ::: bot acc: 0.0659
top acc: 0.0308 ::: bot acc: 0.0423
current epoch: 37
train loss is 0.031766
average val loss: 0.026989, accuracy: 0.0271
average test loss: 0.028993, accuracy: 0.0290
case acc: 0.020927355
case acc: 0.028649878
case acc: 0.028183863
case acc: 0.031102398
case acc: 0.03748646
case acc: 0.027520742
top acc: 0.0301 ::: bot acc: 0.0270
top acc: 0.0097 ::: bot acc: 0.0521
top acc: 0.0395 ::: bot acc: 0.0370
top acc: 0.0409 ::: bot acc: 0.0443
top acc: 0.0257 ::: bot acc: 0.0573
top acc: 0.0381 ::: bot acc: 0.0351
current epoch: 38
train loss is 0.030249
average val loss: 0.025543, accuracy: 0.0252
average test loss: 0.027331, accuracy: 0.0273
case acc: 0.021326242
case acc: 0.022334272
case acc: 0.028319066
case acc: 0.030586448
case acc: 0.033803266
case acc: 0.027293043
top acc: 0.0344 ::: bot acc: 0.0227
top acc: 0.0157 ::: bot acc: 0.0396
top acc: 0.0454 ::: bot acc: 0.0312
top acc: 0.0495 ::: bot acc: 0.0357
top acc: 0.0295 ::: bot acc: 0.0499
top acc: 0.0441 ::: bot acc: 0.0290
current epoch: 39
train loss is 0.029342
average val loss: 0.025407, accuracy: 0.0248
average test loss: 0.026930, accuracy: 0.0269
case acc: 0.021537382
case acc: 0.02018533
case acc: 0.029012632
case acc: 0.031878456
case acc: 0.03149454
case acc: 0.027467161
top acc: 0.0356 ::: bot acc: 0.0215
top acc: 0.0261 ::: bot acc: 0.0290
top acc: 0.0494 ::: bot acc: 0.0271
top acc: 0.0564 ::: bot acc: 0.0288
top acc: 0.0337 ::: bot acc: 0.0443
top acc: 0.0485 ::: bot acc: 0.0247
current epoch: 40
train loss is 0.028808
average val loss: 0.026272, accuracy: 0.0257
average test loss: 0.027373, accuracy: 0.0275
case acc: 0.021668062
case acc: 0.021269538
case acc: 0.03009604
case acc: 0.03382497
case acc: 0.029865026
case acc: 0.027980085
top acc: 0.0362 ::: bot acc: 0.0209
top acc: 0.0357 ::: bot acc: 0.0195
top acc: 0.0533 ::: bot acc: 0.0232
top acc: 0.0628 ::: bot acc: 0.0224
top acc: 0.0389 ::: bot acc: 0.0391
top acc: 0.0526 ::: bot acc: 0.0206
current epoch: 41
train loss is 0.028892
average val loss: 0.027375, accuracy: 0.0267
average test loss: 0.028156, accuracy: 0.0283
case acc: 0.021488847
case acc: 0.023789302
case acc: 0.030945841
case acc: 0.035930034
case acc: 0.02886109
case acc: 0.028608138
top acc: 0.0353 ::: bot acc: 0.0218
top acc: 0.0425 ::: bot acc: 0.0137
top acc: 0.0559 ::: bot acc: 0.0206
top acc: 0.0676 ::: bot acc: 0.0191
top acc: 0.0428 ::: bot acc: 0.0351
top acc: 0.0557 ::: bot acc: 0.0174
current epoch: 42
train loss is 0.029042
average val loss: 0.028269, accuracy: 0.0276
average test loss: 0.028889, accuracy: 0.0290
case acc: 0.02120957
case acc: 0.026052617
case acc: 0.03150243
case acc: 0.037703764
case acc: 0.028386611
case acc: 0.029258937
top acc: 0.0336 ::: bot acc: 0.0235
top acc: 0.0468 ::: bot acc: 0.0119
top acc: 0.0574 ::: bot acc: 0.0192
top acc: 0.0709 ::: bot acc: 0.0178
top acc: 0.0456 ::: bot acc: 0.0324
top acc: 0.0578 ::: bot acc: 0.0153
current epoch: 43
train loss is 0.029354
average val loss: 0.029079, accuracy: 0.0284
average test loss: 0.029566, accuracy: 0.0296
case acc: 0.021021776
case acc: 0.027391346
case acc: 0.03201381
case acc: 0.03918384
case acc: 0.028133217
case acc: 0.030095555
top acc: 0.0324 ::: bot acc: 0.0248
top acc: 0.0493 ::: bot acc: 0.0110
top acc: 0.0587 ::: bot acc: 0.0181
top acc: 0.0735 ::: bot acc: 0.0171
top acc: 0.0480 ::: bot acc: 0.0299
top acc: 0.0599 ::: bot acc: 0.0136
current epoch: 44
train loss is 0.029585
average val loss: 0.029088, accuracy: 0.0284
average test loss: 0.029598, accuracy: 0.0297
case acc: 0.020897407
case acc: 0.02709276
case acc: 0.03186829
case acc: 0.039551053
case acc: 0.028093519
case acc: 0.030443514
top acc: 0.0297 ::: bot acc: 0.0274
top acc: 0.0487 ::: bot acc: 0.0112
top acc: 0.0583 ::: bot acc: 0.0184
top acc: 0.0741 ::: bot acc: 0.0169
top acc: 0.0489 ::: bot acc: 0.0290
top acc: 0.0606 ::: bot acc: 0.0132
current epoch: 45
train loss is 0.029644
average val loss: 0.028552, accuracy: 0.0279
average test loss: 0.029167, accuracy: 0.0292
case acc: 0.021045193
case acc: 0.025557684
case acc: 0.031248437
case acc: 0.039057903
case acc: 0.028079396
case acc: 0.03033873
top acc: 0.0265 ::: bot acc: 0.0306
top acc: 0.0459 ::: bot acc: 0.0123
top acc: 0.0566 ::: bot acc: 0.0199
top acc: 0.0733 ::: bot acc: 0.0172
top acc: 0.0488 ::: bot acc: 0.0291
top acc: 0.0604 ::: bot acc: 0.0133
current epoch: 46
train loss is 0.029380
average val loss: 0.027450, accuracy: 0.0268
average test loss: 0.028312, accuracy: 0.0283
case acc: 0.021496529
case acc: 0.022795934
case acc: 0.030136125
case acc: 0.037478566
case acc: 0.02816864
case acc: 0.029586138
top acc: 0.0223 ::: bot acc: 0.0348
top acc: 0.0404 ::: bot acc: 0.0149
top acc: 0.0532 ::: bot acc: 0.0233
top acc: 0.0705 ::: bot acc: 0.0179
top acc: 0.0472 ::: bot acc: 0.0307
top acc: 0.0588 ::: bot acc: 0.0144
current epoch: 47
train loss is 0.029520
average val loss: 0.026430, accuracy: 0.0259
average test loss: 0.027610, accuracy: 0.0275
case acc: 0.022525975
case acc: 0.020711549
case acc: 0.028981032
case acc: 0.035599004
case acc: 0.02838565
case acc: 0.028985426
top acc: 0.0186 ::: bot acc: 0.0385
top acc: 0.0332 ::: bot acc: 0.0220
top acc: 0.0491 ::: bot acc: 0.0273
top acc: 0.0670 ::: bot acc: 0.0193
top acc: 0.0454 ::: bot acc: 0.0325
top acc: 0.0570 ::: bot acc: 0.0161
current epoch: 48
train loss is 0.029562
average val loss: 0.025752, accuracy: 0.0254
average test loss: 0.027257, accuracy: 0.0272
case acc: 0.024203131
case acc: 0.020413106
case acc: 0.028158925
case acc: 0.03353789
case acc: 0.028860746
case acc: 0.028260227
top acc: 0.0152 ::: bot acc: 0.0427
top acc: 0.0238 ::: bot acc: 0.0313
top acc: 0.0436 ::: bot acc: 0.0328
top acc: 0.0620 ::: bot acc: 0.0230
top acc: 0.0427 ::: bot acc: 0.0351
top acc: 0.0543 ::: bot acc: 0.0188
current epoch: 49
train loss is 0.030054
average val loss: 0.025979, accuracy: 0.0258
average test loss: 0.027871, accuracy: 0.0280
case acc: 0.027047798
case acc: 0.023956975
case acc: 0.028260125
case acc: 0.031504218
case acc: 0.029903242
case acc: 0.027613377
top acc: 0.0130 ::: bot acc: 0.0481
top acc: 0.0128 ::: bot acc: 0.0434
top acc: 0.0362 ::: bot acc: 0.0403
top acc: 0.0551 ::: bot acc: 0.0299
top acc: 0.0386 ::: bot acc: 0.0392
top acc: 0.0501 ::: bot acc: 0.0229
current epoch: 50
train loss is 0.031859
average val loss: 0.028565, accuracy: 0.0289
average test loss: 0.030726, accuracy: 0.0312
case acc: 0.0320654
case acc: 0.03481349
case acc: 0.030345608
case acc: 0.030519763
case acc: 0.03227013
case acc: 0.027279787
top acc: 0.0124 ::: bot acc: 0.0559
top acc: 0.0116 ::: bot acc: 0.0603
top acc: 0.0250 ::: bot acc: 0.0515
top acc: 0.0443 ::: bot acc: 0.0407
top acc: 0.0317 ::: bot acc: 0.0464
top acc: 0.0428 ::: bot acc: 0.0302
LME_Co_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6798 6798 6798
1.7082474 -0.6288155 0.21141115 -0.19947179
Validation: 756 756 756
Testing: 750 750 750
pre-processing time: 0.00037741661071777344
the split date is 2011-01-01
net initializing with time: 0.003299713134765625
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.290161
average val loss: 0.104450, accuracy: 0.1051
average test loss: 0.093339, accuracy: 0.0941
case acc: 0.17710792
case acc: 0.15081564
case acc: 0.058775615
case acc: 0.046584565
case acc: 0.045150492
case acc: 0.08623375
top acc: 0.1454 ::: bot acc: 0.2101
top acc: 0.1237 ::: bot acc: 0.1789
top acc: 0.0244 ::: bot acc: 0.0994
top acc: 0.0168 ::: bot acc: 0.0794
top acc: 0.0153 ::: bot acc: 0.0802
top acc: 0.0483 ::: bot acc: 0.1227
current epoch: 2
train loss is 0.101481
average val loss: 0.066355, accuracy: 0.0680
average test loss: 0.058085, accuracy: 0.0607
case acc: 0.124060236
case acc: 0.10078644
case acc: 0.035104625
case acc: 0.0273958
case acc: 0.029734502
case acc: 0.046993498
top acc: 0.0922 ::: bot acc: 0.1572
top acc: 0.0737 ::: bot acc: 0.1288
top acc: 0.0313 ::: bot acc: 0.0605
top acc: 0.0270 ::: bot acc: 0.0452
top acc: 0.0371 ::: bot acc: 0.0441
top acc: 0.0198 ::: bot acc: 0.0783
current epoch: 3
train loss is 0.095334
average val loss: 0.097248, accuracy: 0.0977
average test loss: 0.085864, accuracy: 0.0865
case acc: 0.15066276
case acc: 0.13237143
case acc: 0.058157705
case acc: 0.055635285
case acc: 0.050241064
case acc: 0.07195784
top acc: 0.1188 ::: bot acc: 0.1839
top acc: 0.1053 ::: bot acc: 0.1603
top acc: 0.0242 ::: bot acc: 0.0985
top acc: 0.0229 ::: bot acc: 0.0899
top acc: 0.0162 ::: bot acc: 0.0874
top acc: 0.0351 ::: bot acc: 0.1080
current epoch: 4
train loss is 0.089312
average val loss: 0.078826, accuracy: 0.0796
average test loss: 0.067921, accuracy: 0.0691
case acc: 0.12134912
case acc: 0.10782359
case acc: 0.045490414
case acc: 0.04730636
case acc: 0.042324588
case acc: 0.05023618
top acc: 0.0893 ::: bot acc: 0.1547
top acc: 0.0808 ::: bot acc: 0.1357
top acc: 0.0199 ::: bot acc: 0.0817
top acc: 0.0172 ::: bot acc: 0.0802
top acc: 0.0154 ::: bot acc: 0.0760
top acc: 0.0211 ::: bot acc: 0.0826
current epoch: 5
train loss is 0.076625
average val loss: 0.078880, accuracy: 0.0794
average test loss: 0.067745, accuracy: 0.0687
case acc: 0.11170264
case acc: 0.10181693
case acc: 0.04825343
case acc: 0.055344958
case acc: 0.047359396
case acc: 0.04775633
top acc: 0.0797 ::: bot acc: 0.1451
top acc: 0.0748 ::: bot acc: 0.1297
top acc: 0.0204 ::: bot acc: 0.0855
top acc: 0.0226 ::: bot acc: 0.0895
top acc: 0.0155 ::: bot acc: 0.0835
top acc: 0.0199 ::: bot acc: 0.0795
current epoch: 6
train loss is 0.069434
average val loss: 0.077579, accuracy: 0.0780
average test loss: 0.066328, accuracy: 0.0671
case acc: 0.100400284
case acc: 0.09380578
case acc: 0.05029772
case acc: 0.06186112
case acc: 0.051049188
case acc: 0.04531566
top acc: 0.0683 ::: bot acc: 0.1338
top acc: 0.0668 ::: bot acc: 0.1218
top acc: 0.0210 ::: bot acc: 0.0882
top acc: 0.0274 ::: bot acc: 0.0968
top acc: 0.0163 ::: bot acc: 0.0887
top acc: 0.0192 ::: bot acc: 0.0763
current epoch: 7
train loss is 0.062105
average val loss: 0.074698, accuracy: 0.0751
average test loss: 0.063415, accuracy: 0.0641
case acc: 0.087323844
case acc: 0.083417825
case acc: 0.05138875
case acc: 0.066475324
case acc: 0.05297481
case acc: 0.04291441
top acc: 0.0553 ::: bot acc: 0.1208
top acc: 0.0563 ::: bot acc: 0.1115
top acc: 0.0214 ::: bot acc: 0.0896
top acc: 0.0313 ::: bot acc: 0.1017
top acc: 0.0170 ::: bot acc: 0.0912
top acc: 0.0188 ::: bot acc: 0.0730
current epoch: 8
train loss is 0.056789
average val loss: 0.068128, accuracy: 0.0684
average test loss: 0.057031, accuracy: 0.0575
case acc: 0.0702463
case acc: 0.06817756
case acc: 0.049699653
case acc: 0.06653382
case acc: 0.05108208
case acc: 0.039081693
top acc: 0.0383 ::: bot acc: 0.1037
top acc: 0.0410 ::: bot acc: 0.0963
top acc: 0.0208 ::: bot acc: 0.0875
top acc: 0.0314 ::: bot acc: 0.1016
top acc: 0.0161 ::: bot acc: 0.0889
top acc: 0.0184 ::: bot acc: 0.0675
current epoch: 9
train loss is 0.049876
average val loss: 0.061020, accuracy: 0.0610
average test loss: 0.050326, accuracy: 0.0503
case acc: 0.05312978
case acc: 0.05165261
case acc: 0.047732376
case acc: 0.06483711
case acc: 0.04804451
case acc: 0.03622243
top acc: 0.0222 ::: bot acc: 0.0861
top acc: 0.0247 ::: bot acc: 0.0798
top acc: 0.0201 ::: bot acc: 0.0848
top acc: 0.0299 ::: bot acc: 0.0997
top acc: 0.0152 ::: bot acc: 0.0848
top acc: 0.0186 ::: bot acc: 0.0631
current epoch: 10
train loss is 0.044510
average val loss: 0.055024, accuracy: 0.0546
average test loss: 0.044905, accuracy: 0.0441
case acc: 0.03856156
case acc: 0.036549684
case acc: 0.046452302
case acc: 0.06287252
case acc: 0.045268666
case acc: 0.03504611
top acc: 0.0104 ::: bot acc: 0.0701
top acc: 0.0113 ::: bot acc: 0.0640
top acc: 0.0198 ::: bot acc: 0.0831
top acc: 0.0283 ::: bot acc: 0.0976
top acc: 0.0146 ::: bot acc: 0.0809
top acc: 0.0191 ::: bot acc: 0.0612
current epoch: 11
train loss is 0.040977
average val loss: 0.049962, accuracy: 0.0491
average test loss: 0.040891, accuracy: 0.0396
case acc: 0.02896906
case acc: 0.025975639
case acc: 0.044947866
case acc: 0.06035523
case acc: 0.043118834
case acc: 0.034163896
top acc: 0.0103 ::: bot acc: 0.0559
top acc: 0.0096 ::: bot acc: 0.0492
top acc: 0.0195 ::: bot acc: 0.0809
top acc: 0.0263 ::: bot acc: 0.0948
top acc: 0.0146 ::: bot acc: 0.0777
top acc: 0.0198 ::: bot acc: 0.0596
current epoch: 12
train loss is 0.040794
average val loss: 0.043317, accuracy: 0.0423
average test loss: 0.036543, accuracy: 0.0350
case acc: 0.024273291
case acc: 0.020380462
case acc: 0.040886555
case acc: 0.05357202
case acc: 0.039264087
case acc: 0.031677116
top acc: 0.0256 ::: bot acc: 0.0395
top acc: 0.0253 ::: bot acc: 0.0310
top acc: 0.0207 ::: bot acc: 0.0743
top acc: 0.0215 ::: bot acc: 0.0871
top acc: 0.0160 ::: bot acc: 0.0712
top acc: 0.0227 ::: bot acc: 0.0545
current epoch: 13
train loss is 0.046335
average val loss: 0.034165, accuracy: 0.0345
average test loss: 0.032967, accuracy: 0.0320
case acc: 0.029862262
case acc: 0.03381639
case acc: 0.03354413
case acc: 0.03568046
case acc: 0.031300224
case acc: 0.027587773
top acc: 0.0533 ::: bot acc: 0.0136
top acc: 0.0576 ::: bot acc: 0.0128
top acc: 0.0365 ::: bot acc: 0.0546
top acc: 0.0133 ::: bot acc: 0.0644
top acc: 0.0308 ::: bot acc: 0.0519
top acc: 0.0378 ::: bot acc: 0.0385
current epoch: 14
train loss is 0.057677
average val loss: 0.049930, accuracy: 0.0506
average test loss: 0.059742, accuracy: 0.0597
case acc: 0.07955553
case acc: 0.09664489
case acc: 0.05043313
case acc: 0.036299057
case acc: 0.046677496
case acc: 0.04860395
top acc: 0.1113 ::: bot acc: 0.0463
top acc: 0.1244 ::: bot acc: 0.0676
top acc: 0.0901 ::: bot acc: 0.0162
top acc: 0.0662 ::: bot acc: 0.0128
top acc: 0.0844 ::: bot acc: 0.0138
top acc: 0.0846 ::: bot acc: 0.0170
current epoch: 15
train loss is 0.074917
average val loss: 0.088294, accuracy: 0.0884
average test loss: 0.099873, accuracy: 0.0999
case acc: 0.11766947
case acc: 0.14547664
case acc: 0.087712966
case acc: 0.08222619
case acc: 0.088472344
case acc: 0.07776307
top acc: 0.1493 ::: bot acc: 0.0845
top acc: 0.1733 ::: bot acc: 0.1163
top acc: 0.1322 ::: bot acc: 0.0437
top acc: 0.1177 ::: bot acc: 0.0471
top acc: 0.1303 ::: bot acc: 0.0473
top acc: 0.1165 ::: bot acc: 0.0405
current epoch: 16
train loss is 0.069825
average val loss: 0.065306, accuracy: 0.0654
average test loss: 0.076505, accuracy: 0.0767
case acc: 0.083769724
case acc: 0.11749463
case acc: 0.06747694
case acc: 0.06924724
case acc: 0.07287782
case acc: 0.04945496
top acc: 0.1154 ::: bot acc: 0.0505
top acc: 0.1454 ::: bot acc: 0.0880
top acc: 0.1102 ::: bot acc: 0.0269
top acc: 0.1044 ::: bot acc: 0.0348
top acc: 0.1144 ::: bot acc: 0.0322
top acc: 0.0856 ::: bot acc: 0.0175
current epoch: 17
train loss is 0.057916
average val loss: 0.056489, accuracy: 0.0563
average test loss: 0.067468, accuracy: 0.0677
case acc: 0.06363177
case acc: 0.09843337
case acc: 0.06325266
case acc: 0.06815502
case acc: 0.070003115
case acc: 0.042826407
top acc: 0.0950 ::: bot acc: 0.0308
top acc: 0.1264 ::: bot acc: 0.0687
top acc: 0.1054 ::: bot acc: 0.0237
top acc: 0.1032 ::: bot acc: 0.0339
top acc: 0.1115 ::: bot acc: 0.0295
top acc: 0.0770 ::: bot acc: 0.0150
current epoch: 18
train loss is 0.051723
average val loss: 0.049876, accuracy: 0.0496
average test loss: 0.060567, accuracy: 0.0608
case acc: 0.046625283
case acc: 0.07981817
case acc: 0.06186515
case acc: 0.06760851
case acc: 0.06781365
case acc: 0.0407706
top acc: 0.0769 ::: bot acc: 0.0159
top acc: 0.1078 ::: bot acc: 0.0500
top acc: 0.1038 ::: bot acc: 0.0228
top acc: 0.1026 ::: bot acc: 0.0334
top acc: 0.1092 ::: bot acc: 0.0276
top acc: 0.0742 ::: bot acc: 0.0145
current epoch: 19
train loss is 0.048124
average val loss: 0.040852, accuracy: 0.0403
average test loss: 0.050651, accuracy: 0.0509
case acc: 0.031231256
case acc: 0.057103153
case acc: 0.05681666
case acc: 0.061907902
case acc: 0.06140731
case acc: 0.03687112
top acc: 0.0555 ::: bot acc: 0.0128
top acc: 0.0845 ::: bot acc: 0.0283
top acc: 0.0980 ::: bot acc: 0.0193
top acc: 0.0967 ::: bot acc: 0.0283
top acc: 0.1022 ::: bot acc: 0.0224
top acc: 0.0687 ::: bot acc: 0.0141
current epoch: 20
train loss is 0.046824
average val loss: 0.030960, accuracy: 0.0305
average test loss: 0.037583, accuracy: 0.0380
case acc: 0.024160687
case acc: 0.031076755
case acc: 0.045830976
case acc: 0.047697928
case acc: 0.048351746
case acc: 0.030792713
top acc: 0.0278 ::: bot acc: 0.0373
top acc: 0.0532 ::: bot acc: 0.0131
top acc: 0.0836 ::: bot acc: 0.0154
top acc: 0.0809 ::: bot acc: 0.0175
top acc: 0.0869 ::: bot acc: 0.0140
top acc: 0.0564 ::: bot acc: 0.0206
current epoch: 21
train loss is 0.048597
average val loss: 0.033518, accuracy: 0.0341
average test loss: 0.030865, accuracy: 0.0320
case acc: 0.043870706
case acc: 0.026033636
case acc: 0.034144055
case acc: 0.027448699
case acc: 0.032039583
case acc: 0.028365752
top acc: 0.0147 ::: bot acc: 0.0763
top acc: 0.0096 ::: bot acc: 0.0503
top acc: 0.0540 ::: bot acc: 0.0369
top acc: 0.0475 ::: bot acc: 0.0232
top acc: 0.0563 ::: bot acc: 0.0262
top acc: 0.0321 ::: bot acc: 0.0448
current epoch: 22
train loss is 0.053561
average val loss: 0.067710, accuracy: 0.0684
average test loss: 0.056672, accuracy: 0.0578
case acc: 0.089379676
case acc: 0.078867294
case acc: 0.04564416
case acc: 0.04441973
case acc: 0.040317666
case acc: 0.048205987
top acc: 0.0577 ::: bot acc: 0.1231
top acc: 0.0510 ::: bot acc: 0.1087
top acc: 0.0195 ::: bot acc: 0.0820
top acc: 0.0160 ::: bot acc: 0.0760
top acc: 0.0148 ::: bot acc: 0.0733
top acc: 0.0200 ::: bot acc: 0.0807
current epoch: 23
train loss is 0.058846
average val loss: 0.079945, accuracy: 0.0803
average test loss: 0.068356, accuracy: 0.0690
case acc: 0.095192246
case acc: 0.094554044
case acc: 0.05612897
case acc: 0.06398944
case acc: 0.05269255
case acc: 0.051283322
top acc: 0.0635 ::: bot acc: 0.1290
top acc: 0.0668 ::: bot acc: 0.1243
top acc: 0.0228 ::: bot acc: 0.0960
top acc: 0.0291 ::: bot acc: 0.0986
top acc: 0.0165 ::: bot acc: 0.0908
top acc: 0.0213 ::: bot acc: 0.0846
current epoch: 24
train loss is 0.055884
average val loss: 0.059471, accuracy: 0.0599
average test loss: 0.049019, accuracy: 0.0496
case acc: 0.062523626
case acc: 0.06575445
case acc: 0.042362805
case acc: 0.051946454
case acc: 0.041308716
case acc: 0.033958588
top acc: 0.0309 ::: bot acc: 0.0962
top acc: 0.0381 ::: bot acc: 0.0954
top acc: 0.0200 ::: bot acc: 0.0769
top acc: 0.0201 ::: bot acc: 0.0851
top acc: 0.0144 ::: bot acc: 0.0749
top acc: 0.0203 ::: bot acc: 0.0593
current epoch: 25
train loss is 0.047677
average val loss: 0.047869, accuracy: 0.0479
average test loss: 0.038684, accuracy: 0.0387
case acc: 0.040122613
case acc: 0.041432478
case acc: 0.038284834
case acc: 0.04538743
case acc: 0.03643038
case acc: 0.030312376
top acc: 0.0116 ::: bot acc: 0.0723
top acc: 0.0151 ::: bot acc: 0.0704
top acc: 0.0239 ::: bot acc: 0.0690
top acc: 0.0163 ::: bot acc: 0.0773
top acc: 0.0184 ::: bot acc: 0.0657
top acc: 0.0258 ::: bot acc: 0.0512
current epoch: 26
train loss is 0.040597
average val loss: 0.038004, accuracy: 0.0376
average test loss: 0.031344, accuracy: 0.0307
case acc: 0.026423547
case acc: 0.023400916
case acc: 0.035244774
case acc: 0.037970185
case acc: 0.03234369
case acc: 0.028702568
top acc: 0.0159 ::: bot acc: 0.0497
top acc: 0.0127 ::: bot acc: 0.0447
top acc: 0.0298 ::: bot acc: 0.0615
top acc: 0.0137 ::: bot acc: 0.0676
top acc: 0.0269 ::: bot acc: 0.0555
top acc: 0.0306 ::: bot acc: 0.0462
current epoch: 27
train loss is 0.036570
average val loss: 0.030857, accuracy: 0.0306
average test loss: 0.028789, accuracy: 0.0282
case acc: 0.024894444
case acc: 0.023173517
case acc: 0.033193305
case acc: 0.030299447
case acc: 0.030052707
case acc: 0.027723514
top acc: 0.0378 ::: bot acc: 0.0278
top acc: 0.0386 ::: bot acc: 0.0187
top acc: 0.0402 ::: bot acc: 0.0510
top acc: 0.0177 ::: bot acc: 0.0542
top acc: 0.0391 ::: bot acc: 0.0433
top acc: 0.0376 ::: bot acc: 0.0391
current epoch: 28
train loss is 0.037203
average val loss: 0.029760, accuracy: 0.0305
average test loss: 0.034825, accuracy: 0.0348
case acc: 0.0373277
case acc: 0.04568834
case acc: 0.03573365
case acc: 0.026646012
case acc: 0.033433713
case acc: 0.030139852
top acc: 0.0652 ::: bot acc: 0.0117
top acc: 0.0719 ::: bot acc: 0.0191
top acc: 0.0617 ::: bot acc: 0.0295
top acc: 0.0423 ::: bot acc: 0.0287
top acc: 0.0609 ::: bot acc: 0.0216
top acc: 0.0546 ::: bot acc: 0.0222
current epoch: 29
train loss is 0.043435
average val loss: 0.045696, accuracy: 0.0460
average test loss: 0.056084, accuracy: 0.0563
case acc: 0.06258997
case acc: 0.081902966
case acc: 0.051702414
case acc: 0.0460641
case acc: 0.05229677
case acc: 0.043065745
top acc: 0.0941 ::: bot acc: 0.0295
top acc: 0.1096 ::: bot acc: 0.0523
top acc: 0.0917 ::: bot acc: 0.0168
top acc: 0.0790 ::: bot acc: 0.0166
top acc: 0.0917 ::: bot acc: 0.0161
top acc: 0.0774 ::: bot acc: 0.0148
current epoch: 30
train loss is 0.052694
average val loss: 0.059111, accuracy: 0.0591
average test loss: 0.070420, accuracy: 0.0706
case acc: 0.07035842
case acc: 0.09715197
case acc: 0.06663109
case acc: 0.06842116
case acc: 0.070194036
case acc: 0.050763857
top acc: 0.1021 ::: bot acc: 0.0369
top acc: 0.1249 ::: bot acc: 0.0675
top acc: 0.1093 ::: bot acc: 0.0262
top acc: 0.1037 ::: bot acc: 0.0340
top acc: 0.1117 ::: bot acc: 0.0299
top acc: 0.0872 ::: bot acc: 0.0181
current epoch: 31
train loss is 0.051495
average val loss: 0.040634, accuracy: 0.0405
average test loss: 0.050538, accuracy: 0.0508
case acc: 0.04042987
case acc: 0.06759779
case acc: 0.051077563
case acc: 0.055736378
case acc: 0.05586622
case acc: 0.034205455
top acc: 0.0696 ::: bot acc: 0.0123
top acc: 0.0951 ::: bot acc: 0.0383
top acc: 0.0908 ::: bot acc: 0.0166
top acc: 0.0902 ::: bot acc: 0.0230
top acc: 0.0960 ::: bot acc: 0.0183
top acc: 0.0639 ::: bot acc: 0.0154
current epoch: 32
train loss is 0.046803
average val loss: 0.028281, accuracy: 0.0284
average test loss: 0.034109, accuracy: 0.0347
case acc: 0.024593646
case acc: 0.035046596
case acc: 0.039784025
case acc: 0.039626673
case acc: 0.040845454
case acc: 0.02844577
top acc: 0.0357 ::: bot acc: 0.0298
top acc: 0.0588 ::: bot acc: 0.0136
top acc: 0.0723 ::: bot acc: 0.0203
top acc: 0.0709 ::: bot acc: 0.0135
top acc: 0.0759 ::: bot acc: 0.0136
top acc: 0.0468 ::: bot acc: 0.0299
current epoch: 33
train loss is 0.040098
average val loss: 0.028031, accuracy: 0.0282
average test loss: 0.028944, accuracy: 0.0293
case acc: 0.027293919
case acc: 0.020974746
case acc: 0.035555806
case acc: 0.030144695
case acc: 0.034029823
case acc: 0.027786674
top acc: 0.0135 ::: bot acc: 0.0522
top acc: 0.0298 ::: bot acc: 0.0277
top acc: 0.0611 ::: bot acc: 0.0300
top acc: 0.0556 ::: bot acc: 0.0159
top acc: 0.0624 ::: bot acc: 0.0205
top acc: 0.0407 ::: bot acc: 0.0360
current epoch: 34
train loss is 0.039720
average val loss: 0.039955, accuracy: 0.0407
average test loss: 0.033051, accuracy: 0.0341
case acc: 0.047193673
case acc: 0.03646184
case acc: 0.03336102
case acc: 0.026797816
case acc: 0.03024099
case acc: 0.030701503
top acc: 0.0173 ::: bot acc: 0.0800
top acc: 0.0112 ::: bot acc: 0.0650
top acc: 0.0378 ::: bot acc: 0.0533
top acc: 0.0265 ::: bot acc: 0.0443
top acc: 0.0376 ::: bot acc: 0.0450
top acc: 0.0249 ::: bot acc: 0.0523
current epoch: 35
train loss is 0.042545
average val loss: 0.059678, accuracy: 0.0602
average test loss: 0.049026, accuracy: 0.0500
case acc: 0.06701577
case acc: 0.066738375
case acc: 0.042800687
case acc: 0.044612728
case acc: 0.03942194
case acc: 0.03920661
top acc: 0.0352 ::: bot acc: 0.1007
top acc: 0.0390 ::: bot acc: 0.0964
top acc: 0.0198 ::: bot acc: 0.0777
top acc: 0.0160 ::: bot acc: 0.0763
top acc: 0.0156 ::: bot acc: 0.0716
top acc: 0.0183 ::: bot acc: 0.0682
current epoch: 36
train loss is 0.045847
average val loss: 0.059889, accuracy: 0.0602
average test loss: 0.049223, accuracy: 0.0498
case acc: 0.05850745
case acc: 0.06434037
case acc: 0.044617057
case acc: 0.052122623
case acc: 0.042405497
case acc: 0.036529254
top acc: 0.0271 ::: bot acc: 0.0920
top acc: 0.0367 ::: bot acc: 0.0939
top acc: 0.0195 ::: bot acc: 0.0805
top acc: 0.0203 ::: bot acc: 0.0854
top acc: 0.0143 ::: bot acc: 0.0767
top acc: 0.0188 ::: bot acc: 0.0639
current epoch: 37
train loss is 0.046105
average val loss: 0.039316, accuracy: 0.0393
average test loss: 0.032366, accuracy: 0.0320
case acc: 0.028671337
case acc: 0.031309403
case acc: 0.0345105
case acc: 0.037058044
case acc: 0.032409895
case acc: 0.027747082
top acc: 0.0109 ::: bot acc: 0.0555
top acc: 0.0088 ::: bot acc: 0.0583
top acc: 0.0322 ::: bot acc: 0.0591
top acc: 0.0136 ::: bot acc: 0.0663
top acc: 0.0271 ::: bot acc: 0.0555
top acc: 0.0366 ::: bot acc: 0.0402
current epoch: 38
train loss is 0.040024
average val loss: 0.028416, accuracy: 0.0284
average test loss: 0.028492, accuracy: 0.0280
case acc: 0.025691817
case acc: 0.021872059
case acc: 0.03367095
case acc: 0.027263941
case acc: 0.03032785
case acc: 0.029101769
top acc: 0.0419 ::: bot acc: 0.0237
top acc: 0.0348 ::: bot acc: 0.0224
top acc: 0.0495 ::: bot acc: 0.0417
top acc: 0.0250 ::: bot acc: 0.0460
top acc: 0.0474 ::: bot acc: 0.0352
top acc: 0.0505 ::: bot acc: 0.0263
current epoch: 39
train loss is 0.036855
average val loss: 0.029828, accuracy: 0.0302
average test loss: 0.036421, accuracy: 0.0363
case acc: 0.039196227
case acc: 0.04278916
case acc: 0.03788707
case acc: 0.02787518
case acc: 0.036467254
case acc: 0.033604935
top acc: 0.0679 ::: bot acc: 0.0120
top acc: 0.0685 ::: bot acc: 0.0172
top acc: 0.0679 ::: bot acc: 0.0236
top acc: 0.0491 ::: bot acc: 0.0220
top acc: 0.0681 ::: bot acc: 0.0162
top acc: 0.0628 ::: bot acc: 0.0160
current epoch: 40
train loss is 0.039600
average val loss: 0.040993, accuracy: 0.0413
average test loss: 0.051071, accuracy: 0.0512
case acc: 0.05476731
case acc: 0.06879325
case acc: 0.048404858
case acc: 0.04374645
case acc: 0.050543703
case acc: 0.04098564
top acc: 0.0861 ::: bot acc: 0.0222
top acc: 0.0963 ::: bot acc: 0.0396
top acc: 0.0874 ::: bot acc: 0.0157
top acc: 0.0761 ::: bot acc: 0.0153
top acc: 0.0896 ::: bot acc: 0.0151
top acc: 0.0745 ::: bot acc: 0.0143
current epoch: 41
train loss is 0.043208
average val loss: 0.046057, accuracy: 0.0461
average test loss: 0.056678, accuracy: 0.0569
case acc: 0.053468328
case acc: 0.073930755
case acc: 0.055141877
case acc: 0.056842037
case acc: 0.05969693
case acc: 0.042340796
top acc: 0.0848 ::: bot acc: 0.0211
top acc: 0.1015 ::: bot acc: 0.0445
top acc: 0.0961 ::: bot acc: 0.0184
top acc: 0.0914 ::: bot acc: 0.0238
top acc: 0.1003 ::: bot acc: 0.0212
top acc: 0.0764 ::: bot acc: 0.0147
current epoch: 42
train loss is 0.042241
average val loss: 0.033761, accuracy: 0.0337
average test loss: 0.042641, accuracy: 0.0429
case acc: 0.032167543
case acc: 0.0495022
case acc: 0.045456007
case acc: 0.04865645
case acc: 0.049589947
case acc: 0.03220876
top acc: 0.0575 ::: bot acc: 0.0121
top acc: 0.0762 ::: bot acc: 0.0219
top acc: 0.0830 ::: bot acc: 0.0156
top acc: 0.0820 ::: bot acc: 0.0180
top acc: 0.0885 ::: bot acc: 0.0146
top acc: 0.0598 ::: bot acc: 0.0177
current epoch: 43
train loss is 0.038915
average val loss: 0.027028, accuracy: 0.0271
average test loss: 0.030142, accuracy: 0.0306
case acc: 0.024447566
case acc: 0.023971794
case acc: 0.03667278
case acc: 0.034081686
case acc: 0.03652925
case acc: 0.028008929
top acc: 0.0248 ::: bot acc: 0.0407
top acc: 0.0407 ::: bot acc: 0.0169
top acc: 0.0646 ::: bot acc: 0.0265
top acc: 0.0625 ::: bot acc: 0.0135
top acc: 0.0683 ::: bot acc: 0.0162
top acc: 0.0432 ::: bot acc: 0.0336
current epoch: 44
train loss is 0.036668
average val loss: 0.035579, accuracy: 0.0361
average test loss: 0.030246, accuracy: 0.0311
case acc: 0.03908435
case acc: 0.028618181
case acc: 0.033053808
case acc: 0.025949776
case acc: 0.030188356
case acc: 0.02995142
top acc: 0.0108 ::: bot acc: 0.0710
top acc: 0.0087 ::: bot acc: 0.0544
top acc: 0.0422 ::: bot acc: 0.0489
top acc: 0.0354 ::: bot acc: 0.0355
top acc: 0.0437 ::: bot acc: 0.0390
top acc: 0.0263 ::: bot acc: 0.0505
current epoch: 45
train loss is 0.039269
average val loss: 0.053607, accuracy: 0.0542
average test loss: 0.043490, accuracy: 0.0445
case acc: 0.059248388
case acc: 0.05687649
case acc: 0.03986268
case acc: 0.03699419
case acc: 0.03627802
case acc: 0.037807554
top acc: 0.0277 ::: bot acc: 0.0928
top acc: 0.0293 ::: bot acc: 0.0864
top acc: 0.0218 ::: bot acc: 0.0723
top acc: 0.0136 ::: bot acc: 0.0662
top acc: 0.0192 ::: bot acc: 0.0651
top acc: 0.0185 ::: bot acc: 0.0660
current epoch: 46
train loss is 0.042323
average val loss: 0.050784, accuracy: 0.0511
average test loss: 0.041036, accuracy: 0.0416
case acc: 0.047961023
case acc: 0.05160855
case acc: 0.039226472
case acc: 0.040658154
case acc: 0.03691357
case acc: 0.033163793
top acc: 0.0178 ::: bot acc: 0.0809
top acc: 0.0243 ::: bot acc: 0.0810
top acc: 0.0225 ::: bot acc: 0.0710
top acc: 0.0142 ::: bot acc: 0.0714
top acc: 0.0182 ::: bot acc: 0.0666
top acc: 0.0211 ::: bot acc: 0.0578
current epoch: 47
train loss is 0.040373
average val loss: 0.035884, accuracy: 0.0359
average test loss: 0.030019, accuracy: 0.0297
case acc: 0.026690654
case acc: 0.026864462
case acc: 0.033654362
case acc: 0.031978663
case acc: 0.031186033
case acc: 0.02773241
top acc: 0.0154 ::: bot acc: 0.0504
top acc: 0.0091 ::: bot acc: 0.0516
top acc: 0.0363 ::: bot acc: 0.0550
top acc: 0.0156 ::: bot acc: 0.0578
top acc: 0.0317 ::: bot acc: 0.0509
top acc: 0.0373 ::: bot acc: 0.0394
current epoch: 48
train loss is 0.035227
average val loss: 0.027759, accuracy: 0.0278
average test loss: 0.028377, accuracy: 0.0280
case acc: 0.026081447
case acc: 0.02259377
case acc: 0.033789445
case acc: 0.026212221
case acc: 0.030502576
case acc: 0.028871698
top acc: 0.0432 ::: bot acc: 0.0225
top acc: 0.0370 ::: bot acc: 0.0201
top acc: 0.0508 ::: bot acc: 0.0404
top acc: 0.0301 ::: bot acc: 0.0409
top acc: 0.0488 ::: bot acc: 0.0338
top acc: 0.0491 ::: bot acc: 0.0276
current epoch: 49
train loss is 0.033665
average val loss: 0.030047, accuracy: 0.0305
average test loss: 0.037142, accuracy: 0.0371
case acc: 0.03964149
case acc: 0.04362756
case acc: 0.038625572
case acc: 0.029609218
case acc: 0.03741225
case acc: 0.0334832
top acc: 0.0686 ::: bot acc: 0.0121
top acc: 0.0694 ::: bot acc: 0.0178
top acc: 0.0697 ::: bot acc: 0.0221
top acc: 0.0542 ::: bot acc: 0.0172
top acc: 0.0700 ::: bot acc: 0.0153
top acc: 0.0625 ::: bot acc: 0.0160
current epoch: 50
train loss is 0.036787
average val loss: 0.039318, accuracy: 0.0396
average test loss: 0.049294, accuracy: 0.0494
case acc: 0.0508848
case acc: 0.063960485
case acc: 0.047654726
case acc: 0.045033094
case acc: 0.04962411
case acc: 0.039402105
top acc: 0.0821 ::: bot acc: 0.0189
top acc: 0.0913 ::: bot acc: 0.0350
top acc: 0.0864 ::: bot acc: 0.0155
top acc: 0.0776 ::: bot acc: 0.0161
top acc: 0.0885 ::: bot acc: 0.0147
top acc: 0.0724 ::: bot acc: 0.0139
LME_Co_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6774 6774 6774
1.7082474 -0.6288155 0.21141115 -0.19947179
Validation: 756 756 756
Testing: 768 768 768
pre-processing time: 0.00043320655822753906
the split date is 2011-07-01
net initializing with time: 0.004841327667236328
preparing training and testing date with time: 4.76837158203125e-07
current epoch: 1
train loss is 0.269090
average val loss: 0.187305, accuracy: 0.1892
average test loss: 0.187703, accuracy: 0.1892
case acc: 0.3496531
case acc: 0.2808354
case acc: 0.06078967
case acc: 0.10094236
case acc: 0.30114424
case acc: 0.041798137
top acc: 0.3829 ::: bot acc: 0.3188
top acc: 0.3115 ::: bot acc: 0.2519
top acc: 0.0263 ::: bot acc: 0.0989
top acc: 0.1271 ::: bot acc: 0.0735
top acc: 0.3330 ::: bot acc: 0.2665
top acc: 0.0172 ::: bot acc: 0.0737
current epoch: 2
train loss is 0.269362
average val loss: 0.150917, accuracy: 0.1516
average test loss: 0.150821, accuracy: 0.1507
case acc: 0.20245421
case acc: 0.13963288
case acc: 0.18835717
case acc: 0.03846806
case acc: 0.15800115
case acc: 0.17731678
top acc: 0.2361 ::: bot acc: 0.1722
top acc: 0.1687 ::: bot acc: 0.1115
top acc: 0.1453 ::: bot acc: 0.2307
top acc: 0.0155 ::: bot acc: 0.0639
top acc: 0.1905 ::: bot acc: 0.1229
top acc: 0.1375 ::: bot acc: 0.2171
current epoch: 3
train loss is 0.156393
average val loss: 0.151216, accuracy: 0.1524
average test loss: 0.151388, accuracy: 0.1522
case acc: 0.25783432
case acc: 0.20145501
case acc: 0.105324164
case acc: 0.036769733
case acc: 0.21906409
case acc: 0.09295814
top acc: 0.2916 ::: bot acc: 0.2278
top acc: 0.2302 ::: bot acc: 0.1736
top acc: 0.0626 ::: bot acc: 0.1475
top acc: 0.0585 ::: bot acc: 0.0174
top acc: 0.2517 ::: bot acc: 0.1838
top acc: 0.0564 ::: bot acc: 0.1313
current epoch: 4
train loss is 0.162302
average val loss: 0.132820, accuracy: 0.1337
average test loss: 0.132618, accuracy: 0.1326
case acc: 0.21129216
case acc: 0.16166732
case acc: 0.12068966
case acc: 0.019713243
case acc: 0.17810695
case acc: 0.103963956
top acc: 0.2451 ::: bot acc: 0.1813
top acc: 0.1903 ::: bot acc: 0.1339
top acc: 0.0775 ::: bot acc: 0.1631
top acc: 0.0297 ::: bot acc: 0.0226
top acc: 0.2109 ::: bot acc: 0.1428
top acc: 0.0661 ::: bot acc: 0.1430
current epoch: 5
train loss is 0.137662
average val loss: 0.130994, accuracy: 0.1327
average test loss: 0.131250, accuracy: 0.1326
case acc: 0.2330316
case acc: 0.19074863
case acc: 0.06788058
case acc: 0.04837075
case acc: 0.2054997
case acc: 0.050039172
top acc: 0.2668 ::: bot acc: 0.2030
top acc: 0.2194 ::: bot acc: 0.1629
top acc: 0.0305 ::: bot acc: 0.1074
top acc: 0.0719 ::: bot acc: 0.0253
top acc: 0.2384 ::: bot acc: 0.1701
top acc: 0.0204 ::: bot acc: 0.0851
current epoch: 6
train loss is 0.132499
average val loss: 0.110824, accuracy: 0.1121
average test loss: 0.110865, accuracy: 0.1115
case acc: 0.18557549
case acc: 0.15038335
case acc: 0.08228941
case acc: 0.02558299
case acc: 0.16367367
case acc: 0.061309166
top acc: 0.2194 ::: bot acc: 0.1555
top acc: 0.1788 ::: bot acc: 0.1226
top acc: 0.0421 ::: bot acc: 0.1231
top acc: 0.0431 ::: bot acc: 0.0147
top acc: 0.1967 ::: bot acc: 0.1282
top acc: 0.0285 ::: bot acc: 0.0980
current epoch: 7
train loss is 0.118310
average val loss: 0.102343, accuracy: 0.1039
average test loss: 0.102245, accuracy: 0.1035
case acc: 0.17704706
case acc: 0.14909546
case acc: 0.060735226
case acc: 0.033050776
case acc: 0.16069388
case acc: 0.04029136
top acc: 0.2108 ::: bot acc: 0.1471
top acc: 0.1774 ::: bot acc: 0.1214
top acc: 0.0252 ::: bot acc: 0.0991
top acc: 0.0537 ::: bot acc: 0.0157
top acc: 0.1938 ::: bot acc: 0.1251
top acc: 0.0175 ::: bot acc: 0.0722
current epoch: 8
train loss is 0.104464
average val loss: 0.090738, accuracy: 0.0922
average test loss: 0.090377, accuracy: 0.0918
case acc: 0.15489268
case acc: 0.13417096
case acc: 0.052784726
case acc: 0.030389275
case acc: 0.1441573
case acc: 0.03415518
top acc: 0.1887 ::: bot acc: 0.1249
top acc: 0.1624 ::: bot acc: 0.1065
top acc: 0.0203 ::: bot acc: 0.0896
top acc: 0.0502 ::: bot acc: 0.0147
top acc: 0.1774 ::: bot acc: 0.1085
top acc: 0.0218 ::: bot acc: 0.0609
current epoch: 9
train loss is 0.095321
average val loss: 0.081913, accuracy: 0.0832
average test loss: 0.081197, accuracy: 0.0830
case acc: 0.13737464
case acc: 0.12403895
case acc: 0.042748213
case acc: 0.031208761
case acc: 0.13236561
case acc: 0.03024375
top acc: 0.1712 ::: bot acc: 0.1073
top acc: 0.1521 ::: bot acc: 0.0964
top acc: 0.0184 ::: bot acc: 0.0756
top acc: 0.0512 ::: bot acc: 0.0150
top acc: 0.1657 ::: bot acc: 0.0966
top acc: 0.0354 ::: bot acc: 0.0462
current epoch: 10
train loss is 0.083292
average val loss: 0.071336, accuracy: 0.0724
average test loss: 0.070351, accuracy: 0.0721
case acc: 0.11389505
case acc: 0.108009
case acc: 0.038419113
case acc: 0.027464684
case acc: 0.11476902
case acc: 0.030269124
top acc: 0.1478 ::: bot acc: 0.0838
top acc: 0.1360 ::: bot acc: 0.0804
top acc: 0.0207 ::: bot acc: 0.0680
top acc: 0.0461 ::: bot acc: 0.0141
top acc: 0.1481 ::: bot acc: 0.0790
top acc: 0.0431 ::: bot acc: 0.0388
current epoch: 11
train loss is 0.074024
average val loss: 0.065462, accuracy: 0.0664
average test loss: 0.064294, accuracy: 0.0661
case acc: 0.09795111
case acc: 0.09964782
case acc: 0.033006884
case acc: 0.028879367
case acc: 0.10478184
case acc: 0.03233113
top acc: 0.1319 ::: bot acc: 0.0679
top acc: 0.1275 ::: bot acc: 0.0722
top acc: 0.0321 ::: bot acc: 0.0535
top acc: 0.0482 ::: bot acc: 0.0141
top acc: 0.1383 ::: bot acc: 0.0689
top acc: 0.0559 ::: bot acc: 0.0264
current epoch: 12
train loss is 0.065554
average val loss: 0.056186, accuracy: 0.0570
average test loss: 0.054842, accuracy: 0.0563
case acc: 0.07499278
case acc: 0.0844056
case acc: 0.031763542
case acc: 0.025163094
case acc: 0.08798665
case acc: 0.033470456
top acc: 0.1089 ::: bot acc: 0.0449
top acc: 0.1121 ::: bot acc: 0.0570
top acc: 0.0388 ::: bot acc: 0.0470
top acc: 0.0428 ::: bot acc: 0.0139
top acc: 0.1216 ::: bot acc: 0.0520
top acc: 0.0591 ::: bot acc: 0.0239
current epoch: 13
train loss is 0.056914
average val loss: 0.050960, accuracy: 0.0515
average test loss: 0.049593, accuracy: 0.0506
case acc: 0.058299385
case acc: 0.075323924
case acc: 0.032028385
case acc: 0.024964985
case acc: 0.07725404
case acc: 0.03584754
top acc: 0.0923 ::: bot acc: 0.0283
top acc: 0.1028 ::: bot acc: 0.0480
top acc: 0.0503 ::: bot acc: 0.0358
top acc: 0.0426 ::: bot acc: 0.0136
top acc: 0.1110 ::: bot acc: 0.0412
top acc: 0.0644 ::: bot acc: 0.0206
current epoch: 14
train loss is 0.049980
average val loss: 0.046054, accuracy: 0.0463
average test loss: 0.044688, accuracy: 0.0450
case acc: 0.042887233
case acc: 0.0666621
case acc: 0.033773493
case acc: 0.024102747
case acc: 0.06708189
case acc: 0.035759006
top acc: 0.0765 ::: bot acc: 0.0139
top acc: 0.0939 ::: bot acc: 0.0395
top acc: 0.0599 ::: bot acc: 0.0267
top acc: 0.0414 ::: bot acc: 0.0133
top acc: 0.1007 ::: bot acc: 0.0316
top acc: 0.0644 ::: bot acc: 0.0206
current epoch: 15
train loss is 0.043509
average val loss: 0.042665, accuracy: 0.0428
average test loss: 0.041289, accuracy: 0.0412
case acc: 0.03317835
case acc: 0.06011928
case acc: 0.03672734
case acc: 0.023700489
case acc: 0.05905225
case acc: 0.034423508
top acc: 0.0640 ::: bot acc: 0.0101
top acc: 0.0871 ::: bot acc: 0.0331
top acc: 0.0690 ::: bot acc: 0.0185
top acc: 0.0408 ::: bot acc: 0.0130
top acc: 0.0921 ::: bot acc: 0.0249
top acc: 0.0615 ::: bot acc: 0.0225
current epoch: 16
train loss is 0.038707
average val loss: 0.042048, accuracy: 0.0422
average test loss: 0.040745, accuracy: 0.0404
case acc: 0.02889358
case acc: 0.057506967
case acc: 0.04217599
case acc: 0.025102533
case acc: 0.054833867
case acc: 0.03364128
top acc: 0.0573 ::: bot acc: 0.0109
top acc: 0.0844 ::: bot acc: 0.0306
top acc: 0.0793 ::: bot acc: 0.0141
top acc: 0.0430 ::: bot acc: 0.0126
top acc: 0.0877 ::: bot acc: 0.0215
top acc: 0.0597 ::: bot acc: 0.0237
current epoch: 17
train loss is 0.035594
average val loss: 0.046014, accuracy: 0.0462
average test loss: 0.044981, accuracy: 0.0445
case acc: 0.029672982
case acc: 0.06134496
case acc: 0.05229392
case acc: 0.031185925
case acc: 0.05706121
case acc: 0.035597797
top acc: 0.0588 ::: bot acc: 0.0106
top acc: 0.0882 ::: bot acc: 0.0345
top acc: 0.0937 ::: bot acc: 0.0155
top acc: 0.0511 ::: bot acc: 0.0143
top acc: 0.0902 ::: bot acc: 0.0231
top acc: 0.0643 ::: bot acc: 0.0204
current epoch: 18
train loss is 0.037018
average val loss: 0.057782, accuracy: 0.0580
average test loss: 0.057267, accuracy: 0.0570
case acc: 0.03680008
case acc: 0.07422812
case acc: 0.070651926
case acc: 0.046040498
case acc: 0.06884856
case acc: 0.045294363
top acc: 0.0695 ::: bot acc: 0.0105
top acc: 0.1011 ::: bot acc: 0.0474
top acc: 0.1150 ::: bot acc: 0.0280
top acc: 0.0686 ::: bot acc: 0.0237
top acc: 0.1031 ::: bot acc: 0.0328
top acc: 0.0791 ::: bot acc: 0.0198
current epoch: 19
train loss is 0.041524
average val loss: 0.066946, accuracy: 0.0670
average test loss: 0.066694, accuracy: 0.0665
case acc: 0.040223684
case acc: 0.08271478
case acc: 0.08537746
case acc: 0.05866799
case acc: 0.078262344
case acc: 0.0536113
top acc: 0.0738 ::: bot acc: 0.0121
top acc: 0.1095 ::: bot acc: 0.0560
top acc: 0.1300 ::: bot acc: 0.0422
top acc: 0.0822 ::: bot acc: 0.0342
top acc: 0.1128 ::: bot acc: 0.0418
top acc: 0.0898 ::: bot acc: 0.0233
current epoch: 20
train loss is 0.048337
average val loss: 0.066745, accuracy: 0.0667
average test loss: 0.066495, accuracy: 0.0662
case acc: 0.0346189
case acc: 0.08052704
case acc: 0.086664245
case acc: 0.061790477
case acc: 0.078830786
case acc: 0.054947756
top acc: 0.0667 ::: bot acc: 0.0098
top acc: 0.1073 ::: bot acc: 0.0538
top acc: 0.1313 ::: bot acc: 0.0435
top acc: 0.0855 ::: bot acc: 0.0370
top acc: 0.1134 ::: bot acc: 0.0423
top acc: 0.0914 ::: bot acc: 0.0240
current epoch: 21
train loss is 0.053041
average val loss: 0.049249, accuracy: 0.0497
average test loss: 0.048498, accuracy: 0.0481
case acc: 0.02268729
case acc: 0.056259394
case acc: 0.06257683
case acc: 0.04407625
case acc: 0.06016907
case acc: 0.04301148
top acc: 0.0386 ::: bot acc: 0.0255
top acc: 0.0830 ::: bot acc: 0.0295
top acc: 0.1063 ::: bot acc: 0.0214
top acc: 0.0665 ::: bot acc: 0.0221
top acc: 0.0939 ::: bot acc: 0.0254
top acc: 0.0759 ::: bot acc: 0.0196
current epoch: 22
train loss is 0.055155
average val loss: 0.031729, accuracy: 0.0325
average test loss: 0.029622, accuracy: 0.0300
case acc: 0.047522143
case acc: 0.020774297
case acc: 0.032985374
case acc: 0.017937727
case acc: 0.030415414
case acc: 0.030247122
top acc: 0.0232 ::: bot acc: 0.0723
top acc: 0.0355 ::: bot acc: 0.0181
top acc: 0.0538 ::: bot acc: 0.0342
top acc: 0.0223 ::: bot acc: 0.0271
top acc: 0.0520 ::: bot acc: 0.0204
top acc: 0.0426 ::: bot acc: 0.0396
current epoch: 23
train loss is 0.054521
average val loss: 0.045337, accuracy: 0.0462
average test loss: 0.044625, accuracy: 0.0448
case acc: 0.076898634
case acc: 0.035494983
case acc: 0.047739822
case acc: 0.04370831
case acc: 0.029328354
case acc: 0.035674684
top acc: 0.0458 ::: bot acc: 0.1050
top acc: 0.0143 ::: bot acc: 0.0596
top acc: 0.0182 ::: bot acc: 0.0836
top acc: 0.0207 ::: bot acc: 0.0685
top acc: 0.0161 ::: bot acc: 0.0554
top acc: 0.0198 ::: bot acc: 0.0641
current epoch: 24
train loss is 0.042651
average val loss: 0.048033, accuracy: 0.0486
average test loss: 0.047455, accuracy: 0.0478
case acc: 0.072316214
case acc: 0.040936425
case acc: 0.058232203
case acc: 0.051893607
case acc: 0.03114495
case acc: 0.032242812
top acc: 0.0420 ::: bot acc: 0.1001
top acc: 0.0174 ::: bot acc: 0.0663
top acc: 0.0226 ::: bot acc: 0.0970
top acc: 0.0283 ::: bot acc: 0.0769
top acc: 0.0142 ::: bot acc: 0.0590
top acc: 0.0271 ::: bot acc: 0.0552
current epoch: 25
train loss is 0.043009
average val loss: 0.060436, accuracy: 0.0608
average test loss: 0.060364, accuracy: 0.0605
case acc: 0.07769318
case acc: 0.056179915
case acc: 0.07843652
case acc: 0.07204805
case acc: 0.042664997
case acc: 0.0361894
top acc: 0.0465 ::: bot acc: 0.1058
top acc: 0.0299 ::: bot acc: 0.0828
top acc: 0.0372 ::: bot acc: 0.1200
top acc: 0.0480 ::: bot acc: 0.0973
top acc: 0.0162 ::: bot acc: 0.0752
top acc: 0.0192 ::: bot acc: 0.0652
current epoch: 26
train loss is 0.065090
average val loss: 0.034158, accuracy: 0.0339
average test loss: 0.032526, accuracy: 0.0330
case acc: 0.035716064
case acc: 0.022963922
case acc: 0.045650527
case acc: 0.036738846
case acc: 0.025335506
case acc: 0.031333763
top acc: 0.0202 ::: bot acc: 0.0562
top acc: 0.0132 ::: bot acc: 0.0415
top acc: 0.0178 ::: bot acc: 0.0806
top acc: 0.0146 ::: bot acc: 0.0612
top acc: 0.0336 ::: bot acc: 0.0374
top acc: 0.0539 ::: bot acc: 0.0284
current epoch: 27
train loss is 0.059364
average val loss: 0.033261, accuracy: 0.0334
average test loss: 0.031450, accuracy: 0.0313
case acc: 0.026410982
case acc: 0.028696809
case acc: 0.032326713
case acc: 0.01791205
case acc: 0.03964728
case acc: 0.042675637
top acc: 0.0532 ::: bot acc: 0.0126
top acc: 0.0525 ::: bot acc: 0.0080
top acc: 0.0479 ::: bot acc: 0.0401
top acc: 0.0236 ::: bot acc: 0.0258
top acc: 0.0681 ::: bot acc: 0.0156
top acc: 0.0751 ::: bot acc: 0.0201
current epoch: 28
train loss is 0.043178
average val loss: 0.033056, accuracy: 0.0333
average test loss: 0.031253, accuracy: 0.0310
case acc: 0.029046107
case acc: 0.032545127
case acc: 0.033305608
case acc: 0.018143866
case acc: 0.039494436
case acc: 0.033215232
top acc: 0.0582 ::: bot acc: 0.0104
top acc: 0.0577 ::: bot acc: 0.0091
top acc: 0.0565 ::: bot acc: 0.0315
top acc: 0.0264 ::: bot acc: 0.0230
top acc: 0.0678 ::: bot acc: 0.0156
top acc: 0.0595 ::: bot acc: 0.0232
current epoch: 29
train loss is 0.036248
average val loss: 0.036809, accuracy: 0.0369
average test loss: 0.035401, accuracy: 0.0349
case acc: 0.034008816
case acc: 0.041475262
case acc: 0.038469903
case acc: 0.021065451
case acc: 0.043253247
case acc: 0.031151567
top acc: 0.0660 ::: bot acc: 0.0095
top acc: 0.0679 ::: bot acc: 0.0155
top acc: 0.0727 ::: bot acc: 0.0167
top acc: 0.0363 ::: bot acc: 0.0137
top acc: 0.0733 ::: bot acc: 0.0158
top acc: 0.0533 ::: bot acc: 0.0288
current epoch: 30
train loss is 0.032982
average val loss: 0.047878, accuracy: 0.0481
average test loss: 0.047303, accuracy: 0.0470
case acc: 0.04445795
case acc: 0.058186382
case acc: 0.056226198
case acc: 0.03468304
case acc: 0.054078907
case acc: 0.034293547
top acc: 0.0788 ::: bot acc: 0.0151
top acc: 0.0850 ::: bot acc: 0.0314
top acc: 0.0989 ::: bot acc: 0.0173
top acc: 0.0557 ::: bot acc: 0.0156
top acc: 0.0870 ::: bot acc: 0.0208
top acc: 0.0618 ::: bot acc: 0.0218
current epoch: 31
train loss is 0.035711
average val loss: 0.065337, accuracy: 0.0655
average test loss: 0.065338, accuracy: 0.0655
case acc: 0.057155002
case acc: 0.07778021
case acc: 0.08418078
case acc: 0.05658632
case acc: 0.07091502
case acc: 0.046365798
top acc: 0.0917 ::: bot acc: 0.0274
top acc: 0.1046 ::: bot acc: 0.0509
top acc: 0.1289 ::: bot acc: 0.0409
top acc: 0.0801 ::: bot acc: 0.0323
top acc: 0.1052 ::: bot acc: 0.0348
top acc: 0.0802 ::: bot acc: 0.0208
current epoch: 32
train loss is 0.051180
average val loss: 0.057871, accuracy: 0.0580
average test loss: 0.057679, accuracy: 0.0575
case acc: 0.039951865
case acc: 0.06727569
case acc: 0.07978047
case acc: 0.052540176
case acc: 0.06291275
case acc: 0.042513352
top acc: 0.0738 ::: bot acc: 0.0117
top acc: 0.0941 ::: bot acc: 0.0404
top acc: 0.1245 ::: bot acc: 0.0365
top acc: 0.0757 ::: bot acc: 0.0289
top acc: 0.0968 ::: bot acc: 0.0277
top acc: 0.0748 ::: bot acc: 0.0201
current epoch: 33
train loss is 0.054995
average val loss: 0.032369, accuracy: 0.0326
average test loss: 0.030477, accuracy: 0.0301
case acc: 0.026517797
case acc: 0.026920725
case acc: 0.042421333
case acc: 0.021262001
case acc: 0.03335328
case acc: 0.030239208
top acc: 0.0263 ::: bot acc: 0.0395
top acc: 0.0498 ::: bot acc: 0.0082
top acc: 0.0796 ::: bot acc: 0.0147
top acc: 0.0368 ::: bot acc: 0.0134
top acc: 0.0577 ::: bot acc: 0.0175
top acc: 0.0452 ::: bot acc: 0.0368
current epoch: 34
train loss is 0.045773
average val loss: 0.029905, accuracy: 0.0302
average test loss: 0.027597, accuracy: 0.0278
case acc: 0.03961558
case acc: 0.019538082
case acc: 0.032424938
case acc: 0.019465927
case acc: 0.025712121
case acc: 0.030259412
top acc: 0.0203 ::: bot acc: 0.0619
top acc: 0.0244 ::: bot acc: 0.0293
top acc: 0.0487 ::: bot acc: 0.0394
top acc: 0.0135 ::: bot acc: 0.0359
top acc: 0.0377 ::: bot acc: 0.0331
top acc: 0.0376 ::: bot acc: 0.0444
current epoch: 35
train loss is 0.038541
average val loss: 0.031785, accuracy: 0.0320
average test loss: 0.029866, accuracy: 0.0303
case acc: 0.042539448
case acc: 0.022883134
case acc: 0.03491045
case acc: 0.026051797
case acc: 0.02527257
case acc: 0.030103922
top acc: 0.0211 ::: bot acc: 0.0658
top acc: 0.0133 ::: bot acc: 0.0413
top acc: 0.0288 ::: bot acc: 0.0593
top acc: 0.0080 ::: bot acc: 0.0485
top acc: 0.0298 ::: bot acc: 0.0410
top acc: 0.0407 ::: bot acc: 0.0413
current epoch: 36
train loss is 0.032680
average val loss: 0.036832, accuracy: 0.0369
average test loss: 0.035457, accuracy: 0.0360
case acc: 0.04491709
case acc: 0.030483313
case acc: 0.04469619
case acc: 0.038258307
case acc: 0.0274909
case acc: 0.030303426
top acc: 0.0221 ::: bot acc: 0.0689
top acc: 0.0118 ::: bot acc: 0.0533
top acc: 0.0177 ::: bot acc: 0.0794
top acc: 0.0159 ::: bot acc: 0.0628
top acc: 0.0201 ::: bot acc: 0.0507
top acc: 0.0374 ::: bot acc: 0.0446
current epoch: 37
train loss is 0.034947
average val loss: 0.045104, accuracy: 0.0451
average test loss: 0.044317, accuracy: 0.0447
case acc: 0.047912262
case acc: 0.040603135
case acc: 0.059663057
case acc: 0.054049484
case acc: 0.03369551
case acc: 0.03245277
top acc: 0.0236 ::: bot acc: 0.0726
top acc: 0.0171 ::: bot acc: 0.0658
top acc: 0.0236 ::: bot acc: 0.0988
top acc: 0.0303 ::: bot acc: 0.0792
top acc: 0.0134 ::: bot acc: 0.0632
top acc: 0.0259 ::: bot acc: 0.0562
current epoch: 38
train loss is 0.048144
average val loss: 0.032162, accuracy: 0.0318
average test loss: 0.030241, accuracy: 0.0306
case acc: 0.02694753
case acc: 0.022780906
case acc: 0.043087184
case acc: 0.035156228
case acc: 0.025298849
case acc: 0.030192738
top acc: 0.0257 ::: bot acc: 0.0403
top acc: 0.0134 ::: bot acc: 0.0411
top acc: 0.0181 ::: bot acc: 0.0768
top acc: 0.0134 ::: bot acc: 0.0594
top acc: 0.0291 ::: bot acc: 0.0417
top acc: 0.0436 ::: bot acc: 0.0385
current epoch: 39
train loss is 0.047828
average val loss: 0.032243, accuracy: 0.0323
average test loss: 0.030376, accuracy: 0.0302
case acc: 0.033244368
case acc: 0.02678759
case acc: 0.032502145
case acc: 0.01786787
case acc: 0.035134546
case acc: 0.035627518
top acc: 0.0650 ::: bot acc: 0.0094
top acc: 0.0496 ::: bot acc: 0.0082
top acc: 0.0496 ::: bot acc: 0.0386
top acc: 0.0229 ::: bot acc: 0.0265
top acc: 0.0608 ::: bot acc: 0.0166
top acc: 0.0641 ::: bot acc: 0.0209
current epoch: 40
train loss is 0.042234
average val loss: 0.042211, accuracy: 0.0421
average test loss: 0.041425, accuracy: 0.0412
case acc: 0.050246123
case acc: 0.045494355
case acc: 0.041423373
case acc: 0.025988799
case acc: 0.046798877
case acc: 0.03751655
top acc: 0.0848 ::: bot acc: 0.0206
top acc: 0.0721 ::: bot acc: 0.0191
top acc: 0.0778 ::: bot acc: 0.0151
top acc: 0.0447 ::: bot acc: 0.0116
top acc: 0.0782 ::: bot acc: 0.0166
top acc: 0.0673 ::: bot acc: 0.0202
current epoch: 41
train loss is 0.037378
average val loss: 0.054930, accuracy: 0.0552
average test loss: 0.054755, accuracy: 0.0549
case acc: 0.06081266
case acc: 0.063613996
case acc: 0.062268004
case acc: 0.043413278
case acc: 0.059298422
case acc: 0.03992509
top acc: 0.0954 ::: bot acc: 0.0311
top acc: 0.0905 ::: bot acc: 0.0368
top acc: 0.1058 ::: bot acc: 0.0213
top acc: 0.0658 ::: bot acc: 0.0214
top acc: 0.0928 ::: bot acc: 0.0247
top acc: 0.0710 ::: bot acc: 0.0199
current epoch: 42
train loss is 0.040197
average val loss: 0.067672, accuracy: 0.0678
average test loss: 0.067790, accuracy: 0.0681
case acc: 0.06548954
case acc: 0.077445604
case acc: 0.0853686
case acc: 0.06145656
case acc: 0.0714819
case acc: 0.0471346
top acc: 0.1001 ::: bot acc: 0.0358
top acc: 0.1043 ::: bot acc: 0.0506
top acc: 0.1301 ::: bot acc: 0.0420
top acc: 0.0853 ::: bot acc: 0.0365
top acc: 0.1057 ::: bot acc: 0.0354
top acc: 0.0811 ::: bot acc: 0.0211
current epoch: 43
train loss is 0.054779
average val loss: 0.037556, accuracy: 0.0379
average test loss: 0.036248, accuracy: 0.0357
case acc: 0.025657544
case acc: 0.037318088
case acc: 0.050843738
case acc: 0.030153269
case acc: 0.03998422
case acc: 0.030404162
top acc: 0.0519 ::: bot acc: 0.0131
top acc: 0.0634 ::: bot acc: 0.0120
top acc: 0.0917 ::: bot acc: 0.0154
top acc: 0.0502 ::: bot acc: 0.0129
top acc: 0.0684 ::: bot acc: 0.0157
top acc: 0.0480 ::: bot acc: 0.0340
current epoch: 44
train loss is 0.043879
average val loss: 0.029942, accuracy: 0.0301
average test loss: 0.027722, accuracy: 0.0275
case acc: 0.025173776
case acc: 0.022905543
case acc: 0.036660127
case acc: 0.01977524
case acc: 0.030468818
case acc: 0.030155685
top acc: 0.0283 ::: bot acc: 0.0364
top acc: 0.0417 ::: bot acc: 0.0125
top acc: 0.0685 ::: bot acc: 0.0196
top acc: 0.0331 ::: bot acc: 0.0163
top acc: 0.0520 ::: bot acc: 0.0203
top acc: 0.0443 ::: bot acc: 0.0377
current epoch: 45
train loss is 0.036706
average val loss: 0.028475, accuracy: 0.0286
average test loss: 0.025942, accuracy: 0.0263
case acc: 0.031160533
case acc: 0.019498114
case acc: 0.032362606
case acc: 0.018087558
case acc: 0.026481412
case acc: 0.030230744
top acc: 0.0219 ::: bot acc: 0.0485
top acc: 0.0268 ::: bot acc: 0.0269
top acc: 0.0485 ::: bot acc: 0.0396
top acc: 0.0199 ::: bot acc: 0.0295
top acc: 0.0418 ::: bot acc: 0.0289
top acc: 0.0461 ::: bot acc: 0.0358
current epoch: 46
train loss is 0.031870
average val loss: 0.030629, accuracy: 0.0306
average test loss: 0.028616, accuracy: 0.0290
case acc: 0.036453657
case acc: 0.023098338
case acc: 0.035626743
case acc: 0.02377036
case acc: 0.02516627
case acc: 0.030063927
top acc: 0.0202 ::: bot acc: 0.0572
top acc: 0.0130 ::: bot acc: 0.0418
top acc: 0.0265 ::: bot acc: 0.0616
top acc: 0.0082 ::: bot acc: 0.0450
top acc: 0.0303 ::: bot acc: 0.0404
top acc: 0.0428 ::: bot acc: 0.0391
current epoch: 47
train loss is 0.030223
average val loss: 0.036375, accuracy: 0.0363
average test loss: 0.034974, accuracy: 0.0355
case acc: 0.040164866
case acc: 0.031518057
case acc: 0.046382345
case acc: 0.036307346
case acc: 0.02794275
case acc: 0.03048975
top acc: 0.0206 ::: bot acc: 0.0625
top acc: 0.0121 ::: bot acc: 0.0548
top acc: 0.0179 ::: bot acc: 0.0818
top acc: 0.0143 ::: bot acc: 0.0607
top acc: 0.0187 ::: bot acc: 0.0521
top acc: 0.0346 ::: bot acc: 0.0473
current epoch: 48
train loss is 0.035774
average val loss: 0.037599, accuracy: 0.0373
average test loss: 0.036294, accuracy: 0.0367
case acc: 0.035633083
case acc: 0.03246646
case acc: 0.05103894
case acc: 0.041491322
case acc: 0.028906433
case acc: 0.03094835
top acc: 0.0202 ::: bot acc: 0.0559
top acc: 0.0125 ::: bot acc: 0.0560
top acc: 0.0189 ::: bot acc: 0.0883
top acc: 0.0187 ::: bot acc: 0.0662
top acc: 0.0165 ::: bot acc: 0.0546
top acc: 0.0313 ::: bot acc: 0.0506
current epoch: 49
train loss is 0.043601
average val loss: 0.028838, accuracy: 0.0285
average test loss: 0.026395, accuracy: 0.0262
case acc: 0.022839349
case acc: 0.019443892
case acc: 0.03552092
case acc: 0.022575883
case acc: 0.026082162
case acc: 0.030506007
top acc: 0.0426 ::: bot acc: 0.0217
top acc: 0.0256 ::: bot acc: 0.0281
top acc: 0.0268 ::: bot acc: 0.0613
top acc: 0.0087 ::: bot acc: 0.0430
top acc: 0.0400 ::: bot acc: 0.0307
top acc: 0.0494 ::: bot acc: 0.0326
current epoch: 50
train loss is 0.040718
average val loss: 0.034671, accuracy: 0.0347
average test loss: 0.033144, accuracy: 0.0329
case acc: 0.040165395
case acc: 0.030827295
case acc: 0.034136415
case acc: 0.019938178
case acc: 0.037705887
case acc: 0.034818955
top acc: 0.0741 ::: bot acc: 0.0119
top acc: 0.0556 ::: bot acc: 0.0083
top acc: 0.0599 ::: bot acc: 0.0281
top acc: 0.0336 ::: bot acc: 0.0157
top acc: 0.0649 ::: bot acc: 0.0159
top acc: 0.0627 ::: bot acc: 0.0213

		{"drop_out": 0.2, "drop_out_mc": 0.1, "repeat_mc": 50, "hidden": 20, "embedding_size": 5, "batch": 512, "lag": 4}
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
LME_Co_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6768 6768 6768
1.8562728 -0.6288155 0.2585643 -0.19947179
Validation: 756 756 756
Testing: 774 774 774
pre-processing time: 0.0005004405975341797
the split date is 2009-07-01
net initializing with time: 0.00725865364074707
preparing training and testing date with time: 4.76837158203125e-07
current epoch: 1
train loss is 0.404522
average val loss: 0.191110, accuracy: 0.1908
average test loss: 0.184970, accuracy: 0.1851
case acc: 0.12305199
case acc: 0.2678261
case acc: 0.15785517
case acc: 0.024398653
case acc: 0.33928204
case acc: 0.19818376
top acc: 0.1400 ::: bot acc: 0.1063
top acc: 0.2938 ::: bot acc: 0.2435
top acc: 0.1950 ::: bot acc: 0.1239
top acc: 0.0439 ::: bot acc: 0.0133
top acc: 0.3643 ::: bot acc: 0.3112
top acc: 0.2189 ::: bot acc: 0.1751
current epoch: 2
train loss is 0.141032
average val loss: 0.091625, accuracy: 0.0932
average test loss: 0.089410, accuracy: 0.0902
case acc: 0.02239996
case acc: 0.12707722
case acc: 0.028668854
case acc: 0.12732421
case acc: 0.18442746
case acc: 0.051358055
top acc: 0.0091 ::: bot acc: 0.0373
top acc: 0.1536 ::: bot acc: 0.1025
top acc: 0.0539 ::: bot acc: 0.0181
top acc: 0.1005 ::: bot acc: 0.1535
top acc: 0.2092 ::: bot acc: 0.1566
top acc: 0.0720 ::: bot acc: 0.0290
current epoch: 3
train loss is 0.116686
average val loss: 0.087717, accuracy: 0.0878
average test loss: 0.089695, accuracy: 0.0913
case acc: 0.109791234
case acc: 0.030799154
case acc: 0.07054165
case acc: 0.20638041
case acc: 0.096151836
case acc: 0.034208823
top acc: 0.0929 ::: bot acc: 0.1264
top acc: 0.0554 ::: bot acc: 0.0101
top acc: 0.0334 ::: bot acc: 0.1051
top acc: 0.1797 ::: bot acc: 0.2326
top acc: 0.1208 ::: bot acc: 0.0687
top acc: 0.0143 ::: bot acc: 0.0567
current epoch: 4
train loss is 0.117574
average val loss: 0.088757, accuracy: 0.0899
average test loss: 0.085897, accuracy: 0.0860
case acc: 0.016228085
case acc: 0.11957032
case acc: 0.033754144
case acc: 0.10477626
case acc: 0.17893651
case acc: 0.062437907
top acc: 0.0078 ::: bot acc: 0.0284
top acc: 0.1466 ::: bot acc: 0.0942
top acc: 0.0648 ::: bot acc: 0.0115
top acc: 0.0783 ::: bot acc: 0.1309
top acc: 0.2034 ::: bot acc: 0.1517
top acc: 0.0832 ::: bot acc: 0.0398
current epoch: 5
train loss is 0.093461
average val loss: 0.070788, accuracy: 0.0725
average test loss: 0.071022, accuracy: 0.0731
case acc: 0.0695524
case acc: 0.05557446
case acc: 0.03590413
case acc: 0.15831287
case acc: 0.102893695
case acc: 0.016271325
top acc: 0.0524 ::: bot acc: 0.0861
top acc: 0.0826 ::: bot acc: 0.0302
top acc: 0.0152 ::: bot acc: 0.0623
top acc: 0.1319 ::: bot acc: 0.1844
top acc: 0.1272 ::: bot acc: 0.0757
top acc: 0.0224 ::: bot acc: 0.0211
current epoch: 6
train loss is 0.090090
average val loss: 0.065596, accuracy: 0.0676
average test loss: 0.065755, accuracy: 0.0678
case acc: 0.06765465
case acc: 0.048903707
case acc: 0.033946376
case acc: 0.15271907
case acc: 0.08757238
case acc: 0.016073838
top acc: 0.0506 ::: bot acc: 0.0841
top acc: 0.0758 ::: bot acc: 0.0238
top acc: 0.0163 ::: bot acc: 0.0590
top acc: 0.1265 ::: bot acc: 0.1787
top acc: 0.1118 ::: bot acc: 0.0605
top acc: 0.0220 ::: bot acc: 0.0213
current epoch: 7
train loss is 0.092213
average val loss: 0.064629, accuracy: 0.0667
average test loss: 0.062775, accuracy: 0.0646
case acc: 0.037752893
case acc: 0.07362243
case acc: 0.026822858
case acc: 0.12113846
case acc: 0.09796282
case acc: 0.030143013
top acc: 0.0208 ::: bot acc: 0.0541
top acc: 0.1009 ::: bot acc: 0.0479
top acc: 0.0443 ::: bot acc: 0.0283
top acc: 0.0950 ::: bot acc: 0.1470
top acc: 0.1222 ::: bot acc: 0.0711
top acc: 0.0475 ::: bot acc: 0.0144
current epoch: 8
train loss is 0.081919
average val loss: 0.057528, accuracy: 0.0600
average test loss: 0.056450, accuracy: 0.0585
case acc: 0.049593028
case acc: 0.057509948
case acc: 0.02718168
case acc: 0.13109072
case acc: 0.066824876
case acc: 0.019097734
top acc: 0.0326 ::: bot acc: 0.0660
top acc: 0.0848 ::: bot acc: 0.0318
top acc: 0.0341 ::: bot acc: 0.0385
top acc: 0.1051 ::: bot acc: 0.1570
top acc: 0.0910 ::: bot acc: 0.0399
top acc: 0.0311 ::: bot acc: 0.0140
current epoch: 9
train loss is 0.075938
average val loss: 0.052774, accuracy: 0.0553
average test loss: 0.052053, accuracy: 0.0540
case acc: 0.05238883
case acc: 0.04919679
case acc: 0.027413454
case acc: 0.13134357
case acc: 0.047185544
case acc: 0.016550116
top acc: 0.0354 ::: bot acc: 0.0688
top acc: 0.0763 ::: bot acc: 0.0239
top acc: 0.0331 ::: bot acc: 0.0397
top acc: 0.1054 ::: bot acc: 0.1572
top acc: 0.0707 ::: bot acc: 0.0216
top acc: 0.0248 ::: bot acc: 0.0186
current epoch: 10
train loss is 0.073939
average val loss: 0.049944, accuracy: 0.0523
average test loss: 0.048655, accuracy: 0.0502
case acc: 0.04405364
case acc: 0.052409586
case acc: 0.026742013
case acc: 0.12066822
case acc: 0.039289217
case acc: 0.018334625
top acc: 0.0270 ::: bot acc: 0.0605
top acc: 0.0797 ::: bot acc: 0.0269
top acc: 0.0428 ::: bot acc: 0.0300
top acc: 0.0948 ::: bot acc: 0.1465
top acc: 0.0620 ::: bot acc: 0.0154
top acc: 0.0295 ::: bot acc: 0.0147
current epoch: 11
train loss is 0.068584
average val loss: 0.047239, accuracy: 0.0493
average test loss: 0.045790, accuracy: 0.0470
case acc: 0.03924865
case acc: 0.05237035
case acc: 0.027517451
case acc: 0.11310156
case acc: 0.030803075
case acc: 0.019091327
top acc: 0.0222 ::: bot acc: 0.0557
top acc: 0.0797 ::: bot acc: 0.0267
top acc: 0.0488 ::: bot acc: 0.0241
top acc: 0.0874 ::: bot acc: 0.1388
top acc: 0.0512 ::: bot acc: 0.0114
top acc: 0.0311 ::: bot acc: 0.0139
current epoch: 12
train loss is 0.064884
average val loss: 0.045469, accuracy: 0.0471
average test loss: 0.043767, accuracy: 0.0446
case acc: 0.033014335
case acc: 0.05326857
case acc: 0.029975675
case acc: 0.102791026
case acc: 0.027043143
case acc: 0.021468524
top acc: 0.0164 ::: bot acc: 0.0492
top acc: 0.0807 ::: bot acc: 0.0275
top acc: 0.0565 ::: bot acc: 0.0169
top acc: 0.0772 ::: bot acc: 0.1285
top acc: 0.0455 ::: bot acc: 0.0117
top acc: 0.0352 ::: bot acc: 0.0127
current epoch: 13
train loss is 0.062108
average val loss: 0.044360, accuracy: 0.0457
average test loss: 0.042337, accuracy: 0.0427
case acc: 0.02713748
case acc: 0.053241532
case acc: 0.033476446
case acc: 0.09089854
case acc: 0.026307482
case acc: 0.025029378
top acc: 0.0114 ::: bot acc: 0.0429
top acc: 0.0807 ::: bot acc: 0.0274
top acc: 0.0643 ::: bot acc: 0.0118
top acc: 0.0653 ::: bot acc: 0.1166
top acc: 0.0442 ::: bot acc: 0.0121
top acc: 0.0405 ::: bot acc: 0.0128
current epoch: 14
train loss is 0.059945
average val loss: 0.043533, accuracy: 0.0446
average test loss: 0.041184, accuracy: 0.0412
case acc: 0.022987902
case acc: 0.05170142
case acc: 0.03787876
case acc: 0.07886369
case acc: 0.02720585
case acc: 0.028720878
top acc: 0.0089 ::: bot acc: 0.0379
top acc: 0.0791 ::: bot acc: 0.0260
top acc: 0.0711 ::: bot acc: 0.0115
top acc: 0.0532 ::: bot acc: 0.1046
top acc: 0.0458 ::: bot acc: 0.0117
top acc: 0.0455 ::: bot acc: 0.0141
current epoch: 15
train loss is 0.057257
average val loss: 0.041267, accuracy: 0.0422
average test loss: 0.038939, accuracy: 0.0389
case acc: 0.023610724
case acc: 0.044342197
case acc: 0.038672697
case acc: 0.07153604
case acc: 0.026836181
case acc: 0.028672755
top acc: 0.0092 ::: bot acc: 0.0388
top acc: 0.0714 ::: bot acc: 0.0192
top acc: 0.0722 ::: bot acc: 0.0117
top acc: 0.0460 ::: bot acc: 0.0972
top acc: 0.0451 ::: bot acc: 0.0119
top acc: 0.0453 ::: bot acc: 0.0141
current epoch: 16
train loss is 0.053774
average val loss: 0.037680, accuracy: 0.0388
average test loss: 0.035804, accuracy: 0.0359
case acc: 0.02923622
case acc: 0.03269396
case acc: 0.035280738
case acc: 0.06959099
case acc: 0.024299227
case acc: 0.024489084
top acc: 0.0129 ::: bot acc: 0.0453
top acc: 0.0584 ::: bot acc: 0.0102
top acc: 0.0674 ::: bot acc: 0.0112
top acc: 0.0442 ::: bot acc: 0.0952
top acc: 0.0404 ::: bot acc: 0.0138
top acc: 0.0396 ::: bot acc: 0.0130
current epoch: 17
train loss is 0.050687
average val loss: 0.034562, accuracy: 0.0361
average test loss: 0.033533, accuracy: 0.0338
case acc: 0.03702138
case acc: 0.023669483
case acc: 0.03139037
case acc: 0.070384406
case acc: 0.020760218
case acc: 0.01935767
top acc: 0.0200 ::: bot acc: 0.0535
top acc: 0.0450 ::: bot acc: 0.0099
top acc: 0.0602 ::: bot acc: 0.0140
top acc: 0.0450 ::: bot acc: 0.0960
top acc: 0.0326 ::: bot acc: 0.0188
top acc: 0.0315 ::: bot acc: 0.0140
current epoch: 18
train loss is 0.048795
average val loss: 0.032836, accuracy: 0.0345
average test loss: 0.032703, accuracy: 0.0330
case acc: 0.042257905
case acc: 0.020772183
case acc: 0.029422602
case acc: 0.06979388
case acc: 0.019014813
case acc: 0.01673234
top acc: 0.0253 ::: bot acc: 0.0587
top acc: 0.0362 ::: bot acc: 0.0176
top acc: 0.0550 ::: bot acc: 0.0186
top acc: 0.0444 ::: bot acc: 0.0954
top acc: 0.0253 ::: bot acc: 0.0260
top acc: 0.0254 ::: bot acc: 0.0183
current epoch: 19
train loss is 0.047225
average val loss: 0.031678, accuracy: 0.0333
average test loss: 0.031823, accuracy: 0.0320
case acc: 0.04199354
case acc: 0.020480026
case acc: 0.029289989
case acc: 0.06520879
case acc: 0.018696345
case acc: 0.0162894
top acc: 0.0251 ::: bot acc: 0.0584
top acc: 0.0342 ::: bot acc: 0.0197
top acc: 0.0545 ::: bot acc: 0.0193
top acc: 0.0400 ::: bot acc: 0.0907
top acc: 0.0218 ::: bot acc: 0.0296
top acc: 0.0239 ::: bot acc: 0.0198
current epoch: 20
train loss is 0.046307
average val loss: 0.030656, accuracy: 0.0322
average test loss: 0.031001, accuracy: 0.0310
case acc: 0.04131439
case acc: 0.020321392
case acc: 0.029156601
case acc: 0.060535166
case acc: 0.018862724
case acc: 0.01606549
top acc: 0.0245 ::: bot acc: 0.0577
top acc: 0.0330 ::: bot acc: 0.0209
top acc: 0.0541 ::: bot acc: 0.0197
top acc: 0.0354 ::: bot acc: 0.0860
top acc: 0.0187 ::: bot acc: 0.0328
top acc: 0.0227 ::: bot acc: 0.0210
current epoch: 21
train loss is 0.045267
average val loss: 0.029599, accuracy: 0.0309
average test loss: 0.029599, accuracy: 0.0295
case acc: 0.037243497
case acc: 0.02095383
case acc: 0.030229505
case acc: 0.05309074
case acc: 0.018892337
case acc: 0.01644227
top acc: 0.0204 ::: bot acc: 0.0537
top acc: 0.0367 ::: bot acc: 0.0173
top acc: 0.0570 ::: bot acc: 0.0173
top acc: 0.0281 ::: bot acc: 0.0785
top acc: 0.0188 ::: bot acc: 0.0328
top acc: 0.0244 ::: bot acc: 0.0194
current epoch: 22
train loss is 0.044410
average val loss: 0.028776, accuracy: 0.0298
average test loss: 0.028140, accuracy: 0.0277
case acc: 0.03153616
case acc: 0.022552717
case acc: 0.032205373
case acc: 0.043689337
case acc: 0.018736893
case acc: 0.017584722
top acc: 0.0151 ::: bot acc: 0.0478
top acc: 0.0424 ::: bot acc: 0.0120
top acc: 0.0618 ::: bot acc: 0.0136
top acc: 0.0192 ::: bot acc: 0.0689
top acc: 0.0217 ::: bot acc: 0.0298
top acc: 0.0281 ::: bot acc: 0.0159
current epoch: 23
train loss is 0.042828
average val loss: 0.028045, accuracy: 0.0288
average test loss: 0.027183, accuracy: 0.0266
case acc: 0.02924412
case acc: 0.023404438
case acc: 0.0330747
case acc: 0.037530772
case acc: 0.018757295
case acc: 0.017797278
top acc: 0.0132 ::: bot acc: 0.0453
top acc: 0.0444 ::: bot acc: 0.0107
top acc: 0.0636 ::: bot acc: 0.0126
top acc: 0.0141 ::: bot acc: 0.0623
top acc: 0.0226 ::: bot acc: 0.0289
top acc: 0.0287 ::: bot acc: 0.0154
current epoch: 24
train loss is 0.042338
average val loss: 0.027877, accuracy: 0.0282
average test loss: 0.026367, accuracy: 0.0258
case acc: 0.024951318
case acc: 0.025300499
case acc: 0.035759304
case acc: 0.029755566
case acc: 0.019323446
case acc: 0.019559637
top acc: 0.0100 ::: bot acc: 0.0405
top acc: 0.0481 ::: bot acc: 0.0089
top acc: 0.0683 ::: bot acc: 0.0115
top acc: 0.0102 ::: bot acc: 0.0525
top acc: 0.0276 ::: bot acc: 0.0239
top acc: 0.0323 ::: bot acc: 0.0136
current epoch: 25
train loss is 0.041436
average val loss: 0.027580, accuracy: 0.0276
average test loss: 0.025656, accuracy: 0.0251
case acc: 0.023880117
case acc: 0.02490521
case acc: 0.036741428
case acc: 0.024769032
case acc: 0.020438004
case acc: 0.020083182
top acc: 0.0093 ::: bot acc: 0.0392
top acc: 0.0473 ::: bot acc: 0.0092
top acc: 0.0697 ::: bot acc: 0.0115
top acc: 0.0103 ::: bot acc: 0.0450
top acc: 0.0319 ::: bot acc: 0.0196
top acc: 0.0332 ::: bot acc: 0.0134
current epoch: 26
train loss is 0.039742
average val loss: 0.026261, accuracy: 0.0265
average test loss: 0.024627, accuracy: 0.0243
case acc: 0.027286507
case acc: 0.021961948
case acc: 0.03444682
case acc: 0.023332682
case acc: 0.020620298
case acc: 0.018053273
top acc: 0.0116 ::: bot acc: 0.0432
top acc: 0.0409 ::: bot acc: 0.0131
top acc: 0.0661 ::: bot acc: 0.0119
top acc: 0.0114 ::: bot acc: 0.0423
top acc: 0.0326 ::: bot acc: 0.0189
top acc: 0.0292 ::: bot acc: 0.0152
current epoch: 27
train loss is 0.038782
average val loss: 0.025872, accuracy: 0.0261
average test loss: 0.024154, accuracy: 0.0240
case acc: 0.02887737
case acc: 0.020692792
case acc: 0.03356879
case acc: 0.02090739
case acc: 0.022499412
case acc: 0.01765484
top acc: 0.0129 ::: bot acc: 0.0450
top acc: 0.0352 ::: bot acc: 0.0187
top acc: 0.0646 ::: bot acc: 0.0124
top acc: 0.0149 ::: bot acc: 0.0368
top acc: 0.0376 ::: bot acc: 0.0146
top acc: 0.0283 ::: bot acc: 0.0160
current epoch: 28
train loss is 0.038409
average val loss: 0.025956, accuracy: 0.0263
average test loss: 0.024483, accuracy: 0.0246
case acc: 0.032599207
case acc: 0.020497924
case acc: 0.03187408
case acc: 0.019473681
case acc: 0.026013274
case acc: 0.017163176
top acc: 0.0162 ::: bot acc: 0.0489
top acc: 0.0246 ::: bot acc: 0.0293
top acc: 0.0610 ::: bot acc: 0.0145
top acc: 0.0193 ::: bot acc: 0.0322
top acc: 0.0448 ::: bot acc: 0.0109
top acc: 0.0270 ::: bot acc: 0.0173
current epoch: 29
train loss is 0.039658
average val loss: 0.027789, accuracy: 0.0281
average test loss: 0.026700, accuracy: 0.0269
case acc: 0.037804145
case acc: 0.025105877
case acc: 0.029899608
case acc: 0.018899348
case acc: 0.033035588
case acc: 0.016901545
top acc: 0.0212 ::: bot acc: 0.0542
top acc: 0.0119 ::: bot acc: 0.0440
top acc: 0.0560 ::: bot acc: 0.0186
top acc: 0.0235 ::: bot acc: 0.0279
top acc: 0.0550 ::: bot acc: 0.0114
top acc: 0.0262 ::: bot acc: 0.0180
current epoch: 30
train loss is 0.045995
average val loss: 0.033121, accuracy: 0.0345
average test loss: 0.034387, accuracy: 0.0354
case acc: 0.059900727
case acc: 0.054108635
case acc: 0.027565893
case acc: 0.022065707
case acc: 0.031410903
case acc: 0.017520815
top acc: 0.0433 ::: bot acc: 0.0763
top acc: 0.0267 ::: bot acc: 0.0801
top acc: 0.0343 ::: bot acc: 0.0400
top acc: 0.0128 ::: bot acc: 0.0396
top acc: 0.0529 ::: bot acc: 0.0109
top acc: 0.0115 ::: bot acc: 0.0326
current epoch: 31
train loss is 0.065439
average val loss: 0.106487, accuracy: 0.1064
average test loss: 0.113154, accuracy: 0.1131
case acc: 0.15555999
case acc: 0.16657981
case acc: 0.09996108
case acc: 0.10477046
case acc: 0.054162912
case acc: 0.097698584
top acc: 0.1386 ::: bot acc: 0.1720
top acc: 0.1389 ::: bot acc: 0.1929
top acc: 0.0619 ::: bot acc: 0.1357
top acc: 0.0793 ::: bot acc: 0.1304
top acc: 0.0298 ::: bot acc: 0.0811
top acc: 0.0767 ::: bot acc: 0.1203
current epoch: 32
train loss is 0.103988
average val loss: 0.079637, accuracy: 0.0797
average test loss: 0.086027, accuracy: 0.0859
case acc: 0.12183552
case acc: 0.1251853
case acc: 0.07350358
case acc: 0.08680738
case acc: 0.03934469
case acc: 0.06901339
top acc: 0.1048 ::: bot acc: 0.1383
top acc: 0.0974 ::: bot acc: 0.1515
top acc: 0.0356 ::: bot acc: 0.1092
top acc: 0.0614 ::: bot acc: 0.1125
top acc: 0.0158 ::: bot acc: 0.0658
top acc: 0.0482 ::: bot acc: 0.0915
current epoch: 33
train loss is 0.069707
average val loss: 0.049980, accuracy: 0.0510
average test loss: 0.055749, accuracy: 0.0560
case acc: 0.079902604
case acc: 0.0640138
case acc: 0.04411081
case acc: 0.0693829
case acc: 0.037894182
case acc: 0.0407274
top acc: 0.0630 ::: bot acc: 0.0962
top acc: 0.0362 ::: bot acc: 0.0904
top acc: 0.0149 ::: bot acc: 0.0755
top acc: 0.0442 ::: bot acc: 0.0949
top acc: 0.0145 ::: bot acc: 0.0642
top acc: 0.0203 ::: bot acc: 0.0630
current epoch: 34
train loss is 0.045720
average val loss: 0.036004, accuracy: 0.0380
average test loss: 0.040670, accuracy: 0.0413
case acc: 0.053755496
case acc: 0.026996676
case acc: 0.03256468
case acc: 0.06266957
case acc: 0.044316683
case acc: 0.02734666
top acc: 0.0369 ::: bot acc: 0.0701
top acc: 0.0104 ::: bot acc: 0.0478
top acc: 0.0186 ::: bot acc: 0.0563
top acc: 0.0377 ::: bot acc: 0.0881
top acc: 0.0202 ::: bot acc: 0.0710
top acc: 0.0095 ::: bot acc: 0.0483
current epoch: 35
train loss is 0.040225
average val loss: 0.025754, accuracy: 0.0260
average test loss: 0.025119, accuracy: 0.0243
case acc: 0.01883799
case acc: 0.025902392
case acc: 0.028622713
case acc: 0.032187574
case acc: 0.022582669
case acc: 0.01786394
top acc: 0.0081 ::: bot acc: 0.0321
top acc: 0.0493 ::: bot acc: 0.0084
top acc: 0.0523 ::: bot acc: 0.0217
top acc: 0.0112 ::: bot acc: 0.0556
top acc: 0.0092 ::: bot acc: 0.0439
top acc: 0.0291 ::: bot acc: 0.0148
current epoch: 36
train loss is 0.040834
average val loss: 0.032993, accuracy: 0.0322
average test loss: 0.028999, accuracy: 0.0278
case acc: 0.012769889
case acc: 0.045257546
case acc: 0.039423347
case acc: 0.019120298
case acc: 0.019399306
case acc: 0.03091458
top acc: 0.0224 ::: bot acc: 0.0107
top acc: 0.0728 ::: bot acc: 0.0195
top acc: 0.0734 ::: bot acc: 0.0119
top acc: 0.0193 ::: bot acc: 0.0316
top acc: 0.0282 ::: bot acc: 0.0230
top acc: 0.0487 ::: bot acc: 0.0149
current epoch: 37
train loss is 0.043838
average val loss: 0.044528, accuracy: 0.0442
average test loss: 0.038642, accuracy: 0.0379
case acc: 0.02117567
case acc: 0.061111886
case acc: 0.05301968
case acc: 0.022586469
case acc: 0.026979089
case acc: 0.042432327
top acc: 0.0370 ::: bot acc: 0.0068
top acc: 0.0890 ::: bot acc: 0.0347
top acc: 0.0898 ::: bot acc: 0.0200
top acc: 0.0402 ::: bot acc: 0.0124
top acc: 0.0460 ::: bot acc: 0.0108
top acc: 0.0623 ::: bot acc: 0.0222
current epoch: 38
train loss is 0.045869
average val loss: 0.050187, accuracy: 0.0501
average test loss: 0.043815, accuracy: 0.0431
case acc: 0.023502382
case acc: 0.062509306
case acc: 0.05859392
case acc: 0.03132554
case acc: 0.036731128
case acc: 0.04593044
top acc: 0.0397 ::: bot acc: 0.0084
top acc: 0.0903 ::: bot acc: 0.0361
top acc: 0.0959 ::: bot acc: 0.0244
top acc: 0.0534 ::: bot acc: 0.0121
top acc: 0.0594 ::: bot acc: 0.0134
top acc: 0.0662 ::: bot acc: 0.0249
current epoch: 39
train loss is 0.042969
average val loss: 0.041183, accuracy: 0.0408
average test loss: 0.035415, accuracy: 0.0344
case acc: 0.013084502
case acc: 0.041710377
case acc: 0.04828122
case acc: 0.029755088
case acc: 0.038437333
case acc: 0.035042673
top acc: 0.0235 ::: bot acc: 0.0097
top acc: 0.0691 ::: bot acc: 0.0163
top acc: 0.0843 ::: bot acc: 0.0167
top acc: 0.0513 ::: bot acc: 0.0116
top acc: 0.0614 ::: bot acc: 0.0145
top acc: 0.0536 ::: bot acc: 0.0173
current epoch: 40
train loss is 0.036006
average val loss: 0.028009, accuracy: 0.0281
average test loss: 0.024771, accuracy: 0.0246
case acc: 0.021606594
case acc: 0.02087634
case acc: 0.03229614
case acc: 0.021809416
case acc: 0.03213591
case acc: 0.019024417
top acc: 0.0085 ::: bot acc: 0.0360
top acc: 0.0358 ::: bot acc: 0.0184
top acc: 0.0620 ::: bot acc: 0.0134
top acc: 0.0386 ::: bot acc: 0.0132
top acc: 0.0535 ::: bot acc: 0.0114
top acc: 0.0316 ::: bot acc: 0.0133
current epoch: 41
train loss is 0.036922
average val loss: 0.026051, accuracy: 0.0269
average test loss: 0.026061, accuracy: 0.0268
case acc: 0.042596012
case acc: 0.028461602
case acc: 0.026785968
case acc: 0.018577704
case acc: 0.027930144
case acc: 0.016538886
top acc: 0.0258 ::: bot acc: 0.0589
top acc: 0.0101 ::: bot acc: 0.0501
top acc: 0.0411 ::: bot acc: 0.0329
top acc: 0.0261 ::: bot acc: 0.0248
top acc: 0.0476 ::: bot acc: 0.0107
top acc: 0.0137 ::: bot acc: 0.0298
current epoch: 42
train loss is 0.050544
average val loss: 0.058802, accuracy: 0.0591
average test loss: 0.064624, accuracy: 0.0648
case acc: 0.1011738
case acc: 0.096017115
case acc: 0.05660554
case acc: 0.051766396
case acc: 0.024223013
case acc: 0.058965024
top acc: 0.0843 ::: bot acc: 0.1174
top acc: 0.0682 ::: bot acc: 0.1224
top acc: 0.0215 ::: bot acc: 0.0909
top acc: 0.0273 ::: bot acc: 0.0770
top acc: 0.0083 ::: bot acc: 0.0468
top acc: 0.0380 ::: bot acc: 0.0815
current epoch: 43
train loss is 0.076725
average val loss: 0.086446, accuracy: 0.0865
average test loss: 0.093020, accuracy: 0.0930
case acc: 0.12607634
case acc: 0.12353562
case acc: 0.084067196
case acc: 0.08550075
case acc: 0.0546525
case acc: 0.08410769
top acc: 0.1092 ::: bot acc: 0.1423
top acc: 0.0957 ::: bot acc: 0.1499
top acc: 0.0459 ::: bot acc: 0.1199
top acc: 0.0603 ::: bot acc: 0.1111
top acc: 0.0303 ::: bot acc: 0.0814
top acc: 0.0632 ::: bot acc: 0.1066
current epoch: 44
train loss is 0.067603
average val loss: 0.043987, accuracy: 0.0450
average test loss: 0.049554, accuracy: 0.0499
case acc: 0.071313255
case acc: 0.05372477
case acc: 0.04222983
case acc: 0.05421257
case acc: 0.03753747
case acc: 0.04008308
top acc: 0.0545 ::: bot acc: 0.0875
top acc: 0.0262 ::: bot acc: 0.0800
top acc: 0.0145 ::: bot acc: 0.0729
top acc: 0.0297 ::: bot acc: 0.0795
top acc: 0.0143 ::: bot acc: 0.0638
top acc: 0.0196 ::: bot acc: 0.0623
current epoch: 45
train loss is 0.042598
average val loss: 0.027232, accuracy: 0.0291
average test loss: 0.030410, accuracy: 0.0310
case acc: 0.038532004
case acc: 0.021144424
case acc: 0.02910872
case acc: 0.04085472
case acc: 0.035417043
case acc: 0.0209292
top acc: 0.0217 ::: bot acc: 0.0547
top acc: 0.0217 ::: bot acc: 0.0326
top acc: 0.0273 ::: bot acc: 0.0468
top acc: 0.0172 ::: bot acc: 0.0657
top acc: 0.0128 ::: bot acc: 0.0613
top acc: 0.0079 ::: bot acc: 0.0394
current epoch: 46
train loss is 0.037356
average val loss: 0.027739, accuracy: 0.0269
average test loss: 0.024684, accuracy: 0.0235
case acc: 0.012136376
case acc: 0.03518759
case acc: 0.032387987
case acc: 0.01910077
case acc: 0.018656122
case acc: 0.02351307
top acc: 0.0155 ::: bot acc: 0.0175
top acc: 0.0618 ::: bot acc: 0.0113
top acc: 0.0621 ::: bot acc: 0.0134
top acc: 0.0186 ::: bot acc: 0.0321
top acc: 0.0219 ::: bot acc: 0.0292
top acc: 0.0389 ::: bot acc: 0.0123
current epoch: 47
train loss is 0.038631
average val loss: 0.036564, accuracy: 0.0360
average test loss: 0.031203, accuracy: 0.0304
case acc: 0.015009442
case acc: 0.04560796
case acc: 0.04121612
case acc: 0.02136927
case acc: 0.025197618
case acc: 0.033789553
top acc: 0.0280 ::: bot acc: 0.0064
top acc: 0.0732 ::: bot acc: 0.0197
top acc: 0.0758 ::: bot acc: 0.0125
top acc: 0.0374 ::: bot acc: 0.0139
top acc: 0.0427 ::: bot acc: 0.0121
top acc: 0.0522 ::: bot acc: 0.0164
current epoch: 48
train loss is 0.038608
average val loss: 0.034759, accuracy: 0.0343
average test loss: 0.029495, accuracy: 0.0286
case acc: 0.012590465
case acc: 0.037378687
case acc: 0.038785715
case acc: 0.023357973
case acc: 0.02882081
case acc: 0.030449653
top acc: 0.0220 ::: bot acc: 0.0109
top acc: 0.0643 ::: bot acc: 0.0128
top acc: 0.0727 ::: bot acc: 0.0115
top acc: 0.0414 ::: bot acc: 0.0118
top acc: 0.0489 ::: bot acc: 0.0107
top acc: 0.0481 ::: bot acc: 0.0146
current epoch: 49
train loss is 0.034780
average val loss: 0.026575, accuracy: 0.0264
average test loss: 0.023034, accuracy: 0.0228
case acc: 0.016093966
case acc: 0.022997282
case acc: 0.030940989
case acc: 0.02031481
case acc: 0.026226228
case acc: 0.020056248
top acc: 0.0083 ::: bot acc: 0.0278
top acc: 0.0436 ::: bot acc: 0.0111
top acc: 0.0587 ::: bot acc: 0.0160
top acc: 0.0347 ::: bot acc: 0.0160
top acc: 0.0447 ::: bot acc: 0.0114
top acc: 0.0336 ::: bot acc: 0.0126
current epoch: 50
train loss is 0.033431
average val loss: 0.022794, accuracy: 0.0234
average test loss: 0.021820, accuracy: 0.0225
case acc: 0.028552618
case acc: 0.021088323
case acc: 0.026992919
case acc: 0.018593496
case acc: 0.02391701
case acc: 0.015797917
top acc: 0.0127 ::: bot acc: 0.0443
top acc: 0.0218 ::: bot acc: 0.0324
top acc: 0.0440 ::: bot acc: 0.0301
top acc: 0.0265 ::: bot acc: 0.0243
top acc: 0.0403 ::: bot acc: 0.0133
top acc: 0.0199 ::: bot acc: 0.0237
LME_Co_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6798 6798 6798
1.8562728 -0.6288155 0.21141115 -0.19947179
Validation: 756 756 756
Testing: 744 744 744
pre-processing time: 0.0005865097045898438
the split date is 2010-01-01
net initializing with time: 0.0034515857696533203
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.155670
average val loss: 0.038509, accuracy: 0.0389
average test loss: 0.042187, accuracy: 0.0431
case acc: 0.047841385
case acc: 0.020571487
case acc: 0.035113297
case acc: 0.042977776
case acc: 0.024058377
case acc: 0.0882221
top acc: 0.0228 ::: bot acc: 0.0759
top acc: 0.0164 ::: bot acc: 0.0332
top acc: 0.0547 ::: bot acc: 0.0361
top acc: 0.0210 ::: bot acc: 0.0698
top acc: 0.0406 ::: bot acc: 0.0244
top acc: 0.1325 ::: bot acc: 0.0489
current epoch: 2
train loss is 0.055237
average val loss: 0.114448, accuracy: 0.1143
average test loss: 0.116578, accuracy: 0.1173
case acc: 0.16043906
case acc: 0.12606487
case acc: 0.10881529
case acc: 0.15056504
case acc: 0.11105053
case acc: 0.046903487
top acc: 0.1292 ::: bot acc: 0.1918
top acc: 0.1017 ::: bot acc: 0.1498
top acc: 0.0643 ::: bot acc: 0.1546
top acc: 0.1175 ::: bot acc: 0.1831
top acc: 0.0783 ::: bot acc: 0.1419
top acc: 0.0208 ::: bot acc: 0.0766
current epoch: 3
train loss is 0.088819
average val loss: 0.120602, accuracy: 0.1206
average test loss: 0.122412, accuracy: 0.1228
case acc: 0.16152833
case acc: 0.12885363
case acc: 0.116154686
case acc: 0.15623713
case acc: 0.11599475
case acc: 0.058098942
top acc: 0.1295 ::: bot acc: 0.1934
top acc: 0.1044 ::: bot acc: 0.1533
top acc: 0.0717 ::: bot acc: 0.1615
top acc: 0.1239 ::: bot acc: 0.1884
top acc: 0.0836 ::: bot acc: 0.1463
top acc: 0.0246 ::: bot acc: 0.0910
current epoch: 4
train loss is 0.106103
average val loss: 0.050977, accuracy: 0.0506
average test loss: 0.054234, accuracy: 0.0553
case acc: 0.08087032
case acc: 0.05106218
case acc: 0.04687287
case acc: 0.082234554
case acc: 0.042101834
case acc: 0.028622067
top acc: 0.0488 ::: bot acc: 0.1128
top acc: 0.0281 ::: bot acc: 0.0748
top acc: 0.0140 ::: bot acc: 0.0865
top acc: 0.0520 ::: bot acc: 0.1134
top acc: 0.0158 ::: bot acc: 0.0695
top acc: 0.0542 ::: bot acc: 0.0233
current epoch: 5
train loss is 0.065942
average val loss: 0.078775, accuracy: 0.0785
average test loss: 0.081115, accuracy: 0.0819
case acc: 0.1088622
case acc: 0.08002411
case acc: 0.07589281
case acc: 0.11517796
case acc: 0.07087668
case acc: 0.04035325
top acc: 0.0767 ::: bot acc: 0.1408
top acc: 0.0551 ::: bot acc: 0.1048
top acc: 0.0324 ::: bot acc: 0.1208
top acc: 0.0837 ::: bot acc: 0.1470
top acc: 0.0387 ::: bot acc: 0.1011
top acc: 0.0213 ::: bot acc: 0.0658
current epoch: 6
train loss is 0.071591
average val loss: 0.078586, accuracy: 0.0784
average test loss: 0.080722, accuracy: 0.0813
case acc: 0.10375404
case acc: 0.076621436
case acc: 0.07680324
case acc: 0.11585728
case acc: 0.069052175
case acc: 0.045688987
top acc: 0.0715 ::: bot acc: 0.1356
top acc: 0.0519 ::: bot acc: 0.1014
top acc: 0.0334 ::: bot acc: 0.1216
top acc: 0.0846 ::: bot acc: 0.1476
top acc: 0.0370 ::: bot acc: 0.0990
top acc: 0.0205 ::: bot acc: 0.0742
current epoch: 7
train loss is 0.071236
average val loss: 0.063814, accuracy: 0.0636
average test loss: 0.066468, accuracy: 0.0670
case acc: 0.083799064
case acc: 0.05879524
case acc: 0.06345008
case acc: 0.10212723
case acc: 0.052933544
case acc: 0.041040175
top acc: 0.0515 ::: bot acc: 0.1155
top acc: 0.0351 ::: bot acc: 0.0833
top acc: 0.0216 ::: bot acc: 0.1075
top acc: 0.0714 ::: bot acc: 0.1335
top acc: 0.0225 ::: bot acc: 0.0820
top acc: 0.0208 ::: bot acc: 0.0673
current epoch: 8
train loss is 0.064245
average val loss: 0.057766, accuracy: 0.0576
average test loss: 0.060720, accuracy: 0.0612
case acc: 0.07315132
case acc: 0.04997706
case acc: 0.059050128
case acc: 0.0967963
case acc: 0.046199426
case acc: 0.042078573
top acc: 0.0411 ::: bot acc: 0.1047
top acc: 0.0272 ::: bot acc: 0.0742
top acc: 0.0188 ::: bot acc: 0.1022
top acc: 0.0664 ::: bot acc: 0.1280
top acc: 0.0182 ::: bot acc: 0.0739
top acc: 0.0206 ::: bot acc: 0.0691
current epoch: 9
train loss is 0.055834
average val loss: 0.056826, accuracy: 0.0568
average test loss: 0.059748, accuracy: 0.0601
case acc: 0.068191
case acc: 0.04647296
case acc: 0.059134
case acc: 0.09636832
case acc: 0.044159755
case acc: 0.046029896
top acc: 0.0363 ::: bot acc: 0.0995
top acc: 0.0242 ::: bot acc: 0.0706
top acc: 0.0189 ::: bot acc: 0.1022
top acc: 0.0662 ::: bot acc: 0.1274
top acc: 0.0170 ::: bot acc: 0.0713
top acc: 0.0203 ::: bot acc: 0.0752
current epoch: 10
train loss is 0.048832
average val loss: 0.056019, accuracy: 0.0561
average test loss: 0.058941, accuracy: 0.0591
case acc: 0.064074114
case acc: 0.04366613
case acc: 0.059204753
case acc: 0.095965154
case acc: 0.042520724
case acc: 0.049207974
top acc: 0.0326 ::: bot acc: 0.0952
top acc: 0.0217 ::: bot acc: 0.0678
top acc: 0.0189 ::: bot acc: 0.1023
top acc: 0.0659 ::: bot acc: 0.1269
top acc: 0.0163 ::: bot acc: 0.0691
top acc: 0.0206 ::: bot acc: 0.0797
current epoch: 11
train loss is 0.042986
average val loss: 0.049540, accuracy: 0.0497
average test loss: 0.053007, accuracy: 0.0532
case acc: 0.055618867
case acc: 0.03671849
case acc: 0.054258417
case acc: 0.089090474
case acc: 0.036994096
case acc: 0.04663527
top acc: 0.0260 ::: bot acc: 0.0859
top acc: 0.0167 ::: bot acc: 0.0600
top acc: 0.0162 ::: bot acc: 0.0962
top acc: 0.0594 ::: bot acc: 0.1199
top acc: 0.0146 ::: bot acc: 0.0617
top acc: 0.0203 ::: bot acc: 0.0759
current epoch: 12
train loss is 0.039224
average val loss: 0.041298, accuracy: 0.0416
average test loss: 0.045565, accuracy: 0.0459
case acc: 0.046913616
case acc: 0.02997924
case acc: 0.04799577
case acc: 0.077695355
case acc: 0.03136175
case acc: 0.041293453
top acc: 0.0203 ::: bot acc: 0.0757
top acc: 0.0138 ::: bot acc: 0.0516
top acc: 0.0141 ::: bot acc: 0.0879
top acc: 0.0487 ::: bot acc: 0.1081
top acc: 0.0147 ::: bot acc: 0.0531
top acc: 0.0204 ::: bot acc: 0.0679
current epoch: 13
train loss is 0.037262
average val loss: 0.036330, accuracy: 0.0368
average test loss: 0.041093, accuracy: 0.0415
case acc: 0.042148527
case acc: 0.026569994
case acc: 0.044901364
case acc: 0.06818606
case acc: 0.028716017
case acc: 0.03820957
top acc: 0.0184 ::: bot acc: 0.0696
top acc: 0.0132 ::: bot acc: 0.0469
top acc: 0.0146 ::: bot acc: 0.0830
top acc: 0.0399 ::: bot acc: 0.0982
top acc: 0.0159 ::: bot acc: 0.0484
top acc: 0.0210 ::: bot acc: 0.0629
current epoch: 14
train loss is 0.036056
average val loss: 0.034210, accuracy: 0.0346
average test loss: 0.039193, accuracy: 0.0395
case acc: 0.040492076
case acc: 0.026029356
case acc: 0.04389779
case acc: 0.061450593
case acc: 0.028316805
case acc: 0.03701876
top acc: 0.0179 ::: bot acc: 0.0674
top acc: 0.0131 ::: bot acc: 0.0461
top acc: 0.0150 ::: bot acc: 0.0813
top acc: 0.0336 ::: bot acc: 0.0912
top acc: 0.0161 ::: bot acc: 0.0477
top acc: 0.0215 ::: bot acc: 0.0608
current epoch: 15
train loss is 0.034950
average val loss: 0.030673, accuracy: 0.0311
average test loss: 0.036034, accuracy: 0.0364
case acc: 0.03744348
case acc: 0.023981843
case acc: 0.04179764
case acc: 0.053282842
case acc: 0.026853822
case acc: 0.03480639
top acc: 0.0175 ::: bot acc: 0.0631
top acc: 0.0129 ::: bot acc: 0.0431
top acc: 0.0164 ::: bot acc: 0.0775
top acc: 0.0263 ::: bot acc: 0.0826
top acc: 0.0175 ::: bot acc: 0.0449
top acc: 0.0230 ::: bot acc: 0.0566
current epoch: 16
train loss is 0.034243
average val loss: 0.028167, accuracy: 0.0286
average test loss: 0.033809, accuracy: 0.0341
case acc: 0.03551737
case acc: 0.022885226
case acc: 0.040357225
case acc: 0.04662477
case acc: 0.026046453
case acc: 0.033242516
top acc: 0.0177 ::: bot acc: 0.0602
top acc: 0.0132 ::: bot acc: 0.0412
top acc: 0.0177 ::: bot acc: 0.0748
top acc: 0.0210 ::: bot acc: 0.0753
top acc: 0.0187 ::: bot acc: 0.0432
top acc: 0.0248 ::: bot acc: 0.0534
current epoch: 17
train loss is 0.033213
average val loss: 0.026147, accuracy: 0.0266
average test loss: 0.031976, accuracy: 0.0322
case acc: 0.03375173
case acc: 0.02179502
case acc: 0.039185476
case acc: 0.041339133
case acc: 0.025281662
case acc: 0.032048617
top acc: 0.0181 ::: bot acc: 0.0573
top acc: 0.0137 ::: bot acc: 0.0393
top acc: 0.0190 ::: bot acc: 0.0724
top acc: 0.0177 ::: bot acc: 0.0690
top acc: 0.0199 ::: bot acc: 0.0414
top acc: 0.0262 ::: bot acc: 0.0509
current epoch: 18
train loss is 0.032998
average val loss: 0.024436, accuracy: 0.0249
average test loss: 0.030398, accuracy: 0.0306
case acc: 0.03223067
case acc: 0.020952303
case acc: 0.038093925
case acc: 0.03676029
case acc: 0.024660287
case acc: 0.030843515
top acc: 0.0186 ::: bot acc: 0.0547
top acc: 0.0145 ::: bot acc: 0.0376
top acc: 0.0206 ::: bot acc: 0.0700
top acc: 0.0155 ::: bot acc: 0.0633
top acc: 0.0212 ::: bot acc: 0.0398
top acc: 0.0280 ::: bot acc: 0.0483
current epoch: 19
train loss is 0.032490
average val loss: 0.024175, accuracy: 0.0245
average test loss: 0.030169, accuracy: 0.0303
case acc: 0.03212563
case acc: 0.02129951
case acc: 0.038125105
case acc: 0.034395434
case acc: 0.024927413
case acc: 0.03086726
top acc: 0.0186 ::: bot acc: 0.0545
top acc: 0.0142 ::: bot acc: 0.0381
top acc: 0.0205 ::: bot acc: 0.0701
top acc: 0.0147 ::: bot acc: 0.0602
top acc: 0.0206 ::: bot acc: 0.0405
top acc: 0.0280 ::: bot acc: 0.0482
current epoch: 20
train loss is 0.032152
average val loss: 0.021757, accuracy: 0.0221
average test loss: 0.027772, accuracy: 0.0278
case acc: 0.029282933
case acc: 0.01936853
case acc: 0.03628946
case acc: 0.029319987
case acc: 0.023395998
case acc: 0.029168848
top acc: 0.0200 ::: bot acc: 0.0496
top acc: 0.0170 ::: bot acc: 0.0339
top acc: 0.0239 ::: bot acc: 0.0657
top acc: 0.0137 ::: bot acc: 0.0531
top acc: 0.0245 ::: bot acc: 0.0363
top acc: 0.0315 ::: bot acc: 0.0438
current epoch: 21
train loss is 0.031695
average val loss: 0.021590, accuracy: 0.0219
average test loss: 0.027588, accuracy: 0.0276
case acc: 0.029051071
case acc: 0.019371998
case acc: 0.036238696
case acc: 0.028174518
case acc: 0.023435293
case acc: 0.029196648
top acc: 0.0202 ::: bot acc: 0.0491
top acc: 0.0169 ::: bot acc: 0.0338
top acc: 0.0240 ::: bot acc: 0.0656
top acc: 0.0141 ::: bot acc: 0.0513
top acc: 0.0244 ::: bot acc: 0.0364
top acc: 0.0316 ::: bot acc: 0.0439
current epoch: 22
train loss is 0.031608
average val loss: 0.020396, accuracy: 0.0207
average test loss: 0.026303, accuracy: 0.0263
case acc: 0.027221357
case acc: 0.018464899
case acc: 0.03530624
case acc: 0.02595484
case acc: 0.02249499
case acc: 0.028237797
top acc: 0.0216 ::: bot acc: 0.0456
top acc: 0.0198 ::: bot acc: 0.0307
top acc: 0.0270 ::: bot acc: 0.0625
top acc: 0.0163 ::: bot acc: 0.0469
top acc: 0.0274 ::: bot acc: 0.0334
top acc: 0.0344 ::: bot acc: 0.0410
current epoch: 23
train loss is 0.031276
average val loss: 0.020841, accuracy: 0.0211
average test loss: 0.026773, accuracy: 0.0267
case acc: 0.02785427
case acc: 0.018755626
case acc: 0.035706215
case acc: 0.026179258
case acc: 0.02298761
case acc: 0.028766686
top acc: 0.0210 ::: bot acc: 0.0468
top acc: 0.0182 ::: bot acc: 0.0321
top acc: 0.0256 ::: bot acc: 0.0640
top acc: 0.0160 ::: bot acc: 0.0474
top acc: 0.0258 ::: bot acc: 0.0350
top acc: 0.0329 ::: bot acc: 0.0427
current epoch: 24
train loss is 0.031266
average val loss: 0.021447, accuracy: 0.0217
average test loss: 0.027396, accuracy: 0.0273
case acc: 0.0287197
case acc: 0.019443378
case acc: 0.036205176
case acc: 0.02656383
case acc: 0.023635553
case acc: 0.029291583
top acc: 0.0204 ::: bot acc: 0.0484
top acc: 0.0165 ::: bot acc: 0.0340
top acc: 0.0241 ::: bot acc: 0.0655
top acc: 0.0156 ::: bot acc: 0.0482
top acc: 0.0239 ::: bot acc: 0.0369
top acc: 0.0314 ::: bot acc: 0.0442
current epoch: 25
train loss is 0.031272
average val loss: 0.019572, accuracy: 0.0199
average test loss: 0.025346, accuracy: 0.0252
case acc: 0.025378194
case acc: 0.01813284
case acc: 0.034563538
case acc: 0.02380275
case acc: 0.02190908
case acc: 0.02763962
top acc: 0.0237 ::: bot acc: 0.0418
top acc: 0.0226 ::: bot acc: 0.0277
top acc: 0.0300 ::: bot acc: 0.0596
top acc: 0.0209 ::: bot acc: 0.0414
top acc: 0.0301 ::: bot acc: 0.0307
top acc: 0.0365 ::: bot acc: 0.0388
current epoch: 26
train loss is 0.030920
average val loss: 0.019420, accuracy: 0.0197
average test loss: 0.025157, accuracy: 0.0250
case acc: 0.024867851
case acc: 0.01807721
case acc: 0.034398835
case acc: 0.023577359
case acc: 0.0217843
case acc: 0.027572513
top acc: 0.0246 ::: bot acc: 0.0406
top acc: 0.0238 ::: bot acc: 0.0266
top acc: 0.0306 ::: bot acc: 0.0589
top acc: 0.0216 ::: bot acc: 0.0407
top acc: 0.0311 ::: bot acc: 0.0297
top acc: 0.0367 ::: bot acc: 0.0386
current epoch: 27
train loss is 0.030893
average val loss: 0.019762, accuracy: 0.0200
average test loss: 0.025572, accuracy: 0.0255
case acc: 0.02563505
case acc: 0.018150555
case acc: 0.03470295
case acc: 0.024346655
case acc: 0.02204345
case acc: 0.0279218
top acc: 0.0232 ::: bot acc: 0.0424
top acc: 0.0219 ::: bot acc: 0.0284
top acc: 0.0291 ::: bot acc: 0.0604
top acc: 0.0193 ::: bot acc: 0.0430
top acc: 0.0294 ::: bot acc: 0.0315
top acc: 0.0353 ::: bot acc: 0.0400
current epoch: 28
train loss is 0.030806
average val loss: 0.020474, accuracy: 0.0207
average test loss: 0.026352, accuracy: 0.0263
case acc: 0.027025996
case acc: 0.018569248
case acc: 0.035255197
case acc: 0.025416782
case acc: 0.022805361
case acc: 0.028429475
top acc: 0.0217 ::: bot acc: 0.0453
top acc: 0.0190 ::: bot acc: 0.0312
top acc: 0.0268 ::: bot acc: 0.0626
top acc: 0.0169 ::: bot acc: 0.0458
top acc: 0.0265 ::: bot acc: 0.0344
top acc: 0.0335 ::: bot acc: 0.0417
current epoch: 29
train loss is 0.030757
average val loss: 0.021481, accuracy: 0.0216
average test loss: 0.027406, accuracy: 0.0273
case acc: 0.028624874
case acc: 0.019648215
case acc: 0.0360209
case acc: 0.026403723
case acc: 0.023892883
case acc: 0.029186664
top acc: 0.0204 ::: bot acc: 0.0483
top acc: 0.0160 ::: bot acc: 0.0345
top acc: 0.0244 ::: bot acc: 0.0651
top acc: 0.0156 ::: bot acc: 0.0479
top acc: 0.0233 ::: bot acc: 0.0377
top acc: 0.0314 ::: bot acc: 0.0439
current epoch: 30
train loss is 0.030796
average val loss: 0.019894, accuracy: 0.0201
average test loss: 0.025694, accuracy: 0.0256
case acc: 0.025895638
case acc: 0.018305603
case acc: 0.034728326
case acc: 0.02406541
case acc: 0.022477737
case acc: 0.02793299
top acc: 0.0227 ::: bot acc: 0.0431
top acc: 0.0205 ::: bot acc: 0.0297
top acc: 0.0290 ::: bot acc: 0.0605
top acc: 0.0203 ::: bot acc: 0.0421
top acc: 0.0280 ::: bot acc: 0.0331
top acc: 0.0355 ::: bot acc: 0.0399
current epoch: 31
train loss is 0.030529
average val loss: 0.019317, accuracy: 0.0195
average test loss: 0.025001, accuracy: 0.0248
case acc: 0.02441783
case acc: 0.017999848
case acc: 0.03417381
case acc: 0.023069022
case acc: 0.021883119
case acc: 0.027463548
top acc: 0.0253 ::: bot acc: 0.0396
top acc: 0.0238 ::: bot acc: 0.0264
top acc: 0.0316 ::: bot acc: 0.0579
top acc: 0.0236 ::: bot acc: 0.0388
top acc: 0.0312 ::: bot acc: 0.0300
top acc: 0.0374 ::: bot acc: 0.0380
current epoch: 32
train loss is 0.030630
average val loss: 0.018998, accuracy: 0.0192
average test loss: 0.024481, accuracy: 0.0244
case acc: 0.023190044
case acc: 0.018301511
case acc: 0.033657264
case acc: 0.022498166
case acc: 0.021967502
case acc: 0.026883136
top acc: 0.0297 ::: bot acc: 0.0349
top acc: 0.0284 ::: bot acc: 0.0217
top acc: 0.0355 ::: bot acc: 0.0541
top acc: 0.0267 ::: bot acc: 0.0358
top acc: 0.0358 ::: bot acc: 0.0254
top acc: 0.0405 ::: bot acc: 0.0349
current epoch: 33
train loss is 0.030543
average val loss: 0.018972, accuracy: 0.0192
average test loss: 0.024428, accuracy: 0.0244
case acc: 0.023106596
case acc: 0.018516343
case acc: 0.033562854
case acc: 0.022657521
case acc: 0.022047011
case acc: 0.026790954
top acc: 0.0307 ::: bot acc: 0.0340
top acc: 0.0296 ::: bot acc: 0.0204
top acc: 0.0362 ::: bot acc: 0.0533
top acc: 0.0254 ::: bot acc: 0.0371
top acc: 0.0370 ::: bot acc: 0.0241
top acc: 0.0410 ::: bot acc: 0.0344
current epoch: 34
train loss is 0.030533
average val loss: 0.019121, accuracy: 0.0194
average test loss: 0.024770, accuracy: 0.0248
case acc: 0.023842577
case acc: 0.01793407
case acc: 0.033867095
case acc: 0.024192154
case acc: 0.021821467
case acc: 0.027124865
top acc: 0.0267 ::: bot acc: 0.0379
top acc: 0.0259 ::: bot acc: 0.0240
top acc: 0.0334 ::: bot acc: 0.0561
top acc: 0.0199 ::: bot acc: 0.0426
top acc: 0.0335 ::: bot acc: 0.0276
top acc: 0.0390 ::: bot acc: 0.0365
current epoch: 35
train loss is 0.030519
average val loss: 0.021052, accuracy: 0.0213
average test loss: 0.026939, accuracy: 0.0270
case acc: 0.02784663
case acc: 0.019006783
case acc: 0.03531349
case acc: 0.028011912
case acc: 0.023393506
case acc: 0.02836318
top acc: 0.0209 ::: bot acc: 0.0468
top acc: 0.0173 ::: bot acc: 0.0327
top acc: 0.0267 ::: bot acc: 0.0628
top acc: 0.0141 ::: bot acc: 0.0511
top acc: 0.0251 ::: bot acc: 0.0361
top acc: 0.0340 ::: bot acc: 0.0414
current epoch: 36
train loss is 0.030876
average val loss: 0.027586, accuracy: 0.0276
average test loss: 0.033190, accuracy: 0.0332
case acc: 0.035845473
case acc: 0.026920188
case acc: 0.039922338
case acc: 0.03536273
case acc: 0.029254345
case acc: 0.032041624
top acc: 0.0177 ::: bot acc: 0.0604
top acc: 0.0131 ::: bot acc: 0.0466
top acc: 0.0181 ::: bot acc: 0.0741
top acc: 0.0149 ::: bot acc: 0.0617
top acc: 0.0150 ::: bot acc: 0.0498
top acc: 0.0261 ::: bot acc: 0.0508
current epoch: 37
train loss is 0.032572
average val loss: 0.028611, accuracy: 0.0285
average test loss: 0.034091, accuracy: 0.0340
case acc: 0.036997743
case acc: 0.028940748
case acc: 0.04086618
case acc: 0.033126954
case acc: 0.031017765
case acc: 0.032956604
top acc: 0.0176 ::: bot acc: 0.0622
top acc: 0.0134 ::: bot acc: 0.0495
top acc: 0.0172 ::: bot acc: 0.0759
top acc: 0.0142 ::: bot acc: 0.0587
top acc: 0.0142 ::: bot acc: 0.0528
top acc: 0.0249 ::: bot acc: 0.0527
current epoch: 38
train loss is 0.033657
average val loss: 0.019182, accuracy: 0.0192
average test loss: 0.024569, accuracy: 0.0243
case acc: 0.023151549
case acc: 0.01820618
case acc: 0.03346366
case acc: 0.022387298
case acc: 0.021811204
case acc: 0.026542187
top acc: 0.0308 ::: bot acc: 0.0340
top acc: 0.0278 ::: bot acc: 0.0224
top acc: 0.0369 ::: bot acc: 0.0525
top acc: 0.0328 ::: bot acc: 0.0294
top acc: 0.0348 ::: bot acc: 0.0263
top acc: 0.0418 ::: bot acc: 0.0332
current epoch: 39
train loss is 0.032184
average val loss: 0.027182, accuracy: 0.0273
average test loss: 0.030476, accuracy: 0.0306
case acc: 0.029992614
case acc: 0.029585978
case acc: 0.034432445
case acc: 0.029155953
case acc: 0.03115151
case acc: 0.029252104
top acc: 0.0551 ::: bot acc: 0.0142
top acc: 0.0519 ::: bot acc: 0.0095
top acc: 0.0559 ::: bot acc: 0.0335
top acc: 0.0520 ::: bot acc: 0.0138
top acc: 0.0584 ::: bot acc: 0.0110
top acc: 0.0565 ::: bot acc: 0.0190
current epoch: 40
train loss is 0.034527
average val loss: 0.019299, accuracy: 0.0195
average test loss: 0.024567, accuracy: 0.0247
case acc: 0.02312801
case acc: 0.01974323
case acc: 0.033352327
case acc: 0.022297593
case acc: 0.022556264
case acc: 0.026926348
top acc: 0.0361 ::: bot acc: 0.0287
top acc: 0.0341 ::: bot acc: 0.0158
top acc: 0.0382 ::: bot acc: 0.0511
top acc: 0.0280 ::: bot acc: 0.0343
top acc: 0.0409 ::: bot acc: 0.0200
top acc: 0.0396 ::: bot acc: 0.0356
current epoch: 41
train loss is 0.032143
average val loss: 0.023865, accuracy: 0.0241
average test loss: 0.029761, accuracy: 0.0298
case acc: 0.030954031
case acc: 0.021779133
case acc: 0.037471306
case acc: 0.03174494
case acc: 0.025620254
case acc: 0.030941715
top acc: 0.0191 ::: bot acc: 0.0523
top acc: 0.0135 ::: bot acc: 0.0387
top acc: 0.0214 ::: bot acc: 0.0688
top acc: 0.0138 ::: bot acc: 0.0569
top acc: 0.0191 ::: bot acc: 0.0423
top acc: 0.0278 ::: bot acc: 0.0483
current epoch: 42
train loss is 0.031258
average val loss: 0.025119, accuracy: 0.0251
average test loss: 0.030935, accuracy: 0.0309
case acc: 0.03321874
case acc: 0.0244789
case acc: 0.03803639
case acc: 0.031627387
case acc: 0.02758158
case acc: 0.03055446
top acc: 0.0182 ::: bot acc: 0.0561
top acc: 0.0128 ::: bot acc: 0.0431
top acc: 0.0205 ::: bot acc: 0.0700
top acc: 0.0137 ::: bot acc: 0.0567
top acc: 0.0165 ::: bot acc: 0.0465
top acc: 0.0284 ::: bot acc: 0.0474
current epoch: 43
train loss is 0.032096
average val loss: 0.019264, accuracy: 0.0194
average test loss: 0.024946, accuracy: 0.0247
case acc: 0.024434173
case acc: 0.017980676
case acc: 0.033908736
case acc: 0.022939218
case acc: 0.021968897
case acc: 0.027092468
top acc: 0.0254 ::: bot acc: 0.0395
top acc: 0.0226 ::: bot acc: 0.0274
top acc: 0.0329 ::: bot acc: 0.0565
top acc: 0.0236 ::: bot acc: 0.0386
top acc: 0.0298 ::: bot acc: 0.0311
top acc: 0.0388 ::: bot acc: 0.0365
current epoch: 44
train loss is 0.030737
average val loss: 0.020577, accuracy: 0.0206
average test loss: 0.025399, accuracy: 0.0254
case acc: 0.023923514
case acc: 0.02139026
case acc: 0.033261526
case acc: 0.023118813
case acc: 0.023866912
case acc: 0.026594881
top acc: 0.0413 ::: bot acc: 0.0235
top acc: 0.0383 ::: bot acc: 0.0122
top acc: 0.0449 ::: bot acc: 0.0445
top acc: 0.0370 ::: bot acc: 0.0252
top acc: 0.0452 ::: bot acc: 0.0160
top acc: 0.0473 ::: bot acc: 0.0280
current epoch: 45
train loss is 0.031291
average val loss: 0.019447, accuracy: 0.0196
average test loss: 0.024606, accuracy: 0.0247
case acc: 0.023215951
case acc: 0.020058895
case acc: 0.03320968
case acc: 0.022253338
case acc: 0.022793759
case acc: 0.026575739
top acc: 0.0373 ::: bot acc: 0.0275
top acc: 0.0349 ::: bot acc: 0.0150
top acc: 0.0409 ::: bot acc: 0.0485
top acc: 0.0295 ::: bot acc: 0.0328
top acc: 0.0419 ::: bot acc: 0.0190
top acc: 0.0432 ::: bot acc: 0.0321
current epoch: 46
train loss is 0.031194
average val loss: 0.021129, accuracy: 0.0214
average test loss: 0.027012, accuracy: 0.0270
case acc: 0.027368626
case acc: 0.018927123
case acc: 0.035327915
case acc: 0.028031807
case acc: 0.023456346
case acc: 0.028972685
top acc: 0.0213 ::: bot acc: 0.0458
top acc: 0.0172 ::: bot acc: 0.0326
top acc: 0.0266 ::: bot acc: 0.0629
top acc: 0.0141 ::: bot acc: 0.0512
top acc: 0.0246 ::: bot acc: 0.0363
top acc: 0.0321 ::: bot acc: 0.0433
current epoch: 47
train loss is 0.030689
average val loss: 0.026045, accuracy: 0.0261
average test loss: 0.031791, accuracy: 0.0318
case acc: 0.03391066
case acc: 0.025350003
case acc: 0.03861884
case acc: 0.033463445
case acc: 0.028259197
case acc: 0.031254597
top acc: 0.0180 ::: bot acc: 0.0573
top acc: 0.0129 ::: bot acc: 0.0444
top acc: 0.0197 ::: bot acc: 0.0713
top acc: 0.0143 ::: bot acc: 0.0591
top acc: 0.0159 ::: bot acc: 0.0478
top acc: 0.0273 ::: bot acc: 0.0491
current epoch: 48
train loss is 0.031922
average val loss: 0.020924, accuracy: 0.0210
average test loss: 0.026787, accuracy: 0.0267
case acc: 0.027590366
case acc: 0.019659104
case acc: 0.035116673
case acc: 0.025288045
case acc: 0.024085704
case acc: 0.028357014
top acc: 0.0211 ::: bot acc: 0.0463
top acc: 0.0158 ::: bot acc: 0.0344
top acc: 0.0273 ::: bot acc: 0.0622
top acc: 0.0171 ::: bot acc: 0.0455
top acc: 0.0228 ::: bot acc: 0.0382
top acc: 0.0340 ::: bot acc: 0.0414
current epoch: 49
train loss is 0.031395
average val loss: 0.020828, accuracy: 0.0208
average test loss: 0.025583, accuracy: 0.0255
case acc: 0.02408118
case acc: 0.021438174
case acc: 0.033290554
case acc: 0.023483356
case acc: 0.023975076
case acc: 0.026805503
top acc: 0.0420 ::: bot acc: 0.0227
top acc: 0.0385 ::: bot acc: 0.0119
top acc: 0.0460 ::: bot acc: 0.0434
top acc: 0.0384 ::: bot acc: 0.0239
top acc: 0.0454 ::: bot acc: 0.0159
top acc: 0.0486 ::: bot acc: 0.0268
current epoch: 50
train loss is 0.031353
average val loss: 0.020274, accuracy: 0.0203
average test loss: 0.025191, accuracy: 0.0252
case acc: 0.023908297
case acc: 0.021409048
case acc: 0.0332128
case acc: 0.02250105
case acc: 0.023908343
case acc: 0.026497524
top acc: 0.0413 ::: bot acc: 0.0234
top acc: 0.0384 ::: bot acc: 0.0119
top acc: 0.0441 ::: bot acc: 0.0453
top acc: 0.0335 ::: bot acc: 0.0288
top acc: 0.0452 ::: bot acc: 0.0159
top acc: 0.0455 ::: bot acc: 0.0299
LME_Co_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6768 6768 6768
1.7082474 -0.6288155 0.21141115 -0.19947179
Validation: 756 756 756
Testing: 774 774 774
pre-processing time: 0.0005681514739990234
the split date is 2010-07-01
net initializing with time: 0.003545522689819336
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.319601
average val loss: 0.164273, accuracy: 0.1646
average test loss: 0.165699, accuracy: 0.1659
case acc: 0.22289208
case acc: 0.36785468
case acc: 0.08202596
case acc: 0.14949138
case acc: 0.11697389
case acc: 0.055911716
top acc: 0.1942 ::: bot acc: 0.2518
top acc: 0.3409 ::: bot acc: 0.3950
top acc: 0.0449 ::: bot acc: 0.1176
top acc: 0.1055 ::: bot acc: 0.1911
top acc: 0.0740 ::: bot acc: 0.1539
top acc: 0.0243 ::: bot acc: 0.0868
current epoch: 2
train loss is 0.207701
average val loss: 0.105817, accuracy: 0.1051
average test loss: 0.105643, accuracy: 0.1048
case acc: 0.02226312
case acc: 0.13160543
case acc: 0.14053017
case acc: 0.07330315
case acc: 0.09853608
case acc: 0.16273664
top acc: 0.0342 ::: bot acc: 0.0232
top acc: 0.1047 ::: bot acc: 0.1588
top acc: 0.1788 ::: bot acc: 0.1044
top acc: 0.1158 ::: bot acc: 0.0343
top acc: 0.1416 ::: bot acc: 0.0615
top acc: 0.2024 ::: bot acc: 0.1277
current epoch: 3
train loss is 0.078527
average val loss: 0.139421, accuracy: 0.1401
average test loss: 0.141023, accuracy: 0.1412
case acc: 0.19238429
case acc: 0.3171947
case acc: 0.061215147
case acc: 0.12570263
case acc: 0.10384805
case acc: 0.046807457
top acc: 0.1637 ::: bot acc: 0.2214
top acc: 0.2902 ::: bot acc: 0.3445
top acc: 0.0262 ::: bot acc: 0.0960
top acc: 0.0816 ::: bot acc: 0.1672
top acc: 0.0621 ::: bot acc: 0.1399
top acc: 0.0198 ::: bot acc: 0.0753
current epoch: 4
train loss is 0.115021
average val loss: 0.139073, accuracy: 0.1396
average test loss: 0.140699, accuracy: 0.1408
case acc: 0.18713664
case acc: 0.30795187
case acc: 0.06416611
case acc: 0.1272358
case acc: 0.10757082
case acc: 0.050913524
top acc: 0.1586 ::: bot acc: 0.2160
top acc: 0.2809 ::: bot acc: 0.3354
top acc: 0.0286 ::: bot acc: 0.0994
top acc: 0.0830 ::: bot acc: 0.1688
top acc: 0.0653 ::: bot acc: 0.1437
top acc: 0.0217 ::: bot acc: 0.0806
current epoch: 5
train loss is 0.114980
average val loss: 0.102466, accuracy: 0.1040
average test loss: 0.104241, accuracy: 0.1049
case acc: 0.14094736
case acc: 0.25782165
case acc: 0.035655655
case acc: 0.0892084
case acc: 0.075325854
case acc: 0.030329349
top acc: 0.1125 ::: bot acc: 0.1698
top acc: 0.2307 ::: bot acc: 0.2854
top acc: 0.0178 ::: bot acc: 0.0621
top acc: 0.0461 ::: bot acc: 0.1301
top acc: 0.0384 ::: bot acc: 0.1086
top acc: 0.0276 ::: bot acc: 0.0466
current epoch: 6
train loss is 0.103015
average val loss: 0.088629, accuracy: 0.0904
average test loss: 0.090478, accuracy: 0.0914
case acc: 0.11870375
case acc: 0.22983877
case acc: 0.029999046
case acc: 0.07493683
case acc: 0.066487044
case acc: 0.028151942
top acc: 0.0902 ::: bot acc: 0.1476
top acc: 0.2026 ::: bot acc: 0.2575
top acc: 0.0282 ::: bot acc: 0.0480
top acc: 0.0342 ::: bot acc: 0.1145
top acc: 0.0326 ::: bot acc: 0.0982
top acc: 0.0363 ::: bot acc: 0.0375
current epoch: 7
train loss is 0.089883
average val loss: 0.085566, accuracy: 0.0874
average test loss: 0.087437, accuracy: 0.0882
case acc: 0.111047104
case acc: 0.21577582
case acc: 0.029983455
case acc: 0.07368705
case acc: 0.069434434
case acc: 0.029098578
top acc: 0.0826 ::: bot acc: 0.1399
top acc: 0.1885 ::: bot acc: 0.2435
top acc: 0.0279 ::: bot acc: 0.0483
top acc: 0.0333 ::: bot acc: 0.1130
top acc: 0.0342 ::: bot acc: 0.1017
top acc: 0.0310 ::: bot acc: 0.0427
current epoch: 8
train loss is 0.083166
average val loss: 0.087121, accuracy: 0.0887
average test loss: 0.088913, accuracy: 0.0894
case acc: 0.109227926
case acc: 0.2082338
case acc: 0.032046836
case acc: 0.07760036
case acc: 0.07653224
case acc: 0.03273201
top acc: 0.0807 ::: bot acc: 0.1380
top acc: 0.1809 ::: bot acc: 0.2360
top acc: 0.0220 ::: bot acc: 0.0548
top acc: 0.0364 ::: bot acc: 0.1172
top acc: 0.0387 ::: bot acc: 0.1099
top acc: 0.0221 ::: bot acc: 0.0528
current epoch: 9
train loss is 0.081253
average val loss: 0.088554, accuracy: 0.0898
average test loss: 0.090230, accuracy: 0.0905
case acc: 0.10641518
case acc: 0.20147847
case acc: 0.034658168
case acc: 0.081025705
case acc: 0.08232091
case acc: 0.03723215
top acc: 0.0778 ::: bot acc: 0.1352
top acc: 0.1742 ::: bot acc: 0.2292
top acc: 0.0183 ::: bot acc: 0.0608
top acc: 0.0391 ::: bot acc: 0.1210
top acc: 0.0431 ::: bot acc: 0.1164
top acc: 0.0186 ::: bot acc: 0.0613
current epoch: 10
train loss is 0.084338
average val loss: 0.100950, accuracy: 0.1016
average test loss: 0.102497, accuracy: 0.1025
case acc: 0.1150203
case acc: 0.20838906
case acc: 0.04568262
case acc: 0.09602027
case acc: 0.09873313
case acc: 0.051187206
top acc: 0.0863 ::: bot acc: 0.1439
top acc: 0.1810 ::: bot acc: 0.2362
top acc: 0.0165 ::: bot acc: 0.0783
top acc: 0.0521 ::: bot acc: 0.1370
top acc: 0.0571 ::: bot acc: 0.1340
top acc: 0.0217 ::: bot acc: 0.0807
current epoch: 11
train loss is 0.079979
average val loss: 0.084651, accuracy: 0.0858
average test loss: 0.086342, accuracy: 0.0865
case acc: 0.09151466
case acc: 0.18366265
case acc: 0.036024515
case acc: 0.08105353
case acc: 0.0851672
case acc: 0.04152913
top acc: 0.0627 ::: bot acc: 0.1204
top acc: 0.1562 ::: bot acc: 0.2115
top acc: 0.0171 ::: bot acc: 0.0636
top acc: 0.0392 ::: bot acc: 0.1209
top acc: 0.0453 ::: bot acc: 0.1195
top acc: 0.0184 ::: bot acc: 0.0678
current epoch: 12
train loss is 0.074608
average val loss: 0.080834, accuracy: 0.0820
average test loss: 0.082566, accuracy: 0.0827
case acc: 0.0820159
case acc: 0.17283367
case acc: 0.035451476
case acc: 0.07901366
case acc: 0.084455356
case acc: 0.042158917
top acc: 0.0532 ::: bot acc: 0.1110
top acc: 0.1454 ::: bot acc: 0.2006
top acc: 0.0174 ::: bot acc: 0.0625
top acc: 0.0376 ::: bot acc: 0.1187
top acc: 0.0447 ::: bot acc: 0.1187
top acc: 0.0185 ::: bot acc: 0.0687
current epoch: 13
train loss is 0.072496
average val loss: 0.079271, accuracy: 0.0803
average test loss: 0.081026, accuracy: 0.0810
case acc: 0.075153865
case acc: 0.16492538
case acc: 0.03621236
case acc: 0.07933776
case acc: 0.08597296
case acc: 0.044529058
top acc: 0.0463 ::: bot acc: 0.1041
top acc: 0.1374 ::: bot acc: 0.1927
top acc: 0.0168 ::: bot acc: 0.0640
top acc: 0.0379 ::: bot acc: 0.1191
top acc: 0.0460 ::: bot acc: 0.1203
top acc: 0.0191 ::: bot acc: 0.0720
current epoch: 14
train loss is 0.067730
average val loss: 0.070285, accuracy: 0.0716
average test loss: 0.072166, accuracy: 0.0721
case acc: 0.059866488
case acc: 0.14833842
case acc: 0.032526664
case acc: 0.07167593
case acc: 0.07949156
case acc: 0.040535044
top acc: 0.0314 ::: bot acc: 0.0887
top acc: 0.1208 ::: bot acc: 0.1762
top acc: 0.0204 ::: bot acc: 0.0568
top acc: 0.0319 ::: bot acc: 0.1106
top acc: 0.0408 ::: bot acc: 0.1131
top acc: 0.0182 ::: bot acc: 0.0665
current epoch: 15
train loss is 0.062700
average val loss: 0.065305, accuracy: 0.0667
average test loss: 0.067245, accuracy: 0.0670
case acc: 0.049932614
case acc: 0.13617384
case acc: 0.031395786
case acc: 0.067936294
case acc: 0.077014334
case acc: 0.03974847
top acc: 0.0225 ::: bot acc: 0.0781
top acc: 0.1086 ::: bot acc: 0.1641
top acc: 0.0227 ::: bot acc: 0.0539
top acc: 0.0290 ::: bot acc: 0.1065
top acc: 0.0389 ::: bot acc: 0.1103
top acc: 0.0181 ::: bot acc: 0.0654
current epoch: 16
train loss is 0.058614
average val loss: 0.057818, accuracy: 0.0594
average test loss: 0.059798, accuracy: 0.0596
case acc: 0.038257856
case acc: 0.12036106
case acc: 0.029604571
case acc: 0.0612876
case acc: 0.07130623
case acc: 0.036714215
top acc: 0.0146 ::: bot acc: 0.0646
top acc: 0.0928 ::: bot acc: 0.1483
top acc: 0.0292 ::: bot acc: 0.0475
top acc: 0.0249 ::: bot acc: 0.0986
top acc: 0.0350 ::: bot acc: 0.1037
top acc: 0.0188 ::: bot acc: 0.0605
current epoch: 17
train loss is 0.053230
average val loss: 0.050432, accuracy: 0.0520
average test loss: 0.052380, accuracy: 0.0522
case acc: 0.028678453
case acc: 0.103438176
case acc: 0.028471617
case acc: 0.05428358
case acc: 0.06483839
case acc: 0.033432998
top acc: 0.0124 ::: bot acc: 0.0513
top acc: 0.0759 ::: bot acc: 0.1313
top acc: 0.0365 ::: bot acc: 0.0401
top acc: 0.0221 ::: bot acc: 0.0895
top acc: 0.0312 ::: bot acc: 0.0958
top acc: 0.0208 ::: bot acc: 0.0545
current epoch: 18
train loss is 0.048136
average val loss: 0.041448, accuracy: 0.0424
average test loss: 0.043362, accuracy: 0.0433
case acc: 0.021786608
case acc: 0.080896325
case acc: 0.028857317
case acc: 0.044355262
case acc: 0.05472469
case acc: 0.029148757
top acc: 0.0229 ::: bot acc: 0.0348
top acc: 0.0533 ::: bot acc: 0.1087
top acc: 0.0487 ::: bot acc: 0.0279
top acc: 0.0217 ::: bot acc: 0.0748
top acc: 0.0272 ::: bot acc: 0.0827
top acc: 0.0301 ::: bot acc: 0.0433
current epoch: 19
train loss is 0.042979
average val loss: 0.035384, accuracy: 0.0355
average test loss: 0.036985, accuracy: 0.0370
case acc: 0.021817755
case acc: 0.05832071
case acc: 0.032201137
case acc: 0.03644202
case acc: 0.04557827
case acc: 0.027516268
top acc: 0.0355 ::: bot acc: 0.0223
top acc: 0.0313 ::: bot acc: 0.0859
top acc: 0.0597 ::: bot acc: 0.0174
top acc: 0.0271 ::: bot acc: 0.0602
top acc: 0.0246 ::: bot acc: 0.0702
top acc: 0.0403 ::: bot acc: 0.0331
current epoch: 20
train loss is 0.040885
average val loss: 0.031652, accuracy: 0.0308
average test loss: 0.032939, accuracy: 0.0326
case acc: 0.02551749
case acc: 0.034593094
case acc: 0.038962785
case acc: 0.031149935
case acc: 0.037437905
case acc: 0.027677104
top acc: 0.0465 ::: bot acc: 0.0125
top acc: 0.0114 ::: bot acc: 0.0602
top acc: 0.0713 ::: bot acc: 0.0144
top acc: 0.0415 ::: bot acc: 0.0439
top acc: 0.0262 ::: bot acc: 0.0572
top acc: 0.0508 ::: bot acc: 0.0226
current epoch: 21
train loss is 0.039656
average val loss: 0.030495, accuracy: 0.0291
average test loss: 0.031553, accuracy: 0.0311
case acc: 0.027651576
case acc: 0.021875652
case acc: 0.043822058
case acc: 0.031039098
case acc: 0.033863757
case acc: 0.028407138
top acc: 0.0505 ::: bot acc: 0.0111
top acc: 0.0171 ::: bot acc: 0.0382
top acc: 0.0777 ::: bot acc: 0.0160
top acc: 0.0528 ::: bot acc: 0.0326
top acc: 0.0298 ::: bot acc: 0.0500
top acc: 0.0553 ::: bot acc: 0.0181
current epoch: 22
train loss is 0.037602
average val loss: 0.032512, accuracy: 0.0311
average test loss: 0.032943, accuracy: 0.0329
case acc: 0.030045023
case acc: 0.022601094
case acc: 0.049464677
case acc: 0.034401298
case acc: 0.031043574
case acc: 0.029924858
top acc: 0.0542 ::: bot acc: 0.0109
top acc: 0.0399 ::: bot acc: 0.0154
top acc: 0.0848 ::: bot acc: 0.0186
top acc: 0.0645 ::: bot acc: 0.0211
top acc: 0.0355 ::: bot acc: 0.0429
top acc: 0.0598 ::: bot acc: 0.0141
current epoch: 23
train loss is 0.035971
average val loss: 0.039826, accuracy: 0.0388
average test loss: 0.039521, accuracy: 0.0399
case acc: 0.0347448
case acc: 0.039296947
case acc: 0.058903445
case acc: 0.043111164
case acc: 0.028455919
case acc: 0.03461446
top acc: 0.0608 ::: bot acc: 0.0116
top acc: 0.0653 ::: bot acc: 0.0146
top acc: 0.0957 ::: bot acc: 0.0250
top acc: 0.0797 ::: bot acc: 0.0167
top acc: 0.0461 ::: bot acc: 0.0322
top acc: 0.0681 ::: bot acc: 0.0115
current epoch: 24
train loss is 0.037446
average val loss: 0.054346, accuracy: 0.0538
average test loss: 0.053478, accuracy: 0.0536
case acc: 0.044002727
case acc: 0.06704264
case acc: 0.074843355
case acc: 0.05920921
case acc: 0.030311126
case acc: 0.046442408
top acc: 0.0721 ::: bot acc: 0.0169
top acc: 0.0945 ::: bot acc: 0.0394
top acc: 0.1127 ::: bot acc: 0.0387
top acc: 0.1006 ::: bot acc: 0.0232
top acc: 0.0626 ::: bot acc: 0.0158
top acc: 0.0828 ::: bot acc: 0.0175
current epoch: 25
train loss is 0.044987
average val loss: 0.075950, accuracy: 0.0756
average test loss: 0.074904, accuracy: 0.0748
case acc: 0.05873743
case acc: 0.09821525
case acc: 0.097421795
case acc: 0.08350105
case acc: 0.044621646
case acc: 0.06612118
top acc: 0.0876 ::: bot acc: 0.0301
top acc: 0.1257 ::: bot acc: 0.0706
top acc: 0.1360 ::: bot acc: 0.0598
top acc: 0.1269 ::: bot acc: 0.0433
top acc: 0.0855 ::: bot acc: 0.0131
top acc: 0.1044 ::: bot acc: 0.0332
current epoch: 26
train loss is 0.059697
average val loss: 0.067678, accuracy: 0.0673
average test loss: 0.066739, accuracy: 0.0666
case acc: 0.04266444
case acc: 0.09048251
case acc: 0.08955524
case acc: 0.0780714
case acc: 0.03949235
case acc: 0.059496555
top acc: 0.0705 ::: bot acc: 0.0160
top acc: 0.1180 ::: bot acc: 0.0628
top acc: 0.1280 ::: bot acc: 0.0523
top acc: 0.1211 ::: bot acc: 0.0386
top acc: 0.0784 ::: bot acc: 0.0117
top acc: 0.0973 ::: bot acc: 0.0274
current epoch: 27
train loss is 0.065158
average val loss: 0.042854, accuracy: 0.0418
average test loss: 0.042663, accuracy: 0.0427
case acc: 0.021193214
case acc: 0.05478651
case acc: 0.060892712
case acc: 0.053624745
case acc: 0.028002435
case acc: 0.037610304
top acc: 0.0328 ::: bot acc: 0.0247
top acc: 0.0822 ::: bot acc: 0.0274
top acc: 0.0979 ::: bot acc: 0.0265
top acc: 0.0938 ::: bot acc: 0.0199
top acc: 0.0527 ::: bot acc: 0.0253
top acc: 0.0722 ::: bot acc: 0.0118
current epoch: 28
train loss is 0.063779
average val loss: 0.029666, accuracy: 0.0286
average test loss: 0.030934, accuracy: 0.0315
case acc: 0.03709056
case acc: 0.021877507
case acc: 0.034324557
case acc: 0.033572003
case acc: 0.03497952
case acc: 0.027346414
top acc: 0.0140 ::: bot acc: 0.0628
top acc: 0.0378 ::: bot acc: 0.0174
top acc: 0.0636 ::: bot acc: 0.0152
top acc: 0.0622 ::: bot acc: 0.0231
top acc: 0.0276 ::: bot acc: 0.0525
top acc: 0.0462 ::: bot acc: 0.0269
current epoch: 29
train loss is 0.060790
average val loss: 0.037443, accuracy: 0.0378
average test loss: 0.039473, accuracy: 0.0402
case acc: 0.061912507
case acc: 0.0365621
case acc: 0.028568963
case acc: 0.033483956
case acc: 0.049691256
case acc: 0.030844225
top acc: 0.0333 ::: bot acc: 0.0904
top acc: 0.0126 ::: bot acc: 0.0625
top acc: 0.0331 ::: bot acc: 0.0433
top acc: 0.0320 ::: bot acc: 0.0534
top acc: 0.0248 ::: bot acc: 0.0760
top acc: 0.0246 ::: bot acc: 0.0488
current epoch: 30
train loss is 0.052671
average val loss: 0.043527, accuracy: 0.0443
average test loss: 0.045517, accuracy: 0.0464
case acc: 0.06312938
case acc: 0.06055513
case acc: 0.031049818
case acc: 0.038806297
case acc: 0.053104747
case acc: 0.032031346
top acc: 0.0345 ::: bot acc: 0.0916
top acc: 0.0334 ::: bot acc: 0.0881
top acc: 0.0229 ::: bot acc: 0.0536
top acc: 0.0240 ::: bot acc: 0.0654
top acc: 0.0260 ::: bot acc: 0.0805
top acc: 0.0219 ::: bot acc: 0.0520
current epoch: 31
train loss is 0.051939
average val loss: 0.059016, accuracy: 0.0596
average test loss: 0.060776, accuracy: 0.0613
case acc: 0.07356421
case acc: 0.09110657
case acc: 0.04218362
case acc: 0.052906837
case acc: 0.066385075
case acc: 0.04163712
top acc: 0.0448 ::: bot acc: 0.1021
top acc: 0.0636 ::: bot acc: 0.1188
top acc: 0.0149 ::: bot acc: 0.0743
top acc: 0.0218 ::: bot acc: 0.0877
top acc: 0.0320 ::: bot acc: 0.0974
top acc: 0.0184 ::: bot acc: 0.0682
current epoch: 32
train loss is 0.056420
average val loss: 0.063429, accuracy: 0.0639
average test loss: 0.065181, accuracy: 0.0655
case acc: 0.070497856
case acc: 0.09877073
case acc: 0.04755363
case acc: 0.058541484
case acc: 0.07123423
case acc: 0.04619467
top acc: 0.0417 ::: bot acc: 0.0990
top acc: 0.0713 ::: bot acc: 0.1265
top acc: 0.0171 ::: bot acc: 0.0813
top acc: 0.0238 ::: bot acc: 0.0952
top acc: 0.0349 ::: bot acc: 0.1033
top acc: 0.0195 ::: bot acc: 0.0745
current epoch: 33
train loss is 0.049209
average val loss: 0.044862, accuracy: 0.0457
average test loss: 0.046875, accuracy: 0.0474
case acc: 0.042674754
case acc: 0.073203795
case acc: 0.03431443
case acc: 0.0442818
case acc: 0.055658467
case acc: 0.034110513
top acc: 0.0174 ::: bot acc: 0.0695
top acc: 0.0457 ::: bot acc: 0.1009
top acc: 0.0175 ::: bot acc: 0.0612
top acc: 0.0217 ::: bot acc: 0.0748
top acc: 0.0269 ::: bot acc: 0.0839
top acc: 0.0196 ::: bot acc: 0.0563
current epoch: 34
train loss is 0.041788
average val loss: 0.039606, accuracy: 0.0405
average test loss: 0.041663, accuracy: 0.0420
case acc: 0.032471187
case acc: 0.061889738
case acc: 0.032015607
case acc: 0.04084333
case acc: 0.05231739
case acc: 0.032560542
top acc: 0.0123 ::: bot acc: 0.0567
top acc: 0.0347 ::: bot acc: 0.0894
top acc: 0.0203 ::: bot acc: 0.0563
top acc: 0.0229 ::: bot acc: 0.0691
top acc: 0.0257 ::: bot acc: 0.0795
top acc: 0.0211 ::: bot acc: 0.0532
current epoch: 35
train loss is 0.037906
average val loss: 0.034980, accuracy: 0.0358
average test loss: 0.037108, accuracy: 0.0373
case acc: 0.025711125
case acc: 0.051259495
case acc: 0.030393953
case acc: 0.03744958
case acc: 0.048300438
case acc: 0.030903943
top acc: 0.0138 ::: bot acc: 0.0458
top acc: 0.0247 ::: bot acc: 0.0785
top acc: 0.0252 ::: bot acc: 0.0513
top acc: 0.0253 ::: bot acc: 0.0628
top acc: 0.0244 ::: bot acc: 0.0741
top acc: 0.0242 ::: bot acc: 0.0492
current epoch: 36
train loss is 0.034642
average val loss: 0.030212, accuracy: 0.0307
average test loss: 0.032358, accuracy: 0.0324
case acc: 0.021542385
case acc: 0.03920873
case acc: 0.028714739
case acc: 0.033569224
case acc: 0.04265253
case acc: 0.028626254
top acc: 0.0222 ::: bot acc: 0.0350
top acc: 0.0146 ::: bot acc: 0.0655
top acc: 0.0324 ::: bot acc: 0.0442
top acc: 0.0316 ::: bot acc: 0.0539
top acc: 0.0239 ::: bot acc: 0.0659
top acc: 0.0308 ::: bot acc: 0.0423
current epoch: 37
train loss is 0.031766
average val loss: 0.026989, accuracy: 0.0271
average test loss: 0.028993, accuracy: 0.0290
case acc: 0.020927355
case acc: 0.028649878
case acc: 0.028183863
case acc: 0.031102398
case acc: 0.03748646
case acc: 0.027520742
top acc: 0.0301 ::: bot acc: 0.0270
top acc: 0.0097 ::: bot acc: 0.0521
top acc: 0.0395 ::: bot acc: 0.0370
top acc: 0.0409 ::: bot acc: 0.0443
top acc: 0.0257 ::: bot acc: 0.0573
top acc: 0.0381 ::: bot acc: 0.0351
current epoch: 38
train loss is 0.030249
average val loss: 0.025543, accuracy: 0.0252
average test loss: 0.027331, accuracy: 0.0273
case acc: 0.021326242
case acc: 0.022334272
case acc: 0.028319066
case acc: 0.030586448
case acc: 0.033803266
case acc: 0.027293043
top acc: 0.0344 ::: bot acc: 0.0227
top acc: 0.0157 ::: bot acc: 0.0396
top acc: 0.0454 ::: bot acc: 0.0312
top acc: 0.0495 ::: bot acc: 0.0357
top acc: 0.0295 ::: bot acc: 0.0499
top acc: 0.0441 ::: bot acc: 0.0290
current epoch: 39
train loss is 0.029342
average val loss: 0.025407, accuracy: 0.0248
average test loss: 0.026930, accuracy: 0.0269
case acc: 0.021537382
case acc: 0.02018533
case acc: 0.029012632
case acc: 0.031878456
case acc: 0.03149454
case acc: 0.027467161
top acc: 0.0356 ::: bot acc: 0.0215
top acc: 0.0261 ::: bot acc: 0.0290
top acc: 0.0494 ::: bot acc: 0.0271
top acc: 0.0564 ::: bot acc: 0.0288
top acc: 0.0337 ::: bot acc: 0.0443
top acc: 0.0485 ::: bot acc: 0.0247
current epoch: 40
train loss is 0.028808
average val loss: 0.026272, accuracy: 0.0257
average test loss: 0.027373, accuracy: 0.0275
case acc: 0.021668062
case acc: 0.021269538
case acc: 0.03009604
case acc: 0.03382497
case acc: 0.029865026
case acc: 0.027980085
top acc: 0.0362 ::: bot acc: 0.0209
top acc: 0.0357 ::: bot acc: 0.0195
top acc: 0.0533 ::: bot acc: 0.0232
top acc: 0.0628 ::: bot acc: 0.0224
top acc: 0.0389 ::: bot acc: 0.0391
top acc: 0.0526 ::: bot acc: 0.0206
current epoch: 41
train loss is 0.028892
average val loss: 0.027375, accuracy: 0.0267
average test loss: 0.028156, accuracy: 0.0283
case acc: 0.021488847
case acc: 0.023789302
case acc: 0.030945841
case acc: 0.035930034
case acc: 0.02886109
case acc: 0.028608138
top acc: 0.0353 ::: bot acc: 0.0218
top acc: 0.0425 ::: bot acc: 0.0137
top acc: 0.0559 ::: bot acc: 0.0206
top acc: 0.0676 ::: bot acc: 0.0191
top acc: 0.0428 ::: bot acc: 0.0351
top acc: 0.0557 ::: bot acc: 0.0174
current epoch: 42
train loss is 0.029042
average val loss: 0.028269, accuracy: 0.0276
average test loss: 0.028889, accuracy: 0.0290
case acc: 0.02120957
case acc: 0.026052617
case acc: 0.03150243
case acc: 0.037703764
case acc: 0.028386611
case acc: 0.029258937
top acc: 0.0336 ::: bot acc: 0.0235
top acc: 0.0468 ::: bot acc: 0.0119
top acc: 0.0574 ::: bot acc: 0.0192
top acc: 0.0709 ::: bot acc: 0.0178
top acc: 0.0456 ::: bot acc: 0.0324
top acc: 0.0578 ::: bot acc: 0.0153
current epoch: 43
train loss is 0.029354
average val loss: 0.029079, accuracy: 0.0284
average test loss: 0.029566, accuracy: 0.0296
case acc: 0.021021776
case acc: 0.027391346
case acc: 0.03201381
case acc: 0.03918384
case acc: 0.028133217
case acc: 0.030095555
top acc: 0.0324 ::: bot acc: 0.0248
top acc: 0.0493 ::: bot acc: 0.0110
top acc: 0.0587 ::: bot acc: 0.0181
top acc: 0.0735 ::: bot acc: 0.0171
top acc: 0.0480 ::: bot acc: 0.0299
top acc: 0.0599 ::: bot acc: 0.0136
current epoch: 44
train loss is 0.029585
average val loss: 0.029088, accuracy: 0.0284
average test loss: 0.029598, accuracy: 0.0297
case acc: 0.020897407
case acc: 0.02709276
case acc: 0.03186829
case acc: 0.039551053
case acc: 0.028093519
case acc: 0.030443514
top acc: 0.0297 ::: bot acc: 0.0274
top acc: 0.0487 ::: bot acc: 0.0112
top acc: 0.0583 ::: bot acc: 0.0184
top acc: 0.0741 ::: bot acc: 0.0169
top acc: 0.0489 ::: bot acc: 0.0290
top acc: 0.0606 ::: bot acc: 0.0132
current epoch: 45
train loss is 0.029644
average val loss: 0.028552, accuracy: 0.0279
average test loss: 0.029167, accuracy: 0.0292
case acc: 0.021045193
case acc: 0.025557684
case acc: 0.031248437
case acc: 0.039057903
case acc: 0.028079396
case acc: 0.03033873
top acc: 0.0265 ::: bot acc: 0.0306
top acc: 0.0459 ::: bot acc: 0.0123
top acc: 0.0566 ::: bot acc: 0.0199
top acc: 0.0733 ::: bot acc: 0.0172
top acc: 0.0488 ::: bot acc: 0.0291
top acc: 0.0604 ::: bot acc: 0.0133
current epoch: 46
train loss is 0.029380
average val loss: 0.027450, accuracy: 0.0268
average test loss: 0.028312, accuracy: 0.0283
case acc: 0.021496529
case acc: 0.022795934
case acc: 0.030136125
case acc: 0.037478566
case acc: 0.02816864
case acc: 0.029586138
top acc: 0.0223 ::: bot acc: 0.0348
top acc: 0.0404 ::: bot acc: 0.0149
top acc: 0.0532 ::: bot acc: 0.0233
top acc: 0.0705 ::: bot acc: 0.0179
top acc: 0.0472 ::: bot acc: 0.0307
top acc: 0.0588 ::: bot acc: 0.0144
current epoch: 47
train loss is 0.029520
average val loss: 0.026430, accuracy: 0.0259
average test loss: 0.027610, accuracy: 0.0275
case acc: 0.022525975
case acc: 0.020711549
case acc: 0.028981032
case acc: 0.035599004
case acc: 0.02838565
case acc: 0.028985426
top acc: 0.0186 ::: bot acc: 0.0385
top acc: 0.0332 ::: bot acc: 0.0220
top acc: 0.0491 ::: bot acc: 0.0273
top acc: 0.0670 ::: bot acc: 0.0193
top acc: 0.0454 ::: bot acc: 0.0325
top acc: 0.0570 ::: bot acc: 0.0161
current epoch: 48
train loss is 0.029562
average val loss: 0.025752, accuracy: 0.0254
average test loss: 0.027257, accuracy: 0.0272
case acc: 0.024203131
case acc: 0.020413106
case acc: 0.028158925
case acc: 0.03353789
case acc: 0.028860746
case acc: 0.028260227
top acc: 0.0152 ::: bot acc: 0.0427
top acc: 0.0238 ::: bot acc: 0.0313
top acc: 0.0436 ::: bot acc: 0.0328
top acc: 0.0620 ::: bot acc: 0.0230
top acc: 0.0427 ::: bot acc: 0.0351
top acc: 0.0543 ::: bot acc: 0.0188
current epoch: 49
train loss is 0.030054
average val loss: 0.025979, accuracy: 0.0258
average test loss: 0.027871, accuracy: 0.0280
case acc: 0.027047798
case acc: 0.023956975
case acc: 0.028260125
case acc: 0.031504218
case acc: 0.029903242
case acc: 0.027613377
top acc: 0.0130 ::: bot acc: 0.0481
top acc: 0.0128 ::: bot acc: 0.0434
top acc: 0.0362 ::: bot acc: 0.0403
top acc: 0.0551 ::: bot acc: 0.0299
top acc: 0.0386 ::: bot acc: 0.0392
top acc: 0.0501 ::: bot acc: 0.0229
current epoch: 50
train loss is 0.031859
average val loss: 0.028565, accuracy: 0.0289
average test loss: 0.030726, accuracy: 0.0312
case acc: 0.0320654
case acc: 0.03481349
case acc: 0.030345608
case acc: 0.030519763
case acc: 0.03227013
case acc: 0.027279787
top acc: 0.0124 ::: bot acc: 0.0559
top acc: 0.0116 ::: bot acc: 0.0603
top acc: 0.0250 ::: bot acc: 0.0515
top acc: 0.0443 ::: bot acc: 0.0407
top acc: 0.0317 ::: bot acc: 0.0464
top acc: 0.0428 ::: bot acc: 0.0302
LME_Co_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6798 6798 6798
1.7082474 -0.6288155 0.21141115 -0.19947179
Validation: 756 756 756
Testing: 750 750 750
pre-processing time: 0.0007524490356445312
the split date is 2011-01-01
net initializing with time: 0.004990816116333008
preparing training and testing date with time: 4.76837158203125e-07
current epoch: 1
train loss is 0.290161
average val loss: 0.104450, accuracy: 0.1051
average test loss: 0.093339, accuracy: 0.0941
case acc: 0.17710792
case acc: 0.15081564
case acc: 0.058775615
case acc: 0.046584565
case acc: 0.045150492
case acc: 0.08623375
top acc: 0.1454 ::: bot acc: 0.2101
top acc: 0.1237 ::: bot acc: 0.1789
top acc: 0.0244 ::: bot acc: 0.0994
top acc: 0.0168 ::: bot acc: 0.0794
top acc: 0.0153 ::: bot acc: 0.0802
top acc: 0.0483 ::: bot acc: 0.1227
current epoch: 2
train loss is 0.101481
average val loss: 0.066355, accuracy: 0.0680
average test loss: 0.058085, accuracy: 0.0607
case acc: 0.124060236
case acc: 0.10078644
case acc: 0.035104625
case acc: 0.0273958
case acc: 0.029734502
case acc: 0.046993498
top acc: 0.0922 ::: bot acc: 0.1572
top acc: 0.0737 ::: bot acc: 0.1288
top acc: 0.0313 ::: bot acc: 0.0605
top acc: 0.0270 ::: bot acc: 0.0452
top acc: 0.0371 ::: bot acc: 0.0441
top acc: 0.0198 ::: bot acc: 0.0783
current epoch: 3
train loss is 0.095334
average val loss: 0.097248, accuracy: 0.0977
average test loss: 0.085864, accuracy: 0.0865
case acc: 0.15066276
case acc: 0.13237143
case acc: 0.058157705
case acc: 0.055635285
case acc: 0.050241064
case acc: 0.07195784
top acc: 0.1188 ::: bot acc: 0.1839
top acc: 0.1053 ::: bot acc: 0.1603
top acc: 0.0242 ::: bot acc: 0.0985
top acc: 0.0229 ::: bot acc: 0.0899
top acc: 0.0162 ::: bot acc: 0.0874
top acc: 0.0351 ::: bot acc: 0.1080
current epoch: 4
train loss is 0.089312
average val loss: 0.078826, accuracy: 0.0796
average test loss: 0.067921, accuracy: 0.0691
case acc: 0.12134912
case acc: 0.10782359
case acc: 0.045490414
case acc: 0.04730636
case acc: 0.042324588
case acc: 0.05023618
top acc: 0.0893 ::: bot acc: 0.1547
top acc: 0.0808 ::: bot acc: 0.1357
top acc: 0.0199 ::: bot acc: 0.0817
top acc: 0.0172 ::: bot acc: 0.0802
top acc: 0.0154 ::: bot acc: 0.0760
top acc: 0.0211 ::: bot acc: 0.0826
current epoch: 5
train loss is 0.076625
average val loss: 0.078880, accuracy: 0.0794
average test loss: 0.067745, accuracy: 0.0687
case acc: 0.11170264
case acc: 0.10181693
case acc: 0.04825343
case acc: 0.055344958
case acc: 0.047359396
case acc: 0.04775633
top acc: 0.0797 ::: bot acc: 0.1451
top acc: 0.0748 ::: bot acc: 0.1297
top acc: 0.0204 ::: bot acc: 0.0855
top acc: 0.0226 ::: bot acc: 0.0895
top acc: 0.0155 ::: bot acc: 0.0835
top acc: 0.0199 ::: bot acc: 0.0795
current epoch: 6
train loss is 0.069434
average val loss: 0.077579, accuracy: 0.0780
average test loss: 0.066328, accuracy: 0.0671
case acc: 0.100400284
case acc: 0.09380578
case acc: 0.05029772
case acc: 0.06186112
case acc: 0.051049188
case acc: 0.04531566
top acc: 0.0683 ::: bot acc: 0.1338
top acc: 0.0668 ::: bot acc: 0.1218
top acc: 0.0210 ::: bot acc: 0.0882
top acc: 0.0274 ::: bot acc: 0.0968
top acc: 0.0163 ::: bot acc: 0.0887
top acc: 0.0192 ::: bot acc: 0.0763
current epoch: 7
train loss is 0.062105
average val loss: 0.074698, accuracy: 0.0751
average test loss: 0.063415, accuracy: 0.0641
case acc: 0.087323844
case acc: 0.083417825
case acc: 0.05138875
case acc: 0.066475324
case acc: 0.05297481
case acc: 0.04291441
top acc: 0.0553 ::: bot acc: 0.1208
top acc: 0.0563 ::: bot acc: 0.1115
top acc: 0.0214 ::: bot acc: 0.0896
top acc: 0.0313 ::: bot acc: 0.1017
top acc: 0.0170 ::: bot acc: 0.0912
top acc: 0.0188 ::: bot acc: 0.0730
current epoch: 8
train loss is 0.056789
average val loss: 0.068128, accuracy: 0.0684
average test loss: 0.057031, accuracy: 0.0575
case acc: 0.0702463
case acc: 0.06817756
case acc: 0.049699653
case acc: 0.06653382
case acc: 0.05108208
case acc: 0.039081693
top acc: 0.0383 ::: bot acc: 0.1037
top acc: 0.0410 ::: bot acc: 0.0963
top acc: 0.0208 ::: bot acc: 0.0875
top acc: 0.0314 ::: bot acc: 0.1016
top acc: 0.0161 ::: bot acc: 0.0889
top acc: 0.0184 ::: bot acc: 0.0675
current epoch: 9
train loss is 0.049876
average val loss: 0.061020, accuracy: 0.0610
average test loss: 0.050326, accuracy: 0.0503
case acc: 0.05312978
case acc: 0.05165261
case acc: 0.047732376
case acc: 0.06483711
case acc: 0.04804451
case acc: 0.03622243
top acc: 0.0222 ::: bot acc: 0.0861
top acc: 0.0247 ::: bot acc: 0.0798
top acc: 0.0201 ::: bot acc: 0.0848
top acc: 0.0299 ::: bot acc: 0.0997
top acc: 0.0152 ::: bot acc: 0.0848
top acc: 0.0186 ::: bot acc: 0.0631
current epoch: 10
train loss is 0.044510
average val loss: 0.055024, accuracy: 0.0546
average test loss: 0.044905, accuracy: 0.0441
case acc: 0.03856156
case acc: 0.036549684
case acc: 0.046452302
case acc: 0.06287252
case acc: 0.045268666
case acc: 0.03504611
top acc: 0.0104 ::: bot acc: 0.0701
top acc: 0.0113 ::: bot acc: 0.0640
top acc: 0.0198 ::: bot acc: 0.0831
top acc: 0.0283 ::: bot acc: 0.0976
top acc: 0.0146 ::: bot acc: 0.0809
top acc: 0.0191 ::: bot acc: 0.0612
current epoch: 11
train loss is 0.040977
average val loss: 0.049962, accuracy: 0.0491
average test loss: 0.040891, accuracy: 0.0396
case acc: 0.02896906
case acc: 0.025975639
case acc: 0.044947866
case acc: 0.06035523
case acc: 0.043118834
case acc: 0.034163896
top acc: 0.0103 ::: bot acc: 0.0559
top acc: 0.0096 ::: bot acc: 0.0492
top acc: 0.0195 ::: bot acc: 0.0809
top acc: 0.0263 ::: bot acc: 0.0948
top acc: 0.0146 ::: bot acc: 0.0777
top acc: 0.0198 ::: bot acc: 0.0596
current epoch: 12
train loss is 0.040794
average val loss: 0.043317, accuracy: 0.0423
average test loss: 0.036543, accuracy: 0.0350
case acc: 0.024273291
case acc: 0.020380462
case acc: 0.040886555
case acc: 0.05357202
case acc: 0.039264087
case acc: 0.031677116
top acc: 0.0256 ::: bot acc: 0.0395
top acc: 0.0253 ::: bot acc: 0.0310
top acc: 0.0207 ::: bot acc: 0.0743
top acc: 0.0215 ::: bot acc: 0.0871
top acc: 0.0160 ::: bot acc: 0.0712
top acc: 0.0227 ::: bot acc: 0.0545
current epoch: 13
train loss is 0.046335
average val loss: 0.034165, accuracy: 0.0345
average test loss: 0.032967, accuracy: 0.0320
case acc: 0.029862262
case acc: 0.03381639
case acc: 0.03354413
case acc: 0.03568046
case acc: 0.031300224
case acc: 0.027587773
top acc: 0.0533 ::: bot acc: 0.0136
top acc: 0.0576 ::: bot acc: 0.0128
top acc: 0.0365 ::: bot acc: 0.0546
top acc: 0.0133 ::: bot acc: 0.0644
top acc: 0.0308 ::: bot acc: 0.0519
top acc: 0.0378 ::: bot acc: 0.0385
current epoch: 14
train loss is 0.057677
average val loss: 0.049930, accuracy: 0.0506
average test loss: 0.059742, accuracy: 0.0597
case acc: 0.07955553
case acc: 0.09664489
case acc: 0.05043313
case acc: 0.036299057
case acc: 0.046677496
case acc: 0.04860395
top acc: 0.1113 ::: bot acc: 0.0463
top acc: 0.1244 ::: bot acc: 0.0676
top acc: 0.0901 ::: bot acc: 0.0162
top acc: 0.0662 ::: bot acc: 0.0128
top acc: 0.0844 ::: bot acc: 0.0138
top acc: 0.0846 ::: bot acc: 0.0170
current epoch: 15
train loss is 0.074917
average val loss: 0.088294, accuracy: 0.0884
average test loss: 0.099873, accuracy: 0.0999
case acc: 0.11766947
case acc: 0.14547664
case acc: 0.087712966
case acc: 0.08222619
case acc: 0.088472344
case acc: 0.07776307
top acc: 0.1493 ::: bot acc: 0.0845
top acc: 0.1733 ::: bot acc: 0.1163
top acc: 0.1322 ::: bot acc: 0.0437
top acc: 0.1177 ::: bot acc: 0.0471
top acc: 0.1303 ::: bot acc: 0.0473
top acc: 0.1165 ::: bot acc: 0.0405
current epoch: 16
train loss is 0.069825
average val loss: 0.065306, accuracy: 0.0654
average test loss: 0.076505, accuracy: 0.0767
case acc: 0.083769724
case acc: 0.11749463
case acc: 0.06747694
case acc: 0.06924724
case acc: 0.07287782
case acc: 0.04945496
top acc: 0.1154 ::: bot acc: 0.0505
top acc: 0.1454 ::: bot acc: 0.0880
top acc: 0.1102 ::: bot acc: 0.0269
top acc: 0.1044 ::: bot acc: 0.0348
top acc: 0.1144 ::: bot acc: 0.0322
top acc: 0.0856 ::: bot acc: 0.0175
current epoch: 17
train loss is 0.057916
average val loss: 0.056489, accuracy: 0.0563
average test loss: 0.067468, accuracy: 0.0677
case acc: 0.06363177
case acc: 0.09843337
case acc: 0.06325266
case acc: 0.06815502
case acc: 0.070003115
case acc: 0.042826407
top acc: 0.0950 ::: bot acc: 0.0308
top acc: 0.1264 ::: bot acc: 0.0687
top acc: 0.1054 ::: bot acc: 0.0237
top acc: 0.1032 ::: bot acc: 0.0339
top acc: 0.1115 ::: bot acc: 0.0295
top acc: 0.0770 ::: bot acc: 0.0150
current epoch: 18
train loss is 0.051723
average val loss: 0.049876, accuracy: 0.0496
average test loss: 0.060567, accuracy: 0.0608
case acc: 0.046625283
case acc: 0.07981817
case acc: 0.06186515
case acc: 0.06760851
case acc: 0.06781365
case acc: 0.0407706
top acc: 0.0769 ::: bot acc: 0.0159
top acc: 0.1078 ::: bot acc: 0.0500
top acc: 0.1038 ::: bot acc: 0.0228
top acc: 0.1026 ::: bot acc: 0.0334
top acc: 0.1092 ::: bot acc: 0.0276
top acc: 0.0742 ::: bot acc: 0.0145
current epoch: 19
train loss is 0.048124
average val loss: 0.040852, accuracy: 0.0403
average test loss: 0.050651, accuracy: 0.0509
case acc: 0.031231256
case acc: 0.057103153
case acc: 0.05681666
case acc: 0.061907902
case acc: 0.06140731
case acc: 0.03687112
top acc: 0.0555 ::: bot acc: 0.0128
top acc: 0.0845 ::: bot acc: 0.0283
top acc: 0.0980 ::: bot acc: 0.0193
top acc: 0.0967 ::: bot acc: 0.0283
top acc: 0.1022 ::: bot acc: 0.0224
top acc: 0.0687 ::: bot acc: 0.0141
current epoch: 20
train loss is 0.046824
average val loss: 0.030960, accuracy: 0.0305
average test loss: 0.037583, accuracy: 0.0380
case acc: 0.024160687
case acc: 0.031076755
case acc: 0.045830976
case acc: 0.047697928
case acc: 0.048351746
case acc: 0.030792713
top acc: 0.0278 ::: bot acc: 0.0373
top acc: 0.0532 ::: bot acc: 0.0131
top acc: 0.0836 ::: bot acc: 0.0154
top acc: 0.0809 ::: bot acc: 0.0175
top acc: 0.0869 ::: bot acc: 0.0140
top acc: 0.0564 ::: bot acc: 0.0206
current epoch: 21
train loss is 0.048597
average val loss: 0.033518, accuracy: 0.0341
average test loss: 0.030865, accuracy: 0.0320
case acc: 0.043870706
case acc: 0.026033636
case acc: 0.034144055
case acc: 0.027448699
case acc: 0.032039583
case acc: 0.028365752
top acc: 0.0147 ::: bot acc: 0.0763
top acc: 0.0096 ::: bot acc: 0.0503
top acc: 0.0540 ::: bot acc: 0.0369
top acc: 0.0475 ::: bot acc: 0.0232
top acc: 0.0563 ::: bot acc: 0.0262
top acc: 0.0321 ::: bot acc: 0.0448
current epoch: 22
train loss is 0.053561
average val loss: 0.067710, accuracy: 0.0684
average test loss: 0.056672, accuracy: 0.0578
case acc: 0.089379676
case acc: 0.078867294
case acc: 0.04564416
case acc: 0.04441973
case acc: 0.040317666
case acc: 0.048205987
top acc: 0.0577 ::: bot acc: 0.1231
top acc: 0.0510 ::: bot acc: 0.1087
top acc: 0.0195 ::: bot acc: 0.0820
top acc: 0.0160 ::: bot acc: 0.0760
top acc: 0.0148 ::: bot acc: 0.0733
top acc: 0.0200 ::: bot acc: 0.0807
current epoch: 23
train loss is 0.058846
average val loss: 0.079945, accuracy: 0.0803
average test loss: 0.068356, accuracy: 0.0690
case acc: 0.095192246
case acc: 0.094554044
case acc: 0.05612897
case acc: 0.06398944
case acc: 0.05269255
case acc: 0.051283322
top acc: 0.0635 ::: bot acc: 0.1290
top acc: 0.0668 ::: bot acc: 0.1243
top acc: 0.0228 ::: bot acc: 0.0960
top acc: 0.0291 ::: bot acc: 0.0986
top acc: 0.0165 ::: bot acc: 0.0908
top acc: 0.0213 ::: bot acc: 0.0846
current epoch: 24
train loss is 0.055884
average val loss: 0.059471, accuracy: 0.0599
average test loss: 0.049019, accuracy: 0.0496
case acc: 0.062523626
case acc: 0.06575445
case acc: 0.042362805
case acc: 0.051946454
case acc: 0.041308716
case acc: 0.033958588
top acc: 0.0309 ::: bot acc: 0.0962
top acc: 0.0381 ::: bot acc: 0.0954
top acc: 0.0200 ::: bot acc: 0.0769
top acc: 0.0201 ::: bot acc: 0.0851
top acc: 0.0144 ::: bot acc: 0.0749
top acc: 0.0203 ::: bot acc: 0.0593
current epoch: 25
train loss is 0.047677
average val loss: 0.047869, accuracy: 0.0479
average test loss: 0.038684, accuracy: 0.0387
case acc: 0.040122613
case acc: 0.041432478
case acc: 0.038284834
case acc: 0.04538743
case acc: 0.03643038
case acc: 0.030312376
top acc: 0.0116 ::: bot acc: 0.0723
top acc: 0.0151 ::: bot acc: 0.0704
top acc: 0.0239 ::: bot acc: 0.0690
top acc: 0.0163 ::: bot acc: 0.0773
top acc: 0.0184 ::: bot acc: 0.0657
top acc: 0.0258 ::: bot acc: 0.0512
current epoch: 26
train loss is 0.040597
average val loss: 0.038004, accuracy: 0.0376
average test loss: 0.031344, accuracy: 0.0307
case acc: 0.026423547
case acc: 0.023400916
case acc: 0.035244774
case acc: 0.037970185
case acc: 0.03234369
case acc: 0.028702568
top acc: 0.0159 ::: bot acc: 0.0497
top acc: 0.0127 ::: bot acc: 0.0447
top acc: 0.0298 ::: bot acc: 0.0615
top acc: 0.0137 ::: bot acc: 0.0676
top acc: 0.0269 ::: bot acc: 0.0555
top acc: 0.0306 ::: bot acc: 0.0462
current epoch: 27
train loss is 0.036570
average val loss: 0.030857, accuracy: 0.0306
average test loss: 0.028789, accuracy: 0.0282
case acc: 0.024894444
case acc: 0.023173517
case acc: 0.033193305
case acc: 0.030299447
case acc: 0.030052707
case acc: 0.027723514
top acc: 0.0378 ::: bot acc: 0.0278
top acc: 0.0386 ::: bot acc: 0.0187
top acc: 0.0402 ::: bot acc: 0.0510
top acc: 0.0177 ::: bot acc: 0.0542
top acc: 0.0391 ::: bot acc: 0.0433
top acc: 0.0376 ::: bot acc: 0.0391
current epoch: 28
train loss is 0.037203
average val loss: 0.029760, accuracy: 0.0305
average test loss: 0.034825, accuracy: 0.0348
case acc: 0.0373277
case acc: 0.04568834
case acc: 0.03573365
case acc: 0.026646012
case acc: 0.033433713
case acc: 0.030139852
top acc: 0.0652 ::: bot acc: 0.0117
top acc: 0.0719 ::: bot acc: 0.0191
top acc: 0.0617 ::: bot acc: 0.0295
top acc: 0.0423 ::: bot acc: 0.0287
top acc: 0.0609 ::: bot acc: 0.0216
top acc: 0.0546 ::: bot acc: 0.0222
current epoch: 29
train loss is 0.043435
average val loss: 0.045696, accuracy: 0.0460
average test loss: 0.056084, accuracy: 0.0563
case acc: 0.06258997
case acc: 0.081902966
case acc: 0.051702414
case acc: 0.0460641
case acc: 0.05229677
case acc: 0.043065745
top acc: 0.0941 ::: bot acc: 0.0295
top acc: 0.1096 ::: bot acc: 0.0523
top acc: 0.0917 ::: bot acc: 0.0168
top acc: 0.0790 ::: bot acc: 0.0166
top acc: 0.0917 ::: bot acc: 0.0161
top acc: 0.0774 ::: bot acc: 0.0148
current epoch: 30
train loss is 0.052694
average val loss: 0.059111, accuracy: 0.0591
average test loss: 0.070420, accuracy: 0.0706
case acc: 0.07035842
case acc: 0.09715197
case acc: 0.06663109
case acc: 0.06842116
case acc: 0.070194036
case acc: 0.050763857
top acc: 0.1021 ::: bot acc: 0.0369
top acc: 0.1249 ::: bot acc: 0.0675
top acc: 0.1093 ::: bot acc: 0.0262
top acc: 0.1037 ::: bot acc: 0.0340
top acc: 0.1117 ::: bot acc: 0.0299
top acc: 0.0872 ::: bot acc: 0.0181
current epoch: 31
train loss is 0.051495
average val loss: 0.040634, accuracy: 0.0405
average test loss: 0.050538, accuracy: 0.0508
case acc: 0.04042987
case acc: 0.06759779
case acc: 0.051077563
case acc: 0.055736378
case acc: 0.05586622
case acc: 0.034205455
top acc: 0.0696 ::: bot acc: 0.0123
top acc: 0.0951 ::: bot acc: 0.0383
top acc: 0.0908 ::: bot acc: 0.0166
top acc: 0.0902 ::: bot acc: 0.0230
top acc: 0.0960 ::: bot acc: 0.0183
top acc: 0.0639 ::: bot acc: 0.0154
current epoch: 32
train loss is 0.046803
average val loss: 0.028281, accuracy: 0.0284
average test loss: 0.034109, accuracy: 0.0347
case acc: 0.024593646
case acc: 0.035046596
case acc: 0.039784025
case acc: 0.039626673
case acc: 0.040845454
case acc: 0.02844577
top acc: 0.0357 ::: bot acc: 0.0298
top acc: 0.0588 ::: bot acc: 0.0136
top acc: 0.0723 ::: bot acc: 0.0203
top acc: 0.0709 ::: bot acc: 0.0135
top acc: 0.0759 ::: bot acc: 0.0136
top acc: 0.0468 ::: bot acc: 0.0299
current epoch: 33
train loss is 0.040098
average val loss: 0.028031, accuracy: 0.0282
average test loss: 0.028944, accuracy: 0.0293
case acc: 0.027293919
case acc: 0.020974746
case acc: 0.035555806
case acc: 0.030144695
case acc: 0.034029823
case acc: 0.027786674
top acc: 0.0135 ::: bot acc: 0.0522
top acc: 0.0298 ::: bot acc: 0.0277
top acc: 0.0611 ::: bot acc: 0.0300
top acc: 0.0556 ::: bot acc: 0.0159
top acc: 0.0624 ::: bot acc: 0.0205
top acc: 0.0407 ::: bot acc: 0.0360
current epoch: 34
train loss is 0.039720
average val loss: 0.039955, accuracy: 0.0407
average test loss: 0.033051, accuracy: 0.0341
case acc: 0.047193673
case acc: 0.03646184
case acc: 0.03336102
case acc: 0.026797816
case acc: 0.03024099
case acc: 0.030701503
top acc: 0.0173 ::: bot acc: 0.0800
top acc: 0.0112 ::: bot acc: 0.0650
top acc: 0.0378 ::: bot acc: 0.0533
top acc: 0.0265 ::: bot acc: 0.0443
top acc: 0.0376 ::: bot acc: 0.0450
top acc: 0.0249 ::: bot acc: 0.0523
current epoch: 35
train loss is 0.042545
average val loss: 0.059678, accuracy: 0.0602
average test loss: 0.049026, accuracy: 0.0500
case acc: 0.06701577
case acc: 0.066738375
case acc: 0.042800687
case acc: 0.044612728
case acc: 0.03942194
case acc: 0.03920661
top acc: 0.0352 ::: bot acc: 0.1007
top acc: 0.0390 ::: bot acc: 0.0964
top acc: 0.0198 ::: bot acc: 0.0777
top acc: 0.0160 ::: bot acc: 0.0763
top acc: 0.0156 ::: bot acc: 0.0716
top acc: 0.0183 ::: bot acc: 0.0682
current epoch: 36
train loss is 0.045847
average val loss: 0.059889, accuracy: 0.0602
average test loss: 0.049223, accuracy: 0.0498
case acc: 0.05850745
case acc: 0.06434037
case acc: 0.044617057
case acc: 0.052122623
case acc: 0.042405497
case acc: 0.036529254
top acc: 0.0271 ::: bot acc: 0.0920
top acc: 0.0367 ::: bot acc: 0.0939
top acc: 0.0195 ::: bot acc: 0.0805
top acc: 0.0203 ::: bot acc: 0.0854
top acc: 0.0143 ::: bot acc: 0.0767
top acc: 0.0188 ::: bot acc: 0.0639
current epoch: 37
train loss is 0.046105
average val loss: 0.039316, accuracy: 0.0393
average test loss: 0.032366, accuracy: 0.0320
case acc: 0.028671337
case acc: 0.031309403
case acc: 0.0345105
case acc: 0.037058044
case acc: 0.032409895
case acc: 0.027747082
top acc: 0.0109 ::: bot acc: 0.0555
top acc: 0.0088 ::: bot acc: 0.0583
top acc: 0.0322 ::: bot acc: 0.0591
top acc: 0.0136 ::: bot acc: 0.0663
top acc: 0.0271 ::: bot acc: 0.0555
top acc: 0.0366 ::: bot acc: 0.0402
current epoch: 38
train loss is 0.040024
average val loss: 0.028416, accuracy: 0.0284
average test loss: 0.028492, accuracy: 0.0280
case acc: 0.025691817
case acc: 0.021872059
case acc: 0.03367095
case acc: 0.027263941
case acc: 0.03032785
case acc: 0.029101769
top acc: 0.0419 ::: bot acc: 0.0237
top acc: 0.0348 ::: bot acc: 0.0224
top acc: 0.0495 ::: bot acc: 0.0417
top acc: 0.0250 ::: bot acc: 0.0460
top acc: 0.0474 ::: bot acc: 0.0352
top acc: 0.0505 ::: bot acc: 0.0263
current epoch: 39
train loss is 0.036855
average val loss: 0.029828, accuracy: 0.0302
average test loss: 0.036421, accuracy: 0.0363
case acc: 0.039196227
case acc: 0.04278916
case acc: 0.03788707
case acc: 0.02787518
case acc: 0.036467254
case acc: 0.033604935
top acc: 0.0679 ::: bot acc: 0.0120
top acc: 0.0685 ::: bot acc: 0.0172
top acc: 0.0679 ::: bot acc: 0.0236
top acc: 0.0491 ::: bot acc: 0.0220
top acc: 0.0681 ::: bot acc: 0.0162
top acc: 0.0628 ::: bot acc: 0.0160
current epoch: 40
train loss is 0.039600
average val loss: 0.040993, accuracy: 0.0413
average test loss: 0.051071, accuracy: 0.0512
case acc: 0.05476731
case acc: 0.06879325
case acc: 0.048404858
case acc: 0.04374645
case acc: 0.050543703
case acc: 0.04098564
top acc: 0.0861 ::: bot acc: 0.0222
top acc: 0.0963 ::: bot acc: 0.0396
top acc: 0.0874 ::: bot acc: 0.0157
top acc: 0.0761 ::: bot acc: 0.0153
top acc: 0.0896 ::: bot acc: 0.0151
top acc: 0.0745 ::: bot acc: 0.0143
current epoch: 41
train loss is 0.043208
average val loss: 0.046057, accuracy: 0.0461
average test loss: 0.056678, accuracy: 0.0569
case acc: 0.053468328
case acc: 0.073930755
case acc: 0.055141877
case acc: 0.056842037
case acc: 0.05969693
case acc: 0.042340796
top acc: 0.0848 ::: bot acc: 0.0211
top acc: 0.1015 ::: bot acc: 0.0445
top acc: 0.0961 ::: bot acc: 0.0184
top acc: 0.0914 ::: bot acc: 0.0238
top acc: 0.1003 ::: bot acc: 0.0212
top acc: 0.0764 ::: bot acc: 0.0147
current epoch: 42
train loss is 0.042241
average val loss: 0.033761, accuracy: 0.0337
average test loss: 0.042641, accuracy: 0.0429
case acc: 0.032167543
case acc: 0.0495022
case acc: 0.045456007
case acc: 0.04865645
case acc: 0.049589947
case acc: 0.03220876
top acc: 0.0575 ::: bot acc: 0.0121
top acc: 0.0762 ::: bot acc: 0.0219
top acc: 0.0830 ::: bot acc: 0.0156
top acc: 0.0820 ::: bot acc: 0.0180
top acc: 0.0885 ::: bot acc: 0.0146
top acc: 0.0598 ::: bot acc: 0.0177
current epoch: 43
train loss is 0.038915
average val loss: 0.027028, accuracy: 0.0271
average test loss: 0.030142, accuracy: 0.0306
case acc: 0.024447566
case acc: 0.023971794
case acc: 0.03667278
case acc: 0.034081686
case acc: 0.03652925
case acc: 0.028008929
top acc: 0.0248 ::: bot acc: 0.0407
top acc: 0.0407 ::: bot acc: 0.0169
top acc: 0.0646 ::: bot acc: 0.0265
top acc: 0.0625 ::: bot acc: 0.0135
top acc: 0.0683 ::: bot acc: 0.0162
top acc: 0.0432 ::: bot acc: 0.0336
current epoch: 44
train loss is 0.036668
average val loss: 0.035579, accuracy: 0.0361
average test loss: 0.030246, accuracy: 0.0311
case acc: 0.03908435
case acc: 0.028618181
case acc: 0.033053808
case acc: 0.025949776
case acc: 0.030188356
case acc: 0.02995142
top acc: 0.0108 ::: bot acc: 0.0710
top acc: 0.0087 ::: bot acc: 0.0544
top acc: 0.0422 ::: bot acc: 0.0489
top acc: 0.0354 ::: bot acc: 0.0355
top acc: 0.0437 ::: bot acc: 0.0390
top acc: 0.0263 ::: bot acc: 0.0505
current epoch: 45
train loss is 0.039269
average val loss: 0.053607, accuracy: 0.0542
average test loss: 0.043490, accuracy: 0.0445
case acc: 0.059248388
case acc: 0.05687649
case acc: 0.03986268
case acc: 0.03699419
case acc: 0.03627802
case acc: 0.037807554
top acc: 0.0277 ::: bot acc: 0.0928
top acc: 0.0293 ::: bot acc: 0.0864
top acc: 0.0218 ::: bot acc: 0.0723
top acc: 0.0136 ::: bot acc: 0.0662
top acc: 0.0192 ::: bot acc: 0.0651
top acc: 0.0185 ::: bot acc: 0.0660
current epoch: 46
train loss is 0.042323
average val loss: 0.050784, accuracy: 0.0511
average test loss: 0.041036, accuracy: 0.0416
case acc: 0.047961023
case acc: 0.05160855
case acc: 0.039226472
case acc: 0.040658154
case acc: 0.03691357
case acc: 0.033163793
top acc: 0.0178 ::: bot acc: 0.0809
top acc: 0.0243 ::: bot acc: 0.0810
top acc: 0.0225 ::: bot acc: 0.0710
top acc: 0.0142 ::: bot acc: 0.0714
top acc: 0.0182 ::: bot acc: 0.0666
top acc: 0.0211 ::: bot acc: 0.0578
current epoch: 47
train loss is 0.040373
average val loss: 0.035884, accuracy: 0.0359
average test loss: 0.030019, accuracy: 0.0297
case acc: 0.026690654
case acc: 0.026864462
case acc: 0.033654362
case acc: 0.031978663
case acc: 0.031186033
case acc: 0.02773241
top acc: 0.0154 ::: bot acc: 0.0504
top acc: 0.0091 ::: bot acc: 0.0516
top acc: 0.0363 ::: bot acc: 0.0550
top acc: 0.0156 ::: bot acc: 0.0578
top acc: 0.0317 ::: bot acc: 0.0509
top acc: 0.0373 ::: bot acc: 0.0394
current epoch: 48
train loss is 0.035227
average val loss: 0.027759, accuracy: 0.0278
average test loss: 0.028377, accuracy: 0.0280
case acc: 0.026081447
case acc: 0.02259377
case acc: 0.033789445
case acc: 0.026212221
case acc: 0.030502576
case acc: 0.028871698
top acc: 0.0432 ::: bot acc: 0.0225
top acc: 0.0370 ::: bot acc: 0.0201
top acc: 0.0508 ::: bot acc: 0.0404
top acc: 0.0301 ::: bot acc: 0.0409
top acc: 0.0488 ::: bot acc: 0.0338
top acc: 0.0491 ::: bot acc: 0.0276
current epoch: 49
train loss is 0.033665
average val loss: 0.030047, accuracy: 0.0305
average test loss: 0.037142, accuracy: 0.0371
case acc: 0.03964149
case acc: 0.04362756
case acc: 0.038625572
case acc: 0.029609218
case acc: 0.03741225
case acc: 0.0334832
top acc: 0.0686 ::: bot acc: 0.0121
top acc: 0.0694 ::: bot acc: 0.0178
top acc: 0.0697 ::: bot acc: 0.0221
top acc: 0.0542 ::: bot acc: 0.0172
top acc: 0.0700 ::: bot acc: 0.0153
top acc: 0.0625 ::: bot acc: 0.0160
current epoch: 50
train loss is 0.036787
average val loss: 0.039318, accuracy: 0.0396
average test loss: 0.049294, accuracy: 0.0494
case acc: 0.0508848
case acc: 0.063960485
case acc: 0.047654726
case acc: 0.045033094
case acc: 0.04962411
case acc: 0.039402105
top acc: 0.0821 ::: bot acc: 0.0189
top acc: 0.0913 ::: bot acc: 0.0350
top acc: 0.0864 ::: bot acc: 0.0155
top acc: 0.0776 ::: bot acc: 0.0161
top acc: 0.0885 ::: bot acc: 0.0147
top acc: 0.0724 ::: bot acc: 0.0139
LME_Co_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6774 6774 6774
1.7082474 -0.6288155 0.21141115 -0.19947179
Validation: 756 756 756
Testing: 768 768 768
pre-processing time: 0.0006296634674072266
the split date is 2011-07-01
net initializing with time: 0.004021644592285156
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.269090
average val loss: 0.187305, accuracy: 0.1892
average test loss: 0.187703, accuracy: 0.1892
case acc: 0.3496531
case acc: 0.2808354
case acc: 0.06078967
case acc: 0.10094236
case acc: 0.30114424
case acc: 0.041798137
top acc: 0.3829 ::: bot acc: 0.3188
top acc: 0.3115 ::: bot acc: 0.2519
top acc: 0.0263 ::: bot acc: 0.0989
top acc: 0.1271 ::: bot acc: 0.0735
top acc: 0.3330 ::: bot acc: 0.2665
top acc: 0.0172 ::: bot acc: 0.0737
current epoch: 2
train loss is 0.269362
average val loss: 0.150917, accuracy: 0.1516
average test loss: 0.150821, accuracy: 0.1507
case acc: 0.20245421
case acc: 0.13963288
case acc: 0.18835717
case acc: 0.03846806
case acc: 0.15800115
case acc: 0.17731678
top acc: 0.2361 ::: bot acc: 0.1722
top acc: 0.1687 ::: bot acc: 0.1115
top acc: 0.1453 ::: bot acc: 0.2307
top acc: 0.0155 ::: bot acc: 0.0639
top acc: 0.1905 ::: bot acc: 0.1229
top acc: 0.1375 ::: bot acc: 0.2171
current epoch: 3
train loss is 0.156393
average val loss: 0.151216, accuracy: 0.1524
average test loss: 0.151388, accuracy: 0.1522
case acc: 0.25783432
case acc: 0.20145501
case acc: 0.105324164
case acc: 0.036769733
case acc: 0.21906409
case acc: 0.09295814
top acc: 0.2916 ::: bot acc: 0.2278
top acc: 0.2302 ::: bot acc: 0.1736
top acc: 0.0626 ::: bot acc: 0.1475
top acc: 0.0585 ::: bot acc: 0.0174
top acc: 0.2517 ::: bot acc: 0.1838
top acc: 0.0564 ::: bot acc: 0.1313
current epoch: 4
train loss is 0.162302
average val loss: 0.132820, accuracy: 0.1337
average test loss: 0.132618, accuracy: 0.1326
case acc: 0.21129216
case acc: 0.16166732
case acc: 0.12068966
case acc: 0.019713243
case acc: 0.17810695
case acc: 0.103963956
top acc: 0.2451 ::: bot acc: 0.1813
top acc: 0.1903 ::: bot acc: 0.1339
top acc: 0.0775 ::: bot acc: 0.1631
top acc: 0.0297 ::: bot acc: 0.0226
top acc: 0.2109 ::: bot acc: 0.1428
top acc: 0.0661 ::: bot acc: 0.1430
current epoch: 5
train loss is 0.137662
average val loss: 0.130994, accuracy: 0.1327
average test loss: 0.131250, accuracy: 0.1326
case acc: 0.2330316
case acc: 0.19074863
case acc: 0.06788058
case acc: 0.04837075
case acc: 0.2054997
case acc: 0.050039172
top acc: 0.2668 ::: bot acc: 0.2030
top acc: 0.2194 ::: bot acc: 0.1629
top acc: 0.0305 ::: bot acc: 0.1074
top acc: 0.0719 ::: bot acc: 0.0253
top acc: 0.2384 ::: bot acc: 0.1701
top acc: 0.0204 ::: bot acc: 0.0851
current epoch: 6
train loss is 0.132499
average val loss: 0.110824, accuracy: 0.1121
average test loss: 0.110865, accuracy: 0.1115
case acc: 0.18557549
case acc: 0.15038335
case acc: 0.08228941
case acc: 0.02558299
case acc: 0.16367367
case acc: 0.061309166
top acc: 0.2194 ::: bot acc: 0.1555
top acc: 0.1788 ::: bot acc: 0.1226
top acc: 0.0421 ::: bot acc: 0.1231
top acc: 0.0431 ::: bot acc: 0.0147
top acc: 0.1967 ::: bot acc: 0.1282
top acc: 0.0285 ::: bot acc: 0.0980
current epoch: 7
train loss is 0.118310
average val loss: 0.102343, accuracy: 0.1039
average test loss: 0.102245, accuracy: 0.1035
case acc: 0.17704706
case acc: 0.14909546
case acc: 0.060735226
case acc: 0.033050776
case acc: 0.16069388
case acc: 0.04029136
top acc: 0.2108 ::: bot acc: 0.1471
top acc: 0.1774 ::: bot acc: 0.1214
top acc: 0.0252 ::: bot acc: 0.0991
top acc: 0.0537 ::: bot acc: 0.0157
top acc: 0.1938 ::: bot acc: 0.1251
top acc: 0.0175 ::: bot acc: 0.0722
current epoch: 8
train loss is 0.104464
average val loss: 0.090738, accuracy: 0.0922
average test loss: 0.090377, accuracy: 0.0918
case acc: 0.15489268
case acc: 0.13417096
case acc: 0.052784726
case acc: 0.030389275
case acc: 0.1441573
case acc: 0.03415518
top acc: 0.1887 ::: bot acc: 0.1249
top acc: 0.1624 ::: bot acc: 0.1065
top acc: 0.0203 ::: bot acc: 0.0896
top acc: 0.0502 ::: bot acc: 0.0147
top acc: 0.1774 ::: bot acc: 0.1085
top acc: 0.0218 ::: bot acc: 0.0609
current epoch: 9
train loss is 0.095321
average val loss: 0.081913, accuracy: 0.0832
average test loss: 0.081197, accuracy: 0.0830
case acc: 0.13737464
case acc: 0.12403895
case acc: 0.042748213
case acc: 0.031208761
case acc: 0.13236561
case acc: 0.03024375
top acc: 0.1712 ::: bot acc: 0.1073
top acc: 0.1521 ::: bot acc: 0.0964
top acc: 0.0184 ::: bot acc: 0.0756
top acc: 0.0512 ::: bot acc: 0.0150
top acc: 0.1657 ::: bot acc: 0.0966
top acc: 0.0354 ::: bot acc: 0.0462
current epoch: 10
train loss is 0.083292
average val loss: 0.071336, accuracy: 0.0724
average test loss: 0.070351, accuracy: 0.0721
case acc: 0.11389505
case acc: 0.108009
case acc: 0.038419113
case acc: 0.027464684
case acc: 0.11476902
case acc: 0.030269124
top acc: 0.1478 ::: bot acc: 0.0838
top acc: 0.1360 ::: bot acc: 0.0804
top acc: 0.0207 ::: bot acc: 0.0680
top acc: 0.0461 ::: bot acc: 0.0141
top acc: 0.1481 ::: bot acc: 0.0790
top acc: 0.0431 ::: bot acc: 0.0388
current epoch: 11
train loss is 0.074024
average val loss: 0.065462, accuracy: 0.0664
average test loss: 0.064294, accuracy: 0.0661
case acc: 0.09795111
case acc: 0.09964782
case acc: 0.033006884
case acc: 0.028879367
case acc: 0.10478184
case acc: 0.03233113
top acc: 0.1319 ::: bot acc: 0.0679
top acc: 0.1275 ::: bot acc: 0.0722
top acc: 0.0321 ::: bot acc: 0.0535
top acc: 0.0482 ::: bot acc: 0.0141
top acc: 0.1383 ::: bot acc: 0.0689
top acc: 0.0559 ::: bot acc: 0.0264
current epoch: 12
train loss is 0.065554
average val loss: 0.056186, accuracy: 0.0570
average test loss: 0.054842, accuracy: 0.0563
case acc: 0.07499278
case acc: 0.0844056
case acc: 0.031763542
case acc: 0.025163094
case acc: 0.08798665
case acc: 0.033470456
top acc: 0.1089 ::: bot acc: 0.0449
top acc: 0.1121 ::: bot acc: 0.0570
top acc: 0.0388 ::: bot acc: 0.0470
top acc: 0.0428 ::: bot acc: 0.0139
top acc: 0.1216 ::: bot acc: 0.0520
top acc: 0.0591 ::: bot acc: 0.0239
current epoch: 13
train loss is 0.056914
average val loss: 0.050960, accuracy: 0.0515
average test loss: 0.049593, accuracy: 0.0506
case acc: 0.058299385
case acc: 0.075323924
case acc: 0.032028385
case acc: 0.024964985
case acc: 0.07725404
case acc: 0.03584754
top acc: 0.0923 ::: bot acc: 0.0283
top acc: 0.1028 ::: bot acc: 0.0480
top acc: 0.0503 ::: bot acc: 0.0358
top acc: 0.0426 ::: bot acc: 0.0136
top acc: 0.1110 ::: bot acc: 0.0412
top acc: 0.0644 ::: bot acc: 0.0206
current epoch: 14
train loss is 0.049980
average val loss: 0.046054, accuracy: 0.0463
average test loss: 0.044688, accuracy: 0.0450
case acc: 0.042887233
case acc: 0.0666621
case acc: 0.033773493
case acc: 0.024102747
case acc: 0.06708189
case acc: 0.035759006
top acc: 0.0765 ::: bot acc: 0.0139
top acc: 0.0939 ::: bot acc: 0.0395
top acc: 0.0599 ::: bot acc: 0.0267
top acc: 0.0414 ::: bot acc: 0.0133
top acc: 0.1007 ::: bot acc: 0.0316
top acc: 0.0644 ::: bot acc: 0.0206
current epoch: 15
train loss is 0.043509
average val loss: 0.042665, accuracy: 0.0428
average test loss: 0.041289, accuracy: 0.0412
case acc: 0.03317835
case acc: 0.06011928
case acc: 0.03672734
case acc: 0.023700489
case acc: 0.05905225
case acc: 0.034423508
top acc: 0.0640 ::: bot acc: 0.0101
top acc: 0.0871 ::: bot acc: 0.0331
top acc: 0.0690 ::: bot acc: 0.0185
top acc: 0.0408 ::: bot acc: 0.0130
top acc: 0.0921 ::: bot acc: 0.0249
top acc: 0.0615 ::: bot acc: 0.0225
current epoch: 16
train loss is 0.038707
average val loss: 0.042048, accuracy: 0.0422
average test loss: 0.040745, accuracy: 0.0404
case acc: 0.02889358
case acc: 0.057506967
case acc: 0.04217599
case acc: 0.025102533
case acc: 0.054833867
case acc: 0.03364128
top acc: 0.0573 ::: bot acc: 0.0109
top acc: 0.0844 ::: bot acc: 0.0306
top acc: 0.0793 ::: bot acc: 0.0141
top acc: 0.0430 ::: bot acc: 0.0126
top acc: 0.0877 ::: bot acc: 0.0215
top acc: 0.0597 ::: bot acc: 0.0237
current epoch: 17
train loss is 0.035594
average val loss: 0.046014, accuracy: 0.0462
average test loss: 0.044981, accuracy: 0.0445
case acc: 0.029672982
case acc: 0.06134496
case acc: 0.05229392
case acc: 0.031185925
case acc: 0.05706121
case acc: 0.035597797
top acc: 0.0588 ::: bot acc: 0.0106
top acc: 0.0882 ::: bot acc: 0.0345
top acc: 0.0937 ::: bot acc: 0.0155
top acc: 0.0511 ::: bot acc: 0.0143
top acc: 0.0902 ::: bot acc: 0.0231
top acc: 0.0643 ::: bot acc: 0.0204
current epoch: 18
train loss is 0.037018
average val loss: 0.057782, accuracy: 0.0580
average test loss: 0.057267, accuracy: 0.0570
case acc: 0.03680008
case acc: 0.07422812
case acc: 0.070651926
case acc: 0.046040498
case acc: 0.06884856
case acc: 0.045294363
top acc: 0.0695 ::: bot acc: 0.0105
top acc: 0.1011 ::: bot acc: 0.0474
top acc: 0.1150 ::: bot acc: 0.0280
top acc: 0.0686 ::: bot acc: 0.0237
top acc: 0.1031 ::: bot acc: 0.0328
top acc: 0.0791 ::: bot acc: 0.0198
current epoch: 19
train loss is 0.041524
average val loss: 0.066946, accuracy: 0.0670
average test loss: 0.066694, accuracy: 0.0665
case acc: 0.040223684
case acc: 0.08271478
case acc: 0.08537746
case acc: 0.05866799
case acc: 0.078262344
case acc: 0.0536113
top acc: 0.0738 ::: bot acc: 0.0121
top acc: 0.1095 ::: bot acc: 0.0560
top acc: 0.1300 ::: bot acc: 0.0422
top acc: 0.0822 ::: bot acc: 0.0342
top acc: 0.1128 ::: bot acc: 0.0418
top acc: 0.0898 ::: bot acc: 0.0233
current epoch: 20
train loss is 0.048337
average val loss: 0.066745, accuracy: 0.0667
average test loss: 0.066495, accuracy: 0.0662
case acc: 0.0346189
case acc: 0.08052704
case acc: 0.086664245
case acc: 0.061790477
case acc: 0.078830786
case acc: 0.054947756
top acc: 0.0667 ::: bot acc: 0.0098
top acc: 0.1073 ::: bot acc: 0.0538
top acc: 0.1313 ::: bot acc: 0.0435
top acc: 0.0855 ::: bot acc: 0.0370
top acc: 0.1134 ::: bot acc: 0.0423
top acc: 0.0914 ::: bot acc: 0.0240
current epoch: 21
train loss is 0.053041
average val loss: 0.049249, accuracy: 0.0497
average test loss: 0.048498, accuracy: 0.0481
case acc: 0.02268729
case acc: 0.056259394
case acc: 0.06257683
case acc: 0.04407625
case acc: 0.06016907
case acc: 0.04301148
top acc: 0.0386 ::: bot acc: 0.0255
top acc: 0.0830 ::: bot acc: 0.0295
top acc: 0.1063 ::: bot acc: 0.0214
top acc: 0.0665 ::: bot acc: 0.0221
top acc: 0.0939 ::: bot acc: 0.0254
top acc: 0.0759 ::: bot acc: 0.0196
current epoch: 22
train loss is 0.055155
average val loss: 0.031729, accuracy: 0.0325
average test loss: 0.029622, accuracy: 0.0300
case acc: 0.047522143
case acc: 0.020774297
case acc: 0.032985374
case acc: 0.017937727
case acc: 0.030415414
case acc: 0.030247122
top acc: 0.0232 ::: bot acc: 0.0723
top acc: 0.0355 ::: bot acc: 0.0181
top acc: 0.0538 ::: bot acc: 0.0342
top acc: 0.0223 ::: bot acc: 0.0271
top acc: 0.0520 ::: bot acc: 0.0204
top acc: 0.0426 ::: bot acc: 0.0396
current epoch: 23
train loss is 0.054521
average val loss: 0.045337, accuracy: 0.0462
average test loss: 0.044625, accuracy: 0.0448
case acc: 0.076898634
case acc: 0.035494983
case acc: 0.047739822
case acc: 0.04370831
case acc: 0.029328354
case acc: 0.035674684
top acc: 0.0458 ::: bot acc: 0.1050
top acc: 0.0143 ::: bot acc: 0.0596
top acc: 0.0182 ::: bot acc: 0.0836
top acc: 0.0207 ::: bot acc: 0.0685
top acc: 0.0161 ::: bot acc: 0.0554
top acc: 0.0198 ::: bot acc: 0.0641
current epoch: 24
train loss is 0.042651
average val loss: 0.048033, accuracy: 0.0486
average test loss: 0.047455, accuracy: 0.0478
case acc: 0.072316214
case acc: 0.040936425
case acc: 0.058232203
case acc: 0.051893607
case acc: 0.03114495
case acc: 0.032242812
top acc: 0.0420 ::: bot acc: 0.1001
top acc: 0.0174 ::: bot acc: 0.0663
top acc: 0.0226 ::: bot acc: 0.0970
top acc: 0.0283 ::: bot acc: 0.0769
top acc: 0.0142 ::: bot acc: 0.0590
top acc: 0.0271 ::: bot acc: 0.0552
current epoch: 25
train loss is 0.043009
average val loss: 0.060436, accuracy: 0.0608
average test loss: 0.060364, accuracy: 0.0605
case acc: 0.07769318
case acc: 0.056179915
case acc: 0.07843652
case acc: 0.07204805
case acc: 0.042664997
case acc: 0.0361894
top acc: 0.0465 ::: bot acc: 0.1058
top acc: 0.0299 ::: bot acc: 0.0828
top acc: 0.0372 ::: bot acc: 0.1200
top acc: 0.0480 ::: bot acc: 0.0973
top acc: 0.0162 ::: bot acc: 0.0752
top acc: 0.0192 ::: bot acc: 0.0652
current epoch: 26
train loss is 0.065090
average val loss: 0.034158, accuracy: 0.0339
average test loss: 0.032526, accuracy: 0.0330
case acc: 0.035716064
case acc: 0.022963922
case acc: 0.045650527
case acc: 0.036738846
case acc: 0.025335506
case acc: 0.031333763
top acc: 0.0202 ::: bot acc: 0.0562
top acc: 0.0132 ::: bot acc: 0.0415
top acc: 0.0178 ::: bot acc: 0.0806
top acc: 0.0146 ::: bot acc: 0.0612
top acc: 0.0336 ::: bot acc: 0.0374
top acc: 0.0539 ::: bot acc: 0.0284
current epoch: 27
train loss is 0.059364
average val loss: 0.033261, accuracy: 0.0334
average test loss: 0.031450, accuracy: 0.0313
case acc: 0.026410982
case acc: 0.028696809
case acc: 0.032326713
case acc: 0.01791205
case acc: 0.03964728
case acc: 0.042675637
top acc: 0.0532 ::: bot acc: 0.0126
top acc: 0.0525 ::: bot acc: 0.0080
top acc: 0.0479 ::: bot acc: 0.0401
top acc: 0.0236 ::: bot acc: 0.0258
top acc: 0.0681 ::: bot acc: 0.0156
top acc: 0.0751 ::: bot acc: 0.0201
current epoch: 28
train loss is 0.043178
average val loss: 0.033056, accuracy: 0.0333
average test loss: 0.031253, accuracy: 0.0310
case acc: 0.029046107
case acc: 0.032545127
case acc: 0.033305608
case acc: 0.018143866
case acc: 0.039494436
case acc: 0.033215232
top acc: 0.0582 ::: bot acc: 0.0104
top acc: 0.0577 ::: bot acc: 0.0091
top acc: 0.0565 ::: bot acc: 0.0315
top acc: 0.0264 ::: bot acc: 0.0230
top acc: 0.0678 ::: bot acc: 0.0156
top acc: 0.0595 ::: bot acc: 0.0232
current epoch: 29
train loss is 0.036248
average val loss: 0.036809, accuracy: 0.0369
average test loss: 0.035401, accuracy: 0.0349
case acc: 0.034008816
case acc: 0.041475262
case acc: 0.038469903
case acc: 0.021065451
case acc: 0.043253247
case acc: 0.031151567
top acc: 0.0660 ::: bot acc: 0.0095
top acc: 0.0679 ::: bot acc: 0.0155
top acc: 0.0727 ::: bot acc: 0.0167
top acc: 0.0363 ::: bot acc: 0.0137
top acc: 0.0733 ::: bot acc: 0.0158
top acc: 0.0533 ::: bot acc: 0.0288
current epoch: 30
train loss is 0.032982
average val loss: 0.047878, accuracy: 0.0481
average test loss: 0.047303, accuracy: 0.0470
case acc: 0.04445795
case acc: 0.058186382
case acc: 0.056226198
case acc: 0.03468304
case acc: 0.054078907
case acc: 0.034293547
top acc: 0.0788 ::: bot acc: 0.0151
top acc: 0.0850 ::: bot acc: 0.0314
top acc: 0.0989 ::: bot acc: 0.0173
top acc: 0.0557 ::: bot acc: 0.0156
top acc: 0.0870 ::: bot acc: 0.0208
top acc: 0.0618 ::: bot acc: 0.0218
current epoch: 31
train loss is 0.035711
average val loss: 0.065337, accuracy: 0.0655
average test loss: 0.065338, accuracy: 0.0655
case acc: 0.057155002
case acc: 0.07778021
case acc: 0.08418078
case acc: 0.05658632
case acc: 0.07091502
case acc: 0.046365798
top acc: 0.0917 ::: bot acc: 0.0274
top acc: 0.1046 ::: bot acc: 0.0509
top acc: 0.1289 ::: bot acc: 0.0409
top acc: 0.0801 ::: bot acc: 0.0323
top acc: 0.1052 ::: bot acc: 0.0348
top acc: 0.0802 ::: bot acc: 0.0208
current epoch: 32
train loss is 0.051180
average val loss: 0.057871, accuracy: 0.0580
average test loss: 0.057679, accuracy: 0.0575
case acc: 0.039951865
case acc: 0.06727569
case acc: 0.07978047
case acc: 0.052540176
case acc: 0.06291275
case acc: 0.042513352
top acc: 0.0738 ::: bot acc: 0.0117
top acc: 0.0941 ::: bot acc: 0.0404
top acc: 0.1245 ::: bot acc: 0.0365
top acc: 0.0757 ::: bot acc: 0.0289
top acc: 0.0968 ::: bot acc: 0.0277
top acc: 0.0748 ::: bot acc: 0.0201
current epoch: 33
train loss is 0.054995
average val loss: 0.032369, accuracy: 0.0326
average test loss: 0.030477, accuracy: 0.0301
case acc: 0.026517797
case acc: 0.026920725
case acc: 0.042421333
case acc: 0.021262001
case acc: 0.03335328
case acc: 0.030239208
top acc: 0.0263 ::: bot acc: 0.0395
top acc: 0.0498 ::: bot acc: 0.0082
top acc: 0.0796 ::: bot acc: 0.0147
top acc: 0.0368 ::: bot acc: 0.0134
top acc: 0.0577 ::: bot acc: 0.0175
top acc: 0.0452 ::: bot acc: 0.0368
current epoch: 34
train loss is 0.045773
average val loss: 0.029905, accuracy: 0.0302
average test loss: 0.027597, accuracy: 0.0278
case acc: 0.03961558
case acc: 0.019538082
case acc: 0.032424938
case acc: 0.019465927
case acc: 0.025712121
case acc: 0.030259412
top acc: 0.0203 ::: bot acc: 0.0619
top acc: 0.0244 ::: bot acc: 0.0293
top acc: 0.0487 ::: bot acc: 0.0394
top acc: 0.0135 ::: bot acc: 0.0359
top acc: 0.0377 ::: bot acc: 0.0331
top acc: 0.0376 ::: bot acc: 0.0444
current epoch: 35
train loss is 0.038541
average val loss: 0.031785, accuracy: 0.0320
average test loss: 0.029866, accuracy: 0.0303
case acc: 0.042539448
case acc: 0.022883134
case acc: 0.03491045
case acc: 0.026051797
case acc: 0.02527257
case acc: 0.030103922
top acc: 0.0211 ::: bot acc: 0.0658
top acc: 0.0133 ::: bot acc: 0.0413
top acc: 0.0288 ::: bot acc: 0.0593
top acc: 0.0080 ::: bot acc: 0.0485
top acc: 0.0298 ::: bot acc: 0.0410
top acc: 0.0407 ::: bot acc: 0.0413
current epoch: 36
train loss is 0.032680
average val loss: 0.036832, accuracy: 0.0369
average test loss: 0.035457, accuracy: 0.0360
case acc: 0.04491709
case acc: 0.030483313
case acc: 0.04469619
case acc: 0.038258307
case acc: 0.0274909
case acc: 0.030303426
top acc: 0.0221 ::: bot acc: 0.0689
top acc: 0.0118 ::: bot acc: 0.0533
top acc: 0.0177 ::: bot acc: 0.0794
top acc: 0.0159 ::: bot acc: 0.0628
top acc: 0.0201 ::: bot acc: 0.0507
top acc: 0.0374 ::: bot acc: 0.0446
current epoch: 37
train loss is 0.034947
average val loss: 0.045104, accuracy: 0.0451
average test loss: 0.044317, accuracy: 0.0447
case acc: 0.047912262
case acc: 0.040603135
case acc: 0.059663057
case acc: 0.054049484
case acc: 0.03369551
case acc: 0.03245277
top acc: 0.0236 ::: bot acc: 0.0726
top acc: 0.0171 ::: bot acc: 0.0658
top acc: 0.0236 ::: bot acc: 0.0988
top acc: 0.0303 ::: bot acc: 0.0792
top acc: 0.0134 ::: bot acc: 0.0632
top acc: 0.0259 ::: bot acc: 0.0562
current epoch: 38
train loss is 0.048144
average val loss: 0.032162, accuracy: 0.0318
average test loss: 0.030241, accuracy: 0.0306
case acc: 0.02694753
case acc: 0.022780906
case acc: 0.043087184
case acc: 0.035156228
case acc: 0.025298849
case acc: 0.030192738
top acc: 0.0257 ::: bot acc: 0.0403
top acc: 0.0134 ::: bot acc: 0.0411
top acc: 0.0181 ::: bot acc: 0.0768
top acc: 0.0134 ::: bot acc: 0.0594
top acc: 0.0291 ::: bot acc: 0.0417
top acc: 0.0436 ::: bot acc: 0.0385
current epoch: 39
train loss is 0.047828
average val loss: 0.032243, accuracy: 0.0323
average test loss: 0.030376, accuracy: 0.0302
case acc: 0.033244368
case acc: 0.02678759
case acc: 0.032502145
case acc: 0.01786787
case acc: 0.035134546
case acc: 0.035627518
top acc: 0.0650 ::: bot acc: 0.0094
top acc: 0.0496 ::: bot acc: 0.0082
top acc: 0.0496 ::: bot acc: 0.0386
top acc: 0.0229 ::: bot acc: 0.0265
top acc: 0.0608 ::: bot acc: 0.0166
top acc: 0.0641 ::: bot acc: 0.0209
current epoch: 40
train loss is 0.042234
average val loss: 0.042211, accuracy: 0.0421
average test loss: 0.041425, accuracy: 0.0412
case acc: 0.050246123
case acc: 0.045494355
case acc: 0.041423373
case acc: 0.025988799
case acc: 0.046798877
case acc: 0.03751655
top acc: 0.0848 ::: bot acc: 0.0206
top acc: 0.0721 ::: bot acc: 0.0191
top acc: 0.0778 ::: bot acc: 0.0151
top acc: 0.0447 ::: bot acc: 0.0116
top acc: 0.0782 ::: bot acc: 0.0166
top acc: 0.0673 ::: bot acc: 0.0202
current epoch: 41
train loss is 0.037378
average val loss: 0.054930, accuracy: 0.0552
average test loss: 0.054755, accuracy: 0.0549
case acc: 0.06081266
case acc: 0.063613996
case acc: 0.062268004
case acc: 0.043413278
case acc: 0.059298422
case acc: 0.03992509
top acc: 0.0954 ::: bot acc: 0.0311
top acc: 0.0905 ::: bot acc: 0.0368
top acc: 0.1058 ::: bot acc: 0.0213
top acc: 0.0658 ::: bot acc: 0.0214
top acc: 0.0928 ::: bot acc: 0.0247
top acc: 0.0710 ::: bot acc: 0.0199
current epoch: 42
train loss is 0.040197
average val loss: 0.067672, accuracy: 0.0678
average test loss: 0.067790, accuracy: 0.0681
case acc: 0.06548954
case acc: 0.077445604
case acc: 0.0853686
case acc: 0.06145656
case acc: 0.0714819
case acc: 0.0471346
top acc: 0.1001 ::: bot acc: 0.0358
top acc: 0.1043 ::: bot acc: 0.0506
top acc: 0.1301 ::: bot acc: 0.0420
top acc: 0.0853 ::: bot acc: 0.0365
top acc: 0.1057 ::: bot acc: 0.0354
top acc: 0.0811 ::: bot acc: 0.0211
current epoch: 43
train loss is 0.054779
average val loss: 0.037556, accuracy: 0.0379
average test loss: 0.036248, accuracy: 0.0357
case acc: 0.025657544
case acc: 0.037318088
case acc: 0.050843738
case acc: 0.030153269
case acc: 0.03998422
case acc: 0.030404162
top acc: 0.0519 ::: bot acc: 0.0131
top acc: 0.0634 ::: bot acc: 0.0120
top acc: 0.0917 ::: bot acc: 0.0154
top acc: 0.0502 ::: bot acc: 0.0129
top acc: 0.0684 ::: bot acc: 0.0157
top acc: 0.0480 ::: bot acc: 0.0340
current epoch: 44
train loss is 0.043879
average val loss: 0.029942, accuracy: 0.0301
average test loss: 0.027722, accuracy: 0.0275
case acc: 0.025173776
case acc: 0.022905543
case acc: 0.036660127
case acc: 0.01977524
case acc: 0.030468818
case acc: 0.030155685
top acc: 0.0283 ::: bot acc: 0.0364
top acc: 0.0417 ::: bot acc: 0.0125
top acc: 0.0685 ::: bot acc: 0.0196
top acc: 0.0331 ::: bot acc: 0.0163
top acc: 0.0520 ::: bot acc: 0.0203
top acc: 0.0443 ::: bot acc: 0.0377
current epoch: 45
train loss is 0.036706
average val loss: 0.028475, accuracy: 0.0286
average test loss: 0.025942, accuracy: 0.0263
case acc: 0.031160533
case acc: 0.019498114
case acc: 0.032362606
case acc: 0.018087558
case acc: 0.026481412
case acc: 0.030230744
top acc: 0.0219 ::: bot acc: 0.0485
top acc: 0.0268 ::: bot acc: 0.0269
top acc: 0.0485 ::: bot acc: 0.0396
top acc: 0.0199 ::: bot acc: 0.0295
top acc: 0.0418 ::: bot acc: 0.0289
top acc: 0.0461 ::: bot acc: 0.0358
current epoch: 46
train loss is 0.031870
average val loss: 0.030629, accuracy: 0.0306
average test loss: 0.028616, accuracy: 0.0290
case acc: 0.036453657
case acc: 0.023098338
case acc: 0.035626743
case acc: 0.02377036
case acc: 0.02516627
case acc: 0.030063927
top acc: 0.0202 ::: bot acc: 0.0572
top acc: 0.0130 ::: bot acc: 0.0418
top acc: 0.0265 ::: bot acc: 0.0616
top acc: 0.0082 ::: bot acc: 0.0450
top acc: 0.0303 ::: bot acc: 0.0404
top acc: 0.0428 ::: bot acc: 0.0391
current epoch: 47
train loss is 0.030223
average val loss: 0.036375, accuracy: 0.0363
average test loss: 0.034974, accuracy: 0.0355
case acc: 0.040164866
case acc: 0.031518057
case acc: 0.046382345
case acc: 0.036307346
case acc: 0.02794275
case acc: 0.03048975
top acc: 0.0206 ::: bot acc: 0.0625
top acc: 0.0121 ::: bot acc: 0.0548
top acc: 0.0179 ::: bot acc: 0.0818
top acc: 0.0143 ::: bot acc: 0.0607
top acc: 0.0187 ::: bot acc: 0.0521
top acc: 0.0346 ::: bot acc: 0.0473
current epoch: 48
train loss is 0.035774
average val loss: 0.037599, accuracy: 0.0373
average test loss: 0.036294, accuracy: 0.0367
case acc: 0.035633083
case acc: 0.03246646
case acc: 0.05103894
case acc: 0.041491322
case acc: 0.028906433
case acc: 0.03094835
top acc: 0.0202 ::: bot acc: 0.0559
top acc: 0.0125 ::: bot acc: 0.0560
top acc: 0.0189 ::: bot acc: 0.0883
top acc: 0.0187 ::: bot acc: 0.0662
top acc: 0.0165 ::: bot acc: 0.0546
top acc: 0.0313 ::: bot acc: 0.0506
current epoch: 49
train loss is 0.043601
average val loss: 0.028838, accuracy: 0.0285
average test loss: 0.026395, accuracy: 0.0262
case acc: 0.022839349
case acc: 0.019443892
case acc: 0.03552092
case acc: 0.022575883
case acc: 0.026082162
case acc: 0.030506007
top acc: 0.0426 ::: bot acc: 0.0217
top acc: 0.0256 ::: bot acc: 0.0281
top acc: 0.0268 ::: bot acc: 0.0613
top acc: 0.0087 ::: bot acc: 0.0430
top acc: 0.0400 ::: bot acc: 0.0307
top acc: 0.0494 ::: bot acc: 0.0326
current epoch: 50
train loss is 0.040718
average val loss: 0.034671, accuracy: 0.0347
average test loss: 0.033144, accuracy: 0.0329
case acc: 0.040165395
case acc: 0.030827295
case acc: 0.034136415
case acc: 0.019938178
case acc: 0.037705887
case acc: 0.034818955
top acc: 0.0741 ::: bot acc: 0.0119
top acc: 0.0556 ::: bot acc: 0.0083
top acc: 0.0599 ::: bot acc: 0.0281
top acc: 0.0336 ::: bot acc: 0.0157
top acc: 0.0649 ::: bot acc: 0.0159
top acc: 0.0627 ::: bot acc: 0.0213

		{"drop_out": 0.2, "drop_out_mc": 0.15, "repeat_mc": 50, "hidden": 20, "embedding_size": 5, "batch": 512, "lag": 4}
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
LME_Co_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2009-01-01', '2009-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6768 6768 6768
1.8562728 -0.6288155 0.2585643 -0.19947179
Validation: 756 756 756
Testing: 774 774 774
pre-processing time: 0.00048041343688964844
the split date is 2009-07-01
net initializing with time: 0.005297183990478516
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.404522
average val loss: 0.191110, accuracy: 0.1908
average test loss: 0.184970, accuracy: 0.1851
case acc: 0.12305199
case acc: 0.2678261
case acc: 0.15785517
case acc: 0.024398653
case acc: 0.33928204
case acc: 0.19818376
top acc: 0.1400 ::: bot acc: 0.1063
top acc: 0.2938 ::: bot acc: 0.2435
top acc: 0.1950 ::: bot acc: 0.1239
top acc: 0.0439 ::: bot acc: 0.0133
top acc: 0.3643 ::: bot acc: 0.3112
top acc: 0.2189 ::: bot acc: 0.1751
current epoch: 2
train loss is 0.141032
average val loss: 0.091625, accuracy: 0.0932
average test loss: 0.089410, accuracy: 0.0902
case acc: 0.02239996
case acc: 0.12707722
case acc: 0.028668854
case acc: 0.12732421
case acc: 0.18442746
case acc: 0.051358055
top acc: 0.0091 ::: bot acc: 0.0373
top acc: 0.1536 ::: bot acc: 0.1025
top acc: 0.0539 ::: bot acc: 0.0181
top acc: 0.1005 ::: bot acc: 0.1535
top acc: 0.2092 ::: bot acc: 0.1566
top acc: 0.0720 ::: bot acc: 0.0290
current epoch: 3
train loss is 0.116686
average val loss: 0.087717, accuracy: 0.0878
average test loss: 0.089695, accuracy: 0.0913
case acc: 0.109791234
case acc: 0.030799154
case acc: 0.07054165
case acc: 0.20638041
case acc: 0.096151836
case acc: 0.034208823
top acc: 0.0929 ::: bot acc: 0.1264
top acc: 0.0554 ::: bot acc: 0.0101
top acc: 0.0334 ::: bot acc: 0.1051
top acc: 0.1797 ::: bot acc: 0.2326
top acc: 0.1208 ::: bot acc: 0.0687
top acc: 0.0143 ::: bot acc: 0.0567
current epoch: 4
train loss is 0.117574
average val loss: 0.088757, accuracy: 0.0899
average test loss: 0.085897, accuracy: 0.0860
case acc: 0.016228085
case acc: 0.11957032
case acc: 0.033754144
case acc: 0.10477626
case acc: 0.17893651
case acc: 0.062437907
top acc: 0.0078 ::: bot acc: 0.0284
top acc: 0.1466 ::: bot acc: 0.0942
top acc: 0.0648 ::: bot acc: 0.0115
top acc: 0.0783 ::: bot acc: 0.1309
top acc: 0.2034 ::: bot acc: 0.1517
top acc: 0.0832 ::: bot acc: 0.0398
current epoch: 5
train loss is 0.093461
average val loss: 0.070788, accuracy: 0.0725
average test loss: 0.071022, accuracy: 0.0731
case acc: 0.0695524
case acc: 0.05557446
case acc: 0.03590413
case acc: 0.15831287
case acc: 0.102893695
case acc: 0.016271325
top acc: 0.0524 ::: bot acc: 0.0861
top acc: 0.0826 ::: bot acc: 0.0302
top acc: 0.0152 ::: bot acc: 0.0623
top acc: 0.1319 ::: bot acc: 0.1844
top acc: 0.1272 ::: bot acc: 0.0757
top acc: 0.0224 ::: bot acc: 0.0211
current epoch: 6
train loss is 0.090090
average val loss: 0.065596, accuracy: 0.0676
average test loss: 0.065755, accuracy: 0.0678
case acc: 0.06765465
case acc: 0.048903707
case acc: 0.033946376
case acc: 0.15271907
case acc: 0.08757238
case acc: 0.016073838
top acc: 0.0506 ::: bot acc: 0.0841
top acc: 0.0758 ::: bot acc: 0.0238
top acc: 0.0163 ::: bot acc: 0.0590
top acc: 0.1265 ::: bot acc: 0.1787
top acc: 0.1118 ::: bot acc: 0.0605
top acc: 0.0220 ::: bot acc: 0.0213
current epoch: 7
train loss is 0.092213
average val loss: 0.064629, accuracy: 0.0667
average test loss: 0.062775, accuracy: 0.0646
case acc: 0.037752893
case acc: 0.07362243
case acc: 0.026822858
case acc: 0.12113846
case acc: 0.09796282
case acc: 0.030143013
top acc: 0.0208 ::: bot acc: 0.0541
top acc: 0.1009 ::: bot acc: 0.0479
top acc: 0.0443 ::: bot acc: 0.0283
top acc: 0.0950 ::: bot acc: 0.1470
top acc: 0.1222 ::: bot acc: 0.0711
top acc: 0.0475 ::: bot acc: 0.0144
current epoch: 8
train loss is 0.081919
average val loss: 0.057528, accuracy: 0.0600
average test loss: 0.056450, accuracy: 0.0585
case acc: 0.049593028
case acc: 0.057509948
case acc: 0.02718168
case acc: 0.13109072
case acc: 0.066824876
case acc: 0.019097734
top acc: 0.0326 ::: bot acc: 0.0660
top acc: 0.0848 ::: bot acc: 0.0318
top acc: 0.0341 ::: bot acc: 0.0385
top acc: 0.1051 ::: bot acc: 0.1570
top acc: 0.0910 ::: bot acc: 0.0399
top acc: 0.0311 ::: bot acc: 0.0140
current epoch: 9
train loss is 0.075938
average val loss: 0.052774, accuracy: 0.0553
average test loss: 0.052053, accuracy: 0.0540
case acc: 0.05238883
case acc: 0.04919679
case acc: 0.027413454
case acc: 0.13134357
case acc: 0.047185544
case acc: 0.016550116
top acc: 0.0354 ::: bot acc: 0.0688
top acc: 0.0763 ::: bot acc: 0.0239
top acc: 0.0331 ::: bot acc: 0.0397
top acc: 0.1054 ::: bot acc: 0.1572
top acc: 0.0707 ::: bot acc: 0.0216
top acc: 0.0248 ::: bot acc: 0.0186
current epoch: 10
train loss is 0.073939
average val loss: 0.049944, accuracy: 0.0523
average test loss: 0.048655, accuracy: 0.0502
case acc: 0.04405364
case acc: 0.052409586
case acc: 0.026742013
case acc: 0.12066822
case acc: 0.039289217
case acc: 0.018334625
top acc: 0.0270 ::: bot acc: 0.0605
top acc: 0.0797 ::: bot acc: 0.0269
top acc: 0.0428 ::: bot acc: 0.0300
top acc: 0.0948 ::: bot acc: 0.1465
top acc: 0.0620 ::: bot acc: 0.0154
top acc: 0.0295 ::: bot acc: 0.0147
current epoch: 11
train loss is 0.068584
average val loss: 0.047239, accuracy: 0.0493
average test loss: 0.045790, accuracy: 0.0470
case acc: 0.03924865
case acc: 0.05237035
case acc: 0.027517451
case acc: 0.11310156
case acc: 0.030803075
case acc: 0.019091327
top acc: 0.0222 ::: bot acc: 0.0557
top acc: 0.0797 ::: bot acc: 0.0267
top acc: 0.0488 ::: bot acc: 0.0241
top acc: 0.0874 ::: bot acc: 0.1388
top acc: 0.0512 ::: bot acc: 0.0114
top acc: 0.0311 ::: bot acc: 0.0139
current epoch: 12
train loss is 0.064884
average val loss: 0.045469, accuracy: 0.0471
average test loss: 0.043767, accuracy: 0.0446
case acc: 0.033014335
case acc: 0.05326857
case acc: 0.029975675
case acc: 0.102791026
case acc: 0.027043143
case acc: 0.021468524
top acc: 0.0164 ::: bot acc: 0.0492
top acc: 0.0807 ::: bot acc: 0.0275
top acc: 0.0565 ::: bot acc: 0.0169
top acc: 0.0772 ::: bot acc: 0.1285
top acc: 0.0455 ::: bot acc: 0.0117
top acc: 0.0352 ::: bot acc: 0.0127
current epoch: 13
train loss is 0.062108
average val loss: 0.044360, accuracy: 0.0457
average test loss: 0.042337, accuracy: 0.0427
case acc: 0.02713748
case acc: 0.053241532
case acc: 0.033476446
case acc: 0.09089854
case acc: 0.026307482
case acc: 0.025029378
top acc: 0.0114 ::: bot acc: 0.0429
top acc: 0.0807 ::: bot acc: 0.0274
top acc: 0.0643 ::: bot acc: 0.0118
top acc: 0.0653 ::: bot acc: 0.1166
top acc: 0.0442 ::: bot acc: 0.0121
top acc: 0.0405 ::: bot acc: 0.0128
current epoch: 14
train loss is 0.059945
average val loss: 0.043533, accuracy: 0.0446
average test loss: 0.041184, accuracy: 0.0412
case acc: 0.022987902
case acc: 0.05170142
case acc: 0.03787876
case acc: 0.07886369
case acc: 0.02720585
case acc: 0.028720878
top acc: 0.0089 ::: bot acc: 0.0379
top acc: 0.0791 ::: bot acc: 0.0260
top acc: 0.0711 ::: bot acc: 0.0115
top acc: 0.0532 ::: bot acc: 0.1046
top acc: 0.0458 ::: bot acc: 0.0117
top acc: 0.0455 ::: bot acc: 0.0141
current epoch: 15
train loss is 0.057257
average val loss: 0.041267, accuracy: 0.0422
average test loss: 0.038939, accuracy: 0.0389
case acc: 0.023610724
case acc: 0.044342197
case acc: 0.038672697
case acc: 0.07153604
case acc: 0.026836181
case acc: 0.028672755
top acc: 0.0092 ::: bot acc: 0.0388
top acc: 0.0714 ::: bot acc: 0.0192
top acc: 0.0722 ::: bot acc: 0.0117
top acc: 0.0460 ::: bot acc: 0.0972
top acc: 0.0451 ::: bot acc: 0.0119
top acc: 0.0453 ::: bot acc: 0.0141
current epoch: 16
train loss is 0.053774
average val loss: 0.037680, accuracy: 0.0388
average test loss: 0.035804, accuracy: 0.0359
case acc: 0.02923622
case acc: 0.03269396
case acc: 0.035280738
case acc: 0.06959099
case acc: 0.024299227
case acc: 0.024489084
top acc: 0.0129 ::: bot acc: 0.0453
top acc: 0.0584 ::: bot acc: 0.0102
top acc: 0.0674 ::: bot acc: 0.0112
top acc: 0.0442 ::: bot acc: 0.0952
top acc: 0.0404 ::: bot acc: 0.0138
top acc: 0.0396 ::: bot acc: 0.0130
current epoch: 17
train loss is 0.050687
average val loss: 0.034562, accuracy: 0.0361
average test loss: 0.033533, accuracy: 0.0338
case acc: 0.03702138
case acc: 0.023669483
case acc: 0.03139037
case acc: 0.070384406
case acc: 0.020760218
case acc: 0.01935767
top acc: 0.0200 ::: bot acc: 0.0535
top acc: 0.0450 ::: bot acc: 0.0099
top acc: 0.0602 ::: bot acc: 0.0140
top acc: 0.0450 ::: bot acc: 0.0960
top acc: 0.0326 ::: bot acc: 0.0188
top acc: 0.0315 ::: bot acc: 0.0140
current epoch: 18
train loss is 0.048795
average val loss: 0.032836, accuracy: 0.0345
average test loss: 0.032703, accuracy: 0.0330
case acc: 0.042257905
case acc: 0.020772183
case acc: 0.029422602
case acc: 0.06979388
case acc: 0.019014813
case acc: 0.01673234
top acc: 0.0253 ::: bot acc: 0.0587
top acc: 0.0362 ::: bot acc: 0.0176
top acc: 0.0550 ::: bot acc: 0.0186
top acc: 0.0444 ::: bot acc: 0.0954
top acc: 0.0253 ::: bot acc: 0.0260
top acc: 0.0254 ::: bot acc: 0.0183
current epoch: 19
train loss is 0.047225
average val loss: 0.031678, accuracy: 0.0333
average test loss: 0.031823, accuracy: 0.0320
case acc: 0.04199354
case acc: 0.020480026
case acc: 0.029289989
case acc: 0.06520879
case acc: 0.018696345
case acc: 0.0162894
top acc: 0.0251 ::: bot acc: 0.0584
top acc: 0.0342 ::: bot acc: 0.0197
top acc: 0.0545 ::: bot acc: 0.0193
top acc: 0.0400 ::: bot acc: 0.0907
top acc: 0.0218 ::: bot acc: 0.0296
top acc: 0.0239 ::: bot acc: 0.0198
current epoch: 20
train loss is 0.046307
average val loss: 0.030656, accuracy: 0.0322
average test loss: 0.031001, accuracy: 0.0310
case acc: 0.04131439
case acc: 0.020321392
case acc: 0.029156601
case acc: 0.060535166
case acc: 0.018862724
case acc: 0.01606549
top acc: 0.0245 ::: bot acc: 0.0577
top acc: 0.0330 ::: bot acc: 0.0209
top acc: 0.0541 ::: bot acc: 0.0197
top acc: 0.0354 ::: bot acc: 0.0860
top acc: 0.0187 ::: bot acc: 0.0328
top acc: 0.0227 ::: bot acc: 0.0210
current epoch: 21
train loss is 0.045267
average val loss: 0.029599, accuracy: 0.0309
average test loss: 0.029599, accuracy: 0.0295
case acc: 0.037243497
case acc: 0.02095383
case acc: 0.030229505
case acc: 0.05309074
case acc: 0.018892337
case acc: 0.01644227
top acc: 0.0204 ::: bot acc: 0.0537
top acc: 0.0367 ::: bot acc: 0.0173
top acc: 0.0570 ::: bot acc: 0.0173
top acc: 0.0281 ::: bot acc: 0.0785
top acc: 0.0188 ::: bot acc: 0.0328
top acc: 0.0244 ::: bot acc: 0.0194
current epoch: 22
train loss is 0.044410
average val loss: 0.028776, accuracy: 0.0298
average test loss: 0.028140, accuracy: 0.0277
case acc: 0.03153616
case acc: 0.022552717
case acc: 0.032205373
case acc: 0.043689337
case acc: 0.018736893
case acc: 0.017584722
top acc: 0.0151 ::: bot acc: 0.0478
top acc: 0.0424 ::: bot acc: 0.0120
top acc: 0.0618 ::: bot acc: 0.0136
top acc: 0.0192 ::: bot acc: 0.0689
top acc: 0.0217 ::: bot acc: 0.0298
top acc: 0.0281 ::: bot acc: 0.0159
current epoch: 23
train loss is 0.042828
average val loss: 0.028045, accuracy: 0.0288
average test loss: 0.027183, accuracy: 0.0266
case acc: 0.02924412
case acc: 0.023404438
case acc: 0.0330747
case acc: 0.037530772
case acc: 0.018757295
case acc: 0.017797278
top acc: 0.0132 ::: bot acc: 0.0453
top acc: 0.0444 ::: bot acc: 0.0107
top acc: 0.0636 ::: bot acc: 0.0126
top acc: 0.0141 ::: bot acc: 0.0623
top acc: 0.0226 ::: bot acc: 0.0289
top acc: 0.0287 ::: bot acc: 0.0154
current epoch: 24
train loss is 0.042338
average val loss: 0.027877, accuracy: 0.0282
average test loss: 0.026367, accuracy: 0.0258
case acc: 0.024951318
case acc: 0.025300499
case acc: 0.035759304
case acc: 0.029755566
case acc: 0.019323446
case acc: 0.019559637
top acc: 0.0100 ::: bot acc: 0.0405
top acc: 0.0481 ::: bot acc: 0.0089
top acc: 0.0683 ::: bot acc: 0.0115
top acc: 0.0102 ::: bot acc: 0.0525
top acc: 0.0276 ::: bot acc: 0.0239
top acc: 0.0323 ::: bot acc: 0.0136
current epoch: 25
train loss is 0.041436
average val loss: 0.027580, accuracy: 0.0276
average test loss: 0.025656, accuracy: 0.0251
case acc: 0.023880117
case acc: 0.02490521
case acc: 0.036741428
case acc: 0.024769032
case acc: 0.020438004
case acc: 0.020083182
top acc: 0.0093 ::: bot acc: 0.0392
top acc: 0.0473 ::: bot acc: 0.0092
top acc: 0.0697 ::: bot acc: 0.0115
top acc: 0.0103 ::: bot acc: 0.0450
top acc: 0.0319 ::: bot acc: 0.0196
top acc: 0.0332 ::: bot acc: 0.0134
current epoch: 26
train loss is 0.039742
average val loss: 0.026261, accuracy: 0.0265
average test loss: 0.024627, accuracy: 0.0243
case acc: 0.027286507
case acc: 0.021961948
case acc: 0.03444682
case acc: 0.023332682
case acc: 0.020620298
case acc: 0.018053273
top acc: 0.0116 ::: bot acc: 0.0432
top acc: 0.0409 ::: bot acc: 0.0131
top acc: 0.0661 ::: bot acc: 0.0119
top acc: 0.0114 ::: bot acc: 0.0423
top acc: 0.0326 ::: bot acc: 0.0189
top acc: 0.0292 ::: bot acc: 0.0152
current epoch: 27
train loss is 0.038782
average val loss: 0.025872, accuracy: 0.0261
average test loss: 0.024154, accuracy: 0.0240
case acc: 0.02887737
case acc: 0.020692792
case acc: 0.03356879
case acc: 0.02090739
case acc: 0.022499412
case acc: 0.01765484
top acc: 0.0129 ::: bot acc: 0.0450
top acc: 0.0352 ::: bot acc: 0.0187
top acc: 0.0646 ::: bot acc: 0.0124
top acc: 0.0149 ::: bot acc: 0.0368
top acc: 0.0376 ::: bot acc: 0.0146
top acc: 0.0283 ::: bot acc: 0.0160
current epoch: 28
train loss is 0.038409
average val loss: 0.025956, accuracy: 0.0263
average test loss: 0.024483, accuracy: 0.0246
case acc: 0.032599207
case acc: 0.020497924
case acc: 0.03187408
case acc: 0.019473681
case acc: 0.026013274
case acc: 0.017163176
top acc: 0.0162 ::: bot acc: 0.0489
top acc: 0.0246 ::: bot acc: 0.0293
top acc: 0.0610 ::: bot acc: 0.0145
top acc: 0.0193 ::: bot acc: 0.0322
top acc: 0.0448 ::: bot acc: 0.0109
top acc: 0.0270 ::: bot acc: 0.0173
current epoch: 29
train loss is 0.039658
average val loss: 0.027789, accuracy: 0.0281
average test loss: 0.026700, accuracy: 0.0269
case acc: 0.037804145
case acc: 0.025105877
case acc: 0.029899608
case acc: 0.018899348
case acc: 0.033035588
case acc: 0.016901545
top acc: 0.0212 ::: bot acc: 0.0542
top acc: 0.0119 ::: bot acc: 0.0440
top acc: 0.0560 ::: bot acc: 0.0186
top acc: 0.0235 ::: bot acc: 0.0279
top acc: 0.0550 ::: bot acc: 0.0114
top acc: 0.0262 ::: bot acc: 0.0180
current epoch: 30
train loss is 0.045995
average val loss: 0.033121, accuracy: 0.0345
average test loss: 0.034387, accuracy: 0.0354
case acc: 0.059900727
case acc: 0.054108635
case acc: 0.027565893
case acc: 0.022065707
case acc: 0.031410903
case acc: 0.017520815
top acc: 0.0433 ::: bot acc: 0.0763
top acc: 0.0267 ::: bot acc: 0.0801
top acc: 0.0343 ::: bot acc: 0.0400
top acc: 0.0128 ::: bot acc: 0.0396
top acc: 0.0529 ::: bot acc: 0.0109
top acc: 0.0115 ::: bot acc: 0.0326
current epoch: 31
train loss is 0.065439
average val loss: 0.106487, accuracy: 0.1064
average test loss: 0.113154, accuracy: 0.1131
case acc: 0.15555999
case acc: 0.16657981
case acc: 0.09996108
case acc: 0.10477046
case acc: 0.054162912
case acc: 0.097698584
top acc: 0.1386 ::: bot acc: 0.1720
top acc: 0.1389 ::: bot acc: 0.1929
top acc: 0.0619 ::: bot acc: 0.1357
top acc: 0.0793 ::: bot acc: 0.1304
top acc: 0.0298 ::: bot acc: 0.0811
top acc: 0.0767 ::: bot acc: 0.1203
current epoch: 32
train loss is 0.103988
average val loss: 0.079637, accuracy: 0.0797
average test loss: 0.086027, accuracy: 0.0859
case acc: 0.12183552
case acc: 0.1251853
case acc: 0.07350358
case acc: 0.08680738
case acc: 0.03934469
case acc: 0.06901339
top acc: 0.1048 ::: bot acc: 0.1383
top acc: 0.0974 ::: bot acc: 0.1515
top acc: 0.0356 ::: bot acc: 0.1092
top acc: 0.0614 ::: bot acc: 0.1125
top acc: 0.0158 ::: bot acc: 0.0658
top acc: 0.0482 ::: bot acc: 0.0915
current epoch: 33
train loss is 0.069707
average val loss: 0.049980, accuracy: 0.0510
average test loss: 0.055749, accuracy: 0.0560
case acc: 0.079902604
case acc: 0.0640138
case acc: 0.04411081
case acc: 0.0693829
case acc: 0.037894182
case acc: 0.0407274
top acc: 0.0630 ::: bot acc: 0.0962
top acc: 0.0362 ::: bot acc: 0.0904
top acc: 0.0149 ::: bot acc: 0.0755
top acc: 0.0442 ::: bot acc: 0.0949
top acc: 0.0145 ::: bot acc: 0.0642
top acc: 0.0203 ::: bot acc: 0.0630
current epoch: 34
train loss is 0.045720
average val loss: 0.036004, accuracy: 0.0380
average test loss: 0.040670, accuracy: 0.0413
case acc: 0.053755496
case acc: 0.026996676
case acc: 0.03256468
case acc: 0.06266957
case acc: 0.044316683
case acc: 0.02734666
top acc: 0.0369 ::: bot acc: 0.0701
top acc: 0.0104 ::: bot acc: 0.0478
top acc: 0.0186 ::: bot acc: 0.0563
top acc: 0.0377 ::: bot acc: 0.0881
top acc: 0.0202 ::: bot acc: 0.0710
top acc: 0.0095 ::: bot acc: 0.0483
current epoch: 35
train loss is 0.040225
average val loss: 0.025754, accuracy: 0.0260
average test loss: 0.025119, accuracy: 0.0243
case acc: 0.01883799
case acc: 0.025902392
case acc: 0.028622713
case acc: 0.032187574
case acc: 0.022582669
case acc: 0.01786394
top acc: 0.0081 ::: bot acc: 0.0321
top acc: 0.0493 ::: bot acc: 0.0084
top acc: 0.0523 ::: bot acc: 0.0217
top acc: 0.0112 ::: bot acc: 0.0556
top acc: 0.0092 ::: bot acc: 0.0439
top acc: 0.0291 ::: bot acc: 0.0148
current epoch: 36
train loss is 0.040834
average val loss: 0.032993, accuracy: 0.0322
average test loss: 0.028999, accuracy: 0.0278
case acc: 0.012769889
case acc: 0.045257546
case acc: 0.039423347
case acc: 0.019120298
case acc: 0.019399306
case acc: 0.03091458
top acc: 0.0224 ::: bot acc: 0.0107
top acc: 0.0728 ::: bot acc: 0.0195
top acc: 0.0734 ::: bot acc: 0.0119
top acc: 0.0193 ::: bot acc: 0.0316
top acc: 0.0282 ::: bot acc: 0.0230
top acc: 0.0487 ::: bot acc: 0.0149
current epoch: 37
train loss is 0.043838
average val loss: 0.044528, accuracy: 0.0442
average test loss: 0.038642, accuracy: 0.0379
case acc: 0.02117567
case acc: 0.061111886
case acc: 0.05301968
case acc: 0.022586469
case acc: 0.026979089
case acc: 0.042432327
top acc: 0.0370 ::: bot acc: 0.0068
top acc: 0.0890 ::: bot acc: 0.0347
top acc: 0.0898 ::: bot acc: 0.0200
top acc: 0.0402 ::: bot acc: 0.0124
top acc: 0.0460 ::: bot acc: 0.0108
top acc: 0.0623 ::: bot acc: 0.0222
current epoch: 38
train loss is 0.045869
average val loss: 0.050187, accuracy: 0.0501
average test loss: 0.043815, accuracy: 0.0431
case acc: 0.023502382
case acc: 0.062509306
case acc: 0.05859392
case acc: 0.03132554
case acc: 0.036731128
case acc: 0.04593044
top acc: 0.0397 ::: bot acc: 0.0084
top acc: 0.0903 ::: bot acc: 0.0361
top acc: 0.0959 ::: bot acc: 0.0244
top acc: 0.0534 ::: bot acc: 0.0121
top acc: 0.0594 ::: bot acc: 0.0134
top acc: 0.0662 ::: bot acc: 0.0249
current epoch: 39
train loss is 0.042969
average val loss: 0.041183, accuracy: 0.0408
average test loss: 0.035415, accuracy: 0.0344
case acc: 0.013084502
case acc: 0.041710377
case acc: 0.04828122
case acc: 0.029755088
case acc: 0.038437333
case acc: 0.035042673
top acc: 0.0235 ::: bot acc: 0.0097
top acc: 0.0691 ::: bot acc: 0.0163
top acc: 0.0843 ::: bot acc: 0.0167
top acc: 0.0513 ::: bot acc: 0.0116
top acc: 0.0614 ::: bot acc: 0.0145
top acc: 0.0536 ::: bot acc: 0.0173
current epoch: 40
train loss is 0.036006
average val loss: 0.028009, accuracy: 0.0281
average test loss: 0.024771, accuracy: 0.0246
case acc: 0.021606594
case acc: 0.02087634
case acc: 0.03229614
case acc: 0.021809416
case acc: 0.03213591
case acc: 0.019024417
top acc: 0.0085 ::: bot acc: 0.0360
top acc: 0.0358 ::: bot acc: 0.0184
top acc: 0.0620 ::: bot acc: 0.0134
top acc: 0.0386 ::: bot acc: 0.0132
top acc: 0.0535 ::: bot acc: 0.0114
top acc: 0.0316 ::: bot acc: 0.0133
current epoch: 41
train loss is 0.036922
average val loss: 0.026051, accuracy: 0.0269
average test loss: 0.026061, accuracy: 0.0268
case acc: 0.042596012
case acc: 0.028461602
case acc: 0.026785968
case acc: 0.018577704
case acc: 0.027930144
case acc: 0.016538886
top acc: 0.0258 ::: bot acc: 0.0589
top acc: 0.0101 ::: bot acc: 0.0501
top acc: 0.0411 ::: bot acc: 0.0329
top acc: 0.0261 ::: bot acc: 0.0248
top acc: 0.0476 ::: bot acc: 0.0107
top acc: 0.0137 ::: bot acc: 0.0298
current epoch: 42
train loss is 0.050544
average val loss: 0.058802, accuracy: 0.0591
average test loss: 0.064624, accuracy: 0.0648
case acc: 0.1011738
case acc: 0.096017115
case acc: 0.05660554
case acc: 0.051766396
case acc: 0.024223013
case acc: 0.058965024
top acc: 0.0843 ::: bot acc: 0.1174
top acc: 0.0682 ::: bot acc: 0.1224
top acc: 0.0215 ::: bot acc: 0.0909
top acc: 0.0273 ::: bot acc: 0.0770
top acc: 0.0083 ::: bot acc: 0.0468
top acc: 0.0380 ::: bot acc: 0.0815
current epoch: 43
train loss is 0.076725
average val loss: 0.086446, accuracy: 0.0865
average test loss: 0.093020, accuracy: 0.0930
case acc: 0.12607634
case acc: 0.12353562
case acc: 0.084067196
case acc: 0.08550075
case acc: 0.0546525
case acc: 0.08410769
top acc: 0.1092 ::: bot acc: 0.1423
top acc: 0.0957 ::: bot acc: 0.1499
top acc: 0.0459 ::: bot acc: 0.1199
top acc: 0.0603 ::: bot acc: 0.1111
top acc: 0.0303 ::: bot acc: 0.0814
top acc: 0.0632 ::: bot acc: 0.1066
current epoch: 44
train loss is 0.067603
average val loss: 0.043987, accuracy: 0.0450
average test loss: 0.049554, accuracy: 0.0499
case acc: 0.071313255
case acc: 0.05372477
case acc: 0.04222983
case acc: 0.05421257
case acc: 0.03753747
case acc: 0.04008308
top acc: 0.0545 ::: bot acc: 0.0875
top acc: 0.0262 ::: bot acc: 0.0800
top acc: 0.0145 ::: bot acc: 0.0729
top acc: 0.0297 ::: bot acc: 0.0795
top acc: 0.0143 ::: bot acc: 0.0638
top acc: 0.0196 ::: bot acc: 0.0623
current epoch: 45
train loss is 0.042598
average val loss: 0.027232, accuracy: 0.0291
average test loss: 0.030410, accuracy: 0.0310
case acc: 0.038532004
case acc: 0.021144424
case acc: 0.02910872
case acc: 0.04085472
case acc: 0.035417043
case acc: 0.0209292
top acc: 0.0217 ::: bot acc: 0.0547
top acc: 0.0217 ::: bot acc: 0.0326
top acc: 0.0273 ::: bot acc: 0.0468
top acc: 0.0172 ::: bot acc: 0.0657
top acc: 0.0128 ::: bot acc: 0.0613
top acc: 0.0079 ::: bot acc: 0.0394
current epoch: 46
train loss is 0.037356
average val loss: 0.027739, accuracy: 0.0269
average test loss: 0.024684, accuracy: 0.0235
case acc: 0.012136376
case acc: 0.03518759
case acc: 0.032387987
case acc: 0.01910077
case acc: 0.018656122
case acc: 0.02351307
top acc: 0.0155 ::: bot acc: 0.0175
top acc: 0.0618 ::: bot acc: 0.0113
top acc: 0.0621 ::: bot acc: 0.0134
top acc: 0.0186 ::: bot acc: 0.0321
top acc: 0.0219 ::: bot acc: 0.0292
top acc: 0.0389 ::: bot acc: 0.0123
current epoch: 47
train loss is 0.038631
average val loss: 0.036564, accuracy: 0.0360
average test loss: 0.031203, accuracy: 0.0304
case acc: 0.015009442
case acc: 0.04560796
case acc: 0.04121612
case acc: 0.02136927
case acc: 0.025197618
case acc: 0.033789553
top acc: 0.0280 ::: bot acc: 0.0064
top acc: 0.0732 ::: bot acc: 0.0197
top acc: 0.0758 ::: bot acc: 0.0125
top acc: 0.0374 ::: bot acc: 0.0139
top acc: 0.0427 ::: bot acc: 0.0121
top acc: 0.0522 ::: bot acc: 0.0164
current epoch: 48
train loss is 0.038608
average val loss: 0.034759, accuracy: 0.0343
average test loss: 0.029495, accuracy: 0.0286
case acc: 0.012590465
case acc: 0.037378687
case acc: 0.038785715
case acc: 0.023357973
case acc: 0.02882081
case acc: 0.030449653
top acc: 0.0220 ::: bot acc: 0.0109
top acc: 0.0643 ::: bot acc: 0.0128
top acc: 0.0727 ::: bot acc: 0.0115
top acc: 0.0414 ::: bot acc: 0.0118
top acc: 0.0489 ::: bot acc: 0.0107
top acc: 0.0481 ::: bot acc: 0.0146
current epoch: 49
train loss is 0.034780
average val loss: 0.026575, accuracy: 0.0264
average test loss: 0.023034, accuracy: 0.0228
case acc: 0.016093966
case acc: 0.022997282
case acc: 0.030940989
case acc: 0.02031481
case acc: 0.026226228
case acc: 0.020056248
top acc: 0.0083 ::: bot acc: 0.0278
top acc: 0.0436 ::: bot acc: 0.0111
top acc: 0.0587 ::: bot acc: 0.0160
top acc: 0.0347 ::: bot acc: 0.0160
top acc: 0.0447 ::: bot acc: 0.0114
top acc: 0.0336 ::: bot acc: 0.0126
current epoch: 50
train loss is 0.033431
average val loss: 0.022794, accuracy: 0.0234
average test loss: 0.021820, accuracy: 0.0225
case acc: 0.028552618
case acc: 0.021088323
case acc: 0.026992919
case acc: 0.018593496
case acc: 0.02391701
case acc: 0.015797917
top acc: 0.0127 ::: bot acc: 0.0443
top acc: 0.0218 ::: bot acc: 0.0324
top acc: 0.0440 ::: bot acc: 0.0301
top acc: 0.0265 ::: bot acc: 0.0243
top acc: 0.0403 ::: bot acc: 0.0133
top acc: 0.0199 ::: bot acc: 0.0237
LME_Co_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2009-07-01', '2010-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6798 6798 6798
1.8562728 -0.6288155 0.21141115 -0.19947179
Validation: 756 756 756
Testing: 744 744 744
pre-processing time: 0.0008411407470703125
the split date is 2010-01-01
net initializing with time: 0.006041288375854492
preparing training and testing date with time: 4.76837158203125e-07
current epoch: 1
train loss is 0.155670
average val loss: 0.038509, accuracy: 0.0389
average test loss: 0.042187, accuracy: 0.0431
case acc: 0.047841385
case acc: 0.020571487
case acc: 0.035113297
case acc: 0.042977776
case acc: 0.024058377
case acc: 0.0882221
top acc: 0.0228 ::: bot acc: 0.0759
top acc: 0.0164 ::: bot acc: 0.0332
top acc: 0.0547 ::: bot acc: 0.0361
top acc: 0.0210 ::: bot acc: 0.0698
top acc: 0.0406 ::: bot acc: 0.0244
top acc: 0.1325 ::: bot acc: 0.0489
current epoch: 2
train loss is 0.055237
average val loss: 0.114448, accuracy: 0.1143
average test loss: 0.116578, accuracy: 0.1173
case acc: 0.16043906
case acc: 0.12606487
case acc: 0.10881529
case acc: 0.15056504
case acc: 0.11105053
case acc: 0.046903487
top acc: 0.1292 ::: bot acc: 0.1918
top acc: 0.1017 ::: bot acc: 0.1498
top acc: 0.0643 ::: bot acc: 0.1546
top acc: 0.1175 ::: bot acc: 0.1831
top acc: 0.0783 ::: bot acc: 0.1419
top acc: 0.0208 ::: bot acc: 0.0766
current epoch: 3
train loss is 0.088819
average val loss: 0.120602, accuracy: 0.1206
average test loss: 0.122412, accuracy: 0.1228
case acc: 0.16152833
case acc: 0.12885363
case acc: 0.116154686
case acc: 0.15623713
case acc: 0.11599475
case acc: 0.058098942
top acc: 0.1295 ::: bot acc: 0.1934
top acc: 0.1044 ::: bot acc: 0.1533
top acc: 0.0717 ::: bot acc: 0.1615
top acc: 0.1239 ::: bot acc: 0.1884
top acc: 0.0836 ::: bot acc: 0.1463
top acc: 0.0246 ::: bot acc: 0.0910
current epoch: 4
train loss is 0.106103
average val loss: 0.050977, accuracy: 0.0506
average test loss: 0.054234, accuracy: 0.0553
case acc: 0.08087032
case acc: 0.05106218
case acc: 0.04687287
case acc: 0.082234554
case acc: 0.042101834
case acc: 0.028622067
top acc: 0.0488 ::: bot acc: 0.1128
top acc: 0.0281 ::: bot acc: 0.0748
top acc: 0.0140 ::: bot acc: 0.0865
top acc: 0.0520 ::: bot acc: 0.1134
top acc: 0.0158 ::: bot acc: 0.0695
top acc: 0.0542 ::: bot acc: 0.0233
current epoch: 5
train loss is 0.065942
average val loss: 0.078775, accuracy: 0.0785
average test loss: 0.081115, accuracy: 0.0819
case acc: 0.1088622
case acc: 0.08002411
case acc: 0.07589281
case acc: 0.11517796
case acc: 0.07087668
case acc: 0.04035325
top acc: 0.0767 ::: bot acc: 0.1408
top acc: 0.0551 ::: bot acc: 0.1048
top acc: 0.0324 ::: bot acc: 0.1208
top acc: 0.0837 ::: bot acc: 0.1470
top acc: 0.0387 ::: bot acc: 0.1011
top acc: 0.0213 ::: bot acc: 0.0658
current epoch: 6
train loss is 0.071591
average val loss: 0.078586, accuracy: 0.0784
average test loss: 0.080722, accuracy: 0.0813
case acc: 0.10375404
case acc: 0.076621436
case acc: 0.07680324
case acc: 0.11585728
case acc: 0.069052175
case acc: 0.045688987
top acc: 0.0715 ::: bot acc: 0.1356
top acc: 0.0519 ::: bot acc: 0.1014
top acc: 0.0334 ::: bot acc: 0.1216
top acc: 0.0846 ::: bot acc: 0.1476
top acc: 0.0370 ::: bot acc: 0.0990
top acc: 0.0205 ::: bot acc: 0.0742
current epoch: 7
train loss is 0.071236
average val loss: 0.063814, accuracy: 0.0636
average test loss: 0.066468, accuracy: 0.0670
case acc: 0.083799064
case acc: 0.05879524
case acc: 0.06345008
case acc: 0.10212723
case acc: 0.052933544
case acc: 0.041040175
top acc: 0.0515 ::: bot acc: 0.1155
top acc: 0.0351 ::: bot acc: 0.0833
top acc: 0.0216 ::: bot acc: 0.1075
top acc: 0.0714 ::: bot acc: 0.1335
top acc: 0.0225 ::: bot acc: 0.0820
top acc: 0.0208 ::: bot acc: 0.0673
current epoch: 8
train loss is 0.064245
average val loss: 0.057766, accuracy: 0.0576
average test loss: 0.060720, accuracy: 0.0612
case acc: 0.07315132
case acc: 0.04997706
case acc: 0.059050128
case acc: 0.0967963
case acc: 0.046199426
case acc: 0.042078573
top acc: 0.0411 ::: bot acc: 0.1047
top acc: 0.0272 ::: bot acc: 0.0742
top acc: 0.0188 ::: bot acc: 0.1022
top acc: 0.0664 ::: bot acc: 0.1280
top acc: 0.0182 ::: bot acc: 0.0739
top acc: 0.0206 ::: bot acc: 0.0691
current epoch: 9
train loss is 0.055834
average val loss: 0.056826, accuracy: 0.0568
average test loss: 0.059748, accuracy: 0.0601
case acc: 0.068191
case acc: 0.04647296
case acc: 0.059134
case acc: 0.09636832
case acc: 0.044159755
case acc: 0.046029896
top acc: 0.0363 ::: bot acc: 0.0995
top acc: 0.0242 ::: bot acc: 0.0706
top acc: 0.0189 ::: bot acc: 0.1022
top acc: 0.0662 ::: bot acc: 0.1274
top acc: 0.0170 ::: bot acc: 0.0713
top acc: 0.0203 ::: bot acc: 0.0752
current epoch: 10
train loss is 0.048832
average val loss: 0.056019, accuracy: 0.0561
average test loss: 0.058941, accuracy: 0.0591
case acc: 0.064074114
case acc: 0.04366613
case acc: 0.059204753
case acc: 0.095965154
case acc: 0.042520724
case acc: 0.049207974
top acc: 0.0326 ::: bot acc: 0.0952
top acc: 0.0217 ::: bot acc: 0.0678
top acc: 0.0189 ::: bot acc: 0.1023
top acc: 0.0659 ::: bot acc: 0.1269
top acc: 0.0163 ::: bot acc: 0.0691
top acc: 0.0206 ::: bot acc: 0.0797
current epoch: 11
train loss is 0.042986
average val loss: 0.049540, accuracy: 0.0497
average test loss: 0.053007, accuracy: 0.0532
case acc: 0.055618867
case acc: 0.03671849
case acc: 0.054258417
case acc: 0.089090474
case acc: 0.036994096
case acc: 0.04663527
top acc: 0.0260 ::: bot acc: 0.0859
top acc: 0.0167 ::: bot acc: 0.0600
top acc: 0.0162 ::: bot acc: 0.0962
top acc: 0.0594 ::: bot acc: 0.1199
top acc: 0.0146 ::: bot acc: 0.0617
top acc: 0.0203 ::: bot acc: 0.0759
current epoch: 12
train loss is 0.039224
average val loss: 0.041298, accuracy: 0.0416
average test loss: 0.045565, accuracy: 0.0459
case acc: 0.046913616
case acc: 0.02997924
case acc: 0.04799577
case acc: 0.077695355
case acc: 0.03136175
case acc: 0.041293453
top acc: 0.0203 ::: bot acc: 0.0757
top acc: 0.0138 ::: bot acc: 0.0516
top acc: 0.0141 ::: bot acc: 0.0879
top acc: 0.0487 ::: bot acc: 0.1081
top acc: 0.0147 ::: bot acc: 0.0531
top acc: 0.0204 ::: bot acc: 0.0679
current epoch: 13
train loss is 0.037262
average val loss: 0.036330, accuracy: 0.0368
average test loss: 0.041093, accuracy: 0.0415
case acc: 0.042148527
case acc: 0.026569994
case acc: 0.044901364
case acc: 0.06818606
case acc: 0.028716017
case acc: 0.03820957
top acc: 0.0184 ::: bot acc: 0.0696
top acc: 0.0132 ::: bot acc: 0.0469
top acc: 0.0146 ::: bot acc: 0.0830
top acc: 0.0399 ::: bot acc: 0.0982
top acc: 0.0159 ::: bot acc: 0.0484
top acc: 0.0210 ::: bot acc: 0.0629
current epoch: 14
train loss is 0.036056
average val loss: 0.034210, accuracy: 0.0346
average test loss: 0.039193, accuracy: 0.0395
case acc: 0.040492076
case acc: 0.026029356
case acc: 0.04389779
case acc: 0.061450593
case acc: 0.028316805
case acc: 0.03701876
top acc: 0.0179 ::: bot acc: 0.0674
top acc: 0.0131 ::: bot acc: 0.0461
top acc: 0.0150 ::: bot acc: 0.0813
top acc: 0.0336 ::: bot acc: 0.0912
top acc: 0.0161 ::: bot acc: 0.0477
top acc: 0.0215 ::: bot acc: 0.0608
current epoch: 15
train loss is 0.034950
average val loss: 0.030673, accuracy: 0.0311
average test loss: 0.036034, accuracy: 0.0364
case acc: 0.03744348
case acc: 0.023981843
case acc: 0.04179764
case acc: 0.053282842
case acc: 0.026853822
case acc: 0.03480639
top acc: 0.0175 ::: bot acc: 0.0631
top acc: 0.0129 ::: bot acc: 0.0431
top acc: 0.0164 ::: bot acc: 0.0775
top acc: 0.0263 ::: bot acc: 0.0826
top acc: 0.0175 ::: bot acc: 0.0449
top acc: 0.0230 ::: bot acc: 0.0566
current epoch: 16
train loss is 0.034243
average val loss: 0.028167, accuracy: 0.0286
average test loss: 0.033809, accuracy: 0.0341
case acc: 0.03551737
case acc: 0.022885226
case acc: 0.040357225
case acc: 0.04662477
case acc: 0.026046453
case acc: 0.033242516
top acc: 0.0177 ::: bot acc: 0.0602
top acc: 0.0132 ::: bot acc: 0.0412
top acc: 0.0177 ::: bot acc: 0.0748
top acc: 0.0210 ::: bot acc: 0.0753
top acc: 0.0187 ::: bot acc: 0.0432
top acc: 0.0248 ::: bot acc: 0.0534
current epoch: 17
train loss is 0.033213
average val loss: 0.026147, accuracy: 0.0266
average test loss: 0.031976, accuracy: 0.0322
case acc: 0.03375173
case acc: 0.02179502
case acc: 0.039185476
case acc: 0.041339133
case acc: 0.025281662
case acc: 0.032048617
top acc: 0.0181 ::: bot acc: 0.0573
top acc: 0.0137 ::: bot acc: 0.0393
top acc: 0.0190 ::: bot acc: 0.0724
top acc: 0.0177 ::: bot acc: 0.0690
top acc: 0.0199 ::: bot acc: 0.0414
top acc: 0.0262 ::: bot acc: 0.0509
current epoch: 18
train loss is 0.032998
average val loss: 0.024436, accuracy: 0.0249
average test loss: 0.030398, accuracy: 0.0306
case acc: 0.03223067
case acc: 0.020952303
case acc: 0.038093925
case acc: 0.03676029
case acc: 0.024660287
case acc: 0.030843515
top acc: 0.0186 ::: bot acc: 0.0547
top acc: 0.0145 ::: bot acc: 0.0376
top acc: 0.0206 ::: bot acc: 0.0700
top acc: 0.0155 ::: bot acc: 0.0633
top acc: 0.0212 ::: bot acc: 0.0398
top acc: 0.0280 ::: bot acc: 0.0483
current epoch: 19
train loss is 0.032490
average val loss: 0.024175, accuracy: 0.0245
average test loss: 0.030169, accuracy: 0.0303
case acc: 0.03212563
case acc: 0.02129951
case acc: 0.038125105
case acc: 0.034395434
case acc: 0.024927413
case acc: 0.03086726
top acc: 0.0186 ::: bot acc: 0.0545
top acc: 0.0142 ::: bot acc: 0.0381
top acc: 0.0205 ::: bot acc: 0.0701
top acc: 0.0147 ::: bot acc: 0.0602
top acc: 0.0206 ::: bot acc: 0.0405
top acc: 0.0280 ::: bot acc: 0.0482
current epoch: 20
train loss is 0.032152
average val loss: 0.021757, accuracy: 0.0221
average test loss: 0.027772, accuracy: 0.0278
case acc: 0.029282933
case acc: 0.01936853
case acc: 0.03628946
case acc: 0.029319987
case acc: 0.023395998
case acc: 0.029168848
top acc: 0.0200 ::: bot acc: 0.0496
top acc: 0.0170 ::: bot acc: 0.0339
top acc: 0.0239 ::: bot acc: 0.0657
top acc: 0.0137 ::: bot acc: 0.0531
top acc: 0.0245 ::: bot acc: 0.0363
top acc: 0.0315 ::: bot acc: 0.0438
current epoch: 21
train loss is 0.031695
average val loss: 0.021590, accuracy: 0.0219
average test loss: 0.027588, accuracy: 0.0276
case acc: 0.029051071
case acc: 0.019371998
case acc: 0.036238696
case acc: 0.028174518
case acc: 0.023435293
case acc: 0.029196648
top acc: 0.0202 ::: bot acc: 0.0491
top acc: 0.0169 ::: bot acc: 0.0338
top acc: 0.0240 ::: bot acc: 0.0656
top acc: 0.0141 ::: bot acc: 0.0513
top acc: 0.0244 ::: bot acc: 0.0364
top acc: 0.0316 ::: bot acc: 0.0439
current epoch: 22
train loss is 0.031608
average val loss: 0.020396, accuracy: 0.0207
average test loss: 0.026303, accuracy: 0.0263
case acc: 0.027221357
case acc: 0.018464899
case acc: 0.03530624
case acc: 0.02595484
case acc: 0.02249499
case acc: 0.028237797
top acc: 0.0216 ::: bot acc: 0.0456
top acc: 0.0198 ::: bot acc: 0.0307
top acc: 0.0270 ::: bot acc: 0.0625
top acc: 0.0163 ::: bot acc: 0.0469
top acc: 0.0274 ::: bot acc: 0.0334
top acc: 0.0344 ::: bot acc: 0.0410
current epoch: 23
train loss is 0.031276
average val loss: 0.020841, accuracy: 0.0211
average test loss: 0.026773, accuracy: 0.0267
case acc: 0.02785427
case acc: 0.018755626
case acc: 0.035706215
case acc: 0.026179258
case acc: 0.02298761
case acc: 0.028766686
top acc: 0.0210 ::: bot acc: 0.0468
top acc: 0.0182 ::: bot acc: 0.0321
top acc: 0.0256 ::: bot acc: 0.0640
top acc: 0.0160 ::: bot acc: 0.0474
top acc: 0.0258 ::: bot acc: 0.0350
top acc: 0.0329 ::: bot acc: 0.0427
current epoch: 24
train loss is 0.031266
average val loss: 0.021447, accuracy: 0.0217
average test loss: 0.027396, accuracy: 0.0273
case acc: 0.0287197
case acc: 0.019443378
case acc: 0.036205176
case acc: 0.02656383
case acc: 0.023635553
case acc: 0.029291583
top acc: 0.0204 ::: bot acc: 0.0484
top acc: 0.0165 ::: bot acc: 0.0340
top acc: 0.0241 ::: bot acc: 0.0655
top acc: 0.0156 ::: bot acc: 0.0482
top acc: 0.0239 ::: bot acc: 0.0369
top acc: 0.0314 ::: bot acc: 0.0442
current epoch: 25
train loss is 0.031272
average val loss: 0.019572, accuracy: 0.0199
average test loss: 0.025346, accuracy: 0.0252
case acc: 0.025378194
case acc: 0.01813284
case acc: 0.034563538
case acc: 0.02380275
case acc: 0.02190908
case acc: 0.02763962
top acc: 0.0237 ::: bot acc: 0.0418
top acc: 0.0226 ::: bot acc: 0.0277
top acc: 0.0300 ::: bot acc: 0.0596
top acc: 0.0209 ::: bot acc: 0.0414
top acc: 0.0301 ::: bot acc: 0.0307
top acc: 0.0365 ::: bot acc: 0.0388
current epoch: 26
train loss is 0.030920
average val loss: 0.019420, accuracy: 0.0197
average test loss: 0.025157, accuracy: 0.0250
case acc: 0.024867851
case acc: 0.01807721
case acc: 0.034398835
case acc: 0.023577359
case acc: 0.0217843
case acc: 0.027572513
top acc: 0.0246 ::: bot acc: 0.0406
top acc: 0.0238 ::: bot acc: 0.0266
top acc: 0.0306 ::: bot acc: 0.0589
top acc: 0.0216 ::: bot acc: 0.0407
top acc: 0.0311 ::: bot acc: 0.0297
top acc: 0.0367 ::: bot acc: 0.0386
current epoch: 27
train loss is 0.030893
average val loss: 0.019762, accuracy: 0.0200
average test loss: 0.025572, accuracy: 0.0255
case acc: 0.02563505
case acc: 0.018150555
case acc: 0.03470295
case acc: 0.024346655
case acc: 0.02204345
case acc: 0.0279218
top acc: 0.0232 ::: bot acc: 0.0424
top acc: 0.0219 ::: bot acc: 0.0284
top acc: 0.0291 ::: bot acc: 0.0604
top acc: 0.0193 ::: bot acc: 0.0430
top acc: 0.0294 ::: bot acc: 0.0315
top acc: 0.0353 ::: bot acc: 0.0400
current epoch: 28
train loss is 0.030806
average val loss: 0.020474, accuracy: 0.0207
average test loss: 0.026352, accuracy: 0.0263
case acc: 0.027025996
case acc: 0.018569248
case acc: 0.035255197
case acc: 0.025416782
case acc: 0.022805361
case acc: 0.028429475
top acc: 0.0217 ::: bot acc: 0.0453
top acc: 0.0190 ::: bot acc: 0.0312
top acc: 0.0268 ::: bot acc: 0.0626
top acc: 0.0169 ::: bot acc: 0.0458
top acc: 0.0265 ::: bot acc: 0.0344
top acc: 0.0335 ::: bot acc: 0.0417
current epoch: 29
train loss is 0.030757
average val loss: 0.021481, accuracy: 0.0216
average test loss: 0.027406, accuracy: 0.0273
case acc: 0.028624874
case acc: 0.019648215
case acc: 0.0360209
case acc: 0.026403723
case acc: 0.023892883
case acc: 0.029186664
top acc: 0.0204 ::: bot acc: 0.0483
top acc: 0.0160 ::: bot acc: 0.0345
top acc: 0.0244 ::: bot acc: 0.0651
top acc: 0.0156 ::: bot acc: 0.0479
top acc: 0.0233 ::: bot acc: 0.0377
top acc: 0.0314 ::: bot acc: 0.0439
current epoch: 30
train loss is 0.030796
average val loss: 0.019894, accuracy: 0.0201
average test loss: 0.025694, accuracy: 0.0256
case acc: 0.025895638
case acc: 0.018305603
case acc: 0.034728326
case acc: 0.02406541
case acc: 0.022477737
case acc: 0.02793299
top acc: 0.0227 ::: bot acc: 0.0431
top acc: 0.0205 ::: bot acc: 0.0297
top acc: 0.0290 ::: bot acc: 0.0605
top acc: 0.0203 ::: bot acc: 0.0421
top acc: 0.0280 ::: bot acc: 0.0331
top acc: 0.0355 ::: bot acc: 0.0399
current epoch: 31
train loss is 0.030529
average val loss: 0.019317, accuracy: 0.0195
average test loss: 0.025001, accuracy: 0.0248
case acc: 0.02441783
case acc: 0.017999848
case acc: 0.03417381
case acc: 0.023069022
case acc: 0.021883119
case acc: 0.027463548
top acc: 0.0253 ::: bot acc: 0.0396
top acc: 0.0238 ::: bot acc: 0.0264
top acc: 0.0316 ::: bot acc: 0.0579
top acc: 0.0236 ::: bot acc: 0.0388
top acc: 0.0312 ::: bot acc: 0.0300
top acc: 0.0374 ::: bot acc: 0.0380
current epoch: 32
train loss is 0.030630
average val loss: 0.018998, accuracy: 0.0192
average test loss: 0.024481, accuracy: 0.0244
case acc: 0.023190044
case acc: 0.018301511
case acc: 0.033657264
case acc: 0.022498166
case acc: 0.021967502
case acc: 0.026883136
top acc: 0.0297 ::: bot acc: 0.0349
top acc: 0.0284 ::: bot acc: 0.0217
top acc: 0.0355 ::: bot acc: 0.0541
top acc: 0.0267 ::: bot acc: 0.0358
top acc: 0.0358 ::: bot acc: 0.0254
top acc: 0.0405 ::: bot acc: 0.0349
current epoch: 33
train loss is 0.030543
average val loss: 0.018972, accuracy: 0.0192
average test loss: 0.024428, accuracy: 0.0244
case acc: 0.023106596
case acc: 0.018516343
case acc: 0.033562854
case acc: 0.022657521
case acc: 0.022047011
case acc: 0.026790954
top acc: 0.0307 ::: bot acc: 0.0340
top acc: 0.0296 ::: bot acc: 0.0204
top acc: 0.0362 ::: bot acc: 0.0533
top acc: 0.0254 ::: bot acc: 0.0371
top acc: 0.0370 ::: bot acc: 0.0241
top acc: 0.0410 ::: bot acc: 0.0344
current epoch: 34
train loss is 0.030533
average val loss: 0.019121, accuracy: 0.0194
average test loss: 0.024770, accuracy: 0.0248
case acc: 0.023842577
case acc: 0.01793407
case acc: 0.033867095
case acc: 0.024192154
case acc: 0.021821467
case acc: 0.027124865
top acc: 0.0267 ::: bot acc: 0.0379
top acc: 0.0259 ::: bot acc: 0.0240
top acc: 0.0334 ::: bot acc: 0.0561
top acc: 0.0199 ::: bot acc: 0.0426
top acc: 0.0335 ::: bot acc: 0.0276
top acc: 0.0390 ::: bot acc: 0.0365
current epoch: 35
train loss is 0.030519
average val loss: 0.021052, accuracy: 0.0213
average test loss: 0.026939, accuracy: 0.0270
case acc: 0.02784663
case acc: 0.019006783
case acc: 0.03531349
case acc: 0.028011912
case acc: 0.023393506
case acc: 0.02836318
top acc: 0.0209 ::: bot acc: 0.0468
top acc: 0.0173 ::: bot acc: 0.0327
top acc: 0.0267 ::: bot acc: 0.0628
top acc: 0.0141 ::: bot acc: 0.0511
top acc: 0.0251 ::: bot acc: 0.0361
top acc: 0.0340 ::: bot acc: 0.0414
current epoch: 36
train loss is 0.030876
average val loss: 0.027586, accuracy: 0.0276
average test loss: 0.033190, accuracy: 0.0332
case acc: 0.035845473
case acc: 0.026920188
case acc: 0.039922338
case acc: 0.03536273
case acc: 0.029254345
case acc: 0.032041624
top acc: 0.0177 ::: bot acc: 0.0604
top acc: 0.0131 ::: bot acc: 0.0466
top acc: 0.0181 ::: bot acc: 0.0741
top acc: 0.0149 ::: bot acc: 0.0617
top acc: 0.0150 ::: bot acc: 0.0498
top acc: 0.0261 ::: bot acc: 0.0508
current epoch: 37
train loss is 0.032572
average val loss: 0.028611, accuracy: 0.0285
average test loss: 0.034091, accuracy: 0.0340
case acc: 0.036997743
case acc: 0.028940748
case acc: 0.04086618
case acc: 0.033126954
case acc: 0.031017765
case acc: 0.032956604
top acc: 0.0176 ::: bot acc: 0.0622
top acc: 0.0134 ::: bot acc: 0.0495
top acc: 0.0172 ::: bot acc: 0.0759
top acc: 0.0142 ::: bot acc: 0.0587
top acc: 0.0142 ::: bot acc: 0.0528
top acc: 0.0249 ::: bot acc: 0.0527
current epoch: 38
train loss is 0.033657
average val loss: 0.019182, accuracy: 0.0192
average test loss: 0.024569, accuracy: 0.0243
case acc: 0.023151549
case acc: 0.01820618
case acc: 0.03346366
case acc: 0.022387298
case acc: 0.021811204
case acc: 0.026542187
top acc: 0.0308 ::: bot acc: 0.0340
top acc: 0.0278 ::: bot acc: 0.0224
top acc: 0.0369 ::: bot acc: 0.0525
top acc: 0.0328 ::: bot acc: 0.0294
top acc: 0.0348 ::: bot acc: 0.0263
top acc: 0.0418 ::: bot acc: 0.0332
current epoch: 39
train loss is 0.032184
average val loss: 0.027182, accuracy: 0.0273
average test loss: 0.030476, accuracy: 0.0306
case acc: 0.029992614
case acc: 0.029585978
case acc: 0.034432445
case acc: 0.029155953
case acc: 0.03115151
case acc: 0.029252104
top acc: 0.0551 ::: bot acc: 0.0142
top acc: 0.0519 ::: bot acc: 0.0095
top acc: 0.0559 ::: bot acc: 0.0335
top acc: 0.0520 ::: bot acc: 0.0138
top acc: 0.0584 ::: bot acc: 0.0110
top acc: 0.0565 ::: bot acc: 0.0190
current epoch: 40
train loss is 0.034527
average val loss: 0.019299, accuracy: 0.0195
average test loss: 0.024567, accuracy: 0.0247
case acc: 0.02312801
case acc: 0.01974323
case acc: 0.033352327
case acc: 0.022297593
case acc: 0.022556264
case acc: 0.026926348
top acc: 0.0361 ::: bot acc: 0.0287
top acc: 0.0341 ::: bot acc: 0.0158
top acc: 0.0382 ::: bot acc: 0.0511
top acc: 0.0280 ::: bot acc: 0.0343
top acc: 0.0409 ::: bot acc: 0.0200
top acc: 0.0396 ::: bot acc: 0.0356
current epoch: 41
train loss is 0.032143
average val loss: 0.023865, accuracy: 0.0241
average test loss: 0.029761, accuracy: 0.0298
case acc: 0.030954031
case acc: 0.021779133
case acc: 0.037471306
case acc: 0.03174494
case acc: 0.025620254
case acc: 0.030941715
top acc: 0.0191 ::: bot acc: 0.0523
top acc: 0.0135 ::: bot acc: 0.0387
top acc: 0.0214 ::: bot acc: 0.0688
top acc: 0.0138 ::: bot acc: 0.0569
top acc: 0.0191 ::: bot acc: 0.0423
top acc: 0.0278 ::: bot acc: 0.0483
current epoch: 42
train loss is 0.031258
average val loss: 0.025119, accuracy: 0.0251
average test loss: 0.030935, accuracy: 0.0309
case acc: 0.03321874
case acc: 0.0244789
case acc: 0.03803639
case acc: 0.031627387
case acc: 0.02758158
case acc: 0.03055446
top acc: 0.0182 ::: bot acc: 0.0561
top acc: 0.0128 ::: bot acc: 0.0431
top acc: 0.0205 ::: bot acc: 0.0700
top acc: 0.0137 ::: bot acc: 0.0567
top acc: 0.0165 ::: bot acc: 0.0465
top acc: 0.0284 ::: bot acc: 0.0474
current epoch: 43
train loss is 0.032096
average val loss: 0.019264, accuracy: 0.0194
average test loss: 0.024946, accuracy: 0.0247
case acc: 0.024434173
case acc: 0.017980676
case acc: 0.033908736
case acc: 0.022939218
case acc: 0.021968897
case acc: 0.027092468
top acc: 0.0254 ::: bot acc: 0.0395
top acc: 0.0226 ::: bot acc: 0.0274
top acc: 0.0329 ::: bot acc: 0.0565
top acc: 0.0236 ::: bot acc: 0.0386
top acc: 0.0298 ::: bot acc: 0.0311
top acc: 0.0388 ::: bot acc: 0.0365
current epoch: 44
train loss is 0.030737
average val loss: 0.020577, accuracy: 0.0206
average test loss: 0.025399, accuracy: 0.0254
case acc: 0.023923514
case acc: 0.02139026
case acc: 0.033261526
case acc: 0.023118813
case acc: 0.023866912
case acc: 0.026594881
top acc: 0.0413 ::: bot acc: 0.0235
top acc: 0.0383 ::: bot acc: 0.0122
top acc: 0.0449 ::: bot acc: 0.0445
top acc: 0.0370 ::: bot acc: 0.0252
top acc: 0.0452 ::: bot acc: 0.0160
top acc: 0.0473 ::: bot acc: 0.0280
current epoch: 45
train loss is 0.031291
average val loss: 0.019447, accuracy: 0.0196
average test loss: 0.024606, accuracy: 0.0247
case acc: 0.023215951
case acc: 0.020058895
case acc: 0.03320968
case acc: 0.022253338
case acc: 0.022793759
case acc: 0.026575739
top acc: 0.0373 ::: bot acc: 0.0275
top acc: 0.0349 ::: bot acc: 0.0150
top acc: 0.0409 ::: bot acc: 0.0485
top acc: 0.0295 ::: bot acc: 0.0328
top acc: 0.0419 ::: bot acc: 0.0190
top acc: 0.0432 ::: bot acc: 0.0321
current epoch: 46
train loss is 0.031194
average val loss: 0.021129, accuracy: 0.0214
average test loss: 0.027012, accuracy: 0.0270
case acc: 0.027368626
case acc: 0.018927123
case acc: 0.035327915
case acc: 0.028031807
case acc: 0.023456346
case acc: 0.028972685
top acc: 0.0213 ::: bot acc: 0.0458
top acc: 0.0172 ::: bot acc: 0.0326
top acc: 0.0266 ::: bot acc: 0.0629
top acc: 0.0141 ::: bot acc: 0.0512
top acc: 0.0246 ::: bot acc: 0.0363
top acc: 0.0321 ::: bot acc: 0.0433
current epoch: 47
train loss is 0.030689
average val loss: 0.026045, accuracy: 0.0261
average test loss: 0.031791, accuracy: 0.0318
case acc: 0.03391066
case acc: 0.025350003
case acc: 0.03861884
case acc: 0.033463445
case acc: 0.028259197
case acc: 0.031254597
top acc: 0.0180 ::: bot acc: 0.0573
top acc: 0.0129 ::: bot acc: 0.0444
top acc: 0.0197 ::: bot acc: 0.0713
top acc: 0.0143 ::: bot acc: 0.0591
top acc: 0.0159 ::: bot acc: 0.0478
top acc: 0.0273 ::: bot acc: 0.0491
current epoch: 48
train loss is 0.031922
average val loss: 0.020924, accuracy: 0.0210
average test loss: 0.026787, accuracy: 0.0267
case acc: 0.027590366
case acc: 0.019659104
case acc: 0.035116673
case acc: 0.025288045
case acc: 0.024085704
case acc: 0.028357014
top acc: 0.0211 ::: bot acc: 0.0463
top acc: 0.0158 ::: bot acc: 0.0344
top acc: 0.0273 ::: bot acc: 0.0622
top acc: 0.0171 ::: bot acc: 0.0455
top acc: 0.0228 ::: bot acc: 0.0382
top acc: 0.0340 ::: bot acc: 0.0414
current epoch: 49
train loss is 0.031395
average val loss: 0.020828, accuracy: 0.0208
average test loss: 0.025583, accuracy: 0.0255
case acc: 0.02408118
case acc: 0.021438174
case acc: 0.033290554
case acc: 0.023483356
case acc: 0.023975076
case acc: 0.026805503
top acc: 0.0420 ::: bot acc: 0.0227
top acc: 0.0385 ::: bot acc: 0.0119
top acc: 0.0460 ::: bot acc: 0.0434
top acc: 0.0384 ::: bot acc: 0.0239
top acc: 0.0454 ::: bot acc: 0.0159
top acc: 0.0486 ::: bot acc: 0.0268
current epoch: 50
train loss is 0.031353
average val loss: 0.020274, accuracy: 0.0203
average test loss: 0.025191, accuracy: 0.0252
case acc: 0.023908297
case acc: 0.021409048
case acc: 0.0332128
case acc: 0.02250105
case acc: 0.023908343
case acc: 0.026497524
top acc: 0.0413 ::: bot acc: 0.0234
top acc: 0.0384 ::: bot acc: 0.0119
top acc: 0.0441 ::: bot acc: 0.0453
top acc: 0.0335 ::: bot acc: 0.0288
top acc: 0.0452 ::: bot acc: 0.0159
top acc: 0.0455 ::: bot acc: 0.0299
LME_Co_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6768 6768 6768
1.7082474 -0.6288155 0.21141115 -0.19947179
Validation: 756 756 756
Testing: 774 774 774
pre-processing time: 0.0005548000335693359
the split date is 2010-07-01
net initializing with time: 0.14666509628295898
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.319601
average val loss: 0.164273, accuracy: 0.1646
average test loss: 0.165699, accuracy: 0.1659
case acc: 0.22289208
case acc: 0.36785468
case acc: 0.08202596
case acc: 0.14949138
case acc: 0.11697389
case acc: 0.055911716
top acc: 0.1942 ::: bot acc: 0.2518
top acc: 0.3409 ::: bot acc: 0.3950
top acc: 0.0449 ::: bot acc: 0.1176
top acc: 0.1055 ::: bot acc: 0.1911
top acc: 0.0740 ::: bot acc: 0.1539
top acc: 0.0243 ::: bot acc: 0.0868
current epoch: 2
train loss is 0.207701
average val loss: 0.105817, accuracy: 0.1051
average test loss: 0.105643, accuracy: 0.1048
case acc: 0.02226312
case acc: 0.13160543
case acc: 0.14053017
case acc: 0.07330315
case acc: 0.09853608
case acc: 0.16273664
top acc: 0.0342 ::: bot acc: 0.0232
top acc: 0.1047 ::: bot acc: 0.1588
top acc: 0.1788 ::: bot acc: 0.1044
top acc: 0.1158 ::: bot acc: 0.0343
top acc: 0.1416 ::: bot acc: 0.0615
top acc: 0.2024 ::: bot acc: 0.1277
current epoch: 3
train loss is 0.078527
average val loss: 0.139421, accuracy: 0.1401
average test loss: 0.141023, accuracy: 0.1412
case acc: 0.19238429
case acc: 0.3171947
case acc: 0.061215147
case acc: 0.12570263
case acc: 0.10384805
case acc: 0.046807457
top acc: 0.1637 ::: bot acc: 0.2214
top acc: 0.2902 ::: bot acc: 0.3445
top acc: 0.0262 ::: bot acc: 0.0960
top acc: 0.0816 ::: bot acc: 0.1672
top acc: 0.0621 ::: bot acc: 0.1399
top acc: 0.0198 ::: bot acc: 0.0753
current epoch: 4
train loss is 0.115021
average val loss: 0.139073, accuracy: 0.1396
average test loss: 0.140699, accuracy: 0.1408
case acc: 0.18713664
case acc: 0.30795187
case acc: 0.06416611
case acc: 0.1272358
case acc: 0.10757082
case acc: 0.050913524
top acc: 0.1586 ::: bot acc: 0.2160
top acc: 0.2809 ::: bot acc: 0.3354
top acc: 0.0286 ::: bot acc: 0.0994
top acc: 0.0830 ::: bot acc: 0.1688
top acc: 0.0653 ::: bot acc: 0.1437
top acc: 0.0217 ::: bot acc: 0.0806
current epoch: 5
train loss is 0.114980
average val loss: 0.102466, accuracy: 0.1040
average test loss: 0.104241, accuracy: 0.1049
case acc: 0.14094736
case acc: 0.25782165
case acc: 0.035655655
case acc: 0.0892084
case acc: 0.075325854
case acc: 0.030329349
top acc: 0.1125 ::: bot acc: 0.1698
top acc: 0.2307 ::: bot acc: 0.2854
top acc: 0.0178 ::: bot acc: 0.0621
top acc: 0.0461 ::: bot acc: 0.1301
top acc: 0.0384 ::: bot acc: 0.1086
top acc: 0.0276 ::: bot acc: 0.0466
current epoch: 6
train loss is 0.103015
average val loss: 0.088629, accuracy: 0.0904
average test loss: 0.090478, accuracy: 0.0914
case acc: 0.11870375
case acc: 0.22983877
case acc: 0.029999046
case acc: 0.07493683
case acc: 0.066487044
case acc: 0.028151942
top acc: 0.0902 ::: bot acc: 0.1476
top acc: 0.2026 ::: bot acc: 0.2575
top acc: 0.0282 ::: bot acc: 0.0480
top acc: 0.0342 ::: bot acc: 0.1145
top acc: 0.0326 ::: bot acc: 0.0982
top acc: 0.0363 ::: bot acc: 0.0375
current epoch: 7
train loss is 0.089883
average val loss: 0.085566, accuracy: 0.0874
average test loss: 0.087437, accuracy: 0.0882
case acc: 0.111047104
case acc: 0.21577582
case acc: 0.029983455
case acc: 0.07368705
case acc: 0.069434434
case acc: 0.029098578
top acc: 0.0826 ::: bot acc: 0.1399
top acc: 0.1885 ::: bot acc: 0.2435
top acc: 0.0279 ::: bot acc: 0.0483
top acc: 0.0333 ::: bot acc: 0.1130
top acc: 0.0342 ::: bot acc: 0.1017
top acc: 0.0310 ::: bot acc: 0.0427
current epoch: 8
train loss is 0.083166
average val loss: 0.087121, accuracy: 0.0887
average test loss: 0.088913, accuracy: 0.0894
case acc: 0.109227926
case acc: 0.2082338
case acc: 0.032046836
case acc: 0.07760036
case acc: 0.07653224
case acc: 0.03273201
top acc: 0.0807 ::: bot acc: 0.1380
top acc: 0.1809 ::: bot acc: 0.2360
top acc: 0.0220 ::: bot acc: 0.0548
top acc: 0.0364 ::: bot acc: 0.1172
top acc: 0.0387 ::: bot acc: 0.1099
top acc: 0.0221 ::: bot acc: 0.0528
current epoch: 9
train loss is 0.081253
average val loss: 0.088554, accuracy: 0.0898
average test loss: 0.090230, accuracy: 0.0905
case acc: 0.10641518
case acc: 0.20147847
case acc: 0.034658168
case acc: 0.081025705
case acc: 0.08232091
case acc: 0.03723215
top acc: 0.0778 ::: bot acc: 0.1352
top acc: 0.1742 ::: bot acc: 0.2292
top acc: 0.0183 ::: bot acc: 0.0608
top acc: 0.0391 ::: bot acc: 0.1210
top acc: 0.0431 ::: bot acc: 0.1164
top acc: 0.0186 ::: bot acc: 0.0613
current epoch: 10
train loss is 0.084338
average val loss: 0.100950, accuracy: 0.1016
average test loss: 0.102497, accuracy: 0.1025
case acc: 0.1150203
case acc: 0.20838906
case acc: 0.04568262
case acc: 0.09602027
case acc: 0.09873313
case acc: 0.051187206
top acc: 0.0863 ::: bot acc: 0.1439
top acc: 0.1810 ::: bot acc: 0.2362
top acc: 0.0165 ::: bot acc: 0.0783
top acc: 0.0521 ::: bot acc: 0.1370
top acc: 0.0571 ::: bot acc: 0.1340
top acc: 0.0217 ::: bot acc: 0.0807
current epoch: 11
train loss is 0.079979
average val loss: 0.084651, accuracy: 0.0858
average test loss: 0.086342, accuracy: 0.0865
case acc: 0.09151466
case acc: 0.18366265
case acc: 0.036024515
case acc: 0.08105353
case acc: 0.0851672
case acc: 0.04152913
top acc: 0.0627 ::: bot acc: 0.1204
top acc: 0.1562 ::: bot acc: 0.2115
top acc: 0.0171 ::: bot acc: 0.0636
top acc: 0.0392 ::: bot acc: 0.1209
top acc: 0.0453 ::: bot acc: 0.1195
top acc: 0.0184 ::: bot acc: 0.0678
current epoch: 12
train loss is 0.074608
average val loss: 0.080834, accuracy: 0.0820
average test loss: 0.082566, accuracy: 0.0827
case acc: 0.0820159
case acc: 0.17283367
case acc: 0.035451476
case acc: 0.07901366
case acc: 0.084455356
case acc: 0.042158917
top acc: 0.0532 ::: bot acc: 0.1110
top acc: 0.1454 ::: bot acc: 0.2006
top acc: 0.0174 ::: bot acc: 0.0625
top acc: 0.0376 ::: bot acc: 0.1187
top acc: 0.0447 ::: bot acc: 0.1187
top acc: 0.0185 ::: bot acc: 0.0687
current epoch: 13
train loss is 0.072496
average val loss: 0.079271, accuracy: 0.0803
average test loss: 0.081026, accuracy: 0.0810
case acc: 0.075153865
case acc: 0.16492538
case acc: 0.03621236
case acc: 0.07933776
case acc: 0.08597296
case acc: 0.044529058
top acc: 0.0463 ::: bot acc: 0.1041
top acc: 0.1374 ::: bot acc: 0.1927
top acc: 0.0168 ::: bot acc: 0.0640
top acc: 0.0379 ::: bot acc: 0.1191
top acc: 0.0460 ::: bot acc: 0.1203
top acc: 0.0191 ::: bot acc: 0.0720
current epoch: 14
train loss is 0.067730
average val loss: 0.070285, accuracy: 0.0716
average test loss: 0.072166, accuracy: 0.0721
case acc: 0.059866488
case acc: 0.14833842
case acc: 0.032526664
case acc: 0.07167593
case acc: 0.07949156
case acc: 0.040535044
top acc: 0.0314 ::: bot acc: 0.0887
top acc: 0.1208 ::: bot acc: 0.1762
top acc: 0.0204 ::: bot acc: 0.0568
top acc: 0.0319 ::: bot acc: 0.1106
top acc: 0.0408 ::: bot acc: 0.1131
top acc: 0.0182 ::: bot acc: 0.0665
current epoch: 15
train loss is 0.062700
average val loss: 0.065305, accuracy: 0.0667
average test loss: 0.067245, accuracy: 0.0670
case acc: 0.049932614
case acc: 0.13617384
case acc: 0.031395786
case acc: 0.067936294
case acc: 0.077014334
case acc: 0.03974847
top acc: 0.0225 ::: bot acc: 0.0781
top acc: 0.1086 ::: bot acc: 0.1641
top acc: 0.0227 ::: bot acc: 0.0539
top acc: 0.0290 ::: bot acc: 0.1065
top acc: 0.0389 ::: bot acc: 0.1103
top acc: 0.0181 ::: bot acc: 0.0654
current epoch: 16
train loss is 0.058614
average val loss: 0.057818, accuracy: 0.0594
average test loss: 0.059798, accuracy: 0.0596
case acc: 0.038257856
case acc: 0.12036106
case acc: 0.029604571
case acc: 0.0612876
case acc: 0.07130623
case acc: 0.036714215
top acc: 0.0146 ::: bot acc: 0.0646
top acc: 0.0928 ::: bot acc: 0.1483
top acc: 0.0292 ::: bot acc: 0.0475
top acc: 0.0249 ::: bot acc: 0.0986
top acc: 0.0350 ::: bot acc: 0.1037
top acc: 0.0188 ::: bot acc: 0.0605
current epoch: 17
train loss is 0.053230
average val loss: 0.050432, accuracy: 0.0520
average test loss: 0.052380, accuracy: 0.0522
case acc: 0.028678453
case acc: 0.103438176
case acc: 0.028471617
case acc: 0.05428358
case acc: 0.06483839
case acc: 0.033432998
top acc: 0.0124 ::: bot acc: 0.0513
top acc: 0.0759 ::: bot acc: 0.1313
top acc: 0.0365 ::: bot acc: 0.0401
top acc: 0.0221 ::: bot acc: 0.0895
top acc: 0.0312 ::: bot acc: 0.0958
top acc: 0.0208 ::: bot acc: 0.0545
current epoch: 18
train loss is 0.048136
average val loss: 0.041448, accuracy: 0.0424
average test loss: 0.043362, accuracy: 0.0433
case acc: 0.021786608
case acc: 0.080896325
case acc: 0.028857317
case acc: 0.044355262
case acc: 0.05472469
case acc: 0.029148757
top acc: 0.0229 ::: bot acc: 0.0348
top acc: 0.0533 ::: bot acc: 0.1087
top acc: 0.0487 ::: bot acc: 0.0279
top acc: 0.0217 ::: bot acc: 0.0748
top acc: 0.0272 ::: bot acc: 0.0827
top acc: 0.0301 ::: bot acc: 0.0433
current epoch: 19
train loss is 0.042979
average val loss: 0.035384, accuracy: 0.0355
average test loss: 0.036985, accuracy: 0.0370
case acc: 0.021817755
case acc: 0.05832071
case acc: 0.032201137
case acc: 0.03644202
case acc: 0.04557827
case acc: 0.027516268
top acc: 0.0355 ::: bot acc: 0.0223
top acc: 0.0313 ::: bot acc: 0.0859
top acc: 0.0597 ::: bot acc: 0.0174
top acc: 0.0271 ::: bot acc: 0.0602
top acc: 0.0246 ::: bot acc: 0.0702
top acc: 0.0403 ::: bot acc: 0.0331
current epoch: 20
train loss is 0.040885
average val loss: 0.031652, accuracy: 0.0308
average test loss: 0.032939, accuracy: 0.0326
case acc: 0.02551749
case acc: 0.034593094
case acc: 0.038962785
case acc: 0.031149935
case acc: 0.037437905
case acc: 0.027677104
top acc: 0.0465 ::: bot acc: 0.0125
top acc: 0.0114 ::: bot acc: 0.0602
top acc: 0.0713 ::: bot acc: 0.0144
top acc: 0.0415 ::: bot acc: 0.0439
top acc: 0.0262 ::: bot acc: 0.0572
top acc: 0.0508 ::: bot acc: 0.0226
current epoch: 21
train loss is 0.039656
average val loss: 0.030495, accuracy: 0.0291
average test loss: 0.031553, accuracy: 0.0311
case acc: 0.027651576
case acc: 0.021875652
case acc: 0.043822058
case acc: 0.031039098
case acc: 0.033863757
case acc: 0.028407138
top acc: 0.0505 ::: bot acc: 0.0111
top acc: 0.0171 ::: bot acc: 0.0382
top acc: 0.0777 ::: bot acc: 0.0160
top acc: 0.0528 ::: bot acc: 0.0326
top acc: 0.0298 ::: bot acc: 0.0500
top acc: 0.0553 ::: bot acc: 0.0181
current epoch: 22
train loss is 0.037602
average val loss: 0.032512, accuracy: 0.0311
average test loss: 0.032943, accuracy: 0.0329
case acc: 0.030045023
case acc: 0.022601094
case acc: 0.049464677
case acc: 0.034401298
case acc: 0.031043574
case acc: 0.029924858
top acc: 0.0542 ::: bot acc: 0.0109
top acc: 0.0399 ::: bot acc: 0.0154
top acc: 0.0848 ::: bot acc: 0.0186
top acc: 0.0645 ::: bot acc: 0.0211
top acc: 0.0355 ::: bot acc: 0.0429
top acc: 0.0598 ::: bot acc: 0.0141
current epoch: 23
train loss is 0.035971
average val loss: 0.039826, accuracy: 0.0388
average test loss: 0.039521, accuracy: 0.0399
case acc: 0.0347448
case acc: 0.039296947
case acc: 0.058903445
case acc: 0.043111164
case acc: 0.028455919
case acc: 0.03461446
top acc: 0.0608 ::: bot acc: 0.0116
top acc: 0.0653 ::: bot acc: 0.0146
top acc: 0.0957 ::: bot acc: 0.0250
top acc: 0.0797 ::: bot acc: 0.0167
top acc: 0.0461 ::: bot acc: 0.0322
top acc: 0.0681 ::: bot acc: 0.0115
current epoch: 24
train loss is 0.037446
average val loss: 0.054346, accuracy: 0.0538
average test loss: 0.053478, accuracy: 0.0536
case acc: 0.044002727
case acc: 0.06704264
case acc: 0.074843355
case acc: 0.05920921
case acc: 0.030311126
case acc: 0.046442408
top acc: 0.0721 ::: bot acc: 0.0169
top acc: 0.0945 ::: bot acc: 0.0394
top acc: 0.1127 ::: bot acc: 0.0387
top acc: 0.1006 ::: bot acc: 0.0232
top acc: 0.0626 ::: bot acc: 0.0158
top acc: 0.0828 ::: bot acc: 0.0175
current epoch: 25
train loss is 0.044987
average val loss: 0.075950, accuracy: 0.0756
average test loss: 0.074904, accuracy: 0.0748
case acc: 0.05873743
case acc: 0.09821525
case acc: 0.097421795
case acc: 0.08350105
case acc: 0.044621646
case acc: 0.06612118
top acc: 0.0876 ::: bot acc: 0.0301
top acc: 0.1257 ::: bot acc: 0.0706
top acc: 0.1360 ::: bot acc: 0.0598
top acc: 0.1269 ::: bot acc: 0.0433
top acc: 0.0855 ::: bot acc: 0.0131
top acc: 0.1044 ::: bot acc: 0.0332
current epoch: 26
train loss is 0.059697
average val loss: 0.067678, accuracy: 0.0673
average test loss: 0.066739, accuracy: 0.0666
case acc: 0.04266444
case acc: 0.09048251
case acc: 0.08955524
case acc: 0.0780714
case acc: 0.03949235
case acc: 0.059496555
top acc: 0.0705 ::: bot acc: 0.0160
top acc: 0.1180 ::: bot acc: 0.0628
top acc: 0.1280 ::: bot acc: 0.0523
top acc: 0.1211 ::: bot acc: 0.0386
top acc: 0.0784 ::: bot acc: 0.0117
top acc: 0.0973 ::: bot acc: 0.0274
current epoch: 27
train loss is 0.065158
average val loss: 0.042854, accuracy: 0.0418
average test loss: 0.042663, accuracy: 0.0427
case acc: 0.021193214
case acc: 0.05478651
case acc: 0.060892712
case acc: 0.053624745
case acc: 0.028002435
case acc: 0.037610304
top acc: 0.0328 ::: bot acc: 0.0247
top acc: 0.0822 ::: bot acc: 0.0274
top acc: 0.0979 ::: bot acc: 0.0265
top acc: 0.0938 ::: bot acc: 0.0199
top acc: 0.0527 ::: bot acc: 0.0253
top acc: 0.0722 ::: bot acc: 0.0118
current epoch: 28
train loss is 0.063779
average val loss: 0.029666, accuracy: 0.0286
average test loss: 0.030934, accuracy: 0.0315
case acc: 0.03709056
case acc: 0.021877507
case acc: 0.034324557
case acc: 0.033572003
case acc: 0.03497952
case acc: 0.027346414
top acc: 0.0140 ::: bot acc: 0.0628
top acc: 0.0378 ::: bot acc: 0.0174
top acc: 0.0636 ::: bot acc: 0.0152
top acc: 0.0622 ::: bot acc: 0.0231
top acc: 0.0276 ::: bot acc: 0.0525
top acc: 0.0462 ::: bot acc: 0.0269
current epoch: 29
train loss is 0.060790
average val loss: 0.037443, accuracy: 0.0378
average test loss: 0.039473, accuracy: 0.0402
case acc: 0.061912507
case acc: 0.0365621
case acc: 0.028568963
case acc: 0.033483956
case acc: 0.049691256
case acc: 0.030844225
top acc: 0.0333 ::: bot acc: 0.0904
top acc: 0.0126 ::: bot acc: 0.0625
top acc: 0.0331 ::: bot acc: 0.0433
top acc: 0.0320 ::: bot acc: 0.0534
top acc: 0.0248 ::: bot acc: 0.0760
top acc: 0.0246 ::: bot acc: 0.0488
current epoch: 30
train loss is 0.052671
average val loss: 0.043527, accuracy: 0.0443
average test loss: 0.045517, accuracy: 0.0464
case acc: 0.06312938
case acc: 0.06055513
case acc: 0.031049818
case acc: 0.038806297
case acc: 0.053104747
case acc: 0.032031346
top acc: 0.0345 ::: bot acc: 0.0916
top acc: 0.0334 ::: bot acc: 0.0881
top acc: 0.0229 ::: bot acc: 0.0536
top acc: 0.0240 ::: bot acc: 0.0654
top acc: 0.0260 ::: bot acc: 0.0805
top acc: 0.0219 ::: bot acc: 0.0520
current epoch: 31
train loss is 0.051939
average val loss: 0.059016, accuracy: 0.0596
average test loss: 0.060776, accuracy: 0.0613
case acc: 0.07356421
case acc: 0.09110657
case acc: 0.04218362
case acc: 0.052906837
case acc: 0.066385075
case acc: 0.04163712
top acc: 0.0448 ::: bot acc: 0.1021
top acc: 0.0636 ::: bot acc: 0.1188
top acc: 0.0149 ::: bot acc: 0.0743
top acc: 0.0218 ::: bot acc: 0.0877
top acc: 0.0320 ::: bot acc: 0.0974
top acc: 0.0184 ::: bot acc: 0.0682
current epoch: 32
train loss is 0.056420
average val loss: 0.063429, accuracy: 0.0639
average test loss: 0.065181, accuracy: 0.0655
case acc: 0.070497856
case acc: 0.09877073
case acc: 0.04755363
case acc: 0.058541484
case acc: 0.07123423
case acc: 0.04619467
top acc: 0.0417 ::: bot acc: 0.0990
top acc: 0.0713 ::: bot acc: 0.1265
top acc: 0.0171 ::: bot acc: 0.0813
top acc: 0.0238 ::: bot acc: 0.0952
top acc: 0.0349 ::: bot acc: 0.1033
top acc: 0.0195 ::: bot acc: 0.0745
current epoch: 33
train loss is 0.049209
average val loss: 0.044862, accuracy: 0.0457
average test loss: 0.046875, accuracy: 0.0474
case acc: 0.042674754
case acc: 0.073203795
case acc: 0.03431443
case acc: 0.0442818
case acc: 0.055658467
case acc: 0.034110513
top acc: 0.0174 ::: bot acc: 0.0695
top acc: 0.0457 ::: bot acc: 0.1009
top acc: 0.0175 ::: bot acc: 0.0612
top acc: 0.0217 ::: bot acc: 0.0748
top acc: 0.0269 ::: bot acc: 0.0839
top acc: 0.0196 ::: bot acc: 0.0563
current epoch: 34
train loss is 0.041788
average val loss: 0.039606, accuracy: 0.0405
average test loss: 0.041663, accuracy: 0.0420
case acc: 0.032471187
case acc: 0.061889738
case acc: 0.032015607
case acc: 0.04084333
case acc: 0.05231739
case acc: 0.032560542
top acc: 0.0123 ::: bot acc: 0.0567
top acc: 0.0347 ::: bot acc: 0.0894
top acc: 0.0203 ::: bot acc: 0.0563
top acc: 0.0229 ::: bot acc: 0.0691
top acc: 0.0257 ::: bot acc: 0.0795
top acc: 0.0211 ::: bot acc: 0.0532
current epoch: 35
train loss is 0.037906
average val loss: 0.034980, accuracy: 0.0358
average test loss: 0.037108, accuracy: 0.0373
case acc: 0.025711125
case acc: 0.051259495
case acc: 0.030393953
case acc: 0.03744958
case acc: 0.048300438
case acc: 0.030903943
top acc: 0.0138 ::: bot acc: 0.0458
top acc: 0.0247 ::: bot acc: 0.0785
top acc: 0.0252 ::: bot acc: 0.0513
top acc: 0.0253 ::: bot acc: 0.0628
top acc: 0.0244 ::: bot acc: 0.0741
top acc: 0.0242 ::: bot acc: 0.0492
current epoch: 36
train loss is 0.034642
average val loss: 0.030212, accuracy: 0.0307
average test loss: 0.032358, accuracy: 0.0324
case acc: 0.021542385
case acc: 0.03920873
case acc: 0.028714739
case acc: 0.033569224
case acc: 0.04265253
case acc: 0.028626254
top acc: 0.0222 ::: bot acc: 0.0350
top acc: 0.0146 ::: bot acc: 0.0655
top acc: 0.0324 ::: bot acc: 0.0442
top acc: 0.0316 ::: bot acc: 0.0539
top acc: 0.0239 ::: bot acc: 0.0659
top acc: 0.0308 ::: bot acc: 0.0423
current epoch: 37
train loss is 0.031766
average val loss: 0.026989, accuracy: 0.0271
average test loss: 0.028993, accuracy: 0.0290
case acc: 0.020927355
case acc: 0.028649878
case acc: 0.028183863
case acc: 0.031102398
case acc: 0.03748646
case acc: 0.027520742
top acc: 0.0301 ::: bot acc: 0.0270
top acc: 0.0097 ::: bot acc: 0.0521
top acc: 0.0395 ::: bot acc: 0.0370
top acc: 0.0409 ::: bot acc: 0.0443
top acc: 0.0257 ::: bot acc: 0.0573
top acc: 0.0381 ::: bot acc: 0.0351
current epoch: 38
train loss is 0.030249
average val loss: 0.025543, accuracy: 0.0252
average test loss: 0.027331, accuracy: 0.0273
case acc: 0.021326242
case acc: 0.022334272
case acc: 0.028319066
case acc: 0.030586448
case acc: 0.033803266
case acc: 0.027293043
top acc: 0.0344 ::: bot acc: 0.0227
top acc: 0.0157 ::: bot acc: 0.0396
top acc: 0.0454 ::: bot acc: 0.0312
top acc: 0.0495 ::: bot acc: 0.0357
top acc: 0.0295 ::: bot acc: 0.0499
top acc: 0.0441 ::: bot acc: 0.0290
current epoch: 39
train loss is 0.029342
average val loss: 0.025407, accuracy: 0.0248
average test loss: 0.026930, accuracy: 0.0269
case acc: 0.021537382
case acc: 0.02018533
case acc: 0.029012632
case acc: 0.031878456
case acc: 0.03149454
case acc: 0.027467161
top acc: 0.0356 ::: bot acc: 0.0215
top acc: 0.0261 ::: bot acc: 0.0290
top acc: 0.0494 ::: bot acc: 0.0271
top acc: 0.0564 ::: bot acc: 0.0288
top acc: 0.0337 ::: bot acc: 0.0443
top acc: 0.0485 ::: bot acc: 0.0247
current epoch: 40
train loss is 0.028808
average val loss: 0.026272, accuracy: 0.0257
average test loss: 0.027373, accuracy: 0.0275
case acc: 0.021668062
case acc: 0.021269538
case acc: 0.03009604
case acc: 0.03382497
case acc: 0.029865026
case acc: 0.027980085
top acc: 0.0362 ::: bot acc: 0.0209
top acc: 0.0357 ::: bot acc: 0.0195
top acc: 0.0533 ::: bot acc: 0.0232
top acc: 0.0628 ::: bot acc: 0.0224
top acc: 0.0389 ::: bot acc: 0.0391
top acc: 0.0526 ::: bot acc: 0.0206
current epoch: 41
train loss is 0.028892
average val loss: 0.027375, accuracy: 0.0267
average test loss: 0.028156, accuracy: 0.0283
case acc: 0.021488847
case acc: 0.023789302
case acc: 0.030945841
case acc: 0.035930034
case acc: 0.02886109
case acc: 0.028608138
top acc: 0.0353 ::: bot acc: 0.0218
top acc: 0.0425 ::: bot acc: 0.0137
top acc: 0.0559 ::: bot acc: 0.0206
top acc: 0.0676 ::: bot acc: 0.0191
top acc: 0.0428 ::: bot acc: 0.0351
top acc: 0.0557 ::: bot acc: 0.0174
current epoch: 42
train loss is 0.029042
average val loss: 0.028269, accuracy: 0.0276
average test loss: 0.028889, accuracy: 0.0290
case acc: 0.02120957
case acc: 0.026052617
case acc: 0.03150243
case acc: 0.037703764
case acc: 0.028386611
case acc: 0.029258937
top acc: 0.0336 ::: bot acc: 0.0235
top acc: 0.0468 ::: bot acc: 0.0119
top acc: 0.0574 ::: bot acc: 0.0192
top acc: 0.0709 ::: bot acc: 0.0178
top acc: 0.0456 ::: bot acc: 0.0324
top acc: 0.0578 ::: bot acc: 0.0153
current epoch: 43
train loss is 0.029354
average val loss: 0.029079, accuracy: 0.0284
average test loss: 0.029566, accuracy: 0.0296
case acc: 0.021021776
case acc: 0.027391346
case acc: 0.03201381
case acc: 0.03918384
case acc: 0.028133217
case acc: 0.030095555
top acc: 0.0324 ::: bot acc: 0.0248
top acc: 0.0493 ::: bot acc: 0.0110
top acc: 0.0587 ::: bot acc: 0.0181
top acc: 0.0735 ::: bot acc: 0.0171
top acc: 0.0480 ::: bot acc: 0.0299
top acc: 0.0599 ::: bot acc: 0.0136
current epoch: 44
train loss is 0.029585
average val loss: 0.029088, accuracy: 0.0284
average test loss: 0.029598, accuracy: 0.0297
case acc: 0.020897407
case acc: 0.02709276
case acc: 0.03186829
case acc: 0.039551053
case acc: 0.028093519
case acc: 0.030443514
top acc: 0.0297 ::: bot acc: 0.0274
top acc: 0.0487 ::: bot acc: 0.0112
top acc: 0.0583 ::: bot acc: 0.0184
top acc: 0.0741 ::: bot acc: 0.0169
top acc: 0.0489 ::: bot acc: 0.0290
top acc: 0.0606 ::: bot acc: 0.0132
current epoch: 45
train loss is 0.029644
average val loss: 0.028552, accuracy: 0.0279
average test loss: 0.029167, accuracy: 0.0292
case acc: 0.021045193
case acc: 0.025557684
case acc: 0.031248437
case acc: 0.039057903
case acc: 0.028079396
case acc: 0.03033873
top acc: 0.0265 ::: bot acc: 0.0306
top acc: 0.0459 ::: bot acc: 0.0123
top acc: 0.0566 ::: bot acc: 0.0199
top acc: 0.0733 ::: bot acc: 0.0172
top acc: 0.0488 ::: bot acc: 0.0291
top acc: 0.0604 ::: bot acc: 0.0133
current epoch: 46
train loss is 0.029380
average val loss: 0.027450, accuracy: 0.0268
average test loss: 0.028312, accuracy: 0.0283
case acc: 0.021496529
case acc: 0.022795934
case acc: 0.030136125
case acc: 0.037478566
case acc: 0.02816864
case acc: 0.029586138
top acc: 0.0223 ::: bot acc: 0.0348
top acc: 0.0404 ::: bot acc: 0.0149
top acc: 0.0532 ::: bot acc: 0.0233
top acc: 0.0705 ::: bot acc: 0.0179
top acc: 0.0472 ::: bot acc: 0.0307
top acc: 0.0588 ::: bot acc: 0.0144
current epoch: 47
train loss is 0.029520
average val loss: 0.026430, accuracy: 0.0259
average test loss: 0.027610, accuracy: 0.0275
case acc: 0.022525975
case acc: 0.020711549
case acc: 0.028981032
case acc: 0.035599004
case acc: 0.02838565
case acc: 0.028985426
top acc: 0.0186 ::: bot acc: 0.0385
top acc: 0.0332 ::: bot acc: 0.0220
top acc: 0.0491 ::: bot acc: 0.0273
top acc: 0.0670 ::: bot acc: 0.0193
top acc: 0.0454 ::: bot acc: 0.0325
top acc: 0.0570 ::: bot acc: 0.0161
current epoch: 48
train loss is 0.029562
average val loss: 0.025752, accuracy: 0.0254
average test loss: 0.027257, accuracy: 0.0272
case acc: 0.024203131
case acc: 0.020413106
case acc: 0.028158925
case acc: 0.03353789
case acc: 0.028860746
case acc: 0.028260227
top acc: 0.0152 ::: bot acc: 0.0427
top acc: 0.0238 ::: bot acc: 0.0313
top acc: 0.0436 ::: bot acc: 0.0328
top acc: 0.0620 ::: bot acc: 0.0230
top acc: 0.0427 ::: bot acc: 0.0351
top acc: 0.0543 ::: bot acc: 0.0188
current epoch: 49
train loss is 0.030054
average val loss: 0.025979, accuracy: 0.0258
average test loss: 0.027871, accuracy: 0.0280
case acc: 0.027047798
case acc: 0.023956975
case acc: 0.028260125
case acc: 0.031504218
case acc: 0.029903242
case acc: 0.027613377
top acc: 0.0130 ::: bot acc: 0.0481
top acc: 0.0128 ::: bot acc: 0.0434
top acc: 0.0362 ::: bot acc: 0.0403
top acc: 0.0551 ::: bot acc: 0.0299
top acc: 0.0386 ::: bot acc: 0.0392
top acc: 0.0501 ::: bot acc: 0.0229
current epoch: 50
train loss is 0.031859
average val loss: 0.028565, accuracy: 0.0289
average test loss: 0.030726, accuracy: 0.0312
case acc: 0.0320654
case acc: 0.03481349
case acc: 0.030345608
case acc: 0.030519763
case acc: 0.03227013
case acc: 0.027279787
top acc: 0.0124 ::: bot acc: 0.0559
top acc: 0.0116 ::: bot acc: 0.0603
top acc: 0.0250 ::: bot acc: 0.0515
top acc: 0.0443 ::: bot acc: 0.0407
top acc: 0.0317 ::: bot acc: 0.0464
top acc: 0.0428 ::: bot acc: 0.0302
LME_Co_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6798 6798 6798
1.7082474 -0.6288155 0.21141115 -0.19947179
Validation: 756 756 756
Testing: 750 750 750
pre-processing time: 0.000400543212890625
the split date is 2011-01-01
net initializing with time: 0.10961794853210449
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.290161
average val loss: 0.104450, accuracy: 0.1051
average test loss: 0.093339, accuracy: 0.0941
case acc: 0.17710792
case acc: 0.15081564
case acc: 0.058775615
case acc: 0.046584565
case acc: 0.045150492
case acc: 0.08623375
top acc: 0.1454 ::: bot acc: 0.2101
top acc: 0.1237 ::: bot acc: 0.1789
top acc: 0.0244 ::: bot acc: 0.0994
top acc: 0.0168 ::: bot acc: 0.0794
top acc: 0.0153 ::: bot acc: 0.0802
top acc: 0.0483 ::: bot acc: 0.1227
current epoch: 2
train loss is 0.101481
average val loss: 0.066355, accuracy: 0.0680
average test loss: 0.058085, accuracy: 0.0607
case acc: 0.124060236
case acc: 0.10078644
case acc: 0.035104625
case acc: 0.0273958
case acc: 0.029734502
case acc: 0.046993498
top acc: 0.0922 ::: bot acc: 0.1572
top acc: 0.0737 ::: bot acc: 0.1288
top acc: 0.0313 ::: bot acc: 0.0605
top acc: 0.0270 ::: bot acc: 0.0452
top acc: 0.0371 ::: bot acc: 0.0441
top acc: 0.0198 ::: bot acc: 0.0783
current epoch: 3
train loss is 0.095334
average val loss: 0.097248, accuracy: 0.0977
average test loss: 0.085864, accuracy: 0.0865
case acc: 0.15066276
case acc: 0.13237143
case acc: 0.058157705
case acc: 0.055635285
case acc: 0.050241064
case acc: 0.07195784
top acc: 0.1188 ::: bot acc: 0.1839
top acc: 0.1053 ::: bot acc: 0.1603
top acc: 0.0242 ::: bot acc: 0.0985
top acc: 0.0229 ::: bot acc: 0.0899
top acc: 0.0162 ::: bot acc: 0.0874
top acc: 0.0351 ::: bot acc: 0.1080
current epoch: 4
train loss is 0.089312
average val loss: 0.078826, accuracy: 0.0796
average test loss: 0.067921, accuracy: 0.0691
case acc: 0.12134912
case acc: 0.10782359
case acc: 0.045490414
case acc: 0.04730636
case acc: 0.042324588
case acc: 0.05023618
top acc: 0.0893 ::: bot acc: 0.1547
top acc: 0.0808 ::: bot acc: 0.1357
top acc: 0.0199 ::: bot acc: 0.0817
top acc: 0.0172 ::: bot acc: 0.0802
top acc: 0.0154 ::: bot acc: 0.0760
top acc: 0.0211 ::: bot acc: 0.0826
current epoch: 5
train loss is 0.076625
average val loss: 0.078880, accuracy: 0.0794
average test loss: 0.067745, accuracy: 0.0687
case acc: 0.11170264
case acc: 0.10181693
case acc: 0.04825343
case acc: 0.055344958
case acc: 0.047359396
case acc: 0.04775633
top acc: 0.0797 ::: bot acc: 0.1451
top acc: 0.0748 ::: bot acc: 0.1297
top acc: 0.0204 ::: bot acc: 0.0855
top acc: 0.0226 ::: bot acc: 0.0895
top acc: 0.0155 ::: bot acc: 0.0835
top acc: 0.0199 ::: bot acc: 0.0795
current epoch: 6
train loss is 0.069434
average val loss: 0.077579, accuracy: 0.0780
average test loss: 0.066328, accuracy: 0.0671
case acc: 0.100400284
case acc: 0.09380578
case acc: 0.05029772
case acc: 0.06186112
case acc: 0.051049188
case acc: 0.04531566
top acc: 0.0683 ::: bot acc: 0.1338
top acc: 0.0668 ::: bot acc: 0.1218
top acc: 0.0210 ::: bot acc: 0.0882
top acc: 0.0274 ::: bot acc: 0.0968
top acc: 0.0163 ::: bot acc: 0.0887
top acc: 0.0192 ::: bot acc: 0.0763
current epoch: 7
train loss is 0.062105
average val loss: 0.074698, accuracy: 0.0751
average test loss: 0.063415, accuracy: 0.0641
case acc: 0.087323844
case acc: 0.083417825
case acc: 0.05138875
case acc: 0.066475324
case acc: 0.05297481
case acc: 0.04291441
top acc: 0.0553 ::: bot acc: 0.1208
top acc: 0.0563 ::: bot acc: 0.1115
top acc: 0.0214 ::: bot acc: 0.0896
top acc: 0.0313 ::: bot acc: 0.1017
top acc: 0.0170 ::: bot acc: 0.0912
top acc: 0.0188 ::: bot acc: 0.0730
current epoch: 8
train loss is 0.056789
average val loss: 0.068128, accuracy: 0.0684
average test loss: 0.057031, accuracy: 0.0575
case acc: 0.0702463
case acc: 0.06817756
case acc: 0.049699653
case acc: 0.06653382
case acc: 0.05108208
case acc: 0.039081693
top acc: 0.0383 ::: bot acc: 0.1037
top acc: 0.0410 ::: bot acc: 0.0963
top acc: 0.0208 ::: bot acc: 0.0875
top acc: 0.0314 ::: bot acc: 0.1016
top acc: 0.0161 ::: bot acc: 0.0889
top acc: 0.0184 ::: bot acc: 0.0675
current epoch: 9
train loss is 0.049876
average val loss: 0.061020, accuracy: 0.0610
average test loss: 0.050326, accuracy: 0.0503
case acc: 0.05312978
case acc: 0.05165261
case acc: 0.047732376
case acc: 0.06483711
case acc: 0.04804451
case acc: 0.03622243
top acc: 0.0222 ::: bot acc: 0.0861
top acc: 0.0247 ::: bot acc: 0.0798
top acc: 0.0201 ::: bot acc: 0.0848
top acc: 0.0299 ::: bot acc: 0.0997
top acc: 0.0152 ::: bot acc: 0.0848
top acc: 0.0186 ::: bot acc: 0.0631
current epoch: 10
train loss is 0.044510
average val loss: 0.055024, accuracy: 0.0546
average test loss: 0.044905, accuracy: 0.0441
case acc: 0.03856156
case acc: 0.036549684
case acc: 0.046452302
case acc: 0.06287252
case acc: 0.045268666
case acc: 0.03504611
top acc: 0.0104 ::: bot acc: 0.0701
top acc: 0.0113 ::: bot acc: 0.0640
top acc: 0.0198 ::: bot acc: 0.0831
top acc: 0.0283 ::: bot acc: 0.0976
top acc: 0.0146 ::: bot acc: 0.0809
top acc: 0.0191 ::: bot acc: 0.0612
current epoch: 11
train loss is 0.040977
average val loss: 0.049962, accuracy: 0.0491
average test loss: 0.040891, accuracy: 0.0396
case acc: 0.02896906
case acc: 0.025975639
case acc: 0.044947866
case acc: 0.06035523
case acc: 0.043118834
case acc: 0.034163896
top acc: 0.0103 ::: bot acc: 0.0559
top acc: 0.0096 ::: bot acc: 0.0492
top acc: 0.0195 ::: bot acc: 0.0809
top acc: 0.0263 ::: bot acc: 0.0948
top acc: 0.0146 ::: bot acc: 0.0777
top acc: 0.0198 ::: bot acc: 0.0596
current epoch: 12
train loss is 0.040794
average val loss: 0.043317, accuracy: 0.0423
average test loss: 0.036543, accuracy: 0.0350
case acc: 0.024273291
case acc: 0.020380462
case acc: 0.040886555
case acc: 0.05357202
case acc: 0.039264087
case acc: 0.031677116
top acc: 0.0256 ::: bot acc: 0.0395
top acc: 0.0253 ::: bot acc: 0.0310
top acc: 0.0207 ::: bot acc: 0.0743
top acc: 0.0215 ::: bot acc: 0.0871
top acc: 0.0160 ::: bot acc: 0.0712
top acc: 0.0227 ::: bot acc: 0.0545
current epoch: 13
train loss is 0.046335
average val loss: 0.034165, accuracy: 0.0345
average test loss: 0.032967, accuracy: 0.0320
case acc: 0.029862262
case acc: 0.03381639
case acc: 0.03354413
case acc: 0.03568046
case acc: 0.031300224
case acc: 0.027587773
top acc: 0.0533 ::: bot acc: 0.0136
top acc: 0.0576 ::: bot acc: 0.0128
top acc: 0.0365 ::: bot acc: 0.0546
top acc: 0.0133 ::: bot acc: 0.0644
top acc: 0.0308 ::: bot acc: 0.0519
top acc: 0.0378 ::: bot acc: 0.0385
current epoch: 14
train loss is 0.057677
average val loss: 0.049930, accuracy: 0.0506
average test loss: 0.059742, accuracy: 0.0597
case acc: 0.07955553
case acc: 0.09664489
case acc: 0.05043313
case acc: 0.036299057
case acc: 0.046677496
case acc: 0.04860395
top acc: 0.1113 ::: bot acc: 0.0463
top acc: 0.1244 ::: bot acc: 0.0676
top acc: 0.0901 ::: bot acc: 0.0162
top acc: 0.0662 ::: bot acc: 0.0128
top acc: 0.0844 ::: bot acc: 0.0138
top acc: 0.0846 ::: bot acc: 0.0170
current epoch: 15
train loss is 0.074917
average val loss: 0.088294, accuracy: 0.0884
average test loss: 0.099873, accuracy: 0.0999
case acc: 0.11766947
case acc: 0.14547664
case acc: 0.087712966
case acc: 0.08222619
case acc: 0.088472344
case acc: 0.07776307
top acc: 0.1493 ::: bot acc: 0.0845
top acc: 0.1733 ::: bot acc: 0.1163
top acc: 0.1322 ::: bot acc: 0.0437
top acc: 0.1177 ::: bot acc: 0.0471
top acc: 0.1303 ::: bot acc: 0.0473
top acc: 0.1165 ::: bot acc: 0.0405
current epoch: 16
train loss is 0.069825
average val loss: 0.065306, accuracy: 0.0654
average test loss: 0.076505, accuracy: 0.0767
case acc: 0.083769724
case acc: 0.11749463
case acc: 0.06747694
case acc: 0.06924724
case acc: 0.07287782
case acc: 0.04945496
top acc: 0.1154 ::: bot acc: 0.0505
top acc: 0.1454 ::: bot acc: 0.0880
top acc: 0.1102 ::: bot acc: 0.0269
top acc: 0.1044 ::: bot acc: 0.0348
top acc: 0.1144 ::: bot acc: 0.0322
top acc: 0.0856 ::: bot acc: 0.0175
current epoch: 17
train loss is 0.057916
average val loss: 0.056489, accuracy: 0.0563
average test loss: 0.067468, accuracy: 0.0677
case acc: 0.06363177
case acc: 0.09843337
case acc: 0.06325266
case acc: 0.06815502
case acc: 0.070003115
case acc: 0.042826407
top acc: 0.0950 ::: bot acc: 0.0308
top acc: 0.1264 ::: bot acc: 0.0687
top acc: 0.1054 ::: bot acc: 0.0237
top acc: 0.1032 ::: bot acc: 0.0339
top acc: 0.1115 ::: bot acc: 0.0295
top acc: 0.0770 ::: bot acc: 0.0150
current epoch: 18
train loss is 0.051723
average val loss: 0.049876, accuracy: 0.0496
average test loss: 0.060567, accuracy: 0.0608
case acc: 0.046625283
case acc: 0.07981817
case acc: 0.06186515
case acc: 0.06760851
case acc: 0.06781365
case acc: 0.0407706
top acc: 0.0769 ::: bot acc: 0.0159
top acc: 0.1078 ::: bot acc: 0.0500
top acc: 0.1038 ::: bot acc: 0.0228
top acc: 0.1026 ::: bot acc: 0.0334
top acc: 0.1092 ::: bot acc: 0.0276
top acc: 0.0742 ::: bot acc: 0.0145
current epoch: 19
train loss is 0.048124
average val loss: 0.040852, accuracy: 0.0403
average test loss: 0.050651, accuracy: 0.0509
case acc: 0.031231256
case acc: 0.057103153
case acc: 0.05681666
case acc: 0.061907902
case acc: 0.06140731
case acc: 0.03687112
top acc: 0.0555 ::: bot acc: 0.0128
top acc: 0.0845 ::: bot acc: 0.0283
top acc: 0.0980 ::: bot acc: 0.0193
top acc: 0.0967 ::: bot acc: 0.0283
top acc: 0.1022 ::: bot acc: 0.0224
top acc: 0.0687 ::: bot acc: 0.0141
current epoch: 20
train loss is 0.046824
average val loss: 0.030960, accuracy: 0.0305
average test loss: 0.037583, accuracy: 0.0380
case acc: 0.024160687
case acc: 0.031076755
case acc: 0.045830976
case acc: 0.047697928
case acc: 0.048351746
case acc: 0.030792713
top acc: 0.0278 ::: bot acc: 0.0373
top acc: 0.0532 ::: bot acc: 0.0131
top acc: 0.0836 ::: bot acc: 0.0154
top acc: 0.0809 ::: bot acc: 0.0175
top acc: 0.0869 ::: bot acc: 0.0140
top acc: 0.0564 ::: bot acc: 0.0206
current epoch: 21
train loss is 0.048597
average val loss: 0.033518, accuracy: 0.0341
average test loss: 0.030865, accuracy: 0.0320
case acc: 0.043870706
case acc: 0.026033636
case acc: 0.034144055
case acc: 0.027448699
case acc: 0.032039583
case acc: 0.028365752
top acc: 0.0147 ::: bot acc: 0.0763
top acc: 0.0096 ::: bot acc: 0.0503
top acc: 0.0540 ::: bot acc: 0.0369
top acc: 0.0475 ::: bot acc: 0.0232
top acc: 0.0563 ::: bot acc: 0.0262
top acc: 0.0321 ::: bot acc: 0.0448
current epoch: 22
train loss is 0.053561
average val loss: 0.067710, accuracy: 0.0684
average test loss: 0.056672, accuracy: 0.0578
case acc: 0.089379676
case acc: 0.078867294
case acc: 0.04564416
case acc: 0.04441973
case acc: 0.040317666
case acc: 0.048205987
top acc: 0.0577 ::: bot acc: 0.1231
top acc: 0.0510 ::: bot acc: 0.1087
top acc: 0.0195 ::: bot acc: 0.0820
top acc: 0.0160 ::: bot acc: 0.0760
top acc: 0.0148 ::: bot acc: 0.0733
top acc: 0.0200 ::: bot acc: 0.0807
current epoch: 23
train loss is 0.058846
average val loss: 0.079945, accuracy: 0.0803
average test loss: 0.068356, accuracy: 0.0690
case acc: 0.095192246
case acc: 0.094554044
case acc: 0.05612897
case acc: 0.06398944
case acc: 0.05269255
case acc: 0.051283322
top acc: 0.0635 ::: bot acc: 0.1290
top acc: 0.0668 ::: bot acc: 0.1243
top acc: 0.0228 ::: bot acc: 0.0960
top acc: 0.0291 ::: bot acc: 0.0986
top acc: 0.0165 ::: bot acc: 0.0908
top acc: 0.0213 ::: bot acc: 0.0846
current epoch: 24
train loss is 0.055884
average val loss: 0.059471, accuracy: 0.0599
average test loss: 0.049019, accuracy: 0.0496
case acc: 0.062523626
case acc: 0.06575445
case acc: 0.042362805
case acc: 0.051946454
case acc: 0.041308716
case acc: 0.033958588
top acc: 0.0309 ::: bot acc: 0.0962
top acc: 0.0381 ::: bot acc: 0.0954
top acc: 0.0200 ::: bot acc: 0.0769
top acc: 0.0201 ::: bot acc: 0.0851
top acc: 0.0144 ::: bot acc: 0.0749
top acc: 0.0203 ::: bot acc: 0.0593
current epoch: 25
train loss is 0.047677
average val loss: 0.047869, accuracy: 0.0479
average test loss: 0.038684, accuracy: 0.0387
case acc: 0.040122613
case acc: 0.041432478
case acc: 0.038284834
case acc: 0.04538743
case acc: 0.03643038
case acc: 0.030312376
top acc: 0.0116 ::: bot acc: 0.0723
top acc: 0.0151 ::: bot acc: 0.0704
top acc: 0.0239 ::: bot acc: 0.0690
top acc: 0.0163 ::: bot acc: 0.0773
top acc: 0.0184 ::: bot acc: 0.0657
top acc: 0.0258 ::: bot acc: 0.0512
current epoch: 26
train loss is 0.040597
average val loss: 0.038004, accuracy: 0.0376
average test loss: 0.031344, accuracy: 0.0307
case acc: 0.026423547
case acc: 0.023400916
case acc: 0.035244774
case acc: 0.037970185
case acc: 0.03234369
case acc: 0.028702568
top acc: 0.0159 ::: bot acc: 0.0497
top acc: 0.0127 ::: bot acc: 0.0447
top acc: 0.0298 ::: bot acc: 0.0615
top acc: 0.0137 ::: bot acc: 0.0676
top acc: 0.0269 ::: bot acc: 0.0555
top acc: 0.0306 ::: bot acc: 0.0462
current epoch: 27
train loss is 0.036570
average val loss: 0.030857, accuracy: 0.0306
average test loss: 0.028789, accuracy: 0.0282
case acc: 0.024894444
case acc: 0.023173517
case acc: 0.033193305
case acc: 0.030299447
case acc: 0.030052707
case acc: 0.027723514
top acc: 0.0378 ::: bot acc: 0.0278
top acc: 0.0386 ::: bot acc: 0.0187
top acc: 0.0402 ::: bot acc: 0.0510
top acc: 0.0177 ::: bot acc: 0.0542
top acc: 0.0391 ::: bot acc: 0.0433
top acc: 0.0376 ::: bot acc: 0.0391
current epoch: 28
train loss is 0.037203
average val loss: 0.029760, accuracy: 0.0305
average test loss: 0.034825, accuracy: 0.0348
case acc: 0.0373277
case acc: 0.04568834
case acc: 0.03573365
case acc: 0.026646012
case acc: 0.033433713
case acc: 0.030139852
top acc: 0.0652 ::: bot acc: 0.0117
top acc: 0.0719 ::: bot acc: 0.0191
top acc: 0.0617 ::: bot acc: 0.0295
top acc: 0.0423 ::: bot acc: 0.0287
top acc: 0.0609 ::: bot acc: 0.0216
top acc: 0.0546 ::: bot acc: 0.0222
current epoch: 29
train loss is 0.043435
average val loss: 0.045696, accuracy: 0.0460
average test loss: 0.056084, accuracy: 0.0563
case acc: 0.06258997
case acc: 0.081902966
case acc: 0.051702414
case acc: 0.0460641
case acc: 0.05229677
case acc: 0.043065745
top acc: 0.0941 ::: bot acc: 0.0295
top acc: 0.1096 ::: bot acc: 0.0523
top acc: 0.0917 ::: bot acc: 0.0168
top acc: 0.0790 ::: bot acc: 0.0166
top acc: 0.0917 ::: bot acc: 0.0161
top acc: 0.0774 ::: bot acc: 0.0148
current epoch: 30
train loss is 0.052694
average val loss: 0.059111, accuracy: 0.0591
average test loss: 0.070420, accuracy: 0.0706
case acc: 0.07035842
case acc: 0.09715197
case acc: 0.06663109
case acc: 0.06842116
case acc: 0.070194036
case acc: 0.050763857
top acc: 0.1021 ::: bot acc: 0.0369
top acc: 0.1249 ::: bot acc: 0.0675
top acc: 0.1093 ::: bot acc: 0.0262
top acc: 0.1037 ::: bot acc: 0.0340
top acc: 0.1117 ::: bot acc: 0.0299
top acc: 0.0872 ::: bot acc: 0.0181
current epoch: 31
train loss is 0.051495
average val loss: 0.040634, accuracy: 0.0405
average test loss: 0.050538, accuracy: 0.0508
case acc: 0.04042987
case acc: 0.06759779
case acc: 0.051077563
case acc: 0.055736378
case acc: 0.05586622
case acc: 0.034205455
top acc: 0.0696 ::: bot acc: 0.0123
top acc: 0.0951 ::: bot acc: 0.0383
top acc: 0.0908 ::: bot acc: 0.0166
top acc: 0.0902 ::: bot acc: 0.0230
top acc: 0.0960 ::: bot acc: 0.0183
top acc: 0.0639 ::: bot acc: 0.0154
current epoch: 32
train loss is 0.046803
average val loss: 0.028281, accuracy: 0.0284
average test loss: 0.034109, accuracy: 0.0347
case acc: 0.024593646
case acc: 0.035046596
case acc: 0.039784025
case acc: 0.039626673
case acc: 0.040845454
case acc: 0.02844577
top acc: 0.0357 ::: bot acc: 0.0298
top acc: 0.0588 ::: bot acc: 0.0136
top acc: 0.0723 ::: bot acc: 0.0203
top acc: 0.0709 ::: bot acc: 0.0135
top acc: 0.0759 ::: bot acc: 0.0136
top acc: 0.0468 ::: bot acc: 0.0299
current epoch: 33
train loss is 0.040098
average val loss: 0.028031, accuracy: 0.0282
average test loss: 0.028944, accuracy: 0.0293
case acc: 0.027293919
case acc: 0.020974746
case acc: 0.035555806
case acc: 0.030144695
case acc: 0.034029823
case acc: 0.027786674
top acc: 0.0135 ::: bot acc: 0.0522
top acc: 0.0298 ::: bot acc: 0.0277
top acc: 0.0611 ::: bot acc: 0.0300
top acc: 0.0556 ::: bot acc: 0.0159
top acc: 0.0624 ::: bot acc: 0.0205
top acc: 0.0407 ::: bot acc: 0.0360
current epoch: 34
train loss is 0.039720
average val loss: 0.039955, accuracy: 0.0407
average test loss: 0.033051, accuracy: 0.0341
case acc: 0.047193673
case acc: 0.03646184
case acc: 0.03336102
case acc: 0.026797816
case acc: 0.03024099
case acc: 0.030701503
top acc: 0.0173 ::: bot acc: 0.0800
top acc: 0.0112 ::: bot acc: 0.0650
top acc: 0.0378 ::: bot acc: 0.0533
top acc: 0.0265 ::: bot acc: 0.0443
top acc: 0.0376 ::: bot acc: 0.0450
top acc: 0.0249 ::: bot acc: 0.0523
current epoch: 35
train loss is 0.042545
average val loss: 0.059678, accuracy: 0.0602
average test loss: 0.049026, accuracy: 0.0500
case acc: 0.06701577
case acc: 0.066738375
case acc: 0.042800687
case acc: 0.044612728
case acc: 0.03942194
case acc: 0.03920661
top acc: 0.0352 ::: bot acc: 0.1007
top acc: 0.0390 ::: bot acc: 0.0964
top acc: 0.0198 ::: bot acc: 0.0777
top acc: 0.0160 ::: bot acc: 0.0763
top acc: 0.0156 ::: bot acc: 0.0716
top acc: 0.0183 ::: bot acc: 0.0682
current epoch: 36
train loss is 0.045847
average val loss: 0.059889, accuracy: 0.0602
average test loss: 0.049223, accuracy: 0.0498
case acc: 0.05850745
case acc: 0.06434037
case acc: 0.044617057
case acc: 0.052122623
case acc: 0.042405497
case acc: 0.036529254
top acc: 0.0271 ::: bot acc: 0.0920
top acc: 0.0367 ::: bot acc: 0.0939
top acc: 0.0195 ::: bot acc: 0.0805
top acc: 0.0203 ::: bot acc: 0.0854
top acc: 0.0143 ::: bot acc: 0.0767
top acc: 0.0188 ::: bot acc: 0.0639
current epoch: 37
train loss is 0.046105
average val loss: 0.039316, accuracy: 0.0393
average test loss: 0.032366, accuracy: 0.0320
case acc: 0.028671337
case acc: 0.031309403
case acc: 0.0345105
case acc: 0.037058044
case acc: 0.032409895
case acc: 0.027747082
top acc: 0.0109 ::: bot acc: 0.0555
top acc: 0.0088 ::: bot acc: 0.0583
top acc: 0.0322 ::: bot acc: 0.0591
top acc: 0.0136 ::: bot acc: 0.0663
top acc: 0.0271 ::: bot acc: 0.0555
top acc: 0.0366 ::: bot acc: 0.0402
current epoch: 38
train loss is 0.040024
average val loss: 0.028416, accuracy: 0.0284
average test loss: 0.028492, accuracy: 0.0280
case acc: 0.025691817
case acc: 0.021872059
case acc: 0.03367095
case acc: 0.027263941
case acc: 0.03032785
case acc: 0.029101769
top acc: 0.0419 ::: bot acc: 0.0237
top acc: 0.0348 ::: bot acc: 0.0224
top acc: 0.0495 ::: bot acc: 0.0417
top acc: 0.0250 ::: bot acc: 0.0460
top acc: 0.0474 ::: bot acc: 0.0352
top acc: 0.0505 ::: bot acc: 0.0263
current epoch: 39
train loss is 0.036855
average val loss: 0.029828, accuracy: 0.0302
average test loss: 0.036421, accuracy: 0.0363
case acc: 0.039196227
case acc: 0.04278916
case acc: 0.03788707
case acc: 0.02787518
case acc: 0.036467254
case acc: 0.033604935
top acc: 0.0679 ::: bot acc: 0.0120
top acc: 0.0685 ::: bot acc: 0.0172
top acc: 0.0679 ::: bot acc: 0.0236
top acc: 0.0491 ::: bot acc: 0.0220
top acc: 0.0681 ::: bot acc: 0.0162
top acc: 0.0628 ::: bot acc: 0.0160
current epoch: 40
train loss is 0.039600
average val loss: 0.040993, accuracy: 0.0413
average test loss: 0.051071, accuracy: 0.0512
case acc: 0.05476731
case acc: 0.06879325
case acc: 0.048404858
case acc: 0.04374645
case acc: 0.050543703
case acc: 0.04098564
top acc: 0.0861 ::: bot acc: 0.0222
top acc: 0.0963 ::: bot acc: 0.0396
top acc: 0.0874 ::: bot acc: 0.0157
top acc: 0.0761 ::: bot acc: 0.0153
top acc: 0.0896 ::: bot acc: 0.0151
top acc: 0.0745 ::: bot acc: 0.0143
current epoch: 41
train loss is 0.043208
average val loss: 0.046057, accuracy: 0.0461
average test loss: 0.056678, accuracy: 0.0569
case acc: 0.053468328
case acc: 0.073930755
case acc: 0.055141877
case acc: 0.056842037
case acc: 0.05969693
case acc: 0.042340796
top acc: 0.0848 ::: bot acc: 0.0211
top acc: 0.1015 ::: bot acc: 0.0445
top acc: 0.0961 ::: bot acc: 0.0184
top acc: 0.0914 ::: bot acc: 0.0238
top acc: 0.1003 ::: bot acc: 0.0212
top acc: 0.0764 ::: bot acc: 0.0147
current epoch: 42
train loss is 0.042241
average val loss: 0.033761, accuracy: 0.0337
average test loss: 0.042641, accuracy: 0.0429
case acc: 0.032167543
case acc: 0.0495022
case acc: 0.045456007
case acc: 0.04865645
case acc: 0.049589947
case acc: 0.03220876
top acc: 0.0575 ::: bot acc: 0.0121
top acc: 0.0762 ::: bot acc: 0.0219
top acc: 0.0830 ::: bot acc: 0.0156
top acc: 0.0820 ::: bot acc: 0.0180
top acc: 0.0885 ::: bot acc: 0.0146
top acc: 0.0598 ::: bot acc: 0.0177
current epoch: 43
train loss is 0.038915
average val loss: 0.027028, accuracy: 0.0271
average test loss: 0.030142, accuracy: 0.0306
case acc: 0.024447566
case acc: 0.023971794
case acc: 0.03667278
case acc: 0.034081686
case acc: 0.03652925
case acc: 0.028008929
top acc: 0.0248 ::: bot acc: 0.0407
top acc: 0.0407 ::: bot acc: 0.0169
top acc: 0.0646 ::: bot acc: 0.0265
top acc: 0.0625 ::: bot acc: 0.0135
top acc: 0.0683 ::: bot acc: 0.0162
top acc: 0.0432 ::: bot acc: 0.0336
current epoch: 44
train loss is 0.036668
average val loss: 0.035579, accuracy: 0.0361
average test loss: 0.030246, accuracy: 0.0311
case acc: 0.03908435
case acc: 0.028618181
case acc: 0.033053808
case acc: 0.025949776
case acc: 0.030188356
case acc: 0.02995142
top acc: 0.0108 ::: bot acc: 0.0710
top acc: 0.0087 ::: bot acc: 0.0544
top acc: 0.0422 ::: bot acc: 0.0489
top acc: 0.0354 ::: bot acc: 0.0355
top acc: 0.0437 ::: bot acc: 0.0390
top acc: 0.0263 ::: bot acc: 0.0505
current epoch: 45
train loss is 0.039269
average val loss: 0.053607, accuracy: 0.0542
average test loss: 0.043490, accuracy: 0.0445
case acc: 0.059248388
case acc: 0.05687649
case acc: 0.03986268
case acc: 0.03699419
case acc: 0.03627802
case acc: 0.037807554
top acc: 0.0277 ::: bot acc: 0.0928
top acc: 0.0293 ::: bot acc: 0.0864
top acc: 0.0218 ::: bot acc: 0.0723
top acc: 0.0136 ::: bot acc: 0.0662
top acc: 0.0192 ::: bot acc: 0.0651
top acc: 0.0185 ::: bot acc: 0.0660
current epoch: 46
train loss is 0.042323
average val loss: 0.050784, accuracy: 0.0511
average test loss: 0.041036, accuracy: 0.0416
case acc: 0.047961023
case acc: 0.05160855
case acc: 0.039226472
case acc: 0.040658154
case acc: 0.03691357
case acc: 0.033163793
top acc: 0.0178 ::: bot acc: 0.0809
top acc: 0.0243 ::: bot acc: 0.0810
top acc: 0.0225 ::: bot acc: 0.0710
top acc: 0.0142 ::: bot acc: 0.0714
top acc: 0.0182 ::: bot acc: 0.0666
top acc: 0.0211 ::: bot acc: 0.0578
current epoch: 47
train loss is 0.040373
average val loss: 0.035884, accuracy: 0.0359
average test loss: 0.030019, accuracy: 0.0297
case acc: 0.026690654
case acc: 0.026864462
case acc: 0.033654362
case acc: 0.031978663
case acc: 0.031186033
case acc: 0.02773241
top acc: 0.0154 ::: bot acc: 0.0504
top acc: 0.0091 ::: bot acc: 0.0516
top acc: 0.0363 ::: bot acc: 0.0550
top acc: 0.0156 ::: bot acc: 0.0578
top acc: 0.0317 ::: bot acc: 0.0509
top acc: 0.0373 ::: bot acc: 0.0394
current epoch: 48
train loss is 0.035227
average val loss: 0.027759, accuracy: 0.0278
average test loss: 0.028377, accuracy: 0.0280
case acc: 0.026081447
case acc: 0.02259377
case acc: 0.033789445
case acc: 0.026212221
case acc: 0.030502576
case acc: 0.028871698
top acc: 0.0432 ::: bot acc: 0.0225
top acc: 0.0370 ::: bot acc: 0.0201
top acc: 0.0508 ::: bot acc: 0.0404
top acc: 0.0301 ::: bot acc: 0.0409
top acc: 0.0488 ::: bot acc: 0.0338
top acc: 0.0491 ::: bot acc: 0.0276
current epoch: 49
train loss is 0.033665
average val loss: 0.030047, accuracy: 0.0305
average test loss: 0.037142, accuracy: 0.0371
case acc: 0.03964149
case acc: 0.04362756
case acc: 0.038625572
case acc: 0.029609218
case acc: 0.03741225
case acc: 0.0334832
top acc: 0.0686 ::: bot acc: 0.0121
top acc: 0.0694 ::: bot acc: 0.0178
top acc: 0.0697 ::: bot acc: 0.0221
top acc: 0.0542 ::: bot acc: 0.0172
top acc: 0.0700 ::: bot acc: 0.0153
top acc: 0.0625 ::: bot acc: 0.0160
current epoch: 50
train loss is 0.036787
average val loss: 0.039318, accuracy: 0.0396
average test loss: 0.049294, accuracy: 0.0494
case acc: 0.0508848
case acc: 0.063960485
case acc: 0.047654726
case acc: 0.045033094
case acc: 0.04962411
case acc: 0.039402105
top acc: 0.0821 ::: bot acc: 0.0189
top acc: 0.0913 ::: bot acc: 0.0350
top acc: 0.0864 ::: bot acc: 0.0155
top acc: 0.0776 ::: bot acc: 0.0161
top acc: 0.0885 ::: bot acc: 0.0147
top acc: 0.0724 ::: bot acc: 0.0139
LME_Co_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 6774 6774 6774
1.7082474 -0.6288155 0.21141115 -0.19947179
Validation: 756 756 756
Testing: 768 768 768
pre-processing time: 0.00027751922607421875
the split date is 2011-07-01
net initializing with time: 0.0030639171600341797
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.269090
average val loss: 0.187305, accuracy: 0.1892
average test loss: 0.187703, accuracy: 0.1892
case acc: 0.3496531
case acc: 0.2808354
case acc: 0.06078967
case acc: 0.10094236
case acc: 0.30114424
case acc: 0.041798137
top acc: 0.3829 ::: bot acc: 0.3188
top acc: 0.3115 ::: bot acc: 0.2519
top acc: 0.0263 ::: bot acc: 0.0989
top acc: 0.1271 ::: bot acc: 0.0735
top acc: 0.3330 ::: bot acc: 0.2665
top acc: 0.0172 ::: bot acc: 0.0737
current epoch: 2
train loss is 0.269362
average val loss: 0.150917, accuracy: 0.1516
average test loss: 0.150821, accuracy: 0.1507
case acc: 0.20245421
case acc: 0.13963288
case acc: 0.18835717
case acc: 0.03846806
case acc: 0.15800115
case acc: 0.17731678
top acc: 0.2361 ::: bot acc: 0.1722
top acc: 0.1687 ::: bot acc: 0.1115
top acc: 0.1453 ::: bot acc: 0.2307
top acc: 0.0155 ::: bot acc: 0.0639
top acc: 0.1905 ::: bot acc: 0.1229
top acc: 0.1375 ::: bot acc: 0.2171
current epoch: 3
train loss is 0.156393
average val loss: 0.151216, accuracy: 0.1524
average test loss: 0.151388, accuracy: 0.1522
case acc: 0.25783432
case acc: 0.20145501
case acc: 0.105324164
case acc: 0.036769733
case acc: 0.21906409
case acc: 0.09295814
top acc: 0.2916 ::: bot acc: 0.2278
top acc: 0.2302 ::: bot acc: 0.1736
top acc: 0.0626 ::: bot acc: 0.1475
top acc: 0.0585 ::: bot acc: 0.0174
top acc: 0.2517 ::: bot acc: 0.1838
top acc: 0.0564 ::: bot acc: 0.1313
current epoch: 4
train loss is 0.162302
average val loss: 0.132820, accuracy: 0.1337
average test loss: 0.132618, accuracy: 0.1326
case acc: 0.21129216
case acc: 0.16166732
case acc: 0.12068966
case acc: 0.019713243
case acc: 0.17810695
case acc: 0.103963956
top acc: 0.2451 ::: bot acc: 0.1813
top acc: 0.1903 ::: bot acc: 0.1339
top acc: 0.0775 ::: bot acc: 0.1631
top acc: 0.0297 ::: bot acc: 0.0226
top acc: 0.2109 ::: bot acc: 0.1428
top acc: 0.0661 ::: bot acc: 0.1430
current epoch: 5
train loss is 0.137662
average val loss: 0.130994, accuracy: 0.1327
average test loss: 0.131250, accuracy: 0.1326
case acc: 0.2330316
case acc: 0.19074863
case acc: 0.06788058
case acc: 0.04837075
case acc: 0.2054997
case acc: 0.050039172
top acc: 0.2668 ::: bot acc: 0.2030
top acc: 0.2194 ::: bot acc: 0.1629
top acc: 0.0305 ::: bot acc: 0.1074
top acc: 0.0719 ::: bot acc: 0.0253
top acc: 0.2384 ::: bot acc: 0.1701
top acc: 0.0204 ::: bot acc: 0.0851
current epoch: 6
train loss is 0.132499
average val loss: 0.110824, accuracy: 0.1121
average test loss: 0.110865, accuracy: 0.1115
case acc: 0.18557549
case acc: 0.15038335
case acc: 0.08228941
case acc: 0.02558299
case acc: 0.16367367
case acc: 0.061309166
top acc: 0.2194 ::: bot acc: 0.1555
top acc: 0.1788 ::: bot acc: 0.1226
top acc: 0.0421 ::: bot acc: 0.1231
top acc: 0.0431 ::: bot acc: 0.0147
top acc: 0.1967 ::: bot acc: 0.1282
top acc: 0.0285 ::: bot acc: 0.0980
current epoch: 7
train loss is 0.118310
average val loss: 0.102343, accuracy: 0.1039
average test loss: 0.102245, accuracy: 0.1035
case acc: 0.17704706
case acc: 0.14909546
case acc: 0.060735226
case acc: 0.033050776
case acc: 0.16069388
case acc: 0.04029136
top acc: 0.2108 ::: bot acc: 0.1471
top acc: 0.1774 ::: bot acc: 0.1214
top acc: 0.0252 ::: bot acc: 0.0991
top acc: 0.0537 ::: bot acc: 0.0157
top acc: 0.1938 ::: bot acc: 0.1251
top acc: 0.0175 ::: bot acc: 0.0722
current epoch: 8
train loss is 0.104464
average val loss: 0.090738, accuracy: 0.0922
average test loss: 0.090377, accuracy: 0.0918
case acc: 0.15489268
case acc: 0.13417096
case acc: 0.052784726
case acc: 0.030389275
case acc: 0.1441573
case acc: 0.03415518
top acc: 0.1887 ::: bot acc: 0.1249
top acc: 0.1624 ::: bot acc: 0.1065
top acc: 0.0203 ::: bot acc: 0.0896
top acc: 0.0502 ::: bot acc: 0.0147
top acc: 0.1774 ::: bot acc: 0.1085
top acc: 0.0218 ::: bot acc: 0.0609
current epoch: 9
train loss is 0.095321
average val loss: 0.081913, accuracy: 0.0832
average test loss: 0.081197, accuracy: 0.0830
case acc: 0.13737464
case acc: 0.12403895
case acc: 0.042748213
case acc: 0.031208761
case acc: 0.13236561
case acc: 0.03024375
top acc: 0.1712 ::: bot acc: 0.1073
top acc: 0.1521 ::: bot acc: 0.0964
top acc: 0.0184 ::: bot acc: 0.0756
top acc: 0.0512 ::: bot acc: 0.0150
top acc: 0.1657 ::: bot acc: 0.0966
top acc: 0.0354 ::: bot acc: 0.0462
current epoch: 10
train loss is 0.083292
average val loss: 0.071336, accuracy: 0.0724
average test loss: 0.070351, accuracy: 0.0721
case acc: 0.11389505
case acc: 0.108009
case acc: 0.038419113
case acc: 0.027464684
case acc: 0.11476902
case acc: 0.030269124
top acc: 0.1478 ::: bot acc: 0.0838
top acc: 0.1360 ::: bot acc: 0.0804
top acc: 0.0207 ::: bot acc: 0.0680
top acc: 0.0461 ::: bot acc: 0.0141
top acc: 0.1481 ::: bot acc: 0.0790
top acc: 0.0431 ::: bot acc: 0.0388
current epoch: 11
train loss is 0.074024
average val loss: 0.065462, accuracy: 0.0664
average test loss: 0.064294, accuracy: 0.0661
case acc: 0.09795111
case acc: 0.09964782
case acc: 0.033006884
case acc: 0.028879367
case acc: 0.10478184
case acc: 0.03233113
top acc: 0.1319 ::: bot acc: 0.0679
top acc: 0.1275 ::: bot acc: 0.0722
top acc: 0.0321 ::: bot acc: 0.0535
top acc: 0.0482 ::: bot acc: 0.0141
top acc: 0.1383 ::: bot acc: 0.0689
top acc: 0.0559 ::: bot acc: 0.0264
current epoch: 12
train loss is 0.065554
average val loss: 0.056186, accuracy: 0.0570
average test loss: 0.054842, accuracy: 0.0563
case acc: 0.07499278
case acc: 0.0844056
case acc: 0.031763542
case acc: 0.025163094
case acc: 0.08798665
case acc: 0.033470456
top acc: 0.1089 ::: bot acc: 0.0449
top acc: 0.1121 ::: bot acc: 0.0570
top acc: 0.0388 ::: bot acc: 0.0470
top acc: 0.0428 ::: bot acc: 0.0139
top acc: 0.1216 ::: bot acc: 0.0520
top acc: 0.0591 ::: bot acc: 0.0239
current epoch: 13
train loss is 0.056914
average val loss: 0.050960, accuracy: 0.0515
average test loss: 0.049593, accuracy: 0.0506
case acc: 0.058299385
case acc: 0.075323924
case acc: 0.032028385
case acc: 0.024964985
case acc: 0.07725404
case acc: 0.03584754
top acc: 0.0923 ::: bot acc: 0.0283
top acc: 0.1028 ::: bot acc: 0.0480
top acc: 0.0503 ::: bot acc: 0.0358
top acc: 0.0426 ::: bot acc: 0.0136
top acc: 0.1110 ::: bot acc: 0.0412
top acc: 0.0644 ::: bot acc: 0.0206
current epoch: 14
train loss is 0.049980
average val loss: 0.046054, accuracy: 0.0463
average test loss: 0.044688, accuracy: 0.0450
case acc: 0.042887233
case acc: 0.0666621
case acc: 0.033773493
case acc: 0.024102747
case acc: 0.06708189
case acc: 0.035759006
top acc: 0.0765 ::: bot acc: 0.0139
top acc: 0.0939 ::: bot acc: 0.0395
top acc: 0.0599 ::: bot acc: 0.0267
top acc: 0.0414 ::: bot acc: 0.0133
top acc: 0.1007 ::: bot acc: 0.0316
top acc: 0.0644 ::: bot acc: 0.0206
current epoch: 15
train loss is 0.043509
average val loss: 0.042665, accuracy: 0.0428
average test loss: 0.041289, accuracy: 0.0412
case acc: 0.03317835
case acc: 0.06011928
case acc: 0.03672734
case acc: 0.023700489
case acc: 0.05905225
case acc: 0.034423508
top acc: 0.0640 ::: bot acc: 0.0101
top acc: 0.0871 ::: bot acc: 0.0331
top acc: 0.0690 ::: bot acc: 0.0185
top acc: 0.0408 ::: bot acc: 0.0130
top acc: 0.0921 ::: bot acc: 0.0249
top acc: 0.0615 ::: bot acc: 0.0225
current epoch: 16
train loss is 0.038707
average val loss: 0.042048, accuracy: 0.0422
average test loss: 0.040745, accuracy: 0.0404
case acc: 0.02889358
case acc: 0.057506967
case acc: 0.04217599
case acc: 0.025102533
case acc: 0.054833867
case acc: 0.03364128
top acc: 0.0573 ::: bot acc: 0.0109
top acc: 0.0844 ::: bot acc: 0.0306
top acc: 0.0793 ::: bot acc: 0.0141
top acc: 0.0430 ::: bot acc: 0.0126
top acc: 0.0877 ::: bot acc: 0.0215
top acc: 0.0597 ::: bot acc: 0.0237
current epoch: 17
train loss is 0.035594
average val loss: 0.046014, accuracy: 0.0462
average test loss: 0.044981, accuracy: 0.0445
case acc: 0.029672982
case acc: 0.06134496
case acc: 0.05229392
case acc: 0.031185925
case acc: 0.05706121
case acc: 0.035597797
top acc: 0.0588 ::: bot acc: 0.0106
top acc: 0.0882 ::: bot acc: 0.0345
top acc: 0.0937 ::: bot acc: 0.0155
top acc: 0.0511 ::: bot acc: 0.0143
top acc: 0.0902 ::: bot acc: 0.0231
top acc: 0.0643 ::: bot acc: 0.0204
current epoch: 18
train loss is 0.037018
average val loss: 0.057782, accuracy: 0.0580
average test loss: 0.057267, accuracy: 0.0570
case acc: 0.03680008
case acc: 0.07422812
case acc: 0.070651926
case acc: 0.046040498
case acc: 0.06884856
case acc: 0.045294363
top acc: 0.0695 ::: bot acc: 0.0105
top acc: 0.1011 ::: bot acc: 0.0474
top acc: 0.1150 ::: bot acc: 0.0280
top acc: 0.0686 ::: bot acc: 0.0237
top acc: 0.1031 ::: bot acc: 0.0328
top acc: 0.0791 ::: bot acc: 0.0198
current epoch: 19
train loss is 0.041524
average val loss: 0.066946, accuracy: 0.0670
average test loss: 0.066694, accuracy: 0.0665
case acc: 0.040223684
case acc: 0.08271478
case acc: 0.08537746
case acc: 0.05866799
case acc: 0.078262344
case acc: 0.0536113
top acc: 0.0738 ::: bot acc: 0.0121
top acc: 0.1095 ::: bot acc: 0.0560
top acc: 0.1300 ::: bot acc: 0.0422
top acc: 0.0822 ::: bot acc: 0.0342
top acc: 0.1128 ::: bot acc: 0.0418
top acc: 0.0898 ::: bot acc: 0.0233
current epoch: 20
train loss is 0.048337
average val loss: 0.066745, accuracy: 0.0667
average test loss: 0.066495, accuracy: 0.0662
case acc: 0.0346189
case acc: 0.08052704
case acc: 0.086664245
case acc: 0.061790477
case acc: 0.078830786
case acc: 0.054947756
top acc: 0.0667 ::: bot acc: 0.0098
top acc: 0.1073 ::: bot acc: 0.0538
top acc: 0.1313 ::: bot acc: 0.0435
top acc: 0.0855 ::: bot acc: 0.0370
top acc: 0.1134 ::: bot acc: 0.0423
top acc: 0.0914 ::: bot acc: 0.0240
current epoch: 21
train loss is 0.053041
average val loss: 0.049249, accuracy: 0.0497
average test loss: 0.048498, accuracy: 0.0481
case acc: 0.02268729
case acc: 0.056259394
case acc: 0.06257683
case acc: 0.04407625
case acc: 0.06016907
case acc: 0.04301148
top acc: 0.0386 ::: bot acc: 0.0255
top acc: 0.0830 ::: bot acc: 0.0295
top acc: 0.1063 ::: bot acc: 0.0214
top acc: 0.0665 ::: bot acc: 0.0221
top acc: 0.0939 ::: bot acc: 0.0254
top acc: 0.0759 ::: bot acc: 0.0196
current epoch: 22
train loss is 0.055155
average val loss: 0.031729, accuracy: 0.0325
average test loss: 0.029622, accuracy: 0.0300
case acc: 0.047522143
case acc: 0.020774297
case acc: 0.032985374
case acc: 0.017937727
case acc: 0.030415414
case acc: 0.030247122
top acc: 0.0232 ::: bot acc: 0.0723
top acc: 0.0355 ::: bot acc: 0.0181
top acc: 0.0538 ::: bot acc: 0.0342
top acc: 0.0223 ::: bot acc: 0.0271
top acc: 0.0520 ::: bot acc: 0.0204
top acc: 0.0426 ::: bot acc: 0.0396
current epoch: 23
train loss is 0.054521
average val loss: 0.045337, accuracy: 0.0462
average test loss: 0.044625, accuracy: 0.0448
case acc: 0.076898634
case acc: 0.035494983
case acc: 0.047739822
case acc: 0.04370831
case acc: 0.029328354
case acc: 0.035674684
top acc: 0.0458 ::: bot acc: 0.1050
top acc: 0.0143 ::: bot acc: 0.0596
top acc: 0.0182 ::: bot acc: 0.0836
top acc: 0.0207 ::: bot acc: 0.0685
top acc: 0.0161 ::: bot acc: 0.0554
top acc: 0.0198 ::: bot acc: 0.0641
current epoch: 24
train loss is 0.042651
average val loss: 0.048033, accuracy: 0.0486
average test loss: 0.047455, accuracy: 0.0478
case acc: 0.072316214
case acc: 0.040936425
case acc: 0.058232203
case acc: 0.051893607
case acc: 0.03114495
case acc: 0.032242812
top acc: 0.0420 ::: bot acc: 0.1001
top acc: 0.0174 ::: bot acc: 0.0663
top acc: 0.0226 ::: bot acc: 0.0970
top acc: 0.0283 ::: bot acc: 0.0769
top acc: 0.0142 ::: bot acc: 0.0590
top acc: 0.0271 ::: bot acc: 0.0552
current epoch: 25
train loss is 0.043009
average val loss: 0.060436, accuracy: 0.0608
average test loss: 0.060364, accuracy: 0.0605
case acc: 0.07769318
case acc: 0.056179915
case acc: 0.07843652
case acc: 0.07204805
case acc: 0.042664997
case acc: 0.0361894
top acc: 0.0465 ::: bot acc: 0.1058
top acc: 0.0299 ::: bot acc: 0.0828
top acc: 0.0372 ::: bot acc: 0.1200
top acc: 0.0480 ::: bot acc: 0.0973
top acc: 0.0162 ::: bot acc: 0.0752
top acc: 0.0192 ::: bot acc: 0.0652
current epoch: 26
train loss is 0.065090
average val loss: 0.034158, accuracy: 0.0339
average test loss: 0.032526, accuracy: 0.0330
case acc: 0.035716064
case acc: 0.022963922
case acc: 0.045650527
case acc: 0.036738846
case acc: 0.025335506
case acc: 0.031333763
top acc: 0.0202 ::: bot acc: 0.0562
top acc: 0.0132 ::: bot acc: 0.0415
top acc: 0.0178 ::: bot acc: 0.0806
top acc: 0.0146 ::: bot acc: 0.0612
top acc: 0.0336 ::: bot acc: 0.0374
top acc: 0.0539 ::: bot acc: 0.0284
current epoch: 27
train loss is 0.059364
average val loss: 0.033261, accuracy: 0.0334
average test loss: 0.031450, accuracy: 0.0313
case acc: 0.026410982
case acc: 0.028696809
case acc: 0.032326713
case acc: 0.01791205
case acc: 0.03964728
case acc: 0.042675637
top acc: 0.0532 ::: bot acc: 0.0126
top acc: 0.0525 ::: bot acc: 0.0080
top acc: 0.0479 ::: bot acc: 0.0401
top acc: 0.0236 ::: bot acc: 0.0258
top acc: 0.0681 ::: bot acc: 0.0156
top acc: 0.0751 ::: bot acc: 0.0201
current epoch: 28
train loss is 0.043178
average val loss: 0.033056, accuracy: 0.0333
average test loss: 0.031253, accuracy: 0.0310
case acc: 0.029046107
case acc: 0.032545127
case acc: 0.033305608
case acc: 0.018143866
case acc: 0.039494436
case acc: 0.033215232
top acc: 0.0582 ::: bot acc: 0.0104
top acc: 0.0577 ::: bot acc: 0.0091
top acc: 0.0565 ::: bot acc: 0.0315
top acc: 0.0264 ::: bot acc: 0.0230
top acc: 0.0678 ::: bot acc: 0.0156
top acc: 0.0595 ::: bot acc: 0.0232
current epoch: 29
train loss is 0.036248
average val loss: 0.036809, accuracy: 0.0369
average test loss: 0.035401, accuracy: 0.0349
case acc: 0.034008816
case acc: 0.041475262
case acc: 0.038469903
case acc: 0.021065451
case acc: 0.043253247
case acc: 0.031151567
top acc: 0.0660 ::: bot acc: 0.0095
top acc: 0.0679 ::: bot acc: 0.0155
top acc: 0.0727 ::: bot acc: 0.0167
top acc: 0.0363 ::: bot acc: 0.0137
top acc: 0.0733 ::: bot acc: 0.0158
top acc: 0.0533 ::: bot acc: 0.0288
current epoch: 30
train loss is 0.032982
average val loss: 0.047878, accuracy: 0.0481
average test loss: 0.047303, accuracy: 0.0470
case acc: 0.04445795
case acc: 0.058186382
case acc: 0.056226198
case acc: 0.03468304
case acc: 0.054078907
case acc: 0.034293547
top acc: 0.0788 ::: bot acc: 0.0151
top acc: 0.0850 ::: bot acc: 0.0314
top acc: 0.0989 ::: bot acc: 0.0173
top acc: 0.0557 ::: bot acc: 0.0156
top acc: 0.0870 ::: bot acc: 0.0208
top acc: 0.0618 ::: bot acc: 0.0218
current epoch: 31
train loss is 0.035711
average val loss: 0.065337, accuracy: 0.0655
average test loss: 0.065338, accuracy: 0.0655
case acc: 0.057155002
case acc: 0.07778021
case acc: 0.08418078
case acc: 0.05658632
case acc: 0.07091502
case acc: 0.046365798
top acc: 0.0917 ::: bot acc: 0.0274
top acc: 0.1046 ::: bot acc: 0.0509
top acc: 0.1289 ::: bot acc: 0.0409
top acc: 0.0801 ::: bot acc: 0.0323
top acc: 0.1052 ::: bot acc: 0.0348
top acc: 0.0802 ::: bot acc: 0.0208
current epoch: 32
train loss is 0.051180
average val loss: 0.057871, accuracy: 0.0580
average test loss: 0.057679, accuracy: 0.0575
case acc: 0.039951865
case acc: 0.06727569
case acc: 0.07978047
case acc: 0.052540176
case acc: 0.06291275
case acc: 0.042513352
top acc: 0.0738 ::: bot acc: 0.0117
top acc: 0.0941 ::: bot acc: 0.0404
top acc: 0.1245 ::: bot acc: 0.0365
top acc: 0.0757 ::: bot acc: 0.0289
top acc: 0.0968 ::: bot acc: 0.0277
top acc: 0.0748 ::: bot acc: 0.0201
current epoch: 33
train loss is 0.054995
average val loss: 0.032369, accuracy: 0.0326
average test loss: 0.030477, accuracy: 0.0301
case acc: 0.026517797
case acc: 0.026920725
case acc: 0.042421333
case acc: 0.021262001
case acc: 0.03335328
case acc: 0.030239208
top acc: 0.0263 ::: bot acc: 0.0395
top acc: 0.0498 ::: bot acc: 0.0082
top acc: 0.0796 ::: bot acc: 0.0147
top acc: 0.0368 ::: bot acc: 0.0134
top acc: 0.0577 ::: bot acc: 0.0175
top acc: 0.0452 ::: bot acc: 0.0368
current epoch: 34
train loss is 0.045773
average val loss: 0.029905, accuracy: 0.0302
average test loss: 0.027597, accuracy: 0.0278
case acc: 0.03961558
case acc: 0.019538082
case acc: 0.032424938
case acc: 0.019465927
case acc: 0.025712121
case acc: 0.030259412
top acc: 0.0203 ::: bot acc: 0.0619
top acc: 0.0244 ::: bot acc: 0.0293
top acc: 0.0487 ::: bot acc: 0.0394
top acc: 0.0135 ::: bot acc: 0.0359
top acc: 0.0377 ::: bot acc: 0.0331
top acc: 0.0376 ::: bot acc: 0.0444
current epoch: 35
train loss is 0.038541
average val loss: 0.031785, accuracy: 0.0320
average test loss: 0.029866, accuracy: 0.0303
case acc: 0.042539448
case acc: 0.022883134
case acc: 0.03491045
case acc: 0.026051797
case acc: 0.02527257
case acc: 0.030103922
top acc: 0.0211 ::: bot acc: 0.0658
top acc: 0.0133 ::: bot acc: 0.0413
top acc: 0.0288 ::: bot acc: 0.0593
top acc: 0.0080 ::: bot acc: 0.0485
top acc: 0.0298 ::: bot acc: 0.0410
top acc: 0.0407 ::: bot acc: 0.0413
current epoch: 36
train loss is 0.032680
average val loss: 0.036832, accuracy: 0.0369
average test loss: 0.035457, accuracy: 0.0360
case acc: 0.04491709
case acc: 0.030483313
case acc: 0.04469619
case acc: 0.038258307
case acc: 0.0274909
case acc: 0.030303426
top acc: 0.0221 ::: bot acc: 0.0689
top acc: 0.0118 ::: bot acc: 0.0533
top acc: 0.0177 ::: bot acc: 0.0794
top acc: 0.0159 ::: bot acc: 0.0628
top acc: 0.0201 ::: bot acc: 0.0507
top acc: 0.0374 ::: bot acc: 0.0446
current epoch: 37
train loss is 0.034947
average val loss: 0.045104, accuracy: 0.0451
average test loss: 0.044317, accuracy: 0.0447
case acc: 0.047912262
case acc: 0.040603135
case acc: 0.059663057
case acc: 0.054049484
case acc: 0.03369551
case acc: 0.03245277
top acc: 0.0236 ::: bot acc: 0.0726
top acc: 0.0171 ::: bot acc: 0.0658
top acc: 0.0236 ::: bot acc: 0.0988
top acc: 0.0303 ::: bot acc: 0.0792
top acc: 0.0134 ::: bot acc: 0.0632
top acc: 0.0259 ::: bot acc: 0.0562
current epoch: 38
train loss is 0.048144
average val loss: 0.032162, accuracy: 0.0318
average test loss: 0.030241, accuracy: 0.0306
case acc: 0.02694753
case acc: 0.022780906
case acc: 0.043087184
case acc: 0.035156228
case acc: 0.025298849
case acc: 0.030192738
top acc: 0.0257 ::: bot acc: 0.0403
top acc: 0.0134 ::: bot acc: 0.0411
top acc: 0.0181 ::: bot acc: 0.0768
top acc: 0.0134 ::: bot acc: 0.0594
top acc: 0.0291 ::: bot acc: 0.0417
top acc: 0.0436 ::: bot acc: 0.0385
current epoch: 39
train loss is 0.047828
average val loss: 0.032243, accuracy: 0.0323
average test loss: 0.030376, accuracy: 0.0302
case acc: 0.033244368
case acc: 0.02678759
case acc: 0.032502145
case acc: 0.01786787
case acc: 0.035134546
case acc: 0.035627518
top acc: 0.0650 ::: bot acc: 0.0094
top acc: 0.0496 ::: bot acc: 0.0082
top acc: 0.0496 ::: bot acc: 0.0386
top acc: 0.0229 ::: bot acc: 0.0265
top acc: 0.0608 ::: bot acc: 0.0166
top acc: 0.0641 ::: bot acc: 0.0209
current epoch: 40
train loss is 0.042234
average val loss: 0.042211, accuracy: 0.0421
average test loss: 0.041425, accuracy: 0.0412
case acc: 0.050246123
case acc: 0.045494355
case acc: 0.041423373
case acc: 0.025988799
case acc: 0.046798877
case acc: 0.03751655
top acc: 0.0848 ::: bot acc: 0.0206
top acc: 0.0721 ::: bot acc: 0.0191
top acc: 0.0778 ::: bot acc: 0.0151
top acc: 0.0447 ::: bot acc: 0.0116
top acc: 0.0782 ::: bot acc: 0.0166
top acc: 0.0673 ::: bot acc: 0.0202
current epoch: 41
train loss is 0.037378
average val loss: 0.054930, accuracy: 0.0552
average test loss: 0.054755, accuracy: 0.0549
case acc: 0.06081266
case acc: 0.063613996
case acc: 0.062268004
case acc: 0.043413278
case acc: 0.059298422
case acc: 0.03992509
top acc: 0.0954 ::: bot acc: 0.0311
top acc: 0.0905 ::: bot acc: 0.0368
top acc: 0.1058 ::: bot acc: 0.0213
top acc: 0.0658 ::: bot acc: 0.0214
top acc: 0.0928 ::: bot acc: 0.0247
top acc: 0.0710 ::: bot acc: 0.0199
current epoch: 42
train loss is 0.040197
average val loss: 0.067672, accuracy: 0.0678
average test loss: 0.067790, accuracy: 0.0681
case acc: 0.06548954
case acc: 0.077445604
case acc: 0.0853686
case acc: 0.06145656
case acc: 0.0714819
case acc: 0.0471346
top acc: 0.1001 ::: bot acc: 0.0358
top acc: 0.1043 ::: bot acc: 0.0506
top acc: 0.1301 ::: bot acc: 0.0420
top acc: 0.0853 ::: bot acc: 0.0365
top acc: 0.1057 ::: bot acc: 0.0354
top acc: 0.0811 ::: bot acc: 0.0211
current epoch: 43
train loss is 0.054779
average val loss: 0.037556, accuracy: 0.0379
average test loss: 0.036248, accuracy: 0.0357
case acc: 0.025657544
case acc: 0.037318088
case acc: 0.050843738
case acc: 0.030153269
case acc: 0.03998422
case acc: 0.030404162
top acc: 0.0519 ::: bot acc: 0.0131
top acc: 0.0634 ::: bot acc: 0.0120
top acc: 0.0917 ::: bot acc: 0.0154
top acc: 0.0502 ::: bot acc: 0.0129
top acc: 0.0684 ::: bot acc: 0.0157
top acc: 0.0480 ::: bot acc: 0.0340
current epoch: 44
train loss is 0.043879
average val loss: 0.029942, accuracy: 0.0301
average test loss: 0.027722, accuracy: 0.0275
case acc: 0.025173776
case acc: 0.022905543
case acc: 0.036660127
case acc: 0.01977524
case acc: 0.030468818
case acc: 0.030155685
top acc: 0.0283 ::: bot acc: 0.0364
top acc: 0.0417 ::: bot acc: 0.0125
top acc: 0.0685 ::: bot acc: 0.0196
top acc: 0.0331 ::: bot acc: 0.0163
top acc: 0.0520 ::: bot acc: 0.0203
top acc: 0.0443 ::: bot acc: 0.0377
current epoch: 45
train loss is 0.036706
average val loss: 0.028475, accuracy: 0.0286
average test loss: 0.025942, accuracy: 0.0263
case acc: 0.031160533
case acc: 0.019498114
case acc: 0.032362606
case acc: 0.018087558
case acc: 0.026481412
case acc: 0.030230744
top acc: 0.0219 ::: bot acc: 0.0485
top acc: 0.0268 ::: bot acc: 0.0269
top acc: 0.0485 ::: bot acc: 0.0396
top acc: 0.0199 ::: bot acc: 0.0295
top acc: 0.0418 ::: bot acc: 0.0289
top acc: 0.0461 ::: bot acc: 0.0358
current epoch: 46
train loss is 0.031870
average val loss: 0.030629, accuracy: 0.0306
average test loss: 0.028616, accuracy: 0.0290
case acc: 0.036453657
case acc: 0.023098338
case acc: 0.035626743
case acc: 0.02377036
case acc: 0.02516627
case acc: 0.030063927
top acc: 0.0202 ::: bot acc: 0.0572
top acc: 0.0130 ::: bot acc: 0.0418
top acc: 0.0265 ::: bot acc: 0.0616
top acc: 0.0082 ::: bot acc: 0.0450
top acc: 0.0303 ::: bot acc: 0.0404
top acc: 0.0428 ::: bot acc: 0.0391
current epoch: 47
train loss is 0.030223
average val loss: 0.036375, accuracy: 0.0363
average test loss: 0.034974, accuracy: 0.0355
case acc: 0.040164866
case acc: 0.031518057
case acc: 0.046382345
case acc: 0.036307346
case acc: 0.02794275
case acc: 0.03048975
top acc: 0.0206 ::: bot acc: 0.0625
top acc: 0.0121 ::: bot acc: 0.0548
top acc: 0.0179 ::: bot acc: 0.0818
top acc: 0.0143 ::: bot acc: 0.0607
top acc: 0.0187 ::: bot acc: 0.0521
top acc: 0.0346 ::: bot acc: 0.0473
current epoch: 48
train loss is 0.035774
average val loss: 0.037599, accuracy: 0.0373
average test loss: 0.036294, accuracy: 0.0367
case acc: 0.035633083
case acc: 0.03246646
case acc: 0.05103894
case acc: 0.041491322
case acc: 0.028906433
case acc: 0.03094835
top acc: 0.0202 ::: bot acc: 0.0559
top acc: 0.0125 ::: bot acc: 0.0560
top acc: 0.0189 ::: bot acc: 0.0883
top acc: 0.0187 ::: bot acc: 0.0662
top acc: 0.0165 ::: bot acc: 0.0546
top acc: 0.0313 ::: bot acc: 0.0506
current epoch: 49
train loss is 0.043601
average val loss: 0.028838, accuracy: 0.0285
average test loss: 0.026395, accuracy: 0.0262
case acc: 0.022839349
case acc: 0.019443892
case acc: 0.03552092
case acc: 0.022575883
case acc: 0.026082162
case acc: 0.030506007
top acc: 0.0426 ::: bot acc: 0.0217
top acc: 0.0256 ::: bot acc: 0.0281
top acc: 0.0268 ::: bot acc: 0.0613
top acc: 0.0087 ::: bot acc: 0.0430
top acc: 0.0400 ::: bot acc: 0.0307
top acc: 0.0494 ::: bot acc: 0.0326
current epoch: 50
train loss is 0.040718
average val loss: 0.034671, accuracy: 0.0347
average test loss: 0.033144, accuracy: 0.0329
case acc: 0.040165395
case acc: 0.030827295
case acc: 0.034136415
case acc: 0.019938178
case acc: 0.037705887
case acc: 0.034818955
top acc: 0.0741 ::: bot acc: 0.0119
top acc: 0.0556 ::: bot acc: 0.0083
top acc: 0.0599 ::: bot acc: 0.0281
top acc: 0.0336 ::: bot acc: 0.0157
top acc: 0.0649 ::: bot acc: 0.0159
top acc: 0.0627 ::: bot acc: 0.0213
