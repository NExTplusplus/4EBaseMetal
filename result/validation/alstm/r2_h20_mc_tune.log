
		{"drop_out": 0.4, "drop_out_mc": 0.05, "repeat_mc": 50, "hidden": 30, "embedding_size": 5, "batch": 512, "lag": 3}
LME_Co_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5328 5328 5328
1.7082474 -0.6288155 0.29835227 -0.25048217
Validation: 594 594 594
Testing: 774 774 774
pre-processing time: 0.00031375885009765625
the split date is 2010-07-01
train dropout: 0.4 test dropout: 0.05
net initializing with time: 0.09201312065124512
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.010699
average val loss: 0.005627, accuracy: 0.0934
average test loss: 0.008083, accuracy: 0.1157
case acc: 0.1653523
case acc: 0.08545266
case acc: 0.12801081
case acc: 0.10688835
case acc: 0.13198744
case acc: 0.07663259
top acc: 0.1337 ::: bot acc: 0.2024
top acc: 0.1461 ::: bot acc: 0.0231
top acc: 0.0583 ::: bot acc: 0.1941
top acc: 0.0653 ::: bot acc: 0.1447
top acc: 0.1012 ::: bot acc: 0.1609
top acc: 0.0332 ::: bot acc: 0.1247
current epoch: 2
train loss is 0.009046
average val loss: 0.006360, accuracy: 0.0831
average test loss: 0.004505, accuracy: 0.0652
case acc: 0.037863523
case acc: 0.19729294
case acc: 0.051447637
case acc: 0.031523336
case acc: 0.02595252
case acc: 0.047170345
top acc: 0.0091 ::: bot acc: 0.0742
top acc: 0.2604 ::: bot acc: 0.1293
top acc: 0.0737 ::: bot acc: 0.0654
top acc: 0.0558 ::: bot acc: 0.0239
top acc: 0.0189 ::: bot acc: 0.0429
top acc: 0.0809 ::: bot acc: 0.0192
current epoch: 3
train loss is 0.010133
average val loss: 0.015827, accuracy: 0.1565
average test loss: 0.010884, accuracy: 0.1223
case acc: 0.06498766
case acc: 0.2780029
case acc: 0.09708399
case acc: 0.1016309
case acc: 0.07452731
case acc: 0.11749966
top acc: 0.0900 ::: bot acc: 0.0403
top acc: 0.3416 ::: bot acc: 0.2101
top acc: 0.1675 ::: bot acc: 0.0333
top acc: 0.1439 ::: bot acc: 0.0627
top acc: 0.1057 ::: bot acc: 0.0452
top acc: 0.1611 ::: bot acc: 0.0694
current epoch: 4
train loss is 0.013608
average val loss: 0.008151, accuracy: 0.1019
average test loss: 0.005248, accuracy: 0.0713
case acc: 0.027128091
case acc: 0.2096355
case acc: 0.058052693
case acc: 0.046544403
case acc: 0.027619783
case acc: 0.058793325
top acc: 0.0366 ::: bot acc: 0.0336
top acc: 0.2731 ::: bot acc: 0.1416
top acc: 0.1111 ::: bot acc: 0.0296
top acc: 0.0858 ::: bot acc: 0.0141
top acc: 0.0529 ::: bot acc: 0.0109
top acc: 0.0994 ::: bot acc: 0.0172
current epoch: 5
train loss is 0.011512
average val loss: 0.003203, accuracy: 0.0665
average test loss: 0.004214, accuracy: 0.0806
case acc: 0.10296069
case acc: 0.08876461
case acc: 0.08407416
case acc: 0.068674706
case acc: 0.08190427
case acc: 0.05708449
top acc: 0.0713 ::: bot acc: 0.1408
top acc: 0.1501 ::: bot acc: 0.0244
top acc: 0.0312 ::: bot acc: 0.1416
top acc: 0.0272 ::: bot acc: 0.1074
top acc: 0.0509 ::: bot acc: 0.1116
top acc: 0.0150 ::: bot acc: 0.1051
current epoch: 6
train loss is 0.005762
average val loss: 0.002809, accuracy: 0.0575
average test loss: 0.003159, accuracy: 0.0665
case acc: 0.080552906
case acc: 0.096938
case acc: 0.07323939
case acc: 0.050927997
case acc: 0.05522379
case acc: 0.042082455
top acc: 0.0494 ::: bot acc: 0.1182
top acc: 0.1595 ::: bot acc: 0.0305
top acc: 0.0316 ::: bot acc: 0.1250
top acc: 0.0139 ::: bot acc: 0.0873
top acc: 0.0276 ::: bot acc: 0.0830
top acc: 0.0125 ::: bot acc: 0.0839
current epoch: 7
train loss is 0.004840
average val loss: 0.003309, accuracy: 0.0591
average test loss: 0.002526, accuracy: 0.0532
case acc: 0.045144826
case acc: 0.12115893
case acc: 0.05742725
case acc: 0.033898048
case acc: 0.026534693
case acc: 0.03491661
top acc: 0.0144 ::: bot acc: 0.0822
top acc: 0.1845 ::: bot acc: 0.0534
top acc: 0.0466 ::: bot acc: 0.0939
top acc: 0.0266 ::: bot acc: 0.0549
top acc: 0.0184 ::: bot acc: 0.0445
top acc: 0.0431 ::: bot acc: 0.0513
current epoch: 8
train loss is 0.005223
average val loss: 0.002874, accuracy: 0.0554
average test loss: 0.002363, accuracy: 0.0525
case acc: 0.048936836
case acc: 0.10677274
case acc: 0.06070234
case acc: 0.035804123
case acc: 0.028035268
case acc: 0.03495562
top acc: 0.0181 ::: bot acc: 0.0863
top acc: 0.1705 ::: bot acc: 0.0387
top acc: 0.0406 ::: bot acc: 0.1014
top acc: 0.0196 ::: bot acc: 0.0618
top acc: 0.0174 ::: bot acc: 0.0473
top acc: 0.0357 ::: bot acc: 0.0586
current epoch: 9
train loss is 0.004898
average val loss: 0.002462, accuracy: 0.0519
average test loss: 0.002284, accuracy: 0.0531
case acc: 0.055226456
case acc: 0.091575034
case acc: 0.06567018
case acc: 0.0396629
case acc: 0.030649966
case acc: 0.03595645
top acc: 0.0239 ::: bot acc: 0.0930
top acc: 0.1537 ::: bot acc: 0.0265
top acc: 0.0357 ::: bot acc: 0.1115
top acc: 0.0138 ::: bot acc: 0.0701
top acc: 0.0165 ::: bot acc: 0.0517
top acc: 0.0266 ::: bot acc: 0.0670
current epoch: 10
train loss is 0.004320
average val loss: 0.002266, accuracy: 0.0503
average test loss: 0.002176, accuracy: 0.0520
case acc: 0.054638874
case acc: 0.08354426
case acc: 0.06728791
case acc: 0.04059611
case acc: 0.029315172
case acc: 0.036544275
top acc: 0.0232 ::: bot acc: 0.0920
top acc: 0.1443 ::: bot acc: 0.0211
top acc: 0.0346 ::: bot acc: 0.1142
top acc: 0.0133 ::: bot acc: 0.0720
top acc: 0.0165 ::: bot acc: 0.0496
top acc: 0.0253 ::: bot acc: 0.0690
current epoch: 11
train loss is 0.004021
average val loss: 0.002279, accuracy: 0.0505
average test loss: 0.001997, accuracy: 0.0487
case acc: 0.046780825
case acc: 0.083503984
case acc: 0.06434909
case acc: 0.03748323
case acc: 0.025012583
case acc: 0.035289753
top acc: 0.0161 ::: bot acc: 0.0842
top acc: 0.1441 ::: bot acc: 0.0212
top acc: 0.0368 ::: bot acc: 0.1089
top acc: 0.0166 ::: bot acc: 0.0658
top acc: 0.0211 ::: bot acc: 0.0410
top acc: 0.0315 ::: bot acc: 0.0628
current epoch: 12
train loss is 0.003922
average val loss: 0.002335, accuracy: 0.0512
average test loss: 0.001860, accuracy: 0.0462
case acc: 0.03969671
case acc: 0.08316447
case acc: 0.061836
case acc: 0.035079353
case acc: 0.02239771
case acc: 0.034958337
top acc: 0.0104 ::: bot acc: 0.0763
top acc: 0.1438 ::: bot acc: 0.0209
top acc: 0.0392 ::: bot acc: 0.1039
top acc: 0.0216 ::: bot acc: 0.0596
top acc: 0.0278 ::: bot acc: 0.0333
top acc: 0.0373 ::: bot acc: 0.0572
current epoch: 13
train loss is 0.003802
average val loss: 0.002156, accuracy: 0.0494
average test loss: 0.001795, accuracy: 0.0458
case acc: 0.04059779
case acc: 0.07665921
case acc: 0.06319269
case acc: 0.03603331
case acc: 0.022897616
case acc: 0.035169043
top acc: 0.0112 ::: bot acc: 0.0773
top acc: 0.1355 ::: bot acc: 0.0180
top acc: 0.0378 ::: bot acc: 0.1065
top acc: 0.0193 ::: bot acc: 0.0622
top acc: 0.0266 ::: bot acc: 0.0349
top acc: 0.0342 ::: bot acc: 0.0603
current epoch: 14
train loss is 0.003645
average val loss: 0.002068, accuracy: 0.0484
average test loss: 0.001720, accuracy: 0.0448
case acc: 0.03920573
case acc: 0.07270734
case acc: 0.06331018
case acc: 0.03575913
case acc: 0.022585269
case acc: 0.035106096
top acc: 0.0107 ::: bot acc: 0.0751
top acc: 0.1302 ::: bot acc: 0.0172
top acc: 0.0377 ::: bot acc: 0.1068
top acc: 0.0193 ::: bot acc: 0.0617
top acc: 0.0275 ::: bot acc: 0.0339
top acc: 0.0339 ::: bot acc: 0.0606
current epoch: 15
train loss is 0.003494
average val loss: 0.001991, accuracy: 0.0475
average test loss: 0.001652, accuracy: 0.0440
case acc: 0.03770711
case acc: 0.06956022
case acc: 0.063343935
case acc: 0.03579916
case acc: 0.022429164
case acc: 0.034861684
top acc: 0.0095 ::: bot acc: 0.0734
top acc: 0.1252 ::: bot acc: 0.0175
top acc: 0.0379 ::: bot acc: 0.1066
top acc: 0.0201 ::: bot acc: 0.0614
top acc: 0.0282 ::: bot acc: 0.0330
top acc: 0.0334 ::: bot acc: 0.0605
current epoch: 16
train loss is 0.003375
average val loss: 0.002019, accuracy: 0.0480
average test loss: 0.001570, accuracy: 0.0426
case acc: 0.03394358
case acc: 0.06879039
case acc: 0.061533276
case acc: 0.034375053
case acc: 0.022098452
case acc: 0.034759823
top acc: 0.0080 ::: bot acc: 0.0688
top acc: 0.1238 ::: bot acc: 0.0180
top acc: 0.0401 ::: bot acc: 0.1030
top acc: 0.0234 ::: bot acc: 0.0576
top acc: 0.0319 ::: bot acc: 0.0294
top acc: 0.0367 ::: bot acc: 0.0576
current epoch: 17
train loss is 0.003339
average val loss: 0.002037, accuracy: 0.0483
average test loss: 0.001511, accuracy: 0.0417
case acc: 0.03151076
case acc: 0.067963004
case acc: 0.06032012
case acc: 0.033622105
case acc: 0.022375735
case acc: 0.034595743
top acc: 0.0081 ::: bot acc: 0.0648
top acc: 0.1223 ::: bot acc: 0.0185
top acc: 0.0420 ::: bot acc: 0.1002
top acc: 0.0264 ::: bot acc: 0.0547
top acc: 0.0342 ::: bot acc: 0.0273
top acc: 0.0386 ::: bot acc: 0.0552
current epoch: 18
train loss is 0.003267
average val loss: 0.001985, accuracy: 0.0477
average test loss: 0.001463, accuracy: 0.0411
case acc: 0.030535007
case acc: 0.06580072
case acc: 0.059971575
case acc: 0.03353116
case acc: 0.022172242
case acc: 0.034741968
top acc: 0.0081 ::: bot acc: 0.0635
top acc: 0.1183 ::: bot acc: 0.0200
top acc: 0.0425 ::: bot acc: 0.0994
top acc: 0.0269 ::: bot acc: 0.0542
top acc: 0.0338 ::: bot acc: 0.0274
top acc: 0.0383 ::: bot acc: 0.0559
current epoch: 19
train loss is 0.003176
average val loss: 0.001993, accuracy: 0.0478
average test loss: 0.001420, accuracy: 0.0405
case acc: 0.029119592
case acc: 0.064723715
case acc: 0.05911285
case acc: 0.033122648
case acc: 0.022347063
case acc: 0.03477714
top acc: 0.0092 ::: bot acc: 0.0609
top acc: 0.1163 ::: bot acc: 0.0206
top acc: 0.0439 ::: bot acc: 0.0973
top acc: 0.0288 ::: bot acc: 0.0527
top acc: 0.0348 ::: bot acc: 0.0267
top acc: 0.0389 ::: bot acc: 0.0553
current epoch: 20
train loss is 0.003154
average val loss: 0.001960, accuracy: 0.0473
average test loss: 0.001383, accuracy: 0.0401
case acc: 0.028559534
case acc: 0.06329925
case acc: 0.058671758
case acc: 0.032979272
case acc: 0.022273479
case acc: 0.03469769
top acc: 0.0103 ::: bot acc: 0.0593
top acc: 0.1135 ::: bot acc: 0.0223
top acc: 0.0450 ::: bot acc: 0.0961
top acc: 0.0290 ::: bot acc: 0.0521
top acc: 0.0339 ::: bot acc: 0.0275
top acc: 0.0379 ::: bot acc: 0.0562
current epoch: 21
train loss is 0.003106
average val loss: 0.002074, accuracy: 0.0490
average test loss: 0.001346, accuracy: 0.0394
case acc: 0.0264983
case acc: 0.0639023
case acc: 0.056586932
case acc: 0.032071494
case acc: 0.022649894
case acc: 0.034786712
top acc: 0.0146 ::: bot acc: 0.0540
top acc: 0.1148 ::: bot acc: 0.0214
top acc: 0.0492 ::: bot acc: 0.0910
top acc: 0.0338 ::: bot acc: 0.0473
top acc: 0.0370 ::: bot acc: 0.0245
top acc: 0.0413 ::: bot acc: 0.0529
current epoch: 22
train loss is 0.003105
average val loss: 0.002173, accuracy: 0.0504
average test loss: 0.001326, accuracy: 0.0391
case acc: 0.025427867
case acc: 0.06434815
case acc: 0.05522144
case acc: 0.03161293
case acc: 0.022983808
case acc: 0.035024107
top acc: 0.0188 ::: bot acc: 0.0497
top acc: 0.1158 ::: bot acc: 0.0210
top acc: 0.0534 ::: bot acc: 0.0867
top acc: 0.0370 ::: bot acc: 0.0440
top acc: 0.0392 ::: bot acc: 0.0223
top acc: 0.0441 ::: bot acc: 0.0506
current epoch: 23
train loss is 0.003106
average val loss: 0.002333, accuracy: 0.0527
average test loss: 0.001325, accuracy: 0.0391
case acc: 0.024767553
case acc: 0.06558254
case acc: 0.053705156
case acc: 0.031291902
case acc: 0.023859402
case acc: 0.035465587
top acc: 0.0243 ::: bot acc: 0.0444
top acc: 0.1180 ::: bot acc: 0.0204
top acc: 0.0590 ::: bot acc: 0.0810
top acc: 0.0420 ::: bot acc: 0.0391
top acc: 0.0427 ::: bot acc: 0.0186
top acc: 0.0474 ::: bot acc: 0.0471
current epoch: 24
train loss is 0.003101
average val loss: 0.002285, accuracy: 0.0520
average test loss: 0.001296, accuracy: 0.0387
case acc: 0.024830276
case acc: 0.06414186
case acc: 0.053572387
case acc: 0.031291466
case acc: 0.023476034
case acc: 0.035132635
top acc: 0.0245 ::: bot acc: 0.0443
top acc: 0.1151 ::: bot acc: 0.0215
top acc: 0.0592 ::: bot acc: 0.0808
top acc: 0.0417 ::: bot acc: 0.0393
top acc: 0.0416 ::: bot acc: 0.0196
top acc: 0.0459 ::: bot acc: 0.0484
current epoch: 25
train loss is 0.003072
average val loss: 0.002265, accuracy: 0.0518
average test loss: 0.001274, accuracy: 0.0385
case acc: 0.024879038
case acc: 0.06292769
case acc: 0.053299
case acc: 0.03147211
case acc: 0.023427423
case acc: 0.035131406
top acc: 0.0252 ::: bot acc: 0.0438
top acc: 0.1127 ::: bot acc: 0.0227
top acc: 0.0600 ::: bot acc: 0.0797
top acc: 0.0421 ::: bot acc: 0.0393
top acc: 0.0412 ::: bot acc: 0.0200
top acc: 0.0454 ::: bot acc: 0.0490
current epoch: 26
train loss is 0.003033
average val loss: 0.002320, accuracy: 0.0525
average test loss: 0.001268, accuracy: 0.0385
case acc: 0.02500139
case acc: 0.06271807
case acc: 0.052985605
case acc: 0.031314798
case acc: 0.02378249
case acc: 0.035199832
top acc: 0.0273 ::: bot acc: 0.0415
top acc: 0.1125 ::: bot acc: 0.0227
top acc: 0.0626 ::: bot acc: 0.0775
top acc: 0.0439 ::: bot acc: 0.0373
top acc: 0.0427 ::: bot acc: 0.0188
top acc: 0.0460 ::: bot acc: 0.0485
current epoch: 27
train loss is 0.003030
average val loss: 0.002269, accuracy: 0.0517
average test loss: 0.001245, accuracy: 0.0382
case acc: 0.024954023
case acc: 0.06140534
case acc: 0.05296961
case acc: 0.031386364
case acc: 0.023626491
case acc: 0.035056394
top acc: 0.0270 ::: bot acc: 0.0418
top acc: 0.1095 ::: bot acc: 0.0245
top acc: 0.0626 ::: bot acc: 0.0776
top acc: 0.0432 ::: bot acc: 0.0382
top acc: 0.0418 ::: bot acc: 0.0199
top acc: 0.0446 ::: bot acc: 0.0498
current epoch: 28
train loss is 0.002984
average val loss: 0.002226, accuracy: 0.0511
average test loss: 0.001223, accuracy: 0.0379
case acc: 0.024884595
case acc: 0.060244028
case acc: 0.053146604
case acc: 0.031301714
case acc: 0.02323769
case acc: 0.034822475
top acc: 0.0268 ::: bot acc: 0.0419
top acc: 0.1069 ::: bot acc: 0.0264
top acc: 0.0627 ::: bot acc: 0.0778
top acc: 0.0425 ::: bot acc: 0.0386
top acc: 0.0408 ::: bot acc: 0.0202
top acc: 0.0430 ::: bot acc: 0.0510
current epoch: 29
train loss is 0.002947
average val loss: 0.002263, accuracy: 0.0516
average test loss: 0.001218, accuracy: 0.0379
case acc: 0.024937881
case acc: 0.06003538
case acc: 0.052758556
case acc: 0.031302646
case acc: 0.02360172
case acc: 0.034911495
top acc: 0.0280 ::: bot acc: 0.0405
top acc: 0.1063 ::: bot acc: 0.0270
top acc: 0.0643 ::: bot acc: 0.0758
top acc: 0.0439 ::: bot acc: 0.0372
top acc: 0.0421 ::: bot acc: 0.0194
top acc: 0.0438 ::: bot acc: 0.0505
current epoch: 30
train loss is 0.002933
average val loss: 0.002267, accuracy: 0.0517
average test loss: 0.001211, accuracy: 0.0379
case acc: 0.025016217
case acc: 0.059552573
case acc: 0.052738987
case acc: 0.031221537
case acc: 0.023714326
case acc: 0.035099342
top acc: 0.0287 ::: bot acc: 0.0399
top acc: 0.1051 ::: bot acc: 0.0281
top acc: 0.0650 ::: bot acc: 0.0754
top acc: 0.0443 ::: bot acc: 0.0366
top acc: 0.0424 ::: bot acc: 0.0189
top acc: 0.0442 ::: bot acc: 0.0505
current epoch: 31
train loss is 0.002927
average val loss: 0.002262, accuracy: 0.0516
average test loss: 0.001200, accuracy: 0.0378
case acc: 0.02510222
case acc: 0.058951642
case acc: 0.052604105
case acc: 0.03139876
case acc: 0.023748217
case acc: 0.034980435
top acc: 0.0290 ::: bot acc: 0.0398
top acc: 0.1036 ::: bot acc: 0.0290
top acc: 0.0658 ::: bot acc: 0.0746
top acc: 0.0453 ::: bot acc: 0.0361
top acc: 0.0424 ::: bot acc: 0.0189
top acc: 0.0441 ::: bot acc: 0.0503
current epoch: 32
train loss is 0.002887
average val loss: 0.002220, accuracy: 0.0509
average test loss: 0.001184, accuracy: 0.0376
case acc: 0.024975378
case acc: 0.058139425
case acc: 0.052630913
case acc: 0.031270888
case acc: 0.023447419
case acc: 0.034842983
top acc: 0.0283 ::: bot acc: 0.0405
top acc: 0.1014 ::: bot acc: 0.0311
top acc: 0.0654 ::: bot acc: 0.0749
top acc: 0.0446 ::: bot acc: 0.0367
top acc: 0.0413 ::: bot acc: 0.0198
top acc: 0.0430 ::: bot acc: 0.0513
current epoch: 33
train loss is 0.002865
average val loss: 0.002173, accuracy: 0.0502
average test loss: 0.001170, accuracy: 0.0374
case acc: 0.0249272
case acc: 0.05733887
case acc: 0.052756783
case acc: 0.03144806
case acc: 0.023098089
case acc: 0.034786433
top acc: 0.0272 ::: bot acc: 0.0416
top acc: 0.0991 ::: bot acc: 0.0333
top acc: 0.0647 ::: bot acc: 0.0755
top acc: 0.0439 ::: bot acc: 0.0376
top acc: 0.0401 ::: bot acc: 0.0210
top acc: 0.0419 ::: bot acc: 0.0524
current epoch: 34
train loss is 0.002826
average val loss: 0.002099, accuracy: 0.0490
average test loss: 0.001157, accuracy: 0.0372
case acc: 0.024715224
case acc: 0.05647932
case acc: 0.05299236
case acc: 0.031381756
case acc: 0.022850798
case acc: 0.034741785
top acc: 0.0252 ::: bot acc: 0.0434
top acc: 0.0963 ::: bot acc: 0.0364
top acc: 0.0632 ::: bot acc: 0.0771
top acc: 0.0424 ::: bot acc: 0.0390
top acc: 0.0383 ::: bot acc: 0.0232
top acc: 0.0401 ::: bot acc: 0.0543
current epoch: 35
train loss is 0.002786
average val loss: 0.002139, accuracy: 0.0496
average test loss: 0.001153, accuracy: 0.0372
case acc: 0.024809485
case acc: 0.056388944
case acc: 0.05277369
case acc: 0.031361632
case acc: 0.022996042
case acc: 0.03468191
top acc: 0.0265 ::: bot acc: 0.0422
top acc: 0.0961 ::: bot acc: 0.0362
top acc: 0.0648 ::: bot acc: 0.0755
top acc: 0.0438 ::: bot acc: 0.0375
top acc: 0.0393 ::: bot acc: 0.0218
top acc: 0.0411 ::: bot acc: 0.0532
current epoch: 36
train loss is 0.002804
average val loss: 0.002180, accuracy: 0.0502
average test loss: 0.001155, accuracy: 0.0372
case acc: 0.025000801
case acc: 0.05654788
case acc: 0.052489787
case acc: 0.03131343
case acc: 0.023274994
case acc: 0.034856975
top acc: 0.0278 ::: bot acc: 0.0411
top acc: 0.0969 ::: bot acc: 0.0358
top acc: 0.0662 ::: bot acc: 0.0739
top acc: 0.0451 ::: bot acc: 0.0362
top acc: 0.0404 ::: bot acc: 0.0208
top acc: 0.0423 ::: bot acc: 0.0522
current epoch: 37
train loss is 0.002791
average val loss: 0.002142, accuracy: 0.0496
average test loss: 0.001145, accuracy: 0.0371
case acc: 0.024832932
case acc: 0.05610906
case acc: 0.05264045
case acc: 0.031205798
case acc: 0.023058444
case acc: 0.034721028
top acc: 0.0264 ::: bot acc: 0.0424
top acc: 0.0949 ::: bot acc: 0.0375
top acc: 0.0658 ::: bot acc: 0.0746
top acc: 0.0443 ::: bot acc: 0.0367
top acc: 0.0393 ::: bot acc: 0.0219
top acc: 0.0413 ::: bot acc: 0.0530
current epoch: 38
train loss is 0.002771
average val loss: 0.002084, accuracy: 0.0487
average test loss: 0.001139, accuracy: 0.0370
case acc: 0.024738466
case acc: 0.05555966
case acc: 0.052843355
case acc: 0.031269733
case acc: 0.022762464
case acc: 0.03484416
top acc: 0.0249 ::: bot acc: 0.0438
top acc: 0.0929 ::: bot acc: 0.0398
top acc: 0.0648 ::: bot acc: 0.0756
top acc: 0.0432 ::: bot acc: 0.0378
top acc: 0.0379 ::: bot acc: 0.0233
top acc: 0.0398 ::: bot acc: 0.0549
current epoch: 39
train loss is 0.002750
average val loss: 0.002042, accuracy: 0.0480
average test loss: 0.001130, accuracy: 0.0369
case acc: 0.024756845
case acc: 0.055075977
case acc: 0.052861005
case acc: 0.03130855
case acc: 0.022452246
case acc: 0.034735035
top acc: 0.0238 ::: bot acc: 0.0448
top acc: 0.0910 ::: bot acc: 0.0416
top acc: 0.0641 ::: bot acc: 0.0763
top acc: 0.0420 ::: bot acc: 0.0391
top acc: 0.0363 ::: bot acc: 0.0247
top acc: 0.0378 ::: bot acc: 0.0566
current epoch: 40
train loss is 0.002728
average val loss: 0.002123, accuracy: 0.0492
average test loss: 0.001135, accuracy: 0.0370
case acc: 0.024791883
case acc: 0.055465374
case acc: 0.052484322
case acc: 0.031264357
case acc: 0.02290482
case acc: 0.034864943
top acc: 0.0258 ::: bot acc: 0.0430
top acc: 0.0927 ::: bot acc: 0.0399
top acc: 0.0666 ::: bot acc: 0.0737
top acc: 0.0445 ::: bot acc: 0.0367
top acc: 0.0384 ::: bot acc: 0.0230
top acc: 0.0400 ::: bot acc: 0.0548
current epoch: 41
train loss is 0.002740
average val loss: 0.002144, accuracy: 0.0495
average test loss: 0.001135, accuracy: 0.0370
case acc: 0.02486022
case acc: 0.05543444
case acc: 0.05245859
case acc: 0.03138908
case acc: 0.023032105
case acc: 0.034858197
top acc: 0.0263 ::: bot acc: 0.0426
top acc: 0.0927 ::: bot acc: 0.0398
top acc: 0.0677 ::: bot acc: 0.0728
top acc: 0.0452 ::: bot acc: 0.0363
top acc: 0.0390 ::: bot acc: 0.0225
top acc: 0.0401 ::: bot acc: 0.0544
current epoch: 42
train loss is 0.002736
average val loss: 0.002158, accuracy: 0.0496
average test loss: 0.001131, accuracy: 0.0369
case acc: 0.024814272
case acc: 0.055380475
case acc: 0.0522676
case acc: 0.031246407
case acc: 0.02297204
case acc: 0.03489248
top acc: 0.0265 ::: bot acc: 0.0423
top acc: 0.0924 ::: bot acc: 0.0401
top acc: 0.0681 ::: bot acc: 0.0721
top acc: 0.0453 ::: bot acc: 0.0359
top acc: 0.0393 ::: bot acc: 0.0218
top acc: 0.0403 ::: bot acc: 0.0544
current epoch: 43
train loss is 0.002723
average val loss: 0.002130, accuracy: 0.0492
average test loss: 0.001128, accuracy: 0.0369
case acc: 0.024821639
case acc: 0.05497902
case acc: 0.052412674
case acc: 0.031354025
case acc: 0.022938011
case acc: 0.03477837
top acc: 0.0255 ::: bot acc: 0.0434
top acc: 0.0911 ::: bot acc: 0.0413
top acc: 0.0677 ::: bot acc: 0.0727
top acc: 0.0447 ::: bot acc: 0.0367
top acc: 0.0391 ::: bot acc: 0.0221
top acc: 0.0394 ::: bot acc: 0.0552
current epoch: 44
train loss is 0.002707
average val loss: 0.002090, accuracy: 0.0486
average test loss: 0.001124, accuracy: 0.0368
case acc: 0.024888808
case acc: 0.054751676
case acc: 0.05248645
case acc: 0.031243926
case acc: 0.022750119
case acc: 0.034748923
top acc: 0.0241 ::: bot acc: 0.0447
top acc: 0.0899 ::: bot acc: 0.0428
top acc: 0.0669 ::: bot acc: 0.0735
top acc: 0.0437 ::: bot acc: 0.0374
top acc: 0.0382 ::: bot acc: 0.0230
top acc: 0.0382 ::: bot acc: 0.0563
current epoch: 45
train loss is 0.002706
average val loss: 0.002092, accuracy: 0.0486
average test loss: 0.001123, accuracy: 0.0368
case acc: 0.024874607
case acc: 0.05467596
case acc: 0.052593432
case acc: 0.0312726
case acc: 0.022815999
case acc: 0.03479774
top acc: 0.0239 ::: bot acc: 0.0449
top acc: 0.0892 ::: bot acc: 0.0436
top acc: 0.0672 ::: bot acc: 0.0734
top acc: 0.0440 ::: bot acc: 0.0372
top acc: 0.0383 ::: bot acc: 0.0228
top acc: 0.0385 ::: bot acc: 0.0561
current epoch: 46
train loss is 0.002693
average val loss: 0.002098, accuracy: 0.0487
average test loss: 0.001121, accuracy: 0.0368
case acc: 0.02487362
case acc: 0.054463495
case acc: 0.052450754
case acc: 0.031303894
case acc: 0.022884332
case acc: 0.034801755
top acc: 0.0241 ::: bot acc: 0.0449
top acc: 0.0889 ::: bot acc: 0.0437
top acc: 0.0675 ::: bot acc: 0.0730
top acc: 0.0442 ::: bot acc: 0.0370
top acc: 0.0388 ::: bot acc: 0.0223
top acc: 0.0388 ::: bot acc: 0.0560
current epoch: 47
train loss is 0.002687
average val loss: 0.002070, accuracy: 0.0483
average test loss: 0.001117, accuracy: 0.0368
case acc: 0.024939533
case acc: 0.054223794
case acc: 0.05254834
case acc: 0.03125196
case acc: 0.022784114
case acc: 0.034812186
top acc: 0.0229 ::: bot acc: 0.0458
top acc: 0.0876 ::: bot acc: 0.0449
top acc: 0.0669 ::: bot acc: 0.0735
top acc: 0.0436 ::: bot acc: 0.0374
top acc: 0.0385 ::: bot acc: 0.0226
top acc: 0.0382 ::: bot acc: 0.0564
current epoch: 48
train loss is 0.002677
average val loss: 0.001995, accuracy: 0.0472
average test loss: 0.001117, accuracy: 0.0368
case acc: 0.025289048
case acc: 0.053667344
case acc: 0.052863788
case acc: 0.031361695
case acc: 0.022519315
case acc: 0.034917064
top acc: 0.0207 ::: bot acc: 0.0482
top acc: 0.0851 ::: bot acc: 0.0476
top acc: 0.0648 ::: bot acc: 0.0759
top acc: 0.0419 ::: bot acc: 0.0394
top acc: 0.0367 ::: bot acc: 0.0245
top acc: 0.0364 ::: bot acc: 0.0583
current epoch: 49
train loss is 0.002656
average val loss: 0.001947, accuracy: 0.0465
average test loss: 0.001115, accuracy: 0.0368
case acc: 0.025560563
case acc: 0.05328585
case acc: 0.05297268
case acc: 0.031460237
case acc: 0.022398893
case acc: 0.0349491
top acc: 0.0189 ::: bot acc: 0.0499
top acc: 0.0832 ::: bot acc: 0.0493
top acc: 0.0634 ::: bot acc: 0.0770
top acc: 0.0407 ::: bot acc: 0.0407
top acc: 0.0356 ::: bot acc: 0.0257
top acc: 0.0353 ::: bot acc: 0.0593
current epoch: 50
train loss is 0.002651
average val loss: 0.001942, accuracy: 0.0464
average test loss: 0.001115, accuracy: 0.0368
case acc: 0.025700266
case acc: 0.0532264
case acc: 0.05311614
case acc: 0.031344157
case acc: 0.022346133
case acc: 0.034895986
top acc: 0.0184 ::: bot acc: 0.0504
top acc: 0.0827 ::: bot acc: 0.0499
top acc: 0.0633 ::: bot acc: 0.0773
top acc: 0.0407 ::: bot acc: 0.0404
top acc: 0.0355 ::: bot acc: 0.0256
top acc: 0.0358 ::: bot acc: 0.0588
LME_Co_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5352 5352 5352
1.7082474 -0.6288155 0.29835227 -0.25048217
Validation: 600 600 600
Testing: 744 744 744
pre-processing time: 0.00024819374084472656
the split date is 2011-01-01
train dropout: 0.4 test dropout: 0.05
net initializing with time: 0.0025517940521240234
preparing training and testing date with time: 4.76837158203125e-07
current epoch: 1
train loss is 0.010504
average val loss: 0.006184, accuracy: 0.1001
average test loss: 0.006730, accuracy: 0.1045
case acc: 0.12364954
case acc: 0.09732934
case acc: 0.11697119
case acc: 0.11000103
case acc: 0.10830078
case acc: 0.070725225
top acc: 0.0636 ::: bot acc: 0.1880
top acc: 0.1454 ::: bot acc: 0.0523
top acc: 0.0596 ::: bot acc: 0.1710
top acc: 0.0719 ::: bot acc: 0.1461
top acc: 0.0365 ::: bot acc: 0.1718
top acc: 0.0577 ::: bot acc: 0.1058
current epoch: 2
train loss is 0.008519
average val loss: 0.004611, accuracy: 0.0643
average test loss: 0.005315, accuracy: 0.0751
case acc: 0.04724309
case acc: 0.19665521
case acc: 0.04938995
case acc: 0.0302314
case acc: 0.05532405
case acc: 0.0720541
top acc: 0.0504 ::: bot acc: 0.0741
top acc: 0.2451 ::: bot acc: 0.1512
top acc: 0.0790 ::: bot acc: 0.0577
top acc: 0.0439 ::: bot acc: 0.0399
top acc: 0.0790 ::: bot acc: 0.0683
top acc: 0.1547 ::: bot acc: 0.0128
current epoch: 3
train loss is 0.009880
average val loss: 0.010722, accuracy: 0.1213
average test loss: 0.011537, accuracy: 0.1240
case acc: 0.080067
case acc: 0.2694231
case acc: 0.09291021
case acc: 0.07948252
case acc: 0.08369083
case acc: 0.13858181
top acc: 0.1378 ::: bot acc: 0.0202
top acc: 0.3177 ::: bot acc: 0.2242
top acc: 0.1648 ::: bot acc: 0.0360
top acc: 0.1234 ::: bot acc: 0.0409
top acc: 0.1600 ::: bot acc: 0.0212
top acc: 0.2276 ::: bot acc: 0.0666
current epoch: 4
train loss is 0.012713
average val loss: 0.005336, accuracy: 0.0718
average test loss: 0.006073, accuracy: 0.0809
case acc: 0.052713256
case acc: 0.20496204
case acc: 0.05443071
case acc: 0.034707993
case acc: 0.056169413
case acc: 0.082274325
top acc: 0.0885 ::: bot acc: 0.0365
top acc: 0.2536 ::: bot acc: 0.1594
top acc: 0.1121 ::: bot acc: 0.0265
top acc: 0.0683 ::: bot acc: 0.0171
top acc: 0.1111 ::: bot acc: 0.0356
top acc: 0.1694 ::: bot acc: 0.0145
current epoch: 5
train loss is 0.010863
average val loss: 0.003579, accuracy: 0.0736
average test loss: 0.004186, accuracy: 0.0804
case acc: 0.074092515
case acc: 0.089033924
case acc: 0.08543243
case acc: 0.086621754
case acc: 0.07965619
case acc: 0.06753985
top acc: 0.0169 ::: bot acc: 0.1375
top acc: 0.1373 ::: bot acc: 0.0440
top acc: 0.0438 ::: bot acc: 0.1318
top acc: 0.0521 ::: bot acc: 0.1215
top acc: 0.0336 ::: bot acc: 0.1309
top acc: 0.0632 ::: bot acc: 0.0979
current epoch: 6
train loss is 0.005578
average val loss: 0.002999, accuracy: 0.0651
average test loss: 0.003636, accuracy: 0.0741
case acc: 0.06456337
case acc: 0.08834927
case acc: 0.08029636
case acc: 0.078157045
case acc: 0.06936122
case acc: 0.0640272
top acc: 0.0140 ::: bot acc: 0.1249
top acc: 0.1366 ::: bot acc: 0.0436
top acc: 0.0419 ::: bot acc: 0.1249
top acc: 0.0453 ::: bot acc: 0.1124
top acc: 0.0406 ::: bot acc: 0.1121
top acc: 0.0730 ::: bot acc: 0.0873
current epoch: 7
train loss is 0.004440
average val loss: 0.002371, accuracy: 0.0507
average test loss: 0.003113, accuracy: 0.0648
case acc: 0.048817895
case acc: 0.11445887
case acc: 0.060998954
case acc: 0.049740296
case acc: 0.05562109
case acc: 0.05939484
top acc: 0.0386 ::: bot acc: 0.0871
top acc: 0.1633 ::: bot acc: 0.0685
top acc: 0.0491 ::: bot acc: 0.0928
top acc: 0.0287 ::: bot acc: 0.0782
top acc: 0.0753 ::: bot acc: 0.0717
top acc: 0.1075 ::: bot acc: 0.0527
current epoch: 8
train loss is 0.004614
average val loss: 0.002206, accuracy: 0.0491
average test loss: 0.002961, accuracy: 0.0637
case acc: 0.048753984
case acc: 0.10541336
case acc: 0.062272884
case acc: 0.05123877
case acc: 0.05498861
case acc: 0.059245866
top acc: 0.0401 ::: bot acc: 0.0860
top acc: 0.1540 ::: bot acc: 0.0594
top acc: 0.0476 ::: bot acc: 0.0955
top acc: 0.0290 ::: bot acc: 0.0802
top acc: 0.0776 ::: bot acc: 0.0685
top acc: 0.1048 ::: bot acc: 0.0552
current epoch: 9
train loss is 0.004468
average val loss: 0.002073, accuracy: 0.0492
average test loss: 0.002827, accuracy: 0.0634
case acc: 0.049764954
case acc: 0.088464506
case acc: 0.068131484
case acc: 0.058579627
case acc: 0.055692103
case acc: 0.05952683
top acc: 0.0329 ::: bot acc: 0.0925
top acc: 0.1365 ::: bot acc: 0.0436
top acc: 0.0438 ::: bot acc: 0.1058
top acc: 0.0327 ::: bot acc: 0.0893
top acc: 0.0727 ::: bot acc: 0.0737
top acc: 0.0947 ::: bot acc: 0.0656
current epoch: 10
train loss is 0.003979
average val loss: 0.001969, accuracy: 0.0483
average test loss: 0.002742, accuracy: 0.0626
case acc: 0.050037798
case acc: 0.078847356
case acc: 0.070190944
case acc: 0.061218873
case acc: 0.055524476
case acc: 0.059611913
top acc: 0.0330 ::: bot acc: 0.0934
top acc: 0.1259 ::: bot acc: 0.0357
top acc: 0.0428 ::: bot acc: 0.1099
top acc: 0.0340 ::: bot acc: 0.0926
top acc: 0.0738 ::: bot acc: 0.0729
top acc: 0.0920 ::: bot acc: 0.0680
current epoch: 11
train loss is 0.003734
average val loss: 0.001830, accuracy: 0.0458
average test loss: 0.002622, accuracy: 0.0609
case acc: 0.048822183
case acc: 0.07658776
case acc: 0.068318695
case acc: 0.05781484
case acc: 0.05457871
case acc: 0.05931612
top acc: 0.0392 ::: bot acc: 0.0867
top acc: 0.1239 ::: bot acc: 0.0338
top acc: 0.0436 ::: bot acc: 0.1062
top acc: 0.0322 ::: bot acc: 0.0885
top acc: 0.0805 ::: bot acc: 0.0658
top acc: 0.0957 ::: bot acc: 0.0645
current epoch: 12
train loss is 0.003452
average val loss: 0.001729, accuracy: 0.0442
average test loss: 0.002529, accuracy: 0.0596
case acc: 0.048274703
case acc: 0.073344335
case acc: 0.06686203
case acc: 0.055318866
case acc: 0.05436453
case acc: 0.05924049
top acc: 0.0443 ::: bot acc: 0.0816
top acc: 0.1203 ::: bot acc: 0.0311
top acc: 0.0443 ::: bot acc: 0.1040
top acc: 0.0310 ::: bot acc: 0.0853
top acc: 0.0853 ::: bot acc: 0.0612
top acc: 0.0983 ::: bot acc: 0.0619
current epoch: 13
train loss is 0.003367
average val loss: 0.001642, accuracy: 0.0431
average test loss: 0.002458, accuracy: 0.0586
case acc: 0.04795924
case acc: 0.06932901
case acc: 0.06611074
case acc: 0.054284792
case acc: 0.054303583
case acc: 0.05943956
top acc: 0.0473 ::: bot acc: 0.0780
top acc: 0.1160 ::: bot acc: 0.0277
top acc: 0.0446 ::: bot acc: 0.1025
top acc: 0.0304 ::: bot acc: 0.0840
top acc: 0.0878 ::: bot acc: 0.0589
top acc: 0.0993 ::: bot acc: 0.0614
current epoch: 14
train loss is 0.003211
average val loss: 0.001570, accuracy: 0.0420
average test loss: 0.002394, accuracy: 0.0576
case acc: 0.047977213
case acc: 0.066469386
case acc: 0.06513113
case acc: 0.052349284
case acc: 0.054250732
case acc: 0.05935233
top acc: 0.0514 ::: bot acc: 0.0743
top acc: 0.1128 ::: bot acc: 0.0254
top acc: 0.0457 ::: bot acc: 0.1007
top acc: 0.0297 ::: bot acc: 0.0816
top acc: 0.0907 ::: bot acc: 0.0558
top acc: 0.1008 ::: bot acc: 0.0599
current epoch: 15
train loss is 0.003132
average val loss: 0.001502, accuracy: 0.0411
average test loss: 0.002332, accuracy: 0.0567
case acc: 0.047829222
case acc: 0.06278174
case acc: 0.0645392
case acc: 0.051496457
case acc: 0.054165848
case acc: 0.05912328
top acc: 0.0535 ::: bot acc: 0.0716
top acc: 0.1085 ::: bot acc: 0.0230
top acc: 0.0462 ::: bot acc: 0.0996
top acc: 0.0294 ::: bot acc: 0.0804
top acc: 0.0917 ::: bot acc: 0.0550
top acc: 0.1008 ::: bot acc: 0.0593
current epoch: 16
train loss is 0.003029
average val loss: 0.001445, accuracy: 0.0403
average test loss: 0.002280, accuracy: 0.0560
case acc: 0.04809181
case acc: 0.059702262
case acc: 0.063920386
case acc: 0.050751574
case acc: 0.05416799
case acc: 0.059272837
top acc: 0.0565 ::: bot acc: 0.0691
top acc: 0.1043 ::: bot acc: 0.0217
top acc: 0.0469 ::: bot acc: 0.0982
top acc: 0.0294 ::: bot acc: 0.0794
top acc: 0.0923 ::: bot acc: 0.0544
top acc: 0.1008 ::: bot acc: 0.0594
current epoch: 17
train loss is 0.002930
average val loss: 0.001397, accuracy: 0.0397
average test loss: 0.002236, accuracy: 0.0551
case acc: 0.048315875
case acc: 0.058775663
case acc: 0.06195482
case acc: 0.048223805
case acc: 0.053972498
case acc: 0.059226546
top acc: 0.0611 ::: bot acc: 0.0648
top acc: 0.1032 ::: bot acc: 0.0212
top acc: 0.0484 ::: bot acc: 0.0945
top acc: 0.0286 ::: bot acc: 0.0758
top acc: 0.0950 ::: bot acc: 0.0514
top acc: 0.1035 ::: bot acc: 0.0568
current epoch: 18
train loss is 0.002876
average val loss: 0.001354, accuracy: 0.0392
average test loss: 0.002190, accuracy: 0.0546
case acc: 0.048350677
case acc: 0.05453026
case acc: 0.06260444
case acc: 0.048780337
case acc: 0.053941146
case acc: 0.059200473
top acc: 0.0608 ::: bot acc: 0.0649
top acc: 0.0980 ::: bot acc: 0.0196
top acc: 0.0482 ::: bot acc: 0.0956
top acc: 0.0288 ::: bot acc: 0.0766
top acc: 0.0928 ::: bot acc: 0.0534
top acc: 0.1017 ::: bot acc: 0.0583
current epoch: 19
train loss is 0.002809
average val loss: 0.001314, accuracy: 0.0387
average test loss: 0.002154, accuracy: 0.0540
case acc: 0.048534077
case acc: 0.052789602
case acc: 0.06169813
case acc: 0.047852
case acc: 0.05395002
case acc: 0.059149515
top acc: 0.0628 ::: bot acc: 0.0631
top acc: 0.0951 ::: bot acc: 0.0193
top acc: 0.0487 ::: bot acc: 0.0939
top acc: 0.0287 ::: bot acc: 0.0754
top acc: 0.0929 ::: bot acc: 0.0533
top acc: 0.1020 ::: bot acc: 0.0582
current epoch: 20
train loss is 0.002722
average val loss: 0.001284, accuracy: 0.0383
average test loss: 0.002122, accuracy: 0.0534
case acc: 0.048761837
case acc: 0.051186483
case acc: 0.0607462
case acc: 0.046636846
case acc: 0.054015484
case acc: 0.059021693
top acc: 0.0654 ::: bot acc: 0.0607
top acc: 0.0931 ::: bot acc: 0.0188
top acc: 0.0500 ::: bot acc: 0.0921
top acc: 0.0283 ::: bot acc: 0.0737
top acc: 0.0933 ::: bot acc: 0.0533
top acc: 0.1021 ::: bot acc: 0.0577
current epoch: 21
train loss is 0.002725
average val loss: 0.001259, accuracy: 0.0379
average test loss: 0.002101, accuracy: 0.0528
case acc: 0.049256645
case acc: 0.050929688
case acc: 0.0591571
case acc: 0.044582512
case acc: 0.053961758
case acc: 0.059142005
top acc: 0.0696 ::: bot acc: 0.0567
top acc: 0.0928 ::: bot acc: 0.0187
top acc: 0.0519 ::: bot acc: 0.0886
top acc: 0.0281 ::: bot acc: 0.0708
top acc: 0.0945 ::: bot acc: 0.0516
top acc: 0.1041 ::: bot acc: 0.0561
current epoch: 22
train loss is 0.002692
average val loss: 0.001237, accuracy: 0.0376
average test loss: 0.002078, accuracy: 0.0523
case acc: 0.049440283
case acc: 0.050037652
case acc: 0.05788885
case acc: 0.04338237
case acc: 0.054079197
case acc: 0.05903027
top acc: 0.0717 ::: bot acc: 0.0545
top acc: 0.0915 ::: bot acc: 0.0185
top acc: 0.0537 ::: bot acc: 0.0859
top acc: 0.0279 ::: bot acc: 0.0690
top acc: 0.0953 ::: bot acc: 0.0511
top acc: 0.1049 ::: bot acc: 0.0550
current epoch: 23
train loss is 0.002686
average val loss: 0.001240, accuracy: 0.0377
average test loss: 0.002086, accuracy: 0.0520
case acc: 0.050389525
case acc: 0.051334757
case acc: 0.05611513
case acc: 0.040792696
case acc: 0.05416927
case acc: 0.059148755
top acc: 0.0769 ::: bot acc: 0.0496
top acc: 0.0933 ::: bot acc: 0.0188
top acc: 0.0578 ::: bot acc: 0.0810
top acc: 0.0285 ::: bot acc: 0.0650
top acc: 0.0982 ::: bot acc: 0.0481
top acc: 0.1077 ::: bot acc: 0.0521
current epoch: 24
train loss is 0.002672
average val loss: 0.001238, accuracy: 0.0377
average test loss: 0.002081, accuracy: 0.0516
case acc: 0.0508139
case acc: 0.051089216
case acc: 0.05463166
case acc: 0.039511304
case acc: 0.054357875
case acc: 0.059305165
top acc: 0.0795 ::: bot acc: 0.0468
top acc: 0.0930 ::: bot acc: 0.0188
top acc: 0.0598 ::: bot acc: 0.0778
top acc: 0.0291 ::: bot acc: 0.0629
top acc: 0.0994 ::: bot acc: 0.0470
top acc: 0.1087 ::: bot acc: 0.0515
current epoch: 25
train loss is 0.002645
average val loss: 0.001218, accuracy: 0.0375
average test loss: 0.002059, accuracy: 0.0513
case acc: 0.050772216
case acc: 0.049346793
case acc: 0.054464586
case acc: 0.039553836
case acc: 0.054359745
case acc: 0.05938189
top acc: 0.0799 ::: bot acc: 0.0462
top acc: 0.0908 ::: bot acc: 0.0183
top acc: 0.0605 ::: bot acc: 0.0772
top acc: 0.0292 ::: bot acc: 0.0626
top acc: 0.0986 ::: bot acc: 0.0480
top acc: 0.1076 ::: bot acc: 0.0527
current epoch: 26
train loss is 0.002611
average val loss: 0.001212, accuracy: 0.0375
average test loss: 0.002052, accuracy: 0.0511
case acc: 0.050994374
case acc: 0.0486463
case acc: 0.053834505
case acc: 0.039341997
case acc: 0.054479536
case acc: 0.0592249
top acc: 0.0811 ::: bot acc: 0.0447
top acc: 0.0898 ::: bot acc: 0.0180
top acc: 0.0622 ::: bot acc: 0.0756
top acc: 0.0296 ::: bot acc: 0.0622
top acc: 0.0991 ::: bot acc: 0.0476
top acc: 0.1071 ::: bot acc: 0.0531
current epoch: 27
train loss is 0.002593
average val loss: 0.001207, accuracy: 0.0375
average test loss: 0.002046, accuracy: 0.0507
case acc: 0.051198456
case acc: 0.048019662
case acc: 0.053119063
case acc: 0.038247216
case acc: 0.05443352
case acc: 0.059316162
top acc: 0.0827 ::: bot acc: 0.0426
top acc: 0.0892 ::: bot acc: 0.0174
top acc: 0.0641 ::: bot acc: 0.0736
top acc: 0.0297 ::: bot acc: 0.0605
top acc: 0.0998 ::: bot acc: 0.0468
top acc: 0.1078 ::: bot acc: 0.0526
current epoch: 28
train loss is 0.002598
average val loss: 0.001217, accuracy: 0.0377
average test loss: 0.002058, accuracy: 0.0506
case acc: 0.05203196
case acc: 0.048329234
case acc: 0.052248728
case acc: 0.037005156
case acc: 0.054497592
case acc: 0.059380066
top acc: 0.0858 ::: bot acc: 0.0406
top acc: 0.0894 ::: bot acc: 0.0177
top acc: 0.0668 ::: bot acc: 0.0707
top acc: 0.0303 ::: bot acc: 0.0583
top acc: 0.1014 ::: bot acc: 0.0451
top acc: 0.1089 ::: bot acc: 0.0516
current epoch: 29
train loss is 0.002581
average val loss: 0.001226, accuracy: 0.0379
average test loss: 0.002064, accuracy: 0.0504
case acc: 0.052469265
case acc: 0.048393417
case acc: 0.05163106
case acc: 0.035986308
case acc: 0.054697726
case acc: 0.0592887
top acc: 0.0875 ::: bot acc: 0.0386
top acc: 0.0894 ::: bot acc: 0.0178
top acc: 0.0692 ::: bot acc: 0.0684
top acc: 0.0309 ::: bot acc: 0.0564
top acc: 0.1024 ::: bot acc: 0.0439
top acc: 0.1098 ::: bot acc: 0.0502
current epoch: 30
train loss is 0.002605
average val loss: 0.001216, accuracy: 0.0378
average test loss: 0.002051, accuracy: 0.0502
case acc: 0.052533757
case acc: 0.047277335
case acc: 0.05138264
case acc: 0.035852507
case acc: 0.05464606
case acc: 0.05920789
top acc: 0.0878 ::: bot acc: 0.0385
top acc: 0.0878 ::: bot acc: 0.0177
top acc: 0.0695 ::: bot acc: 0.0678
top acc: 0.0309 ::: bot acc: 0.0564
top acc: 0.1022 ::: bot acc: 0.0442
top acc: 0.1093 ::: bot acc: 0.0505
current epoch: 31
train loss is 0.002549
average val loss: 0.001203, accuracy: 0.0377
average test loss: 0.002037, accuracy: 0.0500
case acc: 0.052560467
case acc: 0.04610613
case acc: 0.051347177
case acc: 0.03594291
case acc: 0.054629155
case acc: 0.059241407
top acc: 0.0878 ::: bot acc: 0.0386
top acc: 0.0862 ::: bot acc: 0.0176
top acc: 0.0698 ::: bot acc: 0.0676
top acc: 0.0307 ::: bot acc: 0.0565
top acc: 0.1021 ::: bot acc: 0.0442
top acc: 0.1084 ::: bot acc: 0.0517
current epoch: 32
train loss is 0.002536
average val loss: 0.001199, accuracy: 0.0377
average test loss: 0.002031, accuracy: 0.0498
case acc: 0.052671347
case acc: 0.04537894
case acc: 0.051105037
case acc: 0.03551813
case acc: 0.054696344
case acc: 0.05918508
top acc: 0.0882 ::: bot acc: 0.0382
top acc: 0.0853 ::: bot acc: 0.0172
top acc: 0.0707 ::: bot acc: 0.0666
top acc: 0.0307 ::: bot acc: 0.0558
top acc: 0.1025 ::: bot acc: 0.0437
top acc: 0.1083 ::: bot acc: 0.0515
current epoch: 33
train loss is 0.002520
average val loss: 0.001198, accuracy: 0.0377
average test loss: 0.002034, accuracy: 0.0497
case acc: 0.052820947
case acc: 0.045007348
case acc: 0.050973233
case acc: 0.035158034
case acc: 0.054794267
case acc: 0.059251573
top acc: 0.0889 ::: bot acc: 0.0374
top acc: 0.0846 ::: bot acc: 0.0175
top acc: 0.0718 ::: bot acc: 0.0656
top acc: 0.0312 ::: bot acc: 0.0552
top acc: 0.1033 ::: bot acc: 0.0431
top acc: 0.1088 ::: bot acc: 0.0513
current epoch: 34
train loss is 0.002509
average val loss: 0.001168, accuracy: 0.0373
average test loss: 0.001995, accuracy: 0.0494
case acc: 0.05240905
case acc: 0.04307504
case acc: 0.051225375
case acc: 0.036009304
case acc: 0.054447938
case acc: 0.05909617
top acc: 0.0870 ::: bot acc: 0.0395
top acc: 0.0814 ::: bot acc: 0.0179
top acc: 0.0703 ::: bot acc: 0.0671
top acc: 0.0307 ::: bot acc: 0.0568
top acc: 0.1014 ::: bot acc: 0.0446
top acc: 0.1068 ::: bot acc: 0.0531
current epoch: 35
train loss is 0.002471
average val loss: 0.001151, accuracy: 0.0370
average test loss: 0.001975, accuracy: 0.0492
case acc: 0.052096196
case acc: 0.04198783
case acc: 0.051416494
case acc: 0.03645496
case acc: 0.05439048
case acc: 0.059014462
top acc: 0.0857 ::: bot acc: 0.0405
top acc: 0.0794 ::: bot acc: 0.0188
top acc: 0.0695 ::: bot acc: 0.0677
top acc: 0.0304 ::: bot acc: 0.0574
top acc: 0.1001 ::: bot acc: 0.0458
top acc: 0.1055 ::: bot acc: 0.0542
current epoch: 36
train loss is 0.002440
average val loss: 0.001149, accuracy: 0.0371
average test loss: 0.001975, accuracy: 0.0491
case acc: 0.05216034
case acc: 0.041702572
case acc: 0.051299915
case acc: 0.03610563
case acc: 0.054477558
case acc: 0.05903567
top acc: 0.0861 ::: bot acc: 0.0402
top acc: 0.0788 ::: bot acc: 0.0190
top acc: 0.0703 ::: bot acc: 0.0673
top acc: 0.0306 ::: bot acc: 0.0568
top acc: 0.1007 ::: bot acc: 0.0455
top acc: 0.1059 ::: bot acc: 0.0539
current epoch: 37
train loss is 0.002427
average val loss: 0.001137, accuracy: 0.0369
average test loss: 0.001963, accuracy: 0.0490
case acc: 0.052001067
case acc: 0.04100597
case acc: 0.05136378
case acc: 0.036163907
case acc: 0.05447704
case acc: 0.05901315
top acc: 0.0855 ::: bot acc: 0.0408
top acc: 0.0774 ::: bot acc: 0.0198
top acc: 0.0699 ::: bot acc: 0.0676
top acc: 0.0305 ::: bot acc: 0.0570
top acc: 0.1002 ::: bot acc: 0.0462
top acc: 0.1055 ::: bot acc: 0.0543
current epoch: 38
train loss is 0.002426
average val loss: 0.001135, accuracy: 0.0368
average test loss: 0.001961, accuracy: 0.0489
case acc: 0.052010853
case acc: 0.04067796
case acc: 0.051292192
case acc: 0.03597089
case acc: 0.054435406
case acc: 0.059134904
top acc: 0.0852 ::: bot acc: 0.0412
top acc: 0.0767 ::: bot acc: 0.0203
top acc: 0.0701 ::: bot acc: 0.0673
top acc: 0.0306 ::: bot acc: 0.0567
top acc: 0.1007 ::: bot acc: 0.0456
top acc: 0.1059 ::: bot acc: 0.0539
current epoch: 39
train loss is 0.002402
average val loss: 0.001111, accuracy: 0.0365
average test loss: 0.001937, accuracy: 0.0488
case acc: 0.05157124
case acc: 0.03903246
case acc: 0.05194757
case acc: 0.036876526
case acc: 0.05428505
case acc: 0.059156958
top acc: 0.0831 ::: bot acc: 0.0432
top acc: 0.0735 ::: bot acc: 0.0217
top acc: 0.0683 ::: bot acc: 0.0693
top acc: 0.0301 ::: bot acc: 0.0582
top acc: 0.0990 ::: bot acc: 0.0472
top acc: 0.1042 ::: bot acc: 0.0558
current epoch: 40
train loss is 0.002392
average val loss: 0.001101, accuracy: 0.0364
average test loss: 0.001930, accuracy: 0.0487
case acc: 0.051399637
case acc: 0.038471285
case acc: 0.05205178
case acc: 0.03705688
case acc: 0.054304294
case acc: 0.059060957
top acc: 0.0825 ::: bot acc: 0.0439
top acc: 0.0722 ::: bot acc: 0.0228
top acc: 0.0678 ::: bot acc: 0.0699
top acc: 0.0302 ::: bot acc: 0.0586
top acc: 0.0991 ::: bot acc: 0.0473
top acc: 0.1043 ::: bot acc: 0.0555
current epoch: 41
train loss is 0.002364
average val loss: 0.001080, accuracy: 0.0361
average test loss: 0.001901, accuracy: 0.0488
case acc: 0.05081456
case acc: 0.037019674
case acc: 0.05294346
case acc: 0.038844675
case acc: 0.054070484
case acc: 0.058901105
top acc: 0.0789 ::: bot acc: 0.0475
top acc: 0.0682 ::: bot acc: 0.0262
top acc: 0.0643 ::: bot acc: 0.0731
top acc: 0.0296 ::: bot acc: 0.0614
top acc: 0.0962 ::: bot acc: 0.0502
top acc: 0.1012 ::: bot acc: 0.0584
current epoch: 42
train loss is 0.002340
average val loss: 0.001068, accuracy: 0.0359
average test loss: 0.001886, accuracy: 0.0489
case acc: 0.050332483
case acc: 0.036249556
case acc: 0.053839207
case acc: 0.04006199
case acc: 0.05397088
case acc: 0.05903142
top acc: 0.0763 ::: bot acc: 0.0504
top acc: 0.0648 ::: bot acc: 0.0297
top acc: 0.0620 ::: bot acc: 0.0755
top acc: 0.0290 ::: bot acc: 0.0635
top acc: 0.0938 ::: bot acc: 0.0524
top acc: 0.0993 ::: bot acc: 0.0606
current epoch: 43
train loss is 0.002314
average val loss: 0.001066, accuracy: 0.0359
average test loss: 0.001878, accuracy: 0.0492
case acc: 0.049752098
case acc: 0.03561161
case acc: 0.055189572
case acc: 0.041961234
case acc: 0.053899564
case acc: 0.059056345
top acc: 0.0727 ::: bot acc: 0.0537
top acc: 0.0609 ::: bot acc: 0.0338
top acc: 0.0589 ::: bot acc: 0.0791
top acc: 0.0284 ::: bot acc: 0.0667
top acc: 0.0907 ::: bot acc: 0.0554
top acc: 0.0961 ::: bot acc: 0.0637
current epoch: 44
train loss is 0.002302
average val loss: 0.001066, accuracy: 0.0360
average test loss: 0.001878, accuracy: 0.0494
case acc: 0.049621537
case acc: 0.035266954
case acc: 0.055756673
case acc: 0.04268384
case acc: 0.054016203
case acc: 0.059080362
top acc: 0.0710 ::: bot acc: 0.0554
top acc: 0.0590 ::: bot acc: 0.0357
top acc: 0.0577 ::: bot acc: 0.0807
top acc: 0.0283 ::: bot acc: 0.0678
top acc: 0.0897 ::: bot acc: 0.0566
top acc: 0.0955 ::: bot acc: 0.0644
current epoch: 45
train loss is 0.002292
average val loss: 0.001074, accuracy: 0.0362
average test loss: 0.001883, accuracy: 0.0497
case acc: 0.04929419
case acc: 0.034961756
case acc: 0.056941535
case acc: 0.044116274
case acc: 0.054054894
case acc: 0.05912327
top acc: 0.0683 ::: bot acc: 0.0580
top acc: 0.0558 ::: bot acc: 0.0386
top acc: 0.0560 ::: bot acc: 0.0833
top acc: 0.0281 ::: bot acc: 0.0701
top acc: 0.0876 ::: bot acc: 0.0586
top acc: 0.0935 ::: bot acc: 0.0663
current epoch: 46
train loss is 0.002285
average val loss: 0.001083, accuracy: 0.0364
average test loss: 0.001892, accuracy: 0.0501
case acc: 0.04907407
case acc: 0.03489401
case acc: 0.057920057
case acc: 0.045357272
case acc: 0.054010823
case acc: 0.059298042
top acc: 0.0661 ::: bot acc: 0.0603
top acc: 0.0530 ::: bot acc: 0.0416
top acc: 0.0540 ::: bot acc: 0.0857
top acc: 0.0281 ::: bot acc: 0.0720
top acc: 0.0859 ::: bot acc: 0.0599
top acc: 0.0921 ::: bot acc: 0.0679
current epoch: 47
train loss is 0.002283
average val loss: 0.001117, accuracy: 0.0370
average test loss: 0.001927, accuracy: 0.0509
case acc: 0.04873667
case acc: 0.034863897
case acc: 0.05989127
case acc: 0.048103075
case acc: 0.054375786
case acc: 0.059701443
top acc: 0.0623 ::: bot acc: 0.0642
top acc: 0.0486 ::: bot acc: 0.0460
top acc: 0.0510 ::: bot acc: 0.0901
top acc: 0.0288 ::: bot acc: 0.0757
top acc: 0.0830 ::: bot acc: 0.0633
top acc: 0.0890 ::: bot acc: 0.0711
current epoch: 48
train loss is 0.002286
average val loss: 0.001140, accuracy: 0.0374
average test loss: 0.001953, accuracy: 0.0514
case acc: 0.04848166
case acc: 0.03500319
case acc: 0.06126635
case acc: 0.049582407
case acc: 0.0545164
case acc: 0.059833515
top acc: 0.0596 ::: bot acc: 0.0667
top acc: 0.0456 ::: bot acc: 0.0491
top acc: 0.0493 ::: bot acc: 0.0930
top acc: 0.0291 ::: bot acc: 0.0778
top acc: 0.0815 ::: bot acc: 0.0649
top acc: 0.0877 ::: bot acc: 0.0722
current epoch: 49
train loss is 0.002298
average val loss: 0.001164, accuracy: 0.0377
average test loss: 0.001983, accuracy: 0.0520
case acc: 0.04843509
case acc: 0.035423134
case acc: 0.06269646
case acc: 0.05089902
case acc: 0.054534744
case acc: 0.05997243
top acc: 0.0575 ::: bot acc: 0.0688
top acc: 0.0426 ::: bot acc: 0.0523
top acc: 0.0477 ::: bot acc: 0.0959
top acc: 0.0296 ::: bot acc: 0.0795
top acc: 0.0806 ::: bot acc: 0.0658
top acc: 0.0870 ::: bot acc: 0.0731
current epoch: 50
train loss is 0.002307
average val loss: 0.001259, accuracy: 0.0393
average test loss: 0.002071, accuracy: 0.0536
case acc: 0.048400253
case acc: 0.036536165
case acc: 0.066041514
case acc: 0.054790787
case acc: 0.054933436
case acc: 0.060687907
top acc: 0.0522 ::: bot acc: 0.0742
top acc: 0.0358 ::: bot acc: 0.0585
top acc: 0.0451 ::: bot acc: 0.1022
top acc: 0.0310 ::: bot acc: 0.0846
top acc: 0.0763 ::: bot acc: 0.0699
top acc: 0.0829 ::: bot acc: 0.0768
LME_Co_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5328 5328 5328
1.7082474 -0.6288155 0.29835227 -0.25048217
Validation: 594 594 594
Testing: 774 774 774
pre-processing time: 0.0003445148468017578
the split date is 2011-07-01
train dropout: 0.4 test dropout: 0.05
net initializing with time: 0.003986358642578125
preparing training and testing date with time: 4.76837158203125e-07
current epoch: 1
train loss is 0.010665
average val loss: 0.005540, accuracy: 0.0945
average test loss: 0.006180, accuracy: 0.0973
case acc: 0.12963904
case acc: 0.11291807
case acc: 0.100695916
case acc: 0.0697477
case acc: 0.12300738
case acc: 0.047865883
top acc: 0.0789 ::: bot acc: 0.1824
top acc: 0.1549 ::: bot acc: 0.0734
top acc: 0.0391 ::: bot acc: 0.1638
top acc: 0.0277 ::: bot acc: 0.1249
top acc: 0.0615 ::: bot acc: 0.1821
top acc: 0.0402 ::: bot acc: 0.0747
current epoch: 2
train loss is 0.009137
average val loss: 0.006789, accuracy: 0.0848
average test loss: 0.006248, accuracy: 0.0847
case acc: 0.038360603
case acc: 0.21942206
case acc: 0.051584028
case acc: 0.06642917
case acc: 0.045882877
case acc: 0.086418636
top acc: 0.0424 ::: bot acc: 0.0606
top acc: 0.2616 ::: bot acc: 0.1797
top acc: 0.0865 ::: bot acc: 0.0432
top acc: 0.1218 ::: bot acc: 0.0199
top acc: 0.0488 ::: bot acc: 0.0711
top acc: 0.1470 ::: bot acc: 0.0341
current epoch: 3
train loss is 0.010384
average val loss: 0.012951, accuracy: 0.1348
average test loss: 0.011726, accuracy: 0.1265
case acc: 0.06536974
case acc: 0.27592397
case acc: 0.093731575
case acc: 0.11983351
case acc: 0.060948487
case acc: 0.14311507
top acc: 0.1130 ::: bot acc: 0.0193
top acc: 0.3186 ::: bot acc: 0.2359
top acc: 0.1560 ::: bot acc: 0.0353
top acc: 0.1851 ::: bot acc: 0.0534
top acc: 0.1150 ::: bot acc: 0.0159
top acc: 0.2044 ::: bot acc: 0.0894
current epoch: 4
train loss is 0.013263
average val loss: 0.004602, accuracy: 0.0713
average test loss: 0.004227, accuracy: 0.0711
case acc: 0.04292506
case acc: 0.17347685
case acc: 0.049324635
case acc: 0.053833775
case acc: 0.052065145
case acc: 0.05511964
top acc: 0.0265 ::: bot acc: 0.0777
top acc: 0.2156 ::: bot acc: 0.1340
top acc: 0.0644 ::: bot acc: 0.0665
top acc: 0.0928 ::: bot acc: 0.0405
top acc: 0.0317 ::: bot acc: 0.0903
top acc: 0.1089 ::: bot acc: 0.0158
current epoch: 5
train loss is 0.009282
average val loss: 0.004314, accuracy: 0.0821
average test loss: 0.004714, accuracy: 0.0840
case acc: 0.10537675
case acc: 0.07843476
case acc: 0.09139367
case acc: 0.069268934
case acc: 0.106096745
case acc: 0.053503267
top acc: 0.0545 ::: bot acc: 0.1579
top acc: 0.1207 ::: bot acc: 0.0389
top acc: 0.0335 ::: bot acc: 0.1531
top acc: 0.0277 ::: bot acc: 0.1241
top acc: 0.0460 ::: bot acc: 0.1645
top acc: 0.0276 ::: bot acc: 0.0899
current epoch: 6
train loss is 0.004957
average val loss: 0.003257, accuracy: 0.0686
average test loss: 0.003199, accuracy: 0.0669
case acc: 0.06827826
case acc: 0.10524984
case acc: 0.065994635
case acc: 0.05046523
case acc: 0.06867693
case acc: 0.042718608
top acc: 0.0222 ::: bot acc: 0.1183
top acc: 0.1475 ::: bot acc: 0.0657
top acc: 0.0253 ::: bot acc: 0.1186
top acc: 0.0446 ::: bot acc: 0.0873
top acc: 0.0233 ::: bot acc: 0.1197
top acc: 0.0625 ::: bot acc: 0.0525
current epoch: 7
train loss is 0.004525
average val loss: 0.003248, accuracy: 0.0652
average test loss: 0.002903, accuracy: 0.0616
case acc: 0.049356956
case acc: 0.12067808
case acc: 0.05458102
case acc: 0.048307266
case acc: 0.052141346
case acc: 0.044671632
top acc: 0.0187 ::: bot acc: 0.0915
top acc: 0.1631 ::: bot acc: 0.0809
top acc: 0.0345 ::: bot acc: 0.0972
top acc: 0.0676 ::: bot acc: 0.0649
top acc: 0.0310 ::: bot acc: 0.0914
top acc: 0.0851 ::: bot acc: 0.0304
current epoch: 8
train loss is 0.004787
average val loss: 0.002967, accuracy: 0.0644
average test loss: 0.002698, accuracy: 0.0606
case acc: 0.054448165
case acc: 0.10142733
case acc: 0.06092529
case acc: 0.048519727
case acc: 0.05568204
case acc: 0.042724997
top acc: 0.0172 ::: bot acc: 0.1003
top acc: 0.1436 ::: bot acc: 0.0619
top acc: 0.0280 ::: bot acc: 0.1103
top acc: 0.0558 ::: bot acc: 0.0764
top acc: 0.0278 ::: bot acc: 0.0979
top acc: 0.0731 ::: bot acc: 0.0425
current epoch: 9
train loss is 0.004330
average val loss: 0.002834, accuracy: 0.0639
average test loss: 0.002584, accuracy: 0.0598
case acc: 0.057397738
case acc: 0.086533
case acc: 0.06558566
case acc: 0.049659602
case acc: 0.056663718
case acc: 0.042697422
top acc: 0.0177 ::: bot acc: 0.1040
top acc: 0.1285 ::: bot acc: 0.0470
top acc: 0.0256 ::: bot acc: 0.1183
top acc: 0.0493 ::: bot acc: 0.0830
top acc: 0.0264 ::: bot acc: 0.1001
top acc: 0.0659 ::: bot acc: 0.0498
current epoch: 10
train loss is 0.003777
average val loss: 0.002732, accuracy: 0.0628
average test loss: 0.002458, accuracy: 0.0581
case acc: 0.05617123
case acc: 0.078068696
case acc: 0.066951506
case acc: 0.049732607
case acc: 0.055177037
case acc: 0.042694468
top acc: 0.0176 ::: bot acc: 0.1024
top acc: 0.1205 ::: bot acc: 0.0385
top acc: 0.0256 ::: bot acc: 0.1203
top acc: 0.0484 ::: bot acc: 0.0840
top acc: 0.0276 ::: bot acc: 0.0974
top acc: 0.0649 ::: bot acc: 0.0504
current epoch: 11
train loss is 0.003513
average val loss: 0.002650, accuracy: 0.0613
average test loss: 0.002285, accuracy: 0.0557
case acc: 0.050593734
case acc: 0.07820502
case acc: 0.06321865
case acc: 0.048851345
case acc: 0.050956003
case acc: 0.042645175
top acc: 0.0184 ::: bot acc: 0.0937
top acc: 0.1207 ::: bot acc: 0.0385
top acc: 0.0264 ::: bot acc: 0.1140
top acc: 0.0551 ::: bot acc: 0.0777
top acc: 0.0330 ::: bot acc: 0.0882
top acc: 0.0714 ::: bot acc: 0.0441
current epoch: 12
train loss is 0.003409
average val loss: 0.002607, accuracy: 0.0602
average test loss: 0.002163, accuracy: 0.0540
case acc: 0.046239935
case acc: 0.077453114
case acc: 0.06050189
case acc: 0.048468195
case acc: 0.048541475
case acc: 0.043089278
top acc: 0.0213 ::: bot acc: 0.0857
top acc: 0.1198 ::: bot acc: 0.0377
top acc: 0.0280 ::: bot acc: 0.1093
top acc: 0.0611 ::: bot acc: 0.0719
top acc: 0.0399 ::: bot acc: 0.0813
top acc: 0.0766 ::: bot acc: 0.0389
current epoch: 13
train loss is 0.003266
average val loss: 0.002519, accuracy: 0.0595
average test loss: 0.002085, accuracy: 0.0529
case acc: 0.046491984
case acc: 0.06902494
case acc: 0.062129308
case acc: 0.048400164
case acc: 0.04882934
case acc: 0.04275356
top acc: 0.0206 ::: bot acc: 0.0862
top acc: 0.1119 ::: bot acc: 0.0290
top acc: 0.0272 ::: bot acc: 0.1122
top acc: 0.0583 ::: bot acc: 0.0740
top acc: 0.0379 ::: bot acc: 0.0828
top acc: 0.0734 ::: bot acc: 0.0421
current epoch: 14
train loss is 0.003100
average val loss: 0.002457, accuracy: 0.0588
average test loss: 0.002009, accuracy: 0.0519
case acc: 0.045766372
case acc: 0.06319185
case acc: 0.06222223
case acc: 0.048374064
case acc: 0.048892215
case acc: 0.042707693
top acc: 0.0221 ::: bot acc: 0.0845
top acc: 0.1057 ::: bot acc: 0.0234
top acc: 0.0269 ::: bot acc: 0.1125
top acc: 0.0585 ::: bot acc: 0.0737
top acc: 0.0389 ::: bot acc: 0.0822
top acc: 0.0727 ::: bot acc: 0.0428
current epoch: 15
train loss is 0.002952
average val loss: 0.002404, accuracy: 0.0580
average test loss: 0.001944, accuracy: 0.0508
case acc: 0.044747893
case acc: 0.05826922
case acc: 0.06206505
case acc: 0.04833368
case acc: 0.048453733
case acc: 0.042958386
top acc: 0.0233 ::: bot acc: 0.0825
top acc: 0.1007 ::: bot acc: 0.0187
top acc: 0.0269 ::: bot acc: 0.1122
top acc: 0.0591 ::: bot acc: 0.0732
top acc: 0.0392 ::: bot acc: 0.0815
top acc: 0.0729 ::: bot acc: 0.0435
current epoch: 16
train loss is 0.002836
average val loss: 0.002373, accuracy: 0.0573
average test loss: 0.001869, accuracy: 0.0498
case acc: 0.04272355
case acc: 0.056642577
case acc: 0.060209695
case acc: 0.048123382
case acc: 0.047867753
case acc: 0.04316811
top acc: 0.0270 ::: bot acc: 0.0774
top acc: 0.0991 ::: bot acc: 0.0172
top acc: 0.0286 ::: bot acc: 0.1087
top acc: 0.0626 ::: bot acc: 0.0695
top acc: 0.0425 ::: bot acc: 0.0786
top acc: 0.0754 ::: bot acc: 0.0405
current epoch: 17
train loss is 0.002799
average val loss: 0.002348, accuracy: 0.0565
average test loss: 0.001813, accuracy: 0.0490
case acc: 0.04119605
case acc: 0.05525773
case acc: 0.058545258
case acc: 0.048185423
case acc: 0.04728559
case acc: 0.04343183
top acc: 0.0310 ::: bot acc: 0.0729
top acc: 0.0976 ::: bot acc: 0.0161
top acc: 0.0301 ::: bot acc: 0.1054
top acc: 0.0660 ::: bot acc: 0.0661
top acc: 0.0449 ::: bot acc: 0.0761
top acc: 0.0779 ::: bot acc: 0.0381
current epoch: 18
train loss is 0.002712
average val loss: 0.002301, accuracy: 0.0560
average test loss: 0.001773, accuracy: 0.0483
case acc: 0.040762953
case acc: 0.051202938
case acc: 0.058332708
case acc: 0.04842894
case acc: 0.04762444
case acc: 0.043416265
top acc: 0.0322 ::: bot acc: 0.0715
top acc: 0.0930 ::: bot acc: 0.0130
top acc: 0.0300 ::: bot acc: 0.1051
top acc: 0.0664 ::: bot acc: 0.0665
top acc: 0.0438 ::: bot acc: 0.0774
top acc: 0.0769 ::: bot acc: 0.0394
current epoch: 19
train loss is 0.002622
average val loss: 0.002279, accuracy: 0.0556
average test loss: 0.001734, accuracy: 0.0477
case acc: 0.04021789
case acc: 0.04938054
case acc: 0.057475008
case acc: 0.048278008
case acc: 0.047503185
case acc: 0.043298062
top acc: 0.0348 ::: bot acc: 0.0694
top acc: 0.0907 ::: bot acc: 0.0121
top acc: 0.0311 ::: bot acc: 0.1033
top acc: 0.0677 ::: bot acc: 0.0645
top acc: 0.0444 ::: bot acc: 0.0768
top acc: 0.0773 ::: bot acc: 0.0388
current epoch: 20
train loss is 0.002599
average val loss: 0.002237, accuracy: 0.0551
average test loss: 0.001701, accuracy: 0.0472
case acc: 0.040012922
case acc: 0.046683498
case acc: 0.05693754
case acc: 0.04837254
case acc: 0.047886167
case acc: 0.04334392
top acc: 0.0366 ::: bot acc: 0.0678
top acc: 0.0873 ::: bot acc: 0.0110
top acc: 0.0317 ::: bot acc: 0.1022
top acc: 0.0683 ::: bot acc: 0.0643
top acc: 0.0432 ::: bot acc: 0.0784
top acc: 0.0760 ::: bot acc: 0.0402
current epoch: 21
train loss is 0.002542
average val loss: 0.002257, accuracy: 0.0547
average test loss: 0.001674, accuracy: 0.0468
case acc: 0.038941376
case acc: 0.047949716
case acc: 0.054529063
case acc: 0.048709374
case acc: 0.047112223
case acc: 0.043671444
top acc: 0.0422 ::: bot acc: 0.0619
top acc: 0.0891 ::: bot acc: 0.0113
top acc: 0.0355 ::: bot acc: 0.0969
top acc: 0.0732 ::: bot acc: 0.0590
top acc: 0.0461 ::: bot acc: 0.0751
top acc: 0.0797 ::: bot acc: 0.0365
current epoch: 22
train loss is 0.002543
average val loss: 0.002271, accuracy: 0.0544
average test loss: 0.001662, accuracy: 0.0466
case acc: 0.03861977
case acc: 0.04844596
case acc: 0.052905567
case acc: 0.049254153
case acc: 0.046620872
case acc: 0.043948773
top acc: 0.0470 ::: bot acc: 0.0575
top acc: 0.0898 ::: bot acc: 0.0114
top acc: 0.0395 ::: bot acc: 0.0923
top acc: 0.0769 ::: bot acc: 0.0556
top acc: 0.0483 ::: bot acc: 0.0730
top acc: 0.0817 ::: bot acc: 0.0342
current epoch: 23
train loss is 0.002540
average val loss: 0.002347, accuracy: 0.0544
average test loss: 0.001678, accuracy: 0.0470
case acc: 0.038730267
case acc: 0.05082959
case acc: 0.051239345
case acc: 0.050468143
case acc: 0.04564235
case acc: 0.044792246
top acc: 0.0534 ::: bot acc: 0.0512
top acc: 0.0926 ::: bot acc: 0.0128
top acc: 0.0459 ::: bot acc: 0.0859
top acc: 0.0825 ::: bot acc: 0.0498
top acc: 0.0524 ::: bot acc: 0.0687
top acc: 0.0859 ::: bot acc: 0.0300
current epoch: 24
train loss is 0.002540
average val loss: 0.002315, accuracy: 0.0539
average test loss: 0.001656, accuracy: 0.0467
case acc: 0.038931653
case acc: 0.048848145
case acc: 0.051125627
case acc: 0.05052733
case acc: 0.046011813
case acc: 0.044626057
top acc: 0.0542 ::: bot acc: 0.0503
top acc: 0.0903 ::: bot acc: 0.0116
top acc: 0.0469 ::: bot acc: 0.0850
top acc: 0.0827 ::: bot acc: 0.0496
top acc: 0.0519 ::: bot acc: 0.0696
top acc: 0.0850 ::: bot acc: 0.0313
current epoch: 25
train loss is 0.002521
average val loss: 0.002310, accuracy: 0.0537
average test loss: 0.001638, accuracy: 0.0464
case acc: 0.038988322
case acc: 0.04734168
case acc: 0.050889563
case acc: 0.050714884
case acc: 0.046097986
case acc: 0.044584002
top acc: 0.0554 ::: bot acc: 0.0488
top acc: 0.0882 ::: bot acc: 0.0111
top acc: 0.0483 ::: bot acc: 0.0837
top acc: 0.0834 ::: bot acc: 0.0489
top acc: 0.0518 ::: bot acc: 0.0697
top acc: 0.0847 ::: bot acc: 0.0315
current epoch: 26
train loss is 0.002480
average val loss: 0.002338, accuracy: 0.0537
average test loss: 0.001646, accuracy: 0.0466
case acc: 0.039347842
case acc: 0.047479175
case acc: 0.05038276
case acc: 0.051694382
case acc: 0.045680627
case acc: 0.044816025
top acc: 0.0585 ::: bot acc: 0.0459
top acc: 0.0886 ::: bot acc: 0.0110
top acc: 0.0512 ::: bot acc: 0.0804
top acc: 0.0862 ::: bot acc: 0.0465
top acc: 0.0537 ::: bot acc: 0.0677
top acc: 0.0861 ::: bot acc: 0.0302
current epoch: 27
train loss is 0.002466
average val loss: 0.002309, accuracy: 0.0533
average test loss: 0.001619, accuracy: 0.0462
case acc: 0.039329935
case acc: 0.04553417
case acc: 0.05037799
case acc: 0.05141857
case acc: 0.045757707
case acc: 0.044542577
top acc: 0.0587 ::: bot acc: 0.0454
top acc: 0.0860 ::: bot acc: 0.0104
top acc: 0.0514 ::: bot acc: 0.0803
top acc: 0.0859 ::: bot acc: 0.0467
top acc: 0.0531 ::: bot acc: 0.0685
top acc: 0.0847 ::: bot acc: 0.0313
current epoch: 28
train loss is 0.002442
average val loss: 0.002284, accuracy: 0.0531
average test loss: 0.001599, accuracy: 0.0459
case acc: 0.039454497
case acc: 0.043565482
case acc: 0.05039677
case acc: 0.051376764
case acc: 0.045997925
case acc: 0.04435206
top acc: 0.0588 ::: bot acc: 0.0458
top acc: 0.0833 ::: bot acc: 0.0098
top acc: 0.0515 ::: bot acc: 0.0803
top acc: 0.0855 ::: bot acc: 0.0470
top acc: 0.0526 ::: bot acc: 0.0692
top acc: 0.0835 ::: bot acc: 0.0328
current epoch: 29
train loss is 0.002407
average val loss: 0.002289, accuracy: 0.0529
average test loss: 0.001596, accuracy: 0.0458
case acc: 0.039749995
case acc: 0.042841222
case acc: 0.050248146
case acc: 0.05180383
case acc: 0.045826975
case acc: 0.04441558
top acc: 0.0602 ::: bot acc: 0.0444
top acc: 0.0824 ::: bot acc: 0.0095
top acc: 0.0529 ::: bot acc: 0.0789
top acc: 0.0869 ::: bot acc: 0.0458
top acc: 0.0534 ::: bot acc: 0.0681
top acc: 0.0842 ::: bot acc: 0.0322
current epoch: 30
train loss is 0.002377
average val loss: 0.002279, accuracy: 0.0528
average test loss: 0.001582, accuracy: 0.0456
case acc: 0.039737355
case acc: 0.04166995
case acc: 0.050137065
case acc: 0.05182064
case acc: 0.045753546
case acc: 0.04425895
top acc: 0.0603 ::: bot acc: 0.0441
top acc: 0.0807 ::: bot acc: 0.0096
top acc: 0.0528 ::: bot acc: 0.0787
top acc: 0.0870 ::: bot acc: 0.0456
top acc: 0.0533 ::: bot acc: 0.0682
top acc: 0.0836 ::: bot acc: 0.0325
current epoch: 31
train loss is 0.002372
average val loss: 0.002260, accuracy: 0.0525
average test loss: 0.001568, accuracy: 0.0454
case acc: 0.039666943
case acc: 0.04049867
case acc: 0.05017861
case acc: 0.051818814
case acc: 0.045768064
case acc: 0.044234715
top acc: 0.0601 ::: bot acc: 0.0443
top acc: 0.0787 ::: bot acc: 0.0097
top acc: 0.0531 ::: bot acc: 0.0785
top acc: 0.0870 ::: bot acc: 0.0454
top acc: 0.0528 ::: bot acc: 0.0685
top acc: 0.0830 ::: bot acc: 0.0332
current epoch: 32
train loss is 0.002326
average val loss: 0.002230, accuracy: 0.0522
average test loss: 0.001554, accuracy: 0.0451
case acc: 0.0395929
case acc: 0.039079454
case acc: 0.05034425
case acc: 0.051776763
case acc: 0.045888208
case acc: 0.044154454
top acc: 0.0594 ::: bot acc: 0.0449
top acc: 0.0764 ::: bot acc: 0.0102
top acc: 0.0527 ::: bot acc: 0.0792
top acc: 0.0867 ::: bot acc: 0.0460
top acc: 0.0520 ::: bot acc: 0.0694
top acc: 0.0821 ::: bot acc: 0.0343
current epoch: 33
train loss is 0.002299
average val loss: 0.002188, accuracy: 0.0519
average test loss: 0.001533, accuracy: 0.0448
case acc: 0.039354563
case acc: 0.037398495
case acc: 0.050476644
case acc: 0.051308643
case acc: 0.04647723
case acc: 0.04385861
top acc: 0.0578 ::: bot acc: 0.0468
top acc: 0.0733 ::: bot acc: 0.0114
top acc: 0.0512 ::: bot acc: 0.0808
top acc: 0.0853 ::: bot acc: 0.0472
top acc: 0.0500 ::: bot acc: 0.0719
top acc: 0.0803 ::: bot acc: 0.0361
current epoch: 34
train loss is 0.002268
average val loss: 0.002158, accuracy: 0.0517
average test loss: 0.001517, accuracy: 0.0446
case acc: 0.039215703
case acc: 0.03619586
case acc: 0.050637312
case acc: 0.051084172
case acc: 0.04654964
case acc: 0.043691974
top acc: 0.0564 ::: bot acc: 0.0482
top acc: 0.0708 ::: bot acc: 0.0129
top acc: 0.0501 ::: bot acc: 0.0819
top acc: 0.0843 ::: bot acc: 0.0481
top acc: 0.0483 ::: bot acc: 0.0731
top acc: 0.0791 ::: bot acc: 0.0372
current epoch: 35
train loss is 0.002236
average val loss: 0.002173, accuracy: 0.0517
average test loss: 0.001524, accuracy: 0.0447
case acc: 0.03936606
case acc: 0.036289733
case acc: 0.05048775
case acc: 0.05149215
case acc: 0.04637674
case acc: 0.043932118
top acc: 0.0578 ::: bot acc: 0.0470
top acc: 0.0710 ::: bot acc: 0.0130
top acc: 0.0515 ::: bot acc: 0.0806
top acc: 0.0858 ::: bot acc: 0.0465
top acc: 0.0496 ::: bot acc: 0.0719
top acc: 0.0804 ::: bot acc: 0.0364
current epoch: 36
train loss is 0.002243
average val loss: 0.002190, accuracy: 0.0516
average test loss: 0.001533, accuracy: 0.0448
case acc: 0.039640043
case acc: 0.036498748
case acc: 0.05035197
case acc: 0.052032996
case acc: 0.046256296
case acc: 0.044153027
top acc: 0.0593 ::: bot acc: 0.0455
top acc: 0.0715 ::: bot acc: 0.0125
top acc: 0.0531 ::: bot acc: 0.0790
top acc: 0.0876 ::: bot acc: 0.0449
top acc: 0.0510 ::: bot acc: 0.0708
top acc: 0.0816 ::: bot acc: 0.0351
current epoch: 37
train loss is 0.002235
average val loss: 0.002156, accuracy: 0.0515
average test loss: 0.001516, accuracy: 0.0446
case acc: 0.039402414
case acc: 0.035601623
case acc: 0.050412226
case acc: 0.05160962
case acc: 0.046544425
case acc: 0.043897614
top acc: 0.0575 ::: bot acc: 0.0472
top acc: 0.0691 ::: bot acc: 0.0144
top acc: 0.0517 ::: bot acc: 0.0802
top acc: 0.0863 ::: bot acc: 0.0462
top acc: 0.0493 ::: bot acc: 0.0723
top acc: 0.0800 ::: bot acc: 0.0367
current epoch: 38
train loss is 0.002214
average val loss: 0.002118, accuracy: 0.0513
average test loss: 0.001500, accuracy: 0.0443
case acc: 0.039068487
case acc: 0.034447182
case acc: 0.050689943
case acc: 0.05119876
case acc: 0.046983138
case acc: 0.043489
top acc: 0.0556 ::: bot acc: 0.0490
top acc: 0.0662 ::: bot acc: 0.0168
top acc: 0.0501 ::: bot acc: 0.0820
top acc: 0.0849 ::: bot acc: 0.0477
top acc: 0.0472 ::: bot acc: 0.0745
top acc: 0.0777 ::: bot acc: 0.0387
current epoch: 39
train loss is 0.002184
average val loss: 0.002089, accuracy: 0.0512
average test loss: 0.001491, accuracy: 0.0442
case acc: 0.03894958
case acc: 0.033944312
case acc: 0.05076293
case acc: 0.050815046
case acc: 0.047432885
case acc: 0.04337361
top acc: 0.0541 ::: bot acc: 0.0506
top acc: 0.0640 ::: bot acc: 0.0192
top acc: 0.0488 ::: bot acc: 0.0831
top acc: 0.0834 ::: bot acc: 0.0493
top acc: 0.0455 ::: bot acc: 0.0762
top acc: 0.0759 ::: bot acc: 0.0408
current epoch: 40
train loss is 0.002166
average val loss: 0.002117, accuracy: 0.0512
average test loss: 0.001495, accuracy: 0.0443
case acc: 0.039099243
case acc: 0.03423893
case acc: 0.050554026
case acc: 0.05136763
case acc: 0.0469684
case acc: 0.043378737
top acc: 0.0558 ::: bot acc: 0.0490
top acc: 0.0654 ::: bot acc: 0.0176
top acc: 0.0509 ::: bot acc: 0.0811
top acc: 0.0855 ::: bot acc: 0.0471
top acc: 0.0469 ::: bot acc: 0.0747
top acc: 0.0772 ::: bot acc: 0.0391
current epoch: 41
train loss is 0.002163
average val loss: 0.002113, accuracy: 0.0512
average test loss: 0.001497, accuracy: 0.0443
case acc: 0.039161395
case acc: 0.034260664
case acc: 0.050442904
case acc: 0.05147027
case acc: 0.0470437
case acc: 0.043530963
top acc: 0.0562 ::: bot acc: 0.0486
top acc: 0.0653 ::: bot acc: 0.0181
top acc: 0.0513 ::: bot acc: 0.0806
top acc: 0.0857 ::: bot acc: 0.0467
top acc: 0.0473 ::: bot acc: 0.0745
top acc: 0.0773 ::: bot acc: 0.0393
current epoch: 42
train loss is 0.002161
average val loss: 0.002106, accuracy: 0.0510
average test loss: 0.001493, accuracy: 0.0443
case acc: 0.039257172
case acc: 0.034097597
case acc: 0.050464053
case acc: 0.051506516
case acc: 0.046858486
case acc: 0.043357857
top acc: 0.0560 ::: bot acc: 0.0489
top acc: 0.0647 ::: bot acc: 0.0187
top acc: 0.0515 ::: bot acc: 0.0806
top acc: 0.0860 ::: bot acc: 0.0467
top acc: 0.0472 ::: bot acc: 0.0743
top acc: 0.0769 ::: bot acc: 0.0394
current epoch: 43
train loss is 0.002152
average val loss: 0.002105, accuracy: 0.0510
average test loss: 0.001489, accuracy: 0.0442
case acc: 0.039079044
case acc: 0.033849314
case acc: 0.05048076
case acc: 0.051444113
case acc: 0.046983942
case acc: 0.043426298
top acc: 0.0554 ::: bot acc: 0.0492
top acc: 0.0639 ::: bot acc: 0.0195
top acc: 0.0511 ::: bot acc: 0.0808
top acc: 0.0855 ::: bot acc: 0.0468
top acc: 0.0473 ::: bot acc: 0.0745
top acc: 0.0766 ::: bot acc: 0.0400
current epoch: 44
train loss is 0.002135
average val loss: 0.002079, accuracy: 0.0510
average test loss: 0.001480, accuracy: 0.0441
case acc: 0.03892001
case acc: 0.033241946
case acc: 0.050701614
case acc: 0.051143933
case acc: 0.047246143
case acc: 0.04322067
top acc: 0.0539 ::: bot acc: 0.0508
top acc: 0.0619 ::: bot acc: 0.0212
top acc: 0.0499 ::: bot acc: 0.0821
top acc: 0.0845 ::: bot acc: 0.0483
top acc: 0.0459 ::: bot acc: 0.0756
top acc: 0.0749 ::: bot acc: 0.0415
current epoch: 45
train loss is 0.002137
average val loss: 0.002081, accuracy: 0.0509
average test loss: 0.001480, accuracy: 0.0441
case acc: 0.038872506
case acc: 0.033244114
case acc: 0.050650313
case acc: 0.051157936
case acc: 0.047290534
case acc: 0.043303534
top acc: 0.0540 ::: bot acc: 0.0508
top acc: 0.0617 ::: bot acc: 0.0216
top acc: 0.0501 ::: bot acc: 0.0819
top acc: 0.0849 ::: bot acc: 0.0475
top acc: 0.0464 ::: bot acc: 0.0754
top acc: 0.0752 ::: bot acc: 0.0414
current epoch: 46
train loss is 0.002127
average val loss: 0.002076, accuracy: 0.0509
average test loss: 0.001481, accuracy: 0.0441
case acc: 0.03893575
case acc: 0.03306346
case acc: 0.050701134
case acc: 0.051223796
case acc: 0.04725378
case acc: 0.04323252
top acc: 0.0539 ::: bot acc: 0.0509
top acc: 0.0613 ::: bot acc: 0.0219
top acc: 0.0501 ::: bot acc: 0.0821
top acc: 0.0850 ::: bot acc: 0.0476
top acc: 0.0463 ::: bot acc: 0.0755
top acc: 0.0750 ::: bot acc: 0.0415
current epoch: 47
train loss is 0.002118
average val loss: 0.002072, accuracy: 0.0509
average test loss: 0.001479, accuracy: 0.0440
case acc: 0.038989928
case acc: 0.03288906
case acc: 0.050717294
case acc: 0.05123834
case acc: 0.04723305
case acc: 0.043226756
top acc: 0.0535 ::: bot acc: 0.0516
top acc: 0.0607 ::: bot acc: 0.0227
top acc: 0.0498 ::: bot acc: 0.0822
top acc: 0.0850 ::: bot acc: 0.0476
top acc: 0.0462 ::: bot acc: 0.0755
top acc: 0.0748 ::: bot acc: 0.0416
current epoch: 48
train loss is 0.002112
average val loss: 0.002051, accuracy: 0.0508
average test loss: 0.001475, accuracy: 0.0440
case acc: 0.03890183
case acc: 0.032457538
case acc: 0.0509291
case acc: 0.050870404
case acc: 0.047539555
case acc: 0.04319105
top acc: 0.0520 ::: bot acc: 0.0531
top acc: 0.0588 ::: bot acc: 0.0245
top acc: 0.0482 ::: bot acc: 0.0839
top acc: 0.0837 ::: bot acc: 0.0489
top acc: 0.0450 ::: bot acc: 0.0768
top acc: 0.0736 ::: bot acc: 0.0430
current epoch: 49
train loss is 0.002098
average val loss: 0.002032, accuracy: 0.0509
average test loss: 0.001473, accuracy: 0.0439
case acc: 0.03886019
case acc: 0.032077868
case acc: 0.0512169
case acc: 0.050465718
case acc: 0.047856092
case acc: 0.043048505
top acc: 0.0504 ::: bot acc: 0.0548
top acc: 0.0571 ::: bot acc: 0.0262
top acc: 0.0469 ::: bot acc: 0.0852
top acc: 0.0825 ::: bot acc: 0.0498
top acc: 0.0436 ::: bot acc: 0.0781
top acc: 0.0723 ::: bot acc: 0.0442
current epoch: 50
train loss is 0.002098
average val loss: 0.002036, accuracy: 0.0508
average test loss: 0.001477, accuracy: 0.0440
case acc: 0.038860034
case acc: 0.032039646
case acc: 0.05110941
case acc: 0.05074456
case acc: 0.04781263
case acc: 0.0432361
top acc: 0.0506 ::: bot acc: 0.0546
top acc: 0.0573 ::: bot acc: 0.0260
top acc: 0.0471 ::: bot acc: 0.0849
top acc: 0.0834 ::: bot acc: 0.0492
top acc: 0.0443 ::: bot acc: 0.0777
top acc: 0.0733 ::: bot acc: 0.0434
LME_Co_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5358 5358 5358
1.7082474 -0.6288155 0.2588177 -0.21218425
Validation: 600 600 600
Testing: 750 750 750
pre-processing time: 0.0003941059112548828
the split date is 2012-01-01
train dropout: 0.4 test dropout: 0.05
net initializing with time: 0.004293918609619141
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.011003
average val loss: 0.005978, accuracy: 0.0939
average test loss: 0.004122, accuracy: 0.0715
case acc: 0.07707336
case acc: 0.15398175
case acc: 0.06686574
case acc: 0.045702
case acc: 0.039463494
case acc: 0.04587273
top acc: 0.0276 ::: bot acc: 0.1316
top acc: 0.2048 ::: bot acc: 0.0935
top acc: 0.0806 ::: bot acc: 0.0989
top acc: 0.0697 ::: bot acc: 0.0592
top acc: 0.0399 ::: bot acc: 0.0643
top acc: 0.0766 ::: bot acc: 0.0436
current epoch: 2
train loss is 0.009249
average val loss: 0.007562, accuracy: 0.0948
average test loss: 0.012311, accuracy: 0.1331
case acc: 0.058002826
case acc: 0.2666922
case acc: 0.116504595
case acc: 0.12525825
case acc: 0.102842696
case acc: 0.12934776
top acc: 0.1030 ::: bot acc: 0.0172
top acc: 0.3179 ::: bot acc: 0.2056
top acc: 0.2069 ::: bot acc: 0.0358
top acc: 0.1896 ::: bot acc: 0.0636
top acc: 0.1564 ::: bot acc: 0.0518
top acc: 0.1902 ::: bot acc: 0.0693
current epoch: 3
train loss is 0.011098
average val loss: 0.011059, accuracy: 0.1211
average test loss: 0.018057, accuracy: 0.1716
case acc: 0.09754482
case acc: 0.29966915
case acc: 0.15865938
case acc: 0.16464251
case acc: 0.14483748
case acc: 0.16419756
top acc: 0.1489 ::: bot acc: 0.0424
top acc: 0.3507 ::: bot acc: 0.2392
top acc: 0.2515 ::: bot acc: 0.0722
top acc: 0.2302 ::: bot acc: 0.1011
top acc: 0.1978 ::: bot acc: 0.0940
top acc: 0.2248 ::: bot acc: 0.1047
current epoch: 4
train loss is 0.013123
average val loss: 0.003981, accuracy: 0.0727
average test loss: 0.004744, accuracy: 0.0754
case acc: 0.040251855
case acc: 0.17298229
case acc: 0.0713727
case acc: 0.06204642
case acc: 0.04780948
case acc: 0.05778595
top acc: 0.0375 ::: bot acc: 0.0698
top acc: 0.2240 ::: bot acc: 0.1127
top acc: 0.1358 ::: bot acc: 0.0440
top acc: 0.1138 ::: bot acc: 0.0270
top acc: 0.0891 ::: bot acc: 0.0214
top acc: 0.1059 ::: bot acc: 0.0241
current epoch: 5
train loss is 0.007920
average val loss: 0.004834, accuracy: 0.0834
average test loss: 0.003003, accuracy: 0.0619
case acc: 0.064136915
case acc: 0.11300934
case acc: 0.06671603
case acc: 0.045722384
case acc: 0.037861053
case acc: 0.043948937
top acc: 0.0214 ::: bot acc: 0.1155
top acc: 0.1643 ::: bot acc: 0.0524
top acc: 0.0839 ::: bot acc: 0.0956
top acc: 0.0642 ::: bot acc: 0.0649
top acc: 0.0495 ::: bot acc: 0.0548
top acc: 0.0572 ::: bot acc: 0.0636
current epoch: 6
train loss is 0.004577
average val loss: 0.003367, accuracy: 0.0674
average test loss: 0.004021, accuracy: 0.0709
case acc: 0.039976038
case acc: 0.14805207
case acc: 0.06849748
case acc: 0.058247533
case acc: 0.05548755
case acc: 0.0550764
top acc: 0.0388 ::: bot acc: 0.0681
top acc: 0.1991 ::: bot acc: 0.0880
top acc: 0.1258 ::: bot acc: 0.0536
top acc: 0.1075 ::: bot acc: 0.0277
top acc: 0.1011 ::: bot acc: 0.0198
top acc: 0.1010 ::: bot acc: 0.0258
current epoch: 7
train loss is 0.004788
average val loss: 0.003158, accuracy: 0.0649
average test loss: 0.004599, accuracy: 0.0773
case acc: 0.039790343
case acc: 0.15334262
case acc: 0.07204898
case acc: 0.06680026
case acc: 0.06923864
case acc: 0.06246863
top acc: 0.0551 ::: bot acc: 0.0519
top acc: 0.2042 ::: bot acc: 0.0931
top acc: 0.1377 ::: bot acc: 0.0423
top acc: 0.1209 ::: bot acc: 0.0269
top acc: 0.1195 ::: bot acc: 0.0246
top acc: 0.1141 ::: bot acc: 0.0224
current epoch: 8
train loss is 0.004787
average val loss: 0.003000, accuracy: 0.0639
average test loss: 0.003456, accuracy: 0.0665
case acc: 0.03988502
case acc: 0.12676632
case acc: 0.06699718
case acc: 0.05522214
case acc: 0.05839271
case acc: 0.051555682
top acc: 0.0398 ::: bot acc: 0.0678
top acc: 0.1774 ::: bot acc: 0.0669
top acc: 0.1179 ::: bot acc: 0.0623
top acc: 0.1020 ::: bot acc: 0.0307
top acc: 0.1050 ::: bot acc: 0.0206
top acc: 0.0945 ::: bot acc: 0.0287
current epoch: 9
train loss is 0.004078
average val loss: 0.002820, accuracy: 0.0619
average test loss: 0.003342, accuracy: 0.0658
case acc: 0.039621327
case acc: 0.11909711
case acc: 0.06678561
case acc: 0.055853486
case acc: 0.06185494
case acc: 0.05158627
top acc: 0.0428 ::: bot acc: 0.0646
top acc: 0.1699 ::: bot acc: 0.0592
top acc: 0.1171 ::: bot acc: 0.0627
top acc: 0.1033 ::: bot acc: 0.0309
top acc: 0.1099 ::: bot acc: 0.0210
top acc: 0.0940 ::: bot acc: 0.0291
current epoch: 10
train loss is 0.003682
average val loss: 0.002616, accuracy: 0.0595
average test loss: 0.003549, accuracy: 0.0684
case acc: 0.039427854
case acc: 0.118787356
case acc: 0.0679055
case acc: 0.059464682
case acc: 0.06999925
case acc: 0.054865323
top acc: 0.0523 ::: bot acc: 0.0551
top acc: 0.1697 ::: bot acc: 0.0587
top acc: 0.1233 ::: bot acc: 0.0564
top acc: 0.1102 ::: bot acc: 0.0277
top acc: 0.1201 ::: bot acc: 0.0251
top acc: 0.1006 ::: bot acc: 0.0257
current epoch: 11
train loss is 0.003617
average val loss: 0.002475, accuracy: 0.0578
average test loss: 0.003611, accuracy: 0.0695
case acc: 0.03987571
case acc: 0.11549989
case acc: 0.06846661
case acc: 0.061750546
case acc: 0.07466564
case acc: 0.056715015
top acc: 0.0577 ::: bot acc: 0.0491
top acc: 0.1662 ::: bot acc: 0.0561
top acc: 0.1262 ::: bot acc: 0.0535
top acc: 0.1136 ::: bot acc: 0.0276
top acc: 0.1257 ::: bot acc: 0.0278
top acc: 0.1038 ::: bot acc: 0.0250
current epoch: 12
train loss is 0.003358
average val loss: 0.002382, accuracy: 0.0566
average test loss: 0.003395, accuracy: 0.0673
case acc: 0.039879285
case acc: 0.10650607
case acc: 0.06792773
case acc: 0.060218018
case acc: 0.07435791
case acc: 0.055208012
top acc: 0.0581 ::: bot acc: 0.0490
top acc: 0.1572 ::: bot acc: 0.0470
top acc: 0.1236 ::: bot acc: 0.0565
top acc: 0.1118 ::: bot acc: 0.0271
top acc: 0.1254 ::: bot acc: 0.0278
top acc: 0.1013 ::: bot acc: 0.0258
current epoch: 13
train loss is 0.003141
average val loss: 0.002288, accuracy: 0.0554
average test loss: 0.003349, accuracy: 0.0671
case acc: 0.04037964
case acc: 0.10137031
case acc: 0.06809573
case acc: 0.061122518
case acc: 0.07620907
case acc: 0.055432126
top acc: 0.0610 ::: bot acc: 0.0461
top acc: 0.1519 ::: bot acc: 0.0417
top acc: 0.1241 ::: bot acc: 0.0559
top acc: 0.1128 ::: bot acc: 0.0275
top acc: 0.1275 ::: bot acc: 0.0289
top acc: 0.1018 ::: bot acc: 0.0257
current epoch: 14
train loss is 0.002992
average val loss: 0.002200, accuracy: 0.0542
average test loss: 0.003306, accuracy: 0.0667
case acc: 0.04089716
case acc: 0.09645002
case acc: 0.0681908
case acc: 0.06171788
case acc: 0.07742907
case acc: 0.05569505
top acc: 0.0634 ::: bot acc: 0.0434
top acc: 0.1472 ::: bot acc: 0.0368
top acc: 0.1247 ::: bot acc: 0.0552
top acc: 0.1138 ::: bot acc: 0.0272
top acc: 0.1290 ::: bot acc: 0.0299
top acc: 0.1023 ::: bot acc: 0.0257
current epoch: 15
train loss is 0.002854
average val loss: 0.002126, accuracy: 0.0532
average test loss: 0.003278, accuracy: 0.0666
case acc: 0.04185821
case acc: 0.092097364
case acc: 0.06855429
case acc: 0.062594935
case acc: 0.07853692
case acc: 0.05588687
top acc: 0.0664 ::: bot acc: 0.0406
top acc: 0.1428 ::: bot acc: 0.0323
top acc: 0.1262 ::: bot acc: 0.0540
top acc: 0.1151 ::: bot acc: 0.0271
top acc: 0.1301 ::: bot acc: 0.0307
top acc: 0.1024 ::: bot acc: 0.0252
current epoch: 16
train loss is 0.002710
average val loss: 0.002070, accuracy: 0.0523
average test loss: 0.003150, accuracy: 0.0651
case acc: 0.042075932
case acc: 0.08588508
case acc: 0.068373896
case acc: 0.062272687
case acc: 0.077357806
case acc: 0.054881804
top acc: 0.0671 ::: bot acc: 0.0401
top acc: 0.1365 ::: bot acc: 0.0264
top acc: 0.1252 ::: bot acc: 0.0549
top acc: 0.1143 ::: bot acc: 0.0276
top acc: 0.1286 ::: bot acc: 0.0301
top acc: 0.1009 ::: bot acc: 0.0259
current epoch: 17
train loss is 0.002628
average val loss: 0.002001, accuracy: 0.0516
average test loss: 0.003296, accuracy: 0.0669
case acc: 0.043701455
case acc: 0.08544349
case acc: 0.06942334
case acc: 0.065125756
case acc: 0.080669664
case acc: 0.056982525
top acc: 0.0731 ::: bot acc: 0.0342
top acc: 0.1361 ::: bot acc: 0.0258
top acc: 0.1296 ::: bot acc: 0.0503
top acc: 0.1186 ::: bot acc: 0.0274
top acc: 0.1326 ::: bot acc: 0.0322
top acc: 0.1046 ::: bot acc: 0.0248
current epoch: 18
train loss is 0.002565
average val loss: 0.001948, accuracy: 0.0510
average test loss: 0.003372, accuracy: 0.0678
case acc: 0.045252185
case acc: 0.08391064
case acc: 0.07044152
case acc: 0.06718512
case acc: 0.08210086
case acc: 0.057625014
top acc: 0.0772 ::: bot acc: 0.0303
top acc: 0.1346 ::: bot acc: 0.0246
top acc: 0.1329 ::: bot acc: 0.0472
top acc: 0.1216 ::: bot acc: 0.0272
top acc: 0.1344 ::: bot acc: 0.0331
top acc: 0.1060 ::: bot acc: 0.0239
current epoch: 19
train loss is 0.002513
average val loss: 0.001907, accuracy: 0.0506
average test loss: 0.003460, accuracy: 0.0688
case acc: 0.046761855
case acc: 0.08296082
case acc: 0.07153818
case acc: 0.06917368
case acc: 0.08358127
case acc: 0.058792368
top acc: 0.0813 ::: bot acc: 0.0267
top acc: 0.1336 ::: bot acc: 0.0239
top acc: 0.1362 ::: bot acc: 0.0437
top acc: 0.1241 ::: bot acc: 0.0276
top acc: 0.1358 ::: bot acc: 0.0345
top acc: 0.1081 ::: bot acc: 0.0235
current epoch: 20
train loss is 0.002467
average val loss: 0.001875, accuracy: 0.0503
average test loss: 0.003484, accuracy: 0.0690
case acc: 0.04781591
case acc: 0.08081395
case acc: 0.07234438
case acc: 0.070411965
case acc: 0.08396966
case acc: 0.058859933
top acc: 0.0838 ::: bot acc: 0.0242
top acc: 0.1311 ::: bot acc: 0.0224
top acc: 0.1384 ::: bot acc: 0.0415
top acc: 0.1260 ::: bot acc: 0.0278
top acc: 0.1362 ::: bot acc: 0.0345
top acc: 0.1084 ::: bot acc: 0.0231
current epoch: 21
train loss is 0.002376
average val loss: 0.001837, accuracy: 0.0498
average test loss: 0.003413, accuracy: 0.0683
case acc: 0.048307747
case acc: 0.077521175
case acc: 0.07246795
case acc: 0.07060985
case acc: 0.08257409
case acc: 0.058274917
top acc: 0.0850 ::: bot acc: 0.0235
top acc: 0.1271 ::: bot acc: 0.0203
top acc: 0.1388 ::: bot acc: 0.0409
top acc: 0.1262 ::: bot acc: 0.0279
top acc: 0.1346 ::: bot acc: 0.0336
top acc: 0.1073 ::: bot acc: 0.0234
current epoch: 22
train loss is 0.002320
average val loss: 0.001811, accuracy: 0.0494
average test loss: 0.003401, accuracy: 0.0682
case acc: 0.04882428
case acc: 0.07544304
case acc: 0.0729443
case acc: 0.071186006
case acc: 0.08237335
case acc: 0.05813835
top acc: 0.0867 ::: bot acc: 0.0219
top acc: 0.1247 ::: bot acc: 0.0194
top acc: 0.1402 ::: bot acc: 0.0396
top acc: 0.1270 ::: bot acc: 0.0279
top acc: 0.1343 ::: bot acc: 0.0335
top acc: 0.1068 ::: bot acc: 0.0236
current epoch: 23
train loss is 0.002263
average val loss: 0.001793, accuracy: 0.0493
average test loss: 0.003518, accuracy: 0.0695
case acc: 0.050930668
case acc: 0.075239696
case acc: 0.07470924
case acc: 0.07322546
case acc: 0.08393533
case acc: 0.058993313
top acc: 0.0906 ::: bot acc: 0.0201
top acc: 0.1242 ::: bot acc: 0.0190
top acc: 0.1440 ::: bot acc: 0.0374
top acc: 0.1295 ::: bot acc: 0.0285
top acc: 0.1361 ::: bot acc: 0.0347
top acc: 0.1082 ::: bot acc: 0.0234
current epoch: 24
train loss is 0.002247
average val loss: 0.001778, accuracy: 0.0492
average test loss: 0.003609, accuracy: 0.0705
case acc: 0.052555494
case acc: 0.07477952
case acc: 0.07610218
case acc: 0.07504236
case acc: 0.084862806
case acc: 0.059673272
top acc: 0.0938 ::: bot acc: 0.0189
top acc: 0.1238 ::: bot acc: 0.0192
top acc: 0.1469 ::: bot acc: 0.0355
top acc: 0.1321 ::: bot acc: 0.0290
top acc: 0.1372 ::: bot acc: 0.0354
top acc: 0.1095 ::: bot acc: 0.0232
current epoch: 25
train loss is 0.002212
average val loss: 0.001762, accuracy: 0.0491
average test loss: 0.003605, accuracy: 0.0705
case acc: 0.05329586
case acc: 0.073019855
case acc: 0.07658389
case acc: 0.07569714
case acc: 0.084574565
case acc: 0.05962239
top acc: 0.0950 ::: bot acc: 0.0187
top acc: 0.1214 ::: bot acc: 0.0183
top acc: 0.1481 ::: bot acc: 0.0346
top acc: 0.1330 ::: bot acc: 0.0291
top acc: 0.1370 ::: bot acc: 0.0350
top acc: 0.1096 ::: bot acc: 0.0230
current epoch: 26
train loss is 0.002190
average val loss: 0.001758, accuracy: 0.0491
average test loss: 0.003718, accuracy: 0.0717
case acc: 0.055184484
case acc: 0.072978884
case acc: 0.07813663
case acc: 0.07746038
case acc: 0.086380824
case acc: 0.060307126
top acc: 0.0980 ::: bot acc: 0.0179
top acc: 0.1214 ::: bot acc: 0.0182
top acc: 0.1510 ::: bot acc: 0.0333
top acc: 0.1353 ::: bot acc: 0.0297
top acc: 0.1390 ::: bot acc: 0.0365
top acc: 0.1105 ::: bot acc: 0.0227
current epoch: 27
train loss is 0.002200
average val loss: 0.001756, accuracy: 0.0493
average test loss: 0.003814, accuracy: 0.0728
case acc: 0.056717977
case acc: 0.072716154
case acc: 0.07954277
case acc: 0.07914893
case acc: 0.08756451
case acc: 0.061077513
top acc: 0.1007 ::: bot acc: 0.0174
top acc: 0.1211 ::: bot acc: 0.0183
top acc: 0.1536 ::: bot acc: 0.0322
top acc: 0.1374 ::: bot acc: 0.0304
top acc: 0.1399 ::: bot acc: 0.0377
top acc: 0.1118 ::: bot acc: 0.0226
current epoch: 28
train loss is 0.002170
average val loss: 0.001758, accuracy: 0.0494
average test loss: 0.003912, accuracy: 0.0738
case acc: 0.058331005
case acc: 0.07250136
case acc: 0.08092836
case acc: 0.08076677
case acc: 0.08891361
case acc: 0.061599508
top acc: 0.1031 ::: bot acc: 0.0172
top acc: 0.1208 ::: bot acc: 0.0183
top acc: 0.1562 ::: bot acc: 0.0312
top acc: 0.1396 ::: bot acc: 0.0309
top acc: 0.1414 ::: bot acc: 0.0390
top acc: 0.1128 ::: bot acc: 0.0222
current epoch: 29
train loss is 0.002160
average val loss: 0.001748, accuracy: 0.0493
average test loss: 0.003910, accuracy: 0.0738
case acc: 0.058835194
case acc: 0.0712188
case acc: 0.081463754
case acc: 0.081048205
case acc: 0.089004196
case acc: 0.0615192
top acc: 0.1038 ::: bot acc: 0.0172
top acc: 0.1190 ::: bot acc: 0.0180
top acc: 0.1569 ::: bot acc: 0.0312
top acc: 0.1399 ::: bot acc: 0.0310
top acc: 0.1416 ::: bot acc: 0.0388
top acc: 0.1123 ::: bot acc: 0.0225
current epoch: 30
train loss is 0.002124
average val loss: 0.001719, accuracy: 0.0488
average test loss: 0.003799, accuracy: 0.0726
case acc: 0.058215838
case acc: 0.06868885
case acc: 0.08073946
case acc: 0.080109924
case acc: 0.08746271
case acc: 0.060415376
top acc: 0.1029 ::: bot acc: 0.0174
top acc: 0.1153 ::: bot acc: 0.0177
top acc: 0.1558 ::: bot acc: 0.0314
top acc: 0.1389 ::: bot acc: 0.0305
top acc: 0.1401 ::: bot acc: 0.0374
top acc: 0.1108 ::: bot acc: 0.0227
current epoch: 31
train loss is 0.002067
average val loss: 0.001697, accuracy: 0.0485
average test loss: 0.003742, accuracy: 0.0720
case acc: 0.05790074
case acc: 0.06714517
case acc: 0.08062987
case acc: 0.07986382
case acc: 0.087088056
case acc: 0.059663642
top acc: 0.1023 ::: bot acc: 0.0175
top acc: 0.1128 ::: bot acc: 0.0181
top acc: 0.1556 ::: bot acc: 0.0315
top acc: 0.1384 ::: bot acc: 0.0308
top acc: 0.1395 ::: bot acc: 0.0373
top acc: 0.1096 ::: bot acc: 0.0229
current epoch: 32
train loss is 0.002048
average val loss: 0.001676, accuracy: 0.0481
average test loss: 0.003637, accuracy: 0.0709
case acc: 0.05699248
case acc: 0.06515264
case acc: 0.079922155
case acc: 0.07890805
case acc: 0.0855806
case acc: 0.058641054
top acc: 0.1008 ::: bot acc: 0.0176
top acc: 0.1098 ::: bot acc: 0.0182
top acc: 0.1545 ::: bot acc: 0.0317
top acc: 0.1371 ::: bot acc: 0.0303
top acc: 0.1380 ::: bot acc: 0.0359
top acc: 0.1077 ::: bot acc: 0.0235
current epoch: 33
train loss is 0.002022
average val loss: 0.001657, accuracy: 0.0478
average test loss: 0.003511, accuracy: 0.0695
case acc: 0.055965647
case acc: 0.06305673
case acc: 0.079219535
case acc: 0.07764848
case acc: 0.0836608
case acc: 0.057292696
top acc: 0.0993 ::: bot acc: 0.0178
top acc: 0.1066 ::: bot acc: 0.0186
top acc: 0.1528 ::: bot acc: 0.0326
top acc: 0.1353 ::: bot acc: 0.0299
top acc: 0.1357 ::: bot acc: 0.0346
top acc: 0.1055 ::: bot acc: 0.0239
current epoch: 34
train loss is 0.001979
average val loss: 0.001654, accuracy: 0.0478
average test loss: 0.003530, accuracy: 0.0697
case acc: 0.056283887
case acc: 0.06258204
case acc: 0.079637215
case acc: 0.07821411
case acc: 0.08380789
case acc: 0.057381358
top acc: 0.0998 ::: bot acc: 0.0176
top acc: 0.1058 ::: bot acc: 0.0188
top acc: 0.1538 ::: bot acc: 0.0320
top acc: 0.1362 ::: bot acc: 0.0300
top acc: 0.1358 ::: bot acc: 0.0348
top acc: 0.1058 ::: bot acc: 0.0238
current epoch: 35
train loss is 0.001968
average val loss: 0.001658, accuracy: 0.0480
average test loss: 0.003614, accuracy: 0.0706
case acc: 0.05735051
case acc: 0.06278505
case acc: 0.08074475
case acc: 0.07971108
case acc: 0.08480797
case acc: 0.05819986
top acc: 0.1015 ::: bot acc: 0.0175
top acc: 0.1062 ::: bot acc: 0.0184
top acc: 0.1558 ::: bot acc: 0.0314
top acc: 0.1379 ::: bot acc: 0.0307
top acc: 0.1369 ::: bot acc: 0.0357
top acc: 0.1071 ::: bot acc: 0.0236
current epoch: 36
train loss is 0.001959
average val loss: 0.001647, accuracy: 0.0478
average test loss: 0.003544, accuracy: 0.0698
case acc: 0.056782737
case acc: 0.061642922
case acc: 0.080362804
case acc: 0.07907014
case acc: 0.08360025
case acc: 0.057434384
top acc: 0.1007 ::: bot acc: 0.0177
top acc: 0.1044 ::: bot acc: 0.0190
top acc: 0.1550 ::: bot acc: 0.0316
top acc: 0.1372 ::: bot acc: 0.0304
top acc: 0.1355 ::: bot acc: 0.0347
top acc: 0.1058 ::: bot acc: 0.0240
current epoch: 37
train loss is 0.001951
average val loss: 0.001648, accuracy: 0.0479
average test loss: 0.003568, accuracy: 0.0701
case acc: 0.05711437
case acc: 0.06138686
case acc: 0.08079342
case acc: 0.079597995
case acc: 0.083843105
case acc: 0.05763629
top acc: 0.1010 ::: bot acc: 0.0177
top acc: 0.1040 ::: bot acc: 0.0191
top acc: 0.1558 ::: bot acc: 0.0312
top acc: 0.1378 ::: bot acc: 0.0306
top acc: 0.1359 ::: bot acc: 0.0347
top acc: 0.1061 ::: bot acc: 0.0240
current epoch: 38
train loss is 0.001940
average val loss: 0.001652, accuracy: 0.0480
average test loss: 0.003629, accuracy: 0.0707
case acc: 0.057700764
case acc: 0.061536446
case acc: 0.08166919
case acc: 0.080674276
case acc: 0.08467783
case acc: 0.05809115
top acc: 0.1020 ::: bot acc: 0.0174
top acc: 0.1042 ::: bot acc: 0.0190
top acc: 0.1573 ::: bot acc: 0.0309
top acc: 0.1392 ::: bot acc: 0.0310
top acc: 0.1369 ::: bot acc: 0.0353
top acc: 0.1069 ::: bot acc: 0.0235
current epoch: 39
train loss is 0.001957
average val loss: 0.001639, accuracy: 0.0477
average test loss: 0.003543, accuracy: 0.0697
case acc: 0.056916602
case acc: 0.06018864
case acc: 0.081173085
case acc: 0.079769395
case acc: 0.08328911
case acc: 0.057158787
top acc: 0.1007 ::: bot acc: 0.0178
top acc: 0.1019 ::: bot acc: 0.0195
top acc: 0.1565 ::: bot acc: 0.0311
top acc: 0.1381 ::: bot acc: 0.0307
top acc: 0.1353 ::: bot acc: 0.0343
top acc: 0.1052 ::: bot acc: 0.0241
current epoch: 40
train loss is 0.001920
average val loss: 0.001639, accuracy: 0.0478
average test loss: 0.003568, accuracy: 0.0700
case acc: 0.057195958
case acc: 0.060154833
case acc: 0.081766404
case acc: 0.080116436
case acc: 0.083564505
case acc: 0.05725072
top acc: 0.1013 ::: bot acc: 0.0175
top acc: 0.1017 ::: bot acc: 0.0195
top acc: 0.1574 ::: bot acc: 0.0310
top acc: 0.1386 ::: bot acc: 0.0307
top acc: 0.1357 ::: bot acc: 0.0344
top acc: 0.1053 ::: bot acc: 0.0242
current epoch: 41
train loss is 0.001933
average val loss: 0.001646, accuracy: 0.0480
average test loss: 0.003685, accuracy: 0.0713
case acc: 0.058385786
case acc: 0.060960416
case acc: 0.08304124
case acc: 0.08185263
case acc: 0.08524427
case acc: 0.058132034
top acc: 0.1031 ::: bot acc: 0.0173
top acc: 0.1032 ::: bot acc: 0.0191
top acc: 0.1599 ::: bot acc: 0.0301
top acc: 0.1407 ::: bot acc: 0.0315
top acc: 0.1375 ::: bot acc: 0.0357
top acc: 0.1069 ::: bot acc: 0.0236
current epoch: 42
train loss is 0.001916
average val loss: 0.001643, accuracy: 0.0479
average test loss: 0.003650, accuracy: 0.0709
case acc: 0.057982497
case acc: 0.060220044
case acc: 0.08300231
case acc: 0.081646115
case acc: 0.084655054
case acc: 0.05766266
top acc: 0.1025 ::: bot acc: 0.0175
top acc: 0.1020 ::: bot acc: 0.0192
top acc: 0.1597 ::: bot acc: 0.0300
top acc: 0.1406 ::: bot acc: 0.0313
top acc: 0.1369 ::: bot acc: 0.0353
top acc: 0.1060 ::: bot acc: 0.0240
current epoch: 43
train loss is 0.001914
average val loss: 0.001638, accuracy: 0.0479
average test loss: 0.003673, accuracy: 0.0711
case acc: 0.058069352
case acc: 0.060233034
case acc: 0.0833936
case acc: 0.08199071
case acc: 0.08500421
case acc: 0.057707917
top acc: 0.1027 ::: bot acc: 0.0172
top acc: 0.1021 ::: bot acc: 0.0193
top acc: 0.1604 ::: bot acc: 0.0299
top acc: 0.1411 ::: bot acc: 0.0315
top acc: 0.1372 ::: bot acc: 0.0355
top acc: 0.1062 ::: bot acc: 0.0239
current epoch: 44
train loss is 0.001903
average val loss: 0.001643, accuracy: 0.0479
average test loss: 0.003715, accuracy: 0.0716
case acc: 0.058471892
case acc: 0.060344443
case acc: 0.083818495
case acc: 0.08264508
case acc: 0.08593966
case acc: 0.058134653
top acc: 0.1033 ::: bot acc: 0.0173
top acc: 0.1022 ::: bot acc: 0.0194
top acc: 0.1611 ::: bot acc: 0.0295
top acc: 0.1418 ::: bot acc: 0.0319
top acc: 0.1383 ::: bot acc: 0.0363
top acc: 0.1069 ::: bot acc: 0.0237
current epoch: 45
train loss is 0.001909
average val loss: 0.001635, accuracy: 0.0478
average test loss: 0.003684, accuracy: 0.0712
case acc: 0.05802653
case acc: 0.05964807
case acc: 0.083655916
case acc: 0.08231046
case acc: 0.08587409
case acc: 0.057677854
top acc: 0.1026 ::: bot acc: 0.0174
top acc: 0.1010 ::: bot acc: 0.0195
top acc: 0.1609 ::: bot acc: 0.0296
top acc: 0.1413 ::: bot acc: 0.0317
top acc: 0.1381 ::: bot acc: 0.0364
top acc: 0.1062 ::: bot acc: 0.0237
current epoch: 46
train loss is 0.001887
average val loss: 0.001627, accuracy: 0.0477
average test loss: 0.003628, accuracy: 0.0706
case acc: 0.05733833
case acc: 0.058835145
case acc: 0.08316511
case acc: 0.08185756
case acc: 0.08510387
case acc: 0.057186294
top acc: 0.1014 ::: bot acc: 0.0177
top acc: 0.0997 ::: bot acc: 0.0199
top acc: 0.1601 ::: bot acc: 0.0298
top acc: 0.1407 ::: bot acc: 0.0315
top acc: 0.1372 ::: bot acc: 0.0357
top acc: 0.1053 ::: bot acc: 0.0243
current epoch: 47
train loss is 0.001877
average val loss: 0.001626, accuracy: 0.0477
average test loss: 0.003640, accuracy: 0.0707
case acc: 0.057318065
case acc: 0.0586154
case acc: 0.08338991
case acc: 0.08208619
case acc: 0.085454516
case acc: 0.057416383
top acc: 0.1014 ::: bot acc: 0.0174
top acc: 0.0992 ::: bot acc: 0.0201
top acc: 0.1602 ::: bot acc: 0.0300
top acc: 0.1410 ::: bot acc: 0.0317
top acc: 0.1377 ::: bot acc: 0.0359
top acc: 0.1055 ::: bot acc: 0.0242
current epoch: 48
train loss is 0.001872
average val loss: 0.001614, accuracy: 0.0474
average test loss: 0.003517, accuracy: 0.0693
case acc: 0.05572857
case acc: 0.05706565
case acc: 0.082116485
case acc: 0.080806606
case acc: 0.08377263
case acc: 0.05646652
top acc: 0.0989 ::: bot acc: 0.0178
top acc: 0.0966 ::: bot acc: 0.0209
top acc: 0.1582 ::: bot acc: 0.0306
top acc: 0.1395 ::: bot acc: 0.0310
top acc: 0.1359 ::: bot acc: 0.0347
top acc: 0.1038 ::: bot acc: 0.0248
current epoch: 49
train loss is 0.001852
average val loss: 0.001611, accuracy: 0.0474
average test loss: 0.003517, accuracy: 0.0693
case acc: 0.055587582
case acc: 0.05660542
case acc: 0.08192255
case acc: 0.08093935
case acc: 0.08420451
case acc: 0.05667932
top acc: 0.0988 ::: bot acc: 0.0179
top acc: 0.0959 ::: bot acc: 0.0210
top acc: 0.1578 ::: bot acc: 0.0306
top acc: 0.1398 ::: bot acc: 0.0311
top acc: 0.1362 ::: bot acc: 0.0350
top acc: 0.1041 ::: bot acc: 0.0247
current epoch: 50
train loss is 0.001838
average val loss: 0.001600, accuracy: 0.0471
average test loss: 0.003352, accuracy: 0.0675
case acc: 0.05371464
case acc: 0.054691054
case acc: 0.08017592
case acc: 0.07884249
case acc: 0.08215899
case acc: 0.05516213
top acc: 0.0955 ::: bot acc: 0.0186
top acc: 0.0922 ::: bot acc: 0.0227
top acc: 0.1547 ::: bot acc: 0.0316
top acc: 0.1371 ::: bot acc: 0.0304
top acc: 0.1338 ::: bot acc: 0.0336
top acc: 0.1016 ::: bot acc: 0.0254
LME_Co_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5346 5346 5346
1.7082474 -0.6288155 0.25454274 -0.21218425
Validation: 594 594 594
Testing: 768 768 768
pre-processing time: 0.00029969215393066406
the split date is 2012-07-01
train dropout: 0.4 test dropout: 0.05
net initializing with time: 0.003673076629638672
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.010265
average val loss: 0.004257, accuracy: 0.0742
average test loss: 0.004529, accuracy: 0.0789
case acc: 0.100792885
case acc: 0.1394434
case acc: 0.07770042
case acc: 0.03464905
case acc: 0.063777186
case acc: 0.05686431
top acc: 0.0638 ::: bot acc: 0.1458
top acc: 0.1838 ::: bot acc: 0.0966
top acc: 0.0268 ::: bot acc: 0.1402
top acc: 0.0117 ::: bot acc: 0.0671
top acc: 0.0246 ::: bot acc: 0.1059
top acc: 0.0881 ::: bot acc: 0.0619
current epoch: 2
train loss is 0.008281
average val loss: 0.009694, accuracy: 0.1128
average test loss: 0.009965, accuracy: 0.1145
case acc: 0.063395254
case acc: 0.2516281
case acc: 0.07700026
case acc: 0.09147189
case acc: 0.0692013
case acc: 0.13401744
top acc: 0.1383 ::: bot acc: 0.0205
top acc: 0.2956 ::: bot acc: 0.2093
top acc: 0.1486 ::: bot acc: 0.0231
top acc: 0.1271 ::: bot acc: 0.0527
top acc: 0.1271 ::: bot acc: 0.0268
top acc: 0.2006 ::: bot acc: 0.0681
current epoch: 3
train loss is 0.010285
average val loss: 0.018945, accuracy: 0.1761
average test loss: 0.019190, accuracy: 0.1764
case acc: 0.12517588
case acc: 0.31239006
case acc: 0.13808745
case acc: 0.15901054
case acc: 0.13251798
case acc: 0.19120134
top acc: 0.2123 ::: bot acc: 0.0556
top acc: 0.3562 ::: bot acc: 0.2700
top acc: 0.2220 ::: bot acc: 0.0591
top acc: 0.1945 ::: bot acc: 0.1204
top acc: 0.1962 ::: bot acc: 0.0782
top acc: 0.2620 ::: bot acc: 0.1156
current epoch: 4
train loss is 0.013448
average val loss: 0.006824, accuracy: 0.0924
average test loss: 0.007044, accuracy: 0.0946
case acc: 0.059367597
case acc: 0.2102009
case acc: 0.06867711
case acc: 0.067742616
case acc: 0.055604864
case acc: 0.106038935
top acc: 0.1248 ::: bot acc: 0.0317
top acc: 0.2541 ::: bot acc: 0.1674
top acc: 0.1310 ::: bot acc: 0.0335
top acc: 0.1024 ::: bot acc: 0.0308
top acc: 0.1096 ::: bot acc: 0.0216
top acc: 0.1669 ::: bot acc: 0.0507
current epoch: 5
train loss is 0.009421
average val loss: 0.003088, accuracy: 0.0627
average test loss: 0.003267, accuracy: 0.0665
case acc: 0.079603344
case acc: 0.11207751
case acc: 0.06947411
case acc: 0.030465314
case acc: 0.053155154
case acc: 0.054411057
top acc: 0.0632 ::: bot acc: 0.1148
top acc: 0.1562 ::: bot acc: 0.0694
top acc: 0.0421 ::: bot acc: 0.1207
top acc: 0.0154 ::: bot acc: 0.0594
top acc: 0.0331 ::: bot acc: 0.0858
top acc: 0.0799 ::: bot acc: 0.0677
current epoch: 6
train loss is 0.004654
average val loss: 0.003329, accuracy: 0.0632
average test loss: 0.003524, accuracy: 0.0664
case acc: 0.060038026
case acc: 0.13635162
case acc: 0.061657242
case acc: 0.02984284
case acc: 0.04285097
case acc: 0.06742743
top acc: 0.0785 ::: bot acc: 0.0779
top acc: 0.1799 ::: bot acc: 0.0936
top acc: 0.0734 ::: bot acc: 0.0902
top acc: 0.0490 ::: bot acc: 0.0249
top acc: 0.0739 ::: bot acc: 0.0441
top acc: 0.1151 ::: bot acc: 0.0394
current epoch: 7
train loss is 0.004280
average val loss: 0.004195, accuracy: 0.0723
average test loss: 0.004360, accuracy: 0.0744
case acc: 0.056560695
case acc: 0.15260217
case acc: 0.060438022
case acc: 0.0421642
case acc: 0.0512188
case acc: 0.08336662
top acc: 0.1054 ::: bot acc: 0.0504
top acc: 0.1963 ::: bot acc: 0.1099
top acc: 0.0952 ::: bot acc: 0.0673
top acc: 0.0725 ::: bot acc: 0.0140
top acc: 0.1027 ::: bot acc: 0.0217
top acc: 0.1384 ::: bot acc: 0.0399
current epoch: 8
train loss is 0.004516
average val loss: 0.003479, accuracy: 0.0658
average test loss: 0.003668, accuracy: 0.0680
case acc: 0.05670879
case acc: 0.13370734
case acc: 0.06048861
case acc: 0.035262287
case acc: 0.047190234
case acc: 0.07472716
top acc: 0.0971 ::: bot acc: 0.0586
top acc: 0.1775 ::: bot acc: 0.0910
top acc: 0.0833 ::: bot acc: 0.0800
top acc: 0.0613 ::: bot acc: 0.0159
top acc: 0.0959 ::: bot acc: 0.0236
top acc: 0.1268 ::: bot acc: 0.0372
current epoch: 9
train loss is 0.004021
average val loss: 0.003258, accuracy: 0.0639
average test loss: 0.003428, accuracy: 0.0661
case acc: 0.05677931
case acc: 0.12343705
case acc: 0.06070148
case acc: 0.0342907
case acc: 0.04804147
case acc: 0.07311486
top acc: 0.0973 ::: bot acc: 0.0587
top acc: 0.1669 ::: bot acc: 0.0808
top acc: 0.0797 ::: bot acc: 0.0834
top acc: 0.0594 ::: bot acc: 0.0170
top acc: 0.0977 ::: bot acc: 0.0227
top acc: 0.1247 ::: bot acc: 0.0371
current epoch: 10
train loss is 0.003649
average val loss: 0.003142, accuracy: 0.0631
average test loss: 0.003308, accuracy: 0.0652
case acc: 0.056739252
case acc: 0.11620993
case acc: 0.060661346
case acc: 0.034370307
case acc: 0.049998827
case acc: 0.073300004
top acc: 0.0999 ::: bot acc: 0.0559
top acc: 0.1598 ::: bot acc: 0.0735
top acc: 0.0791 ::: bot acc: 0.0841
top acc: 0.0595 ::: bot acc: 0.0165
top acc: 0.1009 ::: bot acc: 0.0219
top acc: 0.1245 ::: bot acc: 0.0373
current epoch: 11
train loss is 0.003353
average val loss: 0.003200, accuracy: 0.0641
average test loss: 0.003372, accuracy: 0.0663
case acc: 0.056855656
case acc: 0.11374059
case acc: 0.0605754
case acc: 0.036707677
case acc: 0.053936593
case acc: 0.07575086
top acc: 0.1064 ::: bot acc: 0.0496
top acc: 0.1569 ::: bot acc: 0.0716
top acc: 0.0826 ::: bot acc: 0.0804
top acc: 0.0637 ::: bot acc: 0.0152
top acc: 0.1073 ::: bot acc: 0.0209
top acc: 0.1284 ::: bot acc: 0.0373
current epoch: 12
train loss is 0.003167
average val loss: 0.003251, accuracy: 0.0650
average test loss: 0.003413, accuracy: 0.0669
case acc: 0.057167724
case acc: 0.11047065
case acc: 0.060622003
case acc: 0.038577743
case acc: 0.057169486
case acc: 0.07766352
top acc: 0.1116 ::: bot acc: 0.0442
top acc: 0.1539 ::: bot acc: 0.0677
top acc: 0.0852 ::: bot acc: 0.0782
top acc: 0.0668 ::: bot acc: 0.0140
top acc: 0.1121 ::: bot acc: 0.0209
top acc: 0.1309 ::: bot acc: 0.0377
current epoch: 13
train loss is 0.003083
average val loss: 0.003168, accuracy: 0.0643
average test loss: 0.003332, accuracy: 0.0662
case acc: 0.05730283
case acc: 0.104895696
case acc: 0.060273495
case acc: 0.03894689
case acc: 0.058073163
case acc: 0.077512376
top acc: 0.1139 ::: bot acc: 0.0417
top acc: 0.1481 ::: bot acc: 0.0627
top acc: 0.0854 ::: bot acc: 0.0777
top acc: 0.0674 ::: bot acc: 0.0141
top acc: 0.1136 ::: bot acc: 0.0207
top acc: 0.1308 ::: bot acc: 0.0379
current epoch: 14
train loss is 0.002934
average val loss: 0.003156, accuracy: 0.0644
average test loss: 0.003322, accuracy: 0.0662
case acc: 0.05781427
case acc: 0.10085198
case acc: 0.060328692
case acc: 0.040181614
case acc: 0.05982923
case acc: 0.078455955
top acc: 0.1173 ::: bot acc: 0.0381
top acc: 0.1441 ::: bot acc: 0.0584
top acc: 0.0868 ::: bot acc: 0.0763
top acc: 0.0694 ::: bot acc: 0.0139
top acc: 0.1159 ::: bot acc: 0.0213
top acc: 0.1320 ::: bot acc: 0.0381
current epoch: 15
train loss is 0.002787
average val loss: 0.003100, accuracy: 0.0639
average test loss: 0.003269, accuracy: 0.0657
case acc: 0.058179352
case acc: 0.096030995
case acc: 0.06037371
case acc: 0.04090622
case acc: 0.060173664
case acc: 0.07849343
top acc: 0.1197 ::: bot acc: 0.0359
top acc: 0.1395 ::: bot acc: 0.0536
top acc: 0.0874 ::: bot acc: 0.0754
top acc: 0.0704 ::: bot acc: 0.0138
top acc: 0.1166 ::: bot acc: 0.0210
top acc: 0.1317 ::: bot acc: 0.0380
current epoch: 16
train loss is 0.002683
average val loss: 0.003084, accuracy: 0.0638
average test loss: 0.003263, accuracy: 0.0656
case acc: 0.058708675
case acc: 0.09231286
case acc: 0.060365617
case acc: 0.04211799
case acc: 0.061058395
case acc: 0.079119675
top acc: 0.1228 ::: bot acc: 0.0329
top acc: 0.1357 ::: bot acc: 0.0498
top acc: 0.0896 ::: bot acc: 0.0735
top acc: 0.0723 ::: bot acc: 0.0140
top acc: 0.1179 ::: bot acc: 0.0213
top acc: 0.1331 ::: bot acc: 0.0381
current epoch: 17
train loss is 0.002558
average val loss: 0.003035, accuracy: 0.0632
average test loss: 0.003212, accuracy: 0.0650
case acc: 0.05909408
case acc: 0.08809006
case acc: 0.06052855
case acc: 0.042559315
case acc: 0.06115392
case acc: 0.07876905
top acc: 0.1248 ::: bot acc: 0.0307
top acc: 0.1314 ::: bot acc: 0.0456
top acc: 0.0904 ::: bot acc: 0.0731
top acc: 0.0726 ::: bot acc: 0.0141
top acc: 0.1179 ::: bot acc: 0.0215
top acc: 0.1325 ::: bot acc: 0.0381
current epoch: 18
train loss is 0.002481
average val loss: 0.003065, accuracy: 0.0635
average test loss: 0.003241, accuracy: 0.0654
case acc: 0.059929583
case acc: 0.08593817
case acc: 0.06052407
case acc: 0.044547774
case acc: 0.06207888
case acc: 0.07962108
top acc: 0.1280 ::: bot acc: 0.0273
top acc: 0.1292 ::: bot acc: 0.0436
top acc: 0.0928 ::: bot acc: 0.0702
top acc: 0.0753 ::: bot acc: 0.0150
top acc: 0.1188 ::: bot acc: 0.0218
top acc: 0.1337 ::: bot acc: 0.0382
current epoch: 19
train loss is 0.002439
average val loss: 0.003193, accuracy: 0.0650
average test loss: 0.003372, accuracy: 0.0671
case acc: 0.061632015
case acc: 0.08603728
case acc: 0.060976222
case acc: 0.04760147
case acc: 0.0644234
case acc: 0.08169642
top acc: 0.1330 ::: bot acc: 0.0229
top acc: 0.1294 ::: bot acc: 0.0437
top acc: 0.0973 ::: bot acc: 0.0659
top acc: 0.0789 ::: bot acc: 0.0165
top acc: 0.1219 ::: bot acc: 0.0228
top acc: 0.1364 ::: bot acc: 0.0391
current epoch: 20
train loss is 0.002375
average val loss: 0.003221, accuracy: 0.0654
average test loss: 0.003396, accuracy: 0.0673
case acc: 0.062480308
case acc: 0.08399764
case acc: 0.06118855
case acc: 0.049150106
case acc: 0.064756826
case acc: 0.08209582
top acc: 0.1358 ::: bot acc: 0.0197
top acc: 0.1273 ::: bot acc: 0.0419
top acc: 0.0998 ::: bot acc: 0.0632
top acc: 0.0811 ::: bot acc: 0.0171
top acc: 0.1225 ::: bot acc: 0.0229
top acc: 0.1370 ::: bot acc: 0.0392
current epoch: 21
train loss is 0.002325
average val loss: 0.003320, accuracy: 0.0665
average test loss: 0.003492, accuracy: 0.0684
case acc: 0.064185224
case acc: 0.08343351
case acc: 0.06150668
case acc: 0.051837437
case acc: 0.06610826
case acc: 0.08357285
top acc: 0.1395 ::: bot acc: 0.0170
top acc: 0.1266 ::: bot acc: 0.0414
top acc: 0.1032 ::: bot acc: 0.0598
top acc: 0.0844 ::: bot acc: 0.0186
top acc: 0.1241 ::: bot acc: 0.0237
top acc: 0.1389 ::: bot acc: 0.0397
current epoch: 22
train loss is 0.002299
average val loss: 0.003324, accuracy: 0.0666
average test loss: 0.003499, accuracy: 0.0684
case acc: 0.06499992
case acc: 0.081424594
case acc: 0.061750256
case acc: 0.05262821
case acc: 0.06629274
case acc: 0.083536446
top acc: 0.1416 ::: bot acc: 0.0158
top acc: 0.1246 ::: bot acc: 0.0394
top acc: 0.1048 ::: bot acc: 0.0581
top acc: 0.0853 ::: bot acc: 0.0191
top acc: 0.1245 ::: bot acc: 0.0237
top acc: 0.1389 ::: bot acc: 0.0395
current epoch: 23
train loss is 0.002284
average val loss: 0.003389, accuracy: 0.0674
average test loss: 0.003556, accuracy: 0.0692
case acc: 0.06624526
case acc: 0.08047501
case acc: 0.062202662
case acc: 0.05456423
case acc: 0.06725198
case acc: 0.0841803
top acc: 0.1441 ::: bot acc: 0.0141
top acc: 0.1236 ::: bot acc: 0.0385
top acc: 0.1071 ::: bot acc: 0.0556
top acc: 0.0874 ::: bot acc: 0.0206
top acc: 0.1255 ::: bot acc: 0.0244
top acc: 0.1397 ::: bot acc: 0.0397
current epoch: 24
train loss is 0.002246
average val loss: 0.003397, accuracy: 0.0675
average test loss: 0.003568, accuracy: 0.0692
case acc: 0.066983834
case acc: 0.07867724
case acc: 0.062459204
case acc: 0.05549915
case acc: 0.06755453
case acc: 0.084205106
top acc: 0.1459 ::: bot acc: 0.0129
top acc: 0.1217 ::: bot acc: 0.0370
top acc: 0.1088 ::: bot acc: 0.0539
top acc: 0.0883 ::: bot acc: 0.0213
top acc: 0.1260 ::: bot acc: 0.0247
top acc: 0.1397 ::: bot acc: 0.0397
current epoch: 25
train loss is 0.002198
average val loss: 0.003371, accuracy: 0.0672
average test loss: 0.003533, accuracy: 0.0688
case acc: 0.06735639
case acc: 0.0762961
case acc: 0.06257313
case acc: 0.055789877
case acc: 0.067016065
case acc: 0.08374679
top acc: 0.1463 ::: bot acc: 0.0129
top acc: 0.1190 ::: bot acc: 0.0348
top acc: 0.1094 ::: bot acc: 0.0534
top acc: 0.0889 ::: bot acc: 0.0214
top acc: 0.1252 ::: bot acc: 0.0245
top acc: 0.1391 ::: bot acc: 0.0394
current epoch: 26
train loss is 0.002162
average val loss: 0.003299, accuracy: 0.0664
average test loss: 0.003461, accuracy: 0.0679
case acc: 0.06732941
case acc: 0.07346317
case acc: 0.06261781
case acc: 0.055441745
case acc: 0.06591305
case acc: 0.08263979
top acc: 0.1461 ::: bot acc: 0.0132
top acc: 0.1159 ::: bot acc: 0.0327
top acc: 0.1094 ::: bot acc: 0.0535
top acc: 0.0885 ::: bot acc: 0.0212
top acc: 0.1238 ::: bot acc: 0.0238
top acc: 0.1377 ::: bot acc: 0.0390
current epoch: 27
train loss is 0.002121
average val loss: 0.003322, accuracy: 0.0667
average test loss: 0.003489, accuracy: 0.0682
case acc: 0.06818408
case acc: 0.07247192
case acc: 0.0628613
case acc: 0.056525692
case acc: 0.066079706
case acc: 0.0828861
top acc: 0.1477 ::: bot acc: 0.0123
top acc: 0.1149 ::: bot acc: 0.0319
top acc: 0.1112 ::: bot acc: 0.0516
top acc: 0.0897 ::: bot acc: 0.0219
top acc: 0.1240 ::: bot acc: 0.0239
top acc: 0.1379 ::: bot acc: 0.0391
current epoch: 28
train loss is 0.002096
average val loss: 0.003407, accuracy: 0.0677
average test loss: 0.003562, accuracy: 0.0690
case acc: 0.06926955
case acc: 0.072423786
case acc: 0.06326687
case acc: 0.05845072
case acc: 0.06709316
case acc: 0.08354507
top acc: 0.1497 ::: bot acc: 0.0114
top acc: 0.1148 ::: bot acc: 0.0317
top acc: 0.1136 ::: bot acc: 0.0491
top acc: 0.0918 ::: bot acc: 0.0235
top acc: 0.1253 ::: bot acc: 0.0243
top acc: 0.1389 ::: bot acc: 0.0396
current epoch: 29
train loss is 0.002092
average val loss: 0.003397, accuracy: 0.0675
average test loss: 0.003549, accuracy: 0.0688
case acc: 0.06977532
case acc: 0.070880614
case acc: 0.06352156
case acc: 0.05878035
case acc: 0.0667817
case acc: 0.08321663
top acc: 0.1506 ::: bot acc: 0.0114
top acc: 0.1130 ::: bot acc: 0.0305
top acc: 0.1144 ::: bot acc: 0.0483
top acc: 0.0922 ::: bot acc: 0.0238
top acc: 0.1247 ::: bot acc: 0.0244
top acc: 0.1383 ::: bot acc: 0.0393
current epoch: 30
train loss is 0.002092
average val loss: 0.003499, accuracy: 0.0687
average test loss: 0.003649, accuracy: 0.0699
case acc: 0.071243554
case acc: 0.0713042
case acc: 0.06402426
case acc: 0.060834534
case acc: 0.068027645
case acc: 0.08425231
top acc: 0.1528 ::: bot acc: 0.0110
top acc: 0.1134 ::: bot acc: 0.0309
top acc: 0.1173 ::: bot acc: 0.0455
top acc: 0.0945 ::: bot acc: 0.0252
top acc: 0.1266 ::: bot acc: 0.0248
top acc: 0.1398 ::: bot acc: 0.0397
current epoch: 31
train loss is 0.002081
average val loss: 0.003527, accuracy: 0.0690
average test loss: 0.003675, accuracy: 0.0703
case acc: 0.0718658
case acc: 0.070732936
case acc: 0.064330384
case acc: 0.06178979
case acc: 0.068280146
case acc: 0.08451715
top acc: 0.1538 ::: bot acc: 0.0108
top acc: 0.1130 ::: bot acc: 0.0305
top acc: 0.1187 ::: bot acc: 0.0439
top acc: 0.0955 ::: bot acc: 0.0259
top acc: 0.1266 ::: bot acc: 0.0250
top acc: 0.1399 ::: bot acc: 0.0400
current epoch: 32
train loss is 0.002062
average val loss: 0.003626, accuracy: 0.0701
average test loss: 0.003770, accuracy: 0.0713
case acc: 0.07323584
case acc: 0.07108423
case acc: 0.065000474
case acc: 0.06360358
case acc: 0.06964395
case acc: 0.08542441
top acc: 0.1558 ::: bot acc: 0.0110
top acc: 0.1132 ::: bot acc: 0.0308
top acc: 0.1212 ::: bot acc: 0.0417
top acc: 0.0974 ::: bot acc: 0.0275
top acc: 0.1284 ::: bot acc: 0.0257
top acc: 0.1412 ::: bot acc: 0.0403
current epoch: 33
train loss is 0.002068
average val loss: 0.003605, accuracy: 0.0698
average test loss: 0.003745, accuracy: 0.0710
case acc: 0.07327836
case acc: 0.069711074
case acc: 0.0650595
case acc: 0.063660696
case acc: 0.069508
case acc: 0.08487512
top acc: 0.1559 ::: bot acc: 0.0111
top acc: 0.1116 ::: bot acc: 0.0298
top acc: 0.1215 ::: bot acc: 0.0414
top acc: 0.0974 ::: bot acc: 0.0276
top acc: 0.1281 ::: bot acc: 0.0258
top acc: 0.1405 ::: bot acc: 0.0399
current epoch: 34
train loss is 0.002057
average val loss: 0.003642, accuracy: 0.0702
average test loss: 0.003779, accuracy: 0.0714
case acc: 0.07366128
case acc: 0.06919892
case acc: 0.06539092
case acc: 0.06438478
case acc: 0.070498794
case acc: 0.08532831
top acc: 0.1565 ::: bot acc: 0.0110
top acc: 0.1112 ::: bot acc: 0.0294
top acc: 0.1224 ::: bot acc: 0.0405
top acc: 0.0984 ::: bot acc: 0.0281
top acc: 0.1292 ::: bot acc: 0.0265
top acc: 0.1411 ::: bot acc: 0.0403
current epoch: 35
train loss is 0.002047
average val loss: 0.003651, accuracy: 0.0703
average test loss: 0.003790, accuracy: 0.0715
case acc: 0.07390759
case acc: 0.06797838
case acc: 0.065505564
case acc: 0.064938284
case acc: 0.071227804
case acc: 0.085417084
top acc: 0.1569 ::: bot acc: 0.0112
top acc: 0.1099 ::: bot acc: 0.0283
top acc: 0.1229 ::: bot acc: 0.0400
top acc: 0.0991 ::: bot acc: 0.0285
top acc: 0.1303 ::: bot acc: 0.0267
top acc: 0.1411 ::: bot acc: 0.0403
current epoch: 36
train loss is 0.002045
average val loss: 0.003594, accuracy: 0.0697
average test loss: 0.003728, accuracy: 0.0708
case acc: 0.07312908
case acc: 0.06627759
case acc: 0.06532949
case acc: 0.064190775
case acc: 0.07083905
case acc: 0.08481391
top acc: 0.1558 ::: bot acc: 0.0109
top acc: 0.1082 ::: bot acc: 0.0271
top acc: 0.1220 ::: bot acc: 0.0411
top acc: 0.0983 ::: bot acc: 0.0279
top acc: 0.1298 ::: bot acc: 0.0266
top acc: 0.1404 ::: bot acc: 0.0400
current epoch: 37
train loss is 0.002003
average val loss: 0.003394, accuracy: 0.0676
average test loss: 0.003530, accuracy: 0.0684
case acc: 0.0709145
case acc: 0.062240355
case acc: 0.06447096
case acc: 0.061571687
case acc: 0.06861737
case acc: 0.082393765
top acc: 0.1526 ::: bot acc: 0.0111
top acc: 0.1032 ::: bot acc: 0.0244
top acc: 0.1187 ::: bot acc: 0.0444
top acc: 0.0955 ::: bot acc: 0.0259
top acc: 0.1271 ::: bot acc: 0.0251
top acc: 0.1374 ::: bot acc: 0.0390
current epoch: 38
train loss is 0.001946
average val loss: 0.003163, accuracy: 0.0651
average test loss: 0.003289, accuracy: 0.0654
case acc: 0.06831679
case acc: 0.057740003
case acc: 0.06351167
case acc: 0.05830346
case acc: 0.06533748
case acc: 0.07946883
top acc: 0.1480 ::: bot acc: 0.0122
top acc: 0.0978 ::: bot acc: 0.0217
top acc: 0.1144 ::: bot acc: 0.0484
top acc: 0.0917 ::: bot acc: 0.0234
top acc: 0.1233 ::: bot acc: 0.0232
top acc: 0.1334 ::: bot acc: 0.0382
current epoch: 39
train loss is 0.001885
average val loss: 0.003043, accuracy: 0.0637
average test loss: 0.003168, accuracy: 0.0639
case acc: 0.06706466
case acc: 0.054908648
case acc: 0.06314865
case acc: 0.056488898
case acc: 0.06366676
case acc: 0.07788928
top acc: 0.1459 ::: bot acc: 0.0128
top acc: 0.0947 ::: bot acc: 0.0198
top acc: 0.1124 ::: bot acc: 0.0505
top acc: 0.0896 ::: bot acc: 0.0221
top acc: 0.1211 ::: bot acc: 0.0222
top acc: 0.1312 ::: bot acc: 0.0378
current epoch: 40
train loss is 0.001868
average val loss: 0.002963, accuracy: 0.0628
average test loss: 0.003090, accuracy: 0.0629
case acc: 0.066164166
case acc: 0.05312932
case acc: 0.06285001
case acc: 0.055701658
case acc: 0.062412754
case acc: 0.077037826
top acc: 0.1441 ::: bot acc: 0.0138
top acc: 0.0923 ::: bot acc: 0.0189
top acc: 0.1110 ::: bot acc: 0.0519
top acc: 0.0886 ::: bot acc: 0.0216
top acc: 0.1195 ::: bot acc: 0.0218
top acc: 0.1301 ::: bot acc: 0.0376
current epoch: 41
train loss is 0.001852
average val loss: 0.002885, accuracy: 0.0620
average test loss: 0.003010, accuracy: 0.0618
case acc: 0.06537777
case acc: 0.051232174
case acc: 0.062615566
case acc: 0.05465094
case acc: 0.06097584
case acc: 0.076163426
top acc: 0.1424 ::: bot acc: 0.0147
top acc: 0.0899 ::: bot acc: 0.0180
top acc: 0.1095 ::: bot acc: 0.0534
top acc: 0.0875 ::: bot acc: 0.0205
top acc: 0.1176 ::: bot acc: 0.0212
top acc: 0.1289 ::: bot acc: 0.0373
current epoch: 42
train loss is 0.001831
average val loss: 0.002834, accuracy: 0.0614
average test loss: 0.002953, accuracy: 0.0611
case acc: 0.064728186
case acc: 0.049782004
case acc: 0.06240245
case acc: 0.05389373
case acc: 0.060133334
case acc: 0.075520076
top acc: 0.1410 ::: bot acc: 0.0155
top acc: 0.0882 ::: bot acc: 0.0173
top acc: 0.1083 ::: bot acc: 0.0544
top acc: 0.0867 ::: bot acc: 0.0201
top acc: 0.1166 ::: bot acc: 0.0211
top acc: 0.1280 ::: bot acc: 0.0372
current epoch: 43
train loss is 0.001822
average val loss: 0.002775, accuracy: 0.0607
average test loss: 0.002897, accuracy: 0.0603
case acc: 0.06411761
case acc: 0.048210055
case acc: 0.062204972
case acc: 0.05308328
case acc: 0.05939488
case acc: 0.07489522
top acc: 0.1396 ::: bot acc: 0.0163
top acc: 0.0861 ::: bot acc: 0.0164
top acc: 0.1072 ::: bot acc: 0.0558
top acc: 0.0858 ::: bot acc: 0.0195
top acc: 0.1156 ::: bot acc: 0.0209
top acc: 0.1271 ::: bot acc: 0.0371
current epoch: 44
train loss is 0.001806
average val loss: 0.002721, accuracy: 0.0601
average test loss: 0.002844, accuracy: 0.0596
case acc: 0.063517846
case acc: 0.04699728
case acc: 0.06205844
case acc: 0.05228477
case acc: 0.058712147
case acc: 0.07409139
top acc: 0.1385 ::: bot acc: 0.0170
top acc: 0.0847 ::: bot acc: 0.0159
top acc: 0.1059 ::: bot acc: 0.0569
top acc: 0.0847 ::: bot acc: 0.0191
top acc: 0.1145 ::: bot acc: 0.0207
top acc: 0.1259 ::: bot acc: 0.0370
current epoch: 45
train loss is 0.001800
average val loss: 0.002613, accuracy: 0.0588
average test loss: 0.002737, accuracy: 0.0581
case acc: 0.062452335
case acc: 0.044723757
case acc: 0.06174232
case acc: 0.05039874
case acc: 0.05688093
case acc: 0.072661735
top acc: 0.1357 ::: bot acc: 0.0193
top acc: 0.0817 ::: bot acc: 0.0149
top acc: 0.1038 ::: bot acc: 0.0594
top acc: 0.0825 ::: bot acc: 0.0179
top acc: 0.1120 ::: bot acc: 0.0204
top acc: 0.1238 ::: bot acc: 0.0370
current epoch: 46
train loss is 0.001793
average val loss: 0.002570, accuracy: 0.0583
average test loss: 0.002697, accuracy: 0.0576
case acc: 0.062072538
case acc: 0.043813758
case acc: 0.061579276
case acc: 0.04983709
case acc: 0.056011796
case acc: 0.0721653
top acc: 0.1348 ::: bot acc: 0.0200
top acc: 0.0807 ::: bot acc: 0.0145
top acc: 0.1029 ::: bot acc: 0.0602
top acc: 0.0819 ::: bot acc: 0.0175
top acc: 0.1106 ::: bot acc: 0.0205
top acc: 0.1230 ::: bot acc: 0.0370
current epoch: 47
train loss is 0.001784
average val loss: 0.002512, accuracy: 0.0576
average test loss: 0.002638, accuracy: 0.0567
case acc: 0.061562967
case acc: 0.042646445
case acc: 0.061362103
case acc: 0.048730496
case acc: 0.054923873
case acc: 0.07123854
top acc: 0.1334 ::: bot acc: 0.0215
top acc: 0.0788 ::: bot acc: 0.0143
top acc: 0.1017 ::: bot acc: 0.0613
top acc: 0.0804 ::: bot acc: 0.0169
top acc: 0.1091 ::: bot acc: 0.0205
top acc: 0.1216 ::: bot acc: 0.0372
current epoch: 48
train loss is 0.001775
average val loss: 0.002499, accuracy: 0.0574
average test loss: 0.002622, accuracy: 0.0565
case acc: 0.06155926
case acc: 0.0423064
case acc: 0.061339386
case acc: 0.04868296
case acc: 0.05444973
case acc: 0.07091864
top acc: 0.1331 ::: bot acc: 0.0219
top acc: 0.0784 ::: bot acc: 0.0141
top acc: 0.1015 ::: bot acc: 0.0615
top acc: 0.0805 ::: bot acc: 0.0169
top acc: 0.1083 ::: bot acc: 0.0205
top acc: 0.1210 ::: bot acc: 0.0373
current epoch: 49
train loss is 0.001771
average val loss: 0.002548, accuracy: 0.0580
average test loss: 0.002670, accuracy: 0.0572
case acc: 0.061861835
case acc: 0.043185793
case acc: 0.06159558
case acc: 0.049601927
case acc: 0.055121448
case acc: 0.07172082
top acc: 0.1343 ::: bot acc: 0.0206
top acc: 0.0798 ::: bot acc: 0.0144
top acc: 0.1029 ::: bot acc: 0.0602
top acc: 0.0817 ::: bot acc: 0.0174
top acc: 0.1093 ::: bot acc: 0.0204
top acc: 0.1222 ::: bot acc: 0.0373
current epoch: 50
train loss is 0.001773
average val loss: 0.002637, accuracy: 0.0590
average test loss: 0.002761, accuracy: 0.0584
case acc: 0.0626931
case acc: 0.04485369
case acc: 0.06197673
case acc: 0.051529013
case acc: 0.056569114
case acc: 0.07301686
top acc: 0.1364 ::: bot acc: 0.0186
top acc: 0.0819 ::: bot acc: 0.0149
top acc: 0.1053 ::: bot acc: 0.0578
top acc: 0.0839 ::: bot acc: 0.0186
top acc: 0.1115 ::: bot acc: 0.0205
top acc: 0.1244 ::: bot acc: 0.0370

		{"drop_out": 0.4, "drop_out_mc": 0.1, "repeat_mc": 50, "hidden": 30, "embedding_size": 5, "batch": 512, "lag": 3}
LME_Co_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5328 5328 5328
1.7082474 -0.6288155 0.29835227 -0.25048217
Validation: 594 594 594
Testing: 774 774 774
pre-processing time: 0.00024771690368652344
the split date is 2010-07-01
train dropout: 0.4 test dropout: 0.1
net initializing with time: 0.0037381649017333984
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.010699
average val loss: 0.005594, accuracy: 0.0931
average test loss: 0.008034, accuracy: 0.1153
case acc: 0.16447662
case acc: 0.08647536
case acc: 0.12726536
case acc: 0.10615536
case acc: 0.13151515
case acc: 0.07620515
top acc: 0.1328 ::: bot acc: 0.2015
top acc: 0.1477 ::: bot acc: 0.0238
top acc: 0.0572 ::: bot acc: 0.1934
top acc: 0.0645 ::: bot acc: 0.1437
top acc: 0.1012 ::: bot acc: 0.1608
top acc: 0.0331 ::: bot acc: 0.1243
current epoch: 2
train loss is 0.009046
average val loss: 0.006366, accuracy: 0.0832
average test loss: 0.004522, accuracy: 0.0654
case acc: 0.037737444
case acc: 0.19778088
case acc: 0.05170017
case acc: 0.031625398
case acc: 0.025915142
case acc: 0.04756713
top acc: 0.0094 ::: bot acc: 0.0737
top acc: 0.2608 ::: bot acc: 0.1301
top acc: 0.0739 ::: bot acc: 0.0655
top acc: 0.0557 ::: bot acc: 0.0238
top acc: 0.0188 ::: bot acc: 0.0426
top acc: 0.0811 ::: bot acc: 0.0200
current epoch: 3
train loss is 0.010133
average val loss: 0.015904, accuracy: 0.1569
average test loss: 0.010924, accuracy: 0.1226
case acc: 0.0652401
case acc: 0.27834097
case acc: 0.097305596
case acc: 0.10217439
case acc: 0.07481884
case acc: 0.117847554
top acc: 0.0902 ::: bot acc: 0.0406
top acc: 0.3416 ::: bot acc: 0.2104
top acc: 0.1678 ::: bot acc: 0.0334
top acc: 0.1440 ::: bot acc: 0.0634
top acc: 0.1056 ::: bot acc: 0.0457
top acc: 0.1613 ::: bot acc: 0.0698
current epoch: 4
train loss is 0.013608
average val loss: 0.008283, accuracy: 0.1031
average test loss: 0.005317, accuracy: 0.0720
case acc: 0.027530827
case acc: 0.21061917
case acc: 0.05881986
case acc: 0.04721061
case acc: 0.028199071
case acc: 0.059510615
top acc: 0.0377 ::: bot acc: 0.0327
top acc: 0.2741 ::: bot acc: 0.1424
top acc: 0.1123 ::: bot acc: 0.0293
top acc: 0.0868 ::: bot acc: 0.0142
top acc: 0.0537 ::: bot acc: 0.0104
top acc: 0.1008 ::: bot acc: 0.0172
current epoch: 5
train loss is 0.011512
average val loss: 0.003188, accuracy: 0.0660
average test loss: 0.004156, accuracy: 0.0798
case acc: 0.10175272
case acc: 0.09001643
case acc: 0.08305803
case acc: 0.06753752
case acc: 0.080448896
case acc: 0.056279093
top acc: 0.0701 ::: bot acc: 0.1393
top acc: 0.1515 ::: bot acc: 0.0253
top acc: 0.0307 ::: bot acc: 0.1404
top acc: 0.0261 ::: bot acc: 0.1061
top acc: 0.0494 ::: bot acc: 0.1101
top acc: 0.0146 ::: bot acc: 0.1041
current epoch: 6
train loss is 0.005762
average val loss: 0.002815, accuracy: 0.0573
average test loss: 0.003124, accuracy: 0.0659
case acc: 0.079228155
case acc: 0.09815447
case acc: 0.072433114
case acc: 0.05006742
case acc: 0.0539387
case acc: 0.041433565
top acc: 0.0476 ::: bot acc: 0.1169
top acc: 0.1610 ::: bot acc: 0.0315
top acc: 0.0320 ::: bot acc: 0.1238
top acc: 0.0134 ::: bot acc: 0.0862
top acc: 0.0267 ::: bot acc: 0.0815
top acc: 0.0130 ::: bot acc: 0.0825
current epoch: 7
train loss is 0.004840
average val loss: 0.003374, accuracy: 0.0598
average test loss: 0.002531, accuracy: 0.0529
case acc: 0.043918293
case acc: 0.122743286
case acc: 0.056584794
case acc: 0.03352308
case acc: 0.02584644
case acc: 0.034726102
top acc: 0.0134 ::: bot acc: 0.0808
top acc: 0.1862 ::: bot acc: 0.0551
top acc: 0.0474 ::: bot acc: 0.0922
top acc: 0.0282 ::: bot acc: 0.0537
top acc: 0.0193 ::: bot acc: 0.0432
top acc: 0.0439 ::: bot acc: 0.0495
current epoch: 8
train loss is 0.005223
average val loss: 0.002912, accuracy: 0.0558
average test loss: 0.002356, accuracy: 0.0522
case acc: 0.04772742
case acc: 0.10792737
case acc: 0.06011148
case acc: 0.035499018
case acc: 0.027280856
case acc: 0.034594696
top acc: 0.0171 ::: bot acc: 0.0849
top acc: 0.1716 ::: bot acc: 0.0396
top acc: 0.0416 ::: bot acc: 0.1001
top acc: 0.0208 ::: bot acc: 0.0608
top acc: 0.0180 ::: bot acc: 0.0458
top acc: 0.0367 ::: bot acc: 0.0571
current epoch: 9
train loss is 0.004898
average val loss: 0.002489, accuracy: 0.0521
average test loss: 0.002268, accuracy: 0.0527
case acc: 0.05412107
case acc: 0.09265565
case acc: 0.06510616
case acc: 0.03926338
case acc: 0.02968215
case acc: 0.03560709
top acc: 0.0228 ::: bot acc: 0.0917
top acc: 0.1547 ::: bot acc: 0.0275
top acc: 0.0363 ::: bot acc: 0.1104
top acc: 0.0145 ::: bot acc: 0.0693
top acc: 0.0160 ::: bot acc: 0.0505
top acc: 0.0275 ::: bot acc: 0.0656
current epoch: 10
train loss is 0.004320
average val loss: 0.002291, accuracy: 0.0505
average test loss: 0.002156, accuracy: 0.0516
case acc: 0.053466994
case acc: 0.08430565
case acc: 0.0667826
case acc: 0.040013157
case acc: 0.028908113
case acc: 0.03617059
top acc: 0.0221 ::: bot acc: 0.0908
top acc: 0.1449 ::: bot acc: 0.0215
top acc: 0.0354 ::: bot acc: 0.1128
top acc: 0.0134 ::: bot acc: 0.0713
top acc: 0.0171 ::: bot acc: 0.0486
top acc: 0.0261 ::: bot acc: 0.0678
current epoch: 11
train loss is 0.004021
average val loss: 0.002309, accuracy: 0.0508
average test loss: 0.001992, accuracy: 0.0485
case acc: 0.046179764
case acc: 0.08431899
case acc: 0.06395352
case acc: 0.037002787
case acc: 0.024510523
case acc: 0.035156168
top acc: 0.0157 ::: bot acc: 0.0835
top acc: 0.1454 ::: bot acc: 0.0215
top acc: 0.0373 ::: bot acc: 0.1082
top acc: 0.0173 ::: bot acc: 0.0648
top acc: 0.0220 ::: bot acc: 0.0397
top acc: 0.0323 ::: bot acc: 0.0620
current epoch: 12
train loss is 0.003922
average val loss: 0.002355, accuracy: 0.0514
average test loss: 0.001861, accuracy: 0.0461
case acc: 0.039356787
case acc: 0.08387607
case acc: 0.061595503
case acc: 0.03462565
case acc: 0.022400834
case acc: 0.034721103
top acc: 0.0104 ::: bot acc: 0.0758
top acc: 0.1448 ::: bot acc: 0.0215
top acc: 0.0398 ::: bot acc: 0.1034
top acc: 0.0220 ::: bot acc: 0.0587
top acc: 0.0286 ::: bot acc: 0.0328
top acc: 0.0378 ::: bot acc: 0.0563
current epoch: 13
train loss is 0.003802
average val loss: 0.002175, accuracy: 0.0496
average test loss: 0.001792, accuracy: 0.0456
case acc: 0.040235218
case acc: 0.076985046
case acc: 0.063069046
case acc: 0.035693333
case acc: 0.022651909
case acc: 0.035157435
top acc: 0.0111 ::: bot acc: 0.0766
top acc: 0.1359 ::: bot acc: 0.0180
top acc: 0.0380 ::: bot acc: 0.1064
top acc: 0.0198 ::: bot acc: 0.0617
top acc: 0.0270 ::: bot acc: 0.0340
top acc: 0.0349 ::: bot acc: 0.0598
current epoch: 14
train loss is 0.003645
average val loss: 0.002088, accuracy: 0.0486
average test loss: 0.001717, accuracy: 0.0447
case acc: 0.0389053
case acc: 0.073050946
case acc: 0.06314093
case acc: 0.03548117
case acc: 0.02245536
case acc: 0.035098195
top acc: 0.0105 ::: bot acc: 0.0747
top acc: 0.1308 ::: bot acc: 0.0172
top acc: 0.0381 ::: bot acc: 0.1064
top acc: 0.0197 ::: bot acc: 0.0612
top acc: 0.0280 ::: bot acc: 0.0334
top acc: 0.0348 ::: bot acc: 0.0598
current epoch: 15
train loss is 0.003494
average val loss: 0.001998, accuracy: 0.0476
average test loss: 0.001650, accuracy: 0.0439
case acc: 0.037649382
case acc: 0.069742896
case acc: 0.0632021
case acc: 0.03562572
case acc: 0.022487182
case acc: 0.034847666
top acc: 0.0097 ::: bot acc: 0.0730
top acc: 0.1256 ::: bot acc: 0.0178
top acc: 0.0381 ::: bot acc: 0.1062
top acc: 0.0204 ::: bot acc: 0.0612
top acc: 0.0284 ::: bot acc: 0.0328
top acc: 0.0336 ::: bot acc: 0.0604
current epoch: 16
train loss is 0.003375
average val loss: 0.002022, accuracy: 0.0481
average test loss: 0.001572, accuracy: 0.0426
case acc: 0.034053333
case acc: 0.06867231
case acc: 0.061654277
case acc: 0.034342427
case acc: 0.02215889
case acc: 0.03491296
top acc: 0.0078 ::: bot acc: 0.0691
top acc: 0.1235 ::: bot acc: 0.0178
top acc: 0.0404 ::: bot acc: 0.1032
top acc: 0.0233 ::: bot acc: 0.0577
top acc: 0.0320 ::: bot acc: 0.0295
top acc: 0.0368 ::: bot acc: 0.0576
current epoch: 17
train loss is 0.003339
average val loss: 0.002036, accuracy: 0.0484
average test loss: 0.001513, accuracy: 0.0417
case acc: 0.03145198
case acc: 0.06802466
case acc: 0.06028196
case acc: 0.033564582
case acc: 0.022409353
case acc: 0.034513474
top acc: 0.0080 ::: bot acc: 0.0649
top acc: 0.1225 ::: bot acc: 0.0184
top acc: 0.0418 ::: bot acc: 0.1001
top acc: 0.0263 ::: bot acc: 0.0548
top acc: 0.0340 ::: bot acc: 0.0277
top acc: 0.0383 ::: bot acc: 0.0554
current epoch: 18
train loss is 0.003267
average val loss: 0.001977, accuracy: 0.0476
average test loss: 0.001465, accuracy: 0.0412
case acc: 0.030890275
case acc: 0.06565013
case acc: 0.060284153
case acc: 0.033673707
case acc: 0.022225553
case acc: 0.034626205
top acc: 0.0084 ::: bot acc: 0.0639
top acc: 0.1179 ::: bot acc: 0.0201
top acc: 0.0426 ::: bot acc: 0.0998
top acc: 0.0266 ::: bot acc: 0.0547
top acc: 0.0336 ::: bot acc: 0.0279
top acc: 0.0378 ::: bot acc: 0.0561
current epoch: 19
train loss is 0.003176
average val loss: 0.001980, accuracy: 0.0476
average test loss: 0.001424, accuracy: 0.0406
case acc: 0.029416446
case acc: 0.06449466
case acc: 0.05935135
case acc: 0.033248376
case acc: 0.022310412
case acc: 0.03496898
top acc: 0.0090 ::: bot acc: 0.0615
top acc: 0.1160 ::: bot acc: 0.0209
top acc: 0.0436 ::: bot acc: 0.0980
top acc: 0.0285 ::: bot acc: 0.0533
top acc: 0.0344 ::: bot acc: 0.0270
top acc: 0.0386 ::: bot acc: 0.0557
current epoch: 20
train loss is 0.003154
average val loss: 0.001948, accuracy: 0.0472
average test loss: 0.001387, accuracy: 0.0402
case acc: 0.028781414
case acc: 0.06302838
case acc: 0.05898554
case acc: 0.033268034
case acc: 0.022240331
case acc: 0.034660835
top acc: 0.0099 ::: bot acc: 0.0599
top acc: 0.1129 ::: bot acc: 0.0226
top acc: 0.0449 ::: bot acc: 0.0967
top acc: 0.0290 ::: bot acc: 0.0528
top acc: 0.0332 ::: bot acc: 0.0282
top acc: 0.0371 ::: bot acc: 0.0570
current epoch: 21
train loss is 0.003106
average val loss: 0.002059, accuracy: 0.0488
average test loss: 0.001346, accuracy: 0.0394
case acc: 0.026707962
case acc: 0.06377279
case acc: 0.05675307
case acc: 0.032168515
case acc: 0.022561908
case acc: 0.034705404
top acc: 0.0142 ::: bot acc: 0.0545
top acc: 0.1143 ::: bot acc: 0.0218
top acc: 0.0489 ::: bot acc: 0.0912
top acc: 0.0333 ::: bot acc: 0.0477
top acc: 0.0365 ::: bot acc: 0.0249
top acc: 0.0409 ::: bot acc: 0.0533
current epoch: 22
train loss is 0.003105
average val loss: 0.002154, accuracy: 0.0501
average test loss: 0.001326, accuracy: 0.0391
case acc: 0.025680004
case acc: 0.064031675
case acc: 0.05545183
case acc: 0.031703282
case acc: 0.022931665
case acc: 0.034965936
top acc: 0.0184 ::: bot acc: 0.0506
top acc: 0.1153 ::: bot acc: 0.0213
top acc: 0.0531 ::: bot acc: 0.0873
top acc: 0.0366 ::: bot acc: 0.0446
top acc: 0.0388 ::: bot acc: 0.0228
top acc: 0.0434 ::: bot acc: 0.0512
current epoch: 23
train loss is 0.003106
average val loss: 0.002306, accuracy: 0.0523
average test loss: 0.001322, accuracy: 0.0391
case acc: 0.024878038
case acc: 0.06530237
case acc: 0.053808633
case acc: 0.03127915
case acc: 0.023650823
case acc: 0.035417993
top acc: 0.0237 ::: bot acc: 0.0452
top acc: 0.1174 ::: bot acc: 0.0207
top acc: 0.0580 ::: bot acc: 0.0817
top acc: 0.0414 ::: bot acc: 0.0395
top acc: 0.0420 ::: bot acc: 0.0193
top acc: 0.0469 ::: bot acc: 0.0480
current epoch: 24
train loss is 0.003101
average val loss: 0.002255, accuracy: 0.0516
average test loss: 0.001291, accuracy: 0.0387
case acc: 0.02485243
case acc: 0.06371832
case acc: 0.053700365
case acc: 0.03133956
case acc: 0.023344325
case acc: 0.035202034
top acc: 0.0237 ::: bot acc: 0.0450
top acc: 0.1142 ::: bot acc: 0.0218
top acc: 0.0585 ::: bot acc: 0.0814
top acc: 0.0410 ::: bot acc: 0.0400
top acc: 0.0409 ::: bot acc: 0.0205
top acc: 0.0453 ::: bot acc: 0.0491
current epoch: 25
train loss is 0.003072
average val loss: 0.002233, accuracy: 0.0513
average test loss: 0.001269, accuracy: 0.0385
case acc: 0.024915317
case acc: 0.06260233
case acc: 0.053427875
case acc: 0.031484492
case acc: 0.023294248
case acc: 0.03505264
top acc: 0.0244 ::: bot acc: 0.0445
top acc: 0.1121 ::: bot acc: 0.0231
top acc: 0.0591 ::: bot acc: 0.0805
top acc: 0.0412 ::: bot acc: 0.0402
top acc: 0.0405 ::: bot acc: 0.0208
top acc: 0.0445 ::: bot acc: 0.0499
current epoch: 26
train loss is 0.003033
average val loss: 0.002286, accuracy: 0.0520
average test loss: 0.001263, accuracy: 0.0384
case acc: 0.02506043
case acc: 0.062365856
case acc: 0.053127956
case acc: 0.031338885
case acc: 0.023526609
case acc: 0.03503472
top acc: 0.0266 ::: bot acc: 0.0424
top acc: 0.1117 ::: bot acc: 0.0231
top acc: 0.0620 ::: bot acc: 0.0783
top acc: 0.0431 ::: bot acc: 0.0382
top acc: 0.0419 ::: bot acc: 0.0196
top acc: 0.0452 ::: bot acc: 0.0492
current epoch: 27
train loss is 0.003030
average val loss: 0.002231, accuracy: 0.0512
average test loss: 0.001240, accuracy: 0.0382
case acc: 0.024973433
case acc: 0.06105664
case acc: 0.053137343
case acc: 0.031405885
case acc: 0.023424618
case acc: 0.035101723
top acc: 0.0260 ::: bot acc: 0.0429
top acc: 0.1086 ::: bot acc: 0.0252
top acc: 0.0616 ::: bot acc: 0.0784
top acc: 0.0422 ::: bot acc: 0.0390
top acc: 0.0408 ::: bot acc: 0.0208
top acc: 0.0437 ::: bot acc: 0.0510
current epoch: 28
train loss is 0.002984
average val loss: 0.002193, accuracy: 0.0506
average test loss: 0.001218, accuracy: 0.0379
case acc: 0.024730328
case acc: 0.059867475
case acc: 0.05330945
case acc: 0.031403713
case acc: 0.023087291
case acc: 0.03481452
top acc: 0.0259 ::: bot acc: 0.0427
top acc: 0.1061 ::: bot acc: 0.0270
top acc: 0.0620 ::: bot acc: 0.0786
top acc: 0.0416 ::: bot acc: 0.0395
top acc: 0.0400 ::: bot acc: 0.0212
top acc: 0.0422 ::: bot acc: 0.0518
current epoch: 29
train loss is 0.002947
average val loss: 0.002227, accuracy: 0.0511
average test loss: 0.001211, accuracy: 0.0378
case acc: 0.024807611
case acc: 0.059678137
case acc: 0.05287739
case acc: 0.031302076
case acc: 0.023385452
case acc: 0.034901455
top acc: 0.0270 ::: bot acc: 0.0413
top acc: 0.1053 ::: bot acc: 0.0278
top acc: 0.0632 ::: bot acc: 0.0769
top acc: 0.0430 ::: bot acc: 0.0382
top acc: 0.0412 ::: bot acc: 0.0202
top acc: 0.0429 ::: bot acc: 0.0514
current epoch: 30
train loss is 0.002933
average val loss: 0.002225, accuracy: 0.0510
average test loss: 0.001204, accuracy: 0.0378
case acc: 0.02496946
case acc: 0.059116133
case acc: 0.052901447
case acc: 0.03135503
case acc: 0.023456477
case acc: 0.034914076
top acc: 0.0279 ::: bot acc: 0.0410
top acc: 0.1042 ::: bot acc: 0.0287
top acc: 0.0640 ::: bot acc: 0.0763
top acc: 0.0435 ::: bot acc: 0.0375
top acc: 0.0414 ::: bot acc: 0.0199
top acc: 0.0430 ::: bot acc: 0.0513
current epoch: 31
train loss is 0.002927
average val loss: 0.002221, accuracy: 0.0510
average test loss: 0.001191, accuracy: 0.0377
case acc: 0.025008466
case acc: 0.058480922
case acc: 0.05274069
case acc: 0.031337257
case acc: 0.023543369
case acc: 0.034989435
top acc: 0.0279 ::: bot acc: 0.0412
top acc: 0.1025 ::: bot acc: 0.0299
top acc: 0.0645 ::: bot acc: 0.0756
top acc: 0.0441 ::: bot acc: 0.0371
top acc: 0.0414 ::: bot acc: 0.0200
top acc: 0.0430 ::: bot acc: 0.0514
current epoch: 32
train loss is 0.002887
average val loss: 0.002182, accuracy: 0.0503
average test loss: 0.001178, accuracy: 0.0375
case acc: 0.025043614
case acc: 0.057647992
case acc: 0.052863803
case acc: 0.031427197
case acc: 0.023280505
case acc: 0.034804046
top acc: 0.0273 ::: bot acc: 0.0417
top acc: 0.1001 ::: bot acc: 0.0321
top acc: 0.0644 ::: bot acc: 0.0761
top acc: 0.0437 ::: bot acc: 0.0379
top acc: 0.0404 ::: bot acc: 0.0209
top acc: 0.0419 ::: bot acc: 0.0523
current epoch: 33
train loss is 0.002865
average val loss: 0.002130, accuracy: 0.0495
average test loss: 0.001167, accuracy: 0.0373
case acc: 0.024801744
case acc: 0.05691686
case acc: 0.052976795
case acc: 0.031685162
case acc: 0.02295154
case acc: 0.03476393
top acc: 0.0260 ::: bot acc: 0.0427
top acc: 0.0979 ::: bot acc: 0.0344
top acc: 0.0639 ::: bot acc: 0.0766
top acc: 0.0431 ::: bot acc: 0.0389
top acc: 0.0391 ::: bot acc: 0.0222
top acc: 0.0407 ::: bot acc: 0.0536
current epoch: 34
train loss is 0.002826
average val loss: 0.002054, accuracy: 0.0484
average test loss: 0.001156, accuracy: 0.0372
case acc: 0.0248207
case acc: 0.056172162
case acc: 0.053184826
case acc: 0.031511158
case acc: 0.022606693
case acc: 0.034793943
top acc: 0.0242 ::: bot acc: 0.0448
top acc: 0.0951 ::: bot acc: 0.0377
top acc: 0.0620 ::: bot acc: 0.0782
top acc: 0.0413 ::: bot acc: 0.0402
top acc: 0.0372 ::: bot acc: 0.0242
top acc: 0.0391 ::: bot acc: 0.0555
current epoch: 35
train loss is 0.002786
average val loss: 0.002089, accuracy: 0.0489
average test loss: 0.001150, accuracy: 0.0371
case acc: 0.024750175
case acc: 0.055996526
case acc: 0.052980185
case acc: 0.03142099
case acc: 0.022804346
case acc: 0.03478756
top acc: 0.0251 ::: bot acc: 0.0435
top acc: 0.0948 ::: bot acc: 0.0374
top acc: 0.0635 ::: bot acc: 0.0769
top acc: 0.0428 ::: bot acc: 0.0385
top acc: 0.0381 ::: bot acc: 0.0231
top acc: 0.0399 ::: bot acc: 0.0545
current epoch: 36
train loss is 0.002804
average val loss: 0.002129, accuracy: 0.0495
average test loss: 0.001152, accuracy: 0.0372
case acc: 0.024885705
case acc: 0.056175735
case acc: 0.052714024
case acc: 0.0314224
case acc: 0.023051124
case acc: 0.034830716
top acc: 0.0264 ::: bot acc: 0.0425
top acc: 0.0956 ::: bot acc: 0.0371
top acc: 0.0651 ::: bot acc: 0.0752
top acc: 0.0440 ::: bot acc: 0.0375
top acc: 0.0393 ::: bot acc: 0.0223
top acc: 0.0410 ::: bot acc: 0.0535
current epoch: 37
train loss is 0.002791
average val loss: 0.002091, accuracy: 0.0489
average test loss: 0.001141, accuracy: 0.0370
case acc: 0.024822352
case acc: 0.055751808
case acc: 0.052819163
case acc: 0.03127521
case acc: 0.022734284
case acc: 0.034785986
top acc: 0.0253 ::: bot acc: 0.0437
top acc: 0.0937 ::: bot acc: 0.0387
top acc: 0.0644 ::: bot acc: 0.0759
top acc: 0.0430 ::: bot acc: 0.0380
top acc: 0.0380 ::: bot acc: 0.0230
top acc: 0.0399 ::: bot acc: 0.0544
current epoch: 38
train loss is 0.002771
average val loss: 0.002036, accuracy: 0.0480
average test loss: 0.001137, accuracy: 0.0370
case acc: 0.02475647
case acc: 0.055270832
case acc: 0.052899074
case acc: 0.03139538
case acc: 0.022578401
case acc: 0.03485117
top acc: 0.0237 ::: bot acc: 0.0450
top acc: 0.0915 ::: bot acc: 0.0412
top acc: 0.0634 ::: bot acc: 0.0768
top acc: 0.0422 ::: bot acc: 0.0391
top acc: 0.0366 ::: bot acc: 0.0246
top acc: 0.0385 ::: bot acc: 0.0563
current epoch: 39
train loss is 0.002750
average val loss: 0.001993, accuracy: 0.0473
average test loss: 0.001131, accuracy: 0.0369
case acc: 0.024888271
case acc: 0.05473758
case acc: 0.05310191
case acc: 0.031370822
case acc: 0.022446096
case acc: 0.034872655
top acc: 0.0224 ::: bot acc: 0.0464
top acc: 0.0897 ::: bot acc: 0.0430
top acc: 0.0627 ::: bot acc: 0.0777
top acc: 0.0406 ::: bot acc: 0.0405
top acc: 0.0351 ::: bot acc: 0.0264
top acc: 0.0365 ::: bot acc: 0.0582
current epoch: 40
train loss is 0.002728
average val loss: 0.002072, accuracy: 0.0484
average test loss: 0.001130, accuracy: 0.0369
case acc: 0.024748093
case acc: 0.055063147
case acc: 0.052623417
case acc: 0.03134284
case acc: 0.022739442
case acc: 0.034974918
top acc: 0.0244 ::: bot acc: 0.0443
top acc: 0.0913 ::: bot acc: 0.0412
top acc: 0.0652 ::: bot acc: 0.0750
top acc: 0.0431 ::: bot acc: 0.0381
top acc: 0.0372 ::: bot acc: 0.0246
top acc: 0.0387 ::: bot acc: 0.0563
current epoch: 41
train loss is 0.002740
average val loss: 0.002096, accuracy: 0.0487
average test loss: 0.001131, accuracy: 0.0369
case acc: 0.024833992
case acc: 0.055046037
case acc: 0.052489433
case acc: 0.03140031
case acc: 0.02264078
case acc: 0.03497562
top acc: 0.0248 ::: bot acc: 0.0440
top acc: 0.0914 ::: bot acc: 0.0411
top acc: 0.0662 ::: bot acc: 0.0740
top acc: 0.0438 ::: bot acc: 0.0377
top acc: 0.0374 ::: bot acc: 0.0238
top acc: 0.0390 ::: bot acc: 0.0559
current epoch: 42
train loss is 0.002736
average val loss: 0.002104, accuracy: 0.0488
average test loss: 0.001127, accuracy: 0.0368
case acc: 0.024767721
case acc: 0.055079624
case acc: 0.052315436
case acc: 0.031288598
case acc: 0.022660093
case acc: 0.03494041
top acc: 0.0250 ::: bot acc: 0.0438
top acc: 0.0910 ::: bot acc: 0.0415
top acc: 0.0665 ::: bot acc: 0.0734
top acc: 0.0439 ::: bot acc: 0.0374
top acc: 0.0378 ::: bot acc: 0.0232
top acc: 0.0391 ::: bot acc: 0.0559
current epoch: 43
train loss is 0.002723
average val loss: 0.002074, accuracy: 0.0484
average test loss: 0.001127, accuracy: 0.0369
case acc: 0.024996545
case acc: 0.05470806
case acc: 0.052636296
case acc: 0.03139656
case acc: 0.022683883
case acc: 0.0348894
top acc: 0.0243 ::: bot acc: 0.0449
top acc: 0.0898 ::: bot acc: 0.0428
top acc: 0.0665 ::: bot acc: 0.0742
top acc: 0.0434 ::: bot acc: 0.0379
top acc: 0.0376 ::: bot acc: 0.0237
top acc: 0.0380 ::: bot acc: 0.0567
current epoch: 44
train loss is 0.002707
average val loss: 0.002035, accuracy: 0.0477
average test loss: 0.001121, accuracy: 0.0368
case acc: 0.02502795
case acc: 0.0542915
case acc: 0.052696466
case acc: 0.03137539
case acc: 0.02248726
case acc: 0.034769937
top acc: 0.0228 ::: bot acc: 0.0462
top acc: 0.0882 ::: bot acc: 0.0442
top acc: 0.0655 ::: bot acc: 0.0750
top acc: 0.0424 ::: bot acc: 0.0389
top acc: 0.0368 ::: bot acc: 0.0244
top acc: 0.0367 ::: bot acc: 0.0577
current epoch: 45
train loss is 0.002706
average val loss: 0.002040, accuracy: 0.0478
average test loss: 0.001122, accuracy: 0.0368
case acc: 0.024983576
case acc: 0.054374263
case acc: 0.05279354
case acc: 0.031259373
case acc: 0.022591606
case acc: 0.035007965
top acc: 0.0223 ::: bot acc: 0.0465
top acc: 0.0878 ::: bot acc: 0.0449
top acc: 0.0657 ::: bot acc: 0.0749
top acc: 0.0426 ::: bot acc: 0.0385
top acc: 0.0369 ::: bot acc: 0.0242
top acc: 0.0372 ::: bot acc: 0.0577
current epoch: 46
train loss is 0.002693
average val loss: 0.002046, accuracy: 0.0479
average test loss: 0.001119, accuracy: 0.0368
case acc: 0.02507707
case acc: 0.05413309
case acc: 0.05259365
case acc: 0.03128831
case acc: 0.022661738
case acc: 0.034895837
top acc: 0.0227 ::: bot acc: 0.0464
top acc: 0.0875 ::: bot acc: 0.0451
top acc: 0.0658 ::: bot acc: 0.0745
top acc: 0.0427 ::: bot acc: 0.0384
top acc: 0.0374 ::: bot acc: 0.0239
top acc: 0.0374 ::: bot acc: 0.0575
current epoch: 47
train loss is 0.002687
average val loss: 0.002015, accuracy: 0.0475
average test loss: 0.001115, accuracy: 0.0368
case acc: 0.025192441
case acc: 0.053770054
case acc: 0.052725114
case acc: 0.031352147
case acc: 0.022548638
case acc: 0.034981504
top acc: 0.0215 ::: bot acc: 0.0472
top acc: 0.0860 ::: bot acc: 0.0463
top acc: 0.0654 ::: bot acc: 0.0751
top acc: 0.0422 ::: bot acc: 0.0390
top acc: 0.0371 ::: bot acc: 0.0242
top acc: 0.0369 ::: bot acc: 0.0579
current epoch: 48
train loss is 0.002677
average val loss: 0.001945, accuracy: 0.0464
average test loss: 0.001117, accuracy: 0.0368
case acc: 0.02559645
case acc: 0.05344929
case acc: 0.053072613
case acc: 0.031425055
case acc: 0.022350783
case acc: 0.03496378
top acc: 0.0192 ::: bot acc: 0.0497
top acc: 0.0836 ::: bot acc: 0.0492
top acc: 0.0632 ::: bot acc: 0.0772
top acc: 0.0404 ::: bot acc: 0.0409
top acc: 0.0353 ::: bot acc: 0.0261
top acc: 0.0349 ::: bot acc: 0.0597
current epoch: 49
train loss is 0.002656
average val loss: 0.001900, accuracy: 0.0458
average test loss: 0.001118, accuracy: 0.0368
case acc: 0.025831163
case acc: 0.052984357
case acc: 0.05322822
case acc: 0.031510387
case acc: 0.022305805
case acc: 0.035099443
top acc: 0.0172 ::: bot acc: 0.0515
top acc: 0.0817 ::: bot acc: 0.0508
top acc: 0.0620 ::: bot acc: 0.0783
top acc: 0.0392 ::: bot acc: 0.0422
top acc: 0.0343 ::: bot acc: 0.0273
top acc: 0.0339 ::: bot acc: 0.0607
current epoch: 50
train loss is 0.002651
average val loss: 0.001890, accuracy: 0.0457
average test loss: 0.001120, accuracy: 0.0369
case acc: 0.02604233
case acc: 0.05296427
case acc: 0.053400297
case acc: 0.031541307
case acc: 0.022237372
case acc: 0.03501479
top acc: 0.0168 ::: bot acc: 0.0520
top acc: 0.0811 ::: bot acc: 0.0516
top acc: 0.0617 ::: bot acc: 0.0788
top acc: 0.0393 ::: bot acc: 0.0419
top acc: 0.0340 ::: bot acc: 0.0272
top acc: 0.0343 ::: bot acc: 0.0605
LME_Co_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5352 5352 5352
1.7082474 -0.6288155 0.29835227 -0.25048217
Validation: 600 600 600
Testing: 744 744 744
pre-processing time: 0.00019550323486328125
the split date is 2011-01-01
train dropout: 0.4 test dropout: 0.1
net initializing with time: 0.0023031234741210938
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.010504
average val loss: 0.006146, accuracy: 0.0997
average test loss: 0.006699, accuracy: 0.1043
case acc: 0.12277991
case acc: 0.09849476
case acc: 0.11623768
case acc: 0.1091445
case acc: 0.10804139
case acc: 0.07106305
top acc: 0.0626 ::: bot acc: 0.1871
top acc: 0.1467 ::: bot acc: 0.0531
top acc: 0.0592 ::: bot acc: 0.1697
top acc: 0.0709 ::: bot acc: 0.1449
top acc: 0.0369 ::: bot acc: 0.1709
top acc: 0.0586 ::: bot acc: 0.1059
current epoch: 2
train loss is 0.008519
average val loss: 0.004615, accuracy: 0.0643
average test loss: 0.005337, accuracy: 0.0753
case acc: 0.04769097
case acc: 0.19729584
case acc: 0.049221955
case acc: 0.030252503
case acc: 0.055079382
case acc: 0.072208494
top acc: 0.0512 ::: bot acc: 0.0741
top acc: 0.2460 ::: bot acc: 0.1516
top acc: 0.0787 ::: bot acc: 0.0574
top acc: 0.0439 ::: bot acc: 0.0397
top acc: 0.0785 ::: bot acc: 0.0682
top acc: 0.1545 ::: bot acc: 0.0129
current epoch: 3
train loss is 0.009880
average val loss: 0.010732, accuracy: 0.1215
average test loss: 0.011544, accuracy: 0.1240
case acc: 0.07993842
case acc: 0.26955223
case acc: 0.09271357
case acc: 0.079612754
case acc: 0.08365918
case acc: 0.13867496
top acc: 0.1379 ::: bot acc: 0.0199
top acc: 0.3180 ::: bot acc: 0.2242
top acc: 0.1647 ::: bot acc: 0.0358
top acc: 0.1233 ::: bot acc: 0.0412
top acc: 0.1599 ::: bot acc: 0.0214
top acc: 0.2277 ::: bot acc: 0.0667
current epoch: 4
train loss is 0.012713
average val loss: 0.005380, accuracy: 0.0723
average test loss: 0.006102, accuracy: 0.0811
case acc: 0.05280527
case acc: 0.20540196
case acc: 0.05460196
case acc: 0.034777127
case acc: 0.056427356
case acc: 0.0828066
top acc: 0.0893 ::: bot acc: 0.0357
top acc: 0.2543 ::: bot acc: 0.1596
top acc: 0.1122 ::: bot acc: 0.0263
top acc: 0.0682 ::: bot acc: 0.0171
top acc: 0.1115 ::: bot acc: 0.0356
top acc: 0.1694 ::: bot acc: 0.0153
current epoch: 5
train loss is 0.010863
average val loss: 0.003559, accuracy: 0.0732
average test loss: 0.004165, accuracy: 0.0802
case acc: 0.07369198
case acc: 0.08981728
case acc: 0.08504603
case acc: 0.08585218
case acc: 0.07933069
case acc: 0.06726186
top acc: 0.0168 ::: bot acc: 0.1369
top acc: 0.1384 ::: bot acc: 0.0445
top acc: 0.0438 ::: bot acc: 0.1313
top acc: 0.0513 ::: bot acc: 0.1210
top acc: 0.0343 ::: bot acc: 0.1302
top acc: 0.0638 ::: bot acc: 0.0973
current epoch: 6
train loss is 0.005578
average val loss: 0.002978, accuracy: 0.0647
average test loss: 0.003606, accuracy: 0.0737
case acc: 0.064010926
case acc: 0.08912792
case acc: 0.079429284
case acc: 0.07753569
case acc: 0.06849692
case acc: 0.063778035
top acc: 0.0139 ::: bot acc: 0.1241
top acc: 0.1374 ::: bot acc: 0.0443
top acc: 0.0420 ::: bot acc: 0.1235
top acc: 0.0451 ::: bot acc: 0.1117
top acc: 0.0405 ::: bot acc: 0.1107
top acc: 0.0737 ::: bot acc: 0.0867
current epoch: 7
train loss is 0.004440
average val loss: 0.002378, accuracy: 0.0506
average test loss: 0.003125, accuracy: 0.0648
case acc: 0.048624255
case acc: 0.11574384
case acc: 0.060601033
case acc: 0.048867453
case acc: 0.055299755
case acc: 0.059400983
top acc: 0.0394 ::: bot acc: 0.0863
top acc: 0.1645 ::: bot acc: 0.0696
top acc: 0.0497 ::: bot acc: 0.0919
top acc: 0.0286 ::: bot acc: 0.0768
top acc: 0.0763 ::: bot acc: 0.0705
top acc: 0.1084 ::: bot acc: 0.0521
current epoch: 8
train loss is 0.004614
average val loss: 0.002207, accuracy: 0.0490
average test loss: 0.002961, accuracy: 0.0635
case acc: 0.04882124
case acc: 0.106269486
case acc: 0.06187339
case acc: 0.05060654
case acc: 0.054777153
case acc: 0.058909368
top acc: 0.0414 ::: bot acc: 0.0853
top acc: 0.1551 ::: bot acc: 0.0603
top acc: 0.0482 ::: bot acc: 0.0947
top acc: 0.0292 ::: bot acc: 0.0791
top acc: 0.0788 ::: bot acc: 0.0671
top acc: 0.1055 ::: bot acc: 0.0539
current epoch: 9
train loss is 0.004468
average val loss: 0.002076, accuracy: 0.0490
average test loss: 0.002818, accuracy: 0.0632
case acc: 0.049714223
case acc: 0.08930144
case acc: 0.06751522
case acc: 0.057728548
case acc: 0.055595133
case acc: 0.05925378
top acc: 0.0337 ::: bot acc: 0.0916
top acc: 0.1372 ::: bot acc: 0.0443
top acc: 0.0441 ::: bot acc: 0.1048
top acc: 0.0322 ::: bot acc: 0.0880
top acc: 0.0737 ::: bot acc: 0.0728
top acc: 0.0955 ::: bot acc: 0.0644
current epoch: 10
train loss is 0.003979
average val loss: 0.001963, accuracy: 0.0480
average test loss: 0.002730, accuracy: 0.0624
case acc: 0.049914017
case acc: 0.0795327
case acc: 0.069631904
case acc: 0.060521014
case acc: 0.05532174
case acc: 0.05933939
top acc: 0.0339 ::: bot acc: 0.0927
top acc: 0.1264 ::: bot acc: 0.0365
top acc: 0.0428 ::: bot acc: 0.1093
top acc: 0.0336 ::: bot acc: 0.0917
top acc: 0.0743 ::: bot acc: 0.0720
top acc: 0.0927 ::: bot acc: 0.0672
current epoch: 11
train loss is 0.003734
average val loss: 0.001830, accuracy: 0.0457
average test loss: 0.002619, accuracy: 0.0608
case acc: 0.048824057
case acc: 0.077333815
case acc: 0.06790536
case acc: 0.057018086
case acc: 0.054492828
case acc: 0.059093278
top acc: 0.0398 ::: bot acc: 0.0861
top acc: 0.1247 ::: bot acc: 0.0343
top acc: 0.0437 ::: bot acc: 0.1056
top acc: 0.0317 ::: bot acc: 0.0875
top acc: 0.0813 ::: bot acc: 0.0650
top acc: 0.0964 ::: bot acc: 0.0636
current epoch: 12
train loss is 0.003452
average val loss: 0.001733, accuracy: 0.0442
average test loss: 0.002536, accuracy: 0.0596
case acc: 0.04831877
case acc: 0.07401865
case acc: 0.0665849
case acc: 0.054908536
case acc: 0.054481853
case acc: 0.05924858
top acc: 0.0448 ::: bot acc: 0.0814
top acc: 0.1212 ::: bot acc: 0.0319
top acc: 0.0445 ::: bot acc: 0.1035
top acc: 0.0308 ::: bot acc: 0.0848
top acc: 0.0863 ::: bot acc: 0.0607
top acc: 0.0990 ::: bot acc: 0.0613
current epoch: 13
train loss is 0.003367
average val loss: 0.001639, accuracy: 0.0429
average test loss: 0.002462, accuracy: 0.0586
case acc: 0.048076756
case acc: 0.06952699
case acc: 0.06604657
case acc: 0.054239906
case acc: 0.05428692
case acc: 0.0594924
top acc: 0.0477 ::: bot acc: 0.0779
top acc: 0.1161 ::: bot acc: 0.0279
top acc: 0.0445 ::: bot acc: 0.1023
top acc: 0.0306 ::: bot acc: 0.0839
top acc: 0.0881 ::: bot acc: 0.0588
top acc: 0.0998 ::: bot acc: 0.0612
current epoch: 14
train loss is 0.003211
average val loss: 0.001572, accuracy: 0.0420
average test loss: 0.002401, accuracy: 0.0577
case acc: 0.0482515
case acc: 0.06671162
case acc: 0.065067485
case acc: 0.05213868
case acc: 0.054448936
case acc: 0.05943064
top acc: 0.0518 ::: bot acc: 0.0746
top acc: 0.1127 ::: bot acc: 0.0257
top acc: 0.0456 ::: bot acc: 0.1006
top acc: 0.0295 ::: bot acc: 0.0814
top acc: 0.0910 ::: bot acc: 0.0559
top acc: 0.1013 ::: bot acc: 0.0595
current epoch: 15
train loss is 0.003132
average val loss: 0.001508, accuracy: 0.0412
average test loss: 0.002332, accuracy: 0.0567
case acc: 0.047939844
case acc: 0.06291498
case acc: 0.06449692
case acc: 0.051322035
case acc: 0.054192416
case acc: 0.05914965
top acc: 0.0533 ::: bot acc: 0.0718
top acc: 0.1087 ::: bot acc: 0.0232
top acc: 0.0463 ::: bot acc: 0.0992
top acc: 0.0292 ::: bot acc: 0.0803
top acc: 0.0917 ::: bot acc: 0.0548
top acc: 0.1011 ::: bot acc: 0.0591
current epoch: 16
train loss is 0.003029
average val loss: 0.001445, accuracy: 0.0403
average test loss: 0.002284, accuracy: 0.0560
case acc: 0.0481942
case acc: 0.059832178
case acc: 0.06371495
case acc: 0.050679132
case acc: 0.054358575
case acc: 0.059290886
top acc: 0.0568 ::: bot acc: 0.0691
top acc: 0.1043 ::: bot acc: 0.0217
top acc: 0.0467 ::: bot acc: 0.0982
top acc: 0.0294 ::: bot acc: 0.0794
top acc: 0.0928 ::: bot acc: 0.0544
top acc: 0.1012 ::: bot acc: 0.0593
current epoch: 17
train loss is 0.002930
average val loss: 0.001399, accuracy: 0.0397
average test loss: 0.002238, accuracy: 0.0551
case acc: 0.048233654
case acc: 0.058895055
case acc: 0.061982445
case acc: 0.048172276
case acc: 0.053917654
case acc: 0.05925683
top acc: 0.0609 ::: bot acc: 0.0647
top acc: 0.1033 ::: bot acc: 0.0214
top acc: 0.0484 ::: bot acc: 0.0945
top acc: 0.0287 ::: bot acc: 0.0758
top acc: 0.0952 ::: bot acc: 0.0514
top acc: 0.1035 ::: bot acc: 0.0567
current epoch: 18
train loss is 0.002876
average val loss: 0.001357, accuracy: 0.0392
average test loss: 0.002191, accuracy: 0.0546
case acc: 0.048383676
case acc: 0.054560937
case acc: 0.06263884
case acc: 0.04897766
case acc: 0.054108884
case acc: 0.059184894
top acc: 0.0607 ::: bot acc: 0.0653
top acc: 0.0976 ::: bot acc: 0.0200
top acc: 0.0478 ::: bot acc: 0.0959
top acc: 0.0290 ::: bot acc: 0.0767
top acc: 0.0929 ::: bot acc: 0.0539
top acc: 0.1016 ::: bot acc: 0.0583
current epoch: 19
train loss is 0.002809
average val loss: 0.001314, accuracy: 0.0387
average test loss: 0.002152, accuracy: 0.0540
case acc: 0.048316464
case acc: 0.05253932
case acc: 0.06184076
case acc: 0.04810941
case acc: 0.0538815
case acc: 0.05919441
top acc: 0.0624 ::: bot acc: 0.0634
top acc: 0.0947 ::: bot acc: 0.0195
top acc: 0.0486 ::: bot acc: 0.0942
top acc: 0.0289 ::: bot acc: 0.0757
top acc: 0.0926 ::: bot acc: 0.0534
top acc: 0.1020 ::: bot acc: 0.0582
current epoch: 20
train loss is 0.002722
average val loss: 0.001285, accuracy: 0.0383
average test loss: 0.002120, accuracy: 0.0534
case acc: 0.048807394
case acc: 0.0510511
case acc: 0.060788065
case acc: 0.0467551
case acc: 0.053972855
case acc: 0.059198413
top acc: 0.0651 ::: bot acc: 0.0610
top acc: 0.0928 ::: bot acc: 0.0189
top acc: 0.0495 ::: bot acc: 0.0922
top acc: 0.0282 ::: bot acc: 0.0737
top acc: 0.0927 ::: bot acc: 0.0537
top acc: 0.1018 ::: bot acc: 0.0585
current epoch: 21
train loss is 0.002725
average val loss: 0.001257, accuracy: 0.0379
average test loss: 0.002101, accuracy: 0.0529
case acc: 0.04928043
case acc: 0.050823856
case acc: 0.059365258
case acc: 0.045006596
case acc: 0.05389164
case acc: 0.059225768
top acc: 0.0693 ::: bot acc: 0.0572
top acc: 0.0925 ::: bot acc: 0.0190
top acc: 0.0517 ::: bot acc: 0.0890
top acc: 0.0282 ::: bot acc: 0.0712
top acc: 0.0943 ::: bot acc: 0.0517
top acc: 0.1038 ::: bot acc: 0.0566
current epoch: 22
train loss is 0.002692
average val loss: 0.001236, accuracy: 0.0375
average test loss: 0.002076, accuracy: 0.0523
case acc: 0.049259845
case acc: 0.049670335
case acc: 0.057964236
case acc: 0.04373287
case acc: 0.05411833
case acc: 0.059038762
top acc: 0.0711 ::: bot acc: 0.0549
top acc: 0.0914 ::: bot acc: 0.0180
top acc: 0.0534 ::: bot acc: 0.0861
top acc: 0.0279 ::: bot acc: 0.0694
top acc: 0.0949 ::: bot acc: 0.0515
top acc: 0.1045 ::: bot acc: 0.0555
current epoch: 23
train loss is 0.002686
average val loss: 0.001236, accuracy: 0.0376
average test loss: 0.002078, accuracy: 0.0519
case acc: 0.050235003
case acc: 0.05082052
case acc: 0.05616299
case acc: 0.04126515
case acc: 0.0541251
case acc: 0.05898276
top acc: 0.0761 ::: bot acc: 0.0503
top acc: 0.0925 ::: bot acc: 0.0187
top acc: 0.0571 ::: bot acc: 0.0815
top acc: 0.0285 ::: bot acc: 0.0658
top acc: 0.0976 ::: bot acc: 0.0486
top acc: 0.1071 ::: bot acc: 0.0526
current epoch: 24
train loss is 0.002672
average val loss: 0.001233, accuracy: 0.0376
average test loss: 0.002074, accuracy: 0.0516
case acc: 0.050669413
case acc: 0.05060303
case acc: 0.05484139
case acc: 0.039951228
case acc: 0.05430068
case acc: 0.059261404
top acc: 0.0789 ::: bot acc: 0.0474
top acc: 0.0924 ::: bot acc: 0.0187
top acc: 0.0592 ::: bot acc: 0.0784
top acc: 0.0289 ::: bot acc: 0.0635
top acc: 0.0989 ::: bot acc: 0.0476
top acc: 0.1080 ::: bot acc: 0.0522
current epoch: 25
train loss is 0.002645
average val loss: 0.001214, accuracy: 0.0375
average test loss: 0.002053, accuracy: 0.0514
case acc: 0.050678294
case acc: 0.048821688
case acc: 0.054793686
case acc: 0.04011773
case acc: 0.054384213
case acc: 0.059451934
top acc: 0.0792 ::: bot acc: 0.0470
top acc: 0.0901 ::: bot acc: 0.0182
top acc: 0.0598 ::: bot acc: 0.0781
top acc: 0.0291 ::: bot acc: 0.0637
top acc: 0.0978 ::: bot acc: 0.0488
top acc: 0.1068 ::: bot acc: 0.0536
current epoch: 26
train loss is 0.002611
average val loss: 0.001204, accuracy: 0.0374
average test loss: 0.002040, accuracy: 0.0510
case acc: 0.050848715
case acc: 0.047846362
case acc: 0.054075517
case acc: 0.03991703
case acc: 0.05433289
case acc: 0.059228458
top acc: 0.0801 ::: bot acc: 0.0456
top acc: 0.0887 ::: bot acc: 0.0176
top acc: 0.0613 ::: bot acc: 0.0765
top acc: 0.0293 ::: bot acc: 0.0631
top acc: 0.0982 ::: bot acc: 0.0485
top acc: 0.1063 ::: bot acc: 0.0540
current epoch: 27
train loss is 0.002593
average val loss: 0.001200, accuracy: 0.0374
average test loss: 0.002037, accuracy: 0.0507
case acc: 0.050952546
case acc: 0.04720291
case acc: 0.053448323
case acc: 0.038790215
case acc: 0.05446052
case acc: 0.05939967
top acc: 0.0819 ::: bot acc: 0.0434
top acc: 0.0880 ::: bot acc: 0.0172
top acc: 0.0633 ::: bot acc: 0.0745
top acc: 0.0296 ::: bot acc: 0.0614
top acc: 0.0991 ::: bot acc: 0.0478
top acc: 0.1069 ::: bot acc: 0.0538
current epoch: 28
train loss is 0.002598
average val loss: 0.001206, accuracy: 0.0376
average test loss: 0.002049, accuracy: 0.0506
case acc: 0.05183935
case acc: 0.04764081
case acc: 0.052659612
case acc: 0.03749314
case acc: 0.054549366
case acc: 0.0593435
top acc: 0.0850 ::: bot acc: 0.0416
top acc: 0.0885 ::: bot acc: 0.0173
top acc: 0.0660 ::: bot acc: 0.0717
top acc: 0.0299 ::: bot acc: 0.0595
top acc: 0.1006 ::: bot acc: 0.0462
top acc: 0.1079 ::: bot acc: 0.0525
current epoch: 29
train loss is 0.002581
average val loss: 0.001214, accuracy: 0.0377
average test loss: 0.002049, accuracy: 0.0504
case acc: 0.052203305
case acc: 0.04768378
case acc: 0.051876385
case acc: 0.036617666
case acc: 0.05464604
case acc: 0.059296723
top acc: 0.0865 ::: bot acc: 0.0394
top acc: 0.0885 ::: bot acc: 0.0178
top acc: 0.0682 ::: bot acc: 0.0693
top acc: 0.0308 ::: bot acc: 0.0573
top acc: 0.1016 ::: bot acc: 0.0449
top acc: 0.1087 ::: bot acc: 0.0512
current epoch: 30
train loss is 0.002605
average val loss: 0.001200, accuracy: 0.0376
average test loss: 0.002037, accuracy: 0.0501
case acc: 0.052097898
case acc: 0.04651677
case acc: 0.051628966
case acc: 0.03657869
case acc: 0.05467258
case acc: 0.059259895
top acc: 0.0866 ::: bot acc: 0.0394
top acc: 0.0867 ::: bot acc: 0.0175
top acc: 0.0684 ::: bot acc: 0.0686
top acc: 0.0307 ::: bot acc: 0.0576
top acc: 0.1012 ::: bot acc: 0.0455
top acc: 0.1084 ::: bot acc: 0.0518
current epoch: 31
train loss is 0.002549
average val loss: 0.001190, accuracy: 0.0375
average test loss: 0.002022, accuracy: 0.0499
case acc: 0.05210188
case acc: 0.04532583
case acc: 0.05177934
case acc: 0.036609966
case acc: 0.054532725
case acc: 0.05923725
top acc: 0.0866 ::: bot acc: 0.0395
top acc: 0.0851 ::: bot acc: 0.0176
top acc: 0.0689 ::: bot acc: 0.0688
top acc: 0.0302 ::: bot acc: 0.0578
top acc: 0.1011 ::: bot acc: 0.0452
top acc: 0.1073 ::: bot acc: 0.0528
current epoch: 32
train loss is 0.002536
average val loss: 0.001184, accuracy: 0.0375
average test loss: 0.002018, accuracy: 0.0497
case acc: 0.052252755
case acc: 0.044673037
case acc: 0.051436547
case acc: 0.03626964
case acc: 0.05456317
case acc: 0.059249662
top acc: 0.0870 ::: bot acc: 0.0393
top acc: 0.0842 ::: bot acc: 0.0173
top acc: 0.0696 ::: bot acc: 0.0678
top acc: 0.0304 ::: bot acc: 0.0571
top acc: 0.1015 ::: bot acc: 0.0446
top acc: 0.1073 ::: bot acc: 0.0528
current epoch: 33
train loss is 0.002520
average val loss: 0.001180, accuracy: 0.0374
average test loss: 0.002015, accuracy: 0.0496
case acc: 0.05247517
case acc: 0.04415824
case acc: 0.051294487
case acc: 0.035938766
case acc: 0.054555856
case acc: 0.059170052
top acc: 0.0876 ::: bot acc: 0.0387
top acc: 0.0832 ::: bot acc: 0.0177
top acc: 0.0705 ::: bot acc: 0.0669
top acc: 0.0307 ::: bot acc: 0.0568
top acc: 0.1020 ::: bot acc: 0.0442
top acc: 0.1076 ::: bot acc: 0.0524
current epoch: 34
train loss is 0.002509
average val loss: 0.001156, accuracy: 0.0371
average test loss: 0.001979, accuracy: 0.0494
case acc: 0.052058633
case acc: 0.042249072
case acc: 0.051580757
case acc: 0.036895614
case acc: 0.054321088
case acc: 0.059164587
top acc: 0.0857 ::: bot acc: 0.0406
top acc: 0.0802 ::: bot acc: 0.0181
top acc: 0.0690 ::: bot acc: 0.0684
top acc: 0.0304 ::: bot acc: 0.0582
top acc: 0.1001 ::: bot acc: 0.0461
top acc: 0.1054 ::: bot acc: 0.0545
current epoch: 35
train loss is 0.002471
average val loss: 0.001139, accuracy: 0.0369
average test loss: 0.001961, accuracy: 0.0492
case acc: 0.051692907
case acc: 0.041260723
case acc: 0.05193686
case acc: 0.037280705
case acc: 0.05424429
case acc: 0.05898214
top acc: 0.0841 ::: bot acc: 0.0417
top acc: 0.0781 ::: bot acc: 0.0195
top acc: 0.0683 ::: bot acc: 0.0693
top acc: 0.0299 ::: bot acc: 0.0589
top acc: 0.0990 ::: bot acc: 0.0471
top acc: 0.1041 ::: bot acc: 0.0555
current epoch: 36
train loss is 0.002440
average val loss: 0.001138, accuracy: 0.0369
average test loss: 0.001960, accuracy: 0.0492
case acc: 0.051842663
case acc: 0.041037668
case acc: 0.051706307
case acc: 0.036941823
case acc: 0.05439115
case acc: 0.05912262
top acc: 0.0846 ::: bot acc: 0.0415
top acc: 0.0774 ::: bot acc: 0.0198
top acc: 0.0690 ::: bot acc: 0.0688
top acc: 0.0302 ::: bot acc: 0.0583
top acc: 0.0994 ::: bot acc: 0.0470
top acc: 0.1046 ::: bot acc: 0.0553
current epoch: 37
train loss is 0.002427
average val loss: 0.001125, accuracy: 0.0367
average test loss: 0.001949, accuracy: 0.0490
case acc: 0.051689185
case acc: 0.040403843
case acc: 0.0517074
case acc: 0.036880236
case acc: 0.054338526
case acc: 0.059043515
top acc: 0.0840 ::: bot acc: 0.0423
top acc: 0.0761 ::: bot acc: 0.0206
top acc: 0.0686 ::: bot acc: 0.0687
top acc: 0.0299 ::: bot acc: 0.0584
top acc: 0.0989 ::: bot acc: 0.0474
top acc: 0.1044 ::: bot acc: 0.0556
current epoch: 38
train loss is 0.002426
average val loss: 0.001122, accuracy: 0.0367
average test loss: 0.001945, accuracy: 0.0489
case acc: 0.051657584
case acc: 0.039892063
case acc: 0.051740594
case acc: 0.03671107
case acc: 0.05433775
case acc: 0.05906352
top acc: 0.0838 ::: bot acc: 0.0425
top acc: 0.0753 ::: bot acc: 0.0209
top acc: 0.0688 ::: bot acc: 0.0688
top acc: 0.0301 ::: bot acc: 0.0581
top acc: 0.0994 ::: bot acc: 0.0470
top acc: 0.1045 ::: bot acc: 0.0553
current epoch: 39
train loss is 0.002402
average val loss: 0.001102, accuracy: 0.0364
average test loss: 0.001927, accuracy: 0.0489
case acc: 0.051282976
case acc: 0.038416855
case acc: 0.052333556
case acc: 0.037663013
case acc: 0.05428854
case acc: 0.05922492
top acc: 0.0817 ::: bot acc: 0.0446
top acc: 0.0722 ::: bot acc: 0.0227
top acc: 0.0669 ::: bot acc: 0.0707
top acc: 0.0299 ::: bot acc: 0.0595
top acc: 0.0978 ::: bot acc: 0.0488
top acc: 0.1029 ::: bot acc: 0.0573
current epoch: 40
train loss is 0.002392
average val loss: 0.001090, accuracy: 0.0363
average test loss: 0.001918, accuracy: 0.0488
case acc: 0.05113547
case acc: 0.037796535
case acc: 0.052423734
case acc: 0.03785668
case acc: 0.0542995
case acc: 0.0590399
top acc: 0.0808 ::: bot acc: 0.0455
top acc: 0.0707 ::: bot acc: 0.0239
top acc: 0.0664 ::: bot acc: 0.0712
top acc: 0.0299 ::: bot acc: 0.0599
top acc: 0.0977 ::: bot acc: 0.0490
top acc: 0.1028 ::: bot acc: 0.0570
current epoch: 41
train loss is 0.002364
average val loss: 0.001076, accuracy: 0.0360
average test loss: 0.001891, accuracy: 0.0488
case acc: 0.050442465
case acc: 0.036645636
case acc: 0.053384613
case acc: 0.039612122
case acc: 0.053972926
case acc: 0.05895133
top acc: 0.0773 ::: bot acc: 0.0488
top acc: 0.0667 ::: bot acc: 0.0278
top acc: 0.0629 ::: bot acc: 0.0745
top acc: 0.0291 ::: bot acc: 0.0628
top acc: 0.0947 ::: bot acc: 0.0516
top acc: 0.0999 ::: bot acc: 0.0598
current epoch: 42
train loss is 0.002340
average val loss: 0.001069, accuracy: 0.0360
average test loss: 0.001879, accuracy: 0.0490
case acc: 0.049980417
case acc: 0.035929564
case acc: 0.054330025
case acc: 0.04102755
case acc: 0.053972714
case acc: 0.05904438
top acc: 0.0746 ::: bot acc: 0.0517
top acc: 0.0634 ::: bot acc: 0.0311
top acc: 0.0607 ::: bot acc: 0.0767
top acc: 0.0287 ::: bot acc: 0.0650
top acc: 0.0923 ::: bot acc: 0.0538
top acc: 0.0977 ::: bot acc: 0.0621
current epoch: 43
train loss is 0.002314
average val loss: 0.001072, accuracy: 0.0361
average test loss: 0.001878, accuracy: 0.0495
case acc: 0.04951038
case acc: 0.035406627
case acc: 0.05582058
case acc: 0.04293671
case acc: 0.05393723
case acc: 0.059192017
top acc: 0.0710 ::: bot acc: 0.0552
top acc: 0.0594 ::: bot acc: 0.0355
top acc: 0.0577 ::: bot acc: 0.0805
top acc: 0.0281 ::: bot acc: 0.0684
top acc: 0.0891 ::: bot acc: 0.0570
top acc: 0.0947 ::: bot acc: 0.0652
current epoch: 44
train loss is 0.002302
average val loss: 0.001073, accuracy: 0.0361
average test loss: 0.001879, accuracy: 0.0496
case acc: 0.049397524
case acc: 0.03503982
case acc: 0.05639975
case acc: 0.043669175
case acc: 0.054034166
case acc: 0.059215948
top acc: 0.0694 ::: bot acc: 0.0569
top acc: 0.0572 ::: bot acc: 0.0373
top acc: 0.0566 ::: bot acc: 0.0821
top acc: 0.0282 ::: bot acc: 0.0693
top acc: 0.0881 ::: bot acc: 0.0581
top acc: 0.0939 ::: bot acc: 0.0661
current epoch: 45
train loss is 0.002292
average val loss: 0.001084, accuracy: 0.0364
average test loss: 0.001895, accuracy: 0.0501
case acc: 0.049190957
case acc: 0.034948353
case acc: 0.05761436
case acc: 0.045245867
case acc: 0.054213256
case acc: 0.05940923
top acc: 0.0667 ::: bot acc: 0.0596
top acc: 0.0544 ::: bot acc: 0.0403
top acc: 0.0548 ::: bot acc: 0.0850
top acc: 0.0282 ::: bot acc: 0.0717
top acc: 0.0861 ::: bot acc: 0.0603
top acc: 0.0921 ::: bot acc: 0.0681
current epoch: 46
train loss is 0.002285
average val loss: 0.001096, accuracy: 0.0366
average test loss: 0.001901, accuracy: 0.0504
case acc: 0.048847906
case acc: 0.034741398
case acc: 0.058639493
case acc: 0.046486396
case acc: 0.05419779
case acc: 0.059468336
top acc: 0.0644 ::: bot acc: 0.0618
top acc: 0.0514 ::: bot acc: 0.0430
top acc: 0.0528 ::: bot acc: 0.0873
top acc: 0.0282 ::: bot acc: 0.0735
top acc: 0.0845 ::: bot acc: 0.0614
top acc: 0.0904 ::: bot acc: 0.0696
current epoch: 47
train loss is 0.002283
average val loss: 0.001135, accuracy: 0.0373
average test loss: 0.001942, accuracy: 0.0513
case acc: 0.048536677
case acc: 0.034919307
case acc: 0.06053882
case acc: 0.04925382
case acc: 0.054505434
case acc: 0.059926443
top acc: 0.0607 ::: bot acc: 0.0656
top acc: 0.0472 ::: bot acc: 0.0475
top acc: 0.0500 ::: bot acc: 0.0916
top acc: 0.0290 ::: bot acc: 0.0774
top acc: 0.0815 ::: bot acc: 0.0648
top acc: 0.0874 ::: bot acc: 0.0727
current epoch: 48
train loss is 0.002286
average val loss: 0.001163, accuracy: 0.0378
average test loss: 0.001974, accuracy: 0.0519
case acc: 0.048462674
case acc: 0.035254586
case acc: 0.06202261
case acc: 0.050719574
case acc: 0.05464962
case acc: 0.060167566
top acc: 0.0582 ::: bot acc: 0.0683
top acc: 0.0442 ::: bot acc: 0.0508
top acc: 0.0487 ::: bot acc: 0.0945
top acc: 0.0295 ::: bot acc: 0.0793
top acc: 0.0801 ::: bot acc: 0.0664
top acc: 0.0863 ::: bot acc: 0.0738
current epoch: 49
train loss is 0.002298
average val loss: 0.001188, accuracy: 0.0381
average test loss: 0.002007, accuracy: 0.0524
case acc: 0.048420135
case acc: 0.03563571
case acc: 0.06352379
case acc: 0.052050833
case acc: 0.05463764
case acc: 0.060405426
top acc: 0.0560 ::: bot acc: 0.0703
top acc: 0.0410 ::: bot acc: 0.0537
top acc: 0.0471 ::: bot acc: 0.0976
top acc: 0.0299 ::: bot acc: 0.0811
top acc: 0.0789 ::: bot acc: 0.0673
top acc: 0.0856 ::: bot acc: 0.0746
current epoch: 50
train loss is 0.002307
average val loss: 0.001292, accuracy: 0.0399
average test loss: 0.002100, accuracy: 0.0541
case acc: 0.048379716
case acc: 0.03693
case acc: 0.06688186
case acc: 0.056042008
case acc: 0.05515047
case acc: 0.061028857
top acc: 0.0505 ::: bot acc: 0.0758
top acc: 0.0343 ::: bot acc: 0.0600
top acc: 0.0447 ::: bot acc: 0.1037
top acc: 0.0317 ::: bot acc: 0.0862
top acc: 0.0747 ::: bot acc: 0.0714
top acc: 0.0814 ::: bot acc: 0.0783
LME_Co_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5328 5328 5328
1.7082474 -0.6288155 0.29835227 -0.25048217
Validation: 594 594 594
Testing: 774 774 774
pre-processing time: 0.00020599365234375
the split date is 2011-07-01
train dropout: 0.4 test dropout: 0.1
net initializing with time: 0.0022361278533935547
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.010665
average val loss: 0.005525, accuracy: 0.0943
average test loss: 0.006153, accuracy: 0.0971
case acc: 0.12876296
case acc: 0.11395083
case acc: 0.10014976
case acc: 0.06916626
case acc: 0.12256037
case acc: 0.047845505
top acc: 0.0781 ::: bot acc: 0.1814
top acc: 0.1557 ::: bot acc: 0.0746
top acc: 0.0393 ::: bot acc: 0.1634
top acc: 0.0278 ::: bot acc: 0.1238
top acc: 0.0607 ::: bot acc: 0.1827
top acc: 0.0406 ::: bot acc: 0.0742
current epoch: 2
train loss is 0.009137
average val loss: 0.006784, accuracy: 0.0849
average test loss: 0.006269, accuracy: 0.0848
case acc: 0.03871005
case acc: 0.21984659
case acc: 0.05167059
case acc: 0.06596914
case acc: 0.045969784
case acc: 0.08646134
top acc: 0.0432 ::: bot acc: 0.0610
top acc: 0.2625 ::: bot acc: 0.1796
top acc: 0.0865 ::: bot acc: 0.0431
top acc: 0.1223 ::: bot acc: 0.0188
top acc: 0.0487 ::: bot acc: 0.0713
top acc: 0.1471 ::: bot acc: 0.0340
current epoch: 3
train loss is 0.010384
average val loss: 0.012976, accuracy: 0.1349
average test loss: 0.011747, accuracy: 0.1266
case acc: 0.06531291
case acc: 0.27609235
case acc: 0.093708895
case acc: 0.12026566
case acc: 0.061048303
case acc: 0.14323284
top acc: 0.1131 ::: bot acc: 0.0188
top acc: 0.3190 ::: bot acc: 0.2358
top acc: 0.1562 ::: bot acc: 0.0350
top acc: 0.1852 ::: bot acc: 0.0539
top acc: 0.1149 ::: bot acc: 0.0166
top acc: 0.2047 ::: bot acc: 0.0894
current epoch: 4
train loss is 0.013263
average val loss: 0.004646, accuracy: 0.0716
average test loss: 0.004248, accuracy: 0.0713
case acc: 0.042972226
case acc: 0.1740831
case acc: 0.04928004
case acc: 0.053824395
case acc: 0.051905446
case acc: 0.05565201
top acc: 0.0278 ::: bot acc: 0.0768
top acc: 0.2166 ::: bot acc: 0.1343
top acc: 0.0652 ::: bot acc: 0.0653
top acc: 0.0931 ::: bot acc: 0.0396
top acc: 0.0322 ::: bot acc: 0.0892
top acc: 0.1100 ::: bot acc: 0.0157
current epoch: 5
train loss is 0.009282
average val loss: 0.004279, accuracy: 0.0818
average test loss: 0.004674, accuracy: 0.0837
case acc: 0.10450067
case acc: 0.07954592
case acc: 0.090845615
case acc: 0.06858554
case acc: 0.10503643
case acc: 0.05338627
top acc: 0.0536 ::: bot acc: 0.1571
top acc: 0.1216 ::: bot acc: 0.0400
top acc: 0.0334 ::: bot acc: 0.1524
top acc: 0.0269 ::: bot acc: 0.1234
top acc: 0.0450 ::: bot acc: 0.1633
top acc: 0.0287 ::: bot acc: 0.0896
current epoch: 6
train loss is 0.004957
average val loss: 0.003250, accuracy: 0.0683
average test loss: 0.003182, accuracy: 0.0667
case acc: 0.06740455
case acc: 0.1065332
case acc: 0.06507624
case acc: 0.05009152
case acc: 0.06788968
case acc: 0.042905092
top acc: 0.0217 ::: bot acc: 0.1173
top acc: 0.1487 ::: bot acc: 0.0672
top acc: 0.0255 ::: bot acc: 0.1173
top acc: 0.0455 ::: bot acc: 0.0860
top acc: 0.0232 ::: bot acc: 0.1184
top acc: 0.0638 ::: bot acc: 0.0516
current epoch: 7
train loss is 0.004525
average val loss: 0.003268, accuracy: 0.0652
average test loss: 0.002920, accuracy: 0.0617
case acc: 0.04880089
case acc: 0.12203591
case acc: 0.054063804
case acc: 0.048367012
case acc: 0.05169116
case acc: 0.045361985
top acc: 0.0191 ::: bot acc: 0.0907
top acc: 0.1644 ::: bot acc: 0.0823
top acc: 0.0357 ::: bot acc: 0.0962
top acc: 0.0691 ::: bot acc: 0.0632
top acc: 0.0321 ::: bot acc: 0.0902
top acc: 0.0867 ::: bot acc: 0.0298
current epoch: 8
train loss is 0.004787
average val loss: 0.002966, accuracy: 0.0642
average test loss: 0.002691, accuracy: 0.0605
case acc: 0.05367172
case acc: 0.102440074
case acc: 0.060327344
case acc: 0.048338894
case acc: 0.05503565
case acc: 0.04300222
top acc: 0.0174 ::: bot acc: 0.0989
top acc: 0.1448 ::: bot acc: 0.0630
top acc: 0.0284 ::: bot acc: 0.1094
top acc: 0.0567 ::: bot acc: 0.0754
top acc: 0.0285 ::: bot acc: 0.0965
top acc: 0.0746 ::: bot acc: 0.0413
current epoch: 9
train loss is 0.004330
average val loss: 0.002840, accuracy: 0.0638
average test loss: 0.002576, accuracy: 0.0596
case acc: 0.05694141
case acc: 0.08765604
case acc: 0.06497686
case acc: 0.04937208
case acc: 0.05617274
case acc: 0.042676672
top acc: 0.0180 ::: bot acc: 0.1034
top acc: 0.1295 ::: bot acc: 0.0479
top acc: 0.0258 ::: bot acc: 0.1173
top acc: 0.0503 ::: bot acc: 0.0818
top acc: 0.0268 ::: bot acc: 0.0991
top acc: 0.0671 ::: bot acc: 0.0485
current epoch: 10
train loss is 0.003777
average val loss: 0.002732, accuracy: 0.0628
average test loss: 0.002447, accuracy: 0.0580
case acc: 0.05554223
case acc: 0.078996144
case acc: 0.0662239
case acc: 0.04947777
case acc: 0.054809824
case acc: 0.042785794
top acc: 0.0180 ::: bot acc: 0.1013
top acc: 0.1216 ::: bot acc: 0.0393
top acc: 0.0258 ::: bot acc: 0.1193
top acc: 0.0494 ::: bot acc: 0.0830
top acc: 0.0283 ::: bot acc: 0.0967
top acc: 0.0662 ::: bot acc: 0.0492
current epoch: 11
train loss is 0.003513
average val loss: 0.002658, accuracy: 0.0613
average test loss: 0.002285, accuracy: 0.0557
case acc: 0.050395146
case acc: 0.07923859
case acc: 0.06283098
case acc: 0.04873201
case acc: 0.050413202
case acc: 0.042880856
top acc: 0.0188 ::: bot acc: 0.0932
top acc: 0.1218 ::: bot acc: 0.0397
top acc: 0.0267 ::: bot acc: 0.1132
top acc: 0.0562 ::: bot acc: 0.0767
top acc: 0.0338 ::: bot acc: 0.0868
top acc: 0.0725 ::: bot acc: 0.0431
current epoch: 12
train loss is 0.003409
average val loss: 0.002618, accuracy: 0.0603
average test loss: 0.002162, accuracy: 0.0540
case acc: 0.046189416
case acc: 0.07811533
case acc: 0.05998601
case acc: 0.048304625
case acc: 0.048272636
case acc: 0.043173168
top acc: 0.0215 ::: bot acc: 0.0857
top acc: 0.1205 ::: bot acc: 0.0385
top acc: 0.0278 ::: bot acc: 0.1085
top acc: 0.0618 ::: bot acc: 0.0709
top acc: 0.0403 ::: bot acc: 0.0808
top acc: 0.0772 ::: bot acc: 0.0381
current epoch: 13
train loss is 0.003266
average val loss: 0.002524, accuracy: 0.0595
average test loss: 0.002084, accuracy: 0.0529
case acc: 0.046246756
case acc: 0.069437444
case acc: 0.06181213
case acc: 0.04834148
case acc: 0.04867507
case acc: 0.042731047
top acc: 0.0208 ::: bot acc: 0.0859
top acc: 0.1124 ::: bot acc: 0.0293
top acc: 0.0272 ::: bot acc: 0.1117
top acc: 0.0590 ::: bot acc: 0.0735
top acc: 0.0384 ::: bot acc: 0.0824
top acc: 0.0739 ::: bot acc: 0.0414
current epoch: 14
train loss is 0.003100
average val loss: 0.002460, accuracy: 0.0588
average test loss: 0.002005, accuracy: 0.0518
case acc: 0.045568444
case acc: 0.06369029
case acc: 0.061875094
case acc: 0.04830702
case acc: 0.04867796
case acc: 0.042779196
top acc: 0.0224 ::: bot acc: 0.0840
top acc: 0.1063 ::: bot acc: 0.0237
top acc: 0.0271 ::: bot acc: 0.1116
top acc: 0.0590 ::: bot acc: 0.0732
top acc: 0.0391 ::: bot acc: 0.0818
top acc: 0.0734 ::: bot acc: 0.0422
current epoch: 15
train loss is 0.002952
average val loss: 0.002412, accuracy: 0.0581
average test loss: 0.001940, accuracy: 0.0508
case acc: 0.044561695
case acc: 0.05830423
case acc: 0.061917417
case acc: 0.048390552
case acc: 0.04854869
case acc: 0.04289755
top acc: 0.0233 ::: bot acc: 0.0820
top acc: 0.1006 ::: bot acc: 0.0191
top acc: 0.0272 ::: bot acc: 0.1117
top acc: 0.0597 ::: bot acc: 0.0729
top acc: 0.0395 ::: bot acc: 0.0813
top acc: 0.0732 ::: bot acc: 0.0431
current epoch: 16
train loss is 0.002836
average val loss: 0.002379, accuracy: 0.0573
average test loss: 0.001870, accuracy: 0.0498
case acc: 0.042751342
case acc: 0.05677332
case acc: 0.06004038
case acc: 0.048105273
case acc: 0.04782993
case acc: 0.04313224
top acc: 0.0268 ::: bot acc: 0.0777
top acc: 0.0994 ::: bot acc: 0.0174
top acc: 0.0283 ::: bot acc: 0.1086
top acc: 0.0627 ::: bot acc: 0.0694
top acc: 0.0423 ::: bot acc: 0.0787
top acc: 0.0754 ::: bot acc: 0.0403
current epoch: 17
train loss is 0.002799
average val loss: 0.002344, accuracy: 0.0565
average test loss: 0.001814, accuracy: 0.0490
case acc: 0.041127324
case acc: 0.055356525
case acc: 0.05859761
case acc: 0.04834376
case acc: 0.04723386
case acc: 0.043385293
top acc: 0.0312 ::: bot acc: 0.0725
top acc: 0.0976 ::: bot acc: 0.0165
top acc: 0.0302 ::: bot acc: 0.1056
top acc: 0.0663 ::: bot acc: 0.0662
top acc: 0.0448 ::: bot acc: 0.0763
top acc: 0.0779 ::: bot acc: 0.0380
current epoch: 18
train loss is 0.002712
average val loss: 0.002305, accuracy: 0.0561
average test loss: 0.001778, accuracy: 0.0484
case acc: 0.040793788
case acc: 0.051014554
case acc: 0.05851005
case acc: 0.048496805
case acc: 0.04772983
case acc: 0.043597348
top acc: 0.0319 ::: bot acc: 0.0719
top acc: 0.0927 ::: bot acc: 0.0129
top acc: 0.0301 ::: bot acc: 0.1052
top acc: 0.0663 ::: bot acc: 0.0668
top acc: 0.0437 ::: bot acc: 0.0778
top acc: 0.0772 ::: bot acc: 0.0396
current epoch: 19
train loss is 0.002622
average val loss: 0.002276, accuracy: 0.0556
average test loss: 0.001735, accuracy: 0.0477
case acc: 0.040191088
case acc: 0.049109664
case acc: 0.057713065
case acc: 0.04827802
case acc: 0.047474608
case acc: 0.04339686
top acc: 0.0343 ::: bot acc: 0.0697
top acc: 0.0905 ::: bot acc: 0.0119
top acc: 0.0310 ::: bot acc: 0.1038
top acc: 0.0674 ::: bot acc: 0.0645
top acc: 0.0442 ::: bot acc: 0.0770
top acc: 0.0771 ::: bot acc: 0.0392
current epoch: 20
train loss is 0.002599
average val loss: 0.002238, accuracy: 0.0551
average test loss: 0.001700, accuracy: 0.0472
case acc: 0.040133316
case acc: 0.046361998
case acc: 0.057030976
case acc: 0.048408598
case acc: 0.048100907
case acc: 0.043337867
top acc: 0.0361 ::: bot acc: 0.0682
top acc: 0.0866 ::: bot acc: 0.0111
top acc: 0.0314 ::: bot acc: 0.1023
top acc: 0.0680 ::: bot acc: 0.0645
top acc: 0.0428 ::: bot acc: 0.0790
top acc: 0.0754 ::: bot acc: 0.0410
current epoch: 21
train loss is 0.002542
average val loss: 0.002251, accuracy: 0.0547
average test loss: 0.001672, accuracy: 0.0468
case acc: 0.039020922
case acc: 0.04774331
case acc: 0.05466726
case acc: 0.048741348
case acc: 0.04704383
case acc: 0.043667756
top acc: 0.0420 ::: bot acc: 0.0621
top acc: 0.0888 ::: bot acc: 0.0112
top acc: 0.0354 ::: bot acc: 0.0970
top acc: 0.0728 ::: bot acc: 0.0595
top acc: 0.0456 ::: bot acc: 0.0754
top acc: 0.0794 ::: bot acc: 0.0369
current epoch: 22
train loss is 0.002543
average val loss: 0.002266, accuracy: 0.0544
average test loss: 0.001661, accuracy: 0.0466
case acc: 0.038601562
case acc: 0.048010655
case acc: 0.053030632
case acc: 0.049381755
case acc: 0.04683231
case acc: 0.043946095
top acc: 0.0465 ::: bot acc: 0.0580
top acc: 0.0892 ::: bot acc: 0.0110
top acc: 0.0390 ::: bot acc: 0.0927
top acc: 0.0767 ::: bot acc: 0.0562
top acc: 0.0480 ::: bot acc: 0.0735
top acc: 0.0813 ::: bot acc: 0.0348
current epoch: 23
train loss is 0.002540
average val loss: 0.002334, accuracy: 0.0543
average test loss: 0.001672, accuracy: 0.0468
case acc: 0.03858208
case acc: 0.050372493
case acc: 0.05131002
case acc: 0.05029791
case acc: 0.045834243
case acc: 0.04461265
top acc: 0.0530 ::: bot acc: 0.0516
top acc: 0.0920 ::: bot acc: 0.0126
top acc: 0.0451 ::: bot acc: 0.0865
top acc: 0.0820 ::: bot acc: 0.0503
top acc: 0.0519 ::: bot acc: 0.0695
top acc: 0.0853 ::: bot acc: 0.0307
current epoch: 24
train loss is 0.002540
average val loss: 0.002305, accuracy: 0.0539
average test loss: 0.001648, accuracy: 0.0465
case acc: 0.03879105
case acc: 0.04827223
case acc: 0.05123522
case acc: 0.05033623
case acc: 0.046170823
case acc: 0.04438546
top acc: 0.0535 ::: bot acc: 0.0509
top acc: 0.0896 ::: bot acc: 0.0114
top acc: 0.0462 ::: bot acc: 0.0857
top acc: 0.0822 ::: bot acc: 0.0502
top acc: 0.0512 ::: bot acc: 0.0703
top acc: 0.0842 ::: bot acc: 0.0317
current epoch: 25
train loss is 0.002521
average val loss: 0.002298, accuracy: 0.0538
average test loss: 0.001632, accuracy: 0.0463
case acc: 0.038996443
case acc: 0.046801116
case acc: 0.05108663
case acc: 0.050613448
case acc: 0.04600607
case acc: 0.044509526
top acc: 0.0550 ::: bot acc: 0.0495
top acc: 0.0873 ::: bot acc: 0.0108
top acc: 0.0476 ::: bot acc: 0.0844
top acc: 0.0828 ::: bot acc: 0.0497
top acc: 0.0508 ::: bot acc: 0.0703
top acc: 0.0840 ::: bot acc: 0.0322
current epoch: 26
train loss is 0.002480
average val loss: 0.002321, accuracy: 0.0536
average test loss: 0.001636, accuracy: 0.0465
case acc: 0.0393019
case acc: 0.046918646
case acc: 0.050602965
case acc: 0.051371656
case acc: 0.04580771
case acc: 0.044797327
top acc: 0.0580 ::: bot acc: 0.0467
top acc: 0.0876 ::: bot acc: 0.0109
top acc: 0.0508 ::: bot acc: 0.0811
top acc: 0.0852 ::: bot acc: 0.0472
top acc: 0.0528 ::: bot acc: 0.0684
top acc: 0.0854 ::: bot acc: 0.0310
current epoch: 27
train loss is 0.002466
average val loss: 0.002289, accuracy: 0.0532
average test loss: 0.001609, accuracy: 0.0460
case acc: 0.039253473
case acc: 0.04488849
case acc: 0.050612863
case acc: 0.05115012
case acc: 0.0459442
case acc: 0.044297796
top acc: 0.0578 ::: bot acc: 0.0465
top acc: 0.0851 ::: bot acc: 0.0104
top acc: 0.0507 ::: bot acc: 0.0813
top acc: 0.0849 ::: bot acc: 0.0478
top acc: 0.0522 ::: bot acc: 0.0695
top acc: 0.0836 ::: bot acc: 0.0322
current epoch: 28
train loss is 0.002442
average val loss: 0.002272, accuracy: 0.0531
average test loss: 0.001590, accuracy: 0.0457
case acc: 0.03924303
case acc: 0.042941686
case acc: 0.050443035
case acc: 0.051186014
case acc: 0.04607461
case acc: 0.044170912
top acc: 0.0579 ::: bot acc: 0.0465
top acc: 0.0824 ::: bot acc: 0.0095
top acc: 0.0507 ::: bot acc: 0.0810
top acc: 0.0848 ::: bot acc: 0.0480
top acc: 0.0517 ::: bot acc: 0.0701
top acc: 0.0827 ::: bot acc: 0.0336
current epoch: 29
train loss is 0.002407
average val loss: 0.002272, accuracy: 0.0529
average test loss: 0.001585, accuracy: 0.0456
case acc: 0.03955968
case acc: 0.04223581
case acc: 0.050295178
case acc: 0.051529545
case acc: 0.04579901
case acc: 0.044240918
top acc: 0.0592 ::: bot acc: 0.0453
top acc: 0.0815 ::: bot acc: 0.0094
top acc: 0.0519 ::: bot acc: 0.0796
top acc: 0.0861 ::: bot acc: 0.0467
top acc: 0.0522 ::: bot acc: 0.0688
top acc: 0.0832 ::: bot acc: 0.0330
current epoch: 30
train loss is 0.002377
average val loss: 0.002256, accuracy: 0.0527
average test loss: 0.001575, accuracy: 0.0454
case acc: 0.039693475
case acc: 0.04094992
case acc: 0.05031141
case acc: 0.051519077
case acc: 0.045918528
case acc: 0.044130567
top acc: 0.0596 ::: bot acc: 0.0452
top acc: 0.0799 ::: bot acc: 0.0093
top acc: 0.0519 ::: bot acc: 0.0797
top acc: 0.0860 ::: bot acc: 0.0466
top acc: 0.0522 ::: bot acc: 0.0690
top acc: 0.0828 ::: bot acc: 0.0334
current epoch: 31
train loss is 0.002372
average val loss: 0.002237, accuracy: 0.0524
average test loss: 0.001557, accuracy: 0.0452
case acc: 0.039456673
case acc: 0.03981217
case acc: 0.050399195
case acc: 0.05155646
case acc: 0.045866158
case acc: 0.044062983
top acc: 0.0590 ::: bot acc: 0.0456
top acc: 0.0775 ::: bot acc: 0.0098
top acc: 0.0522 ::: bot acc: 0.0796
top acc: 0.0861 ::: bot acc: 0.0466
top acc: 0.0517 ::: bot acc: 0.0695
top acc: 0.0820 ::: bot acc: 0.0342
current epoch: 32
train loss is 0.002326
average val loss: 0.002212, accuracy: 0.0522
average test loss: 0.001544, accuracy: 0.0450
case acc: 0.039485946
case acc: 0.03851857
case acc: 0.05047742
case acc: 0.05147238
case acc: 0.046068866
case acc: 0.04398131
top acc: 0.0584 ::: bot acc: 0.0459
top acc: 0.0754 ::: bot acc: 0.0106
top acc: 0.0517 ::: bot acc: 0.0802
top acc: 0.0856 ::: bot acc: 0.0471
top acc: 0.0510 ::: bot acc: 0.0706
top acc: 0.0810 ::: bot acc: 0.0353
current epoch: 33
train loss is 0.002299
average val loss: 0.002167, accuracy: 0.0519
average test loss: 0.001526, accuracy: 0.0447
case acc: 0.03916634
case acc: 0.03689111
case acc: 0.050592557
case acc: 0.05093292
case acc: 0.04658932
case acc: 0.043787636
top acc: 0.0568 ::: bot acc: 0.0478
top acc: 0.0723 ::: bot acc: 0.0122
top acc: 0.0501 ::: bot acc: 0.0817
top acc: 0.0843 ::: bot acc: 0.0483
top acc: 0.0487 ::: bot acc: 0.0731
top acc: 0.0794 ::: bot acc: 0.0372
current epoch: 34
train loss is 0.002268
average val loss: 0.002142, accuracy: 0.0518
average test loss: 0.001509, accuracy: 0.0444
case acc: 0.039049007
case acc: 0.035614878
case acc: 0.050771993
case acc: 0.050765652
case acc: 0.046792403
case acc: 0.043499053
top acc: 0.0553 ::: bot acc: 0.0494
top acc: 0.0695 ::: bot acc: 0.0135
top acc: 0.0490 ::: bot acc: 0.0829
top acc: 0.0832 ::: bot acc: 0.0494
top acc: 0.0474 ::: bot acc: 0.0741
top acc: 0.0779 ::: bot acc: 0.0382
current epoch: 35
train loss is 0.002236
average val loss: 0.002155, accuracy: 0.0518
average test loss: 0.001517, accuracy: 0.0445
case acc: 0.039127186
case acc: 0.035764735
case acc: 0.050685935
case acc: 0.051110793
case acc: 0.046757706
case acc: 0.043769456
top acc: 0.0566 ::: bot acc: 0.0482
top acc: 0.0697 ::: bot acc: 0.0139
top acc: 0.0501 ::: bot acc: 0.0819
top acc: 0.0847 ::: bot acc: 0.0477
top acc: 0.0485 ::: bot acc: 0.0733
top acc: 0.0791 ::: bot acc: 0.0376
current epoch: 36
train loss is 0.002243
average val loss: 0.002166, accuracy: 0.0515
average test loss: 0.001522, accuracy: 0.0446
case acc: 0.039322928
case acc: 0.03582579
case acc: 0.05045227
case acc: 0.051673785
case acc: 0.046526484
case acc: 0.044003975
top acc: 0.0578 ::: bot acc: 0.0467
top acc: 0.0703 ::: bot acc: 0.0131
top acc: 0.0518 ::: bot acc: 0.0803
top acc: 0.0864 ::: bot acc: 0.0462
top acc: 0.0496 ::: bot acc: 0.0721
top acc: 0.0804 ::: bot acc: 0.0364
current epoch: 37
train loss is 0.002235
average val loss: 0.002135, accuracy: 0.0514
average test loss: 0.001508, accuracy: 0.0445
case acc: 0.039270155
case acc: 0.035230692
case acc: 0.05056165
case acc: 0.051230192
case acc: 0.04673798
case acc: 0.043695003
top acc: 0.0563 ::: bot acc: 0.0486
top acc: 0.0678 ::: bot acc: 0.0156
top acc: 0.0502 ::: bot acc: 0.0815
top acc: 0.0851 ::: bot acc: 0.0474
top acc: 0.0478 ::: bot acc: 0.0736
top acc: 0.0786 ::: bot acc: 0.0381
current epoch: 38
train loss is 0.002214
average val loss: 0.002099, accuracy: 0.0513
average test loss: 0.001498, accuracy: 0.0443
case acc: 0.03897632
case acc: 0.034015898
case acc: 0.050962947
case acc: 0.050880402
case acc: 0.047342524
case acc: 0.04334608
top acc: 0.0545 ::: bot acc: 0.0503
top acc: 0.0650 ::: bot acc: 0.0180
top acc: 0.0489 ::: bot acc: 0.0833
top acc: 0.0838 ::: bot acc: 0.0489
top acc: 0.0460 ::: bot acc: 0.0760
top acc: 0.0764 ::: bot acc: 0.0400
current epoch: 39
train loss is 0.002184
average val loss: 0.002068, accuracy: 0.0512
average test loss: 0.001485, accuracy: 0.0441
case acc: 0.038749747
case acc: 0.03345292
case acc: 0.050957184
case acc: 0.05036788
case acc: 0.047770873
case acc: 0.04321602
top acc: 0.0526 ::: bot acc: 0.0519
top acc: 0.0627 ::: bot acc: 0.0205
top acc: 0.0474 ::: bot acc: 0.0844
top acc: 0.0818 ::: bot acc: 0.0507
top acc: 0.0441 ::: bot acc: 0.0777
top acc: 0.0744 ::: bot acc: 0.0421
current epoch: 40
train loss is 0.002166
average val loss: 0.002090, accuracy: 0.0512
average test loss: 0.001488, accuracy: 0.0441
case acc: 0.038881265
case acc: 0.03383559
case acc: 0.050809935
case acc: 0.05092276
case acc: 0.047305364
case acc: 0.043116946
top acc: 0.0543 ::: bot acc: 0.0503
top acc: 0.0640 ::: bot acc: 0.0191
top acc: 0.0497 ::: bot acc: 0.0825
top acc: 0.0840 ::: bot acc: 0.0486
top acc: 0.0455 ::: bot acc: 0.0760
top acc: 0.0756 ::: bot acc: 0.0403
current epoch: 41
train loss is 0.002163
average val loss: 0.002092, accuracy: 0.0511
average test loss: 0.001491, accuracy: 0.0442
case acc: 0.038996134
case acc: 0.033791393
case acc: 0.050624944
case acc: 0.051117755
case acc: 0.04733856
case acc: 0.043224607
top acc: 0.0547 ::: bot acc: 0.0502
top acc: 0.0639 ::: bot acc: 0.0195
top acc: 0.0499 ::: bot acc: 0.0822
top acc: 0.0844 ::: bot acc: 0.0484
top acc: 0.0457 ::: bot acc: 0.0759
top acc: 0.0758 ::: bot acc: 0.0406
current epoch: 42
train loss is 0.002161
average val loss: 0.002085, accuracy: 0.0510
average test loss: 0.001489, accuracy: 0.0442
case acc: 0.03909047
case acc: 0.03371371
case acc: 0.050685536
case acc: 0.05107897
case acc: 0.04717992
case acc: 0.043198988
top acc: 0.0545 ::: bot acc: 0.0505
top acc: 0.0633 ::: bot acc: 0.0202
top acc: 0.0500 ::: bot acc: 0.0823
top acc: 0.0845 ::: bot acc: 0.0482
top acc: 0.0456 ::: bot acc: 0.0758
top acc: 0.0756 ::: bot acc: 0.0408
current epoch: 43
train loss is 0.002152
average val loss: 0.002083, accuracy: 0.0511
average test loss: 0.001483, accuracy: 0.0441
case acc: 0.038966108
case acc: 0.033382792
case acc: 0.050565396
case acc: 0.051067166
case acc: 0.047288317
case acc: 0.04328627
top acc: 0.0540 ::: bot acc: 0.0507
top acc: 0.0624 ::: bot acc: 0.0209
top acc: 0.0495 ::: bot acc: 0.0821
top acc: 0.0842 ::: bot acc: 0.0484
top acc: 0.0457 ::: bot acc: 0.0761
top acc: 0.0750 ::: bot acc: 0.0415
current epoch: 44
train loss is 0.002135
average val loss: 0.002060, accuracy: 0.0510
average test loss: 0.001477, accuracy: 0.0440
case acc: 0.03869988
case acc: 0.032856055
case acc: 0.050943673
case acc: 0.050689235
case acc: 0.047668163
case acc: 0.04312399
top acc: 0.0523 ::: bot acc: 0.0522
top acc: 0.0604 ::: bot acc: 0.0227
top acc: 0.0484 ::: bot acc: 0.0838
top acc: 0.0828 ::: bot acc: 0.0498
top acc: 0.0445 ::: bot acc: 0.0773
top acc: 0.0734 ::: bot acc: 0.0431
current epoch: 45
train loss is 0.002137
average val loss: 0.002061, accuracy: 0.0510
average test loss: 0.001476, accuracy: 0.0440
case acc: 0.03868667
case acc: 0.032857616
case acc: 0.050899792
case acc: 0.050679855
case acc: 0.047545604
case acc: 0.0431819
top acc: 0.0523 ::: bot acc: 0.0524
top acc: 0.0604 ::: bot acc: 0.0231
top acc: 0.0486 ::: bot acc: 0.0835
top acc: 0.0833 ::: bot acc: 0.0490
top acc: 0.0449 ::: bot acc: 0.0768
top acc: 0.0735 ::: bot acc: 0.0428
current epoch: 46
train loss is 0.002127
average val loss: 0.002058, accuracy: 0.0509
average test loss: 0.001476, accuracy: 0.0440
case acc: 0.038718447
case acc: 0.03266788
case acc: 0.05096533
case acc: 0.050784566
case acc: 0.047627967
case acc: 0.042993538
top acc: 0.0522 ::: bot acc: 0.0524
top acc: 0.0597 ::: bot acc: 0.0235
top acc: 0.0485 ::: bot acc: 0.0838
top acc: 0.0835 ::: bot acc: 0.0493
top acc: 0.0446 ::: bot acc: 0.0771
top acc: 0.0733 ::: bot acc: 0.0429
current epoch: 47
train loss is 0.002118
average val loss: 0.002055, accuracy: 0.0510
average test loss: 0.001476, accuracy: 0.0440
case acc: 0.038889963
case acc: 0.032505166
case acc: 0.050915938
case acc: 0.050860863
case acc: 0.047594372
case acc: 0.043074578
top acc: 0.0519 ::: bot acc: 0.0532
top acc: 0.0590 ::: bot acc: 0.0243
top acc: 0.0483 ::: bot acc: 0.0838
top acc: 0.0834 ::: bot acc: 0.0494
top acc: 0.0447 ::: bot acc: 0.0772
top acc: 0.0731 ::: bot acc: 0.0432
current epoch: 48
train loss is 0.002112
average val loss: 0.002031, accuracy: 0.0509
average test loss: 0.001473, accuracy: 0.0439
case acc: 0.03883829
case acc: 0.032083683
case acc: 0.051253248
case acc: 0.050372522
case acc: 0.04788375
case acc: 0.04307883
top acc: 0.0504 ::: bot acc: 0.0546
top acc: 0.0572 ::: bot acc: 0.0261
top acc: 0.0468 ::: bot acc: 0.0855
top acc: 0.0821 ::: bot acc: 0.0504
top acc: 0.0434 ::: bot acc: 0.0783
top acc: 0.0721 ::: bot acc: 0.0444
current epoch: 49
train loss is 0.002098
average val loss: 0.002019, accuracy: 0.0510
average test loss: 0.001473, accuracy: 0.0439
case acc: 0.038866818
case acc: 0.03166513
case acc: 0.051531807
case acc: 0.05013568
case acc: 0.0482123
case acc: 0.042928156
top acc: 0.0488 ::: bot acc: 0.0564
top acc: 0.0555 ::: bot acc: 0.0277
top acc: 0.0454 ::: bot acc: 0.0867
top acc: 0.0811 ::: bot acc: 0.0516
top acc: 0.0420 ::: bot acc: 0.0796
top acc: 0.0706 ::: bot acc: 0.0456
current epoch: 50
train loss is 0.002098
average val loss: 0.002018, accuracy: 0.0509
average test loss: 0.001477, accuracy: 0.0439
case acc: 0.038955327
case acc: 0.031695444
case acc: 0.051386036
case acc: 0.050284866
case acc: 0.048222277
case acc: 0.043112285
top acc: 0.0491 ::: bot acc: 0.0563
top acc: 0.0555 ::: bot acc: 0.0277
top acc: 0.0455 ::: bot acc: 0.0866
top acc: 0.0817 ::: bot acc: 0.0508
top acc: 0.0428 ::: bot acc: 0.0793
top acc: 0.0715 ::: bot acc: 0.0451
LME_Co_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5358 5358 5358
1.7082474 -0.6288155 0.2588177 -0.21218425
Validation: 600 600 600
Testing: 750 750 750
pre-processing time: 0.00023603439331054688
the split date is 2012-01-01
train dropout: 0.4 test dropout: 0.1
net initializing with time: 0.0023148059844970703
preparing training and testing date with time: 0.0
current epoch: 1
train loss is 0.011003
average val loss: 0.005952, accuracy: 0.0936
average test loss: 0.004147, accuracy: 0.0715
case acc: 0.07644778
case acc: 0.15510102
case acc: 0.06710241
case acc: 0.0456885
case acc: 0.038908318
case acc: 0.045662157
top acc: 0.0269 ::: bot acc: 0.1314
top acc: 0.2060 ::: bot acc: 0.0945
top acc: 0.0818 ::: bot acc: 0.0988
top acc: 0.0704 ::: bot acc: 0.0579
top acc: 0.0395 ::: bot acc: 0.0643
top acc: 0.0767 ::: bot acc: 0.0429
current epoch: 2
train loss is 0.009249
average val loss: 0.007574, accuracy: 0.0949
average test loss: 0.012337, accuracy: 0.1332
case acc: 0.058007568
case acc: 0.26728198
case acc: 0.11632562
case acc: 0.12542409
case acc: 0.102767706
case acc: 0.12949568
top acc: 0.1034 ::: bot acc: 0.0170
top acc: 0.3181 ::: bot acc: 0.2064
top acc: 0.2065 ::: bot acc: 0.0359
top acc: 0.1894 ::: bot acc: 0.0641
top acc: 0.1563 ::: bot acc: 0.0518
top acc: 0.1905 ::: bot acc: 0.0695
current epoch: 3
train loss is 0.011098
average val loss: 0.011122, accuracy: 0.1215
average test loss: 0.018131, accuracy: 0.1720
case acc: 0.09792506
case acc: 0.30016562
case acc: 0.15881412
case acc: 0.16507685
case acc: 0.14512457
case acc: 0.16470502
top acc: 0.1491 ::: bot acc: 0.0432
top acc: 0.3519 ::: bot acc: 0.2391
top acc: 0.2518 ::: bot acc: 0.0719
top acc: 0.2308 ::: bot acc: 0.1016
top acc: 0.1982 ::: bot acc: 0.0941
top acc: 0.2249 ::: bot acc: 0.1056
current epoch: 4
train loss is 0.013123
average val loss: 0.003990, accuracy: 0.0726
average test loss: 0.004813, accuracy: 0.0760
case acc: 0.039930433
case acc: 0.17426254
case acc: 0.07176547
case acc: 0.06276138
case acc: 0.04875554
case acc: 0.058519702
top acc: 0.0385 ::: bot acc: 0.0685
top acc: 0.2253 ::: bot acc: 0.1139
top acc: 0.1369 ::: bot acc: 0.0423
top acc: 0.1148 ::: bot acc: 0.0270
top acc: 0.0902 ::: bot acc: 0.0218
top acc: 0.1077 ::: bot acc: 0.0236
current epoch: 5
train loss is 0.007920
average val loss: 0.004764, accuracy: 0.0828
average test loss: 0.003014, accuracy: 0.0619
case acc: 0.06304195
case acc: 0.11455327
case acc: 0.06631392
case acc: 0.045590397
case acc: 0.037635595
case acc: 0.04406427
top acc: 0.0208 ::: bot acc: 0.1142
top acc: 0.1658 ::: bot acc: 0.0540
top acc: 0.0852 ::: bot acc: 0.0939
top acc: 0.0657 ::: bot acc: 0.0631
top acc: 0.0511 ::: bot acc: 0.0529
top acc: 0.0585 ::: bot acc: 0.0625
current epoch: 6
train loss is 0.004577
average val loss: 0.003364, accuracy: 0.0674
average test loss: 0.004090, accuracy: 0.0715
case acc: 0.03987237
case acc: 0.14944391
case acc: 0.06883573
case acc: 0.059066337
case acc: 0.05650019
case acc: 0.055483878
top acc: 0.0402 ::: bot acc: 0.0667
top acc: 0.2006 ::: bot acc: 0.0893
top acc: 0.1271 ::: bot acc: 0.0518
top acc: 0.1086 ::: bot acc: 0.0275
top acc: 0.1025 ::: bot acc: 0.0196
top acc: 0.1024 ::: bot acc: 0.0249
current epoch: 7
train loss is 0.004788
average val loss: 0.003173, accuracy: 0.0649
average test loss: 0.004693, accuracy: 0.0781
case acc: 0.039792705
case acc: 0.15498699
case acc: 0.07258316
case acc: 0.06774258
case acc: 0.070243336
case acc: 0.063393414
top acc: 0.0564 ::: bot acc: 0.0505
top acc: 0.2058 ::: bot acc: 0.0943
top acc: 0.1387 ::: bot acc: 0.0413
top acc: 0.1221 ::: bot acc: 0.0271
top acc: 0.1208 ::: bot acc: 0.0253
top acc: 0.1159 ::: bot acc: 0.0223
current epoch: 8
train loss is 0.004787
average val loss: 0.002978, accuracy: 0.0636
average test loss: 0.003517, accuracy: 0.0672
case acc: 0.040064692
case acc: 0.1280089
case acc: 0.06726702
case acc: 0.055881474
case acc: 0.059695434
case acc: 0.052192144
top acc: 0.0415 ::: bot acc: 0.0667
top acc: 0.1785 ::: bot acc: 0.0683
top acc: 0.1190 ::: bot acc: 0.0611
top acc: 0.1032 ::: bot acc: 0.0299
top acc: 0.1070 ::: bot acc: 0.0211
top acc: 0.0956 ::: bot acc: 0.0281
current epoch: 9
train loss is 0.004078
average val loss: 0.002812, accuracy: 0.0617
average test loss: 0.003399, accuracy: 0.0664
case acc: 0.03926344
case acc: 0.12041653
case acc: 0.066956624
case acc: 0.05639217
case acc: 0.06297221
case acc: 0.05226077
top acc: 0.0437 ::: bot acc: 0.0633
top acc: 0.1714 ::: bot acc: 0.0606
top acc: 0.1186 ::: bot acc: 0.0611
top acc: 0.1047 ::: bot acc: 0.0301
top acc: 0.1113 ::: bot acc: 0.0215
top acc: 0.0953 ::: bot acc: 0.0280
current epoch: 10
train loss is 0.003682
average val loss: 0.002616, accuracy: 0.0595
average test loss: 0.003604, accuracy: 0.0690
case acc: 0.039319534
case acc: 0.119833045
case acc: 0.06812945
case acc: 0.06022792
case acc: 0.0708688
case acc: 0.055667102
top acc: 0.0535 ::: bot acc: 0.0537
top acc: 0.1706 ::: bot acc: 0.0597
top acc: 0.1245 ::: bot acc: 0.0552
top acc: 0.1113 ::: bot acc: 0.0274
top acc: 0.1213 ::: bot acc: 0.0257
top acc: 0.1023 ::: bot acc: 0.0251
current epoch: 11
train loss is 0.003617
average val loss: 0.002474, accuracy: 0.0578
average test loss: 0.003673, accuracy: 0.0701
case acc: 0.039991334
case acc: 0.11667671
case acc: 0.06890715
case acc: 0.06244477
case acc: 0.07561934
case acc: 0.05711466
top acc: 0.0585 ::: bot acc: 0.0482
top acc: 0.1675 ::: bot acc: 0.0571
top acc: 0.1272 ::: bot acc: 0.0527
top acc: 0.1150 ::: bot acc: 0.0272
top acc: 0.1268 ::: bot acc: 0.0287
top acc: 0.1046 ::: bot acc: 0.0243
current epoch: 12
train loss is 0.003358
average val loss: 0.002375, accuracy: 0.0566
average test loss: 0.003440, accuracy: 0.0679
case acc: 0.039946422
case acc: 0.10748633
case acc: 0.068243094
case acc: 0.060722474
case acc: 0.07515012
case acc: 0.055741407
top acc: 0.0587 ::: bot acc: 0.0484
top acc: 0.1582 ::: bot acc: 0.0479
top acc: 0.1245 ::: bot acc: 0.0559
top acc: 0.1127 ::: bot acc: 0.0268
top acc: 0.1262 ::: bot acc: 0.0285
top acc: 0.1018 ::: bot acc: 0.0255
current epoch: 13
train loss is 0.003141
average val loss: 0.002284, accuracy: 0.0554
average test loss: 0.003388, accuracy: 0.0676
case acc: 0.040686022
case acc: 0.10209741
case acc: 0.06832025
case acc: 0.06166107
case acc: 0.07694431
case acc: 0.055870906
top acc: 0.0621 ::: bot acc: 0.0454
top acc: 0.1526 ::: bot acc: 0.0424
top acc: 0.1246 ::: bot acc: 0.0555
top acc: 0.1134 ::: bot acc: 0.0278
top acc: 0.1283 ::: bot acc: 0.0297
top acc: 0.1026 ::: bot acc: 0.0253
current epoch: 14
train loss is 0.002992
average val loss: 0.002200, accuracy: 0.0542
average test loss: 0.003344, accuracy: 0.0672
case acc: 0.041122552
case acc: 0.09719341
case acc: 0.06839999
case acc: 0.062135678
case acc: 0.078034274
case acc: 0.056210008
top acc: 0.0641 ::: bot acc: 0.0428
top acc: 0.1482 ::: bot acc: 0.0374
top acc: 0.1254 ::: bot acc: 0.0546
top acc: 0.1147 ::: bot acc: 0.0270
top acc: 0.1295 ::: bot acc: 0.0305
top acc: 0.1031 ::: bot acc: 0.0255
current epoch: 15
train loss is 0.002854
average val loss: 0.002126, accuracy: 0.0532
average test loss: 0.003308, accuracy: 0.0669
case acc: 0.041937016
case acc: 0.09261106
case acc: 0.06888156
case acc: 0.06287497
case acc: 0.07905791
case acc: 0.05622903
top acc: 0.0667 ::: bot acc: 0.0401
top acc: 0.1433 ::: bot acc: 0.0328
top acc: 0.1270 ::: bot acc: 0.0536
top acc: 0.1156 ::: bot acc: 0.0268
top acc: 0.1306 ::: bot acc: 0.0310
top acc: 0.1028 ::: bot acc: 0.0251
current epoch: 16
train loss is 0.002710
average val loss: 0.002070, accuracy: 0.0523
average test loss: 0.003172, accuracy: 0.0654
case acc: 0.042271394
case acc: 0.08630684
case acc: 0.06834571
case acc: 0.06259718
case acc: 0.07775122
case acc: 0.055189013
top acc: 0.0677 ::: bot acc: 0.0397
top acc: 0.1370 ::: bot acc: 0.0268
top acc: 0.1254 ::: bot acc: 0.0542
top acc: 0.1148 ::: bot acc: 0.0275
top acc: 0.1288 ::: bot acc: 0.0304
top acc: 0.1013 ::: bot acc: 0.0258
current epoch: 17
train loss is 0.002628
average val loss: 0.002008, accuracy: 0.0517
average test loss: 0.003324, accuracy: 0.0672
case acc: 0.04370136
case acc: 0.08600672
case acc: 0.06958701
case acc: 0.06554758
case acc: 0.08110025
case acc: 0.05723136
top acc: 0.0735 ::: bot acc: 0.0335
top acc: 0.1367 ::: bot acc: 0.0262
top acc: 0.1301 ::: bot acc: 0.0499
top acc: 0.1190 ::: bot acc: 0.0274
top acc: 0.1331 ::: bot acc: 0.0325
top acc: 0.1053 ::: bot acc: 0.0247
current epoch: 18
train loss is 0.002565
average val loss: 0.001947, accuracy: 0.0511
average test loss: 0.003390, accuracy: 0.0680
case acc: 0.04536574
case acc: 0.08427901
case acc: 0.07056062
case acc: 0.0674189
case acc: 0.08231025
case acc: 0.057923406
top acc: 0.0776 ::: bot acc: 0.0299
top acc: 0.1349 ::: bot acc: 0.0248
top acc: 0.1333 ::: bot acc: 0.0469
top acc: 0.1218 ::: bot acc: 0.0273
top acc: 0.1347 ::: bot acc: 0.0334
top acc: 0.1066 ::: bot acc: 0.0238
current epoch: 19
train loss is 0.002513
average val loss: 0.001907, accuracy: 0.0506
average test loss: 0.003460, accuracy: 0.0688
case acc: 0.0466256
case acc: 0.08293747
case acc: 0.07153637
case acc: 0.06913758
case acc: 0.083712384
case acc: 0.058766834
top acc: 0.0810 ::: bot acc: 0.0267
top acc: 0.1334 ::: bot acc: 0.0237
top acc: 0.1360 ::: bot acc: 0.0437
top acc: 0.1241 ::: bot acc: 0.0275
top acc: 0.1361 ::: bot acc: 0.0344
top acc: 0.1080 ::: bot acc: 0.0233
current epoch: 20
train loss is 0.002467
average val loss: 0.001876, accuracy: 0.0503
average test loss: 0.003477, accuracy: 0.0690
case acc: 0.04794872
case acc: 0.08073435
case acc: 0.072245575
case acc: 0.07033996
case acc: 0.08375256
case acc: 0.058746167
top acc: 0.0839 ::: bot acc: 0.0244
top acc: 0.1308 ::: bot acc: 0.0223
top acc: 0.1383 ::: bot acc: 0.0415
top acc: 0.1260 ::: bot acc: 0.0277
top acc: 0.1362 ::: bot acc: 0.0342
top acc: 0.1080 ::: bot acc: 0.0233
current epoch: 21
train loss is 0.002376
average val loss: 0.001836, accuracy: 0.0498
average test loss: 0.003402, accuracy: 0.0682
case acc: 0.048166085
case acc: 0.077430576
case acc: 0.072457306
case acc: 0.070509985
case acc: 0.08241382
case acc: 0.05806912
top acc: 0.0848 ::: bot acc: 0.0235
top acc: 0.1271 ::: bot acc: 0.0203
top acc: 0.1385 ::: bot acc: 0.0413
top acc: 0.1260 ::: bot acc: 0.0281
top acc: 0.1344 ::: bot acc: 0.0335
top acc: 0.1068 ::: bot acc: 0.0236
current epoch: 22
train loss is 0.002320
average val loss: 0.001812, accuracy: 0.0494
average test loss: 0.003380, accuracy: 0.0679
case acc: 0.04854174
case acc: 0.0751671
case acc: 0.072937146
case acc: 0.07094646
case acc: 0.082109205
case acc: 0.057769846
top acc: 0.0864 ::: bot acc: 0.0220
top acc: 0.1242 ::: bot acc: 0.0194
top acc: 0.1397 ::: bot acc: 0.0400
top acc: 0.1266 ::: bot acc: 0.0279
top acc: 0.1342 ::: bot acc: 0.0332
top acc: 0.1063 ::: bot acc: 0.0235
current epoch: 23
train loss is 0.002263
average val loss: 0.001789, accuracy: 0.0492
average test loss: 0.003490, accuracy: 0.0692
case acc: 0.050661154
case acc: 0.074939996
case acc: 0.07443113
case acc: 0.07279468
case acc: 0.08353737
case acc: 0.058792535
top acc: 0.0902 ::: bot acc: 0.0202
top acc: 0.1238 ::: bot acc: 0.0190
top acc: 0.1435 ::: bot acc: 0.0378
top acc: 0.1291 ::: bot acc: 0.0282
top acc: 0.1356 ::: bot acc: 0.0345
top acc: 0.1077 ::: bot acc: 0.0235
current epoch: 24
train loss is 0.002247
average val loss: 0.001779, accuracy: 0.0492
average test loss: 0.003580, accuracy: 0.0702
case acc: 0.052346777
case acc: 0.07434252
case acc: 0.07583973
case acc: 0.07468526
case acc: 0.08450917
case acc: 0.05942993
top acc: 0.0933 ::: bot acc: 0.0192
top acc: 0.1233 ::: bot acc: 0.0190
top acc: 0.1463 ::: bot acc: 0.0358
top acc: 0.1316 ::: bot acc: 0.0287
top acc: 0.1367 ::: bot acc: 0.0353
top acc: 0.1089 ::: bot acc: 0.0235
current epoch: 25
train loss is 0.002212
average val loss: 0.001761, accuracy: 0.0490
average test loss: 0.003572, accuracy: 0.0701
case acc: 0.053005952
case acc: 0.0726292
case acc: 0.07636961
case acc: 0.0751861
case acc: 0.084152386
case acc: 0.059357475
top acc: 0.0943 ::: bot acc: 0.0186
top acc: 0.1208 ::: bot acc: 0.0182
top acc: 0.1476 ::: bot acc: 0.0350
top acc: 0.1323 ::: bot acc: 0.0289
top acc: 0.1366 ::: bot acc: 0.0347
top acc: 0.1091 ::: bot acc: 0.0232
current epoch: 26
train loss is 0.002190
average val loss: 0.001755, accuracy: 0.0491
average test loss: 0.003681, accuracy: 0.0713
case acc: 0.054847974
case acc: 0.07245399
case acc: 0.07786727
case acc: 0.07696897
case acc: 0.08583977
case acc: 0.059968017
top acc: 0.0975 ::: bot acc: 0.0181
top acc: 0.1208 ::: bot acc: 0.0179
top acc: 0.1507 ::: bot acc: 0.0335
top acc: 0.1347 ::: bot acc: 0.0295
top acc: 0.1384 ::: bot acc: 0.0364
top acc: 0.1099 ::: bot acc: 0.0230
current epoch: 27
train loss is 0.002200
average val loss: 0.001749, accuracy: 0.0491
average test loss: 0.003771, accuracy: 0.0723
case acc: 0.056355983
case acc: 0.07209217
case acc: 0.07922852
case acc: 0.07870721
case acc: 0.08693961
case acc: 0.060622487
top acc: 0.1000 ::: bot acc: 0.0176
top acc: 0.1203 ::: bot acc: 0.0180
top acc: 0.1530 ::: bot acc: 0.0325
top acc: 0.1369 ::: bot acc: 0.0303
top acc: 0.1392 ::: bot acc: 0.0371
top acc: 0.1109 ::: bot acc: 0.0228
current epoch: 28
train loss is 0.002170
average val loss: 0.001750, accuracy: 0.0493
average test loss: 0.003860, accuracy: 0.0733
case acc: 0.057826933
case acc: 0.07191041
case acc: 0.08036012
case acc: 0.08018416
case acc: 0.08820564
case acc: 0.061164696
top acc: 0.1022 ::: bot acc: 0.0173
top acc: 0.1200 ::: bot acc: 0.0181
top acc: 0.1553 ::: bot acc: 0.0312
top acc: 0.1389 ::: bot acc: 0.0307
top acc: 0.1407 ::: bot acc: 0.0384
top acc: 0.1120 ::: bot acc: 0.0225
current epoch: 29
train loss is 0.002160
average val loss: 0.001738, accuracy: 0.0491
average test loss: 0.003851, accuracy: 0.0732
case acc: 0.058338393
case acc: 0.07057208
case acc: 0.08098323
case acc: 0.080290765
case acc: 0.08822027
case acc: 0.060947802
top acc: 0.1029 ::: bot acc: 0.0174
top acc: 0.1182 ::: bot acc: 0.0178
top acc: 0.1560 ::: bot acc: 0.0314
top acc: 0.1392 ::: bot acc: 0.0307
top acc: 0.1406 ::: bot acc: 0.0383
top acc: 0.1114 ::: bot acc: 0.0226
current epoch: 30
train loss is 0.002124
average val loss: 0.001711, accuracy: 0.0487
average test loss: 0.003747, accuracy: 0.0720
case acc: 0.057622865
case acc: 0.068260625
case acc: 0.080319
case acc: 0.079431094
case acc: 0.086641386
case acc: 0.059975483
top acc: 0.1021 ::: bot acc: 0.0174
top acc: 0.1148 ::: bot acc: 0.0176
top acc: 0.1549 ::: bot acc: 0.0317
top acc: 0.1381 ::: bot acc: 0.0304
top acc: 0.1392 ::: bot acc: 0.0366
top acc: 0.1100 ::: bot acc: 0.0229
current epoch: 31
train loss is 0.002067
average val loss: 0.001692, accuracy: 0.0483
average test loss: 0.003684, accuracy: 0.0714
case acc: 0.05726693
case acc: 0.066499986
case acc: 0.08015489
case acc: 0.079151526
case acc: 0.08631257
case acc: 0.059133526
top acc: 0.1013 ::: bot acc: 0.0174
top acc: 0.1119 ::: bot acc: 0.0181
top acc: 0.1549 ::: bot acc: 0.0318
top acc: 0.1375 ::: bot acc: 0.0307
top acc: 0.1386 ::: bot acc: 0.0368
top acc: 0.1088 ::: bot acc: 0.0230
current epoch: 32
train loss is 0.002048
average val loss: 0.001671, accuracy: 0.0480
average test loss: 0.003580, accuracy: 0.0702
case acc: 0.056384534
case acc: 0.06430912
case acc: 0.079510905
case acc: 0.07817907
case acc: 0.084879935
case acc: 0.058062997
top acc: 0.1000 ::: bot acc: 0.0177
top acc: 0.1085 ::: bot acc: 0.0180
top acc: 0.1537 ::: bot acc: 0.0321
top acc: 0.1360 ::: bot acc: 0.0301
top acc: 0.1371 ::: bot acc: 0.0353
top acc: 0.1069 ::: bot acc: 0.0236
current epoch: 33
train loss is 0.002022
average val loss: 0.001653, accuracy: 0.0477
average test loss: 0.003447, accuracy: 0.0688
case acc: 0.055300724
case acc: 0.062234826
case acc: 0.07869528
case acc: 0.07688532
case acc: 0.082778536
case acc: 0.05669557
top acc: 0.0982 ::: bot acc: 0.0180
top acc: 0.1053 ::: bot acc: 0.0188
top acc: 0.1519 ::: bot acc: 0.0330
top acc: 0.1346 ::: bot acc: 0.0297
top acc: 0.1346 ::: bot acc: 0.0338
top acc: 0.1042 ::: bot acc: 0.0244
current epoch: 34
train loss is 0.001979
average val loss: 0.001649, accuracy: 0.0477
average test loss: 0.003472, accuracy: 0.0690
case acc: 0.055667683
case acc: 0.061955657
case acc: 0.07908687
case acc: 0.07737751
case acc: 0.082907826
case acc: 0.056978554
top acc: 0.0987 ::: bot acc: 0.0178
top acc: 0.1048 ::: bot acc: 0.0189
top acc: 0.1528 ::: bot acc: 0.0323
top acc: 0.1352 ::: bot acc: 0.0296
top acc: 0.1349 ::: bot acc: 0.0339
top acc: 0.1049 ::: bot acc: 0.0245
current epoch: 35
train loss is 0.001968
average val loss: 0.001650, accuracy: 0.0478
average test loss: 0.003549, accuracy: 0.0698
case acc: 0.056649253
case acc: 0.061959557
case acc: 0.08007705
case acc: 0.07886056
case acc: 0.083904855
case acc: 0.05763113
top acc: 0.1004 ::: bot acc: 0.0176
top acc: 0.1051 ::: bot acc: 0.0182
top acc: 0.1547 ::: bot acc: 0.0317
top acc: 0.1367 ::: bot acc: 0.0303
top acc: 0.1360 ::: bot acc: 0.0349
top acc: 0.1061 ::: bot acc: 0.0239
current epoch: 36
train loss is 0.001959
average val loss: 0.001642, accuracy: 0.0476
average test loss: 0.003480, accuracy: 0.0691
case acc: 0.056058783
case acc: 0.060893796
case acc: 0.07980356
case acc: 0.07827656
case acc: 0.082646705
case acc: 0.05684684
top acc: 0.0996 ::: bot acc: 0.0178
top acc: 0.1032 ::: bot acc: 0.0190
top acc: 0.1540 ::: bot acc: 0.0321
top acc: 0.1362 ::: bot acc: 0.0301
top acc: 0.1345 ::: bot acc: 0.0340
top acc: 0.1047 ::: bot acc: 0.0244
current epoch: 37
train loss is 0.001951
average val loss: 0.001642, accuracy: 0.0477
average test loss: 0.003503, accuracy: 0.0693
case acc: 0.05627511
case acc: 0.060713187
case acc: 0.080110215
case acc: 0.07884074
case acc: 0.08298915
case acc: 0.057123777
top acc: 0.0998 ::: bot acc: 0.0177
top acc: 0.1029 ::: bot acc: 0.0192
top acc: 0.1546 ::: bot acc: 0.0315
top acc: 0.1370 ::: bot acc: 0.0305
top acc: 0.1348 ::: bot acc: 0.0341
top acc: 0.1049 ::: bot acc: 0.0245
current epoch: 38
train loss is 0.001940
average val loss: 0.001646, accuracy: 0.0479
average test loss: 0.003564, accuracy: 0.0700
case acc: 0.05698111
case acc: 0.060714033
case acc: 0.08109584
case acc: 0.07988847
case acc: 0.08370585
case acc: 0.057475105
top acc: 0.1009 ::: bot acc: 0.0175
top acc: 0.1029 ::: bot acc: 0.0191
top acc: 0.1564 ::: bot acc: 0.0313
top acc: 0.1383 ::: bot acc: 0.0307
top acc: 0.1357 ::: bot acc: 0.0344
top acc: 0.1059 ::: bot acc: 0.0237
current epoch: 39
train loss is 0.001957
average val loss: 0.001636, accuracy: 0.0476
average test loss: 0.003481, accuracy: 0.0690
case acc: 0.056201987
case acc: 0.059631474
case acc: 0.08058868
case acc: 0.0789281
case acc: 0.08232688
case acc: 0.05660489
top acc: 0.0996 ::: bot acc: 0.0179
top acc: 0.1009 ::: bot acc: 0.0197
top acc: 0.1554 ::: bot acc: 0.0315
top acc: 0.1372 ::: bot acc: 0.0303
top acc: 0.1342 ::: bot acc: 0.0336
top acc: 0.1041 ::: bot acc: 0.0246
current epoch: 40
train loss is 0.001920
average val loss: 0.001632, accuracy: 0.0476
average test loss: 0.003501, accuracy: 0.0693
case acc: 0.056440722
case acc: 0.059457358
case acc: 0.08111722
case acc: 0.07930295
case acc: 0.082605615
case acc: 0.056631453
top acc: 0.1000 ::: bot acc: 0.0176
top acc: 0.1005 ::: bot acc: 0.0198
top acc: 0.1563 ::: bot acc: 0.0312
top acc: 0.1376 ::: bot acc: 0.0306
top acc: 0.1347 ::: bot acc: 0.0337
top acc: 0.1041 ::: bot acc: 0.0247
current epoch: 41
train loss is 0.001933
average val loss: 0.001637, accuracy: 0.0478
average test loss: 0.003608, accuracy: 0.0704
case acc: 0.057603404
case acc: 0.06017806
case acc: 0.08224577
case acc: 0.08103648
case acc: 0.08411708
case acc: 0.057477657
top acc: 0.1018 ::: bot acc: 0.0174
top acc: 0.1019 ::: bot acc: 0.0194
top acc: 0.1585 ::: bot acc: 0.0303
top acc: 0.1398 ::: bot acc: 0.0313
top acc: 0.1362 ::: bot acc: 0.0348
top acc: 0.1058 ::: bot acc: 0.0239
current epoch: 42
train loss is 0.001916
average val loss: 0.001637, accuracy: 0.0478
average test loss: 0.003569, accuracy: 0.0700
case acc: 0.05716055
case acc: 0.059444472
case acc: 0.08229469
case acc: 0.08060003
case acc: 0.083500676
case acc: 0.057022974
top acc: 0.1011 ::: bot acc: 0.0176
top acc: 0.1007 ::: bot acc: 0.0195
top acc: 0.1584 ::: bot acc: 0.0305
top acc: 0.1393 ::: bot acc: 0.0309
top acc: 0.1356 ::: bot acc: 0.0345
top acc: 0.1047 ::: bot acc: 0.0245
current epoch: 43
train loss is 0.001914
average val loss: 0.001631, accuracy: 0.0477
average test loss: 0.003590, accuracy: 0.0702
case acc: 0.057204217
case acc: 0.059416644
case acc: 0.08253711
case acc: 0.08094293
case acc: 0.08385495
case acc: 0.056983355
top acc: 0.1014 ::: bot acc: 0.0174
top acc: 0.1007 ::: bot acc: 0.0197
top acc: 0.1589 ::: bot acc: 0.0303
top acc: 0.1399 ::: bot acc: 0.0310
top acc: 0.1359 ::: bot acc: 0.0347
top acc: 0.1049 ::: bot acc: 0.0242
current epoch: 44
train loss is 0.001903
average val loss: 0.001631, accuracy: 0.0477
average test loss: 0.003634, accuracy: 0.0707
case acc: 0.05768556
case acc: 0.059525684
case acc: 0.083074525
case acc: 0.081687346
case acc: 0.08470718
case acc: 0.057314012
top acc: 0.1020 ::: bot acc: 0.0175
top acc: 0.1008 ::: bot acc: 0.0197
top acc: 0.1600 ::: bot acc: 0.0298
top acc: 0.1405 ::: bot acc: 0.0316
top acc: 0.1369 ::: bot acc: 0.0352
top acc: 0.1054 ::: bot acc: 0.0241
current epoch: 45
train loss is 0.001909
average val loss: 0.001624, accuracy: 0.0476
average test loss: 0.003600, accuracy: 0.0703
case acc: 0.057088956
case acc: 0.058983546
case acc: 0.08285938
case acc: 0.08118012
case acc: 0.0846374
case acc: 0.056878407
top acc: 0.1011 ::: bot acc: 0.0175
top acc: 0.0997 ::: bot acc: 0.0201
top acc: 0.1595 ::: bot acc: 0.0299
top acc: 0.1401 ::: bot acc: 0.0311
top acc: 0.1369 ::: bot acc: 0.0352
top acc: 0.1049 ::: bot acc: 0.0242
current epoch: 46
train loss is 0.001887
average val loss: 0.001619, accuracy: 0.0475
average test loss: 0.003546, accuracy: 0.0697
case acc: 0.056381486
case acc: 0.058034025
case acc: 0.08236377
case acc: 0.080829196
case acc: 0.08396458
case acc: 0.056345496
top acc: 0.0998 ::: bot acc: 0.0179
top acc: 0.0982 ::: bot acc: 0.0203
top acc: 0.1587 ::: bot acc: 0.0303
top acc: 0.1396 ::: bot acc: 0.0310
top acc: 0.1360 ::: bot acc: 0.0349
top acc: 0.1039 ::: bot acc: 0.0246
current epoch: 47
train loss is 0.001877
average val loss: 0.001617, accuracy: 0.0475
average test loss: 0.003555, accuracy: 0.0698
case acc: 0.05640766
case acc: 0.057892982
case acc: 0.082546555
case acc: 0.080935545
case acc: 0.08413207
case acc: 0.056651592
top acc: 0.1001 ::: bot acc: 0.0175
top acc: 0.0979 ::: bot acc: 0.0206
top acc: 0.1590 ::: bot acc: 0.0303
top acc: 0.1395 ::: bot acc: 0.0312
top acc: 0.1363 ::: bot acc: 0.0349
top acc: 0.1040 ::: bot acc: 0.0247
current epoch: 48
train loss is 0.001872
average val loss: 0.001608, accuracy: 0.0473
average test loss: 0.003436, accuracy: 0.0684
case acc: 0.054866
case acc: 0.056324
case acc: 0.081377804
case acc: 0.07977476
case acc: 0.08249231
case acc: 0.05577643
top acc: 0.0974 ::: bot acc: 0.0180
top acc: 0.0951 ::: bot acc: 0.0214
top acc: 0.1568 ::: bot acc: 0.0312
top acc: 0.1382 ::: bot acc: 0.0307
top acc: 0.1344 ::: bot acc: 0.0338
top acc: 0.1025 ::: bot acc: 0.0254
current epoch: 49
train loss is 0.001852
average val loss: 0.001604, accuracy: 0.0472
average test loss: 0.003435, accuracy: 0.0684
case acc: 0.05468151
case acc: 0.055856667
case acc: 0.08108128
case acc: 0.07980698
case acc: 0.08296133
case acc: 0.05592451
top acc: 0.0974 ::: bot acc: 0.0181
top acc: 0.0945 ::: bot acc: 0.0215
top acc: 0.1565 ::: bot acc: 0.0311
top acc: 0.1383 ::: bot acc: 0.0305
top acc: 0.1349 ::: bot acc: 0.0341
top acc: 0.1028 ::: bot acc: 0.0253
current epoch: 50
train loss is 0.001838
average val loss: 0.001598, accuracy: 0.0470
average test loss: 0.003270, accuracy: 0.0665
case acc: 0.052769963
case acc: 0.05402278
case acc: 0.079466544
case acc: 0.07773868
case acc: 0.08088329
case acc: 0.054328643
top acc: 0.0940 ::: bot acc: 0.0189
top acc: 0.0909 ::: bot acc: 0.0235
top acc: 0.1533 ::: bot acc: 0.0324
top acc: 0.1355 ::: bot acc: 0.0300
top acc: 0.1323 ::: bot acc: 0.0328
top acc: 0.1001 ::: bot acc: 0.0259
LME_Co_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5346 5346 5346
1.7082474 -0.6288155 0.25454274 -0.21218425
Validation: 594 594 594
Testing: 768 768 768
pre-processing time: 0.0003390312194824219
the split date is 2012-07-01
train dropout: 0.4 test dropout: 0.1
net initializing with time: 0.003747224807739258
preparing training and testing date with time: 4.76837158203125e-07
current epoch: 1
train loss is 0.010265
average val loss: 0.004247, accuracy: 0.0738
average test loss: 0.004519, accuracy: 0.0787
case acc: 0.09999089
case acc: 0.14048186
case acc: 0.07702969
case acc: 0.0340823
case acc: 0.063219294
case acc: 0.05714661
top acc: 0.0630 ::: bot acc: 0.1449
top acc: 0.1848 ::: bot acc: 0.0976
top acc: 0.0267 ::: bot acc: 0.1388
top acc: 0.0116 ::: bot acc: 0.0665
top acc: 0.0246 ::: bot acc: 0.1049
top acc: 0.0886 ::: bot acc: 0.0618
current epoch: 2
train loss is 0.008281
average val loss: 0.009678, accuracy: 0.1126
average test loss: 0.009968, accuracy: 0.1144
case acc: 0.06350021
case acc: 0.2519054
case acc: 0.076883584
case acc: 0.09147771
case acc: 0.068818785
case acc: 0.13390961
top acc: 0.1379 ::: bot acc: 0.0206
top acc: 0.2956 ::: bot acc: 0.2093
top acc: 0.1484 ::: bot acc: 0.0235
top acc: 0.1275 ::: bot acc: 0.0529
top acc: 0.1270 ::: bot acc: 0.0263
top acc: 0.2005 ::: bot acc: 0.0682
current epoch: 3
train loss is 0.010285
average val loss: 0.019007, accuracy: 0.1764
average test loss: 0.019235, accuracy: 0.1766
case acc: 0.12544133
case acc: 0.31272474
case acc: 0.13804922
case acc: 0.15956585
case acc: 0.13263392
case acc: 0.19143468
top acc: 0.2128 ::: bot acc: 0.0556
top acc: 0.3562 ::: bot acc: 0.2707
top acc: 0.2214 ::: bot acc: 0.0594
top acc: 0.1953 ::: bot acc: 0.1210
top acc: 0.1962 ::: bot acc: 0.0785
top acc: 0.2622 ::: bot acc: 0.1158
current epoch: 4
train loss is 0.013448
average val loss: 0.006925, accuracy: 0.0932
average test loss: 0.007117, accuracy: 0.0952
case acc: 0.059635676
case acc: 0.21110314
case acc: 0.06912616
case acc: 0.068324655
case acc: 0.056340083
case acc: 0.10670881
top acc: 0.1259 ::: bot acc: 0.0306
top acc: 0.2544 ::: bot acc: 0.1686
top acc: 0.1323 ::: bot acc: 0.0325
top acc: 0.1027 ::: bot acc: 0.0313
top acc: 0.1102 ::: bot acc: 0.0219
top acc: 0.1682 ::: bot acc: 0.0512
current epoch: 5
train loss is 0.009421
average val loss: 0.003085, accuracy: 0.0625
average test loss: 0.003268, accuracy: 0.0664
case acc: 0.07895666
case acc: 0.113376215
case acc: 0.06920041
case acc: 0.029866712
case acc: 0.0526373
case acc: 0.05441162
top acc: 0.0632 ::: bot acc: 0.1140
top acc: 0.1576 ::: bot acc: 0.0706
top acc: 0.0433 ::: bot acc: 0.1201
top acc: 0.0161 ::: bot acc: 0.0586
top acc: 0.0341 ::: bot acc: 0.0843
top acc: 0.0803 ::: bot acc: 0.0665
current epoch: 6
train loss is 0.004654
average val loss: 0.003373, accuracy: 0.0636
average test loss: 0.003568, accuracy: 0.0667
case acc: 0.059483744
case acc: 0.1377271
case acc: 0.06167545
case acc: 0.030223709
case acc: 0.04291877
case acc: 0.06806341
top acc: 0.0794 ::: bot acc: 0.0764
top acc: 0.1816 ::: bot acc: 0.0950
top acc: 0.0752 ::: bot acc: 0.0891
top acc: 0.0499 ::: bot acc: 0.0239
top acc: 0.0756 ::: bot acc: 0.0432
top acc: 0.1162 ::: bot acc: 0.0391
current epoch: 7
train loss is 0.004280
average val loss: 0.004275, accuracy: 0.0731
average test loss: 0.004439, accuracy: 0.0752
case acc: 0.056785442
case acc: 0.1542384
case acc: 0.060379393
case acc: 0.043144196
case acc: 0.05227211
case acc: 0.08418396
top acc: 0.1068 ::: bot acc: 0.0495
top acc: 0.1980 ::: bot acc: 0.1118
top acc: 0.0965 ::: bot acc: 0.0653
top acc: 0.0741 ::: bot acc: 0.0139
top acc: 0.1040 ::: bot acc: 0.0219
top acc: 0.1394 ::: bot acc: 0.0400
current epoch: 8
train loss is 0.004516
average val loss: 0.003537, accuracy: 0.0664
average test loss: 0.003721, accuracy: 0.0686
case acc: 0.056722265
case acc: 0.13481136
case acc: 0.0603892
case acc: 0.036026366
case acc: 0.048068203
case acc: 0.07555705
top acc: 0.0984 ::: bot acc: 0.0573
top acc: 0.1784 ::: bot acc: 0.0923
top acc: 0.0843 ::: bot acc: 0.0785
top acc: 0.0626 ::: bot acc: 0.0158
top acc: 0.0976 ::: bot acc: 0.0233
top acc: 0.1284 ::: bot acc: 0.0372
current epoch: 9
train loss is 0.004021
average val loss: 0.003307, accuracy: 0.0644
average test loss: 0.003478, accuracy: 0.0666
case acc: 0.056752164
case acc: 0.124562785
case acc: 0.06053409
case acc: 0.03501553
case acc: 0.04868211
case acc: 0.07387159
top acc: 0.0982 ::: bot acc: 0.0576
top acc: 0.1681 ::: bot acc: 0.0821
top acc: 0.0807 ::: bot acc: 0.0823
top acc: 0.0605 ::: bot acc: 0.0168
top acc: 0.0989 ::: bot acc: 0.0223
top acc: 0.1260 ::: bot acc: 0.0371
current epoch: 10
train loss is 0.003649
average val loss: 0.003182, accuracy: 0.0636
average test loss: 0.003352, accuracy: 0.0657
case acc: 0.056772936
case acc: 0.11721244
case acc: 0.060554624
case acc: 0.035003603
case acc: 0.050675735
case acc: 0.07404195
top acc: 0.1012 ::: bot acc: 0.0545
top acc: 0.1609 ::: bot acc: 0.0744
top acc: 0.0802 ::: bot acc: 0.0830
top acc: 0.0606 ::: bot acc: 0.0163
top acc: 0.1016 ::: bot acc: 0.0221
top acc: 0.1258 ::: bot acc: 0.0372
current epoch: 11
train loss is 0.003353
average val loss: 0.003239, accuracy: 0.0646
average test loss: 0.003418, accuracy: 0.0669
case acc: 0.057060566
case acc: 0.11480749
case acc: 0.06077279
case acc: 0.0373042
case acc: 0.0548842
case acc: 0.07635072
top acc: 0.1072 ::: bot acc: 0.0487
top acc: 0.1579 ::: bot acc: 0.0727
top acc: 0.0837 ::: bot acc: 0.0797
top acc: 0.0645 ::: bot acc: 0.0148
top acc: 0.1086 ::: bot acc: 0.0210
top acc: 0.1292 ::: bot acc: 0.0373
current epoch: 12
train loss is 0.003167
average val loss: 0.003295, accuracy: 0.0655
average test loss: 0.003452, accuracy: 0.0674
case acc: 0.057172522
case acc: 0.11137598
case acc: 0.060460206
case acc: 0.0390796
case acc: 0.057823718
case acc: 0.07844946
top acc: 0.1122 ::: bot acc: 0.0436
top acc: 0.1549 ::: bot acc: 0.0690
top acc: 0.0860 ::: bot acc: 0.0771
top acc: 0.0676 ::: bot acc: 0.0140
top acc: 0.1131 ::: bot acc: 0.0210
top acc: 0.1316 ::: bot acc: 0.0381
current epoch: 13
train loss is 0.003083
average val loss: 0.003204, accuracy: 0.0647
average test loss: 0.003369, accuracy: 0.0666
case acc: 0.057491213
case acc: 0.10562362
case acc: 0.06027059
case acc: 0.039487764
case acc: 0.058606815
case acc: 0.07808125
top acc: 0.1146 ::: bot acc: 0.0413
top acc: 0.1490 ::: bot acc: 0.0633
top acc: 0.0860 ::: bot acc: 0.0773
top acc: 0.0683 ::: bot acc: 0.0142
top acc: 0.1144 ::: bot acc: 0.0205
top acc: 0.1314 ::: bot acc: 0.0381
current epoch: 14
train loss is 0.002934
average val loss: 0.003186, accuracy: 0.0648
average test loss: 0.003346, accuracy: 0.0665
case acc: 0.057762846
case acc: 0.10142934
case acc: 0.060208492
case acc: 0.040412564
case acc: 0.060224883
case acc: 0.07906944
top acc: 0.1176 ::: bot acc: 0.0375
top acc: 0.1447 ::: bot acc: 0.0588
top acc: 0.0872 ::: bot acc: 0.0756
top acc: 0.0698 ::: bot acc: 0.0138
top acc: 0.1163 ::: bot acc: 0.0215
top acc: 0.1325 ::: bot acc: 0.0386
current epoch: 15
train loss is 0.002787
average val loss: 0.003123, accuracy: 0.0642
average test loss: 0.003280, accuracy: 0.0658
case acc: 0.058116257
case acc: 0.096159875
case acc: 0.060405888
case acc: 0.041163027
case acc: 0.060467288
case acc: 0.07866001
top acc: 0.1199 ::: bot acc: 0.0353
top acc: 0.1395 ::: bot acc: 0.0537
top acc: 0.0880 ::: bot acc: 0.0751
top acc: 0.0706 ::: bot acc: 0.0138
top acc: 0.1168 ::: bot acc: 0.0211
top acc: 0.1321 ::: bot acc: 0.0377
current epoch: 16
train loss is 0.002683
average val loss: 0.003097, accuracy: 0.0639
average test loss: 0.003272, accuracy: 0.0657
case acc: 0.058674056
case acc: 0.092522874
case acc: 0.060352854
case acc: 0.04235556
case acc: 0.061158895
case acc: 0.079345055
top acc: 0.1229 ::: bot acc: 0.0328
top acc: 0.1361 ::: bot acc: 0.0497
top acc: 0.0896 ::: bot acc: 0.0734
top acc: 0.0727 ::: bot acc: 0.0142
top acc: 0.1180 ::: bot acc: 0.0211
top acc: 0.1330 ::: bot acc: 0.0386
current epoch: 17
train loss is 0.002558
average val loss: 0.003046, accuracy: 0.0634
average test loss: 0.003228, accuracy: 0.0652
case acc: 0.059192814
case acc: 0.08835517
case acc: 0.06061366
case acc: 0.042895965
case acc: 0.061337624
case acc: 0.07894647
top acc: 0.1253 ::: bot acc: 0.0304
top acc: 0.1318 ::: bot acc: 0.0460
top acc: 0.0907 ::: bot acc: 0.0729
top acc: 0.0730 ::: bot acc: 0.0143
top acc: 0.1180 ::: bot acc: 0.0217
top acc: 0.1328 ::: bot acc: 0.0382
current epoch: 18
train loss is 0.002481
average val loss: 0.003057, accuracy: 0.0635
average test loss: 0.003246, accuracy: 0.0655
case acc: 0.0598428
case acc: 0.08586545
case acc: 0.06068788
case acc: 0.04449291
case acc: 0.062094208
case acc: 0.07973789
top acc: 0.1280 ::: bot acc: 0.0274
top acc: 0.1292 ::: bot acc: 0.0436
top acc: 0.0931 ::: bot acc: 0.0703
top acc: 0.0754 ::: bot acc: 0.0149
top acc: 0.1191 ::: bot acc: 0.0217
top acc: 0.1339 ::: bot acc: 0.0382
current epoch: 19
train loss is 0.002439
average val loss: 0.003182, accuracy: 0.0650
average test loss: 0.003356, accuracy: 0.0669
case acc: 0.06143113
case acc: 0.085800946
case acc: 0.06089429
case acc: 0.047464445
case acc: 0.064383805
case acc: 0.081432074
top acc: 0.1323 ::: bot acc: 0.0230
top acc: 0.1293 ::: bot acc: 0.0435
top acc: 0.0969 ::: bot acc: 0.0660
top acc: 0.0787 ::: bot acc: 0.0166
top acc: 0.1218 ::: bot acc: 0.0228
top acc: 0.1360 ::: bot acc: 0.0388
current epoch: 20
train loss is 0.002375
average val loss: 0.003199, accuracy: 0.0651
average test loss: 0.003373, accuracy: 0.0670
case acc: 0.062272042
case acc: 0.0836156
case acc: 0.061141044
case acc: 0.04890921
case acc: 0.06438239
case acc: 0.081569165
top acc: 0.1354 ::: bot acc: 0.0200
top acc: 0.1268 ::: bot acc: 0.0413
top acc: 0.0994 ::: bot acc: 0.0636
top acc: 0.0809 ::: bot acc: 0.0170
top acc: 0.1222 ::: bot acc: 0.0229
top acc: 0.1363 ::: bot acc: 0.0390
current epoch: 21
train loss is 0.002325
average val loss: 0.003295, accuracy: 0.0662
average test loss: 0.003466, accuracy: 0.0681
case acc: 0.063836895
case acc: 0.08301832
case acc: 0.061408363
case acc: 0.051380873
case acc: 0.06583975
case acc: 0.08321951
top acc: 0.1389 ::: bot acc: 0.0172
top acc: 0.1262 ::: bot acc: 0.0408
top acc: 0.1028 ::: bot acc: 0.0601
top acc: 0.0837 ::: bot acc: 0.0184
top acc: 0.1236 ::: bot acc: 0.0236
top acc: 0.1386 ::: bot acc: 0.0395
current epoch: 22
train loss is 0.002299
average val loss: 0.003296, accuracy: 0.0662
average test loss: 0.003471, accuracy: 0.0681
case acc: 0.06473607
case acc: 0.08092107
case acc: 0.061657816
case acc: 0.052305106
case acc: 0.06595667
case acc: 0.08305877
top acc: 0.1412 ::: bot acc: 0.0161
top acc: 0.1239 ::: bot acc: 0.0390
top acc: 0.1041 ::: bot acc: 0.0586
top acc: 0.0850 ::: bot acc: 0.0189
top acc: 0.1243 ::: bot acc: 0.0235
top acc: 0.1382 ::: bot acc: 0.0393
current epoch: 23
train loss is 0.002284
average val loss: 0.003348, accuracy: 0.0669
average test loss: 0.003517, accuracy: 0.0687
case acc: 0.06594079
case acc: 0.07976921
case acc: 0.062060054
case acc: 0.053893924
case acc: 0.0666919
case acc: 0.08364326
top acc: 0.1436 ::: bot acc: 0.0144
top acc: 0.1229 ::: bot acc: 0.0377
top acc: 0.1062 ::: bot acc: 0.0566
top acc: 0.0867 ::: bot acc: 0.0201
top acc: 0.1249 ::: bot acc: 0.0239
top acc: 0.1390 ::: bot acc: 0.0395
current epoch: 24
train loss is 0.002246
average val loss: 0.003359, accuracy: 0.0670
average test loss: 0.003531, accuracy: 0.0688
case acc: 0.06663111
case acc: 0.07794997
case acc: 0.062413327
case acc: 0.05496969
case acc: 0.0671225
case acc: 0.08368119
top acc: 0.1452 ::: bot acc: 0.0134
top acc: 0.1210 ::: bot acc: 0.0363
top acc: 0.1079 ::: bot acc: 0.0547
top acc: 0.0878 ::: bot acc: 0.0209
top acc: 0.1253 ::: bot acc: 0.0245
top acc: 0.1390 ::: bot acc: 0.0395
current epoch: 25
train loss is 0.002198
average val loss: 0.003332, accuracy: 0.0668
average test loss: 0.003494, accuracy: 0.0683
case acc: 0.06708026
case acc: 0.07571628
case acc: 0.06234523
case acc: 0.0550574
case acc: 0.06652471
case acc: 0.0832178
top acc: 0.1456 ::: bot acc: 0.0134
top acc: 0.1184 ::: bot acc: 0.0343
top acc: 0.1086 ::: bot acc: 0.0540
top acc: 0.0881 ::: bot acc: 0.0210
top acc: 0.1245 ::: bot acc: 0.0244
top acc: 0.1385 ::: bot acc: 0.0393
current epoch: 26
train loss is 0.002162
average val loss: 0.003240, accuracy: 0.0657
average test loss: 0.003409, accuracy: 0.0673
case acc: 0.06690542
case acc: 0.07263025
case acc: 0.0625014
case acc: 0.05446524
case acc: 0.06509118
case acc: 0.081953526
top acc: 0.1453 ::: bot acc: 0.0137
top acc: 0.1149 ::: bot acc: 0.0322
top acc: 0.1086 ::: bot acc: 0.0543
top acc: 0.0874 ::: bot acc: 0.0203
top acc: 0.1229 ::: bot acc: 0.0233
top acc: 0.1368 ::: bot acc: 0.0388
current epoch: 27
train loss is 0.002121
average val loss: 0.003263, accuracy: 0.0660
average test loss: 0.003429, accuracy: 0.0675
case acc: 0.06753944
case acc: 0.071549855
case acc: 0.062756516
case acc: 0.055613287
case acc: 0.0652404
case acc: 0.08206403
top acc: 0.1466 ::: bot acc: 0.0125
top acc: 0.1138 ::: bot acc: 0.0311
top acc: 0.1103 ::: bot acc: 0.0527
top acc: 0.0888 ::: bot acc: 0.0211
top acc: 0.1228 ::: bot acc: 0.0236
top acc: 0.1367 ::: bot acc: 0.0390
current epoch: 28
train loss is 0.002096
average val loss: 0.003351, accuracy: 0.0670
average test loss: 0.003502, accuracy: 0.0683
case acc: 0.06870362
case acc: 0.071412735
case acc: 0.06302655
case acc: 0.05756534
case acc: 0.06620835
case acc: 0.08283281
top acc: 0.1487 ::: bot acc: 0.0118
top acc: 0.1139 ::: bot acc: 0.0309
top acc: 0.1125 ::: bot acc: 0.0500
top acc: 0.0910 ::: bot acc: 0.0228
top acc: 0.1242 ::: bot acc: 0.0239
top acc: 0.1378 ::: bot acc: 0.0395
current epoch: 29
train loss is 0.002092
average val loss: 0.003341, accuracy: 0.0669
average test loss: 0.003489, accuracy: 0.0681
case acc: 0.06914288
case acc: 0.06997574
case acc: 0.06334357
case acc: 0.05781597
case acc: 0.06585668
case acc: 0.08258888
top acc: 0.1496 ::: bot acc: 0.0117
top acc: 0.1120 ::: bot acc: 0.0298
top acc: 0.1133 ::: bot acc: 0.0493
top acc: 0.0913 ::: bot acc: 0.0230
top acc: 0.1235 ::: bot acc: 0.0238
top acc: 0.1374 ::: bot acc: 0.0392
current epoch: 30
train loss is 0.002092
average val loss: 0.003438, accuracy: 0.0680
average test loss: 0.003592, accuracy: 0.0693
case acc: 0.07067627
case acc: 0.07059738
case acc: 0.06383251
case acc: 0.059896573
case acc: 0.06713394
case acc: 0.08352065
top acc: 0.1519 ::: bot acc: 0.0111
top acc: 0.1125 ::: bot acc: 0.0305
top acc: 0.1163 ::: bot acc: 0.0465
top acc: 0.0935 ::: bot acc: 0.0246
top acc: 0.1255 ::: bot acc: 0.0241
top acc: 0.1390 ::: bot acc: 0.0395
current epoch: 31
train loss is 0.002081
average val loss: 0.003464, accuracy: 0.0683
average test loss: 0.003613, accuracy: 0.0695
case acc: 0.07120535
case acc: 0.06979881
case acc: 0.0641138
case acc: 0.060912013
case acc: 0.06738045
case acc: 0.08377974
top acc: 0.1528 ::: bot acc: 0.0110
top acc: 0.1119 ::: bot acc: 0.0298
top acc: 0.1176 ::: bot acc: 0.0451
top acc: 0.0945 ::: bot acc: 0.0254
top acc: 0.1255 ::: bot acc: 0.0244
top acc: 0.1390 ::: bot acc: 0.0397
current epoch: 32
train loss is 0.002062
average val loss: 0.003561, accuracy: 0.0693
average test loss: 0.003702, accuracy: 0.0705
case acc: 0.07244981
case acc: 0.070013285
case acc: 0.064798504
case acc: 0.06256913
case acc: 0.06881091
case acc: 0.084562264
top acc: 0.1547 ::: bot acc: 0.0110
top acc: 0.1119 ::: bot acc: 0.0300
top acc: 0.1202 ::: bot acc: 0.0427
top acc: 0.0964 ::: bot acc: 0.0266
top acc: 0.1275 ::: bot acc: 0.0252
top acc: 0.1401 ::: bot acc: 0.0399
current epoch: 33
train loss is 0.002068
average val loss: 0.003536, accuracy: 0.0690
average test loss: 0.003678, accuracy: 0.0702
case acc: 0.07261264
case acc: 0.068801135
case acc: 0.06471662
case acc: 0.06262933
case acc: 0.06866658
case acc: 0.08396882
top acc: 0.1550 ::: bot acc: 0.0111
top acc: 0.1106 ::: bot acc: 0.0291
top acc: 0.1203 ::: bot acc: 0.0424
top acc: 0.0962 ::: bot acc: 0.0269
top acc: 0.1269 ::: bot acc: 0.0254
top acc: 0.1394 ::: bot acc: 0.0394
current epoch: 34
train loss is 0.002057
average val loss: 0.003572, accuracy: 0.0694
average test loss: 0.003710, accuracy: 0.0706
case acc: 0.07288398
case acc: 0.06816068
case acc: 0.065057725
case acc: 0.06337338
case acc: 0.06967377
case acc: 0.084434114
top acc: 0.1554 ::: bot acc: 0.0111
top acc: 0.1101 ::: bot acc: 0.0285
top acc: 0.1213 ::: bot acc: 0.0416
top acc: 0.0974 ::: bot acc: 0.0274
top acc: 0.1284 ::: bot acc: 0.0260
top acc: 0.1397 ::: bot acc: 0.0401
current epoch: 35
train loss is 0.002047
average val loss: 0.003574, accuracy: 0.0695
average test loss: 0.003720, accuracy: 0.0707
case acc: 0.073056154
case acc: 0.067004584
case acc: 0.0650631
case acc: 0.06396603
case acc: 0.070255764
case acc: 0.0846075
top acc: 0.1557 ::: bot acc: 0.0111
top acc: 0.1088 ::: bot acc: 0.0276
top acc: 0.1215 ::: bot acc: 0.0411
top acc: 0.0980 ::: bot acc: 0.0277
top acc: 0.1293 ::: bot acc: 0.0260
top acc: 0.1402 ::: bot acc: 0.0400
current epoch: 36
train loss is 0.002045
average val loss: 0.003512, accuracy: 0.0689
average test loss: 0.003647, accuracy: 0.0698
case acc: 0.07218751
case acc: 0.06518708
case acc: 0.064870626
case acc: 0.0630071
case acc: 0.069774374
case acc: 0.08384669
top acc: 0.1544 ::: bot acc: 0.0109
top acc: 0.1069 ::: bot acc: 0.0263
top acc: 0.1205 ::: bot acc: 0.0424
top acc: 0.0971 ::: bot acc: 0.0269
top acc: 0.1284 ::: bot acc: 0.0259
top acc: 0.1392 ::: bot acc: 0.0396
current epoch: 37
train loss is 0.002003
average val loss: 0.003316, accuracy: 0.0667
average test loss: 0.003451, accuracy: 0.0674
case acc: 0.07015727
case acc: 0.061110035
case acc: 0.06402125
case acc: 0.060330145
case acc: 0.067561805
case acc: 0.08141907
top acc: 0.1513 ::: bot acc: 0.0115
top acc: 0.1020 ::: bot acc: 0.0236
top acc: 0.1171 ::: bot acc: 0.0457
top acc: 0.0940 ::: bot acc: 0.0251
top acc: 0.1259 ::: bot acc: 0.0245
top acc: 0.1361 ::: bot acc: 0.0388
current epoch: 38
train loss is 0.001946
average val loss: 0.003093, accuracy: 0.0642
average test loss: 0.003220, accuracy: 0.0646
case acc: 0.067645125
case acc: 0.056595854
case acc: 0.06326028
case acc: 0.057069704
case acc: 0.06425241
case acc: 0.07864621
top acc: 0.1468 ::: bot acc: 0.0128
top acc: 0.0963 ::: bot acc: 0.0210
top acc: 0.1131 ::: bot acc: 0.0498
top acc: 0.0903 ::: bot acc: 0.0225
top acc: 0.1221 ::: bot acc: 0.0226
top acc: 0.1322 ::: bot acc: 0.0382
current epoch: 39
train loss is 0.001885
average val loss: 0.002974, accuracy: 0.0629
average test loss: 0.003097, accuracy: 0.0629
case acc: 0.066395044
case acc: 0.053728364
case acc: 0.06286327
case acc: 0.055215813
case acc: 0.062553115
case acc: 0.0768791
top acc: 0.1447 ::: bot acc: 0.0135
top acc: 0.0932 ::: bot acc: 0.0191
top acc: 0.1110 ::: bot acc: 0.0518
top acc: 0.0883 ::: bot acc: 0.0211
top acc: 0.1197 ::: bot acc: 0.0217
top acc: 0.1298 ::: bot acc: 0.0375
current epoch: 40
train loss is 0.001868
average val loss: 0.002897, accuracy: 0.0621
average test loss: 0.003027, accuracy: 0.0621
case acc: 0.06549837
case acc: 0.052126437
case acc: 0.0626362
case acc: 0.054479927
case acc: 0.061476935
case acc: 0.07612845
top acc: 0.1427 ::: bot acc: 0.0145
top acc: 0.0911 ::: bot acc: 0.0184
top acc: 0.1098 ::: bot acc: 0.0532
top acc: 0.0872 ::: bot acc: 0.0205
top acc: 0.1183 ::: bot acc: 0.0215
top acc: 0.1289 ::: bot acc: 0.0374
current epoch: 41
train loss is 0.001852
average val loss: 0.002823, accuracy: 0.0612
average test loss: 0.002951, accuracy: 0.0611
case acc: 0.0647075
case acc: 0.05032596
case acc: 0.062447943
case acc: 0.053491224
case acc: 0.06007496
case acc: 0.07529674
top acc: 0.1410 ::: bot acc: 0.0155
top acc: 0.0887 ::: bot acc: 0.0176
top acc: 0.1084 ::: bot acc: 0.0548
top acc: 0.0865 ::: bot acc: 0.0195
top acc: 0.1164 ::: bot acc: 0.0210
top acc: 0.1276 ::: bot acc: 0.0371
current epoch: 42
train loss is 0.001831
average val loss: 0.002772, accuracy: 0.0607
average test loss: 0.002893, accuracy: 0.0602
case acc: 0.064102836
case acc: 0.048746888
case acc: 0.062157966
case acc: 0.05269286
case acc: 0.059154607
case acc: 0.074583426
top acc: 0.1396 ::: bot acc: 0.0164
top acc: 0.0871 ::: bot acc: 0.0167
top acc: 0.1071 ::: bot acc: 0.0558
top acc: 0.0854 ::: bot acc: 0.0192
top acc: 0.1153 ::: bot acc: 0.0210
top acc: 0.1268 ::: bot acc: 0.0367
current epoch: 43
train loss is 0.001822
average val loss: 0.002717, accuracy: 0.0600
average test loss: 0.002837, accuracy: 0.0595
case acc: 0.063569516
case acc: 0.047243007
case acc: 0.061883043
case acc: 0.05195318
case acc: 0.058501985
case acc: 0.07385982
top acc: 0.1385 ::: bot acc: 0.0172
top acc: 0.0851 ::: bot acc: 0.0160
top acc: 0.1058 ::: bot acc: 0.0570
top acc: 0.0846 ::: bot acc: 0.0187
top acc: 0.1143 ::: bot acc: 0.0208
top acc: 0.1256 ::: bot acc: 0.0369
current epoch: 44
train loss is 0.001806
average val loss: 0.002658, accuracy: 0.0593
average test loss: 0.002781, accuracy: 0.0588
case acc: 0.06290465
case acc: 0.04589297
case acc: 0.061691027
case acc: 0.05120407
case acc: 0.0577221
case acc: 0.07312012
top acc: 0.1373 ::: bot acc: 0.0179
top acc: 0.0833 ::: bot acc: 0.0154
top acc: 0.1043 ::: bot acc: 0.0582
top acc: 0.0836 ::: bot acc: 0.0184
top acc: 0.1131 ::: bot acc: 0.0206
top acc: 0.1245 ::: bot acc: 0.0369
current epoch: 45
train loss is 0.001800
average val loss: 0.002560, accuracy: 0.0582
average test loss: 0.002683, accuracy: 0.0574
case acc: 0.061976146
case acc: 0.04379139
case acc: 0.061554562
case acc: 0.049302664
case acc: 0.055965193
case acc: 0.071841076
top acc: 0.1345 ::: bot acc: 0.0206
top acc: 0.0806 ::: bot acc: 0.0146
top acc: 0.1024 ::: bot acc: 0.0608
top acc: 0.0812 ::: bot acc: 0.0172
top acc: 0.1107 ::: bot acc: 0.0203
top acc: 0.1227 ::: bot acc: 0.0370
current epoch: 46
train loss is 0.001793
average val loss: 0.002516, accuracy: 0.0577
average test loss: 0.002644, accuracy: 0.0569
case acc: 0.061663736
case acc: 0.04290116
case acc: 0.061384924
case acc: 0.048742943
case acc: 0.055107478
case acc: 0.07138649
top acc: 0.1337 ::: bot acc: 0.0214
top acc: 0.0794 ::: bot acc: 0.0143
top acc: 0.1016 ::: bot acc: 0.0615
top acc: 0.0806 ::: bot acc: 0.0169
top acc: 0.1093 ::: bot acc: 0.0205
top acc: 0.1218 ::: bot acc: 0.0373
current epoch: 47
train loss is 0.001784
average val loss: 0.002452, accuracy: 0.0568
average test loss: 0.002582, accuracy: 0.0560
case acc: 0.061113395
case acc: 0.04170539
case acc: 0.061105125
case acc: 0.047554456
case acc: 0.05394648
case acc: 0.070436165
top acc: 0.1321 ::: bot acc: 0.0228
top acc: 0.0775 ::: bot acc: 0.0144
top acc: 0.1003 ::: bot acc: 0.0625
top acc: 0.0790 ::: bot acc: 0.0161
top acc: 0.1077 ::: bot acc: 0.0206
top acc: 0.1201 ::: bot acc: 0.0376
current epoch: 48
train loss is 0.001775
average val loss: 0.002446, accuracy: 0.0568
average test loss: 0.002569, accuracy: 0.0558
case acc: 0.06113791
case acc: 0.04135097
case acc: 0.061233394
case acc: 0.04755287
case acc: 0.053449363
case acc: 0.07018189
top acc: 0.1318 ::: bot acc: 0.0233
top acc: 0.0771 ::: bot acc: 0.0141
top acc: 0.1002 ::: bot acc: 0.0628
top acc: 0.0792 ::: bot acc: 0.0162
top acc: 0.1068 ::: bot acc: 0.0205
top acc: 0.1197 ::: bot acc: 0.0376
current epoch: 49
train loss is 0.001771
average val loss: 0.002491, accuracy: 0.0573
average test loss: 0.002611, accuracy: 0.0564
case acc: 0.0613951
case acc: 0.042186357
case acc: 0.061335035
case acc: 0.04846722
case acc: 0.05416678
case acc: 0.07086317
top acc: 0.1329 ::: bot acc: 0.0220
top acc: 0.0784 ::: bot acc: 0.0141
top acc: 0.1015 ::: bot acc: 0.0615
top acc: 0.0803 ::: bot acc: 0.0168
top acc: 0.1078 ::: bot acc: 0.0205
top acc: 0.1208 ::: bot acc: 0.0377
current epoch: 50
train loss is 0.001773
average val loss: 0.002577, accuracy: 0.0583
average test loss: 0.002700, accuracy: 0.0576
case acc: 0.062180363
case acc: 0.04373669
case acc: 0.061701432
case acc: 0.05024034
case acc: 0.055537514
case acc: 0.072070636
top acc: 0.1350 ::: bot acc: 0.0200
top acc: 0.0804 ::: bot acc: 0.0144
top acc: 0.1037 ::: bot acc: 0.0592
top acc: 0.0825 ::: bot acc: 0.0177
top acc: 0.1100 ::: bot acc: 0.0204
top acc: 0.1231 ::: bot acc: 0.0371

		{"drop_out": 0.4, "drop_out_mc": 0.15, "repeat_mc": 50, "hidden": 30, "embedding_size": 5, "batch": 512, "lag": 3}
LME_Co_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5328 5328 5328
1.7082474 -0.6288155 0.29835227 -0.25048217
Validation: 594 594 594
Testing: 774 774 774
pre-processing time: 0.0005681514739990234
the split date is 2010-07-01
train dropout: 0.4 test dropout: 0.15
net initializing with time: 0.007616281509399414
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.010699
average val loss: 0.005562, accuracy: 0.0927
average test loss: 0.007956, accuracy: 0.1147
case acc: 0.16334912
case acc: 0.087145515
case acc: 0.12645349
case acc: 0.1050774
case acc: 0.13086674
case acc: 0.07555777
top acc: 0.1323 ::: bot acc: 0.1997
top acc: 0.1487 ::: bot acc: 0.0243
top acc: 0.0564 ::: bot acc: 0.1928
top acc: 0.0638 ::: bot acc: 0.1423
top acc: 0.1004 ::: bot acc: 0.1601
top acc: 0.0329 ::: bot acc: 0.1234
current epoch: 2
train loss is 0.009046
average val loss: 0.006407, accuracy: 0.0836
average test loss: 0.004530, accuracy: 0.0653
case acc: 0.037727635
case acc: 0.1982004
case acc: 0.051807333
case acc: 0.031190244
case acc: 0.02541565
case acc: 0.047683652
top acc: 0.0100 ::: bot acc: 0.0733
top acc: 0.2612 ::: bot acc: 0.1307
top acc: 0.0738 ::: bot acc: 0.0653
top acc: 0.0558 ::: bot acc: 0.0227
top acc: 0.0188 ::: bot acc: 0.0414
top acc: 0.0809 ::: bot acc: 0.0200
current epoch: 3
train loss is 0.010133
average val loss: 0.015967, accuracy: 0.1572
average test loss: 0.010975, accuracy: 0.1230
case acc: 0.0656557
case acc: 0.27869374
case acc: 0.097685404
case acc: 0.10244006
case acc: 0.075174645
case acc: 0.11848766
top acc: 0.0908 ::: bot acc: 0.0406
top acc: 0.3419 ::: bot acc: 0.2107
top acc: 0.1685 ::: bot acc: 0.0337
top acc: 0.1440 ::: bot acc: 0.0638
top acc: 0.1053 ::: bot acc: 0.0468
top acc: 0.1619 ::: bot acc: 0.0705
current epoch: 4
train loss is 0.013608
average val loss: 0.008342, accuracy: 0.1036
average test loss: 0.005364, accuracy: 0.0724
case acc: 0.027806278
case acc: 0.21147658
case acc: 0.058872346
case acc: 0.04781352
case acc: 0.028683241
case acc: 0.059794452
top acc: 0.0385 ::: bot acc: 0.0319
top acc: 0.2751 ::: bot acc: 0.1433
top acc: 0.1129 ::: bot acc: 0.0286
top acc: 0.0874 ::: bot acc: 0.0143
top acc: 0.0547 ::: bot acc: 0.0102
top acc: 0.1010 ::: bot acc: 0.0175
current epoch: 5
train loss is 0.011512
average val loss: 0.003193, accuracy: 0.0659
average test loss: 0.004127, accuracy: 0.0794
case acc: 0.1011755
case acc: 0.09056537
case acc: 0.08266797
case acc: 0.06691682
case acc: 0.0796859
case acc: 0.05566367
top acc: 0.0700 ::: bot acc: 0.1384
top acc: 0.1525 ::: bot acc: 0.0255
top acc: 0.0305 ::: bot acc: 0.1393
top acc: 0.0254 ::: bot acc: 0.1057
top acc: 0.0488 ::: bot acc: 0.1089
top acc: 0.0143 ::: bot acc: 0.1036
current epoch: 6
train loss is 0.005762
average val loss: 0.002827, accuracy: 0.0572
average test loss: 0.003105, accuracy: 0.0656
case acc: 0.07864123
case acc: 0.09893689
case acc: 0.07184406
case acc: 0.04966933
case acc: 0.053202063
case acc: 0.04108393
top acc: 0.0471 ::: bot acc: 0.1165
top acc: 0.1618 ::: bot acc: 0.0323
top acc: 0.0317 ::: bot acc: 0.1228
top acc: 0.0132 ::: bot acc: 0.0858
top acc: 0.0263 ::: bot acc: 0.0805
top acc: 0.0136 ::: bot acc: 0.0818
current epoch: 7
train loss is 0.004840
average val loss: 0.003414, accuracy: 0.0601
average test loss: 0.002532, accuracy: 0.0528
case acc: 0.04305903
case acc: 0.12344928
case acc: 0.05643539
case acc: 0.03333996
case acc: 0.025592325
case acc: 0.03467276
top acc: 0.0130 ::: bot acc: 0.0796
top acc: 0.1867 ::: bot acc: 0.0558
top acc: 0.0481 ::: bot acc: 0.0915
top acc: 0.0289 ::: bot acc: 0.0529
top acc: 0.0197 ::: bot acc: 0.0426
top acc: 0.0446 ::: bot acc: 0.0484
current epoch: 8
train loss is 0.005223
average val loss: 0.002931, accuracy: 0.0560
average test loss: 0.002355, accuracy: 0.0521
case acc: 0.04695479
case acc: 0.108558714
case acc: 0.060184915
case acc: 0.03513972
case acc: 0.027121007
case acc: 0.034356132
top acc: 0.0166 ::: bot acc: 0.0839
top acc: 0.1720 ::: bot acc: 0.0405
top acc: 0.0426 ::: bot acc: 0.0998
top acc: 0.0212 ::: bot acc: 0.0600
top acc: 0.0184 ::: bot acc: 0.0451
top acc: 0.0370 ::: bot acc: 0.0561
current epoch: 9
train loss is 0.004898
average val loss: 0.002511, accuracy: 0.0524
average test loss: 0.002258, accuracy: 0.0525
case acc: 0.05356491
case acc: 0.092835255
case acc: 0.06491855
case acc: 0.038873345
case acc: 0.029360972
case acc: 0.035577256
top acc: 0.0224 ::: bot acc: 0.0911
top acc: 0.1549 ::: bot acc: 0.0272
top acc: 0.0369 ::: bot acc: 0.1095
top acc: 0.0144 ::: bot acc: 0.0687
top acc: 0.0162 ::: bot acc: 0.0498
top acc: 0.0276 ::: bot acc: 0.0655
current epoch: 10
train loss is 0.004320
average val loss: 0.002306, accuracy: 0.0506
average test loss: 0.002150, accuracy: 0.0514
case acc: 0.05298451
case acc: 0.084492825
case acc: 0.06651184
case acc: 0.039748374
case acc: 0.028484233
case acc: 0.03623958
top acc: 0.0215 ::: bot acc: 0.0904
top acc: 0.1456 ::: bot acc: 0.0213
top acc: 0.0355 ::: bot acc: 0.1125
top acc: 0.0135 ::: bot acc: 0.0710
top acc: 0.0170 ::: bot acc: 0.0480
top acc: 0.0265 ::: bot acc: 0.0676
current epoch: 11
train loss is 0.004021
average val loss: 0.002328, accuracy: 0.0510
average test loss: 0.001989, accuracy: 0.0484
case acc: 0.04564609
case acc: 0.08459171
case acc: 0.06376879
case acc: 0.036953557
case acc: 0.02437634
case acc: 0.035157274
top acc: 0.0151 ::: bot acc: 0.0829
top acc: 0.1457 ::: bot acc: 0.0217
top acc: 0.0374 ::: bot acc: 0.1079
top acc: 0.0179 ::: bot acc: 0.0646
top acc: 0.0222 ::: bot acc: 0.0394
top acc: 0.0327 ::: bot acc: 0.0617
current epoch: 12
train loss is 0.003922
average val loss: 0.002360, accuracy: 0.0515
average test loss: 0.001859, accuracy: 0.0461
case acc: 0.039280713
case acc: 0.08405724
case acc: 0.06159911
case acc: 0.034620125
case acc: 0.022325978
case acc: 0.03457733
top acc: 0.0105 ::: bot acc: 0.0755
top acc: 0.1447 ::: bot acc: 0.0218
top acc: 0.0398 ::: bot acc: 0.1034
top acc: 0.0222 ::: bot acc: 0.0586
top acc: 0.0286 ::: bot acc: 0.0328
top acc: 0.0379 ::: bot acc: 0.0557
current epoch: 13
train loss is 0.003802
average val loss: 0.002175, accuracy: 0.0496
average test loss: 0.001793, accuracy: 0.0456
case acc: 0.039997574
case acc: 0.07722048
case acc: 0.063181676
case acc: 0.03568972
case acc: 0.022671493
case acc: 0.035136826
top acc: 0.0111 ::: bot acc: 0.0763
top acc: 0.1362 ::: bot acc: 0.0182
top acc: 0.0387 ::: bot acc: 0.1062
top acc: 0.0198 ::: bot acc: 0.0617
top acc: 0.0273 ::: bot acc: 0.0340
top acc: 0.0351 ::: bot acc: 0.0595
current epoch: 14
train loss is 0.003645
average val loss: 0.002090, accuracy: 0.0486
average test loss: 0.001710, accuracy: 0.0446
case acc: 0.03872975
case acc: 0.073110476
case acc: 0.06290934
case acc: 0.035361372
case acc: 0.022604177
case acc: 0.034992572
top acc: 0.0104 ::: bot acc: 0.0744
top acc: 0.1307 ::: bot acc: 0.0174
top acc: 0.0380 ::: bot acc: 0.1060
top acc: 0.0197 ::: bot acc: 0.0611
top acc: 0.0281 ::: bot acc: 0.0334
top acc: 0.0347 ::: bot acc: 0.0597
current epoch: 15
train loss is 0.003494
average val loss: 0.001993, accuracy: 0.0476
average test loss: 0.001652, accuracy: 0.0439
case acc: 0.037857894
case acc: 0.069596276
case acc: 0.063215174
case acc: 0.03561626
case acc: 0.022493191
case acc: 0.034831602
top acc: 0.0097 ::: bot acc: 0.0735
top acc: 0.1254 ::: bot acc: 0.0177
top acc: 0.0379 ::: bot acc: 0.1061
top acc: 0.0204 ::: bot acc: 0.0612
top acc: 0.0282 ::: bot acc: 0.0330
top acc: 0.0333 ::: bot acc: 0.0606
current epoch: 16
train loss is 0.003375
average val loss: 0.002016, accuracy: 0.0480
average test loss: 0.001576, accuracy: 0.0428
case acc: 0.034291573
case acc: 0.068645954
case acc: 0.061773375
case acc: 0.03477955
case acc: 0.02216467
case acc: 0.034889918
top acc: 0.0080 ::: bot acc: 0.0693
top acc: 0.1235 ::: bot acc: 0.0179
top acc: 0.0400 ::: bot acc: 0.1035
top acc: 0.0233 ::: bot acc: 0.0582
top acc: 0.0315 ::: bot acc: 0.0298
top acc: 0.0364 ::: bot acc: 0.0577
current epoch: 17
train loss is 0.003339
average val loss: 0.002026, accuracy: 0.0482
average test loss: 0.001519, accuracy: 0.0419
case acc: 0.03197196
case acc: 0.06783992
case acc: 0.060589183
case acc: 0.03381803
case acc: 0.022324806
case acc: 0.03470317
top acc: 0.0084 ::: bot acc: 0.0655
top acc: 0.1222 ::: bot acc: 0.0184
top acc: 0.0416 ::: bot acc: 0.1009
top acc: 0.0260 ::: bot acc: 0.0553
top acc: 0.0337 ::: bot acc: 0.0280
top acc: 0.0380 ::: bot acc: 0.0560
current epoch: 18
train loss is 0.003267
average val loss: 0.001959, accuracy: 0.0473
average test loss: 0.001465, accuracy: 0.0413
case acc: 0.031131985
case acc: 0.0653214
case acc: 0.060466774
case acc: 0.033734415
case acc: 0.02241928
case acc: 0.034704816
top acc: 0.0083 ::: bot acc: 0.0643
top acc: 0.1172 ::: bot acc: 0.0201
top acc: 0.0418 ::: bot acc: 0.1003
top acc: 0.0259 ::: bot acc: 0.0550
top acc: 0.0335 ::: bot acc: 0.0285
top acc: 0.0376 ::: bot acc: 0.0565
current epoch: 19
train loss is 0.003176
average val loss: 0.001964, accuracy: 0.0474
average test loss: 0.001428, accuracy: 0.0407
case acc: 0.029732222
case acc: 0.064094834
case acc: 0.059620783
case acc: 0.033557583
case acc: 0.022417188
case acc: 0.034929357
top acc: 0.0088 ::: bot acc: 0.0621
top acc: 0.1153 ::: bot acc: 0.0209
top acc: 0.0433 ::: bot acc: 0.0985
top acc: 0.0281 ::: bot acc: 0.0540
top acc: 0.0341 ::: bot acc: 0.0277
top acc: 0.0380 ::: bot acc: 0.0562
current epoch: 20
train loss is 0.003154
average val loss: 0.001922, accuracy: 0.0468
average test loss: 0.001393, accuracy: 0.0403
case acc: 0.02914942
case acc: 0.06274647
case acc: 0.059456594
case acc: 0.033324867
case acc: 0.022140887
case acc: 0.03468238
top acc: 0.0096 ::: bot acc: 0.0606
top acc: 0.1123 ::: bot acc: 0.0230
top acc: 0.0445 ::: bot acc: 0.0977
top acc: 0.0280 ::: bot acc: 0.0536
top acc: 0.0325 ::: bot acc: 0.0286
top acc: 0.0366 ::: bot acc: 0.0575
current epoch: 21
train loss is 0.003106
average val loss: 0.002043, accuracy: 0.0486
average test loss: 0.001347, accuracy: 0.0395
case acc: 0.027044801
case acc: 0.06345707
case acc: 0.05697488
case acc: 0.03216925
case acc: 0.022501241
case acc: 0.03461817
top acc: 0.0140 ::: bot acc: 0.0552
top acc: 0.1138 ::: bot acc: 0.0219
top acc: 0.0483 ::: bot acc: 0.0919
top acc: 0.0326 ::: bot acc: 0.0482
top acc: 0.0360 ::: bot acc: 0.0255
top acc: 0.0399 ::: bot acc: 0.0539
current epoch: 22
train loss is 0.003105
average val loss: 0.002133, accuracy: 0.0498
average test loss: 0.001324, accuracy: 0.0391
case acc: 0.025767518
case acc: 0.06385589
case acc: 0.055563368
case acc: 0.031734638
case acc: 0.02279936
case acc: 0.03509996
top acc: 0.0176 ::: bot acc: 0.0512
top acc: 0.1146 ::: bot acc: 0.0219
top acc: 0.0523 ::: bot acc: 0.0877
top acc: 0.0358 ::: bot acc: 0.0451
top acc: 0.0381 ::: bot acc: 0.0234
top acc: 0.0430 ::: bot acc: 0.0520
current epoch: 23
train loss is 0.003106
average val loss: 0.002276, accuracy: 0.0519
average test loss: 0.001319, accuracy: 0.0391
case acc: 0.02508896
case acc: 0.06507357
case acc: 0.054039482
case acc: 0.03133442
case acc: 0.023749152
case acc: 0.035179604
top acc: 0.0231 ::: bot acc: 0.0462
top acc: 0.1167 ::: bot acc: 0.0210
top acc: 0.0572 ::: bot acc: 0.0824
top acc: 0.0407 ::: bot acc: 0.0403
top acc: 0.0416 ::: bot acc: 0.0202
top acc: 0.0459 ::: bot acc: 0.0486
current epoch: 24
train loss is 0.003101
average val loss: 0.002223, accuracy: 0.0511
average test loss: 0.001286, accuracy: 0.0387
case acc: 0.024900796
case acc: 0.0633022
case acc: 0.05394447
case acc: 0.031460762
case acc: 0.023219109
case acc: 0.03512365
top acc: 0.0229 ::: bot acc: 0.0457
top acc: 0.1131 ::: bot acc: 0.0223
top acc: 0.0578 ::: bot acc: 0.0823
top acc: 0.0401 ::: bot acc: 0.0411
top acc: 0.0402 ::: bot acc: 0.0213
top acc: 0.0446 ::: bot acc: 0.0498
current epoch: 25
train loss is 0.003072
average val loss: 0.002206, accuracy: 0.0508
average test loss: 0.001266, accuracy: 0.0384
case acc: 0.02491748
case acc: 0.06224848
case acc: 0.053709567
case acc: 0.03156043
case acc: 0.023074202
case acc: 0.035095952
top acc: 0.0237 ::: bot acc: 0.0452
top acc: 0.1114 ::: bot acc: 0.0234
top acc: 0.0584 ::: bot acc: 0.0813
top acc: 0.0405 ::: bot acc: 0.0409
top acc: 0.0397 ::: bot acc: 0.0216
top acc: 0.0439 ::: bot acc: 0.0507
current epoch: 26
train loss is 0.003033
average val loss: 0.002254, accuracy: 0.0515
average test loss: 0.001257, accuracy: 0.0383
case acc: 0.025281886
case acc: 0.06190266
case acc: 0.05325211
case acc: 0.031452764
case acc: 0.023297938
case acc: 0.034895018
top acc: 0.0259 ::: bot acc: 0.0437
top acc: 0.1107 ::: bot acc: 0.0236
top acc: 0.0611 ::: bot acc: 0.0791
top acc: 0.0424 ::: bot acc: 0.0391
top acc: 0.0408 ::: bot acc: 0.0203
top acc: 0.0443 ::: bot acc: 0.0500
current epoch: 27
train loss is 0.003030
average val loss: 0.002196, accuracy: 0.0506
average test loss: 0.001235, accuracy: 0.0381
case acc: 0.024944646
case acc: 0.060706973
case acc: 0.053287596
case acc: 0.031339955
case acc: 0.023221744
case acc: 0.03513011
top acc: 0.0254 ::: bot acc: 0.0436
top acc: 0.1077 ::: bot acc: 0.0258
top acc: 0.0605 ::: bot acc: 0.0792
top acc: 0.0414 ::: bot acc: 0.0396
top acc: 0.0401 ::: bot acc: 0.0218
top acc: 0.0429 ::: bot acc: 0.0519
current epoch: 28
train loss is 0.002984
average val loss: 0.002164, accuracy: 0.0501
average test loss: 0.001218, accuracy: 0.0379
case acc: 0.024802275
case acc: 0.059602346
case acc: 0.053529635
case acc: 0.03150926
case acc: 0.022869285
case acc: 0.03481924
top acc: 0.0255 ::: bot acc: 0.0435
top acc: 0.1053 ::: bot acc: 0.0276
top acc: 0.0612 ::: bot acc: 0.0795
top acc: 0.0408 ::: bot acc: 0.0403
top acc: 0.0392 ::: bot acc: 0.0218
top acc: 0.0416 ::: bot acc: 0.0528
current epoch: 29
train loss is 0.002947
average val loss: 0.002196, accuracy: 0.0506
average test loss: 0.001208, accuracy: 0.0378
case acc: 0.02489929
case acc: 0.059440486
case acc: 0.053090084
case acc: 0.031472288
case acc: 0.023209149
case acc: 0.03478681
top acc: 0.0264 ::: bot acc: 0.0424
top acc: 0.1043 ::: bot acc: 0.0287
top acc: 0.0624 ::: bot acc: 0.0779
top acc: 0.0423 ::: bot acc: 0.0391
top acc: 0.0402 ::: bot acc: 0.0212
top acc: 0.0421 ::: bot acc: 0.0521
current epoch: 30
train loss is 0.002933
average val loss: 0.002189, accuracy: 0.0505
average test loss: 0.001199, accuracy: 0.0377
case acc: 0.024868045
case acc: 0.058762062
case acc: 0.052897755
case acc: 0.031423144
case acc: 0.023378534
case acc: 0.034897033
top acc: 0.0269 ::: bot acc: 0.0418
top acc: 0.1032 ::: bot acc: 0.0295
top acc: 0.0631 ::: bot acc: 0.0771
top acc: 0.0426 ::: bot acc: 0.0385
top acc: 0.0405 ::: bot acc: 0.0210
top acc: 0.0421 ::: bot acc: 0.0523
current epoch: 31
train loss is 0.002927
average val loss: 0.002184, accuracy: 0.0505
average test loss: 0.001186, accuracy: 0.0376
case acc: 0.024995506
case acc: 0.058049463
case acc: 0.052881703
case acc: 0.031306487
case acc: 0.023333628
case acc: 0.03500077
top acc: 0.0268 ::: bot acc: 0.0423
top acc: 0.1015 ::: bot acc: 0.0307
top acc: 0.0637 ::: bot acc: 0.0765
top acc: 0.0433 ::: bot acc: 0.0379
top acc: 0.0405 ::: bot acc: 0.0209
top acc: 0.0419 ::: bot acc: 0.0525
current epoch: 32
train loss is 0.002887
average val loss: 0.002141, accuracy: 0.0497
average test loss: 0.001172, accuracy: 0.0374
case acc: 0.025155433
case acc: 0.057327405
case acc: 0.05293351
case acc: 0.031465027
case acc: 0.023047129
case acc: 0.03475347
top acc: 0.0266 ::: bot acc: 0.0427
top acc: 0.0991 ::: bot acc: 0.0331
top acc: 0.0635 ::: bot acc: 0.0769
top acc: 0.0427 ::: bot acc: 0.0388
top acc: 0.0394 ::: bot acc: 0.0218
top acc: 0.0410 ::: bot acc: 0.0533
current epoch: 33
train loss is 0.002865
average val loss: 0.002095, accuracy: 0.0490
average test loss: 0.001164, accuracy: 0.0373
case acc: 0.024676768
case acc: 0.0566227
case acc: 0.05320498
case acc: 0.03163947
case acc: 0.022667363
case acc: 0.034740463
top acc: 0.0249 ::: bot acc: 0.0436
top acc: 0.0970 ::: bot acc: 0.0354
top acc: 0.0629 ::: bot acc: 0.0776
top acc: 0.0420 ::: bot acc: 0.0399
top acc: 0.0379 ::: bot acc: 0.0234
top acc: 0.0395 ::: bot acc: 0.0547
current epoch: 34
train loss is 0.002826
average val loss: 0.002019, accuracy: 0.0478
average test loss: 0.001155, accuracy: 0.0372
case acc: 0.024930583
case acc: 0.055925414
case acc: 0.05337574
case acc: 0.031747445
case acc: 0.02258527
case acc: 0.034825526
top acc: 0.0232 ::: bot acc: 0.0458
top acc: 0.0941 ::: bot acc: 0.0386
top acc: 0.0610 ::: bot acc: 0.0792
top acc: 0.0405 ::: bot acc: 0.0413
top acc: 0.0363 ::: bot acc: 0.0253
top acc: 0.0381 ::: bot acc: 0.0564
current epoch: 35
train loss is 0.002786
average val loss: 0.002047, accuracy: 0.0483
average test loss: 0.001149, accuracy: 0.0371
case acc: 0.02469573
case acc: 0.05575686
case acc: 0.053194698
case acc: 0.031244166
case acc: 0.022712888
case acc: 0.03484819
top acc: 0.0241 ::: bot acc: 0.0445
top acc: 0.0937 ::: bot acc: 0.0385
top acc: 0.0625 ::: bot acc: 0.0780
top acc: 0.0415 ::: bot acc: 0.0394
top acc: 0.0372 ::: bot acc: 0.0243
top acc: 0.0387 ::: bot acc: 0.0558
current epoch: 36
train loss is 0.002804
average val loss: 0.002087, accuracy: 0.0488
average test loss: 0.001147, accuracy: 0.0371
case acc: 0.024821771
case acc: 0.055797517
case acc: 0.0528113
case acc: 0.031503737
case acc: 0.022812698
case acc: 0.034893163
top acc: 0.0255 ::: bot acc: 0.0436
top acc: 0.0943 ::: bot acc: 0.0381
top acc: 0.0639 ::: bot acc: 0.0763
top acc: 0.0430 ::: bot acc: 0.0387
top acc: 0.0382 ::: bot acc: 0.0233
top acc: 0.0398 ::: bot acc: 0.0548
current epoch: 37
train loss is 0.002791
average val loss: 0.002047, accuracy: 0.0482
average test loss: 0.001141, accuracy: 0.0370
case acc: 0.024816893
case acc: 0.05547808
case acc: 0.05299334
case acc: 0.031367496
case acc: 0.022693686
case acc: 0.034863364
top acc: 0.0242 ::: bot acc: 0.0445
top acc: 0.0927 ::: bot acc: 0.0397
top acc: 0.0633 ::: bot acc: 0.0770
top acc: 0.0422 ::: bot acc: 0.0389
top acc: 0.0373 ::: bot acc: 0.0242
top acc: 0.0390 ::: bot acc: 0.0558
current epoch: 38
train loss is 0.002771
average val loss: 0.001999, accuracy: 0.0474
average test loss: 0.001135, accuracy: 0.0370
case acc: 0.02494373
case acc: 0.055006333
case acc: 0.053131245
case acc: 0.031295527
case acc: 0.022401314
case acc: 0.034923382
top acc: 0.0228 ::: bot acc: 0.0461
top acc: 0.0904 ::: bot acc: 0.0424
top acc: 0.0624 ::: bot acc: 0.0779
top acc: 0.0410 ::: bot acc: 0.0399
top acc: 0.0354 ::: bot acc: 0.0257
top acc: 0.0374 ::: bot acc: 0.0574
current epoch: 39
train loss is 0.002750
average val loss: 0.001953, accuracy: 0.0467
average test loss: 0.001132, accuracy: 0.0369
case acc: 0.02493726
case acc: 0.054520246
case acc: 0.053334318
case acc: 0.031513147
case acc: 0.022362078
case acc: 0.034979634
top acc: 0.0212 ::: bot acc: 0.0472
top acc: 0.0886 ::: bot acc: 0.0442
top acc: 0.0617 ::: bot acc: 0.0788
top acc: 0.0396 ::: bot acc: 0.0418
top acc: 0.0339 ::: bot acc: 0.0276
top acc: 0.0354 ::: bot acc: 0.0593
current epoch: 40
train loss is 0.002728
average val loss: 0.002028, accuracy: 0.0478
average test loss: 0.001127, accuracy: 0.0369
case acc: 0.024843935
case acc: 0.05480271
case acc: 0.052717052
case acc: 0.03146127
case acc: 0.022566142
case acc: 0.034940004
top acc: 0.0231 ::: bot acc: 0.0456
top acc: 0.0902 ::: bot acc: 0.0424
top acc: 0.0640 ::: bot acc: 0.0760
top acc: 0.0422 ::: bot acc: 0.0391
top acc: 0.0360 ::: bot acc: 0.0257
top acc: 0.0374 ::: bot acc: 0.0571
current epoch: 41
train loss is 0.002740
average val loss: 0.002052, accuracy: 0.0480
average test loss: 0.001127, accuracy: 0.0369
case acc: 0.024872297
case acc: 0.054762736
case acc: 0.05257793
case acc: 0.03142417
case acc: 0.02248951
case acc: 0.03505637
top acc: 0.0238 ::: bot acc: 0.0451
top acc: 0.0902 ::: bot acc: 0.0423
top acc: 0.0651 ::: bot acc: 0.0750
top acc: 0.0427 ::: bot acc: 0.0387
top acc: 0.0362 ::: bot acc: 0.0250
top acc: 0.0379 ::: bot acc: 0.0571
current epoch: 42
train loss is 0.002736
average val loss: 0.002057, accuracy: 0.0481
average test loss: 0.001125, accuracy: 0.0368
case acc: 0.02491327
case acc: 0.05474208
case acc: 0.052492186
case acc: 0.031364392
case acc: 0.022543726
case acc: 0.035025973
top acc: 0.0239 ::: bot acc: 0.0451
top acc: 0.0898 ::: bot acc: 0.0425
top acc: 0.0653 ::: bot acc: 0.0746
top acc: 0.0428 ::: bot acc: 0.0387
top acc: 0.0367 ::: bot acc: 0.0244
top acc: 0.0380 ::: bot acc: 0.0570
current epoch: 43
train loss is 0.002723
average val loss: 0.002031, accuracy: 0.0478
average test loss: 0.001125, accuracy: 0.0369
case acc: 0.025109118
case acc: 0.054445576
case acc: 0.05277045
case acc: 0.03147198
case acc: 0.022530848
case acc: 0.034929898
top acc: 0.0231 ::: bot acc: 0.0460
top acc: 0.0887 ::: bot acc: 0.0440
top acc: 0.0652 ::: bot acc: 0.0753
top acc: 0.0423 ::: bot acc: 0.0391
top acc: 0.0365 ::: bot acc: 0.0248
top acc: 0.0369 ::: bot acc: 0.0579
current epoch: 44
train loss is 0.002707
average val loss: 0.001996, accuracy: 0.0472
average test loss: 0.001120, accuracy: 0.0368
case acc: 0.025248343
case acc: 0.053965133
case acc: 0.05278058
case acc: 0.03142549
case acc: 0.022362575
case acc: 0.034801777
top acc: 0.0216 ::: bot acc: 0.0476
top acc: 0.0871 ::: bot acc: 0.0452
top acc: 0.0643 ::: bot acc: 0.0761
top acc: 0.0413 ::: bot acc: 0.0400
top acc: 0.0356 ::: bot acc: 0.0257
top acc: 0.0355 ::: bot acc: 0.0588
current epoch: 45
train loss is 0.002706
average val loss: 0.001996, accuracy: 0.0472
average test loss: 0.001123, accuracy: 0.0368
case acc: 0.025209583
case acc: 0.054107226
case acc: 0.052967027
case acc: 0.031349447
case acc: 0.022347165
case acc: 0.035032064
top acc: 0.0211 ::: bot acc: 0.0478
top acc: 0.0866 ::: bot acc: 0.0462
top acc: 0.0645 ::: bot acc: 0.0762
top acc: 0.0414 ::: bot acc: 0.0399
top acc: 0.0356 ::: bot acc: 0.0254
top acc: 0.0359 ::: bot acc: 0.0589
current epoch: 46
train loss is 0.002693
average val loss: 0.002003, accuracy: 0.0473
average test loss: 0.001117, accuracy: 0.0368
case acc: 0.025338672
case acc: 0.05382895
case acc: 0.052789364
case acc: 0.031256083
case acc: 0.022486128
case acc: 0.034983125
top acc: 0.0216 ::: bot acc: 0.0477
top acc: 0.0863 ::: bot acc: 0.0463
top acc: 0.0646 ::: bot acc: 0.0758
top acc: 0.0415 ::: bot acc: 0.0395
top acc: 0.0362 ::: bot acc: 0.0250
top acc: 0.0365 ::: bot acc: 0.0585
current epoch: 47
train loss is 0.002687
average val loss: 0.001973, accuracy: 0.0468
average test loss: 0.001116, accuracy: 0.0368
case acc: 0.02543244
case acc: 0.05348704
case acc: 0.05297357
case acc: 0.031415544
case acc: 0.022375932
case acc: 0.035017535
top acc: 0.0203 ::: bot acc: 0.0483
top acc: 0.0847 ::: bot acc: 0.0474
top acc: 0.0642 ::: bot acc: 0.0764
top acc: 0.0410 ::: bot acc: 0.0403
top acc: 0.0358 ::: bot acc: 0.0254
top acc: 0.0358 ::: bot acc: 0.0590
current epoch: 48
train loss is 0.002677
average val loss: 0.001910, accuracy: 0.0459
average test loss: 0.001120, accuracy: 0.0369
case acc: 0.025869211
case acc: 0.053246506
case acc: 0.053144567
case acc: 0.031597327
case acc: 0.02221965
case acc: 0.035064183
top acc: 0.0180 ::: bot acc: 0.0510
top acc: 0.0825 ::: bot acc: 0.0502
top acc: 0.0618 ::: bot acc: 0.0782
top acc: 0.0393 ::: bot acc: 0.0422
top acc: 0.0340 ::: bot acc: 0.0275
top acc: 0.0336 ::: bot acc: 0.0610
current epoch: 49
train loss is 0.002656
average val loss: 0.001862, accuracy: 0.0452
average test loss: 0.001123, accuracy: 0.0369
case acc: 0.026195467
case acc: 0.05267753
case acc: 0.05358941
case acc: 0.031618495
case acc: 0.022262044
case acc: 0.03521924
top acc: 0.0160 ::: bot acc: 0.0529
top acc: 0.0804 ::: bot acc: 0.0519
top acc: 0.0608 ::: bot acc: 0.0796
top acc: 0.0381 ::: bot acc: 0.0434
top acc: 0.0331 ::: bot acc: 0.0286
top acc: 0.0326 ::: bot acc: 0.0620
current epoch: 50
train loss is 0.002651
average val loss: 0.001852, accuracy: 0.0451
average test loss: 0.001125, accuracy: 0.0370
case acc: 0.026337123
case acc: 0.052779164
case acc: 0.05363737
case acc: 0.03173235
case acc: 0.022220949
case acc: 0.03514628
top acc: 0.0155 ::: bot acc: 0.0531
top acc: 0.0799 ::: bot acc: 0.0528
top acc: 0.0603 ::: bot acc: 0.0801
top acc: 0.0381 ::: bot acc: 0.0431
top acc: 0.0328 ::: bot acc: 0.0285
top acc: 0.0331 ::: bot acc: 0.0617
LME_Co_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5352 5352 5352
1.7082474 -0.6288155 0.29835227 -0.25048217
Validation: 600 600 600
Testing: 744 744 744
pre-processing time: 0.0003113746643066406
the split date is 2011-01-01
train dropout: 0.4 test dropout: 0.15
net initializing with time: 0.0036835670471191406
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.010504
average val loss: 0.006107, accuracy: 0.0992
average test loss: 0.006638, accuracy: 0.1038
case acc: 0.121634595
case acc: 0.09944984
case acc: 0.11572105
case acc: 0.10819664
case acc: 0.107722715
case acc: 0.07036111
top acc: 0.0617 ::: bot acc: 0.1852
top acc: 0.1472 ::: bot acc: 0.0544
top acc: 0.0594 ::: bot acc: 0.1692
top acc: 0.0707 ::: bot acc: 0.1438
top acc: 0.0372 ::: bot acc: 0.1704
top acc: 0.0586 ::: bot acc: 0.1045
current epoch: 2
train loss is 0.008519
average val loss: 0.004640, accuracy: 0.0645
average test loss: 0.005345, accuracy: 0.0754
case acc: 0.047498967
case acc: 0.19769135
case acc: 0.049297586
case acc: 0.03013255
case acc: 0.05512191
case acc: 0.07239136
top acc: 0.0511 ::: bot acc: 0.0743
top acc: 0.2461 ::: bot acc: 0.1517
top acc: 0.0784 ::: bot acc: 0.0573
top acc: 0.0441 ::: bot acc: 0.0387
top acc: 0.0784 ::: bot acc: 0.0676
top acc: 0.1549 ::: bot acc: 0.0133
current epoch: 3
train loss is 0.009880
average val loss: 0.010752, accuracy: 0.1216
average test loss: 0.011568, accuracy: 0.1242
case acc: 0.08016829
case acc: 0.26976997
case acc: 0.09284126
case acc: 0.07976585
case acc: 0.083659366
case acc: 0.1390081
top acc: 0.1379 ::: bot acc: 0.0200
top acc: 0.3185 ::: bot acc: 0.2239
top acc: 0.1644 ::: bot acc: 0.0360
top acc: 0.1232 ::: bot acc: 0.0419
top acc: 0.1599 ::: bot acc: 0.0212
top acc: 0.2279 ::: bot acc: 0.0672
current epoch: 4
train loss is 0.012713
average val loss: 0.005401, accuracy: 0.0726
average test loss: 0.006143, accuracy: 0.0815
case acc: 0.052918237
case acc: 0.2059943
case acc: 0.054958675
case acc: 0.035149537
case acc: 0.05663339
case acc: 0.08316855
top acc: 0.0902 ::: bot acc: 0.0353
top acc: 0.2547 ::: bot acc: 0.1597
top acc: 0.1127 ::: bot acc: 0.0268
top acc: 0.0687 ::: bot acc: 0.0168
top acc: 0.1120 ::: bot acc: 0.0355
top acc: 0.1700 ::: bot acc: 0.0156
current epoch: 5
train loss is 0.010863
average val loss: 0.003540, accuracy: 0.0728
average test loss: 0.004159, accuracy: 0.0801
case acc: 0.07343911
case acc: 0.09051717
case acc: 0.084981
case acc: 0.0855336
case acc: 0.079210594
case acc: 0.066772714
top acc: 0.0165 ::: bot acc: 0.1367
top acc: 0.1389 ::: bot acc: 0.0452
top acc: 0.0437 ::: bot acc: 0.1313
top acc: 0.0511 ::: bot acc: 0.1204
top acc: 0.0347 ::: bot acc: 0.1302
top acc: 0.0638 ::: bot acc: 0.0965
current epoch: 6
train loss is 0.005578
average val loss: 0.002969, accuracy: 0.0645
average test loss: 0.003597, accuracy: 0.0736
case acc: 0.063755095
case acc: 0.08956624
case acc: 0.07939123
case acc: 0.077066705
case acc: 0.068105385
case acc: 0.063507214
top acc: 0.0137 ::: bot acc: 0.1241
top acc: 0.1381 ::: bot acc: 0.0447
top acc: 0.0425 ::: bot acc: 0.1234
top acc: 0.0444 ::: bot acc: 0.1118
top acc: 0.0407 ::: bot acc: 0.1100
top acc: 0.0740 ::: bot acc: 0.0860
current epoch: 7
train loss is 0.004440
average val loss: 0.002385, accuracy: 0.0505
average test loss: 0.003130, accuracy: 0.0647
case acc: 0.048694532
case acc: 0.11628673
case acc: 0.060436163
case acc: 0.04845651
case acc: 0.054802816
case acc: 0.059671577
top acc: 0.0402 ::: bot acc: 0.0858
top acc: 0.1653 ::: bot acc: 0.0701
top acc: 0.0499 ::: bot acc: 0.0916
top acc: 0.0285 ::: bot acc: 0.0765
top acc: 0.0763 ::: bot acc: 0.0696
top acc: 0.1090 ::: bot acc: 0.0519
current epoch: 8
train loss is 0.004614
average val loss: 0.002208, accuracy: 0.0490
average test loss: 0.002968, accuracy: 0.0636
case acc: 0.048830837
case acc: 0.10658597
case acc: 0.061793566
case acc: 0.05050184
case acc: 0.054701842
case acc: 0.058937564
top acc: 0.0420 ::: bot acc: 0.0847
top acc: 0.1553 ::: bot acc: 0.0606
top acc: 0.0487 ::: bot acc: 0.0941
top acc: 0.0290 ::: bot acc: 0.0791
top acc: 0.0789 ::: bot acc: 0.0670
top acc: 0.1057 ::: bot acc: 0.0532
current epoch: 9
train loss is 0.004468
average val loss: 0.002081, accuracy: 0.0490
average test loss: 0.002820, accuracy: 0.0632
case acc: 0.049998842
case acc: 0.08954033
case acc: 0.06726768
case acc: 0.05748627
case acc: 0.0554703
case acc: 0.059364997
top acc: 0.0343 ::: bot acc: 0.0919
top acc: 0.1377 ::: bot acc: 0.0443
top acc: 0.0440 ::: bot acc: 0.1046
top acc: 0.0319 ::: bot acc: 0.0882
top acc: 0.0741 ::: bot acc: 0.0724
top acc: 0.0956 ::: bot acc: 0.0642
current epoch: 10
train loss is 0.003979
average val loss: 0.001958, accuracy: 0.0480
average test loss: 0.002730, accuracy: 0.0624
case acc: 0.05016583
case acc: 0.0797845
case acc: 0.0696082
case acc: 0.060244296
case acc: 0.055209924
case acc: 0.059418548
top acc: 0.0345 ::: bot acc: 0.0925
top acc: 0.1264 ::: bot acc: 0.0367
top acc: 0.0430 ::: bot acc: 0.1092
top acc: 0.0332 ::: bot acc: 0.0914
top acc: 0.0744 ::: bot acc: 0.0719
top acc: 0.0928 ::: bot acc: 0.0675
current epoch: 11
train loss is 0.003734
average val loss: 0.001824, accuracy: 0.0456
average test loss: 0.002618, accuracy: 0.0607
case acc: 0.048791595
case acc: 0.07732101
case acc: 0.06764934
case acc: 0.056890856
case acc: 0.054515194
case acc: 0.05929587
top acc: 0.0399 ::: bot acc: 0.0856
top acc: 0.1248 ::: bot acc: 0.0342
top acc: 0.0436 ::: bot acc: 0.1053
top acc: 0.0319 ::: bot acc: 0.0870
top acc: 0.0812 ::: bot acc: 0.0650
top acc: 0.0966 ::: bot acc: 0.0635
current epoch: 12
train loss is 0.003452
average val loss: 0.001731, accuracy: 0.0442
average test loss: 0.002538, accuracy: 0.0596
case acc: 0.048499323
case acc: 0.074062616
case acc: 0.06669419
case acc: 0.054819442
case acc: 0.054184686
case acc: 0.059451453
top acc: 0.0448 ::: bot acc: 0.0817
top acc: 0.1211 ::: bot acc: 0.0319
top acc: 0.0445 ::: bot acc: 0.1038
top acc: 0.0307 ::: bot acc: 0.0850
top acc: 0.0857 ::: bot acc: 0.0604
top acc: 0.0993 ::: bot acc: 0.0612
current epoch: 13
train loss is 0.003367
average val loss: 0.001640, accuracy: 0.0430
average test loss: 0.002463, accuracy: 0.0586
case acc: 0.04810297
case acc: 0.06929983
case acc: 0.066243954
case acc: 0.054154348
case acc: 0.05429041
case acc: 0.059563894
top acc: 0.0476 ::: bot acc: 0.0781
top acc: 0.1159 ::: bot acc: 0.0279
top acc: 0.0445 ::: bot acc: 0.1026
top acc: 0.0304 ::: bot acc: 0.0838
top acc: 0.0881 ::: bot acc: 0.0589
top acc: 0.0996 ::: bot acc: 0.0615
current epoch: 14
train loss is 0.003211
average val loss: 0.001573, accuracy: 0.0420
average test loss: 0.002395, accuracy: 0.0577
case acc: 0.04821147
case acc: 0.06620414
case acc: 0.065384895
case acc: 0.052277055
case acc: 0.05441826
case acc: 0.059469473
top acc: 0.0515 ::: bot acc: 0.0749
top acc: 0.1120 ::: bot acc: 0.0255
top acc: 0.0455 ::: bot acc: 0.1009
top acc: 0.0294 ::: bot acc: 0.0812
top acc: 0.0907 ::: bot acc: 0.0563
top acc: 0.1006 ::: bot acc: 0.0603
current epoch: 15
train loss is 0.003132
average val loss: 0.001505, accuracy: 0.0412
average test loss: 0.002327, accuracy: 0.0566
case acc: 0.04787321
case acc: 0.06272345
case acc: 0.0645041
case acc: 0.051594216
case acc: 0.05398355
case acc: 0.059121132
top acc: 0.0529 ::: bot acc: 0.0722
top acc: 0.1084 ::: bot acc: 0.0231
top acc: 0.0459 ::: bot acc: 0.0994
top acc: 0.0293 ::: bot acc: 0.0806
top acc: 0.0911 ::: bot acc: 0.0548
top acc: 0.1006 ::: bot acc: 0.0591
current epoch: 16
train loss is 0.003029
average val loss: 0.001445, accuracy: 0.0403
average test loss: 0.002283, accuracy: 0.0560
case acc: 0.048238724
case acc: 0.05963241
case acc: 0.06388813
case acc: 0.05078826
case acc: 0.05426329
case acc: 0.05922338
top acc: 0.0565 ::: bot acc: 0.0695
top acc: 0.1038 ::: bot acc: 0.0218
top acc: 0.0466 ::: bot acc: 0.0984
top acc: 0.0292 ::: bot acc: 0.0799
top acc: 0.0925 ::: bot acc: 0.0545
top acc: 0.1012 ::: bot acc: 0.0593
current epoch: 17
train loss is 0.002930
average val loss: 0.001401, accuracy: 0.0397
average test loss: 0.002238, accuracy: 0.0551
case acc: 0.047979742
case acc: 0.05872228
case acc: 0.062308524
case acc: 0.04850233
case acc: 0.053908583
case acc: 0.059055123
top acc: 0.0601 ::: bot acc: 0.0651
top acc: 0.1030 ::: bot acc: 0.0212
top acc: 0.0484 ::: bot acc: 0.0948
top acc: 0.0286 ::: bot acc: 0.0763
top acc: 0.0947 ::: bot acc: 0.0518
top acc: 0.1030 ::: bot acc: 0.0569
current epoch: 18
train loss is 0.002876
average val loss: 0.001359, accuracy: 0.0393
average test loss: 0.002185, accuracy: 0.0546
case acc: 0.048137013
case acc: 0.054038607
case acc: 0.06294485
case acc: 0.04936529
case acc: 0.05420053
case acc: 0.05909275
top acc: 0.0600 ::: bot acc: 0.0654
top acc: 0.0968 ::: bot acc: 0.0199
top acc: 0.0474 ::: bot acc: 0.0964
top acc: 0.0291 ::: bot acc: 0.0770
top acc: 0.0923 ::: bot acc: 0.0546
top acc: 0.1009 ::: bot acc: 0.0587
current epoch: 19
train loss is 0.002809
average val loss: 0.001315, accuracy: 0.0388
average test loss: 0.002148, accuracy: 0.0540
case acc: 0.04841202
case acc: 0.051960412
case acc: 0.062074047
case acc: 0.048445635
case acc: 0.05389524
case acc: 0.059220783
top acc: 0.0621 ::: bot acc: 0.0642
top acc: 0.0939 ::: bot acc: 0.0193
top acc: 0.0483 ::: bot acc: 0.0948
top acc: 0.0289 ::: bot acc: 0.0759
top acc: 0.0921 ::: bot acc: 0.0541
top acc: 0.1014 ::: bot acc: 0.0588
current epoch: 20
train loss is 0.002722
average val loss: 0.001285, accuracy: 0.0383
average test loss: 0.002117, accuracy: 0.0534
case acc: 0.04872573
case acc: 0.050428275
case acc: 0.061160393
case acc: 0.04698896
case acc: 0.05399704
case acc: 0.059210755
top acc: 0.0644 ::: bot acc: 0.0616
top acc: 0.0921 ::: bot acc: 0.0188
top acc: 0.0495 ::: bot acc: 0.0930
top acc: 0.0280 ::: bot acc: 0.0740
top acc: 0.0920 ::: bot acc: 0.0543
top acc: 0.1012 ::: bot acc: 0.0593
current epoch: 21
train loss is 0.002725
average val loss: 0.001257, accuracy: 0.0380
average test loss: 0.002099, accuracy: 0.0530
case acc: 0.049083997
case acc: 0.050464876
case acc: 0.059580904
case acc: 0.045439143
case acc: 0.05387353
case acc: 0.05930992
top acc: 0.0685 ::: bot acc: 0.0578
top acc: 0.0921 ::: bot acc: 0.0188
top acc: 0.0514 ::: bot acc: 0.0895
top acc: 0.0282 ::: bot acc: 0.0719
top acc: 0.0935 ::: bot acc: 0.0523
top acc: 0.1034 ::: bot acc: 0.0572
current epoch: 22
train loss is 0.002692
average val loss: 0.001238, accuracy: 0.0375
average test loss: 0.002075, accuracy: 0.0523
case acc: 0.0494346
case acc: 0.049291097
case acc: 0.058251634
case acc: 0.043991555
case acc: 0.054166757
case acc: 0.058866926
top acc: 0.0707 ::: bot acc: 0.0555
top acc: 0.0909 ::: bot acc: 0.0181
top acc: 0.0528 ::: bot acc: 0.0870
top acc: 0.0279 ::: bot acc: 0.0699
top acc: 0.0944 ::: bot acc: 0.0520
top acc: 0.1039 ::: bot acc: 0.0558
current epoch: 23
train loss is 0.002686
average val loss: 0.001234, accuracy: 0.0376
average test loss: 0.002071, accuracy: 0.0519
case acc: 0.050037906
case acc: 0.050173923
case acc: 0.05638375
case acc: 0.041952666
case acc: 0.053962354
case acc: 0.05897067
top acc: 0.0754 ::: bot acc: 0.0508
top acc: 0.0916 ::: bot acc: 0.0184
top acc: 0.0566 ::: bot acc: 0.0820
top acc: 0.0285 ::: bot acc: 0.0668
top acc: 0.0969 ::: bot acc: 0.0492
top acc: 0.1064 ::: bot acc: 0.0534
current epoch: 24
train loss is 0.002672
average val loss: 0.001227, accuracy: 0.0376
average test loss: 0.002069, accuracy: 0.0516
case acc: 0.0505028
case acc: 0.050015874
case acc: 0.05517202
case acc: 0.040373098
case acc: 0.054196365
case acc: 0.05933526
top acc: 0.0782 ::: bot acc: 0.0480
top acc: 0.0916 ::: bot acc: 0.0184
top acc: 0.0587 ::: bot acc: 0.0791
top acc: 0.0288 ::: bot acc: 0.0643
top acc: 0.0981 ::: bot acc: 0.0484
top acc: 0.1076 ::: bot acc: 0.0531
current epoch: 25
train loss is 0.002645
average val loss: 0.001206, accuracy: 0.0373
average test loss: 0.002045, accuracy: 0.0514
case acc: 0.050462317
case acc: 0.048196632
case acc: 0.055071842
case acc: 0.04052678
case acc: 0.0543441
case acc: 0.0595213
top acc: 0.0783 ::: bot acc: 0.0478
top acc: 0.0890 ::: bot acc: 0.0180
top acc: 0.0592 ::: bot acc: 0.0786
top acc: 0.0289 ::: bot acc: 0.0644
top acc: 0.0972 ::: bot acc: 0.0497
top acc: 0.1062 ::: bot acc: 0.0546
current epoch: 26
train loss is 0.002611
average val loss: 0.001199, accuracy: 0.0373
average test loss: 0.002033, accuracy: 0.0511
case acc: 0.050815
case acc: 0.04719058
case acc: 0.054362137
case acc: 0.04031808
case acc: 0.05433096
case acc: 0.059391495
top acc: 0.0794 ::: bot acc: 0.0466
top acc: 0.0877 ::: bot acc: 0.0176
top acc: 0.0605 ::: bot acc: 0.0774
top acc: 0.0291 ::: bot acc: 0.0638
top acc: 0.0973 ::: bot acc: 0.0496
top acc: 0.1058 ::: bot acc: 0.0549
current epoch: 27
train loss is 0.002593
average val loss: 0.001192, accuracy: 0.0373
average test loss: 0.002026, accuracy: 0.0507
case acc: 0.05077444
case acc: 0.04671559
case acc: 0.053667746
case acc: 0.03934313
case acc: 0.05442446
case acc: 0.05932003
top acc: 0.0807 ::: bot acc: 0.0444
top acc: 0.0874 ::: bot acc: 0.0170
top acc: 0.0626 ::: bot acc: 0.0751
top acc: 0.0296 ::: bot acc: 0.0622
top acc: 0.0981 ::: bot acc: 0.0486
top acc: 0.1061 ::: bot acc: 0.0545
current epoch: 28
train loss is 0.002598
average val loss: 0.001197, accuracy: 0.0374
average test loss: 0.002040, accuracy: 0.0506
case acc: 0.05162322
case acc: 0.047131192
case acc: 0.05281892
case acc: 0.03797238
case acc: 0.054425076
case acc: 0.05933333
top acc: 0.0842 ::: bot acc: 0.0423
top acc: 0.0878 ::: bot acc: 0.0173
top acc: 0.0650 ::: bot acc: 0.0725
top acc: 0.0297 ::: bot acc: 0.0602
top acc: 0.0999 ::: bot acc: 0.0470
top acc: 0.1072 ::: bot acc: 0.0531
current epoch: 29
train loss is 0.002581
average val loss: 0.001204, accuracy: 0.0375
average test loss: 0.002039, accuracy: 0.0504
case acc: 0.051935762
case acc: 0.046999786
case acc: 0.052228205
case acc: 0.03713838
case acc: 0.054518916
case acc: 0.059420776
top acc: 0.0856 ::: bot acc: 0.0401
top acc: 0.0876 ::: bot acc: 0.0176
top acc: 0.0673 ::: bot acc: 0.0705
top acc: 0.0304 ::: bot acc: 0.0581
top acc: 0.1008 ::: bot acc: 0.0456
top acc: 0.1080 ::: bot acc: 0.0523
current epoch: 30
train loss is 0.002605
average val loss: 0.001191, accuracy: 0.0374
average test loss: 0.002024, accuracy: 0.0501
case acc: 0.051994734
case acc: 0.045702577
case acc: 0.05176924
case acc: 0.037030175
case acc: 0.054633804
case acc: 0.059298776
top acc: 0.0859 ::: bot acc: 0.0403
top acc: 0.0855 ::: bot acc: 0.0171
top acc: 0.0673 ::: bot acc: 0.0695
top acc: 0.0303 ::: bot acc: 0.0586
top acc: 0.1004 ::: bot acc: 0.0464
top acc: 0.1073 ::: bot acc: 0.0529
current epoch: 31
train loss is 0.002549
average val loss: 0.001179, accuracy: 0.0373
average test loss: 0.002011, accuracy: 0.0499
case acc: 0.05191871
case acc: 0.044629756
case acc: 0.05197584
case acc: 0.037110526
case acc: 0.054447934
case acc: 0.0592416
top acc: 0.0859 ::: bot acc: 0.0404
top acc: 0.0839 ::: bot acc: 0.0175
top acc: 0.0679 ::: bot acc: 0.0698
top acc: 0.0301 ::: bot acc: 0.0585
top acc: 0.1004 ::: bot acc: 0.0461
top acc: 0.1064 ::: bot acc: 0.0538
current epoch: 32
train loss is 0.002536
average val loss: 0.001172, accuracy: 0.0373
average test loss: 0.002005, accuracy: 0.0497
case acc: 0.05192757
case acc: 0.04415357
case acc: 0.051668677
case acc: 0.036705144
case acc: 0.054504003
case acc: 0.059280753
top acc: 0.0859 ::: bot acc: 0.0402
top acc: 0.0832 ::: bot acc: 0.0176
top acc: 0.0686 ::: bot acc: 0.0686
top acc: 0.0298 ::: bot acc: 0.0579
top acc: 0.1008 ::: bot acc: 0.0456
top acc: 0.1066 ::: bot acc: 0.0538
current epoch: 33
train loss is 0.002520
average val loss: 0.001167, accuracy: 0.0372
average test loss: 0.002000, accuracy: 0.0496
case acc: 0.052146826
case acc: 0.043598384
case acc: 0.051635925
case acc: 0.036487542
case acc: 0.05443629
case acc: 0.05910683
top acc: 0.0866 ::: bot acc: 0.0396
top acc: 0.0822 ::: bot acc: 0.0177
top acc: 0.0697 ::: bot acc: 0.0681
top acc: 0.0304 ::: bot acc: 0.0578
top acc: 0.1007 ::: bot acc: 0.0453
top acc: 0.1064 ::: bot acc: 0.0537
current epoch: 34
train loss is 0.002509
average val loss: 0.001145, accuracy: 0.0369
average test loss: 0.001968, accuracy: 0.0494
case acc: 0.051772926
case acc: 0.041592635
case acc: 0.051912285
case acc: 0.037588578
case acc: 0.05432928
case acc: 0.059190877
top acc: 0.0846 ::: bot acc: 0.0415
top acc: 0.0791 ::: bot acc: 0.0185
top acc: 0.0681 ::: bot acc: 0.0693
top acc: 0.0300 ::: bot acc: 0.0593
top acc: 0.0993 ::: bot acc: 0.0471
top acc: 0.1045 ::: bot acc: 0.0556
current epoch: 35
train loss is 0.002471
average val loss: 0.001130, accuracy: 0.0367
average test loss: 0.001949, accuracy: 0.0492
case acc: 0.051358227
case acc: 0.040684048
case acc: 0.052238274
case acc: 0.037881423
case acc: 0.054184344
case acc: 0.05897032
top acc: 0.0829 ::: bot acc: 0.0428
top acc: 0.0769 ::: bot acc: 0.0201
top acc: 0.0672 ::: bot acc: 0.0706
top acc: 0.0297 ::: bot acc: 0.0599
top acc: 0.0979 ::: bot acc: 0.0483
top acc: 0.1029 ::: bot acc: 0.0566
current epoch: 36
train loss is 0.002440
average val loss: 0.001129, accuracy: 0.0367
average test loss: 0.001949, accuracy: 0.0492
case acc: 0.051569756
case acc: 0.040394504
case acc: 0.051940247
case acc: 0.037672926
case acc: 0.054276977
case acc: 0.05921356
top acc: 0.0835 ::: bot acc: 0.0425
top acc: 0.0763 ::: bot acc: 0.0203
top acc: 0.0678 ::: bot acc: 0.0699
top acc: 0.0298 ::: bot acc: 0.0595
top acc: 0.0983 ::: bot acc: 0.0482
top acc: 0.1035 ::: bot acc: 0.0566
current epoch: 37
train loss is 0.002427
average val loss: 0.001115, accuracy: 0.0365
average test loss: 0.001937, accuracy: 0.0490
case acc: 0.051448982
case acc: 0.03983804
case acc: 0.05199475
case acc: 0.03755863
case acc: 0.054222636
case acc: 0.058946572
top acc: 0.0828 ::: bot acc: 0.0435
top acc: 0.0747 ::: bot acc: 0.0212
top acc: 0.0673 ::: bot acc: 0.0699
top acc: 0.0295 ::: bot acc: 0.0597
top acc: 0.0977 ::: bot acc: 0.0485
top acc: 0.1032 ::: bot acc: 0.0566
current epoch: 38
train loss is 0.002426
average val loss: 0.001114, accuracy: 0.0366
average test loss: 0.001933, accuracy: 0.0489
case acc: 0.051384535
case acc: 0.03932307
case acc: 0.05210849
case acc: 0.037287485
case acc: 0.05430588
case acc: 0.059060898
top acc: 0.0827 ::: bot acc: 0.0436
top acc: 0.0741 ::: bot acc: 0.0217
top acc: 0.0677 ::: bot acc: 0.0699
top acc: 0.0297 ::: bot acc: 0.0591
top acc: 0.0982 ::: bot acc: 0.0483
top acc: 0.1033 ::: bot acc: 0.0566
current epoch: 39
train loss is 0.002402
average val loss: 0.001095, accuracy: 0.0363
average test loss: 0.001919, accuracy: 0.0489
case acc: 0.051068984
case acc: 0.037886385
case acc: 0.052774414
case acc: 0.038385402
case acc: 0.05417542
case acc: 0.059230804
top acc: 0.0807 ::: bot acc: 0.0457
top acc: 0.0710 ::: bot acc: 0.0236
top acc: 0.0659 ::: bot acc: 0.0720
top acc: 0.0296 ::: bot acc: 0.0607
top acc: 0.0967 ::: bot acc: 0.0499
top acc: 0.1019 ::: bot acc: 0.0585
current epoch: 40
train loss is 0.002392
average val loss: 0.001084, accuracy: 0.0361
average test loss: 0.001913, accuracy: 0.0489
case acc: 0.05105865
case acc: 0.03747836
case acc: 0.05285098
case acc: 0.03864302
case acc: 0.054216392
case acc: 0.05904178
top acc: 0.0800 ::: bot acc: 0.0467
top acc: 0.0696 ::: bot acc: 0.0252
top acc: 0.0653 ::: bot acc: 0.0727
top acc: 0.0297 ::: bot acc: 0.0611
top acc: 0.0964 ::: bot acc: 0.0501
top acc: 0.1016 ::: bot acc: 0.0584
current epoch: 41
train loss is 0.002364
average val loss: 0.001073, accuracy: 0.0360
average test loss: 0.001886, accuracy: 0.0489
case acc: 0.05018506
case acc: 0.036311075
case acc: 0.05383989
case acc: 0.04020419
case acc: 0.054015104
case acc: 0.058949064
top acc: 0.0762 ::: bot acc: 0.0500
top acc: 0.0653 ::: bot acc: 0.0290
top acc: 0.0619 ::: bot acc: 0.0756
top acc: 0.0291 ::: bot acc: 0.0638
top acc: 0.0936 ::: bot acc: 0.0527
top acc: 0.0989 ::: bot acc: 0.0610
current epoch: 42
train loss is 0.002340
average val loss: 0.001067, accuracy: 0.0359
average test loss: 0.001877, accuracy: 0.0492
case acc: 0.04977834
case acc: 0.035652146
case acc: 0.05478358
case acc: 0.04171419
case acc: 0.054074273
case acc: 0.059121888
top acc: 0.0733 ::: bot acc: 0.0527
top acc: 0.0621 ::: bot acc: 0.0324
top acc: 0.0596 ::: bot acc: 0.0781
top acc: 0.0284 ::: bot acc: 0.0662
top acc: 0.0913 ::: bot acc: 0.0552
top acc: 0.0966 ::: bot acc: 0.0633
current epoch: 43
train loss is 0.002314
average val loss: 0.001076, accuracy: 0.0362
average test loss: 0.001879, accuracy: 0.0496
case acc: 0.049296673
case acc: 0.035221737
case acc: 0.056385014
case acc: 0.043760467
case acc: 0.053928535
case acc: 0.05917776
top acc: 0.0696 ::: bot acc: 0.0564
top acc: 0.0581 ::: bot acc: 0.0367
top acc: 0.0567 ::: bot acc: 0.0818
top acc: 0.0281 ::: bot acc: 0.0697
top acc: 0.0878 ::: bot acc: 0.0581
top acc: 0.0935 ::: bot acc: 0.0664
current epoch: 44
train loss is 0.002302
average val loss: 0.001083, accuracy: 0.0363
average test loss: 0.001885, accuracy: 0.0499
case acc: 0.049311634
case acc: 0.034961827
case acc: 0.05693785
case acc: 0.044568706
case acc: 0.054197058
case acc: 0.05936792
top acc: 0.0681 ::: bot acc: 0.0583
top acc: 0.0558 ::: bot acc: 0.0386
top acc: 0.0556 ::: bot acc: 0.0834
top acc: 0.0282 ::: bot acc: 0.0706
top acc: 0.0868 ::: bot acc: 0.0596
top acc: 0.0926 ::: bot acc: 0.0675
current epoch: 45
train loss is 0.002292
average val loss: 0.001095, accuracy: 0.0366
average test loss: 0.001901, accuracy: 0.0503
case acc: 0.04904549
case acc: 0.034954604
case acc: 0.058150146
case acc: 0.046067577
case acc: 0.05426772
case acc: 0.059506316
top acc: 0.0654 ::: bot acc: 0.0610
top acc: 0.0533 ::: bot acc: 0.0416
top acc: 0.0539 ::: bot acc: 0.0862
top acc: 0.0283 ::: bot acc: 0.0728
top acc: 0.0848 ::: bot acc: 0.0616
top acc: 0.0908 ::: bot acc: 0.0694
current epoch: 46
train loss is 0.002285
average val loss: 0.001109, accuracy: 0.0368
average test loss: 0.001911, accuracy: 0.0506
case acc: 0.04865444
case acc: 0.034711197
case acc: 0.05916817
case acc: 0.04742061
case acc: 0.05425806
case acc: 0.05961651
top acc: 0.0630 ::: bot acc: 0.0631
top acc: 0.0501 ::: bot acc: 0.0443
top acc: 0.0520 ::: bot acc: 0.0886
top acc: 0.0284 ::: bot acc: 0.0747
top acc: 0.0832 ::: bot acc: 0.0628
top acc: 0.0892 ::: bot acc: 0.0708
current epoch: 47
train loss is 0.002283
average val loss: 0.001151, accuracy: 0.0375
average test loss: 0.001958, accuracy: 0.0516
case acc: 0.048523005
case acc: 0.035017587
case acc: 0.061190095
case acc: 0.050150234
case acc: 0.05466078
case acc: 0.06004979
top acc: 0.0594 ::: bot acc: 0.0669
top acc: 0.0459 ::: bot acc: 0.0489
top acc: 0.0492 ::: bot acc: 0.0930
top acc: 0.0292 ::: bot acc: 0.0786
top acc: 0.0803 ::: bot acc: 0.0661
top acc: 0.0862 ::: bot acc: 0.0738
current epoch: 48
train loss is 0.002286
average val loss: 0.001184, accuracy: 0.0381
average test loss: 0.001993, accuracy: 0.0522
case acc: 0.048384346
case acc: 0.03536721
case acc: 0.06263172
case acc: 0.051739313
case acc: 0.054790832
case acc: 0.06050212
top acc: 0.0569 ::: bot acc: 0.0695
top acc: 0.0429 ::: bot acc: 0.0520
top acc: 0.0480 ::: bot acc: 0.0958
top acc: 0.0298 ::: bot acc: 0.0807
top acc: 0.0788 ::: bot acc: 0.0678
top acc: 0.0852 ::: bot acc: 0.0750
current epoch: 49
train loss is 0.002298
average val loss: 0.001209, accuracy: 0.0385
average test loss: 0.002026, accuracy: 0.0528
case acc: 0.048362356
case acc: 0.035884466
case acc: 0.06422412
case acc: 0.053016163
case acc: 0.054805644
case acc: 0.060706485
top acc: 0.0546 ::: bot acc: 0.0716
top acc: 0.0398 ::: bot acc: 0.0550
top acc: 0.0467 ::: bot acc: 0.0988
top acc: 0.0304 ::: bot acc: 0.0823
top acc: 0.0776 ::: bot acc: 0.0686
top acc: 0.0843 ::: bot acc: 0.0760
current epoch: 50
train loss is 0.002307
average val loss: 0.001319, accuracy: 0.0403
average test loss: 0.002127, accuracy: 0.0545
case acc: 0.048403904
case acc: 0.037443977
case acc: 0.06758544
case acc: 0.057088975
case acc: 0.05536316
case acc: 0.061298553
top acc: 0.0491 ::: bot acc: 0.0772
top acc: 0.0332 ::: bot acc: 0.0613
top acc: 0.0442 ::: bot acc: 0.1049
top acc: 0.0321 ::: bot acc: 0.0875
top acc: 0.0735 ::: bot acc: 0.0726
top acc: 0.0802 ::: bot acc: 0.0795
LME_Co_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5328 5328 5328
1.7082474 -0.6288155 0.29835227 -0.25048217
Validation: 594 594 594
Testing: 774 774 774
pre-processing time: 0.00025534629821777344
the split date is 2011-07-01
train dropout: 0.4 test dropout: 0.15
net initializing with time: 0.0026645660400390625
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.010665
average val loss: 0.005495, accuracy: 0.0939
average test loss: 0.006118, accuracy: 0.0968
case acc: 0.12779143
case acc: 0.114703156
case acc: 0.0993637
case acc: 0.068573765
case acc: 0.122061856
case acc: 0.0483208
top acc: 0.0768 ::: bot acc: 0.1806
top acc: 0.1564 ::: bot acc: 0.0755
top acc: 0.0390 ::: bot acc: 0.1621
top acc: 0.0280 ::: bot acc: 0.1232
top acc: 0.0601 ::: bot acc: 0.1823
top acc: 0.0421 ::: bot acc: 0.0741
current epoch: 2
train loss is 0.009137
average val loss: 0.006824, accuracy: 0.0852
average test loss: 0.006296, accuracy: 0.0850
case acc: 0.038767688
case acc: 0.22013769
case acc: 0.051826056
case acc: 0.06675825
case acc: 0.04589319
case acc: 0.086658955
top acc: 0.0432 ::: bot acc: 0.0610
top acc: 0.2633 ::: bot acc: 0.1792
top acc: 0.0861 ::: bot acc: 0.0437
top acc: 0.1234 ::: bot acc: 0.0192
top acc: 0.0491 ::: bot acc: 0.0713
top acc: 0.1474 ::: bot acc: 0.0344
current epoch: 3
train loss is 0.010384
average val loss: 0.012997, accuracy: 0.1351
average test loss: 0.011776, accuracy: 0.1268
case acc: 0.0655755
case acc: 0.2762742
case acc: 0.093831554
case acc: 0.120315865
case acc: 0.061166048
case acc: 0.14365855
top acc: 0.1133 ::: bot acc: 0.0191
top acc: 0.3195 ::: bot acc: 0.2360
top acc: 0.1558 ::: bot acc: 0.0354
top acc: 0.1855 ::: bot acc: 0.0537
top acc: 0.1148 ::: bot acc: 0.0167
top acc: 0.2053 ::: bot acc: 0.0897
current epoch: 4
train loss is 0.013263
average val loss: 0.004677, accuracy: 0.0718
average test loss: 0.004273, accuracy: 0.0715
case acc: 0.04297045
case acc: 0.17470923
case acc: 0.049558513
case acc: 0.054208342
case acc: 0.05162929
case acc: 0.055952154
top acc: 0.0285 ::: bot acc: 0.0766
top acc: 0.2174 ::: bot acc: 0.1347
top acc: 0.0660 ::: bot acc: 0.0650
top acc: 0.0941 ::: bot acc: 0.0390
top acc: 0.0328 ::: bot acc: 0.0889
top acc: 0.1104 ::: bot acc: 0.0161
current epoch: 5
train loss is 0.009282
average val loss: 0.004272, accuracy: 0.0817
average test loss: 0.004656, accuracy: 0.0835
case acc: 0.10416498
case acc: 0.08016503
case acc: 0.09069297
case acc: 0.06817946
case acc: 0.104450405
case acc: 0.053322516
top acc: 0.0530 ::: bot acc: 0.1570
top acc: 0.1223 ::: bot acc: 0.0409
top acc: 0.0337 ::: bot acc: 0.1521
top acc: 0.0269 ::: bot acc: 0.1228
top acc: 0.0447 ::: bot acc: 0.1629
top acc: 0.0294 ::: bot acc: 0.0888
current epoch: 6
train loss is 0.004957
average val loss: 0.003240, accuracy: 0.0680
average test loss: 0.003176, accuracy: 0.0665
case acc: 0.066915445
case acc: 0.10718934
case acc: 0.06484245
case acc: 0.049928393
case acc: 0.06741159
case acc: 0.04292185
top acc: 0.0214 ::: bot acc: 0.1163
top acc: 0.1489 ::: bot acc: 0.0681
top acc: 0.0255 ::: bot acc: 0.1173
top acc: 0.0461 ::: bot acc: 0.0853
top acc: 0.0230 ::: bot acc: 0.1176
top acc: 0.0644 ::: bot acc: 0.0508
current epoch: 7
train loss is 0.004525
average val loss: 0.003287, accuracy: 0.0652
average test loss: 0.002932, accuracy: 0.0618
case acc: 0.048399158
case acc: 0.122691624
case acc: 0.053876474
case acc: 0.048659183
case acc: 0.051435087
case acc: 0.045670077
top acc: 0.0197 ::: bot acc: 0.0900
top acc: 0.1653 ::: bot acc: 0.0828
top acc: 0.0361 ::: bot acc: 0.0957
top acc: 0.0700 ::: bot acc: 0.0629
top acc: 0.0326 ::: bot acc: 0.0895
top acc: 0.0876 ::: bot acc: 0.0294
current epoch: 8
train loss is 0.004787
average val loss: 0.002979, accuracy: 0.0643
average test loss: 0.002686, accuracy: 0.0603
case acc: 0.053084478
case acc: 0.102975674
case acc: 0.059703212
case acc: 0.048462484
case acc: 0.05462174
case acc: 0.042955764
top acc: 0.0168 ::: bot acc: 0.0984
top acc: 0.1454 ::: bot acc: 0.0631
top acc: 0.0281 ::: bot acc: 0.1085
top acc: 0.0576 ::: bot acc: 0.0746
top acc: 0.0282 ::: bot acc: 0.0960
top acc: 0.0749 ::: bot acc: 0.0408
current epoch: 9
train loss is 0.004330
average val loss: 0.002841, accuracy: 0.0638
average test loss: 0.002570, accuracy: 0.0595
case acc: 0.05678493
case acc: 0.08780239
case acc: 0.06466822
case acc: 0.04953398
case acc: 0.055846702
case acc: 0.042551298
top acc: 0.0182 ::: bot acc: 0.1032
top acc: 0.1296 ::: bot acc: 0.0483
top acc: 0.0258 ::: bot acc: 0.1168
top acc: 0.0509 ::: bot acc: 0.0818
top acc: 0.0269 ::: bot acc: 0.0987
top acc: 0.0670 ::: bot acc: 0.0484
current epoch: 10
train loss is 0.003777
average val loss: 0.002727, accuracy: 0.0627
average test loss: 0.002442, accuracy: 0.0579
case acc: 0.055287093
case acc: 0.07923895
case acc: 0.06599024
case acc: 0.049374845
case acc: 0.054514278
case acc: 0.042779546
top acc: 0.0180 ::: bot acc: 0.1011
top acc: 0.1218 ::: bot acc: 0.0397
top acc: 0.0257 ::: bot acc: 0.1189
top acc: 0.0498 ::: bot acc: 0.0828
top acc: 0.0280 ::: bot acc: 0.0964
top acc: 0.0663 ::: bot acc: 0.0489
current epoch: 11
train loss is 0.003513
average val loss: 0.002665, accuracy: 0.0613
average test loss: 0.002287, accuracy: 0.0557
case acc: 0.050189942
case acc: 0.07939917
case acc: 0.06266184
case acc: 0.04871422
case acc: 0.050412096
case acc: 0.0430247
top acc: 0.0189 ::: bot acc: 0.0931
top acc: 0.1221 ::: bot acc: 0.0398
top acc: 0.0267 ::: bot acc: 0.1128
top acc: 0.0566 ::: bot acc: 0.0762
top acc: 0.0340 ::: bot acc: 0.0869
top acc: 0.0731 ::: bot acc: 0.0430
current epoch: 12
train loss is 0.003409
average val loss: 0.002624, accuracy: 0.0603
average test loss: 0.002163, accuracy: 0.0540
case acc: 0.04590371
case acc: 0.07818209
case acc: 0.05984783
case acc: 0.048092775
case acc: 0.048370585
case acc: 0.043368634
top acc: 0.0210 ::: bot acc: 0.0856
top acc: 0.1205 ::: bot acc: 0.0387
top acc: 0.0277 ::: bot acc: 0.1086
top acc: 0.0617 ::: bot acc: 0.0705
top acc: 0.0404 ::: bot acc: 0.0807
top acc: 0.0778 ::: bot acc: 0.0380
current epoch: 13
train loss is 0.003266
average val loss: 0.002520, accuracy: 0.0595
average test loss: 0.002087, accuracy: 0.0529
case acc: 0.046257693
case acc: 0.0696786
case acc: 0.061909292
case acc: 0.048379518
case acc: 0.04874024
case acc: 0.042708166
top acc: 0.0211 ::: bot acc: 0.0859
top acc: 0.1126 ::: bot acc: 0.0294
top acc: 0.0274 ::: bot acc: 0.1117
top acc: 0.0589 ::: bot acc: 0.0733
top acc: 0.0387 ::: bot acc: 0.0822
top acc: 0.0740 ::: bot acc: 0.0413
current epoch: 14
train loss is 0.003100
average val loss: 0.002462, accuracy: 0.0587
average test loss: 0.002009, accuracy: 0.0518
case acc: 0.045415517
case acc: 0.06361477
case acc: 0.0619313
case acc: 0.04842801
case acc: 0.048785277
case acc: 0.042889096
top acc: 0.0221 ::: bot acc: 0.0841
top acc: 0.1064 ::: bot acc: 0.0237
top acc: 0.0272 ::: bot acc: 0.1119
top acc: 0.0591 ::: bot acc: 0.0734
top acc: 0.0392 ::: bot acc: 0.0820
top acc: 0.0736 ::: bot acc: 0.0421
current epoch: 15
train loss is 0.002952
average val loss: 0.002411, accuracy: 0.0581
average test loss: 0.001942, accuracy: 0.0508
case acc: 0.04471887
case acc: 0.05806087
case acc: 0.061948325
case acc: 0.048379406
case acc: 0.048508015
case acc: 0.042910717
top acc: 0.0231 ::: bot acc: 0.0823
top acc: 0.1004 ::: bot acc: 0.0188
top acc: 0.0271 ::: bot acc: 0.1122
top acc: 0.0594 ::: bot acc: 0.0731
top acc: 0.0391 ::: bot acc: 0.0812
top acc: 0.0731 ::: bot acc: 0.0432
current epoch: 16
train loss is 0.002836
average val loss: 0.002379, accuracy: 0.0573
average test loss: 0.001870, accuracy: 0.0498
case acc: 0.042755164
case acc: 0.05654764
case acc: 0.060172968
case acc: 0.047908444
case acc: 0.047710933
case acc: 0.04340927
top acc: 0.0264 ::: bot acc: 0.0782
top acc: 0.0990 ::: bot acc: 0.0173
top acc: 0.0281 ::: bot acc: 0.1089
top acc: 0.0621 ::: bot acc: 0.0696
top acc: 0.0416 ::: bot acc: 0.0789
top acc: 0.0754 ::: bot acc: 0.0409
current epoch: 17
train loss is 0.002799
average val loss: 0.002338, accuracy: 0.0564
average test loss: 0.001814, accuracy: 0.0490
case acc: 0.041388936
case acc: 0.0551417
case acc: 0.058600184
case acc: 0.048263293
case acc: 0.04735017
case acc: 0.043384574
top acc: 0.0309 ::: bot acc: 0.0730
top acc: 0.0970 ::: bot acc: 0.0163
top acc: 0.0295 ::: bot acc: 0.1057
top acc: 0.0659 ::: bot acc: 0.0665
top acc: 0.0446 ::: bot acc: 0.0766
top acc: 0.0775 ::: bot acc: 0.0384
current epoch: 18
train loss is 0.002712
average val loss: 0.002298, accuracy: 0.0561
average test loss: 0.001782, accuracy: 0.0484
case acc: 0.041028623
case acc: 0.05064242
case acc: 0.058937427
case acc: 0.048493974
case acc: 0.04762151
case acc: 0.04353437
top acc: 0.0320 ::: bot acc: 0.0723
top acc: 0.0923 ::: bot acc: 0.0125
top acc: 0.0299 ::: bot acc: 0.1061
top acc: 0.0658 ::: bot acc: 0.0673
top acc: 0.0431 ::: bot acc: 0.0778
top acc: 0.0768 ::: bot acc: 0.0398
current epoch: 19
train loss is 0.002622
average val loss: 0.002273, accuracy: 0.0556
average test loss: 0.001736, accuracy: 0.0477
case acc: 0.040327154
case acc: 0.048633184
case acc: 0.05776795
case acc: 0.048282232
case acc: 0.0476737
case acc: 0.043354552
top acc: 0.0339 ::: bot acc: 0.0703
top acc: 0.0900 ::: bot acc: 0.0116
top acc: 0.0304 ::: bot acc: 0.1043
top acc: 0.0668 ::: bot acc: 0.0652
top acc: 0.0437 ::: bot acc: 0.0775
top acc: 0.0766 ::: bot acc: 0.0395
current epoch: 20
train loss is 0.002599
average val loss: 0.002232, accuracy: 0.0551
average test loss: 0.001702, accuracy: 0.0472
case acc: 0.040198278
case acc: 0.045874782
case acc: 0.05742035
case acc: 0.04821992
case acc: 0.048354216
case acc: 0.043270916
top acc: 0.0354 ::: bot acc: 0.0688
top acc: 0.0859 ::: bot acc: 0.0110
top acc: 0.0311 ::: bot acc: 0.1029
top acc: 0.0672 ::: bot acc: 0.0650
top acc: 0.0423 ::: bot acc: 0.0798
top acc: 0.0751 ::: bot acc: 0.0414
current epoch: 21
train loss is 0.002542
average val loss: 0.002246, accuracy: 0.0547
average test loss: 0.001671, accuracy: 0.0468
case acc: 0.038930673
case acc: 0.0473155
case acc: 0.05495906
case acc: 0.048661694
case acc: 0.047147624
case acc: 0.043591768
top acc: 0.0415 ::: bot acc: 0.0623
top acc: 0.0883 ::: bot acc: 0.0108
top acc: 0.0351 ::: bot acc: 0.0976
top acc: 0.0722 ::: bot acc: 0.0600
top acc: 0.0452 ::: bot acc: 0.0759
top acc: 0.0787 ::: bot acc: 0.0377
current epoch: 22
train loss is 0.002543
average val loss: 0.002258, accuracy: 0.0544
average test loss: 0.001661, accuracy: 0.0466
case acc: 0.03870218
case acc: 0.04764886
case acc: 0.053147536
case acc: 0.04928999
case acc: 0.046913598
case acc: 0.04389024
top acc: 0.0459 ::: bot acc: 0.0589
top acc: 0.0890 ::: bot acc: 0.0110
top acc: 0.0385 ::: bot acc: 0.0932
top acc: 0.0761 ::: bot acc: 0.0569
top acc: 0.0474 ::: bot acc: 0.0740
top acc: 0.0808 ::: bot acc: 0.0352
current epoch: 23
train loss is 0.002540
average val loss: 0.002324, accuracy: 0.0544
average test loss: 0.001666, accuracy: 0.0468
case acc: 0.038640108
case acc: 0.049902637
case acc: 0.051553592
case acc: 0.050106633
case acc: 0.045805812
case acc: 0.044512074
top acc: 0.0522 ::: bot acc: 0.0525
top acc: 0.0914 ::: bot acc: 0.0124
top acc: 0.0443 ::: bot acc: 0.0873
top acc: 0.0812 ::: bot acc: 0.0511
top acc: 0.0510 ::: bot acc: 0.0699
top acc: 0.0846 ::: bot acc: 0.0314
current epoch: 24
train loss is 0.002540
average val loss: 0.002293, accuracy: 0.0539
average test loss: 0.001642, accuracy: 0.0464
case acc: 0.03861381
case acc: 0.047839962
case acc: 0.051393487
case acc: 0.05002821
case acc: 0.04645684
case acc: 0.044299334
top acc: 0.0526 ::: bot acc: 0.0515
top acc: 0.0888 ::: bot acc: 0.0115
top acc: 0.0455 ::: bot acc: 0.0864
top acc: 0.0811 ::: bot acc: 0.0512
top acc: 0.0507 ::: bot acc: 0.0713
top acc: 0.0835 ::: bot acc: 0.0324
current epoch: 25
train loss is 0.002521
average val loss: 0.002286, accuracy: 0.0537
average test loss: 0.001626, accuracy: 0.0462
case acc: 0.03883914
case acc: 0.046300884
case acc: 0.051128626
case acc: 0.050510835
case acc: 0.046173096
case acc: 0.044229995
top acc: 0.0542 ::: bot acc: 0.0503
top acc: 0.0867 ::: bot acc: 0.0106
top acc: 0.0468 ::: bot acc: 0.0849
top acc: 0.0822 ::: bot acc: 0.0505
top acc: 0.0501 ::: bot acc: 0.0712
top acc: 0.0833 ::: bot acc: 0.0327
current epoch: 26
train loss is 0.002480
average val loss: 0.002304, accuracy: 0.0535
average test loss: 0.001627, accuracy: 0.0463
case acc: 0.03909154
case acc: 0.046365287
case acc: 0.050645307
case acc: 0.05115582
case acc: 0.04594522
case acc: 0.044572573
top acc: 0.0570 ::: bot acc: 0.0477
top acc: 0.0869 ::: bot acc: 0.0107
top acc: 0.0500 ::: bot acc: 0.0818
top acc: 0.0844 ::: bot acc: 0.0480
top acc: 0.0520 ::: bot acc: 0.0692
top acc: 0.0844 ::: bot acc: 0.0316
current epoch: 27
train loss is 0.002466
average val loss: 0.002270, accuracy: 0.0532
average test loss: 0.001604, accuracy: 0.0459
case acc: 0.039221615
case acc: 0.044273943
case acc: 0.050768677
case acc: 0.051029064
case acc: 0.04595177
case acc: 0.044196155
top acc: 0.0569 ::: bot acc: 0.0475
top acc: 0.0845 ::: bot acc: 0.0101
top acc: 0.0499 ::: bot acc: 0.0823
top acc: 0.0842 ::: bot acc: 0.0486
top acc: 0.0509 ::: bot acc: 0.0705
top acc: 0.0827 ::: bot acc: 0.0331
current epoch: 28
train loss is 0.002442
average val loss: 0.002260, accuracy: 0.0531
average test loss: 0.001583, accuracy: 0.0455
case acc: 0.039081573
case acc: 0.042433534
case acc: 0.05031441
case acc: 0.051040936
case acc: 0.0462744
case acc: 0.044007912
top acc: 0.0572 ::: bot acc: 0.0472
top acc: 0.0817 ::: bot acc: 0.0095
top acc: 0.0497 ::: bot acc: 0.0817
top acc: 0.0842 ::: bot acc: 0.0489
top acc: 0.0510 ::: bot acc: 0.0708
top acc: 0.0818 ::: bot acc: 0.0342
current epoch: 29
train loss is 0.002407
average val loss: 0.002260, accuracy: 0.0528
average test loss: 0.001577, accuracy: 0.0455
case acc: 0.039300345
case acc: 0.04165846
case acc: 0.050420575
case acc: 0.051347278
case acc: 0.045844946
case acc: 0.044208296
top acc: 0.0581 ::: bot acc: 0.0461
top acc: 0.0808 ::: bot acc: 0.0093
top acc: 0.0510 ::: bot acc: 0.0805
top acc: 0.0853 ::: bot acc: 0.0478
top acc: 0.0512 ::: bot acc: 0.0696
top acc: 0.0825 ::: bot acc: 0.0338
current epoch: 30
train loss is 0.002377
average val loss: 0.002241, accuracy: 0.0526
average test loss: 0.001565, accuracy: 0.0452
case acc: 0.03962885
case acc: 0.040341865
case acc: 0.050328735
case acc: 0.0511039
case acc: 0.046064015
case acc: 0.04402784
top acc: 0.0589 ::: bot acc: 0.0461
top acc: 0.0790 ::: bot acc: 0.0093
top acc: 0.0511 ::: bot acc: 0.0804
top acc: 0.0851 ::: bot acc: 0.0473
top acc: 0.0513 ::: bot acc: 0.0697
top acc: 0.0820 ::: bot acc: 0.0343
current epoch: 31
train loss is 0.002372
average val loss: 0.002220, accuracy: 0.0523
average test loss: 0.001548, accuracy: 0.0451
case acc: 0.039326876
case acc: 0.039370492
case acc: 0.050424185
case acc: 0.05132063
case acc: 0.04603048
case acc: 0.043847714
top acc: 0.0581 ::: bot acc: 0.0465
top acc: 0.0767 ::: bot acc: 0.0102
top acc: 0.0514 ::: bot acc: 0.0804
top acc: 0.0853 ::: bot acc: 0.0474
top acc: 0.0508 ::: bot acc: 0.0705
top acc: 0.0809 ::: bot acc: 0.0350
current epoch: 32
train loss is 0.002326
average val loss: 0.002198, accuracy: 0.0522
average test loss: 0.001537, accuracy: 0.0448
case acc: 0.039319232
case acc: 0.03799424
case acc: 0.050559524
case acc: 0.051162343
case acc: 0.046193365
case acc: 0.04385883
top acc: 0.0576 ::: bot acc: 0.0468
top acc: 0.0744 ::: bot acc: 0.0110
top acc: 0.0510 ::: bot acc: 0.0810
top acc: 0.0846 ::: bot acc: 0.0481
top acc: 0.0498 ::: bot acc: 0.0717
top acc: 0.0800 ::: bot acc: 0.0363
current epoch: 33
train loss is 0.002299
average val loss: 0.002153, accuracy: 0.0519
average test loss: 0.001519, accuracy: 0.0445
case acc: 0.039034758
case acc: 0.036463022
case acc: 0.050635576
case acc: 0.050581813
case acc: 0.04683803
case acc: 0.04365439
top acc: 0.0558 ::: bot acc: 0.0488
top acc: 0.0713 ::: bot acc: 0.0126
top acc: 0.0490 ::: bot acc: 0.0827
top acc: 0.0832 ::: bot acc: 0.0491
top acc: 0.0477 ::: bot acc: 0.0742
top acc: 0.0786 ::: bot acc: 0.0382
current epoch: 34
train loss is 0.002268
average val loss: 0.002133, accuracy: 0.0518
average test loss: 0.001506, accuracy: 0.0444
case acc: 0.038892686
case acc: 0.03521907
case acc: 0.050933473
case acc: 0.05067764
case acc: 0.047056288
case acc: 0.043339983
top acc: 0.0543 ::: bot acc: 0.0503
top acc: 0.0686 ::: bot acc: 0.0142
top acc: 0.0481 ::: bot acc: 0.0839
top acc: 0.0825 ::: bot acc: 0.0505
top acc: 0.0464 ::: bot acc: 0.0751
top acc: 0.0769 ::: bot acc: 0.0391
current epoch: 35
train loss is 0.002236
average val loss: 0.002137, accuracy: 0.0518
average test loss: 0.001510, accuracy: 0.0444
case acc: 0.03908596
case acc: 0.035362713
case acc: 0.050670322
case acc: 0.050868634
case acc: 0.046887986
case acc: 0.0435868
top acc: 0.0556 ::: bot acc: 0.0494
top acc: 0.0687 ::: bot acc: 0.0148
top acc: 0.0489 ::: bot acc: 0.0827
top acc: 0.0838 ::: bot acc: 0.0488
top acc: 0.0474 ::: bot acc: 0.0741
top acc: 0.0779 ::: bot acc: 0.0387
current epoch: 36
train loss is 0.002243
average val loss: 0.002146, accuracy: 0.0515
average test loss: 0.001514, accuracy: 0.0445
case acc: 0.03926765
case acc: 0.03524399
case acc: 0.05054548
case acc: 0.05132722
case acc: 0.046645887
case acc: 0.04389223
top acc: 0.0569 ::: bot acc: 0.0479
top acc: 0.0689 ::: bot acc: 0.0139
top acc: 0.0506 ::: bot acc: 0.0814
top acc: 0.0854 ::: bot acc: 0.0473
top acc: 0.0485 ::: bot acc: 0.0731
top acc: 0.0792 ::: bot acc: 0.0377
current epoch: 37
train loss is 0.002235
average val loss: 0.002122, accuracy: 0.0514
average test loss: 0.001503, accuracy: 0.0443
case acc: 0.039202526
case acc: 0.034857146
case acc: 0.050591365
case acc: 0.051009025
case acc: 0.046891805
case acc: 0.043427113
top acc: 0.0556 ::: bot acc: 0.0497
top acc: 0.0666 ::: bot acc: 0.0166
top acc: 0.0492 ::: bot acc: 0.0824
top acc: 0.0844 ::: bot acc: 0.0484
top acc: 0.0467 ::: bot acc: 0.0746
top acc: 0.0775 ::: bot acc: 0.0391
current epoch: 38
train loss is 0.002214
average val loss: 0.002084, accuracy: 0.0513
average test loss: 0.001492, accuracy: 0.0441
case acc: 0.03873144
case acc: 0.033653855
case acc: 0.051072445
case acc: 0.050605945
case acc: 0.047556862
case acc: 0.04325205
top acc: 0.0533 ::: bot acc: 0.0511
top acc: 0.0638 ::: bot acc: 0.0191
top acc: 0.0479 ::: bot acc: 0.0843
top acc: 0.0828 ::: bot acc: 0.0501
top acc: 0.0448 ::: bot acc: 0.0770
top acc: 0.0754 ::: bot acc: 0.0411
current epoch: 39
train loss is 0.002184
average val loss: 0.002056, accuracy: 0.0513
average test loss: 0.001481, accuracy: 0.0440
case acc: 0.03867615
case acc: 0.03298715
case acc: 0.05115066
case acc: 0.049991224
case acc: 0.048035912
case acc: 0.043054678
top acc: 0.0515 ::: bot acc: 0.0530
top acc: 0.0615 ::: bot acc: 0.0214
top acc: 0.0463 ::: bot acc: 0.0854
top acc: 0.0806 ::: bot acc: 0.0516
top acc: 0.0430 ::: bot acc: 0.0788
top acc: 0.0731 ::: bot acc: 0.0432
current epoch: 40
train loss is 0.002166
average val loss: 0.002074, accuracy: 0.0512
average test loss: 0.001483, accuracy: 0.0440
case acc: 0.03876752
case acc: 0.033350315
case acc: 0.05099127
case acc: 0.050539356
case acc: 0.04754369
case acc: 0.043052737
top acc: 0.0530 ::: bot acc: 0.0517
top acc: 0.0628 ::: bot acc: 0.0201
top acc: 0.0485 ::: bot acc: 0.0836
top acc: 0.0828 ::: bot acc: 0.0496
top acc: 0.0442 ::: bot acc: 0.0771
top acc: 0.0745 ::: bot acc: 0.0414
current epoch: 41
train loss is 0.002163
average val loss: 0.002076, accuracy: 0.0512
average test loss: 0.001487, accuracy: 0.0441
case acc: 0.038836904
case acc: 0.033422396
case acc: 0.05085474
case acc: 0.050846796
case acc: 0.04760013
case acc: 0.043081194
top acc: 0.0535 ::: bot acc: 0.0512
top acc: 0.0626 ::: bot acc: 0.0207
top acc: 0.0488 ::: bot acc: 0.0833
top acc: 0.0833 ::: bot acc: 0.0496
top acc: 0.0444 ::: bot acc: 0.0771
top acc: 0.0746 ::: bot acc: 0.0416
current epoch: 42
train loss is 0.002161
average val loss: 0.002070, accuracy: 0.0511
average test loss: 0.001485, accuracy: 0.0441
case acc: 0.038925625
case acc: 0.033474512
case acc: 0.050919287
case acc: 0.050744478
case acc: 0.047409657
case acc: 0.04307678
top acc: 0.0533 ::: bot acc: 0.0517
top acc: 0.0621 ::: bot acc: 0.0215
top acc: 0.0489 ::: bot acc: 0.0836
top acc: 0.0834 ::: bot acc: 0.0494
top acc: 0.0444 ::: bot acc: 0.0770
top acc: 0.0744 ::: bot acc: 0.0418
current epoch: 43
train loss is 0.002152
average val loss: 0.002067, accuracy: 0.0511
average test loss: 0.001480, accuracy: 0.0440
case acc: 0.038803406
case acc: 0.033087596
case acc: 0.050691176
case acc: 0.05075958
case acc: 0.047600973
case acc: 0.04319587
top acc: 0.0528 ::: bot acc: 0.0517
top acc: 0.0612 ::: bot acc: 0.0221
top acc: 0.0483 ::: bot acc: 0.0834
top acc: 0.0831 ::: bot acc: 0.0496
top acc: 0.0446 ::: bot acc: 0.0773
top acc: 0.0738 ::: bot acc: 0.0426
current epoch: 44
train loss is 0.002135
average val loss: 0.002046, accuracy: 0.0511
average test loss: 0.001478, accuracy: 0.0440
case acc: 0.038768206
case acc: 0.032606684
case acc: 0.051238798
case acc: 0.050352406
case acc: 0.047920875
case acc: 0.043198187
top acc: 0.0511 ::: bot acc: 0.0536
top acc: 0.0592 ::: bot acc: 0.0239
top acc: 0.0473 ::: bot acc: 0.0850
top acc: 0.0818 ::: bot acc: 0.0507
top acc: 0.0432 ::: bot acc: 0.0784
top acc: 0.0724 ::: bot acc: 0.0445
current epoch: 45
train loss is 0.002137
average val loss: 0.002049, accuracy: 0.0511
average test loss: 0.001472, accuracy: 0.0439
case acc: 0.03872279
case acc: 0.03251875
case acc: 0.051032115
case acc: 0.050239354
case acc: 0.047843758
case acc: 0.04308537
top acc: 0.0511 ::: bot acc: 0.0538
top acc: 0.0590 ::: bot acc: 0.0243
top acc: 0.0474 ::: bot acc: 0.0846
top acc: 0.0819 ::: bot acc: 0.0502
top acc: 0.0435 ::: bot acc: 0.0781
top acc: 0.0722 ::: bot acc: 0.0442
current epoch: 46
train loss is 0.002127
average val loss: 0.002042, accuracy: 0.0510
average test loss: 0.001474, accuracy: 0.0439
case acc: 0.03883244
case acc: 0.032301467
case acc: 0.051143683
case acc: 0.05048383
case acc: 0.04792928
case acc: 0.04288961
top acc: 0.0511 ::: bot acc: 0.0539
top acc: 0.0584 ::: bot acc: 0.0247
top acc: 0.0471 ::: bot acc: 0.0850
top acc: 0.0823 ::: bot acc: 0.0506
top acc: 0.0434 ::: bot acc: 0.0781
top acc: 0.0722 ::: bot acc: 0.0439
current epoch: 47
train loss is 0.002118
average val loss: 0.002043, accuracy: 0.0511
average test loss: 0.001473, accuracy: 0.0439
case acc: 0.038741566
case acc: 0.032219473
case acc: 0.051135555
case acc: 0.050415516
case acc: 0.047855947
case acc: 0.04304415
top acc: 0.0506 ::: bot acc: 0.0542
top acc: 0.0579 ::: bot acc: 0.0255
top acc: 0.0469 ::: bot acc: 0.0852
top acc: 0.0818 ::: bot acc: 0.0507
top acc: 0.0433 ::: bot acc: 0.0784
top acc: 0.0720 ::: bot acc: 0.0443
current epoch: 48
train loss is 0.002112
average val loss: 0.002020, accuracy: 0.0509
average test loss: 0.001475, accuracy: 0.0439
case acc: 0.03886594
case acc: 0.03182991
case acc: 0.05153137
case acc: 0.05012682
case acc: 0.048214763
case acc: 0.04299487
top acc: 0.0490 ::: bot acc: 0.0560
top acc: 0.0560 ::: bot acc: 0.0273
top acc: 0.0456 ::: bot acc: 0.0868
top acc: 0.0809 ::: bot acc: 0.0518
top acc: 0.0420 ::: bot acc: 0.0796
top acc: 0.0707 ::: bot acc: 0.0458
current epoch: 49
train loss is 0.002098
average val loss: 0.002006, accuracy: 0.0510
average test loss: 0.001477, accuracy: 0.0439
case acc: 0.038994066
case acc: 0.03154416
case acc: 0.051745664
case acc: 0.049909774
case acc: 0.048495397
case acc: 0.042867992
top acc: 0.0475 ::: bot acc: 0.0580
top acc: 0.0544 ::: bot acc: 0.0291
top acc: 0.0440 ::: bot acc: 0.0878
top acc: 0.0798 ::: bot acc: 0.0528
top acc: 0.0408 ::: bot acc: 0.0807
top acc: 0.0694 ::: bot acc: 0.0467
current epoch: 50
train loss is 0.002098
average val loss: 0.002005, accuracy: 0.0510
average test loss: 0.001481, accuracy: 0.0440
case acc: 0.039024435
case acc: 0.031389266
case acc: 0.051644135
case acc: 0.050012715
case acc: 0.048514057
case acc: 0.04316345
top acc: 0.0479 ::: bot acc: 0.0575
top acc: 0.0542 ::: bot acc: 0.0289
top acc: 0.0441 ::: bot acc: 0.0879
top acc: 0.0805 ::: bot acc: 0.0522
top acc: 0.0413 ::: bot acc: 0.0806
top acc: 0.0703 ::: bot acc: 0.0464
LME_Co_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5358 5358 5358
1.7082474 -0.6288155 0.2588177 -0.21218425
Validation: 600 600 600
Testing: 750 750 750
pre-processing time: 0.00020003318786621094
the split date is 2012-01-01
train dropout: 0.4 test dropout: 0.15
net initializing with time: 0.002248525619506836
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.011003
average val loss: 0.005942, accuracy: 0.0934
average test loss: 0.004161, accuracy: 0.0714
case acc: 0.07526745
case acc: 0.15593526
case acc: 0.06680503
case acc: 0.046020485
case acc: 0.038569972
case acc: 0.045847885
top acc: 0.0261 ::: bot acc: 0.1298
top acc: 0.2076 ::: bot acc: 0.0945
top acc: 0.0827 ::: bot acc: 0.0979
top acc: 0.0718 ::: bot acc: 0.0575
top acc: 0.0398 ::: bot acc: 0.0639
top acc: 0.0780 ::: bot acc: 0.0425
current epoch: 2
train loss is 0.009249
average val loss: 0.007617, accuracy: 0.0953
average test loss: 0.012369, accuracy: 0.1335
case acc: 0.058269475
case acc: 0.26766253
case acc: 0.11611388
case acc: 0.12602116
case acc: 0.10322272
case acc: 0.12960298
top acc: 0.1034 ::: bot acc: 0.0175
top acc: 0.3186 ::: bot acc: 0.2069
top acc: 0.2062 ::: bot acc: 0.0360
top acc: 0.1903 ::: bot acc: 0.0646
top acc: 0.1568 ::: bot acc: 0.0522
top acc: 0.1902 ::: bot acc: 0.0698
current epoch: 3
train loss is 0.011098
average val loss: 0.011170, accuracy: 0.1218
average test loss: 0.018214, accuracy: 0.1725
case acc: 0.098603636
case acc: 0.30054978
case acc: 0.1592085
case acc: 0.16564943
case acc: 0.14557214
case acc: 0.16539755
top acc: 0.1495 ::: bot acc: 0.0440
top acc: 0.3524 ::: bot acc: 0.2396
top acc: 0.2519 ::: bot acc: 0.0724
top acc: 0.2311 ::: bot acc: 0.1020
top acc: 0.1985 ::: bot acc: 0.0946
top acc: 0.2252 ::: bot acc: 0.1063
current epoch: 4
train loss is 0.013123
average val loss: 0.004009, accuracy: 0.0727
average test loss: 0.004876, accuracy: 0.0765
case acc: 0.039576665
case acc: 0.17534202
case acc: 0.072533265
case acc: 0.06358109
case acc: 0.049238134
case acc: 0.058793522
top acc: 0.0392 ::: bot acc: 0.0675
top acc: 0.2263 ::: bot acc: 0.1145
top acc: 0.1379 ::: bot acc: 0.0426
top acc: 0.1158 ::: bot acc: 0.0272
top acc: 0.0909 ::: bot acc: 0.0217
top acc: 0.1086 ::: bot acc: 0.0232
current epoch: 5
train loss is 0.007920
average val loss: 0.004714, accuracy: 0.0823
average test loss: 0.003034, accuracy: 0.0620
case acc: 0.062481795
case acc: 0.11576195
case acc: 0.06634336
case acc: 0.045614418
case acc: 0.037446786
case acc: 0.044194024
top acc: 0.0206 ::: bot acc: 0.1131
top acc: 0.1669 ::: bot acc: 0.0548
top acc: 0.0863 ::: bot acc: 0.0937
top acc: 0.0667 ::: bot acc: 0.0625
top acc: 0.0516 ::: bot acc: 0.0518
top acc: 0.0594 ::: bot acc: 0.0617
current epoch: 6
train loss is 0.004577
average val loss: 0.003365, accuracy: 0.0673
average test loss: 0.004134, accuracy: 0.0720
case acc: 0.040056016
case acc: 0.15019423
case acc: 0.06891323
case acc: 0.05972979
case acc: 0.05716416
case acc: 0.055994652
top acc: 0.0410 ::: bot acc: 0.0664
top acc: 0.2017 ::: bot acc: 0.0898
top acc: 0.1277 ::: bot acc: 0.0508
top acc: 0.1096 ::: bot acc: 0.0277
top acc: 0.1036 ::: bot acc: 0.0196
top acc: 0.1032 ::: bot acc: 0.0244
current epoch: 7
train loss is 0.004788
average val loss: 0.003186, accuracy: 0.0649
average test loss: 0.004748, accuracy: 0.0786
case acc: 0.039979164
case acc: 0.15585639
case acc: 0.07284791
case acc: 0.06838117
case acc: 0.07092992
case acc: 0.063896716
top acc: 0.0574 ::: bot acc: 0.0497
top acc: 0.2062 ::: bot acc: 0.0955
top acc: 0.1394 ::: bot acc: 0.0409
top acc: 0.1228 ::: bot acc: 0.0276
top acc: 0.1221 ::: bot acc: 0.0254
top acc: 0.1168 ::: bot acc: 0.0220
current epoch: 8
train loss is 0.004787
average val loss: 0.002971, accuracy: 0.0635
average test loss: 0.003548, accuracy: 0.0675
case acc: 0.040072188
case acc: 0.12867141
case acc: 0.06750352
case acc: 0.056251787
case acc: 0.060051754
case acc: 0.052412894
top acc: 0.0425 ::: bot acc: 0.0657
top acc: 0.1793 ::: bot acc: 0.0684
top acc: 0.1199 ::: bot acc: 0.0602
top acc: 0.1036 ::: bot acc: 0.0297
top acc: 0.1076 ::: bot acc: 0.0211
top acc: 0.0961 ::: bot acc: 0.0277
current epoch: 9
train loss is 0.004078
average val loss: 0.002807, accuracy: 0.0617
average test loss: 0.003425, accuracy: 0.0666
case acc: 0.039058376
case acc: 0.12100406
case acc: 0.06705326
case acc: 0.056519054
case acc: 0.06350777
case acc: 0.052664783
top acc: 0.0441 ::: bot acc: 0.0627
top acc: 0.1718 ::: bot acc: 0.0613
top acc: 0.1194 ::: bot acc: 0.0604
top acc: 0.1050 ::: bot acc: 0.0299
top acc: 0.1119 ::: bot acc: 0.0219
top acc: 0.0959 ::: bot acc: 0.0284
current epoch: 10
train loss is 0.003682
average val loss: 0.002608, accuracy: 0.0594
average test loss: 0.003629, accuracy: 0.0693
case acc: 0.039431706
case acc: 0.12022212
case acc: 0.06827775
case acc: 0.060641848
case acc: 0.07118363
case acc: 0.05594516
top acc: 0.0540 ::: bot acc: 0.0531
top acc: 0.1713 ::: bot acc: 0.0598
top acc: 0.1248 ::: bot acc: 0.0549
top acc: 0.1119 ::: bot acc: 0.0277
top acc: 0.1217 ::: bot acc: 0.0258
top acc: 0.1027 ::: bot acc: 0.0253
current epoch: 11
train loss is 0.003617
average val loss: 0.002485, accuracy: 0.0580
average test loss: 0.003693, accuracy: 0.0703
case acc: 0.039827622
case acc: 0.11694128
case acc: 0.06906832
case acc: 0.06282044
case acc: 0.07590953
case acc: 0.05720932
top acc: 0.0588 ::: bot acc: 0.0473
top acc: 0.1678 ::: bot acc: 0.0575
top acc: 0.1276 ::: bot acc: 0.0523
top acc: 0.1156 ::: bot acc: 0.0271
top acc: 0.1273 ::: bot acc: 0.0288
top acc: 0.1053 ::: bot acc: 0.0236
current epoch: 12
train loss is 0.003358
average val loss: 0.002374, accuracy: 0.0566
average test loss: 0.003456, accuracy: 0.0681
case acc: 0.040072158
case acc: 0.10777637
case acc: 0.06816156
case acc: 0.06101083
case acc: 0.07540445
case acc: 0.055910908
top acc: 0.0594 ::: bot acc: 0.0481
top acc: 0.1586 ::: bot acc: 0.0480
top acc: 0.1246 ::: bot acc: 0.0554
top acc: 0.1129 ::: bot acc: 0.0270
top acc: 0.1266 ::: bot acc: 0.0284
top acc: 0.1021 ::: bot acc: 0.0252
current epoch: 13
train loss is 0.003141
average val loss: 0.002281, accuracy: 0.0553
average test loss: 0.003402, accuracy: 0.0677
case acc: 0.040652815
case acc: 0.1022693
case acc: 0.06828482
case acc: 0.062016316
case acc: 0.07717286
case acc: 0.056040872
top acc: 0.0624 ::: bot acc: 0.0449
top acc: 0.1529 ::: bot acc: 0.0427
top acc: 0.1250 ::: bot acc: 0.0552
top acc: 0.1140 ::: bot acc: 0.0279
top acc: 0.1284 ::: bot acc: 0.0299
top acc: 0.1029 ::: bot acc: 0.0252
current epoch: 14
train loss is 0.002992
average val loss: 0.002200, accuracy: 0.0542
average test loss: 0.003352, accuracy: 0.0673
case acc: 0.04121531
case acc: 0.09715885
case acc: 0.06855137
case acc: 0.06238048
case acc: 0.07809252
case acc: 0.05613679
top acc: 0.0641 ::: bot acc: 0.0426
top acc: 0.1485 ::: bot acc: 0.0370
top acc: 0.1257 ::: bot acc: 0.0547
top acc: 0.1150 ::: bot acc: 0.0271
top acc: 0.1298 ::: bot acc: 0.0302
top acc: 0.1032 ::: bot acc: 0.0252
current epoch: 15
train loss is 0.002854
average val loss: 0.002126, accuracy: 0.0532
average test loss: 0.003314, accuracy: 0.0670
case acc: 0.04196562
case acc: 0.09265932
case acc: 0.06889144
case acc: 0.06287753
case acc: 0.07923787
case acc: 0.056385893
top acc: 0.0670 ::: bot acc: 0.0399
top acc: 0.1433 ::: bot acc: 0.0330
top acc: 0.1269 ::: bot acc: 0.0536
top acc: 0.1155 ::: bot acc: 0.0269
top acc: 0.1309 ::: bot acc: 0.0311
top acc: 0.1031 ::: bot acc: 0.0251
current epoch: 16
train loss is 0.002710
average val loss: 0.002069, accuracy: 0.0523
average test loss: 0.003177, accuracy: 0.0655
case acc: 0.04234757
case acc: 0.086493716
case acc: 0.06813348
case acc: 0.06276518
case acc: 0.077776395
case acc: 0.05537936
top acc: 0.0676 ::: bot acc: 0.0397
top acc: 0.1371 ::: bot acc: 0.0269
top acc: 0.1251 ::: bot acc: 0.0539
top acc: 0.1150 ::: bot acc: 0.0276
top acc: 0.1290 ::: bot acc: 0.0303
top acc: 0.1014 ::: bot acc: 0.0257
current epoch: 17
train loss is 0.002628
average val loss: 0.002007, accuracy: 0.0517
average test loss: 0.003321, accuracy: 0.0672
case acc: 0.043641176
case acc: 0.08608542
case acc: 0.06964038
case acc: 0.06550676
case acc: 0.08103965
case acc: 0.05729011
top acc: 0.0732 ::: bot acc: 0.0336
top acc: 0.1369 ::: bot acc: 0.0264
top acc: 0.1299 ::: bot acc: 0.0502
top acc: 0.1188 ::: bot acc: 0.0273
top acc: 0.1330 ::: bot acc: 0.0323
top acc: 0.1051 ::: bot acc: 0.0249
current epoch: 18
train loss is 0.002565
average val loss: 0.001942, accuracy: 0.0510
average test loss: 0.003379, accuracy: 0.0679
case acc: 0.045421872
case acc: 0.08402939
case acc: 0.070526704
case acc: 0.067195065
case acc: 0.08215696
case acc: 0.057801608
top acc: 0.0777 ::: bot acc: 0.0299
top acc: 0.1347 ::: bot acc: 0.0247
top acc: 0.1330 ::: bot acc: 0.0473
top acc: 0.1216 ::: bot acc: 0.0270
top acc: 0.1345 ::: bot acc: 0.0334
top acc: 0.1064 ::: bot acc: 0.0237
current epoch: 19
train loss is 0.002513
average val loss: 0.001909, accuracy: 0.0506
average test loss: 0.003439, accuracy: 0.0686
case acc: 0.046567075
case acc: 0.082407944
case acc: 0.071392894
case acc: 0.06889289
case acc: 0.08352503
case acc: 0.058607537
top acc: 0.0807 ::: bot acc: 0.0271
top acc: 0.1329 ::: bot acc: 0.0232
top acc: 0.1355 ::: bot acc: 0.0438
top acc: 0.1241 ::: bot acc: 0.0274
top acc: 0.1358 ::: bot acc: 0.0345
top acc: 0.1075 ::: bot acc: 0.0236
current epoch: 20
train loss is 0.002467
average val loss: 0.001871, accuracy: 0.0502
average test loss: 0.003449, accuracy: 0.0687
case acc: 0.0477025
case acc: 0.0802539
case acc: 0.072197475
case acc: 0.06997078
case acc: 0.08327126
case acc: 0.058597054
top acc: 0.0834 ::: bot acc: 0.0249
top acc: 0.1300 ::: bot acc: 0.0222
top acc: 0.1378 ::: bot acc: 0.0422
top acc: 0.1257 ::: bot acc: 0.0275
top acc: 0.1356 ::: bot acc: 0.0339
top acc: 0.1079 ::: bot acc: 0.0236
current epoch: 21
train loss is 0.002376
average val loss: 0.001835, accuracy: 0.0497
average test loss: 0.003377, accuracy: 0.0679
case acc: 0.04800251
case acc: 0.07711002
case acc: 0.072377875
case acc: 0.07018479
case acc: 0.081923805
case acc: 0.057908885
top acc: 0.0845 ::: bot acc: 0.0239
top acc: 0.1266 ::: bot acc: 0.0201
top acc: 0.1381 ::: bot acc: 0.0418
top acc: 0.1254 ::: bot acc: 0.0282
top acc: 0.1338 ::: bot acc: 0.0331
top acc: 0.1064 ::: bot acc: 0.0240
current epoch: 22
train loss is 0.002320
average val loss: 0.001814, accuracy: 0.0494
average test loss: 0.003353, accuracy: 0.0676
case acc: 0.04817175
case acc: 0.07476528
case acc: 0.07266938
case acc: 0.070582084
case acc: 0.081826165
case acc: 0.05761927
top acc: 0.0858 ::: bot acc: 0.0221
top acc: 0.1238 ::: bot acc: 0.0190
top acc: 0.1391 ::: bot acc: 0.0404
top acc: 0.1260 ::: bot acc: 0.0280
top acc: 0.1338 ::: bot acc: 0.0332
top acc: 0.1057 ::: bot acc: 0.0238
current epoch: 23
train loss is 0.002263
average val loss: 0.001788, accuracy: 0.0492
average test loss: 0.003453, accuracy: 0.0688
case acc: 0.050230797
case acc: 0.07442346
case acc: 0.074304834
case acc: 0.07226214
case acc: 0.08298233
case acc: 0.058387734
top acc: 0.0895 ::: bot acc: 0.0202
top acc: 0.1232 ::: bot acc: 0.0186
top acc: 0.1430 ::: bot acc: 0.0383
top acc: 0.1282 ::: bot acc: 0.0282
top acc: 0.1351 ::: bot acc: 0.0340
top acc: 0.1069 ::: bot acc: 0.0237
current epoch: 24
train loss is 0.002247
average val loss: 0.001779, accuracy: 0.0492
average test loss: 0.003540, accuracy: 0.0698
case acc: 0.05189937
case acc: 0.073960856
case acc: 0.07561558
case acc: 0.074132994
case acc: 0.08383464
case acc: 0.059112187
top acc: 0.0927 ::: bot acc: 0.0192
top acc: 0.1227 ::: bot acc: 0.0189
top acc: 0.1457 ::: bot acc: 0.0362
top acc: 0.1310 ::: bot acc: 0.0287
top acc: 0.1359 ::: bot acc: 0.0351
top acc: 0.1083 ::: bot acc: 0.0237
current epoch: 25
train loss is 0.002212
average val loss: 0.001753, accuracy: 0.0489
average test loss: 0.003536, accuracy: 0.0697
case acc: 0.052460372
case acc: 0.072333805
case acc: 0.0761933
case acc: 0.07479306
case acc: 0.083530776
case acc: 0.058822162
top acc: 0.0934 ::: bot acc: 0.0186
top acc: 0.1203 ::: bot acc: 0.0180
top acc: 0.1471 ::: bot acc: 0.0354
top acc: 0.1318 ::: bot acc: 0.0290
top acc: 0.1360 ::: bot acc: 0.0342
top acc: 0.1082 ::: bot acc: 0.0234
current epoch: 26
train loss is 0.002190
average val loss: 0.001753, accuracy: 0.0490
average test loss: 0.003637, accuracy: 0.0709
case acc: 0.05441797
case acc: 0.07194814
case acc: 0.07758406
case acc: 0.076439224
case acc: 0.08511278
case acc: 0.05970593
top acc: 0.0968 ::: bot acc: 0.0182
top acc: 0.1202 ::: bot acc: 0.0178
top acc: 0.1499 ::: bot acc: 0.0341
top acc: 0.1340 ::: bot acc: 0.0294
top acc: 0.1374 ::: bot acc: 0.0359
top acc: 0.1093 ::: bot acc: 0.0232
current epoch: 27
train loss is 0.002200
average val loss: 0.001742, accuracy: 0.0489
average test loss: 0.003725, accuracy: 0.0718
case acc: 0.05581323
case acc: 0.07159138
case acc: 0.07879857
case acc: 0.07825139
case acc: 0.08629448
case acc: 0.060157157
top acc: 0.0991 ::: bot acc: 0.0174
top acc: 0.1196 ::: bot acc: 0.0179
top acc: 0.1523 ::: bot acc: 0.0326
top acc: 0.1360 ::: bot acc: 0.0302
top acc: 0.1386 ::: bot acc: 0.0364
top acc: 0.1100 ::: bot acc: 0.0229
current epoch: 28
train loss is 0.002170
average val loss: 0.001741, accuracy: 0.0491
average test loss: 0.003809, accuracy: 0.0727
case acc: 0.05719995
case acc: 0.07135354
case acc: 0.0799546
case acc: 0.07948372
case acc: 0.08753436
case acc: 0.06064387
top acc: 0.1012 ::: bot acc: 0.0173
top acc: 0.1192 ::: bot acc: 0.0176
top acc: 0.1546 ::: bot acc: 0.0315
top acc: 0.1382 ::: bot acc: 0.0302
top acc: 0.1397 ::: bot acc: 0.0380
top acc: 0.1112 ::: bot acc: 0.0224
current epoch: 29
train loss is 0.002160
average val loss: 0.001729, accuracy: 0.0489
average test loss: 0.003797, accuracy: 0.0726
case acc: 0.05777052
case acc: 0.07005849
case acc: 0.08045019
case acc: 0.07961557
case acc: 0.08746604
case acc: 0.060416985
top acc: 0.1020 ::: bot acc: 0.0174
top acc: 0.1174 ::: bot acc: 0.0177
top acc: 0.1552 ::: bot acc: 0.0316
top acc: 0.1384 ::: bot acc: 0.0303
top acc: 0.1398 ::: bot acc: 0.0376
top acc: 0.1106 ::: bot acc: 0.0227
current epoch: 30
train loss is 0.002124
average val loss: 0.001708, accuracy: 0.0485
average test loss: 0.003694, accuracy: 0.0715
case acc: 0.05704809
case acc: 0.06768642
case acc: 0.079866014
case acc: 0.078865
case acc: 0.08594492
case acc: 0.059386514
top acc: 0.1012 ::: bot acc: 0.0174
top acc: 0.1141 ::: bot acc: 0.0176
top acc: 0.1540 ::: bot acc: 0.0320
top acc: 0.1374 ::: bot acc: 0.0302
top acc: 0.1385 ::: bot acc: 0.0359
top acc: 0.1089 ::: bot acc: 0.0232
current epoch: 31
train loss is 0.002067
average val loss: 0.001685, accuracy: 0.0482
average test loss: 0.003638, accuracy: 0.0709
case acc: 0.05684758
case acc: 0.065992296
case acc: 0.07978275
case acc: 0.07847711
case acc: 0.08556664
case acc: 0.058751848
top acc: 0.1007 ::: bot acc: 0.0176
top acc: 0.1109 ::: bot acc: 0.0182
top acc: 0.1542 ::: bot acc: 0.0321
top acc: 0.1366 ::: bot acc: 0.0303
top acc: 0.1379 ::: bot acc: 0.0361
top acc: 0.1080 ::: bot acc: 0.0235
current epoch: 32
train loss is 0.002048
average val loss: 0.001668, accuracy: 0.0479
average test loss: 0.003524, accuracy: 0.0696
case acc: 0.055810306
case acc: 0.063738085
case acc: 0.0790422
case acc: 0.077327244
case acc: 0.083958976
case acc: 0.057610955
top acc: 0.0991 ::: bot acc: 0.0179
top acc: 0.1077 ::: bot acc: 0.0178
top acc: 0.1527 ::: bot acc: 0.0325
top acc: 0.1352 ::: bot acc: 0.0295
top acc: 0.1360 ::: bot acc: 0.0347
top acc: 0.1060 ::: bot acc: 0.0240
current epoch: 33
train loss is 0.002022
average val loss: 0.001648, accuracy: 0.0476
average test loss: 0.003389, accuracy: 0.0681
case acc: 0.054767232
case acc: 0.061718315
case acc: 0.07805667
case acc: 0.0761458
case acc: 0.08185199
case acc: 0.056146182
top acc: 0.0972 ::: bot acc: 0.0184
top acc: 0.1043 ::: bot acc: 0.0189
top acc: 0.1508 ::: bot acc: 0.0335
top acc: 0.1336 ::: bot acc: 0.0292
top acc: 0.1336 ::: bot acc: 0.0332
top acc: 0.1030 ::: bot acc: 0.0250
current epoch: 34
train loss is 0.001979
average val loss: 0.001648, accuracy: 0.0476
average test loss: 0.003420, accuracy: 0.0684
case acc: 0.05508972
case acc: 0.06136011
case acc: 0.07856247
case acc: 0.07671538
case acc: 0.08218387
case acc: 0.05650091
top acc: 0.0980 ::: bot acc: 0.0177
top acc: 0.1038 ::: bot acc: 0.0190
top acc: 0.1519 ::: bot acc: 0.0325
top acc: 0.1342 ::: bot acc: 0.0295
top acc: 0.1341 ::: bot acc: 0.0335
top acc: 0.1040 ::: bot acc: 0.0247
current epoch: 35
train loss is 0.001968
average val loss: 0.001644, accuracy: 0.0477
average test loss: 0.003491, accuracy: 0.0692
case acc: 0.05601281
case acc: 0.06143446
case acc: 0.079510316
case acc: 0.07821288
case acc: 0.0830009
case acc: 0.05704798
top acc: 0.0994 ::: bot acc: 0.0178
top acc: 0.1041 ::: bot acc: 0.0185
top acc: 0.1537 ::: bot acc: 0.0321
top acc: 0.1360 ::: bot acc: 0.0302
top acc: 0.1350 ::: bot acc: 0.0343
top acc: 0.1049 ::: bot acc: 0.0244
current epoch: 36
train loss is 0.001959
average val loss: 0.001636, accuracy: 0.0475
average test loss: 0.003423, accuracy: 0.0685
case acc: 0.05535632
case acc: 0.060354512
case acc: 0.07927254
case acc: 0.07754728
case acc: 0.08184879
case acc: 0.0563353
top acc: 0.0985 ::: bot acc: 0.0177
top acc: 0.1022 ::: bot acc: 0.0194
top acc: 0.1530 ::: bot acc: 0.0325
top acc: 0.1351 ::: bot acc: 0.0299
top acc: 0.1335 ::: bot acc: 0.0334
top acc: 0.1035 ::: bot acc: 0.0249
current epoch: 37
train loss is 0.001951
average val loss: 0.001637, accuracy: 0.0476
average test loss: 0.003445, accuracy: 0.0687
case acc: 0.055666693
case acc: 0.060169354
case acc: 0.07951612
case acc: 0.07822817
case acc: 0.0821252
case acc: 0.056562744
top acc: 0.0989 ::: bot acc: 0.0177
top acc: 0.1018 ::: bot acc: 0.0195
top acc: 0.1537 ::: bot acc: 0.0319
top acc: 0.1360 ::: bot acc: 0.0303
top acc: 0.1338 ::: bot acc: 0.0334
top acc: 0.1038 ::: bot acc: 0.0249
current epoch: 38
train loss is 0.001940
average val loss: 0.001645, accuracy: 0.0478
average test loss: 0.003501, accuracy: 0.0693
case acc: 0.056268502
case acc: 0.060014658
case acc: 0.0805638
case acc: 0.0791807
case acc: 0.082749344
case acc: 0.056832213
top acc: 0.0997 ::: bot acc: 0.0176
top acc: 0.1018 ::: bot acc: 0.0191
top acc: 0.1555 ::: bot acc: 0.0316
top acc: 0.1374 ::: bot acc: 0.0305
top acc: 0.1346 ::: bot acc: 0.0338
top acc: 0.1047 ::: bot acc: 0.0242
current epoch: 39
train loss is 0.001957
average val loss: 0.001632, accuracy: 0.0475
average test loss: 0.003425, accuracy: 0.0684
case acc: 0.055649463
case acc: 0.05908163
case acc: 0.080041036
case acc: 0.07817853
case acc: 0.08157848
case acc: 0.056037385
top acc: 0.0986 ::: bot acc: 0.0180
top acc: 0.0999 ::: bot acc: 0.0200
top acc: 0.1544 ::: bot acc: 0.0318
top acc: 0.1361 ::: bot acc: 0.0301
top acc: 0.1334 ::: bot acc: 0.0329
top acc: 0.1032 ::: bot acc: 0.0249
current epoch: 40
train loss is 0.001920
average val loss: 0.001625, accuracy: 0.0475
average test loss: 0.003441, accuracy: 0.0686
case acc: 0.05587349
case acc: 0.05887737
case acc: 0.080506146
case acc: 0.07859174
case acc: 0.08164739
case acc: 0.05596695
top acc: 0.0992 ::: bot acc: 0.0178
top acc: 0.0995 ::: bot acc: 0.0200
top acc: 0.1552 ::: bot acc: 0.0315
top acc: 0.1367 ::: bot acc: 0.0303
top acc: 0.1336 ::: bot acc: 0.0330
top acc: 0.1031 ::: bot acc: 0.0248
current epoch: 41
train loss is 0.001933
average val loss: 0.001630, accuracy: 0.0477
average test loss: 0.003544, accuracy: 0.0697
case acc: 0.05688556
case acc: 0.059430916
case acc: 0.08157014
case acc: 0.08020817
case acc: 0.083296515
case acc: 0.0568509
top acc: 0.1008 ::: bot acc: 0.0175
top acc: 0.1004 ::: bot acc: 0.0196
top acc: 0.1573 ::: bot acc: 0.0307
top acc: 0.1388 ::: bot acc: 0.0307
top acc: 0.1353 ::: bot acc: 0.0340
top acc: 0.1048 ::: bot acc: 0.0241
current epoch: 42
train loss is 0.001916
average val loss: 0.001630, accuracy: 0.0476
average test loss: 0.003503, accuracy: 0.0693
case acc: 0.05647741
case acc: 0.05875494
case acc: 0.0818566
case acc: 0.07982773
case acc: 0.082502626
case acc: 0.056258842
top acc: 0.0999 ::: bot acc: 0.0178
top acc: 0.0994 ::: bot acc: 0.0199
top acc: 0.1575 ::: bot acc: 0.0311
top acc: 0.1382 ::: bot acc: 0.0307
top acc: 0.1345 ::: bot acc: 0.0337
top acc: 0.1035 ::: bot acc: 0.0247
current epoch: 43
train loss is 0.001914
average val loss: 0.001622, accuracy: 0.0475
average test loss: 0.003524, accuracy: 0.0694
case acc: 0.056529734
case acc: 0.05879076
case acc: 0.08198091
case acc: 0.08010124
case acc: 0.082840644
case acc: 0.05633731
top acc: 0.1002 ::: bot acc: 0.0177
top acc: 0.0996 ::: bot acc: 0.0200
top acc: 0.1579 ::: bot acc: 0.0307
top acc: 0.1390 ::: bot acc: 0.0306
top acc: 0.1347 ::: bot acc: 0.0342
top acc: 0.1037 ::: bot acc: 0.0245
current epoch: 44
train loss is 0.001903
average val loss: 0.001624, accuracy: 0.0476
average test loss: 0.003566, accuracy: 0.0699
case acc: 0.05688804
case acc: 0.058853876
case acc: 0.082435146
case acc: 0.08083798
case acc: 0.08376729
case acc: 0.05666614
top acc: 0.1010 ::: bot acc: 0.0175
top acc: 0.0997 ::: bot acc: 0.0199
top acc: 0.1587 ::: bot acc: 0.0301
top acc: 0.1394 ::: bot acc: 0.0312
top acc: 0.1359 ::: bot acc: 0.0345
top acc: 0.1041 ::: bot acc: 0.0246
current epoch: 45
train loss is 0.001909
average val loss: 0.001618, accuracy: 0.0475
average test loss: 0.003533, accuracy: 0.0695
case acc: 0.05632398
case acc: 0.058378294
case acc: 0.08216958
case acc: 0.080439195
case acc: 0.08360659
case acc: 0.056302533
top acc: 0.0999 ::: bot acc: 0.0177
top acc: 0.0987 ::: bot acc: 0.0203
top acc: 0.1582 ::: bot acc: 0.0303
top acc: 0.1392 ::: bot acc: 0.0309
top acc: 0.1356 ::: bot acc: 0.0344
top acc: 0.1038 ::: bot acc: 0.0246
current epoch: 46
train loss is 0.001887
average val loss: 0.001612, accuracy: 0.0473
average test loss: 0.003476, accuracy: 0.0689
case acc: 0.055700608
case acc: 0.057349794
case acc: 0.081604905
case acc: 0.07996675
case acc: 0.08295259
case acc: 0.055691533
top acc: 0.0988 ::: bot acc: 0.0180
top acc: 0.0970 ::: bot acc: 0.0207
top acc: 0.1575 ::: bot acc: 0.0305
top acc: 0.1385 ::: bot acc: 0.0308
top acc: 0.1348 ::: bot acc: 0.0340
top acc: 0.1026 ::: bot acc: 0.0250
current epoch: 47
train loss is 0.001877
average val loss: 0.001613, accuracy: 0.0474
average test loss: 0.003485, accuracy: 0.0690
case acc: 0.055674255
case acc: 0.057178147
case acc: 0.08186537
case acc: 0.08004038
case acc: 0.0831444
case acc: 0.056003038
top acc: 0.0988 ::: bot acc: 0.0177
top acc: 0.0966 ::: bot acc: 0.0210
top acc: 0.1579 ::: bot acc: 0.0305
top acc: 0.1385 ::: bot acc: 0.0310
top acc: 0.1352 ::: bot acc: 0.0342
top acc: 0.1028 ::: bot acc: 0.0250
current epoch: 48
train loss is 0.001872
average val loss: 0.001608, accuracy: 0.0472
average test loss: 0.003374, accuracy: 0.0677
case acc: 0.054238
case acc: 0.055753194
case acc: 0.08077865
case acc: 0.0788148
case acc: 0.08159342
case acc: 0.055209227
top acc: 0.0964 ::: bot acc: 0.0183
top acc: 0.0943 ::: bot acc: 0.0218
top acc: 0.1558 ::: bot acc: 0.0315
top acc: 0.1370 ::: bot acc: 0.0303
top acc: 0.1334 ::: bot acc: 0.0332
top acc: 0.1015 ::: bot acc: 0.0259
current epoch: 49
train loss is 0.001852
average val loss: 0.001600, accuracy: 0.0471
average test loss: 0.003366, accuracy: 0.0676
case acc: 0.053942557
case acc: 0.05531094
case acc: 0.08037948
case acc: 0.07892828
case acc: 0.08184528
case acc: 0.055267394
top acc: 0.0962 ::: bot acc: 0.0182
top acc: 0.0934 ::: bot acc: 0.0221
top acc: 0.1554 ::: bot acc: 0.0312
top acc: 0.1372 ::: bot acc: 0.0303
top acc: 0.1336 ::: bot acc: 0.0332
top acc: 0.1017 ::: bot acc: 0.0257
current epoch: 50
train loss is 0.001838
average val loss: 0.001598, accuracy: 0.0469
average test loss: 0.003201, accuracy: 0.0657
case acc: 0.05205065
case acc: 0.05349348
case acc: 0.078855075
case acc: 0.07671131
case acc: 0.07972313
case acc: 0.05364199
top acc: 0.0927 ::: bot acc: 0.0193
top acc: 0.0898 ::: bot acc: 0.0244
top acc: 0.1522 ::: bot acc: 0.0329
top acc: 0.1343 ::: bot acc: 0.0296
top acc: 0.1309 ::: bot acc: 0.0321
top acc: 0.0988 ::: bot acc: 0.0264
LME_Co_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5346 5346 5346
1.7082474 -0.6288155 0.25454274 -0.21218425
Validation: 594 594 594
Testing: 768 768 768
pre-processing time: 0.00020813941955566406
the split date is 2012-07-01
train dropout: 0.4 test dropout: 0.15
net initializing with time: 0.002834796905517578
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.010265
average val loss: 0.004248, accuracy: 0.0737
average test loss: 0.004504, accuracy: 0.0784
case acc: 0.09924266
case acc: 0.14125127
case acc: 0.07651659
case acc: 0.033617724
case acc: 0.06283077
case acc: 0.05708287
top acc: 0.0627 ::: bot acc: 0.1439
top acc: 0.1851 ::: bot acc: 0.0984
top acc: 0.0273 ::: bot acc: 0.1382
top acc: 0.0127 ::: bot acc: 0.0653
top acc: 0.0248 ::: bot acc: 0.1043
top acc: 0.0887 ::: bot acc: 0.0610
current epoch: 2
train loss is 0.008281
average val loss: 0.009708, accuracy: 0.1128
average test loss: 0.009978, accuracy: 0.1145
case acc: 0.0633682
case acc: 0.25210494
case acc: 0.07665068
case acc: 0.09173264
case acc: 0.06910436
case acc: 0.13384886
top acc: 0.1373 ::: bot acc: 0.0208
top acc: 0.2955 ::: bot acc: 0.2101
top acc: 0.1480 ::: bot acc: 0.0238
top acc: 0.1279 ::: bot acc: 0.0521
top acc: 0.1276 ::: bot acc: 0.0267
top acc: 0.2004 ::: bot acc: 0.0679
current epoch: 3
train loss is 0.010285
average val loss: 0.019045, accuracy: 0.1766
average test loss: 0.019286, accuracy: 0.1769
case acc: 0.1258153
case acc: 0.31293398
case acc: 0.1382455
case acc: 0.1597736
case acc: 0.13288485
case acc: 0.19177003
top acc: 0.2133 ::: bot acc: 0.0563
top acc: 0.3564 ::: bot acc: 0.2709
top acc: 0.2219 ::: bot acc: 0.0595
top acc: 0.1953 ::: bot acc: 0.1214
top acc: 0.1964 ::: bot acc: 0.0792
top acc: 0.2623 ::: bot acc: 0.1160
current epoch: 4
train loss is 0.013448
average val loss: 0.006981, accuracy: 0.0937
average test loss: 0.007171, accuracy: 0.0957
case acc: 0.05998344
case acc: 0.21196513
case acc: 0.06923328
case acc: 0.06909974
case acc: 0.056642484
case acc: 0.10707009
top acc: 0.1268 ::: bot acc: 0.0302
top acc: 0.2545 ::: bot acc: 0.1700
top acc: 0.1327 ::: bot acc: 0.0318
top acc: 0.1037 ::: bot acc: 0.0315
top acc: 0.1105 ::: bot acc: 0.0215
top acc: 0.1683 ::: bot acc: 0.0513
current epoch: 5
train loss is 0.009421
average val loss: 0.003095, accuracy: 0.0625
average test loss: 0.003262, accuracy: 0.0663
case acc: 0.078451574
case acc: 0.11428867
case acc: 0.06855801
case acc: 0.029959528
case acc: 0.052047607
case acc: 0.05447325
top acc: 0.0634 ::: bot acc: 0.1134
top acc: 0.1583 ::: bot acc: 0.0717
top acc: 0.0432 ::: bot acc: 0.1194
top acc: 0.0173 ::: bot acc: 0.0581
top acc: 0.0343 ::: bot acc: 0.0834
top acc: 0.0803 ::: bot acc: 0.0655
current epoch: 6
train loss is 0.004654
average val loss: 0.003411, accuracy: 0.0639
average test loss: 0.003600, accuracy: 0.0670
case acc: 0.059437435
case acc: 0.13847391
case acc: 0.0618347
case acc: 0.030550871
case acc: 0.043072328
case acc: 0.06853932
top acc: 0.0802 ::: bot acc: 0.0758
top acc: 0.1829 ::: bot acc: 0.0954
top acc: 0.0760 ::: bot acc: 0.0888
top acc: 0.0506 ::: bot acc: 0.0232
top acc: 0.0765 ::: bot acc: 0.0426
top acc: 0.1167 ::: bot acc: 0.0392
current epoch: 7
train loss is 0.004280
average val loss: 0.004329, accuracy: 0.0736
average test loss: 0.004479, accuracy: 0.0756
case acc: 0.056681022
case acc: 0.15499464
case acc: 0.06041786
case acc: 0.043745957
case acc: 0.052984558
case acc: 0.084653
top acc: 0.1077 ::: bot acc: 0.0479
top acc: 0.1983 ::: bot acc: 0.1130
top acc: 0.0970 ::: bot acc: 0.0648
top acc: 0.0749 ::: bot acc: 0.0142
top acc: 0.1052 ::: bot acc: 0.0219
top acc: 0.1400 ::: bot acc: 0.0398
current epoch: 8
train loss is 0.004516
average val loss: 0.003557, accuracy: 0.0666
average test loss: 0.003750, accuracy: 0.0688
case acc: 0.05653835
case acc: 0.1355083
case acc: 0.060434595
case acc: 0.03636264
case acc: 0.048314054
case acc: 0.07591074
top acc: 0.0987 ::: bot acc: 0.0565
top acc: 0.1793 ::: bot acc: 0.0929
top acc: 0.0851 ::: bot acc: 0.0781
top acc: 0.0632 ::: bot acc: 0.0152
top acc: 0.0979 ::: bot acc: 0.0231
top acc: 0.1288 ::: bot acc: 0.0370
current epoch: 9
train loss is 0.004021
average val loss: 0.003332, accuracy: 0.0647
average test loss: 0.003486, accuracy: 0.0667
case acc: 0.056797907
case acc: 0.12473005
case acc: 0.060438152
case acc: 0.035074785
case acc: 0.04917281
case acc: 0.0741172
top acc: 0.0986 ::: bot acc: 0.0575
top acc: 0.1681 ::: bot acc: 0.0824
top acc: 0.0810 ::: bot acc: 0.0817
top acc: 0.0606 ::: bot acc: 0.0165
top acc: 0.0992 ::: bot acc: 0.0226
top acc: 0.1262 ::: bot acc: 0.0376
current epoch: 10
train loss is 0.003649
average val loss: 0.003197, accuracy: 0.0638
average test loss: 0.003357, accuracy: 0.0657
case acc: 0.056740876
case acc: 0.117390245
case acc: 0.060558237
case acc: 0.0351203
case acc: 0.050482012
case acc: 0.074052274
top acc: 0.1013 ::: bot acc: 0.0539
top acc: 0.1613 ::: bot acc: 0.0741
top acc: 0.0805 ::: bot acc: 0.0827
top acc: 0.0609 ::: bot acc: 0.0162
top acc: 0.1018 ::: bot acc: 0.0216
top acc: 0.1259 ::: bot acc: 0.0372
current epoch: 11
train loss is 0.003353
average val loss: 0.003260, accuracy: 0.0648
average test loss: 0.003423, accuracy: 0.0669
case acc: 0.056746893
case acc: 0.11493161
case acc: 0.060710035
case acc: 0.037551116
case acc: 0.05483085
case acc: 0.076547936
top acc: 0.1070 ::: bot acc: 0.0480
top acc: 0.1582 ::: bot acc: 0.0726
top acc: 0.0839 ::: bot acc: 0.0794
top acc: 0.0648 ::: bot acc: 0.0148
top acc: 0.1087 ::: bot acc: 0.0205
top acc: 0.1294 ::: bot acc: 0.0373
current epoch: 12
train loss is 0.003167
average val loss: 0.003303, accuracy: 0.0656
average test loss: 0.003462, accuracy: 0.0675
case acc: 0.05701985
case acc: 0.1115002
case acc: 0.06053937
case acc: 0.03911902
case acc: 0.057998613
case acc: 0.078804016
top acc: 0.1123 ::: bot acc: 0.0432
top acc: 0.1550 ::: bot acc: 0.0691
top acc: 0.0862 ::: bot acc: 0.0768
top acc: 0.0677 ::: bot acc: 0.0138
top acc: 0.1135 ::: bot acc: 0.0211
top acc: 0.1320 ::: bot acc: 0.0382
current epoch: 13
train loss is 0.003083
average val loss: 0.003206, accuracy: 0.0648
average test loss: 0.003377, accuracy: 0.0667
case acc: 0.05743937
case acc: 0.10579666
case acc: 0.06024689
case acc: 0.039387375
case acc: 0.05881743
case acc: 0.07831369
top acc: 0.1152 ::: bot acc: 0.0407
top acc: 0.1491 ::: bot acc: 0.0635
top acc: 0.0859 ::: bot acc: 0.0771
top acc: 0.0683 ::: bot acc: 0.0138
top acc: 0.1145 ::: bot acc: 0.0206
top acc: 0.1316 ::: bot acc: 0.0384
current epoch: 14
train loss is 0.002934
average val loss: 0.003181, accuracy: 0.0647
average test loss: 0.003344, accuracy: 0.0665
case acc: 0.057738625
case acc: 0.10118164
case acc: 0.060401164
case acc: 0.04046932
case acc: 0.06010059
case acc: 0.078994274
top acc: 0.1179 ::: bot acc: 0.0375
top acc: 0.1444 ::: bot acc: 0.0588
top acc: 0.0874 ::: bot acc: 0.0757
top acc: 0.0699 ::: bot acc: 0.0139
top acc: 0.1164 ::: bot acc: 0.0213
top acc: 0.1326 ::: bot acc: 0.0385
current epoch: 15
train loss is 0.002787
average val loss: 0.003117, accuracy: 0.0641
average test loss: 0.003270, accuracy: 0.0658
case acc: 0.058204405
case acc: 0.0958281
case acc: 0.060439624
case acc: 0.04116153
case acc: 0.06033014
case acc: 0.07857401
top acc: 0.1199 ::: bot acc: 0.0357
top acc: 0.1391 ::: bot acc: 0.0533
top acc: 0.0877 ::: bot acc: 0.0754
top acc: 0.0703 ::: bot acc: 0.0142
top acc: 0.1166 ::: bot acc: 0.0212
top acc: 0.1317 ::: bot acc: 0.0381
current epoch: 16
train loss is 0.002683
average val loss: 0.003086, accuracy: 0.0638
average test loss: 0.003261, accuracy: 0.0656
case acc: 0.058605444
case acc: 0.09241648
case acc: 0.060368434
case acc: 0.04207219
case acc: 0.0609908
case acc: 0.07921129
top acc: 0.1226 ::: bot acc: 0.0331
top acc: 0.1360 ::: bot acc: 0.0496
top acc: 0.0893 ::: bot acc: 0.0736
top acc: 0.0725 ::: bot acc: 0.0140
top acc: 0.1174 ::: bot acc: 0.0212
top acc: 0.1331 ::: bot acc: 0.0385
current epoch: 17
train loss is 0.002558
average val loss: 0.003026, accuracy: 0.0632
average test loss: 0.003214, accuracy: 0.0651
case acc: 0.05930201
case acc: 0.08814165
case acc: 0.060414635
case acc: 0.04265034
case acc: 0.061340295
case acc: 0.078712
top acc: 0.1253 ::: bot acc: 0.0308
top acc: 0.1314 ::: bot acc: 0.0459
top acc: 0.0901 ::: bot acc: 0.0729
top acc: 0.0727 ::: bot acc: 0.0141
top acc: 0.1178 ::: bot acc: 0.0220
top acc: 0.1321 ::: bot acc: 0.0381
current epoch: 18
train loss is 0.002481
average val loss: 0.003030, accuracy: 0.0632
average test loss: 0.003227, accuracy: 0.0652
case acc: 0.059901588
case acc: 0.08537722
case acc: 0.060688227
case acc: 0.04419505
case acc: 0.061831456
case acc: 0.07944949
top acc: 0.1278 ::: bot acc: 0.0278
top acc: 0.1289 ::: bot acc: 0.0431
top acc: 0.0926 ::: bot acc: 0.0711
top acc: 0.0749 ::: bot acc: 0.0149
top acc: 0.1187 ::: bot acc: 0.0216
top acc: 0.1336 ::: bot acc: 0.0382
current epoch: 19
train loss is 0.002439
average val loss: 0.003153, accuracy: 0.0646
average test loss: 0.003328, accuracy: 0.0666
case acc: 0.061233804
case acc: 0.08517579
case acc: 0.060872212
case acc: 0.047037024
case acc: 0.06400191
case acc: 0.081001975
top acc: 0.1317 ::: bot acc: 0.0235
top acc: 0.1287 ::: bot acc: 0.0430
top acc: 0.0965 ::: bot acc: 0.0667
top acc: 0.0783 ::: bot acc: 0.0164
top acc: 0.1214 ::: bot acc: 0.0224
top acc: 0.1353 ::: bot acc: 0.0388
current epoch: 20
train loss is 0.002375
average val loss: 0.003163, accuracy: 0.0648
average test loss: 0.003340, accuracy: 0.0666
case acc: 0.062081777
case acc: 0.08285972
case acc: 0.06096459
case acc: 0.04826983
case acc: 0.06387249
case acc: 0.08125656
top acc: 0.1348 ::: bot acc: 0.0207
top acc: 0.1260 ::: bot acc: 0.0407
top acc: 0.0987 ::: bot acc: 0.0644
top acc: 0.0800 ::: bot acc: 0.0165
top acc: 0.1216 ::: bot acc: 0.0227
top acc: 0.1360 ::: bot acc: 0.0389
current epoch: 21
train loss is 0.002325
average val loss: 0.003258, accuracy: 0.0658
average test loss: 0.003429, accuracy: 0.0677
case acc: 0.06350686
case acc: 0.08244448
case acc: 0.061426714
case acc: 0.050696723
case acc: 0.06522243
case acc: 0.08269496
top acc: 0.1384 ::: bot acc: 0.0174
top acc: 0.1256 ::: bot acc: 0.0404
top acc: 0.1019 ::: bot acc: 0.0609
top acc: 0.0828 ::: bot acc: 0.0181
top acc: 0.1229 ::: bot acc: 0.0232
top acc: 0.1379 ::: bot acc: 0.0395
current epoch: 22
train loss is 0.002299
average val loss: 0.003260, accuracy: 0.0658
average test loss: 0.003432, accuracy: 0.0676
case acc: 0.06437916
case acc: 0.08021415
case acc: 0.061528534
case acc: 0.051739387
case acc: 0.065377235
case acc: 0.08264364
top acc: 0.1405 ::: bot acc: 0.0165
top acc: 0.1230 ::: bot acc: 0.0383
top acc: 0.1032 ::: bot acc: 0.0594
top acc: 0.0843 ::: bot acc: 0.0184
top acc: 0.1234 ::: bot acc: 0.0233
top acc: 0.1377 ::: bot acc: 0.0392
current epoch: 23
train loss is 0.002284
average val loss: 0.003300, accuracy: 0.0663
average test loss: 0.003473, accuracy: 0.0681
case acc: 0.06556523
case acc: 0.07918623
case acc: 0.061891995
case acc: 0.053141613
case acc: 0.06606118
case acc: 0.08298272
top acc: 0.1428 ::: bot acc: 0.0150
top acc: 0.1223 ::: bot acc: 0.0372
top acc: 0.1052 ::: bot acc: 0.0575
top acc: 0.0857 ::: bot acc: 0.0197
top acc: 0.1242 ::: bot acc: 0.0234
top acc: 0.1381 ::: bot acc: 0.0392
current epoch: 24
train loss is 0.002246
average val loss: 0.003317, accuracy: 0.0666
average test loss: 0.003492, accuracy: 0.0683
case acc: 0.06630172
case acc: 0.07725225
case acc: 0.06238863
case acc: 0.054130316
case acc: 0.06653494
case acc: 0.08309901
top acc: 0.1448 ::: bot acc: 0.0138
top acc: 0.1206 ::: bot acc: 0.0355
top acc: 0.1072 ::: bot acc: 0.0556
top acc: 0.0868 ::: bot acc: 0.0202
top acc: 0.1247 ::: bot acc: 0.0241
top acc: 0.1383 ::: bot acc: 0.0393
current epoch: 25
train loss is 0.002198
average val loss: 0.003292, accuracy: 0.0663
average test loss: 0.003457, accuracy: 0.0679
case acc: 0.0666955
case acc: 0.07516477
case acc: 0.062426586
case acc: 0.054483436
case acc: 0.06586381
case acc: 0.08258958
top acc: 0.1449 ::: bot acc: 0.0138
top acc: 0.1175 ::: bot acc: 0.0341
top acc: 0.1082 ::: bot acc: 0.0550
top acc: 0.0875 ::: bot acc: 0.0204
top acc: 0.1237 ::: bot acc: 0.0241
top acc: 0.1378 ::: bot acc: 0.0390
current epoch: 26
train loss is 0.002162
average val loss: 0.003186, accuracy: 0.0651
average test loss: 0.003360, accuracy: 0.0667
case acc: 0.06640073
case acc: 0.0717547
case acc: 0.062297687
case acc: 0.05372126
case acc: 0.06439985
case acc: 0.081346706
top acc: 0.1442 ::: bot acc: 0.0143
top acc: 0.1140 ::: bot acc: 0.0314
top acc: 0.1076 ::: bot acc: 0.0553
top acc: 0.0866 ::: bot acc: 0.0198
top acc: 0.1220 ::: bot acc: 0.0231
top acc: 0.1361 ::: bot acc: 0.0386
current epoch: 27
train loss is 0.002121
average val loss: 0.003206, accuracy: 0.0654
average test loss: 0.003377, accuracy: 0.0668
case acc: 0.06696533
case acc: 0.070724696
case acc: 0.06268266
case acc: 0.054795675
case acc: 0.064389706
case acc: 0.08141373
top acc: 0.1456 ::: bot acc: 0.0130
top acc: 0.1131 ::: bot acc: 0.0303
top acc: 0.1095 ::: bot acc: 0.0538
top acc: 0.0877 ::: bot acc: 0.0205
top acc: 0.1215 ::: bot acc: 0.0232
top acc: 0.1359 ::: bot acc: 0.0386
current epoch: 28
train loss is 0.002096
average val loss: 0.003303, accuracy: 0.0665
average test loss: 0.003451, accuracy: 0.0677
case acc: 0.0681964
case acc: 0.07053037
case acc: 0.062895745
case acc: 0.056808412
case acc: 0.06542072
case acc: 0.08206318
top acc: 0.1479 ::: bot acc: 0.0121
top acc: 0.1131 ::: bot acc: 0.0300
top acc: 0.1115 ::: bot acc: 0.0509
top acc: 0.0901 ::: bot acc: 0.0223
top acc: 0.1236 ::: bot acc: 0.0231
top acc: 0.1368 ::: bot acc: 0.0391
current epoch: 29
train loss is 0.002092
average val loss: 0.003294, accuracy: 0.0664
average test loss: 0.003434, accuracy: 0.0675
case acc: 0.06854442
case acc: 0.0691705
case acc: 0.063097544
case acc: 0.056981087
case acc: 0.06510037
case acc: 0.081869826
top acc: 0.1483 ::: bot acc: 0.0121
top acc: 0.1111 ::: bot acc: 0.0293
top acc: 0.1122 ::: bot acc: 0.0503
top acc: 0.0902 ::: bot acc: 0.0224
top acc: 0.1227 ::: bot acc: 0.0233
top acc: 0.1367 ::: bot acc: 0.0388
current epoch: 30
train loss is 0.002092
average val loss: 0.003383, accuracy: 0.0674
average test loss: 0.003539, accuracy: 0.0686
case acc: 0.070174836
case acc: 0.06972817
case acc: 0.0636352
case acc: 0.059059955
case acc: 0.066519305
case acc: 0.08273265
top acc: 0.1510 ::: bot acc: 0.0115
top acc: 0.1114 ::: bot acc: 0.0298
top acc: 0.1154 ::: bot acc: 0.0475
top acc: 0.0925 ::: bot acc: 0.0239
top acc: 0.1247 ::: bot acc: 0.0238
top acc: 0.1380 ::: bot acc: 0.0392
current epoch: 31
train loss is 0.002081
average val loss: 0.003406, accuracy: 0.0676
average test loss: 0.003556, accuracy: 0.0688
case acc: 0.07058163
case acc: 0.06886351
case acc: 0.06396299
case acc: 0.06014721
case acc: 0.066655055
case acc: 0.08281489
top acc: 0.1517 ::: bot acc: 0.0112
top acc: 0.1110 ::: bot acc: 0.0290
top acc: 0.1168 ::: bot acc: 0.0461
top acc: 0.0937 ::: bot acc: 0.0248
top acc: 0.1246 ::: bot acc: 0.0240
top acc: 0.1379 ::: bot acc: 0.0392
current epoch: 32
train loss is 0.002062
average val loss: 0.003500, accuracy: 0.0686
average test loss: 0.003645, accuracy: 0.0698
case acc: 0.07186988
case acc: 0.06910432
case acc: 0.064493686
case acc: 0.061683506
case acc: 0.06802569
case acc: 0.08377077
top acc: 0.1539 ::: bot acc: 0.0112
top acc: 0.1111 ::: bot acc: 0.0292
top acc: 0.1192 ::: bot acc: 0.0436
top acc: 0.0954 ::: bot acc: 0.0259
top acc: 0.1267 ::: bot acc: 0.0246
top acc: 0.1391 ::: bot acc: 0.0397
current epoch: 33
train loss is 0.002068
average val loss: 0.003476, accuracy: 0.0683
average test loss: 0.003617, accuracy: 0.0695
case acc: 0.07179521
case acc: 0.06781111
case acc: 0.06445451
case acc: 0.06173929
case acc: 0.06775999
case acc: 0.08328885
top acc: 0.1540 ::: bot acc: 0.0110
top acc: 0.1098 ::: bot acc: 0.0282
top acc: 0.1192 ::: bot acc: 0.0435
top acc: 0.0952 ::: bot acc: 0.0261
top acc: 0.1258 ::: bot acc: 0.0247
top acc: 0.1385 ::: bot acc: 0.0392
current epoch: 34
train loss is 0.002057
average val loss: 0.003512, accuracy: 0.0688
average test loss: 0.003653, accuracy: 0.0699
case acc: 0.072238415
case acc: 0.06738985
case acc: 0.06489614
case acc: 0.062439606
case acc: 0.06884752
case acc: 0.08371349
top acc: 0.1545 ::: bot acc: 0.0111
top acc: 0.1092 ::: bot acc: 0.0279
top acc: 0.1204 ::: bot acc: 0.0427
top acc: 0.0965 ::: bot acc: 0.0265
top acc: 0.1275 ::: bot acc: 0.0254
top acc: 0.1388 ::: bot acc: 0.0400
current epoch: 35
train loss is 0.002047
average val loss: 0.003510, accuracy: 0.0688
average test loss: 0.003658, accuracy: 0.0700
case acc: 0.07238718
case acc: 0.06614299
case acc: 0.064713545
case acc: 0.06319668
case acc: 0.069481164
case acc: 0.083782196
top acc: 0.1547 ::: bot acc: 0.0112
top acc: 0.1077 ::: bot acc: 0.0269
top acc: 0.1204 ::: bot acc: 0.0421
top acc: 0.0971 ::: bot acc: 0.0271
top acc: 0.1285 ::: bot acc: 0.0256
top acc: 0.1391 ::: bot acc: 0.0398
current epoch: 36
train loss is 0.002045
average val loss: 0.003447, accuracy: 0.0681
average test loss: 0.003580, accuracy: 0.0690
case acc: 0.07138734
case acc: 0.06401749
case acc: 0.06466588
case acc: 0.062073026
case acc: 0.06888361
case acc: 0.08297668
top acc: 0.1533 ::: bot acc: 0.0108
top acc: 0.1054 ::: bot acc: 0.0256
top acc: 0.1195 ::: bot acc: 0.0435
top acc: 0.0960 ::: bot acc: 0.0262
top acc: 0.1275 ::: bot acc: 0.0254
top acc: 0.1380 ::: bot acc: 0.0393
current epoch: 37
train loss is 0.002003
average val loss: 0.003248, accuracy: 0.0660
average test loss: 0.003389, accuracy: 0.0667
case acc: 0.06938866
case acc: 0.060177196
case acc: 0.06373133
case acc: 0.05955337
case acc: 0.06657317
case acc: 0.0804944
top acc: 0.1503 ::: bot acc: 0.0115
top acc: 0.1011 ::: bot acc: 0.0229
top acc: 0.1160 ::: bot acc: 0.0467
top acc: 0.0932 ::: bot acc: 0.0246
top acc: 0.1248 ::: bot acc: 0.0238
top acc: 0.1348 ::: bot acc: 0.0387
current epoch: 38
train loss is 0.001946
average val loss: 0.003033, accuracy: 0.0636
average test loss: 0.003159, accuracy: 0.0638
case acc: 0.06704767
case acc: 0.055715695
case acc: 0.06316735
case acc: 0.056082364
case acc: 0.06331208
case acc: 0.07772248
top acc: 0.1457 ::: bot acc: 0.0133
top acc: 0.0952 ::: bot acc: 0.0206
top acc: 0.1119 ::: bot acc: 0.0509
top acc: 0.0892 ::: bot acc: 0.0218
top acc: 0.1207 ::: bot acc: 0.0221
top acc: 0.1309 ::: bot acc: 0.0380
current epoch: 39
train loss is 0.001885
average val loss: 0.002915, accuracy: 0.0623
average test loss: 0.003038, accuracy: 0.0622
case acc: 0.0658994
case acc: 0.0528555
case acc: 0.062495053
case acc: 0.05422613
case acc: 0.061758142
case acc: 0.075969264
top acc: 0.1436 ::: bot acc: 0.0143
top acc: 0.0920 ::: bot acc: 0.0187
top acc: 0.1097 ::: bot acc: 0.0527
top acc: 0.0872 ::: bot acc: 0.0204
top acc: 0.1185 ::: bot acc: 0.0216
top acc: 0.1286 ::: bot acc: 0.0373
current epoch: 40
train loss is 0.001868
average val loss: 0.002845, accuracy: 0.0615
average test loss: 0.002974, accuracy: 0.0614
case acc: 0.06490505
case acc: 0.051324572
case acc: 0.062426444
case acc: 0.053514324
case acc: 0.060637042
case acc: 0.0754472
top acc: 0.1413 ::: bot acc: 0.0152
top acc: 0.0902 ::: bot acc: 0.0180
top acc: 0.1088 ::: bot acc: 0.0542
top acc: 0.0862 ::: bot acc: 0.0199
top acc: 0.1172 ::: bot acc: 0.0212
top acc: 0.1279 ::: bot acc: 0.0374
current epoch: 41
train loss is 0.001852
average val loss: 0.002773, accuracy: 0.0606
average test loss: 0.002902, accuracy: 0.0604
case acc: 0.064275905
case acc: 0.049378872
case acc: 0.0621857
case acc: 0.052629236
case acc: 0.059360754
case acc: 0.07447229
top acc: 0.1400 ::: bot acc: 0.0163
top acc: 0.0875 ::: bot acc: 0.0171
top acc: 0.1072 ::: bot acc: 0.0558
top acc: 0.0856 ::: bot acc: 0.0189
top acc: 0.1153 ::: bot acc: 0.0210
top acc: 0.1266 ::: bot acc: 0.0370
current epoch: 42
train loss is 0.001831
average val loss: 0.002719, accuracy: 0.0601
average test loss: 0.002842, accuracy: 0.0595
case acc: 0.06357974
case acc: 0.04781542
case acc: 0.06188561
case acc: 0.051812414
case acc: 0.0583855
case acc: 0.073806725
top acc: 0.1385 ::: bot acc: 0.0172
top acc: 0.0861 ::: bot acc: 0.0162
top acc: 0.1060 ::: bot acc: 0.0567
top acc: 0.0844 ::: bot acc: 0.0186
top acc: 0.1141 ::: bot acc: 0.0210
top acc: 0.1258 ::: bot acc: 0.0368
current epoch: 43
train loss is 0.001822
average val loss: 0.002669, accuracy: 0.0594
average test loss: 0.002788, accuracy: 0.0588
case acc: 0.063126124
case acc: 0.046301074
case acc: 0.061673388
case acc: 0.051009055
case acc: 0.05771545
case acc: 0.07308886
top acc: 0.1373 ::: bot acc: 0.0182
top acc: 0.0840 ::: bot acc: 0.0154
top acc: 0.1047 ::: bot acc: 0.0580
top acc: 0.0836 ::: bot acc: 0.0180
top acc: 0.1132 ::: bot acc: 0.0206
top acc: 0.1244 ::: bot acc: 0.0369
current epoch: 44
train loss is 0.001806
average val loss: 0.002610, accuracy: 0.0588
average test loss: 0.002733, accuracy: 0.0581
case acc: 0.062450837
case acc: 0.045128286
case acc: 0.06147591
case acc: 0.050293006
case acc: 0.056917615
case acc: 0.07240409
top acc: 0.1361 ::: bot acc: 0.0190
top acc: 0.0823 ::: bot acc: 0.0151
top acc: 0.1031 ::: bot acc: 0.0594
top acc: 0.0823 ::: bot acc: 0.0180
top acc: 0.1120 ::: bot acc: 0.0205
top acc: 0.1235 ::: bot acc: 0.0369
current epoch: 45
train loss is 0.001800
average val loss: 0.002513, accuracy: 0.0576
average test loss: 0.002637, accuracy: 0.0568
case acc: 0.061684746
case acc: 0.042998057
case acc: 0.06148472
case acc: 0.04838421
case acc: 0.05511813
case acc: 0.07108244
top acc: 0.1333 ::: bot acc: 0.0219
top acc: 0.0795 ::: bot acc: 0.0145
top acc: 0.1013 ::: bot acc: 0.0620
top acc: 0.0801 ::: bot acc: 0.0167
top acc: 0.1095 ::: bot acc: 0.0203
top acc: 0.1214 ::: bot acc: 0.0372
current epoch: 46
train loss is 0.001793
average val loss: 0.002469, accuracy: 0.0571
average test loss: 0.002599, accuracy: 0.0562
case acc: 0.0612233
case acc: 0.042001
case acc: 0.061205484
case acc: 0.047760118
case acc: 0.054361615
case acc: 0.0706583
top acc: 0.1326 ::: bot acc: 0.0224
top acc: 0.0781 ::: bot acc: 0.0140
top acc: 0.1003 ::: bot acc: 0.0627
top acc: 0.0796 ::: bot acc: 0.0162
top acc: 0.1082 ::: bot acc: 0.0205
top acc: 0.1207 ::: bot acc: 0.0374
current epoch: 47
train loss is 0.001784
average val loss: 0.002406, accuracy: 0.0563
average test loss: 0.002537, accuracy: 0.0553
case acc: 0.060732115
case acc: 0.040951867
case acc: 0.060975343
case acc: 0.046524577
case acc: 0.05316647
case acc: 0.06967281
top acc: 0.1310 ::: bot acc: 0.0239
top acc: 0.0764 ::: bot acc: 0.0145
top acc: 0.0991 ::: bot acc: 0.0638
top acc: 0.0778 ::: bot acc: 0.0155
top acc: 0.1065 ::: bot acc: 0.0207
top acc: 0.1189 ::: bot acc: 0.0379
current epoch: 48
train loss is 0.001775
average val loss: 0.002403, accuracy: 0.0562
average test loss: 0.002523, accuracy: 0.0552
case acc: 0.060757585
case acc: 0.040687233
case acc: 0.060997803
case acc: 0.046608306
case acc: 0.052654404
case acc: 0.06951605
top acc: 0.1306 ::: bot acc: 0.0246
top acc: 0.0761 ::: bot acc: 0.0142
top acc: 0.0990 ::: bot acc: 0.0638
top acc: 0.0780 ::: bot acc: 0.0157
top acc: 0.1055 ::: bot acc: 0.0205
top acc: 0.1184 ::: bot acc: 0.0381
current epoch: 49
train loss is 0.001771
average val loss: 0.002443, accuracy: 0.0567
average test loss: 0.002565, accuracy: 0.0558
case acc: 0.061036628
case acc: 0.041375697
case acc: 0.061152317
case acc: 0.047580518
case acc: 0.053331163
case acc: 0.07016882
top acc: 0.1316 ::: bot acc: 0.0233
top acc: 0.0773 ::: bot acc: 0.0141
top acc: 0.1003 ::: bot acc: 0.0628
top acc: 0.0791 ::: bot acc: 0.0163
top acc: 0.1066 ::: bot acc: 0.0204
top acc: 0.1196 ::: bot acc: 0.0379
current epoch: 50
train loss is 0.001773
average val loss: 0.002525, accuracy: 0.0577
average test loss: 0.002648, accuracy: 0.0569
case acc: 0.061645515
case acc: 0.042851966
case acc: 0.06156542
case acc: 0.049182143
case acc: 0.054563362
case acc: 0.07130616
top acc: 0.1338 ::: bot acc: 0.0210
top acc: 0.0793 ::: bot acc: 0.0143
top acc: 0.1025 ::: bot acc: 0.0605
top acc: 0.0812 ::: bot acc: 0.0170
top acc: 0.1087 ::: bot acc: 0.0202
top acc: 0.1218 ::: bot acc: 0.0373
