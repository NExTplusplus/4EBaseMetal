
		{"drop_out": 0.4, "drop_out_mc": 0.05, "repeat_mc": 50, "hidden": 30, "embedding_size": 5, "batch": 512, "lag": 3}
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
LME_Co_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5328 5328 5328
1.7082474 -0.6288155 0.29835227 -0.25048217
Validation: 594 594 594
Testing: 774 774 774
pre-processing time: 0.00036263465881347656
the split date is 2010-07-01
net initializing with time: 0.004511117935180664
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.123600
average val loss: 0.111782, accuracy: 0.1124
average test loss: 0.134719, accuracy: 0.1391
case acc: 0.19893259
case acc: 0.064742476
case acc: 0.1588102
case acc: 0.13870503
case acc: 0.16504522
case acc: 0.10837966
top acc: 0.1670 ::: bot acc: 0.2368
top acc: 0.1164 ::: bot acc: 0.0194
top acc: 0.0872 ::: bot acc: 0.2259
top acc: 0.0970 ::: bot acc: 0.1775
top acc: 0.1338 ::: bot acc: 0.1943
top acc: 0.0652 ::: bot acc: 0.1557
current epoch: 2
train loss is 0.118590
average val loss: 0.090459, accuracy: 0.0927
average test loss: 0.071379, accuracy: 0.0685
case acc: 0.030484535
case acc: 0.21389659
case acc: 0.052003056
case acc: 0.036281794
case acc: 0.022520725
case acc: 0.05593493
top acc: 0.0093 ::: bot acc: 0.0629
top acc: 0.2768 ::: bot acc: 0.1461
top acc: 0.0873 ::: bot acc: 0.0515
top acc: 0.0703 ::: bot acc: 0.0130
top acc: 0.0283 ::: bot acc: 0.0326
top acc: 0.0953 ::: bot acc: 0.0166
current epoch: 3
train loss is 0.127343
average val loss: 0.146216, accuracy: 0.1466
average test loss: 0.113340, accuracy: 0.1126
case acc: 0.054628193
case acc: 0.26972494
case acc: 0.087802105
case acc: 0.09206559
case acc: 0.061618444
case acc: 0.10963401
top acc: 0.0770 ::: bot acc: 0.0355
top acc: 0.3329 ::: bot acc: 0.2019
top acc: 0.1568 ::: bot acc: 0.0270
top acc: 0.1342 ::: bot acc: 0.0534
top acc: 0.0928 ::: bot acc: 0.0323
top acc: 0.1532 ::: bot acc: 0.0620
current epoch: 4
train loss is 0.123339
average val loss: 0.081195, accuracy: 0.0829
average test loss: 0.063902, accuracy: 0.0615
case acc: 0.02970107
case acc: 0.18743531
case acc: 0.05166351
case acc: 0.032418597
case acc: 0.022690566
case acc: 0.045115568
top acc: 0.0098 ::: bot acc: 0.0616
top acc: 0.2506 ::: bot acc: 0.1195
top acc: 0.0844 ::: bot acc: 0.0543
top acc: 0.0602 ::: bot acc: 0.0206
top acc: 0.0268 ::: bot acc: 0.0342
top acc: 0.0771 ::: bot acc: 0.0209
current epoch: 5
train loss is 0.102125
average val loss: 0.060741, accuracy: 0.0617
average test loss: 0.058970, accuracy: 0.0613
case acc: 0.061183438
case acc: 0.13700758
case acc: 0.058336146
case acc: 0.035818417
case acc: 0.040241003
case acc: 0.03495109
top acc: 0.0295 ::: bot acc: 0.0991
top acc: 0.2001 ::: bot acc: 0.0691
top acc: 0.0436 ::: bot acc: 0.0965
top acc: 0.0203 ::: bot acc: 0.0614
top acc: 0.0182 ::: bot acc: 0.0655
top acc: 0.0372 ::: bot acc: 0.0568
current epoch: 6
train loss is 0.088490
average val loss: 0.059885, accuracy: 0.0609
average test loss: 0.055254, accuracy: 0.0569
case acc: 0.0527609
case acc: 0.13178614
case acc: 0.05730957
case acc: 0.034136366
case acc: 0.030122332
case acc: 0.03510014
top acc: 0.0213 ::: bot acc: 0.0906
top acc: 0.1950 ::: bot acc: 0.0638
top acc: 0.0457 ::: bot acc: 0.0939
top acc: 0.0246 ::: bot acc: 0.0566
top acc: 0.0167 ::: bot acc: 0.0511
top acc: 0.0426 ::: bot acc: 0.0519
current epoch: 7
train loss is 0.084654
average val loss: 0.062951, accuracy: 0.0641
average test loss: 0.052513, accuracy: 0.0529
case acc: 0.03877654
case acc: 0.13348393
case acc: 0.05383747
case acc: 0.031812143
case acc: 0.022428608
case acc: 0.03679401
top acc: 0.0100 ::: bot acc: 0.0752
top acc: 0.1968 ::: bot acc: 0.0654
top acc: 0.0549 ::: bot acc: 0.0837
top acc: 0.0357 ::: bot acc: 0.0456
top acc: 0.0296 ::: bot acc: 0.0320
top acc: 0.0535 ::: bot acc: 0.0414
current epoch: 8
train loss is 0.084477
average val loss: 0.068235, accuracy: 0.0693
average test loss: 0.052346, accuracy: 0.0520
case acc: 0.028701412
case acc: 0.13549486
case acc: 0.051711068
case acc: 0.031357735
case acc: 0.02518719
case acc: 0.03936013
top acc: 0.0101 ::: bot acc: 0.0601
top acc: 0.1991 ::: bot acc: 0.0673
top acc: 0.0661 ::: bot acc: 0.0727
top acc: 0.0462 ::: bot acc: 0.0350
top acc: 0.0470 ::: bot acc: 0.0145
top acc: 0.0627 ::: bot acc: 0.0326
current epoch: 9
train loss is 0.084717
average val loss: 0.069988, accuracy: 0.0709
average test loss: 0.051958, accuracy: 0.0515
case acc: 0.026092667
case acc: 0.13091533
case acc: 0.05130927
case acc: 0.031428833
case acc: 0.029325677
case acc: 0.04004698
top acc: 0.0162 ::: bot acc: 0.0527
top acc: 0.1946 ::: bot acc: 0.0627
top acc: 0.0703 ::: bot acc: 0.0684
top acc: 0.0497 ::: bot acc: 0.0316
top acc: 0.0561 ::: bot acc: 0.0087
top acc: 0.0645 ::: bot acc: 0.0311
current epoch: 10
train loss is 0.083808
average val loss: 0.068647, accuracy: 0.0694
average test loss: 0.050485, accuracy: 0.0502
case acc: 0.025742335
case acc: 0.12212306
case acc: 0.051317424
case acc: 0.031422645
case acc: 0.03141539
case acc: 0.03931087
top acc: 0.0183 ::: bot acc: 0.0507
top acc: 0.1857 ::: bot acc: 0.0539
top acc: 0.0699 ::: bot acc: 0.0689
top acc: 0.0487 ::: bot acc: 0.0326
top acc: 0.0597 ::: bot acc: 0.0080
top acc: 0.0623 ::: bot acc: 0.0332
current epoch: 11
train loss is 0.080854
average val loss: 0.065539, accuracy: 0.0661
average test loss: 0.048322, accuracy: 0.0482
case acc: 0.025837105
case acc: 0.110884346
case acc: 0.05178125
case acc: 0.031337433
case acc: 0.031506415
case acc: 0.037887704
top acc: 0.0173 ::: bot acc: 0.0516
top acc: 0.1747 ::: bot acc: 0.0425
top acc: 0.0667 ::: bot acc: 0.0724
top acc: 0.0455 ::: bot acc: 0.0358
top acc: 0.0599 ::: bot acc: 0.0080
top acc: 0.0581 ::: bot acc: 0.0369
current epoch: 12
train loss is 0.077166
average val loss: 0.060657, accuracy: 0.0612
average test loss: 0.045726, accuracy: 0.0458
case acc: 0.026986482
case acc: 0.09776345
case acc: 0.052853987
case acc: 0.03147468
case acc: 0.029487658
case acc: 0.036464147
top acc: 0.0131 ::: bot acc: 0.0559
top acc: 0.1609 ::: bot acc: 0.0308
top acc: 0.0603 ::: bot acc: 0.0789
top acc: 0.0399 ::: bot acc: 0.0414
top acc: 0.0565 ::: bot acc: 0.0088
top acc: 0.0519 ::: bot acc: 0.0430
current epoch: 13
train loss is 0.074493
average val loss: 0.057997, accuracy: 0.0585
average test loss: 0.044112, accuracy: 0.0444
case acc: 0.027459728
case acc: 0.08872837
case acc: 0.053592127
case acc: 0.031644408
case acc: 0.0287978
case acc: 0.035920985
top acc: 0.0120 ::: bot acc: 0.0572
top acc: 0.1505 ::: bot acc: 0.0244
top acc: 0.0571 ::: bot acc: 0.0820
top acc: 0.0378 ::: bot acc: 0.0435
top acc: 0.0552 ::: bot acc: 0.0093
top acc: 0.0492 ::: bot acc: 0.0457
current epoch: 14
train loss is 0.071694
average val loss: 0.058245, accuracy: 0.0587
average test loss: 0.043407, accuracy: 0.0436
case acc: 0.026341029
case acc: 0.08415542
case acc: 0.053203117
case acc: 0.031496394
case acc: 0.030064814
case acc: 0.036280207
top acc: 0.0152 ::: bot acc: 0.0537
top acc: 0.1452 ::: bot acc: 0.0213
top acc: 0.0591 ::: bot acc: 0.0801
top acc: 0.0406 ::: bot acc: 0.0408
top acc: 0.0575 ::: bot acc: 0.0086
top acc: 0.0513 ::: bot acc: 0.0435
current epoch: 15
train loss is 0.070119
average val loss: 0.056315, accuracy: 0.0567
average test loss: 0.042146, accuracy: 0.0423
case acc: 0.026341138
case acc: 0.077476874
case acc: 0.053725835
case acc: 0.031463765
case acc: 0.028794102
case acc: 0.035825863
top acc: 0.0149 ::: bot acc: 0.0539
top acc: 0.1370 ::: bot acc: 0.0179
top acc: 0.0579 ::: bot acc: 0.0817
top acc: 0.0398 ::: bot acc: 0.0415
top acc: 0.0552 ::: bot acc: 0.0091
top acc: 0.0498 ::: bot acc: 0.0448
current epoch: 16
train loss is 0.068012
average val loss: 0.054984, accuracy: 0.0553
average test loss: 0.041194, accuracy: 0.0413
case acc: 0.026233118
case acc: 0.072789244
case acc: 0.05380624
case acc: 0.031459745
case acc: 0.027809283
case acc: 0.035701632
top acc: 0.0154 ::: bot acc: 0.0534
top acc: 0.1304 ::: bot acc: 0.0172
top acc: 0.0577 ::: bot acc: 0.0820
top acc: 0.0401 ::: bot acc: 0.0413
top acc: 0.0534 ::: bot acc: 0.0098
top acc: 0.0490 ::: bot acc: 0.0456
current epoch: 17
train loss is 0.066789
average val loss: 0.054734, accuracy: 0.0550
average test loss: 0.040639, accuracy: 0.0407
case acc: 0.025733916
case acc: 0.07013273
case acc: 0.053363316
case acc: 0.031412266
case acc: 0.027499845
case acc: 0.035807025
top acc: 0.0176 ::: bot acc: 0.0513
top acc: 0.1261 ::: bot acc: 0.0176
top acc: 0.0595 ::: bot acc: 0.0801
top acc: 0.0419 ::: bot acc: 0.0395
top acc: 0.0528 ::: bot acc: 0.0102
top acc: 0.0494 ::: bot acc: 0.0453
current epoch: 18
train loss is 0.065026
average val loss: 0.054295, accuracy: 0.0545
average test loss: 0.040144, accuracy: 0.0400
case acc: 0.025343113
case acc: 0.067951806
case acc: 0.052971937
case acc: 0.031382
case acc: 0.026718365
case acc: 0.035663456
top acc: 0.0193 ::: bot acc: 0.0496
top acc: 0.1224 ::: bot acc: 0.0185
top acc: 0.0614 ::: bot acc: 0.0783
top acc: 0.0430 ::: bot acc: 0.0384
top acc: 0.0511 ::: bot acc: 0.0112
top acc: 0.0487 ::: bot acc: 0.0459
current epoch: 19
train loss is 0.064041
average val loss: 0.055335, accuracy: 0.0555
average test loss: 0.040168, accuracy: 0.0397
case acc: 0.02482829
case acc: 0.06751006
case acc: 0.052290972
case acc: 0.031333636
case acc: 0.026719222
case acc: 0.035767578
top acc: 0.0231 ::: bot acc: 0.0457
top acc: 0.1216 ::: bot acc: 0.0188
top acc: 0.0659 ::: bot acc: 0.0739
top acc: 0.0460 ::: bot acc: 0.0353
top acc: 0.0512 ::: bot acc: 0.0110
top acc: 0.0496 ::: bot acc: 0.0449
current epoch: 20
train loss is 0.063676
average val loss: 0.055360, accuracy: 0.0556
average test loss: 0.040005, accuracy: 0.0394
case acc: 0.024805734
case acc: 0.06656698
case acc: 0.051965475
case acc: 0.031343587
case acc: 0.025903717
case acc: 0.03556698
top acc: 0.0251 ::: bot acc: 0.0437
top acc: 0.1199 ::: bot acc: 0.0194
top acc: 0.0689 ::: bot acc: 0.0709
top acc: 0.0473 ::: bot acc: 0.0341
top acc: 0.0492 ::: bot acc: 0.0125
top acc: 0.0485 ::: bot acc: 0.0461
current epoch: 21
train loss is 0.063515
average val loss: 0.059744, accuracy: 0.0600
average test loss: 0.041529, accuracy: 0.0404
case acc: 0.025746107
case acc: 0.06943826
case acc: 0.051770616
case acc: 0.031604636
case acc: 0.02773186
case acc: 0.036319315
top acc: 0.0329 ::: bot acc: 0.0358
top acc: 0.1251 ::: bot acc: 0.0178
top acc: 0.0781 ::: bot acc: 0.0619
top acc: 0.0543 ::: bot acc: 0.0270
top acc: 0.0533 ::: bot acc: 0.0098
top acc: 0.0531 ::: bot acc: 0.0413
current epoch: 22
train loss is 0.064385
average val loss: 0.065425, accuracy: 0.0656
average test loss: 0.044194, accuracy: 0.0427
case acc: 0.028634986
case acc: 0.07350358
case acc: 0.052250907
case acc: 0.03283213
case acc: 0.030887881
case acc: 0.03782603
top acc: 0.0415 ::: bot acc: 0.0282
top acc: 0.1316 ::: bot acc: 0.0171
top acc: 0.0882 ::: bot acc: 0.0518
top acc: 0.0622 ::: bot acc: 0.0191
top acc: 0.0589 ::: bot acc: 0.0082
top acc: 0.0586 ::: bot acc: 0.0359
current epoch: 23
train loss is 0.065530
average val loss: 0.074299, accuracy: 0.0744
average test loss: 0.049505, accuracy: 0.0478
case acc: 0.03568481
case acc: 0.08066613
case acc: 0.054513555
case acc: 0.036958884
case acc: 0.037972763
case acc: 0.040899698
top acc: 0.0530 ::: bot acc: 0.0263
top acc: 0.1412 ::: bot acc: 0.0192
top acc: 0.1011 ::: bot acc: 0.0391
top acc: 0.0728 ::: bot acc: 0.0112
top acc: 0.0687 ::: bot acc: 0.0098
top acc: 0.0670 ::: bot acc: 0.0285
current epoch: 24
train loss is 0.068141
average val loss: 0.079798, accuracy: 0.0799
average test loss: 0.053257, accuracy: 0.0517
case acc: 0.040858638
case acc: 0.08455536
case acc: 0.05682762
case acc: 0.0410103
case acc: 0.04433063
case acc: 0.042907227
top acc: 0.0599 ::: bot acc: 0.0282
top acc: 0.1459 ::: bot acc: 0.0215
top acc: 0.1089 ::: bot acc: 0.0318
top acc: 0.0789 ::: bot acc: 0.0110
top acc: 0.0755 ::: bot acc: 0.0152
top acc: 0.0717 ::: bot acc: 0.0251
current epoch: 25
train loss is 0.069185
average val loss: 0.077615, accuracy: 0.0776
average test loss: 0.051655, accuracy: 0.0502
case acc: 0.03917105
case acc: 0.080424614
case acc: 0.05608504
case acc: 0.039548498
case acc: 0.043921735
case acc: 0.04184933
top acc: 0.0577 ::: bot acc: 0.0274
top acc: 0.1409 ::: bot acc: 0.0191
top acc: 0.1068 ::: bot acc: 0.0333
top acc: 0.0769 ::: bot acc: 0.0108
top acc: 0.0751 ::: bot acc: 0.0147
top acc: 0.0692 ::: bot acc: 0.0272
current epoch: 26
train loss is 0.068154
average val loss: 0.072371, accuracy: 0.0723
average test loss: 0.048036, accuracy: 0.0466
case acc: 0.034775116
case acc: 0.07350765
case acc: 0.054294735
case acc: 0.03627806
case acc: 0.041098595
case acc: 0.039830066
top acc: 0.0517 ::: bot acc: 0.0261
top acc: 0.1315 ::: bot acc: 0.0171
top acc: 0.1004 ::: bot acc: 0.0396
top acc: 0.0714 ::: bot acc: 0.0119
top acc: 0.0722 ::: bot acc: 0.0122
top acc: 0.0642 ::: bot acc: 0.0312
current epoch: 27
train loss is 0.066376
average val loss: 0.064148, accuracy: 0.0641
average test loss: 0.043060, accuracy: 0.0420
case acc: 0.02863184
case acc: 0.06543031
case acc: 0.05235956
case acc: 0.03293517
case acc: 0.035427704
case acc: 0.037345927
top acc: 0.0416 ::: bot acc: 0.0279
top acc: 0.1178 ::: bot acc: 0.0204
top acc: 0.0894 ::: bot acc: 0.0506
top acc: 0.0628 ::: bot acc: 0.0187
top acc: 0.0653 ::: bot acc: 0.0087
top acc: 0.0567 ::: bot acc: 0.0383
current epoch: 28
train loss is 0.062664
average val loss: 0.054505, accuracy: 0.0546
average test loss: 0.038732, accuracy: 0.0384
case acc: 0.025002439
case acc: 0.058082994
case acc: 0.051807154
case acc: 0.0314393
case acc: 0.028578596
case acc: 0.035432164
top acc: 0.0284 ::: bot acc: 0.0404
top acc: 0.1013 ::: bot acc: 0.0315
top acc: 0.0754 ::: bot acc: 0.0648
top acc: 0.0513 ::: bot acc: 0.0301
top acc: 0.0547 ::: bot acc: 0.0093
top acc: 0.0467 ::: bot acc: 0.0483
current epoch: 29
train loss is 0.059257
average val loss: 0.047676, accuracy: 0.0480
average test loss: 0.037206, accuracy: 0.0373
case acc: 0.025792941
case acc: 0.05422247
case acc: 0.052947033
case acc: 0.031425156
case acc: 0.024270914
case acc: 0.034927413
top acc: 0.0170 ::: bot acc: 0.0517
top acc: 0.0875 ::: bot acc: 0.0452
top acc: 0.0637 ::: bot acc: 0.0766
top acc: 0.0418 ::: bot acc: 0.0397
top acc: 0.0443 ::: bot acc: 0.0168
top acc: 0.0381 ::: bot acc: 0.0568
current epoch: 30
train loss is 0.057563
average val loss: 0.042702, accuracy: 0.0432
average test loss: 0.037693, accuracy: 0.0379
case acc: 0.02996693
case acc: 0.051839896
case acc: 0.05541994
case acc: 0.032313365
case acc: 0.022162147
case acc: 0.035902392
top acc: 0.0085 ::: bot acc: 0.0624
top acc: 0.0753 ::: bot acc: 0.0573
top acc: 0.0532 ::: bot acc: 0.0870
top acc: 0.0327 ::: bot acc: 0.0487
top acc: 0.0327 ::: bot acc: 0.0284
top acc: 0.0293 ::: bot acc: 0.0656
current epoch: 31
train loss is 0.057149
average val loss: 0.039162, accuracy: 0.0398
average test loss: 0.040737, accuracy: 0.0414
case acc: 0.03858338
case acc: 0.050552428
case acc: 0.05976635
case acc: 0.035118155
case acc: 0.025622476
case acc: 0.03892919
top acc: 0.0102 ::: bot acc: 0.0745
top acc: 0.0629 ::: bot acc: 0.0698
top acc: 0.0437 ::: bot acc: 0.0984
top acc: 0.0221 ::: bot acc: 0.0594
top acc: 0.0195 ::: bot acc: 0.0427
top acc: 0.0185 ::: bot acc: 0.0766
current epoch: 32
train loss is 0.057445
average val loss: 0.038293, accuracy: 0.0390
average test loss: 0.043828, accuracy: 0.0448
case acc: 0.044793535
case acc: 0.050429273
case acc: 0.06266672
case acc: 0.03746974
case acc: 0.031304486
case acc: 0.041836355
top acc: 0.0146 ::: bot acc: 0.0816
top acc: 0.0558 ::: bot acc: 0.0769
top acc: 0.0397 ::: bot acc: 0.1047
top acc: 0.0163 ::: bot acc: 0.0658
top acc: 0.0164 ::: bot acc: 0.0528
top acc: 0.0134 ::: bot acc: 0.0836
current epoch: 33
train loss is 0.057860
average val loss: 0.038291, accuracy: 0.0391
average test loss: 0.044563, accuracy: 0.0455
case acc: 0.045679577
case acc: 0.050437436
case acc: 0.06290636
case acc: 0.03787002
case acc: 0.033626497
case acc: 0.04263247
top acc: 0.0153 ::: bot acc: 0.0825
top acc: 0.0549 ::: bot acc: 0.0778
top acc: 0.0395 ::: bot acc: 0.1052
top acc: 0.0155 ::: bot acc: 0.0668
top acc: 0.0161 ::: bot acc: 0.0564
top acc: 0.0125 ::: bot acc: 0.0852
current epoch: 34
train loss is 0.057579
average val loss: 0.038372, accuracy: 0.0391
average test loss: 0.044132, accuracy: 0.0451
case acc: 0.044343736
case acc: 0.05043528
case acc: 0.062144324
case acc: 0.037416976
case acc: 0.034015153
case acc: 0.04226042
top acc: 0.0142 ::: bot acc: 0.0811
top acc: 0.0563 ::: bot acc: 0.0764
top acc: 0.0404 ::: bot acc: 0.1036
top acc: 0.0164 ::: bot acc: 0.0657
top acc: 0.0161 ::: bot acc: 0.0570
top acc: 0.0129 ::: bot acc: 0.0845
current epoch: 35
train loss is 0.057432
average val loss: 0.038612, accuracy: 0.0392
average test loss: 0.042826, accuracy: 0.0437
case acc: 0.04121876
case acc: 0.05046252
case acc: 0.060575277
case acc: 0.036380574
case acc: 0.03260557
case acc: 0.041074645
top acc: 0.0118 ::: bot acc: 0.0776
top acc: 0.0596 ::: bot acc: 0.0732
top acc: 0.0424 ::: bot acc: 0.1002
top acc: 0.0188 ::: bot acc: 0.0629
top acc: 0.0162 ::: bot acc: 0.0549
top acc: 0.0143 ::: bot acc: 0.0820
current epoch: 36
train loss is 0.057101
average val loss: 0.039000, accuracy: 0.0394
average test loss: 0.041655, accuracy: 0.0424
case acc: 0.038338337
case acc: 0.05058145
case acc: 0.059137404
case acc: 0.03543695
case acc: 0.031038424
case acc: 0.04012737
top acc: 0.0100 ::: bot acc: 0.0742
top acc: 0.0629 ::: bot acc: 0.0699
top acc: 0.0448 ::: bot acc: 0.0969
top acc: 0.0211 ::: bot acc: 0.0604
top acc: 0.0165 ::: bot acc: 0.0524
top acc: 0.0161 ::: bot acc: 0.0797
current epoch: 37
train loss is 0.057012
average val loss: 0.039532, accuracy: 0.0399
average test loss: 0.040689, accuracy: 0.0413
case acc: 0.035729244
case acc: 0.05078519
case acc: 0.05784151
case acc: 0.03460034
case acc: 0.02955601
case acc: 0.03937026
top acc: 0.0086 ::: bot acc: 0.0709
top acc: 0.0661 ::: bot acc: 0.0666
top acc: 0.0474 ::: bot acc: 0.0936
top acc: 0.0233 ::: bot acc: 0.0580
top acc: 0.0168 ::: bot acc: 0.0500
top acc: 0.0177 ::: bot acc: 0.0778
current epoch: 38
train loss is 0.056749
average val loss: 0.040455, accuracy: 0.0407
average test loss: 0.039567, accuracy: 0.0399
case acc: 0.03267765
case acc: 0.051208958
case acc: 0.056148447
case acc: 0.033617515
case acc: 0.027729083
case acc: 0.038302496
top acc: 0.0078 ::: bot acc: 0.0668
top acc: 0.0705 ::: bot acc: 0.0623
top acc: 0.0511 ::: bot acc: 0.0892
top acc: 0.0268 ::: bot acc: 0.0546
top acc: 0.0175 ::: bot acc: 0.0469
top acc: 0.0205 ::: bot acc: 0.0748
current epoch: 39
train loss is 0.056630
average val loss: 0.042321, accuracy: 0.0425
average test loss: 0.038351, accuracy: 0.0384
case acc: 0.029214479
case acc: 0.052162256
case acc: 0.054265488
case acc: 0.03233591
case acc: 0.02528229
case acc: 0.0369462
top acc: 0.0092 ::: bot acc: 0.0608
top acc: 0.0770 ::: bot acc: 0.0558
top acc: 0.0575 ::: bot acc: 0.0827
top acc: 0.0321 ::: bot acc: 0.0492
top acc: 0.0200 ::: bot acc: 0.0420
top acc: 0.0251 ::: bot acc: 0.0700
current epoch: 40
train loss is 0.056709
average val loss: 0.044619, accuracy: 0.0448
average test loss: 0.037743, accuracy: 0.0374
case acc: 0.02691551
case acc: 0.05343203
case acc: 0.052914523
case acc: 0.031669285
case acc: 0.02345758
case acc: 0.03593643
top acc: 0.0136 ::: bot acc: 0.0552
top acc: 0.0836 ::: bot acc: 0.0492
top acc: 0.0640 ::: bot acc: 0.0763
top acc: 0.0373 ::: bot acc: 0.0440
top acc: 0.0235 ::: bot acc: 0.0375
top acc: 0.0295 ::: bot acc: 0.0655
current epoch: 41
train loss is 0.057057
average val loss: 0.047492, accuracy: 0.0476
average test loss: 0.037697, accuracy: 0.0369
case acc: 0.025388569
case acc: 0.05499331
case acc: 0.052077234
case acc: 0.031344738
case acc: 0.02238909
case acc: 0.035148095
top acc: 0.0193 ::: bot acc: 0.0495
top acc: 0.0907 ::: bot acc: 0.0421
top acc: 0.0711 ::: bot acc: 0.0693
top acc: 0.0426 ::: bot acc: 0.0387
top acc: 0.0279 ::: bot acc: 0.0330
top acc: 0.0338 ::: bot acc: 0.0613
current epoch: 42
train loss is 0.057610
average val loss: 0.052483, accuracy: 0.0526
average test loss: 0.038667, accuracy: 0.0372
case acc: 0.024903204
case acc: 0.0579084
case acc: 0.051907733
case acc: 0.031378545
case acc: 0.022266831
case acc: 0.034865472
top acc: 0.0277 ::: bot acc: 0.0411
top acc: 0.1007 ::: bot acc: 0.0321
top acc: 0.0812 ::: bot acc: 0.0592
top acc: 0.0505 ::: bot acc: 0.0308
top acc: 0.0348 ::: bot acc: 0.0262
top acc: 0.0401 ::: bot acc: 0.0548
current epoch: 43
train loss is 0.058704
average val loss: 0.059547, accuracy: 0.0597
average test loss: 0.041428, accuracy: 0.0391
case acc: 0.027027585
case acc: 0.06291936
case acc: 0.052969124
case acc: 0.032377012
case acc: 0.023938911
case acc: 0.035456236
top acc: 0.0378 ::: bot acc: 0.0311
top acc: 0.1130 ::: bot acc: 0.0226
top acc: 0.0935 ::: bot acc: 0.0469
top acc: 0.0599 ::: bot acc: 0.0214
top acc: 0.0433 ::: bot acc: 0.0177
top acc: 0.0475 ::: bot acc: 0.0473
current epoch: 44
train loss is 0.060501
average val loss: 0.072467, accuracy: 0.0726
average test loss: 0.048541, accuracy: 0.0458
case acc: 0.03599352
case acc: 0.07332932
case acc: 0.058143165
case acc: 0.038325004
case acc: 0.0304574
case acc: 0.038430322
top acc: 0.0537 ::: bot acc: 0.0261
top acc: 0.1313 ::: bot acc: 0.0170
top acc: 0.1120 ::: bot acc: 0.0295
top acc: 0.0750 ::: bot acc: 0.0105
top acc: 0.0580 ::: bot acc: 0.0083
top acc: 0.0603 ::: bot acc: 0.0346
current epoch: 45
train loss is 0.064422
average val loss: 0.088497, accuracy: 0.0884
average test loss: 0.060011, accuracy: 0.0581
case acc: 0.0499396
case acc: 0.08902796
case acc: 0.06913958
case acc: 0.051444083
case acc: 0.044657655
case acc: 0.044515476
top acc: 0.0714 ::: bot acc: 0.0325
top acc: 0.1511 ::: bot acc: 0.0245
top acc: 0.1323 ::: bot acc: 0.0217
top acc: 0.0919 ::: bot acc: 0.0162
top acc: 0.0758 ::: bot acc: 0.0153
top acc: 0.0748 ::: bot acc: 0.0239
current epoch: 46
train loss is 0.070060
average val loss: 0.102123, accuracy: 0.1019
average test loss: 0.071032, accuracy: 0.0698
case acc: 0.061188143
case acc: 0.10200128
case acc: 0.08026336
case acc: 0.063194536
case acc: 0.06138926
case acc: 0.050663505
top acc: 0.0854 ::: bot acc: 0.0381
top acc: 0.1657 ::: bot acc: 0.0342
top acc: 0.1482 ::: bot acc: 0.0234
top acc: 0.1050 ::: bot acc: 0.0252
top acc: 0.0928 ::: bot acc: 0.0316
top acc: 0.0865 ::: bot acc: 0.0191
current epoch: 47
train loss is 0.079030
average val loss: 0.098826, accuracy: 0.0987
average test loss: 0.068163, accuracy: 0.0671
case acc: 0.05804215
case acc: 0.09541875
case acc: 0.0768569
case acc: 0.059679348
case acc: 0.0639265
case acc: 0.04875217
top acc: 0.0815 ::: bot acc: 0.0365
top acc: 0.1584 ::: bot acc: 0.0290
top acc: 0.1437 ::: bot acc: 0.0222
top acc: 0.1012 ::: bot acc: 0.0224
top acc: 0.0953 ::: bot acc: 0.0342
top acc: 0.0831 ::: bot acc: 0.0200
current epoch: 48
train loss is 0.080657
average val loss: 0.072428, accuracy: 0.0723
average test loss: 0.048141, accuracy: 0.0465
case acc: 0.03436825
case acc: 0.067512825
case acc: 0.057302747
case acc: 0.03763334
case acc: 0.04367113
case acc: 0.038446113
top acc: 0.0512 ::: bot acc: 0.0258
top acc: 0.1217 ::: bot acc: 0.0188
top acc: 0.1101 ::: bot acc: 0.0306
top acc: 0.0740 ::: bot acc: 0.0109
top acc: 0.0748 ::: bot acc: 0.0145
top acc: 0.0602 ::: bot acc: 0.0351
current epoch: 49
train loss is 0.070022
average val loss: 0.052789, accuracy: 0.0531
average test loss: 0.038222, accuracy: 0.0380
case acc: 0.024866067
case acc: 0.05425709
case acc: 0.051756695
case acc: 0.03150032
case acc: 0.030249417
case acc: 0.035141867
top acc: 0.0241 ::: bot acc: 0.0449
top acc: 0.0875 ::: bot acc: 0.0451
top acc: 0.0782 ::: bot acc: 0.0622
top acc: 0.0516 ::: bot acc: 0.0300
top acc: 0.0576 ::: bot acc: 0.0087
top acc: 0.0438 ::: bot acc: 0.0512
current epoch: 50
train loss is 0.059808
average val loss: 0.039279, accuracy: 0.0402
average test loss: 0.041379, accuracy: 0.0415
case acc: 0.04220013
case acc: 0.050626032
case acc: 0.06061511
case acc: 0.034822974
case acc: 0.022219956
case acc: 0.038269
top acc: 0.0125 ::: bot acc: 0.0788
top acc: 0.0480 ::: bot acc: 0.0846
top acc: 0.0426 ::: bot acc: 0.1003
top acc: 0.0226 ::: bot acc: 0.0589
top acc: 0.0305 ::: bot acc: 0.0306
top acc: 0.0202 ::: bot acc: 0.0747
LME_Co_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5352 5352 5352
1.7082474 -0.6288155 0.29835227 -0.25048217
Validation: 600 600 600
Testing: 744 744 744
pre-processing time: 0.00037741661071777344
the split date is 2011-01-01
net initializing with time: 0.0033915042877197266
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.167073
average val loss: 0.081461, accuracy: 0.0814
average test loss: 0.090881, accuracy: 0.0858
case acc: 0.058340285
case acc: 0.04502758
case acc: 0.048782375
case acc: 0.2371919
case acc: 0.060662463
case acc: 0.06493585
top acc: 0.1016 ::: bot acc: 0.0288
top acc: 0.0239 ::: bot acc: 0.0792
top acc: 0.0694 ::: bot acc: 0.0599
top acc: 0.2772 ::: bot acc: 0.2006
top acc: 0.0479 ::: bot acc: 0.0976
top acc: 0.0649 ::: bot acc: 0.0927
current epoch: 2
train loss is 0.134872
average val loss: 0.108291, accuracy: 0.1074
average test loss: 0.111867, accuracy: 0.1040
case acc: 0.05855734
case acc: 0.10524241
case acc: 0.103357755
case acc: 0.14191705
case acc: 0.10713876
case acc: 0.10798011
top acc: 0.0179 ::: bot acc: 0.1148
top acc: 0.0558 ::: bot acc: 0.1528
top acc: 0.0520 ::: bot acc: 0.1528
top acc: 0.1831 ::: bot acc: 0.1044
top acc: 0.0381 ::: bot acc: 0.1717
top acc: 0.0447 ::: bot acc: 0.1676
current epoch: 3
train loss is 0.124344
average val loss: 0.093883, accuracy: 0.0929
average test loss: 0.099434, accuracy: 0.0928
case acc: 0.052088037
case acc: 0.0839922
case acc: 0.096142255
case acc: 0.14854808
case acc: 0.08694257
case acc: 0.08932937
top acc: 0.0279 ::: bot acc: 0.0997
top acc: 0.0361 ::: bot acc: 0.1304
top acc: 0.0483 ::: bot acc: 0.1440
top acc: 0.1906 ::: bot acc: 0.1105
top acc: 0.0325 ::: bot acc: 0.1440
top acc: 0.0429 ::: bot acc: 0.1407
current epoch: 4
train loss is 0.107619
average val loss: 0.085673, accuracy: 0.0843
average test loss: 0.092460, accuracy: 0.0868
case acc: 0.05056872
case acc: 0.071628205
case acc: 0.09660817
case acc: 0.14548148
case acc: 0.07667334
case acc: 0.0796682
top acc: 0.0325 ::: bot acc: 0.0942
top acc: 0.0265 ::: bot acc: 0.1165
top acc: 0.0487 ::: bot acc: 0.1448
top acc: 0.1880 ::: bot acc: 0.1070
top acc: 0.0353 ::: bot acc: 0.1271
top acc: 0.0464 ::: bot acc: 0.1247
current epoch: 5
train loss is 0.099442
average val loss: 0.082966, accuracy: 0.0815
average test loss: 0.089860, accuracy: 0.0843
case acc: 0.05166199
case acc: 0.067216046
case acc: 0.10408358
case acc: 0.13273892
case acc: 0.07333641
case acc: 0.07694736
top acc: 0.0283 ::: bot acc: 0.0987
top acc: 0.0237 ::: bot acc: 0.1112
top acc: 0.0526 ::: bot acc: 0.1541
top acc: 0.1755 ::: bot acc: 0.0940
top acc: 0.0372 ::: bot acc: 0.1210
top acc: 0.0489 ::: bot acc: 0.1194
current epoch: 6
train loss is 0.096976
average val loss: 0.082802, accuracy: 0.0813
average test loss: 0.089270, accuracy: 0.0836
case acc: 0.05479403
case acc: 0.066404946
case acc: 0.11412004
case acc: 0.11613307
case acc: 0.072934076
case acc: 0.07695795
top acc: 0.0214 ::: bot acc: 0.1068
top acc: 0.0233 ::: bot acc: 0.1101
top acc: 0.0578 ::: bot acc: 0.1666
top acc: 0.1593 ::: bot acc: 0.0771
top acc: 0.0375 ::: bot acc: 0.1201
top acc: 0.0491 ::: bot acc: 0.1194
current epoch: 7
train loss is 0.095314
average val loss: 0.088572, accuracy: 0.0871
average test loss: 0.093313, accuracy: 0.0869
case acc: 0.06387856
case acc: 0.07463116
case acc: 0.13117537
case acc: 0.09115465
case acc: 0.07821383
case acc: 0.0822794
top acc: 0.0148 ::: bot acc: 0.1235
top acc: 0.0293 ::: bot acc: 0.1193
top acc: 0.0673 ::: bot acc: 0.1876
top acc: 0.1343 ::: bot acc: 0.0525
top acc: 0.0345 ::: bot acc: 0.1294
top acc: 0.0452 ::: bot acc: 0.1294
current epoch: 8
train loss is 0.096911
average val loss: 0.094147, accuracy: 0.0927
average test loss: 0.097608, accuracy: 0.0908
case acc: 0.07461792
case acc: 0.08222995
case acc: 0.14696838
case acc: 0.06904682
case acc: 0.083588645
case acc: 0.08821941
top acc: 0.0177 ::: bot acc: 0.1380
top acc: 0.0356 ::: bot acc: 0.1275
top acc: 0.0780 ::: bot acc: 0.2060
top acc: 0.1111 ::: bot acc: 0.0328
top acc: 0.0320 ::: bot acc: 0.1386
top acc: 0.0433 ::: bot acc: 0.1392
current epoch: 9
train loss is 0.100448
average val loss: 0.093843, accuracy: 0.0924
average test loss: 0.097162, accuracy: 0.0902
case acc: 0.07801322
case acc: 0.079202674
case acc: 0.15290977
case acc: 0.05747794
case acc: 0.084294565
case acc: 0.089406855
top acc: 0.0199 ::: bot acc: 0.1420
top acc: 0.0332 ::: bot acc: 0.1241
top acc: 0.0823 ::: bot acc: 0.2128
top acc: 0.0982 ::: bot acc: 0.0239
top acc: 0.0318 ::: bot acc: 0.1397
top acc: 0.0433 ::: bot acc: 0.1410
current epoch: 10
train loss is 0.104110
average val loss: 0.083283, accuracy: 0.0821
average test loss: 0.088263, accuracy: 0.0816
case acc: 0.06765811
case acc: 0.05986486
case acc: 0.14206597
case acc: 0.06104369
case acc: 0.07682787
case acc: 0.082084805
top acc: 0.0149 ::: bot acc: 0.1290
top acc: 0.0199 ::: bot acc: 0.1018
top acc: 0.0743 ::: bot acc: 0.2006
top acc: 0.1022 ::: bot acc: 0.0266
top acc: 0.0353 ::: bot acc: 0.1268
top acc: 0.0457 ::: bot acc: 0.1288
current epoch: 11
train loss is 0.098101
average val loss: 0.070327, accuracy: 0.0704
average test loss: 0.078011, accuracy: 0.0724
case acc: 0.056148347
case acc: 0.041435488
case acc: 0.12454038
case acc: 0.07096423
case acc: 0.06759898
case acc: 0.073617175
top acc: 0.0200 ::: bot acc: 0.1092
top acc: 0.0242 ::: bot acc: 0.0722
top acc: 0.0636 ::: bot acc: 0.1797
top acc: 0.1131 ::: bot acc: 0.0344
top acc: 0.0417 ::: bot acc: 0.1098
top acc: 0.0524 ::: bot acc: 0.1128
current epoch: 12
train loss is 0.085922
average val loss: 0.063954, accuracy: 0.0645
average test loss: 0.072847, accuracy: 0.0681
case acc: 0.051476926
case acc: 0.035257116
case acc: 0.11374693
case acc: 0.07394545
case acc: 0.06381793
case acc: 0.0701519
top acc: 0.0286 ::: bot acc: 0.0980
top acc: 0.0413 ::: bot acc: 0.0527
top acc: 0.0578 ::: bot acc: 0.1665
top acc: 0.1164 ::: bot acc: 0.0368
top acc: 0.0469 ::: bot acc: 0.1015
top acc: 0.0570 ::: bot acc: 0.1054
current epoch: 13
train loss is 0.077481
average val loss: 0.059779, accuracy: 0.0602
average test loss: 0.069345, accuracy: 0.0655
case acc: 0.049511805
case acc: 0.034738477
case acc: 0.10560967
case acc: 0.07480645
case acc: 0.06110836
case acc: 0.067418434
top acc: 0.0368 ::: bot acc: 0.0897
top acc: 0.0530 ::: bot acc: 0.0409
top acc: 0.0536 ::: bot acc: 0.1564
top acc: 0.1174 ::: bot acc: 0.0375
top acc: 0.0528 ::: bot acc: 0.0945
top acc: 0.0622 ::: bot acc: 0.0987
current epoch: 14
train loss is 0.074658
average val loss: 0.056319, accuracy: 0.0563
average test loss: 0.066476, accuracy: 0.0635
case acc: 0.048713498
case acc: 0.03515974
case acc: 0.0991312
case acc: 0.07470324
case acc: 0.05839575
case acc: 0.06468838
top acc: 0.0433 ::: bot acc: 0.0832
top acc: 0.0584 ::: bot acc: 0.0356
top acc: 0.0503 ::: bot acc: 0.1484
top acc: 0.1174 ::: bot acc: 0.0373
top acc: 0.0599 ::: bot acc: 0.0868
top acc: 0.0696 ::: bot acc: 0.0908
current epoch: 15
train loss is 0.073602
average val loss: 0.056075, accuracy: 0.0560
average test loss: 0.066118, accuracy: 0.0625
case acc: 0.04927142
case acc: 0.034717616
case acc: 0.102787755
case acc: 0.06408494
case acc: 0.059133288
case acc: 0.06516631
top acc: 0.0377 ::: bot acc: 0.0887
top acc: 0.0479 ::: bot acc: 0.0461
top acc: 0.0522 ::: bot acc: 0.1530
top acc: 0.1059 ::: bot acc: 0.0287
top acc: 0.0578 ::: bot acc: 0.0890
top acc: 0.0683 ::: bot acc: 0.0922
current epoch: 16
train loss is 0.074059
average val loss: 0.059130, accuracy: 0.0584
average test loss: 0.068275, accuracy: 0.0636
case acc: 0.052608233
case acc: 0.03852317
case acc: 0.1127266
case acc: 0.047907785
case acc: 0.06202
case acc: 0.06769493
top acc: 0.0256 ::: bot acc: 0.1012
top acc: 0.0294 ::: bot acc: 0.0652
top acc: 0.0573 ::: bot acc: 0.1653
top acc: 0.0874 ::: bot acc: 0.0172
top acc: 0.0507 ::: bot acc: 0.0968
top acc: 0.0618 ::: bot acc: 0.0992
current epoch: 17
train loss is 0.074344
average val loss: 0.067924, accuracy: 0.0662
average test loss: 0.075325, accuracy: 0.0696
case acc: 0.062056713
case acc: 0.051702473
case acc: 0.12886117
case acc: 0.032417405
case acc: 0.06853072
case acc: 0.07395966
top acc: 0.0153 ::: bot acc: 0.1204
top acc: 0.0183 ::: bot acc: 0.0904
top acc: 0.0659 ::: bot acc: 0.1853
top acc: 0.0620 ::: bot acc: 0.0215
top acc: 0.0409 ::: bot acc: 0.1113
top acc: 0.0523 ::: bot acc: 0.1133
current epoch: 18
train loss is 0.079925
average val loss: 0.079484, accuracy: 0.0781
average test loss: 0.085418, accuracy: 0.0800
case acc: 0.07535547
case acc: 0.067562334
case acc: 0.14595279
case acc: 0.03188761
case acc: 0.076968886
case acc: 0.082184985
top acc: 0.0177 ::: bot acc: 0.1391
top acc: 0.0244 ::: bot acc: 0.1110
top acc: 0.0772 ::: bot acc: 0.2053
top acc: 0.0369 ::: bot acc: 0.0472
top acc: 0.0352 ::: bot acc: 0.1267
top acc: 0.0456 ::: bot acc: 0.1289
current epoch: 19
train loss is 0.089527
average val loss: 0.073060, accuracy: 0.0718
average test loss: 0.080105, accuracy: 0.0749
case acc: 0.06937584
case acc: 0.057679188
case acc: 0.13972126
case acc: 0.03185311
case acc: 0.07236349
case acc: 0.07813858
top acc: 0.0150 ::: bot acc: 0.1315
top acc: 0.0192 ::: bot acc: 0.0988
top acc: 0.0726 ::: bot acc: 0.1983
top acc: 0.0369 ::: bot acc: 0.0472
top acc: 0.0380 ::: bot acc: 0.1184
top acc: 0.0483 ::: bot acc: 0.1215
current epoch: 20
train loss is 0.088158
average val loss: 0.057355, accuracy: 0.0565
average test loss: 0.066954, accuracy: 0.0624
case acc: 0.0553075
case acc: 0.038810086
case acc: 0.11888287
case acc: 0.030594269
case acc: 0.0621582
case acc: 0.068621375
top acc: 0.0211 ::: bot acc: 0.1075
top acc: 0.0287 ::: bot acc: 0.0660
top acc: 0.0605 ::: bot acc: 0.1731
top acc: 0.0530 ::: bot acc: 0.0304
top acc: 0.0505 ::: bot acc: 0.0970
top acc: 0.0599 ::: bot acc: 0.1016
current epoch: 21
train loss is 0.075123
average val loss: 0.052368, accuracy: 0.0520
average test loss: 0.062660, accuracy: 0.0588
case acc: 0.051088903
case acc: 0.03474774
case acc: 0.10914454
case acc: 0.031230412
case acc: 0.059799146
case acc: 0.0666888
top acc: 0.0294 ::: bot acc: 0.0970
top acc: 0.0469 ::: bot acc: 0.0472
top acc: 0.0555 ::: bot acc: 0.1611
top acc: 0.0567 ::: bot acc: 0.0269
top acc: 0.0561 ::: bot acc: 0.0907
top acc: 0.0638 ::: bot acc: 0.0968
current epoch: 22
train loss is 0.066908
average val loss: 0.050158, accuracy: 0.0499
average test loss: 0.060698, accuracy: 0.0574
case acc: 0.049587842
case acc: 0.0349487
case acc: 0.103234224
case acc: 0.03118052
case acc: 0.058999043
case acc: 0.066217184
top acc: 0.0347 ::: bot acc: 0.0915
top acc: 0.0561 ::: bot acc: 0.0379
top acc: 0.0524 ::: bot acc: 0.1537
top acc: 0.0565 ::: bot acc: 0.0273
top acc: 0.0584 ::: bot acc: 0.0884
top acc: 0.0654 ::: bot acc: 0.0953
current epoch: 23
train loss is 0.063202
average val loss: 0.048227, accuracy: 0.0478
average test loss: 0.059094, accuracy: 0.0562
case acc: 0.048925005
case acc: 0.03524248
case acc: 0.098407604
case acc: 0.031082755
case acc: 0.058063895
case acc: 0.06529724
top acc: 0.0389 ::: bot acc: 0.0873
top acc: 0.0596 ::: bot acc: 0.0344
top acc: 0.0499 ::: bot acc: 0.1478
top acc: 0.0559 ::: bot acc: 0.0280
top acc: 0.0612 ::: bot acc: 0.0854
top acc: 0.0682 ::: bot acc: 0.0924
current epoch: 24
train loss is 0.061994
average val loss: 0.047681, accuracy: 0.0473
average test loss: 0.058863, accuracy: 0.0559
case acc: 0.048922505
case acc: 0.034803018
case acc: 0.097735986
case acc: 0.030479494
case acc: 0.058227018
case acc: 0.06532634
top acc: 0.0383 ::: bot acc: 0.0878
top acc: 0.0550 ::: bot acc: 0.0390
top acc: 0.0495 ::: bot acc: 0.1470
top acc: 0.0510 ::: bot acc: 0.0330
top acc: 0.0610 ::: bot acc: 0.0857
top acc: 0.0684 ::: bot acc: 0.0923
current epoch: 25
train loss is 0.061426
average val loss: 0.048922, accuracy: 0.0488
average test loss: 0.060195, accuracy: 0.0570
case acc: 0.049673814
case acc: 0.034860015
case acc: 0.10087289
case acc: 0.030577147
case acc: 0.059451368
case acc: 0.066323355
top acc: 0.0334 ::: bot acc: 0.0926
top acc: 0.0445 ::: bot acc: 0.0495
top acc: 0.0511 ::: bot acc: 0.1509
top acc: 0.0419 ::: bot acc: 0.0421
top acc: 0.0576 ::: bot acc: 0.0893
top acc: 0.0654 ::: bot acc: 0.0953
current epoch: 26
train loss is 0.061959
average val loss: 0.053162, accuracy: 0.0535
average test loss: 0.064001, accuracy: 0.0609
case acc: 0.05274396
case acc: 0.038019646
case acc: 0.10789542
case acc: 0.035687894
case acc: 0.062168285
case acc: 0.068934195
top acc: 0.0246 ::: bot acc: 0.1019
top acc: 0.0306 ::: bot acc: 0.0637
top acc: 0.0547 ::: bot acc: 0.1597
top acc: 0.0313 ::: bot acc: 0.0559
top acc: 0.0509 ::: bot acc: 0.0967
top acc: 0.0592 ::: bot acc: 0.1023
current epoch: 27
train loss is 0.063313
average val loss: 0.057999, accuracy: 0.0583
average test loss: 0.068197, accuracy: 0.0656
case acc: 0.05638008
case acc: 0.042758923
case acc: 0.11437545
case acc: 0.043487806
case acc: 0.06477016
case acc: 0.071692415
top acc: 0.0190 ::: bot acc: 0.1101
top acc: 0.0227 ::: bot acc: 0.0747
top acc: 0.0580 ::: bot acc: 0.1678
top acc: 0.0284 ::: bot acc: 0.0689
top acc: 0.0457 ::: bot acc: 0.1031
top acc: 0.0546 ::: bot acc: 0.1087
current epoch: 28
train loss is 0.066076
average val loss: 0.062324, accuracy: 0.0626
average test loss: 0.071797, accuracy: 0.0696
case acc: 0.059698705
case acc: 0.045757562
case acc: 0.11934317
case acc: 0.05158807
case acc: 0.06709535
case acc: 0.074383326
top acc: 0.0165 ::: bot acc: 0.1162
top acc: 0.0198 ::: bot acc: 0.0805
top acc: 0.0606 ::: bot acc: 0.1740
top acc: 0.0297 ::: bot acc: 0.0804
top acc: 0.0425 ::: bot acc: 0.1082
top acc: 0.0515 ::: bot acc: 0.1142
current epoch: 29
train loss is 0.068270
average val loss: 0.060307, accuracy: 0.0609
average test loss: 0.070198, accuracy: 0.0684
case acc: 0.058169074
case acc: 0.042638727
case acc: 0.11681112
case acc: 0.053519413
case acc: 0.06579774
case acc: 0.07339473
top acc: 0.0176 ::: bot acc: 0.1134
top acc: 0.0229 ::: bot acc: 0.0744
top acc: 0.0593 ::: bot acc: 0.1708
top acc: 0.0304 ::: bot acc: 0.0830
top acc: 0.0441 ::: bot acc: 0.1054
top acc: 0.0524 ::: bot acc: 0.1123
current epoch: 30
train loss is 0.067669
average val loss: 0.054220, accuracy: 0.0553
average test loss: 0.065214, accuracy: 0.0637
case acc: 0.05371726
case acc: 0.036887232
case acc: 0.10854373
case acc: 0.050253034
case acc: 0.062511496
case acc: 0.07020928
top acc: 0.0233 ::: bot acc: 0.1040
top acc: 0.0342 ::: bot acc: 0.0601
top acc: 0.0551 ::: bot acc: 0.1606
top acc: 0.0292 ::: bot acc: 0.0787
top acc: 0.0499 ::: bot acc: 0.0977
top acc: 0.0564 ::: bot acc: 0.1056
current epoch: 31
train loss is 0.063919
average val loss: 0.047677, accuracy: 0.0488
average test loss: 0.059747, accuracy: 0.0586
case acc: 0.049617257
case acc: 0.03473024
case acc: 0.0973793
case acc: 0.04446717
case acc: 0.058888476
case acc: 0.066694535
top acc: 0.0348 ::: bot acc: 0.0915
top acc: 0.0511 ::: bot acc: 0.0432
top acc: 0.0493 ::: bot acc: 0.1468
top acc: 0.0281 ::: bot acc: 0.0706
top acc: 0.0585 ::: bot acc: 0.0880
top acc: 0.0636 ::: bot acc: 0.0968
current epoch: 32
train loss is 0.059434
average val loss: 0.043464, accuracy: 0.0441
average test loss: 0.056010, accuracy: 0.0555
case acc: 0.048527967
case acc: 0.036082342
case acc: 0.08738793
case acc: 0.039819017
case acc: 0.0568321
case acc: 0.06428831
top acc: 0.0456 ::: bot acc: 0.0808
top acc: 0.0641 ::: bot acc: 0.0303
top acc: 0.0446 ::: bot acc: 0.1342
top acc: 0.0291 ::: bot acc: 0.0631
top acc: 0.0661 ::: bot acc: 0.0803
top acc: 0.0707 ::: bot acc: 0.0895
current epoch: 33
train loss is 0.056755
average val loss: 0.040493, accuracy: 0.0406
average test loss: 0.053171, accuracy: 0.0531
case acc: 0.048407987
case acc: 0.0391177
case acc: 0.07810976
case acc: 0.035167806
case acc: 0.055476174
case acc: 0.062022742
top acc: 0.0559 ::: bot acc: 0.0705
top acc: 0.0735 ::: bot acc: 0.0218
top acc: 0.0418 ::: bot acc: 0.1217
top acc: 0.0315 ::: bot acc: 0.0550
top acc: 0.0736 ::: bot acc: 0.0728
top acc: 0.0786 ::: bot acc: 0.0817
current epoch: 34
train loss is 0.055504
average val loss: 0.038752, accuracy: 0.0384
average test loss: 0.051286, accuracy: 0.0513
case acc: 0.0489211
case acc: 0.04190541
case acc: 0.07052527
case acc: 0.03172113
case acc: 0.05463454
case acc: 0.06025211
top acc: 0.0652 ::: bot acc: 0.0612
top acc: 0.0792 ::: bot acc: 0.0188
top acc: 0.0423 ::: bot acc: 0.1101
top acc: 0.0372 ::: bot acc: 0.0469
top acc: 0.0807 ::: bot acc: 0.0658
top acc: 0.0864 ::: bot acc: 0.0740
current epoch: 35
train loss is 0.054956
average val loss: 0.038017, accuracy: 0.0375
average test loss: 0.050435, accuracy: 0.0504
case acc: 0.049347945
case acc: 0.04180842
case acc: 0.06648831
case acc: 0.030687
case acc: 0.054438144
case acc: 0.059526574
top acc: 0.0700 ::: bot acc: 0.0563
top acc: 0.0790 ::: bot acc: 0.0188
top acc: 0.0446 ::: bot acc: 0.1030
top acc: 0.0416 ::: bot acc: 0.0426
top acc: 0.0839 ::: bot acc: 0.0626
top acc: 0.0908 ::: bot acc: 0.0696
current epoch: 36
train loss is 0.054574
average val loss: 0.037522, accuracy: 0.0370
average test loss: 0.049965, accuracy: 0.0497
case acc: 0.049508844
case acc: 0.040239017
case acc: 0.06426289
case acc: 0.030357836
case acc: 0.054387912
case acc: 0.059328012
top acc: 0.0719 ::: bot acc: 0.0543
top acc: 0.0759 ::: bot acc: 0.0203
top acc: 0.0463 ::: bot acc: 0.0988
top acc: 0.0435 ::: bot acc: 0.0408
top acc: 0.0849 ::: bot acc: 0.0616
top acc: 0.0931 ::: bot acc: 0.0673
current epoch: 37
train loss is 0.054199
average val loss: 0.037171, accuracy: 0.0367
average test loss: 0.049919, accuracy: 0.0493
case acc: 0.049337257
case acc: 0.037529
case acc: 0.064178064
case acc: 0.030757565
case acc: 0.054522425
case acc: 0.05938713
top acc: 0.0701 ::: bot acc: 0.0561
top acc: 0.0696 ::: bot acc: 0.0249
top acc: 0.0463 ::: bot acc: 0.0987
top acc: 0.0415 ::: bot acc: 0.0428
top acc: 0.0829 ::: bot acc: 0.0636
top acc: 0.0923 ::: bot acc: 0.0681
current epoch: 38
train loss is 0.053726
average val loss: 0.037404, accuracy: 0.0373
average test loss: 0.050672, accuracy: 0.0497
case acc: 0.04877823
case acc: 0.035469767
case acc: 0.06645401
case acc: 0.032593187
case acc: 0.05496319
case acc: 0.059953112
top acc: 0.0642 ::: bot acc: 0.0621
top acc: 0.0608 ::: bot acc: 0.0338
top acc: 0.0446 ::: bot acc: 0.1030
top acc: 0.0352 ::: bot acc: 0.0493
top acc: 0.0775 ::: bot acc: 0.0690
top acc: 0.0878 ::: bot acc: 0.0725
current epoch: 39
train loss is 0.053544
average val loss: 0.038568, accuracy: 0.0388
average test loss: 0.052194, accuracy: 0.0512
case acc: 0.048372526
case acc: 0.03481028
case acc: 0.069898635
case acc: 0.036858033
case acc: 0.05592365
case acc: 0.06113458
top acc: 0.0569 ::: bot acc: 0.0694
top acc: 0.0520 ::: bot acc: 0.0426
top acc: 0.0427 ::: bot acc: 0.1091
top acc: 0.0306 ::: bot acc: 0.0579
top acc: 0.0714 ::: bot acc: 0.0752
top acc: 0.0823 ::: bot acc: 0.0781
current epoch: 40
train loss is 0.053450
average val loss: 0.040842, accuracy: 0.0415
average test loss: 0.054433, accuracy: 0.0535
case acc: 0.04837026
case acc: 0.035155926
case acc: 0.07444505
case acc: 0.042909317
case acc: 0.05720795
case acc: 0.06276701
top acc: 0.0488 ::: bot acc: 0.0774
top acc: 0.0435 ::: bot acc: 0.0511
top acc: 0.0416 ::: bot acc: 0.1164
top acc: 0.0285 ::: bot acc: 0.0680
top acc: 0.0650 ::: bot acc: 0.0816
top acc: 0.0762 ::: bot acc: 0.0842
current epoch: 41
train loss is 0.053848
average val loss: 0.044136, accuracy: 0.0449
average test loss: 0.057257, accuracy: 0.0565
case acc: 0.048673436
case acc: 0.036668885
case acc: 0.07984649
case acc: 0.05035521
case acc: 0.05882383
case acc: 0.064614624
top acc: 0.0408 ::: bot acc: 0.0854
top acc: 0.0358 ::: bot acc: 0.0588
top acc: 0.0421 ::: bot acc: 0.1243
top acc: 0.0293 ::: bot acc: 0.0788
top acc: 0.0589 ::: bot acc: 0.0876
top acc: 0.0700 ::: bot acc: 0.0903
current epoch: 42
train loss is 0.054586
average val loss: 0.048558, accuracy: 0.0492
average test loss: 0.060818, accuracy: 0.0604
case acc: 0.04999746
case acc: 0.03908727
case acc: 0.08609194
case acc: 0.05930193
case acc: 0.061040033
case acc: 0.06669765
top acc: 0.0327 ::: bot acc: 0.0936
top acc: 0.0286 ::: bot acc: 0.0662
top acc: 0.0441 ::: bot acc: 0.1326
top acc: 0.0332 ::: bot acc: 0.0902
top acc: 0.0532 ::: bot acc: 0.0937
top acc: 0.0637 ::: bot acc: 0.0967
current epoch: 43
train loss is 0.055457
average val loss: 0.052330, accuracy: 0.0529
average test loss: 0.063790, accuracy: 0.0637
case acc: 0.051787328
case acc: 0.041085005
case acc: 0.09096166
case acc: 0.067146145
case acc: 0.06254127
case acc: 0.06840084
top acc: 0.0270 ::: bot acc: 0.0994
top acc: 0.0249 ::: bot acc: 0.0710
top acc: 0.0463 ::: bot acc: 0.1388
top acc: 0.0377 ::: bot acc: 0.0997
top acc: 0.0497 ::: bot acc: 0.0977
top acc: 0.0597 ::: bot acc: 0.1012
current epoch: 44
train loss is 0.056648
average val loss: 0.056495, accuracy: 0.0571
average test loss: 0.067060, accuracy: 0.0672
case acc: 0.054095622
case acc: 0.042954423
case acc: 0.095849365
case acc: 0.07546155
case acc: 0.064279065
case acc: 0.070377015
top acc: 0.0226 ::: bot acc: 0.1050
top acc: 0.0228 ::: bot acc: 0.0748
top acc: 0.0485 ::: bot acc: 0.1450
top acc: 0.0435 ::: bot acc: 0.1093
top acc: 0.0466 ::: bot acc: 0.1018
top acc: 0.0557 ::: bot acc: 0.1061
current epoch: 45
train loss is 0.057951
average val loss: 0.057711, accuracy: 0.0585
average test loss: 0.068015, accuracy: 0.0683
case acc: 0.05459423
case acc: 0.04233168
case acc: 0.09726999
case acc: 0.08009814
case acc: 0.06447917
case acc: 0.07103937
top acc: 0.0218 ::: bot acc: 0.1061
top acc: 0.0235 ::: bot acc: 0.0736
top acc: 0.0493 ::: bot acc: 0.1468
top acc: 0.0469 ::: bot acc: 0.1146
top acc: 0.0462 ::: bot acc: 0.1023
top acc: 0.0547 ::: bot acc: 0.1077
current epoch: 46
train loss is 0.058239
average val loss: 0.057234, accuracy: 0.0583
average test loss: 0.067636, accuracy: 0.0681
case acc: 0.054004338
case acc: 0.040473234
case acc: 0.09639813
case acc: 0.08235897
case acc: 0.06405554
case acc: 0.07104686
top acc: 0.0227 ::: bot acc: 0.1048
top acc: 0.0259 ::: bot acc: 0.0696
top acc: 0.0488 ::: bot acc: 0.1457
top acc: 0.0486 ::: bot acc: 0.1171
top acc: 0.0470 ::: bot acc: 0.1013
top acc: 0.0547 ::: bot acc: 0.1077
current epoch: 47
train loss is 0.058678
average val loss: 0.052954, accuracy: 0.0543
average test loss: 0.064230, accuracy: 0.0648
case acc: 0.051267892
case acc: 0.03676343
case acc: 0.090712875
case acc: 0.07917275
case acc: 0.06186987
case acc: 0.06929225
top acc: 0.0286 ::: bot acc: 0.0979
top acc: 0.0355 ::: bot acc: 0.0591
top acc: 0.0461 ::: bot acc: 0.1385
top acc: 0.0462 ::: bot acc: 0.1135
top acc: 0.0511 ::: bot acc: 0.0960
top acc: 0.0578 ::: bot acc: 0.1035
current epoch: 48
train loss is 0.057423
average val loss: 0.045994, accuracy: 0.0473
average test loss: 0.058582, accuracy: 0.0597
case acc: 0.048715502
case acc: 0.034815546
case acc: 0.08013496
case acc: 0.07009569
case acc: 0.058352035
case acc: 0.0659153
top acc: 0.0415 ::: bot acc: 0.0850
top acc: 0.0515 ::: bot acc: 0.0431
top acc: 0.0422 ::: bot acc: 0.1246
top acc: 0.0398 ::: bot acc: 0.1031
top acc: 0.0601 ::: bot acc: 0.0863
top acc: 0.0656 ::: bot acc: 0.0945
current epoch: 49
train loss is 0.056125
average val loss: 0.039358, accuracy: 0.0401
average test loss: 0.052675, accuracy: 0.0545
case acc: 0.04862112
case acc: 0.03840099
case acc: 0.06709973
case acc: 0.05549663
case acc: 0.05540713
case acc: 0.061805233
top acc: 0.0602 ::: bot acc: 0.0664
top acc: 0.0720 ::: bot acc: 0.0229
top acc: 0.0443 ::: bot acc: 0.1041
top acc: 0.0313 ::: bot acc: 0.0855
top acc: 0.0740 ::: bot acc: 0.0723
top acc: 0.0793 ::: bot acc: 0.0809
current epoch: 50
train loss is 0.055703
average val loss: 0.038539, accuracy: 0.0383
average test loss: 0.049802, accuracy: 0.0518
case acc: 0.052203253
case acc: 0.054597903
case acc: 0.053666364
case acc: 0.036955886
case acc: 0.05404774
case acc: 0.059114475
top acc: 0.0863 ::: bot acc: 0.0402
top acc: 0.0977 ::: bot acc: 0.0199
top acc: 0.0627 ::: bot acc: 0.0749
top acc: 0.0304 ::: bot acc: 0.0582
top acc: 0.0941 ::: bot acc: 0.0523
top acc: 0.1000 ::: bot acc: 0.0602
LME_Co_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5328 5328 5328
1.7082474 -0.6288155 0.29835227 -0.25048217
Validation: 594 594 594
Testing: 774 774 774
pre-processing time: 0.002232074737548828
the split date is 2011-07-01
net initializing with time: 0.0033004283905029297
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.279754
average val loss: 0.200689, accuracy: 0.2036
average test loss: 0.206793, accuracy: 0.2034
case acc: 0.31740564
case acc: 0.2545362
case acc: 0.30502746
case acc: 0.08182939
case acc: 0.05886789
case acc: 0.20290643
top acc: 0.2644 ::: bot acc: 0.3715
top acc: 0.2094 ::: bot acc: 0.2961
top acc: 0.2408 ::: bot acc: 0.3719
top acc: 0.0336 ::: bot acc: 0.1414
top acc: 0.1101 ::: bot acc: 0.0186
top acc: 0.1404 ::: bot acc: 0.2563
current epoch: 2
train loss is 0.245610
average val loss: 0.121923, accuracy: 0.1182
average test loss: 0.120465, accuracy: 0.1212
case acc: 0.14001483
case acc: 0.07894574
case acc: 0.12964341
case acc: 0.104388185
case acc: 0.22470994
case acc: 0.049364865
top acc: 0.0871 ::: bot acc: 0.1940
top acc: 0.0346 ::: bot acc: 0.1201
top acc: 0.0651 ::: bot acc: 0.1963
top acc: 0.1706 ::: bot acc: 0.0371
top acc: 0.2862 ::: bot acc: 0.1640
top acc: 0.0395 ::: bot acc: 0.0769
current epoch: 3
train loss is 0.140857
average val loss: 0.130567, accuracy: 0.1272
average test loss: 0.132959, accuracy: 0.1324
case acc: 0.188531
case acc: 0.12958382
case acc: 0.17829192
case acc: 0.061437305
case acc: 0.15704295
case acc: 0.079322465
top acc: 0.1356 ::: bot acc: 0.2425
top acc: 0.0849 ::: bot acc: 0.1709
top acc: 0.1135 ::: bot acc: 0.2451
top acc: 0.1109 ::: bot acc: 0.0275
top acc: 0.2187 ::: bot acc: 0.0963
top acc: 0.0238 ::: bot acc: 0.1300
current epoch: 4
train loss is 0.168583
average val loss: 0.112618, accuracy: 0.1090
average test loss: 0.111416, accuracy: 0.1118
case acc: 0.13681725
case acc: 0.08114375
case acc: 0.12730305
case acc: 0.08642028
case acc: 0.18852901
case acc: 0.050814092
top acc: 0.0838 ::: bot acc: 0.1907
top acc: 0.0367 ::: bot acc: 0.1223
top acc: 0.0626 ::: bot acc: 0.1941
top acc: 0.1507 ::: bot acc: 0.0226
top acc: 0.2503 ::: bot acc: 0.1277
top acc: 0.0360 ::: bot acc: 0.0813
current epoch: 5
train loss is 0.129650
average val loss: 0.116063, accuracy: 0.1130
average test loss: 0.117473, accuracy: 0.1171
case acc: 0.16567951
case acc: 0.11262499
case acc: 0.15561676
case acc: 0.06038509
case acc: 0.13986729
case acc: 0.06818588
top acc: 0.1127 ::: bot acc: 0.2195
top acc: 0.0680 ::: bot acc: 0.1539
top acc: 0.0907 ::: bot acc: 0.2226
top acc: 0.1086 ::: bot acc: 0.0284
top acc: 0.2017 ::: bot acc: 0.0790
top acc: 0.0204 ::: bot acc: 0.1154
current epoch: 6
train loss is 0.145691
average val loss: 0.103855, accuracy: 0.1003
average test loss: 0.103195, accuracy: 0.1033
case acc: 0.13536398
case acc: 0.0854562
case acc: 0.12543371
case acc: 0.06904057
case acc: 0.15062933
case acc: 0.053716872
top acc: 0.0823 ::: bot acc: 0.1892
top acc: 0.0409 ::: bot acc: 0.1267
top acc: 0.0607 ::: bot acc: 0.1923
top acc: 0.1263 ::: bot acc: 0.0188
top acc: 0.2126 ::: bot acc: 0.0897
top acc: 0.0300 ::: bot acc: 0.0889
current epoch: 7
train loss is 0.121895
average val loss: 0.102803, accuracy: 0.1000
average test loss: 0.103017, accuracy: 0.1027
case acc: 0.14395623
case acc: 0.09691523
case acc: 0.13320954
case acc: 0.05876414
case acc: 0.12309567
case acc: 0.060148887
top acc: 0.0909 ::: bot acc: 0.1979
top acc: 0.0523 ::: bot acc: 0.1382
top acc: 0.0682 ::: bot acc: 0.2002
top acc: 0.1049 ::: bot acc: 0.0306
top acc: 0.1851 ::: bot acc: 0.0622
top acc: 0.0225 ::: bot acc: 0.1024
current epoch: 8
train loss is 0.124813
average val loss: 0.097917, accuracy: 0.0954
average test loss: 0.097782, accuracy: 0.0973
case acc: 0.13745539
case acc: 0.09359118
case acc: 0.12683871
case acc: 0.05629826
case acc: 0.11085723
case acc: 0.058715843
top acc: 0.0844 ::: bot acc: 0.1914
top acc: 0.0490 ::: bot acc: 0.1349
top acc: 0.0620 ::: bot acc: 0.1938
top acc: 0.0990 ::: bot acc: 0.0349
top acc: 0.1729 ::: bot acc: 0.0499
top acc: 0.0235 ::: bot acc: 0.0999
current epoch: 9
train loss is 0.115658
average val loss: 0.092852, accuracy: 0.0905
average test loss: 0.092215, accuracy: 0.0916
case acc: 0.12975092
case acc: 0.08892759
case acc: 0.11891208
case acc: 0.054638766
case acc: 0.10020878
case acc: 0.05692094
top acc: 0.0767 ::: bot acc: 0.1837
top acc: 0.0446 ::: bot acc: 0.1301
top acc: 0.0542 ::: bot acc: 0.1857
top acc: 0.0945 ::: bot acc: 0.0390
top acc: 0.1622 ::: bot acc: 0.0394
top acc: 0.0252 ::: bot acc: 0.0964
current epoch: 10
train loss is 0.111446
average val loss: 0.089483, accuracy: 0.0878
average test loss: 0.088701, accuracy: 0.0877
case acc: 0.12659602
case acc: 0.088982545
case acc: 0.115789264
case acc: 0.051752422
case acc: 0.08558524
case acc: 0.05735081
top acc: 0.0736 ::: bot acc: 0.1805
top acc: 0.0447 ::: bot acc: 0.1299
top acc: 0.0513 ::: bot acc: 0.1824
top acc: 0.0858 ::: bot acc: 0.0477
top acc: 0.1472 ::: bot acc: 0.0255
top acc: 0.0247 ::: bot acc: 0.0973
current epoch: 11
train loss is 0.105808
average val loss: 0.085576, accuracy: 0.0843
average test loss: 0.084493, accuracy: 0.0834
case acc: 0.121054254
case acc: 0.08673477
case acc: 0.110665016
case acc: 0.05023228
case acc: 0.074789934
case acc: 0.056643084
top acc: 0.0682 ::: bot acc: 0.1749
top acc: 0.0426 ::: bot acc: 0.1276
top acc: 0.0468 ::: bot acc: 0.1769
top acc: 0.0795 ::: bot acc: 0.0539
top acc: 0.1350 ::: bot acc: 0.0178
top acc: 0.0254 ::: bot acc: 0.0960
current epoch: 12
train loss is 0.101067
average val loss: 0.081818, accuracy: 0.0809
average test loss: 0.080492, accuracy: 0.0795
case acc: 0.11530316
case acc: 0.08435529
case acc: 0.105523475
case acc: 0.04933986
case acc: 0.06641049
case acc: 0.05582467
top acc: 0.0624 ::: bot acc: 0.1692
top acc: 0.0403 ::: bot acc: 0.1251
top acc: 0.0425 ::: bot acc: 0.1713
top acc: 0.0738 ::: bot acc: 0.0596
top acc: 0.1231 ::: bot acc: 0.0164
top acc: 0.0264 ::: bot acc: 0.0943
current epoch: 13
train loss is 0.095017
average val loss: 0.079215, accuracy: 0.0789
average test loss: 0.077985, accuracy: 0.0771
case acc: 0.11198329
case acc: 0.08453795
case acc: 0.10302208
case acc: 0.048673455
case acc: 0.058105003
case acc: 0.056057256
top acc: 0.0592 ::: bot acc: 0.1658
top acc: 0.0405 ::: bot acc: 0.1252
top acc: 0.0403 ::: bot acc: 0.1686
top acc: 0.0659 ::: bot acc: 0.0673
top acc: 0.1091 ::: bot acc: 0.0197
top acc: 0.0261 ::: bot acc: 0.0948
current epoch: 14
train loss is 0.091679
average val loss: 0.076582, accuracy: 0.0769
average test loss: 0.075408, accuracy: 0.0746
case acc: 0.10757006
case acc: 0.08367985
case acc: 0.099851966
case acc: 0.048765752
case acc: 0.05206488
case acc: 0.055703178
top acc: 0.0549 ::: bot acc: 0.1614
top acc: 0.0397 ::: bot acc: 0.1243
top acc: 0.0377 ::: bot acc: 0.1650
top acc: 0.0595 ::: bot acc: 0.0738
top acc: 0.0964 ::: bot acc: 0.0270
top acc: 0.0265 ::: bot acc: 0.0941
current epoch: 15
train loss is 0.087945
average val loss: 0.072706, accuracy: 0.0729
average test loss: 0.071019, accuracy: 0.0705
case acc: 0.098997995
case acc: 0.07870624
case acc: 0.093447044
case acc: 0.048911802
case acc: 0.049327046
case acc: 0.053522002
top acc: 0.0466 ::: bot acc: 0.1527
top acc: 0.0350 ::: bot acc: 0.1191
top acc: 0.0339 ::: bot acc: 0.1573
top acc: 0.0577 ::: bot acc: 0.0757
top acc: 0.0882 ::: bot acc: 0.0349
top acc: 0.0297 ::: bot acc: 0.0892
current epoch: 16
train loss is 0.082613
average val loss: 0.071151, accuracy: 0.0719
average test loss: 0.069439, accuracy: 0.0690
case acc: 0.09515976
case acc: 0.0783092
case acc: 0.0912298
case acc: 0.0497122
case acc: 0.046374314
case acc: 0.053448245
top acc: 0.0430 ::: bot acc: 0.1487
top acc: 0.0347 ::: bot acc: 0.1187
top acc: 0.0326 ::: bot acc: 0.1545
top acc: 0.0515 ::: bot acc: 0.0819
top acc: 0.0756 ::: bot acc: 0.0474
top acc: 0.0298 ::: bot acc: 0.0891
current epoch: 17
train loss is 0.079006
average val loss: 0.068620, accuracy: 0.0695
average test loss: 0.066432, accuracy: 0.0662
case acc: 0.08819828
case acc: 0.07474976
case acc: 0.08648317
case acc: 0.050180823
case acc: 0.045474995
case acc: 0.052069813
top acc: 0.0365 ::: bot acc: 0.1415
top acc: 0.0314 ::: bot acc: 0.1150
top acc: 0.0301 ::: bot acc: 0.1486
top acc: 0.0490 ::: bot acc: 0.0844
top acc: 0.0667 ::: bot acc: 0.0563
top acc: 0.0324 ::: bot acc: 0.0857
current epoch: 18
train loss is 0.074498
average val loss: 0.067473, accuracy: 0.0686
average test loss: 0.065062, accuracy: 0.0650
case acc: 0.083636984
case acc: 0.073398106
case acc: 0.08390581
case acc: 0.051247828
case acc: 0.045942467
case acc: 0.05171451
top acc: 0.0323 ::: bot acc: 0.1368
top acc: 0.0303 ::: bot acc: 0.1135
top acc: 0.0288 ::: bot acc: 0.1454
top acc: 0.0446 ::: bot acc: 0.0889
top acc: 0.0559 ::: bot acc: 0.0670
top acc: 0.0330 ::: bot acc: 0.0848
current epoch: 19
train loss is 0.071736
average val loss: 0.066418, accuracy: 0.0676
average test loss: 0.063674, accuracy: 0.0637
case acc: 0.07884067
case acc: 0.071067296
case acc: 0.08126253
case acc: 0.052378096
case acc: 0.047661427
case acc: 0.051209573
top acc: 0.0282 ::: bot acc: 0.1317
top acc: 0.0284 ::: bot acc: 0.1109
top acc: 0.0277 ::: bot acc: 0.1420
top acc: 0.0411 ::: bot acc: 0.0924
top acc: 0.0461 ::: bot acc: 0.0769
top acc: 0.0341 ::: bot acc: 0.0836
current epoch: 20
train loss is 0.069238
average val loss: 0.063668, accuracy: 0.0646
average test loss: 0.060161, accuracy: 0.0603
case acc: 0.07076256
case acc: 0.065146394
case acc: 0.07561268
case acc: 0.052011684
case acc: 0.048843376
case acc: 0.049264543
top acc: 0.0227 ::: bot acc: 0.1223
top acc: 0.0246 ::: bot acc: 0.1039
top acc: 0.0257 ::: bot acc: 0.1344
top acc: 0.0422 ::: bot acc: 0.0913
top acc: 0.0410 ::: bot acc: 0.0819
top acc: 0.0390 ::: bot acc: 0.0782
current epoch: 21
train loss is 0.065164
average val loss: 0.062346, accuracy: 0.0632
average test loss: 0.058463, accuracy: 0.0585
case acc: 0.06556341
case acc: 0.061615027
case acc: 0.072190836
case acc: 0.052386425
case acc: 0.051001858
case acc: 0.048487604
top acc: 0.0197 ::: bot acc: 0.1160
top acc: 0.0225 ::: bot acc: 0.0997
top acc: 0.0246 ::: bot acc: 0.1298
top acc: 0.0410 ::: bot acc: 0.0925
top acc: 0.0339 ::: bot acc: 0.0891
top acc: 0.0414 ::: bot acc: 0.0759
current epoch: 22
train loss is 0.062239
average val loss: 0.060712, accuracy: 0.0614
average test loss: 0.056358, accuracy: 0.0565
case acc: 0.060033023
case acc: 0.05724359
case acc: 0.06855292
case acc: 0.052166842
case acc: 0.05325219
case acc: 0.047497988
top acc: 0.0178 ::: bot acc: 0.1086
top acc: 0.0204 ::: bot acc: 0.0941
top acc: 0.0244 ::: bot acc: 0.1244
top acc: 0.0415 ::: bot acc: 0.0918
top acc: 0.0303 ::: bot acc: 0.0943
top acc: 0.0448 ::: bot acc: 0.0724
current epoch: 23
train loss is 0.059795
average val loss: 0.059239, accuracy: 0.0599
average test loss: 0.054425, accuracy: 0.0545
case acc: 0.05511885
case acc: 0.053094044
case acc: 0.06531348
case acc: 0.051773988
case acc: 0.055402502
case acc: 0.04654635
top acc: 0.0174 ::: bot acc: 0.1014
top acc: 0.0192 ::: bot acc: 0.0886
top acc: 0.0249 ::: bot acc: 0.1193
top acc: 0.0426 ::: bot acc: 0.0907
top acc: 0.0282 ::: bot acc: 0.0985
top acc: 0.0482 ::: bot acc: 0.0690
current epoch: 24
train loss is 0.057598
average val loss: 0.057369, accuracy: 0.0580
average test loss: 0.051926, accuracy: 0.0520
case acc: 0.049948048
case acc: 0.048094563
case acc: 0.061544966
case acc: 0.050802745
case acc: 0.056205202
case acc: 0.04533395
top acc: 0.0188 ::: bot acc: 0.0930
top acc: 0.0187 ::: bot acc: 0.0813
top acc: 0.0265 ::: bot acc: 0.1128
top acc: 0.0460 ::: bot acc: 0.0873
top acc: 0.0275 ::: bot acc: 0.1001
top acc: 0.0531 ::: bot acc: 0.0640
current epoch: 25
train loss is 0.055556
average val loss: 0.056373, accuracy: 0.0571
average test loss: 0.050570, accuracy: 0.0506
case acc: 0.046893217
case acc: 0.044787265
case acc: 0.059484527
case acc: 0.05037333
case acc: 0.057487484
case acc: 0.044742644
top acc: 0.0213 ::: bot acc: 0.0871
top acc: 0.0189 ::: bot acc: 0.0762
top acc: 0.0283 ::: bot acc: 0.1088
top acc: 0.0480 ::: bot acc: 0.0853
top acc: 0.0264 ::: bot acc: 0.1025
top acc: 0.0557 ::: bot acc: 0.0613
current epoch: 26
train loss is 0.054836
average val loss: 0.055307, accuracy: 0.0561
average test loss: 0.049119, accuracy: 0.0491
case acc: 0.044192668
case acc: 0.04137979
case acc: 0.057372686
case acc: 0.049752798
case acc: 0.057848994
case acc: 0.044194747
top acc: 0.0253 ::: bot acc: 0.0810
top acc: 0.0200 ::: bot acc: 0.0706
top acc: 0.0306 ::: bot acc: 0.1045
top acc: 0.0510 ::: bot acc: 0.0823
top acc: 0.0261 ::: bot acc: 0.1031
top acc: 0.0586 ::: bot acc: 0.0583
current epoch: 27
train loss is 0.053977
average val loss: 0.053842, accuracy: 0.0545
average test loss: 0.047141, accuracy: 0.0470
case acc: 0.041471504
case acc: 0.037226673
case acc: 0.05462821
case acc: 0.048880037
case acc: 0.05645758
case acc: 0.043570064
top acc: 0.0322 ::: bot acc: 0.0734
top acc: 0.0229 ::: bot acc: 0.0630
top acc: 0.0349 ::: bot acc: 0.0982
top acc: 0.0564 ::: bot acc: 0.0768
top acc: 0.0270 ::: bot acc: 0.1004
top acc: 0.0635 ::: bot acc: 0.0534
current epoch: 28
train loss is 0.052666
average val loss: 0.053262, accuracy: 0.0539
average test loss: 0.046369, accuracy: 0.0462
case acc: 0.040352844
case acc: 0.035188027
case acc: 0.053582717
case acc: 0.048640743
case acc: 0.056095783
case acc: 0.04340298
top acc: 0.0358 ::: bot acc: 0.0696
top acc: 0.0254 ::: bot acc: 0.0586
top acc: 0.0374 ::: bot acc: 0.0953
top acc: 0.0593 ::: bot acc: 0.0739
top acc: 0.0272 ::: bot acc: 0.0997
top acc: 0.0651 ::: bot acc: 0.0518
current epoch: 29
train loss is 0.052358
average val loss: 0.052659, accuracy: 0.0532
average test loss: 0.045613, accuracy: 0.0454
case acc: 0.039670162
case acc: 0.033477318
case acc: 0.05273207
case acc: 0.048479147
case acc: 0.05508927
case acc: 0.04322351
top acc: 0.0392 ::: bot acc: 0.0662
top acc: 0.0291 ::: bot acc: 0.0542
top acc: 0.0403 ::: bot acc: 0.0924
top acc: 0.0628 ::: bot acc: 0.0703
top acc: 0.0281 ::: bot acc: 0.0977
top acc: 0.0669 ::: bot acc: 0.0499
current epoch: 30
train loss is 0.051716
average val loss: 0.052050, accuracy: 0.0523
average test loss: 0.044897, accuracy: 0.0448
case acc: 0.039179824
case acc: 0.03215692
case acc: 0.052005865
case acc: 0.04850616
case acc: 0.05354026
case acc: 0.043149903
top acc: 0.0424 ::: bot acc: 0.0629
top acc: 0.0335 ::: bot acc: 0.0495
top acc: 0.0435 ::: bot acc: 0.0892
top acc: 0.0671 ::: bot acc: 0.0659
top acc: 0.0297 ::: bot acc: 0.0945
top acc: 0.0690 ::: bot acc: 0.0477
current epoch: 31
train loss is 0.051345
average val loss: 0.051873, accuracy: 0.0520
average test loss: 0.044672, accuracy: 0.0446
case acc: 0.03910716
case acc: 0.031728994
case acc: 0.051875874
case acc: 0.048609484
case acc: 0.05294425
case acc: 0.04314184
top acc: 0.0429 ::: bot acc: 0.0623
top acc: 0.0354 ::: bot acc: 0.0476
top acc: 0.0440 ::: bot acc: 0.0886
top acc: 0.0692 ::: bot acc: 0.0638
top acc: 0.0304 ::: bot acc: 0.0933
top acc: 0.0688 ::: bot acc: 0.0479
current epoch: 32
train loss is 0.050888
average val loss: 0.051944, accuracy: 0.0519
average test loss: 0.044717, accuracy: 0.0446
case acc: 0.03923941
case acc: 0.03170143
case acc: 0.05216567
case acc: 0.048646875
case acc: 0.052968107
case acc: 0.043154784
top acc: 0.0416 ::: bot acc: 0.0635
top acc: 0.0355 ::: bot acc: 0.0475
top acc: 0.0426 ::: bot acc: 0.0900
top acc: 0.0697 ::: bot acc: 0.0633
top acc: 0.0303 ::: bot acc: 0.0933
top acc: 0.0670 ::: bot acc: 0.0497
current epoch: 33
train loss is 0.050536
average val loss: 0.052090, accuracy: 0.0520
average test loss: 0.044860, accuracy: 0.0448
case acc: 0.039441448
case acc: 0.03175148
case acc: 0.052614626
case acc: 0.048650354
case acc: 0.053173795
case acc: 0.043309007
top acc: 0.0400 ::: bot acc: 0.0652
top acc: 0.0351 ::: bot acc: 0.0479
top acc: 0.0408 ::: bot acc: 0.0918
top acc: 0.0697 ::: bot acc: 0.0633
top acc: 0.0301 ::: bot acc: 0.0937
top acc: 0.0649 ::: bot acc: 0.0518
current epoch: 34
train loss is 0.050342
average val loss: 0.052506, accuracy: 0.0523
average test loss: 0.045355, accuracy: 0.0454
case acc: 0.039912418
case acc: 0.03215478
case acc: 0.053598117
case acc: 0.048546746
case acc: 0.054243486
case acc: 0.04364782
top acc: 0.0371 ::: bot acc: 0.0681
top acc: 0.0332 ::: bot acc: 0.0498
top acc: 0.0375 ::: bot acc: 0.0950
top acc: 0.0680 ::: bot acc: 0.0650
top acc: 0.0290 ::: bot acc: 0.0959
top acc: 0.0614 ::: bot acc: 0.0552
current epoch: 35
train loss is 0.050149
average val loss: 0.052929, accuracy: 0.0527
average test loss: 0.045891, accuracy: 0.0459
case acc: 0.040449794
case acc: 0.03251672
case acc: 0.054661434
case acc: 0.04845306
case acc: 0.05539442
case acc: 0.044051766
top acc: 0.0349 ::: bot acc: 0.0703
top acc: 0.0317 ::: bot acc: 0.0513
top acc: 0.0349 ::: bot acc: 0.0979
top acc: 0.0663 ::: bot acc: 0.0667
top acc: 0.0279 ::: bot acc: 0.0982
top acc: 0.0585 ::: bot acc: 0.0581
current epoch: 36
train loss is 0.050131
average val loss: 0.053119, accuracy: 0.0529
average test loss: 0.046136, accuracy: 0.0462
case acc: 0.040604725
case acc: 0.03254241
case acc: 0.05529741
case acc: 0.048419096
case acc: 0.05607863
case acc: 0.04427415
top acc: 0.0344 ::: bot acc: 0.0707
top acc: 0.0317 ::: bot acc: 0.0513
top acc: 0.0338 ::: bot acc: 0.0994
top acc: 0.0656 ::: bot acc: 0.0674
top acc: 0.0273 ::: bot acc: 0.0995
top acc: 0.0571 ::: bot acc: 0.0596
current epoch: 37
train loss is 0.050080
average val loss: 0.053224, accuracy: 0.0531
average test loss: 0.046274, accuracy: 0.0464
case acc: 0.04053407
case acc: 0.032443922
case acc: 0.055652767
case acc: 0.04840113
case acc: 0.056744523
case acc: 0.044441663
top acc: 0.0348 ::: bot acc: 0.0704
top acc: 0.0322 ::: bot acc: 0.0509
top acc: 0.0332 ::: bot acc: 0.1003
top acc: 0.0650 ::: bot acc: 0.0680
top acc: 0.0268 ::: bot acc: 0.1008
top acc: 0.0562 ::: bot acc: 0.0605
current epoch: 38
train loss is 0.050076
average val loss: 0.052631, accuracy: 0.0526
average test loss: 0.045520, accuracy: 0.0457
case acc: 0.039749116
case acc: 0.03161576
case acc: 0.054441843
case acc: 0.048526045
case acc: 0.055642918
case acc: 0.04402239
top acc: 0.0388 ::: bot acc: 0.0665
top acc: 0.0363 ::: bot acc: 0.0469
top acc: 0.0352 ::: bot acc: 0.0975
top acc: 0.0679 ::: bot acc: 0.0651
top acc: 0.0277 ::: bot acc: 0.0987
top acc: 0.0588 ::: bot acc: 0.0579
current epoch: 39
train loss is 0.050003
average val loss: 0.052132, accuracy: 0.0521
average test loss: 0.044925, accuracy: 0.0451
case acc: 0.03923986
case acc: 0.031022485
case acc: 0.053414173
case acc: 0.048741557
case acc: 0.054492023
case acc: 0.043675873
top acc: 0.0423 ::: bot acc: 0.0630
top acc: 0.0401 ::: bot acc: 0.0430
top acc: 0.0378 ::: bot acc: 0.0946
top acc: 0.0708 ::: bot acc: 0.0622
top acc: 0.0287 ::: bot acc: 0.0965
top acc: 0.0613 ::: bot acc: 0.0554
current epoch: 40
train loss is 0.049992
average val loss: 0.051667, accuracy: 0.0517
average test loss: 0.044432, accuracy: 0.0446
case acc: 0.03886633
case acc: 0.030709809
case acc: 0.05247925
case acc: 0.049063418
case acc: 0.0530315
case acc: 0.04341203
top acc: 0.0457 ::: bot acc: 0.0596
top acc: 0.0441 ::: bot acc: 0.0391
top acc: 0.0409 ::: bot acc: 0.0915
top acc: 0.0743 ::: bot acc: 0.0587
top acc: 0.0301 ::: bot acc: 0.0936
top acc: 0.0638 ::: bot acc: 0.0529
current epoch: 41
train loss is 0.049983
average val loss: 0.051303, accuracy: 0.0513
average test loss: 0.044120, accuracy: 0.0443
case acc: 0.038729906
case acc: 0.030796051
case acc: 0.051810287
case acc: 0.049530014
case acc: 0.0516757
case acc: 0.04323689
top acc: 0.0484 ::: bot acc: 0.0569
top acc: 0.0477 ::: bot acc: 0.0356
top acc: 0.0438 ::: bot acc: 0.0886
top acc: 0.0778 ::: bot acc: 0.0551
top acc: 0.0320 ::: bot acc: 0.0906
top acc: 0.0660 ::: bot acc: 0.0508
current epoch: 42
train loss is 0.050022
average val loss: 0.050926, accuracy: 0.0508
average test loss: 0.043887, accuracy: 0.0441
case acc: 0.038757846
case acc: 0.031139355
case acc: 0.051042795
case acc: 0.050607238
case acc: 0.05005024
case acc: 0.043119065
top acc: 0.0516 ::: bot acc: 0.0537
top acc: 0.0520 ::: bot acc: 0.0312
top acc: 0.0478 ::: bot acc: 0.0847
top acc: 0.0826 ::: bot acc: 0.0503
top acc: 0.0360 ::: bot acc: 0.0861
top acc: 0.0689 ::: bot acc: 0.0478
current epoch: 43
train loss is 0.050126
average val loss: 0.050747, accuracy: 0.0504
average test loss: 0.043871, accuracy: 0.0442
case acc: 0.038835734
case acc: 0.03159461
case acc: 0.050658733
case acc: 0.051780686
case acc: 0.04897396
case acc: 0.043102153
top acc: 0.0530 ::: bot acc: 0.0522
top acc: 0.0549 ::: bot acc: 0.0283
top acc: 0.0505 ::: bot acc: 0.0819
top acc: 0.0866 ::: bot acc: 0.0462
top acc: 0.0398 ::: bot acc: 0.0824
top acc: 0.0705 ::: bot acc: 0.0462
current epoch: 44
train loss is 0.049980
average val loss: 0.050680, accuracy: 0.0501
average test loss: 0.043917, accuracy: 0.0442
case acc: 0.038827453
case acc: 0.031899255
case acc: 0.05048876
case acc: 0.05280446
case acc: 0.04822784
case acc: 0.043094415
top acc: 0.0529 ::: bot acc: 0.0523
top acc: 0.0564 ::: bot acc: 0.0268
top acc: 0.0520 ::: bot acc: 0.0804
top acc: 0.0898 ::: bot acc: 0.0431
top acc: 0.0427 ::: bot acc: 0.0795
top acc: 0.0710 ::: bot acc: 0.0457
current epoch: 45
train loss is 0.049700
average val loss: 0.050716, accuracy: 0.0498
average test loss: 0.043905, accuracy: 0.0442
case acc: 0.038690183
case acc: 0.031702895
case acc: 0.050583117
case acc: 0.053157117
case acc: 0.047948163
case acc: 0.0430958
top acc: 0.0504 ::: bot acc: 0.0548
top acc: 0.0555 ::: bot acc: 0.0277
top acc: 0.0511 ::: bot acc: 0.0814
top acc: 0.0908 ::: bot acc: 0.0420
top acc: 0.0438 ::: bot acc: 0.0783
top acc: 0.0693 ::: bot acc: 0.0473
current epoch: 46
train loss is 0.049345
average val loss: 0.050924, accuracy: 0.0497
average test loss: 0.043923, accuracy: 0.0442
case acc: 0.038874477
case acc: 0.03111783
case acc: 0.05115333
case acc: 0.05262238
case acc: 0.048206884
case acc: 0.04325462
top acc: 0.0453 ::: bot acc: 0.0599
top acc: 0.0519 ::: bot acc: 0.0313
top acc: 0.0471 ::: bot acc: 0.0853
top acc: 0.0892 ::: bot acc: 0.0436
top acc: 0.0427 ::: bot acc: 0.0794
top acc: 0.0654 ::: bot acc: 0.0513
current epoch: 47
train loss is 0.049194
average val loss: 0.051564, accuracy: 0.0501
average test loss: 0.044369, accuracy: 0.0446
case acc: 0.039919727
case acc: 0.030729696
case acc: 0.05281537
case acc: 0.051118705
case acc: 0.04925704
case acc: 0.04400469
top acc: 0.0377 ::: bot acc: 0.0676
top acc: 0.0453 ::: bot acc: 0.0379
top acc: 0.0397 ::: bot acc: 0.0928
top acc: 0.0845 ::: bot acc: 0.0483
top acc: 0.0387 ::: bot acc: 0.0835
top acc: 0.0589 ::: bot acc: 0.0578
current epoch: 48
train loss is 0.049363
average val loss: 0.052339, accuracy: 0.0508
average test loss: 0.045232, accuracy: 0.0455
case acc: 0.04146937
case acc: 0.03112333
case acc: 0.055426236
case acc: 0.05000492
case acc: 0.05029279
case acc: 0.04494932
top acc: 0.0316 ::: bot acc: 0.0737
top acc: 0.0397 ::: bot acc: 0.0436
top acc: 0.0335 ::: bot acc: 0.0998
top acc: 0.0802 ::: bot acc: 0.0527
top acc: 0.0353 ::: bot acc: 0.0869
top acc: 0.0539 ::: bot acc: 0.0628
current epoch: 49
train loss is 0.050064
average val loss: 0.053669, accuracy: 0.0523
average test loss: 0.046886, accuracy: 0.0473
case acc: 0.043937474
case acc: 0.032401856
case acc: 0.05968289
case acc: 0.049009938
case acc: 0.05231153
case acc: 0.046298184
top acc: 0.0255 ::: bot acc: 0.0804
top acc: 0.0328 ::: bot acc: 0.0505
top acc: 0.0284 ::: bot acc: 0.1088
top acc: 0.0739 ::: bot acc: 0.0589
top acc: 0.0310 ::: bot acc: 0.0921
top acc: 0.0482 ::: bot acc: 0.0686
current epoch: 50
train loss is 0.051000
average val loss: 0.055595, accuracy: 0.0545
average test loss: 0.049454, accuracy: 0.0499
case acc: 0.046984985
case acc: 0.034882452
case acc: 0.06539858
case acc: 0.048457943
case acc: 0.055668704
case acc: 0.047905706
top acc: 0.0208 ::: bot acc: 0.0874
top acc: 0.0255 ::: bot acc: 0.0582
top acc: 0.0252 ::: bot acc: 0.1189
top acc: 0.0663 ::: bot acc: 0.0667
top acc: 0.0277 ::: bot acc: 0.0988
top acc: 0.0420 ::: bot acc: 0.0746
LME_Co_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5358 5358 5358
1.7082474 -0.6288155 0.2588177 -0.21218425
Validation: 600 600 600
Testing: 750 750 750
pre-processing time: 0.00024080276489257812
the split date is 2012-01-01
net initializing with time: 0.003144502639770508
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.153744
average val loss: 0.179977, accuracy: 0.1788
average test loss: 0.140629, accuracy: 0.1452
case acc: 0.18066226
case acc: 0.03964771
case acc: 0.08769418
case acc: 0.08959136
case acc: 0.05271768
case acc: 0.4209649
top acc: 0.1288 ::: bot acc: 0.2357
top acc: 0.0301 ::: bot acc: 0.0761
top acc: 0.0344 ::: bot acc: 0.1543
top acc: 0.0365 ::: bot acc: 0.1472
top acc: 0.0963 ::: bot acc: 0.0205
top acc: 0.3610 ::: bot acc: 0.4816
current epoch: 2
train loss is 0.185607
average val loss: 0.271719, accuracy: 0.2717
average test loss: 0.217772, accuracy: 0.2176
case acc: 0.2701411
case acc: 0.12455615
case acc: 0.16531023
case acc: 0.1825238
case acc: 0.06799872
case acc: 0.4951836
top acc: 0.2181 ::: bot acc: 0.3258
top acc: 0.0743 ::: bot acc: 0.1822
top acc: 0.0706 ::: bot acc: 0.2526
top acc: 0.1142 ::: bot acc: 0.2478
top acc: 0.0249 ::: bot acc: 0.1140
top acc: 0.4345 ::: bot acc: 0.5553
current epoch: 3
train loss is 0.143672
average val loss: 0.129589, accuracy: 0.1271
average test loss: 0.104856, accuracy: 0.1125
case acc: 0.10675127
case acc: 0.04513443
case acc: 0.06833184
case acc: 0.05748449
case acc: 0.08023513
case acc: 0.31721997
top acc: 0.0547 ::: bot acc: 0.1625
top acc: 0.0704 ::: bot acc: 0.0383
top acc: 0.0817 ::: bot acc: 0.0998
top acc: 0.0375 ::: bot acc: 0.0991
top acc: 0.1317 ::: bot acc: 0.0320
top acc: 0.2564 ::: bot acc: 0.3771
current epoch: 4
train loss is 0.122244
average val loss: 0.138103, accuracy: 0.1367
average test loss: 0.104650, accuracy: 0.1106
case acc: 0.115791224
case acc: 0.039551456
case acc: 0.072358906
case acc: 0.069183476
case acc: 0.057440937
case acc: 0.30925235
top acc: 0.0639 ::: bot acc: 0.1714
top acc: 0.0445 ::: bot acc: 0.0644
top acc: 0.0639 ::: bot acc: 0.1173
top acc: 0.0321 ::: bot acc: 0.1192
top acc: 0.1037 ::: bot acc: 0.0200
top acc: 0.2485 ::: bot acc: 0.3692
current epoch: 5
train loss is 0.118682
average val loss: 0.144408, accuracy: 0.1435
average test loss: 0.104945, accuracy: 0.1093
case acc: 0.11917527
case acc: 0.04215994
case acc: 0.07631789
case acc: 0.07849819
case acc: 0.04409097
case acc: 0.29557484
top acc: 0.0673 ::: bot acc: 0.1747
top acc: 0.0246 ::: bot acc: 0.0846
top acc: 0.0526 ::: bot acc: 0.1290
top acc: 0.0327 ::: bot acc: 0.1327
top acc: 0.0818 ::: bot acc: 0.0242
top acc: 0.2348 ::: bot acc: 0.3556
current epoch: 6
train loss is 0.103909
average val loss: 0.108264, accuracy: 0.1065
average test loss: 0.082468, accuracy: 0.0881
case acc: 0.07220984
case acc: 0.04144845
case acc: 0.06635241
case acc: 0.055093847
case acc: 0.06337827
case acc: 0.22989511
top acc: 0.0244 ::: bot acc: 0.1257
top acc: 0.0564 ::: bot acc: 0.0528
top acc: 0.0924 ::: bot acc: 0.0886
top acc: 0.0398 ::: bot acc: 0.0944
top acc: 0.1116 ::: bot acc: 0.0219
top acc: 0.1691 ::: bot acc: 0.2900
current epoch: 7
train loss is 0.093712
average val loss: 0.092481, accuracy: 0.0900
average test loss: 0.073291, accuracy: 0.0784
case acc: 0.054681946
case acc: 0.044010114
case acc: 0.06617387
case acc: 0.04875519
case acc: 0.069104604
case acc: 0.18747255
top acc: 0.0198 ::: bot acc: 0.1019
top acc: 0.0660 ::: bot acc: 0.0433
top acc: 0.1096 ::: bot acc: 0.0713
top acc: 0.0539 ::: bot acc: 0.0778
top acc: 0.1189 ::: bot acc: 0.0246
top acc: 0.1267 ::: bot acc: 0.2475
current epoch: 8
train loss is 0.086529
average val loss: 0.079628, accuracy: 0.0762
average test loss: 0.066890, accuracy: 0.0711
case acc: 0.043338295
case acc: 0.046663437
case acc: 0.0683242
case acc: 0.0464682
case acc: 0.074173406
case acc: 0.1476298
top acc: 0.0251 ::: bot acc: 0.0824
top acc: 0.0742 ::: bot acc: 0.0351
top acc: 0.1247 ::: bot acc: 0.0559
top acc: 0.0698 ::: bot acc: 0.0618
top acc: 0.1246 ::: bot acc: 0.0279
top acc: 0.0870 ::: bot acc: 0.2076
current epoch: 9
train loss is 0.080632
average val loss: 0.067244, accuracy: 0.0624
average test loss: 0.064288, accuracy: 0.0676
case acc: 0.03952267
case acc: 0.051912837
case acc: 0.07429979
case acc: 0.050735433
case acc: 0.08348914
case acc: 0.1058949
top acc: 0.0460 ::: bot acc: 0.0613
top acc: 0.0874 ::: bot acc: 0.0241
top acc: 0.1437 ::: bot acc: 0.0377
top acc: 0.0909 ::: bot acc: 0.0408
top acc: 0.1352 ::: bot acc: 0.0346
top acc: 0.0477 ::: bot acc: 0.1646
current epoch: 10
train loss is 0.076642
average val loss: 0.060416, accuracy: 0.0553
average test loss: 0.062462, accuracy: 0.0652
case acc: 0.039652992
case acc: 0.053920843
case acc: 0.07868233
case acc: 0.05601093
case acc: 0.08499054
case acc: 0.07818004
top acc: 0.0545 ::: bot acc: 0.0527
top acc: 0.0917 ::: bot acc: 0.0216
top acc: 0.1527 ::: bot acc: 0.0326
top acc: 0.1033 ::: bot acc: 0.0311
top acc: 0.1368 ::: bot acc: 0.0358
top acc: 0.0284 ::: bot acc: 0.1328
current epoch: 11
train loss is 0.074199
average val loss: 0.056446, accuracy: 0.0521
average test loss: 0.059853, accuracy: 0.0621
case acc: 0.03959756
case acc: 0.05276557
case acc: 0.07948365
case acc: 0.05909374
case acc: 0.08069438
case acc: 0.0609241
top acc: 0.0538 ::: bot acc: 0.0533
top acc: 0.0891 ::: bot acc: 0.0231
top acc: 0.1541 ::: bot acc: 0.0320
top acc: 0.1091 ::: bot acc: 0.0286
top acc: 0.1320 ::: bot acc: 0.0326
top acc: 0.0229 ::: bot acc: 0.1098
current epoch: 12
train loss is 0.071487
average val loss: 0.055039, accuracy: 0.0518
average test loss: 0.056091, accuracy: 0.0580
case acc: 0.03953873
case acc: 0.049021073
case acc: 0.07676008
case acc: 0.05843903
case acc: 0.071249925
case acc: 0.05269648
top acc: 0.0462 ::: bot acc: 0.0611
top acc: 0.0804 ::: bot acc: 0.0295
top acc: 0.1488 ::: bot acc: 0.0345
top acc: 0.1081 ::: bot acc: 0.0286
top acc: 0.1213 ::: bot acc: 0.0261
top acc: 0.0283 ::: bot acc: 0.0949
current epoch: 13
train loss is 0.066523
average val loss: 0.055262, accuracy: 0.0532
average test loss: 0.052812, accuracy: 0.0542
case acc: 0.040246397
case acc: 0.04549949
case acc: 0.07352791
case acc: 0.056726646
case acc: 0.061778612
case acc: 0.047688592
top acc: 0.0380 ::: bot acc: 0.0694
top acc: 0.0701 ::: bot acc: 0.0395
top acc: 0.1419 ::: bot acc: 0.0388
top acc: 0.1047 ::: bot acc: 0.0301
top acc: 0.1093 ::: bot acc: 0.0216
top acc: 0.0377 ::: bot acc: 0.0829
current epoch: 14
train loss is 0.063009
average val loss: 0.056762, accuracy: 0.0558
average test loss: 0.050434, accuracy: 0.0511
case acc: 0.041369725
case acc: 0.04232562
case acc: 0.07083384
case acc: 0.054267373
case acc: 0.053139303
case acc: 0.04495809
top acc: 0.0310 ::: bot acc: 0.0765
top acc: 0.0591 ::: bot acc: 0.0506
top acc: 0.1343 ::: bot acc: 0.0461
top acc: 0.0994 ::: bot acc: 0.0333
top acc: 0.0972 ::: bot acc: 0.0199
top acc: 0.0473 ::: bot acc: 0.0732
current epoch: 15
train loss is 0.058729
average val loss: 0.060798, accuracy: 0.0607
average test loss: 0.048581, accuracy: 0.0486
case acc: 0.04422086
case acc: 0.040016707
case acc: 0.0680416
case acc: 0.050461486
case acc: 0.04475115
case acc: 0.044387955
top acc: 0.0238 ::: bot acc: 0.0845
top acc: 0.0453 ::: bot acc: 0.0644
top acc: 0.1238 ::: bot acc: 0.0565
top acc: 0.0900 ::: bot acc: 0.0409
top acc: 0.0830 ::: bot acc: 0.0236
top acc: 0.0515 ::: bot acc: 0.0691
current epoch: 16
train loss is 0.055801
average val loss: 0.064493, accuracy: 0.0649
average test loss: 0.048020, accuracy: 0.0474
case acc: 0.04608414
case acc: 0.04000788
case acc: 0.066672966
case acc: 0.04765123
case acc: 0.03978221
case acc: 0.044066075
top acc: 0.0224 ::: bot acc: 0.0880
top acc: 0.0341 ::: bot acc: 0.0756
top acc: 0.1156 ::: bot acc: 0.0646
top acc: 0.0815 ::: bot acc: 0.0491
top acc: 0.0719 ::: bot acc: 0.0311
top acc: 0.0548 ::: bot acc: 0.0658
current epoch: 17
train loss is 0.053027
average val loss: 0.066454, accuracy: 0.0671
average test loss: 0.048076, accuracy: 0.0468
case acc: 0.045187216
case acc: 0.04139387
case acc: 0.06608988
case acc: 0.046496283
case acc: 0.037868638
case acc: 0.04380855
top acc: 0.0230 ::: bot acc: 0.0864
top acc: 0.0273 ::: bot acc: 0.0825
top acc: 0.1112 ::: bot acc: 0.0691
top acc: 0.0757 ::: bot acc: 0.0549
top acc: 0.0658 ::: bot acc: 0.0373
top acc: 0.0578 ::: bot acc: 0.0628
current epoch: 18
train loss is 0.051499
average val loss: 0.066495, accuracy: 0.0672
average test loss: 0.048095, accuracy: 0.0464
case acc: 0.042635564
case acc: 0.042171318
case acc: 0.06596306
case acc: 0.046186816
case acc: 0.037521448
case acc: 0.043690685
top acc: 0.0268 ::: bot acc: 0.0807
top acc: 0.0248 ::: bot acc: 0.0851
top acc: 0.1102 ::: bot acc: 0.0700
top acc: 0.0726 ::: bot acc: 0.0579
top acc: 0.0642 ::: bot acc: 0.0388
top acc: 0.0602 ::: bot acc: 0.0605
current epoch: 19
train loss is 0.050748
average val loss: 0.064755, accuracy: 0.0653
average test loss: 0.048046, accuracy: 0.0461
case acc: 0.040527456
case acc: 0.04169317
case acc: 0.06621343
case acc: 0.046144743
case acc: 0.038111035
case acc: 0.043618403
top acc: 0.0351 ::: bot acc: 0.0723
top acc: 0.0263 ::: bot acc: 0.0836
top acc: 0.1124 ::: bot acc: 0.0679
top acc: 0.0724 ::: bot acc: 0.0581
top acc: 0.0667 ::: bot acc: 0.0363
top acc: 0.0621 ::: bot acc: 0.0584
current epoch: 20
train loss is 0.050413
average val loss: 0.060900, accuracy: 0.0612
average test loss: 0.048406, accuracy: 0.0463
case acc: 0.039552145
case acc: 0.040274985
case acc: 0.06705606
case acc: 0.046536583
case acc: 0.040561117
case acc: 0.043623485
top acc: 0.0466 ::: bot acc: 0.0609
top acc: 0.0326 ::: bot acc: 0.0774
top acc: 0.1186 ::: bot acc: 0.0616
top acc: 0.0763 ::: bot acc: 0.0540
top acc: 0.0739 ::: bot acc: 0.0295
top acc: 0.0660 ::: bot acc: 0.0545
current epoch: 21
train loss is 0.050370
average val loss: 0.056724, accuracy: 0.0568
average test loss: 0.049669, accuracy: 0.0476
case acc: 0.04019141
case acc: 0.039836388
case acc: 0.068762906
case acc: 0.04801989
case acc: 0.045023337
case acc: 0.043920405
top acc: 0.0582 ::: bot acc: 0.0494
top acc: 0.0415 ::: bot acc: 0.0687
top acc: 0.1268 ::: bot acc: 0.0534
top acc: 0.0829 ::: bot acc: 0.0473
top acc: 0.0835 ::: bot acc: 0.0234
top acc: 0.0698 ::: bot acc: 0.0507
current epoch: 22
train loss is 0.050354
average val loss: 0.052657, accuracy: 0.0524
average test loss: 0.052214, accuracy: 0.0505
case acc: 0.04273761
case acc: 0.041115984
case acc: 0.07169106
case acc: 0.051280223
case acc: 0.0517417
case acc: 0.044657294
top acc: 0.0691 ::: bot acc: 0.0384
top acc: 0.0525 ::: bot acc: 0.0576
top acc: 0.1365 ::: bot acc: 0.0436
top acc: 0.0921 ::: bot acc: 0.0385
top acc: 0.0951 ::: bot acc: 0.0202
top acc: 0.0742 ::: bot acc: 0.0462
current epoch: 23
train loss is 0.050402
average val loss: 0.049297, accuracy: 0.0485
average test loss: 0.056384, accuracy: 0.0551
case acc: 0.04609336
case acc: 0.044441275
case acc: 0.076541536
case acc: 0.056633502
case acc: 0.061283298
case acc: 0.045880623
top acc: 0.0795 ::: bot acc: 0.0282
top acc: 0.0659 ::: bot acc: 0.0443
top acc: 0.1480 ::: bot acc: 0.0350
top acc: 0.1043 ::: bot acc: 0.0299
top acc: 0.1087 ::: bot acc: 0.0214
top acc: 0.0798 ::: bot acc: 0.0407
current epoch: 24
train loss is 0.050149
average val loss: 0.047593, accuracy: 0.0465
average test loss: 0.061563, accuracy: 0.0607
case acc: 0.049113084
case acc: 0.048874717
case acc: 0.08232829
case acc: 0.06461819
case acc: 0.07181441
case acc: 0.04761659
top acc: 0.0870 ::: bot acc: 0.0220
top acc: 0.0792 ::: bot acc: 0.0313
top acc: 0.1589 ::: bot acc: 0.0302
top acc: 0.1173 ::: bot acc: 0.0277
top acc: 0.1220 ::: bot acc: 0.0264
top acc: 0.0851 ::: bot acc: 0.0355
current epoch: 25
train loss is 0.050182
average val loss: 0.047577, accuracy: 0.0462
average test loss: 0.069032, accuracy: 0.0686
case acc: 0.052937042
case acc: 0.056000665
case acc: 0.09021952
case acc: 0.076141804
case acc: 0.085261345
case acc: 0.05096289
top acc: 0.0944 ::: bot acc: 0.0184
top acc: 0.0947 ::: bot acc: 0.0212
top acc: 0.1718 ::: bot acc: 0.0279
top acc: 0.1336 ::: bot acc: 0.0293
top acc: 0.1373 ::: bot acc: 0.0359
top acc: 0.0930 ::: bot acc: 0.0294
current epoch: 26
train loss is 0.051263
average val loss: 0.049523, accuracy: 0.0478
average test loss: 0.077464, accuracy: 0.0773
case acc: 0.055947848
case acc: 0.06482909
case acc: 0.09881573
case acc: 0.08943147
case acc: 0.09907121
case acc: 0.055820003
top acc: 0.0993 ::: bot acc: 0.0175
top acc: 0.1095 ::: bot acc: 0.0177
top acc: 0.1839 ::: bot acc: 0.0292
top acc: 0.1507 ::: bot acc: 0.0349
top acc: 0.1518 ::: bot acc: 0.0484
top acc: 0.1023 ::: bot acc: 0.0251
current epoch: 27
train loss is 0.054017
average val loss: 0.050938, accuracy: 0.0491
average test loss: 0.081203, accuracy: 0.0812
case acc: 0.053399332
case acc: 0.06938332
case acc: 0.10209415
case acc: 0.09846847
case acc: 0.10572537
case acc: 0.058083985
top acc: 0.0952 ::: bot acc: 0.0182
top acc: 0.1165 ::: bot acc: 0.0173
top acc: 0.1884 ::: bot acc: 0.0299
top acc: 0.1614 ::: bot acc: 0.0406
top acc: 0.1584 ::: bot acc: 0.0551
top acc: 0.1064 ::: bot acc: 0.0237
current epoch: 28
train loss is 0.057660
average val loss: 0.049623, accuracy: 0.0483
average test loss: 0.075900, accuracy: 0.0764
case acc: 0.04554718
case acc: 0.06504143
case acc: 0.09606646
case acc: 0.097210824
case acc: 0.098824665
case acc: 0.05540964
top acc: 0.0779 ::: bot acc: 0.0296
top acc: 0.1099 ::: bot acc: 0.0177
top acc: 0.1801 ::: bot acc: 0.0287
top acc: 0.1599 ::: bot acc: 0.0397
top acc: 0.1515 ::: bot acc: 0.0482
top acc: 0.1015 ::: bot acc: 0.0254
current epoch: 29
train loss is 0.061036
average val loss: 0.049325, accuracy: 0.0494
average test loss: 0.060674, accuracy: 0.0618
case acc: 0.039603822
case acc: 0.05049647
case acc: 0.07908744
case acc: 0.08042273
case acc: 0.07400205
case acc: 0.046995852
top acc: 0.0440 ::: bot acc: 0.0634
top acc: 0.0830 ::: bot acc: 0.0284
top acc: 0.1530 ::: bot acc: 0.0323
top acc: 0.1393 ::: bot acc: 0.0309
top acc: 0.1245 ::: bot acc: 0.0279
top acc: 0.0833 ::: bot acc: 0.0371
current epoch: 30
train loss is 0.065923
average val loss: 0.068041, accuracy: 0.0688
average test loss: 0.049427, accuracy: 0.0508
case acc: 0.06143106
case acc: 0.04037293
case acc: 0.06535602
case acc: 0.052098386
case acc: 0.040765595
case acc: 0.044640925
top acc: 0.0205 ::: bot acc: 0.1118
top acc: 0.0327 ::: bot acc: 0.0776
top acc: 0.1051 ::: bot acc: 0.0747
top acc: 0.0944 ::: bot acc: 0.0368
top acc: 0.0743 ::: bot acc: 0.0293
top acc: 0.0488 ::: bot acc: 0.0715
current epoch: 31
train loss is 0.066949
average val loss: 0.092436, accuracy: 0.0932
average test loss: 0.057413, accuracy: 0.0583
case acc: 0.08146215
case acc: 0.06102659
case acc: 0.069431625
case acc: 0.04711457
case acc: 0.04022532
case acc: 0.05032872
top acc: 0.0314 ::: bot acc: 0.1362
top acc: 0.0190 ::: bot acc: 0.1162
top acc: 0.0716 ::: bot acc: 0.1082
top acc: 0.0580 ::: bot acc: 0.0725
top acc: 0.0362 ::: bot acc: 0.0671
top acc: 0.0318 ::: bot acc: 0.0895
current epoch: 32
train loss is 0.063172
average val loss: 0.095846, accuracy: 0.0970
average test loss: 0.059313, accuracy: 0.0594
case acc: 0.07363802
case acc: 0.06982217
case acc: 0.07070447
case acc: 0.05021058
case acc: 0.04434705
case acc: 0.047870915
top acc: 0.0254 ::: bot acc: 0.1275
top acc: 0.0247 ::: bot acc: 0.1265
top acc: 0.0665 ::: bot acc: 0.1133
top acc: 0.0463 ::: bot acc: 0.0841
top acc: 0.0287 ::: bot acc: 0.0768
top acc: 0.0372 ::: bot acc: 0.0832
current epoch: 33
train loss is 0.063639
average val loss: 0.085519, accuracy: 0.0870
average test loss: 0.054189, accuracy: 0.0532
case acc: 0.05457228
case acc: 0.062871076
case acc: 0.06795574
case acc: 0.049140036
case acc: 0.04057435
case acc: 0.04433265
top acc: 0.0198 ::: bot acc: 0.1018
top acc: 0.0200 ::: bot acc: 0.1184
top acc: 0.0780 ::: bot acc: 0.1018
top acc: 0.0498 ::: bot acc: 0.0807
top acc: 0.0352 ::: bot acc: 0.0681
top acc: 0.0513 ::: bot acc: 0.0691
current epoch: 34
train loss is 0.062955
average val loss: 0.060864, accuracy: 0.0619
average test loss: 0.049443, accuracy: 0.0466
case acc: 0.039468523
case acc: 0.04186759
case acc: 0.06628201
case acc: 0.046776734
case acc: 0.039307527
case acc: 0.046195805
top acc: 0.0532 ::: bot acc: 0.0540
top acc: 0.0260 ::: bot acc: 0.0842
top acc: 0.1137 ::: bot acc: 0.0661
top acc: 0.0776 ::: bot acc: 0.0528
top acc: 0.0704 ::: bot acc: 0.0328
top acc: 0.0809 ::: bot acc: 0.0395
current epoch: 35
train loss is 0.059438
average val loss: 0.048112, accuracy: 0.0485
average test loss: 0.064315, accuracy: 0.0629
case acc: 0.057564404
case acc: 0.045967847
case acc: 0.08127042
case acc: 0.06416162
case acc: 0.06752103
case acc: 0.06065752
top acc: 0.1019 ::: bot acc: 0.0171
top acc: 0.0709 ::: bot acc: 0.0394
top acc: 0.1570 ::: bot acc: 0.0308
top acc: 0.1166 ::: bot acc: 0.0277
top acc: 0.1168 ::: bot acc: 0.0238
top acc: 0.1107 ::: bot acc: 0.0227
current epoch: 36
train loss is 0.056351
average val loss: 0.049404, accuracy: 0.0486
average test loss: 0.077135, accuracy: 0.0765
case acc: 0.070602536
case acc: 0.057312302
case acc: 0.09499016
case acc: 0.08068826
case acc: 0.09203773
case acc: 0.06365904
top acc: 0.1195 ::: bot acc: 0.0207
top acc: 0.0972 ::: bot acc: 0.0202
top acc: 0.1785 ::: bot acc: 0.0284
top acc: 0.1396 ::: bot acc: 0.0309
top acc: 0.1445 ::: bot acc: 0.0418
top acc: 0.1155 ::: bot acc: 0.0222
current epoch: 37
train loss is 0.053923
average val loss: 0.048951, accuracy: 0.0476
average test loss: 0.075978, accuracy: 0.0758
case acc: 0.06266976
case acc: 0.05999433
case acc: 0.09443518
case acc: 0.08443102
case acc: 0.097238876
case acc: 0.05595961
top acc: 0.1096 ::: bot acc: 0.0169
top acc: 0.1019 ::: bot acc: 0.0189
top acc: 0.1778 ::: bot acc: 0.0283
top acc: 0.1443 ::: bot acc: 0.0327
top acc: 0.1499 ::: bot acc: 0.0467
top acc: 0.1026 ::: bot acc: 0.0251
current epoch: 38
train loss is 0.052959
average val loss: 0.047339, accuracy: 0.0461
average test loss: 0.066463, accuracy: 0.0666
case acc: 0.048188105
case acc: 0.053948738
case acc: 0.08478858
case acc: 0.07846105
case acc: 0.08634557
case acc: 0.048017353
top acc: 0.0850 ::: bot acc: 0.0232
top acc: 0.0908 ::: bot acc: 0.0232
top acc: 0.1632 ::: bot acc: 0.0288
top acc: 0.1367 ::: bot acc: 0.0301
top acc: 0.1385 ::: bot acc: 0.0367
top acc: 0.0861 ::: bot acc: 0.0346
current epoch: 39
train loss is 0.051282
average val loss: 0.049165, accuracy: 0.0487
average test loss: 0.056403, accuracy: 0.0570
case acc: 0.040019087
case acc: 0.046259917
case acc: 0.07423474
case acc: 0.06820022
case acc: 0.0689659
case acc: 0.044411425
top acc: 0.0577 ::: bot acc: 0.0496
top acc: 0.0717 ::: bot acc: 0.0387
top acc: 0.1431 ::: bot acc: 0.0376
top acc: 0.1225 ::: bot acc: 0.0279
top acc: 0.1186 ::: bot acc: 0.0246
top acc: 0.0728 ::: bot acc: 0.0477
current epoch: 40
train loss is 0.050245
average val loss: 0.056911, accuracy: 0.0571
average test loss: 0.049244, accuracy: 0.0498
case acc: 0.040813316
case acc: 0.040477034
case acc: 0.067240775
case acc: 0.055627003
case acc: 0.050761417
case acc: 0.043589327
top acc: 0.0330 ::: bot acc: 0.0743
top acc: 0.0473 ::: bot acc: 0.0631
top acc: 0.1202 ::: bot acc: 0.0595
top acc: 0.1023 ::: bot acc: 0.0311
top acc: 0.0934 ::: bot acc: 0.0206
top acc: 0.0604 ::: bot acc: 0.0601
current epoch: 41
train loss is 0.050598
average val loss: 0.067372, accuracy: 0.0680
average test loss: 0.047688, accuracy: 0.0476
case acc: 0.046829667
case acc: 0.04229555
case acc: 0.065183304
case acc: 0.047687087
case acc: 0.039434444
case acc: 0.044361535
top acc: 0.0219 ::: bot acc: 0.0894
top acc: 0.0249 ::: bot acc: 0.0854
top acc: 0.1009 ::: bot acc: 0.0788
top acc: 0.0817 ::: bot acc: 0.0486
top acc: 0.0708 ::: bot acc: 0.0325
top acc: 0.0515 ::: bot acc: 0.0689
current epoch: 42
train loss is 0.052135
average val loss: 0.070988, accuracy: 0.0718
average test loss: 0.048260, accuracy: 0.0474
case acc: 0.04534868
case acc: 0.04627342
case acc: 0.06542843
case acc: 0.04607014
case acc: 0.03715518
case acc: 0.04424658
top acc: 0.0228 ::: bot acc: 0.0867
top acc: 0.0179 ::: bot acc: 0.0947
top acc: 0.0950 ::: bot acc: 0.0848
top acc: 0.0714 ::: bot acc: 0.0589
top acc: 0.0614 ::: bot acc: 0.0418
top acc: 0.0526 ::: bot acc: 0.0679
current epoch: 43
train loss is 0.053347
average val loss: 0.063266, accuracy: 0.0639
average test loss: 0.047975, accuracy: 0.0462
case acc: 0.039692905
case acc: 0.042297117
case acc: 0.0655013
case acc: 0.04674474
case acc: 0.039548375
case acc: 0.043607306
top acc: 0.0421 ::: bot acc: 0.0652
top acc: 0.0250 ::: bot acc: 0.0854
top acc: 0.1069 ::: bot acc: 0.0729
top acc: 0.0775 ::: bot acc: 0.0528
top acc: 0.0711 ::: bot acc: 0.0321
top acc: 0.0658 ::: bot acc: 0.0547
current epoch: 44
train loss is 0.052950
average val loss: 0.051161, accuracy: 0.0515
average test loss: 0.053891, accuracy: 0.0522
case acc: 0.044803195
case acc: 0.040960316
case acc: 0.07072704
case acc: 0.05457369
case acc: 0.053773355
case acc: 0.048534926
top acc: 0.0760 ::: bot acc: 0.0313
top acc: 0.0514 ::: bot acc: 0.0590
top acc: 0.1339 ::: bot acc: 0.0458
top acc: 0.0999 ::: bot acc: 0.0327
top acc: 0.0982 ::: bot acc: 0.0199
top acc: 0.0876 ::: bot acc: 0.0333
current epoch: 45
train loss is 0.051668
average val loss: 0.047333, accuracy: 0.0472
average test loss: 0.065773, accuracy: 0.0650
case acc: 0.057627574
case acc: 0.048698705
case acc: 0.08257611
case acc: 0.06932739
case acc: 0.075743616
case acc: 0.055955272
top acc: 0.1020 ::: bot acc: 0.0171
top acc: 0.0788 ::: bot acc: 0.0319
top acc: 0.1592 ::: bot acc: 0.0301
top acc: 0.1241 ::: bot acc: 0.0281
top acc: 0.1264 ::: bot acc: 0.0291
top acc: 0.1027 ::: bot acc: 0.0251
current epoch: 46
train loss is 0.050751
average val loss: 0.047964, accuracy: 0.0472
average test loss: 0.072303, accuracy: 0.0720
case acc: 0.061345063
case acc: 0.055487312
case acc: 0.0893059
case acc: 0.07965502
case acc: 0.08972323
case acc: 0.056279417
top acc: 0.1077 ::: bot acc: 0.0168
top acc: 0.0939 ::: bot acc: 0.0216
top acc: 0.1703 ::: bot acc: 0.0279
top acc: 0.1382 ::: bot acc: 0.0306
top acc: 0.1421 ::: bot acc: 0.0397
top acc: 0.1032 ::: bot acc: 0.0249
current epoch: 47
train loss is 0.050311
average val loss: 0.047534, accuracy: 0.0465
average test loss: 0.069860, accuracy: 0.0698
case acc: 0.053662963
case acc: 0.05563738
case acc: 0.087026656
case acc: 0.08119906
case acc: 0.090005755
case acc: 0.051463224
top acc: 0.0956 ::: bot acc: 0.0181
top acc: 0.0941 ::: bot acc: 0.0215
top acc: 0.1668 ::: bot acc: 0.0281
top acc: 0.1402 ::: bot acc: 0.0312
top acc: 0.1424 ::: bot acc: 0.0399
top acc: 0.0941 ::: bot acc: 0.0288
current epoch: 48
train loss is 0.049490
average val loss: 0.047439, accuracy: 0.0467
average test loss: 0.061500, accuracy: 0.0620
case acc: 0.043848384
case acc: 0.049918987
case acc: 0.07870547
case acc: 0.07465175
case acc: 0.07835691
case acc: 0.046284925
top acc: 0.0730 ::: bot acc: 0.0342
top acc: 0.0817 ::: bot acc: 0.0295
top acc: 0.1522 ::: bot acc: 0.0325
top acc: 0.1315 ::: bot acc: 0.0291
top acc: 0.1295 ::: bot acc: 0.0309
top acc: 0.0813 ::: bot acc: 0.0392
current epoch: 49
train loss is 0.048856
average val loss: 0.051508, accuracy: 0.0515
average test loss: 0.052832, accuracy: 0.0535
case acc: 0.03936969
case acc: 0.043141473
case acc: 0.07013402
case acc: 0.0633162
case acc: 0.06100755
case acc: 0.04376899
top acc: 0.0485 ::: bot acc: 0.0588
top acc: 0.0613 ::: bot acc: 0.0491
top acc: 0.1320 ::: bot acc: 0.0477
top acc: 0.1153 ::: bot acc: 0.0278
top acc: 0.1083 ::: bot acc: 0.0214
top acc: 0.0689 ::: bot acc: 0.0516
current epoch: 50
train loss is 0.049007
average val loss: 0.060220, accuracy: 0.0606
average test loss: 0.048036, accuracy: 0.0483
case acc: 0.04173538
case acc: 0.039897326
case acc: 0.06599438
case acc: 0.05248617
case acc: 0.045823324
case acc: 0.043699354
top acc: 0.0291 ::: bot acc: 0.0781
top acc: 0.0386 ::: bot acc: 0.0719
top acc: 0.1115 ::: bot acc: 0.0682
top acc: 0.0952 ::: bot acc: 0.0360
top acc: 0.0849 ::: bot acc: 0.0229
top acc: 0.0582 ::: bot acc: 0.0622
LME_Co_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5346 5346 5346
1.7082474 -0.6288155 0.25454274 -0.21218425
Validation: 594 594 594
Testing: 768 768 768
pre-processing time: 0.000431060791015625
the split date is 2012-07-01
net initializing with time: 0.0036072731018066406
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.163640
average val loss: 0.077709, accuracy: 0.0756
average test loss: 0.078905, accuracy: 0.0814
case acc: 0.09448253
case acc: 0.12594609
case acc: 0.10552399
case acc: 0.04337319
case acc: 0.06424224
case acc: 0.055095546
top acc: 0.0623 ::: bot acc: 0.1359
top acc: 0.0827 ::: bot acc: 0.1670
top acc: 0.0261 ::: bot acc: 0.1818
top acc: 0.0142 ::: bot acc: 0.0782
top acc: 0.0232 ::: bot acc: 0.1076
top acc: 0.0837 ::: bot acc: 0.0645
current epoch: 2
train loss is 0.098255
average val loss: 0.149557, accuracy: 0.1493
average test loss: 0.150553, accuracy: 0.1509
case acc: 0.1554693
case acc: 0.21343735
case acc: 0.19305131
case acc: 0.124991156
case acc: 0.13985902
case acc: 0.07844444
top acc: 0.0760 ::: bot acc: 0.2204
top acc: 0.1703 ::: bot acc: 0.2548
top acc: 0.1094 ::: bot acc: 0.2720
top acc: 0.0901 ::: bot acc: 0.1629
top acc: 0.0752 ::: bot acc: 0.1948
top acc: 0.0296 ::: bot acc: 0.1433
current epoch: 3
train loss is 0.137821
average val loss: 0.062678, accuracy: 0.0606
average test loss: 0.063290, accuracy: 0.0667
case acc: 0.071361706
case acc: 0.09319042
case acc: 0.08565004
case acc: 0.027301963
case acc: 0.052565686
case acc: 0.07030107
top acc: 0.0643 ::: bot acc: 0.1012
top acc: 0.0502 ::: bot acc: 0.1347
top acc: 0.0200 ::: bot acc: 0.1563
top acc: 0.0229 ::: bot acc: 0.0498
top acc: 0.0367 ::: bot acc: 0.0838
top acc: 0.1203 ::: bot acc: 0.0374
current epoch: 4
train loss is 0.105193
average val loss: 0.058936, accuracy: 0.0583
average test loss: 0.058389, accuracy: 0.0606
case acc: 0.057235807
case acc: 0.037038937
case acc: 0.06162883
case acc: 0.04779028
case acc: 0.045749325
case acc: 0.11398378
top acc: 0.1164 ::: bot acc: 0.0376
top acc: 0.0181 ::: bot acc: 0.0669
top acc: 0.0711 ::: bot acc: 0.0920
top acc: 0.0794 ::: bot acc: 0.0165
top acc: 0.0925 ::: bot acc: 0.0272
top acc: 0.1759 ::: bot acc: 0.0565
current epoch: 5
train loss is 0.073186
average val loss: 0.055556, accuracy: 0.0529
average test loss: 0.055747, accuracy: 0.0585
case acc: 0.061583392
case acc: 0.06661693
case acc: 0.07501107
case acc: 0.026764289
case acc: 0.049834453
case acc: 0.07122198
top acc: 0.0707 ::: bot acc: 0.0838
top acc: 0.0238 ::: bot acc: 0.1083
top acc: 0.0299 ::: bot acc: 0.1357
top acc: 0.0279 ::: bot acc: 0.0450
top acc: 0.0425 ::: bot acc: 0.0769
top acc: 0.1216 ::: bot acc: 0.0372
current epoch: 6
train loss is 0.075196
average val loss: 0.061475, accuracy: 0.0587
average test loss: 0.062163, accuracy: 0.0645
case acc: 0.071518034
case acc: 0.07937765
case acc: 0.08285022
case acc: 0.035540547
case acc: 0.059554458
case acc: 0.058381446
top acc: 0.0645 ::: bot acc: 0.1017
top acc: 0.0362 ::: bot acc: 0.1214
top acc: 0.0211 ::: bot acc: 0.1517
top acc: 0.0104 ::: bot acc: 0.0689
top acc: 0.0267 ::: bot acc: 0.0991
top acc: 0.0955 ::: bot acc: 0.0514
current epoch: 7
train loss is 0.079622
average val loss: 0.053758, accuracy: 0.0503
average test loss: 0.053856, accuracy: 0.0556
case acc: 0.060566224
case acc: 0.055215873
case acc: 0.07267284
case acc: 0.028981002
case acc: 0.052708507
case acc: 0.063733675
top acc: 0.0731 ::: bot acc: 0.0812
top acc: 0.0140 ::: bot acc: 0.0964
top acc: 0.0344 ::: bot acc: 0.1300
top acc: 0.0177 ::: bot acc: 0.0555
top acc: 0.0353 ::: bot acc: 0.0847
top acc: 0.1076 ::: bot acc: 0.0430
current epoch: 8
train loss is 0.073215
average val loss: 0.050183, accuracy: 0.0462
average test loss: 0.049727, accuracy: 0.0509
case acc: 0.056689184
case acc: 0.039182283
case acc: 0.06509476
case acc: 0.02688467
case acc: 0.047624584
case acc: 0.06996669
top acc: 0.0929 ::: bot acc: 0.0614
top acc: 0.0149 ::: bot acc: 0.0721
top acc: 0.0540 ::: bot acc: 0.1089
top acc: 0.0309 ::: bot acc: 0.0425
top acc: 0.0484 ::: bot acc: 0.0705
top acc: 0.1192 ::: bot acc: 0.0381
current epoch: 9
train loss is 0.068730
average val loss: 0.049878, accuracy: 0.0460
average test loss: 0.049107, accuracy: 0.0496
case acc: 0.05648419
case acc: 0.03365153
case acc: 0.062107563
case acc: 0.027129449
case acc: 0.045557175
case acc: 0.07240833
top acc: 0.1051 ::: bot acc: 0.0495
top acc: 0.0300 ::: bot acc: 0.0556
top acc: 0.0677 ::: bot acc: 0.0952
top acc: 0.0373 ::: bot acc: 0.0363
top acc: 0.0562 ::: bot acc: 0.0626
top acc: 0.1233 ::: bot acc: 0.0373
current epoch: 10
train loss is 0.065357
average val loss: 0.052128, accuracy: 0.0489
average test loss: 0.051104, accuracy: 0.0504
case acc: 0.05811011
case acc: 0.03274475
case acc: 0.060251236
case acc: 0.029702593
case acc: 0.043338858
case acc: 0.07821354
top acc: 0.1209 ::: bot acc: 0.0338
top acc: 0.0505 ::: bot acc: 0.0352
top acc: 0.0855 ::: bot acc: 0.0774
top acc: 0.0486 ::: bot acc: 0.0250
top acc: 0.0696 ::: bot acc: 0.0491
top acc: 0.1314 ::: bot acc: 0.0381
current epoch: 11
train loss is 0.064003
average val loss: 0.058577, accuracy: 0.0558
average test loss: 0.057839, accuracy: 0.0557
case acc: 0.0641003
case acc: 0.039367236
case acc: 0.06207361
case acc: 0.0370188
case acc: 0.044289302
case acc: 0.08706345
top acc: 0.1396 ::: bot acc: 0.0167
top acc: 0.0742 ::: bot acc: 0.0146
top acc: 0.1068 ::: bot acc: 0.0559
top acc: 0.0646 ::: bot acc: 0.0148
top acc: 0.0883 ::: bot acc: 0.0302
top acc: 0.1433 ::: bot acc: 0.0409
current epoch: 12
train loss is 0.062930
average val loss: 0.065068, accuracy: 0.0627
average test loss: 0.064947, accuracy: 0.0627
case acc: 0.07061163
case acc: 0.052160166
case acc: 0.065373145
case acc: 0.044678442
case acc: 0.05132992
case acc: 0.0919336
top acc: 0.1518 ::: bot acc: 0.0115
top acc: 0.0913 ::: bot acc: 0.0185
top acc: 0.1225 ::: bot acc: 0.0403
top acc: 0.0759 ::: bot acc: 0.0149
top acc: 0.1034 ::: bot acc: 0.0211
top acc: 0.1493 ::: bot acc: 0.0432
current epoch: 13
train loss is 0.062029
average val loss: 0.071805, accuracy: 0.0699
average test loss: 0.072329, accuracy: 0.0704
case acc: 0.07725608
case acc: 0.06399606
case acc: 0.070648536
case acc: 0.053230375
case acc: 0.061808858
case acc: 0.09539932
top acc: 0.1613 ::: bot acc: 0.0126
top acc: 0.1055 ::: bot acc: 0.0257
top acc: 0.1358 ::: bot acc: 0.0291
top acc: 0.0865 ::: bot acc: 0.0194
top acc: 0.1187 ::: bot acc: 0.0219
top acc: 0.1536 ::: bot acc: 0.0450
current epoch: 14
train loss is 0.060960
average val loss: 0.075271, accuracy: 0.0737
average test loss: 0.076144, accuracy: 0.0744
case acc: 0.07935338
case acc: 0.06987163
case acc: 0.07391287
case acc: 0.05823189
case acc: 0.07073662
case acc: 0.09450434
top acc: 0.1639 ::: bot acc: 0.0138
top acc: 0.1123 ::: bot acc: 0.0298
top acc: 0.1427 ::: bot acc: 0.0250
top acc: 0.0923 ::: bot acc: 0.0230
top acc: 0.1296 ::: bot acc: 0.0268
top acc: 0.1527 ::: bot acc: 0.0444
current epoch: 15
train loss is 0.059928
average val loss: 0.073269, accuracy: 0.0722
average test loss: 0.074013, accuracy: 0.0724
case acc: 0.0746876
case acc: 0.06723091
case acc: 0.07305907
case acc: 0.0571037
case acc: 0.0740255
case acc: 0.08822103
top acc: 0.1580 ::: bot acc: 0.0118
top acc: 0.1093 ::: bot acc: 0.0279
top acc: 0.1410 ::: bot acc: 0.0260
top acc: 0.0909 ::: bot acc: 0.0222
top acc: 0.1335 ::: bot acc: 0.0288
top acc: 0.1448 ::: bot acc: 0.0413
current epoch: 16
train loss is 0.058054
average val loss: 0.070765, accuracy: 0.0703
average test loss: 0.071299, accuracy: 0.0699
case acc: 0.06994128
case acc: 0.062607974
case acc: 0.07138623
case acc: 0.055857714
case acc: 0.07669655
case acc: 0.08266919
top acc: 0.1508 ::: bot acc: 0.0118
top acc: 0.1038 ::: bot acc: 0.0248
top acc: 0.1374 ::: bot acc: 0.0282
top acc: 0.0894 ::: bot acc: 0.0213
top acc: 0.1366 ::: bot acc: 0.0306
top acc: 0.1376 ::: bot acc: 0.0392
current epoch: 17
train loss is 0.056148
average val loss: 0.068298, accuracy: 0.0683
average test loss: 0.068617, accuracy: 0.0674
case acc: 0.066145614
case acc: 0.057014354
case acc: 0.069339156
case acc: 0.05489775
case acc: 0.07875199
case acc: 0.07853055
top acc: 0.1438 ::: bot acc: 0.0146
top acc: 0.0972 ::: bot acc: 0.0213
top acc: 0.1329 ::: bot acc: 0.0312
top acc: 0.0883 ::: bot acc: 0.0206
top acc: 0.1389 ::: bot acc: 0.0320
top acc: 0.1319 ::: bot acc: 0.0381
current epoch: 18
train loss is 0.054922
average val loss: 0.061820, accuracy: 0.0625
average test loss: 0.061305, accuracy: 0.0605
case acc: 0.060412817
case acc: 0.045085296
case acc: 0.0646925
case acc: 0.04807972
case acc: 0.073551014
case acc: 0.070922166
top acc: 0.1299 ::: bot acc: 0.0250
top acc: 0.0824 ::: bot acc: 0.0152
top acc: 0.1203 ::: bot acc: 0.0423
top acc: 0.0803 ::: bot acc: 0.0164
top acc: 0.1329 ::: bot acc: 0.0285
top acc: 0.1208 ::: bot acc: 0.0377
current epoch: 19
train loss is 0.052701
average val loss: 0.055283, accuracy: 0.0565
average test loss: 0.054138, accuracy: 0.0539
case acc: 0.057154153
case acc: 0.03513883
case acc: 0.061627436
case acc: 0.039954387
case acc: 0.06508323
case acc: 0.06444518
top acc: 0.1142 ::: bot acc: 0.0406
top acc: 0.0646 ::: bot acc: 0.0211
top acc: 0.1043 ::: bot acc: 0.0583
top acc: 0.0695 ::: bot acc: 0.0135
top acc: 0.1228 ::: bot acc: 0.0234
top acc: 0.1086 ::: bot acc: 0.0430
current epoch: 20
train loss is 0.051717
average val loss: 0.050028, accuracy: 0.0515
average test loss: 0.048784, accuracy: 0.0492
case acc: 0.05647529
case acc: 0.032421377
case acc: 0.060194395
case acc: 0.03268668
case acc: 0.054505367
case acc: 0.058806382
top acc: 0.0970 ::: bot acc: 0.0577
top acc: 0.0445 ::: bot acc: 0.0412
top acc: 0.0854 ::: bot acc: 0.0772
top acc: 0.0561 ::: bot acc: 0.0189
top acc: 0.1083 ::: bot acc: 0.0208
top acc: 0.0954 ::: bot acc: 0.0528
current epoch: 21
train loss is 0.052607
average val loss: 0.047847, accuracy: 0.0494
average test loss: 0.047274, accuracy: 0.0476
case acc: 0.060192145
case acc: 0.036943696
case acc: 0.06366559
case acc: 0.0271327
case acc: 0.0436553
case acc: 0.053878788
top acc: 0.0751 ::: bot acc: 0.0797
top acc: 0.0190 ::: bot acc: 0.0669
top acc: 0.0601 ::: bot acc: 0.1026
top acc: 0.0361 ::: bot acc: 0.0377
top acc: 0.0855 ::: bot acc: 0.0329
top acc: 0.0775 ::: bot acc: 0.0704
current epoch: 22
train loss is 0.056652
average val loss: 0.052792, accuracy: 0.0541
average test loss: 0.053024, accuracy: 0.0540
case acc: 0.07112123
case acc: 0.051902104
case acc: 0.07235514
case acc: 0.030373698
case acc: 0.0446304
case acc: 0.05339558
top acc: 0.0649 ::: bot acc: 0.1010
top acc: 0.0119 ::: bot acc: 0.0926
top acc: 0.0345 ::: bot acc: 0.1291
top acc: 0.0148 ::: bot acc: 0.0591
top acc: 0.0596 ::: bot acc: 0.0588
top acc: 0.0601 ::: bot acc: 0.0878
current epoch: 23
train loss is 0.060949
average val loss: 0.064046, accuracy: 0.0643
average test loss: 0.064954, accuracy: 0.0660
case acc: 0.08299002
case acc: 0.073956616
case acc: 0.084719405
case acc: 0.04464247
case acc: 0.05323372
case acc: 0.056212462
top acc: 0.0621 ::: bot acc: 0.1201
top acc: 0.0307 ::: bot acc: 0.1163
top acc: 0.0196 ::: bot acc: 0.1548
top acc: 0.0147 ::: bot acc: 0.0804
top acc: 0.0334 ::: bot acc: 0.0862
top acc: 0.0445 ::: bot acc: 0.1034
current epoch: 24
train loss is 0.064968
average val loss: 0.070879, accuracy: 0.0704
average test loss: 0.072119, accuracy: 0.0729
case acc: 0.08675346
case acc: 0.08352747
case acc: 0.0933006
case acc: 0.054280087
case acc: 0.062339645
case acc: 0.05734108
top acc: 0.0618 ::: bot acc: 0.1259
top acc: 0.0402 ::: bot acc: 0.1259
top acc: 0.0192 ::: bot acc: 0.1678
top acc: 0.0214 ::: bot acc: 0.0914
top acc: 0.0245 ::: bot acc: 0.1041
top acc: 0.0404 ::: bot acc: 0.1075
current epoch: 25
train loss is 0.070041
average val loss: 0.067407, accuracy: 0.0663
average test loss: 0.068487, accuracy: 0.0693
case acc: 0.07972743
case acc: 0.07433931
case acc: 0.089985274
case acc: 0.052328818
case acc: 0.06472308
case acc: 0.054988127
top acc: 0.0624 ::: bot acc: 0.1152
top acc: 0.0311 ::: bot acc: 0.1168
top acc: 0.0187 ::: bot acc: 0.1631
top acc: 0.0199 ::: bot acc: 0.0892
top acc: 0.0235 ::: bot acc: 0.1081
top acc: 0.0495 ::: bot acc: 0.0984
current epoch: 26
train loss is 0.069034
average val loss: 0.052257, accuracy: 0.0492
average test loss: 0.052375, accuracy: 0.0528
case acc: 0.059972897
case acc: 0.04304235
case acc: 0.07300095
case acc: 0.03266412
case acc: 0.053539377
case acc: 0.05438092
top acc: 0.0751 ::: bot acc: 0.0795
top acc: 0.0116 ::: bot acc: 0.0798
top acc: 0.0330 ::: bot acc: 0.1309
top acc: 0.0117 ::: bot acc: 0.0640
top acc: 0.0326 ::: bot acc: 0.0871
top acc: 0.0799 ::: bot acc: 0.0680
current epoch: 27
train loss is 0.061407
average val loss: 0.048954, accuracy: 0.0451
average test loss: 0.048087, accuracy: 0.0481
case acc: 0.056711048
case acc: 0.032492835
case acc: 0.06262973
case acc: 0.027097275
case acc: 0.045876253
case acc: 0.06354352
top acc: 0.1088 ::: bot acc: 0.0459
top acc: 0.0429 ::: bot acc: 0.0428
top acc: 0.0646 ::: bot acc: 0.0980
top acc: 0.0339 ::: bot acc: 0.0398
top acc: 0.0538 ::: bot acc: 0.0646
top acc: 0.1066 ::: bot acc: 0.0442
current epoch: 28
train loss is 0.060209
average val loss: 0.060689, accuracy: 0.0575
average test loss: 0.060281, accuracy: 0.0582
case acc: 0.0692786
case acc: 0.049889922
case acc: 0.062117517
case acc: 0.03919326
case acc: 0.044541657
case acc: 0.0843847
top acc: 0.1496 ::: bot acc: 0.0117
top acc: 0.0883 ::: bot acc: 0.0173
top acc: 0.1071 ::: bot acc: 0.0554
top acc: 0.0682 ::: bot acc: 0.0138
top acc: 0.0892 ::: bot acc: 0.0292
top acc: 0.1398 ::: bot acc: 0.0398
current epoch: 29
train loss is 0.063304
average val loss: 0.085016, accuracy: 0.0832
average test loss: 0.086249, accuracy: 0.0851
case acc: 0.100215286
case acc: 0.087224945
case acc: 0.0769212
case acc: 0.06827086
case acc: 0.06962511
case acc: 0.108388916
top acc: 0.1861 ::: bot acc: 0.0313
top acc: 0.1304 ::: bot acc: 0.0449
top acc: 0.1482 ::: bot acc: 0.0227
top acc: 0.1028 ::: bot acc: 0.0315
top acc: 0.1282 ::: bot acc: 0.0260
top acc: 0.1694 ::: bot acc: 0.0523
current epoch: 30
train loss is 0.067768
average val loss: 0.088874, accuracy: 0.0877
average test loss: 0.090345, accuracy: 0.0894
case acc: 0.101628125
case acc: 0.09357692
case acc: 0.08182022
case acc: 0.073231265
case acc: 0.08009908
case acc: 0.10612201
top acc: 0.1878 ::: bot acc: 0.0327
top acc: 0.1369 ::: bot acc: 0.0511
top acc: 0.1563 ::: bot acc: 0.0210
top acc: 0.1081 ::: bot acc: 0.0358
top acc: 0.1404 ::: bot acc: 0.0329
top acc: 0.1668 ::: bot acc: 0.0508
current epoch: 31
train loss is 0.066792
average val loss: 0.077902, accuracy: 0.0767
average test loss: 0.078953, accuracy: 0.0777
case acc: 0.083576694
case acc: 0.07768358
case acc: 0.074546576
case acc: 0.06301174
case acc: 0.07672514
case acc: 0.090729
top acc: 0.1688 ::: bot acc: 0.0163
top acc: 0.1205 ::: bot acc: 0.0361
top acc: 0.1437 ::: bot acc: 0.0244
top acc: 0.0972 ::: bot acc: 0.0270
top acc: 0.1366 ::: bot acc: 0.0305
top acc: 0.1478 ::: bot acc: 0.0427
current epoch: 32
train loss is 0.061643
average val loss: 0.066725, accuracy: 0.0662
average test loss: 0.066968, accuracy: 0.0657
case acc: 0.06836245
case acc: 0.058834106
case acc: 0.06677301
case acc: 0.052383896
case acc: 0.07079332
case acc: 0.077342026
top acc: 0.1482 ::: bot acc: 0.0125
top acc: 0.0994 ::: bot acc: 0.0222
top acc: 0.1265 ::: bot acc: 0.0362
top acc: 0.0855 ::: bot acc: 0.0188
top acc: 0.1297 ::: bot acc: 0.0266
top acc: 0.1304 ::: bot acc: 0.0378
current epoch: 33
train loss is 0.054951
average val loss: 0.058040, accuracy: 0.0582
average test loss: 0.057089, accuracy: 0.0564
case acc: 0.06009012
case acc: 0.041724253
case acc: 0.06227099
case acc: 0.04310353
case acc: 0.06344432
case acc: 0.06795409
top acc: 0.1286 ::: bot acc: 0.0266
top acc: 0.0777 ::: bot acc: 0.0144
top acc: 0.1084 ::: bot acc: 0.0540
top acc: 0.0740 ::: bot acc: 0.0141
top acc: 0.1208 ::: bot acc: 0.0224
top acc: 0.1156 ::: bot acc: 0.0394
current epoch: 34
train loss is 0.050218
average val loss: 0.051252, accuracy: 0.0520
average test loss: 0.049913, accuracy: 0.0500
case acc: 0.05690816
case acc: 0.033322055
case acc: 0.060125202
case acc: 0.034758102
case acc: 0.054016165
case acc: 0.060866214
top acc: 0.1093 ::: bot acc: 0.0460
top acc: 0.0555 ::: bot acc: 0.0304
top acc: 0.0887 ::: bot acc: 0.0737
top acc: 0.0602 ::: bot acc: 0.0167
top acc: 0.1076 ::: bot acc: 0.0208
top acc: 0.1009 ::: bot acc: 0.0479
current epoch: 35
train loss is 0.048879
average val loss: 0.047460, accuracy: 0.0484
average test loss: 0.046618, accuracy: 0.0468
case acc: 0.057543546
case acc: 0.03349009
case acc: 0.062366966
case acc: 0.027965426
case acc: 0.044182207
case acc: 0.055145007
top acc: 0.0883 ::: bot acc: 0.0670
top acc: 0.0315 ::: bot acc: 0.0544
top acc: 0.0658 ::: bot acc: 0.0967
top acc: 0.0419 ::: bot acc: 0.0321
top acc: 0.0877 ::: bot acc: 0.0307
top acc: 0.0839 ::: bot acc: 0.0638
current epoch: 36
train loss is 0.050493
average val loss: 0.048806, accuracy: 0.0493
average test loss: 0.048625, accuracy: 0.0491
case acc: 0.06196126
case acc: 0.040479593
case acc: 0.067975014
case acc: 0.027685877
case acc: 0.04348263
case acc: 0.05291886
top acc: 0.0714 ::: bot acc: 0.0843
top acc: 0.0139 ::: bot acc: 0.0749
top acc: 0.0451 ::: bot acc: 0.1175
top acc: 0.0241 ::: bot acc: 0.0499
top acc: 0.0663 ::: bot acc: 0.0522
top acc: 0.0695 ::: bot acc: 0.0780
current epoch: 37
train loss is 0.053301
average val loss: 0.051955, accuracy: 0.0518
average test loss: 0.052116, accuracy: 0.0526
case acc: 0.06571055
case acc: 0.046432123
case acc: 0.07260455
case acc: 0.031164601
case acc: 0.046936773
case acc: 0.053011686
top acc: 0.0668 ::: bot acc: 0.0921
top acc: 0.0104 ::: bot acc: 0.0854
top acc: 0.0336 ::: bot acc: 0.1301
top acc: 0.0135 ::: bot acc: 0.0611
top acc: 0.0502 ::: bot acc: 0.0684
top acc: 0.0632 ::: bot acc: 0.0842
current epoch: 38
train loss is 0.055133
average val loss: 0.051786, accuracy: 0.0506
average test loss: 0.051937, accuracy: 0.0523
case acc: 0.06335835
case acc: 0.044487394
case acc: 0.07262837
case acc: 0.0317058
case acc: 0.048938535
case acc: 0.05280119
top acc: 0.0689 ::: bot acc: 0.0876
top acc: 0.0107 ::: bot acc: 0.0824
top acc: 0.0336 ::: bot acc: 0.1302
top acc: 0.0127 ::: bot acc: 0.0623
top acc: 0.0438 ::: bot acc: 0.0748
top acc: 0.0674 ::: bot acc: 0.0800
current epoch: 39
train loss is 0.054747
average val loss: 0.048271, accuracy: 0.0460
average test loss: 0.047939, accuracy: 0.0483
case acc: 0.05780738
case acc: 0.0355338
case acc: 0.06693649
case acc: 0.02774648
case acc: 0.04659488
case acc: 0.055010285
top acc: 0.0864 ::: bot acc: 0.0688
top acc: 0.0224 ::: bot acc: 0.0634
top acc: 0.0479 ::: bot acc: 0.1147
top acc: 0.0233 ::: bot acc: 0.0506
top acc: 0.0514 ::: bot acc: 0.0672
top acc: 0.0836 ::: bot acc: 0.0639
current epoch: 40
train loss is 0.052345
average val loss: 0.048529, accuracy: 0.0459
average test loss: 0.047524, accuracy: 0.0473
case acc: 0.056957006
case acc: 0.032490037
case acc: 0.06170258
case acc: 0.027591879
case acc: 0.04360324
case acc: 0.061530665
top acc: 0.1095 ::: bot acc: 0.0457
top acc: 0.0472 ::: bot acc: 0.0386
top acc: 0.0694 ::: bot acc: 0.0932
top acc: 0.0397 ::: bot acc: 0.0342
top acc: 0.0654 ::: bot acc: 0.0532
top acc: 0.1026 ::: bot acc: 0.0465
current epoch: 41
train loss is 0.052209
average val loss: 0.056469, accuracy: 0.0539
average test loss: 0.055507, accuracy: 0.0538
case acc: 0.06350714
case acc: 0.04271248
case acc: 0.06093269
case acc: 0.03670522
case acc: 0.044675115
case acc: 0.07449
top acc: 0.1385 ::: bot acc: 0.0174
top acc: 0.0791 ::: bot acc: 0.0142
top acc: 0.0989 ::: bot acc: 0.0636
top acc: 0.0637 ::: bot acc: 0.0152
top acc: 0.0894 ::: bot acc: 0.0292
top acc: 0.1264 ::: bot acc: 0.0371
current epoch: 42
train loss is 0.055082
average val loss: 0.072283, accuracy: 0.0703
average test loss: 0.072915, accuracy: 0.0715
case acc: 0.08159512
case acc: 0.06919696
case acc: 0.068355195
case acc: 0.056481123
case acc: 0.061555564
case acc: 0.09195149
top acc: 0.1667 ::: bot acc: 0.0148
top acc: 0.1113 ::: bot acc: 0.0290
top acc: 0.1304 ::: bot acc: 0.0330
top acc: 0.0899 ::: bot acc: 0.0220
top acc: 0.1184 ::: bot acc: 0.0215
top acc: 0.1494 ::: bot acc: 0.0434
current epoch: 43
train loss is 0.058639
average val loss: 0.083767, accuracy: 0.0826
average test loss: 0.085116, accuracy: 0.0842
case acc: 0.093722805
case acc: 0.08534351
case acc: 0.07747042
case acc: 0.0703573
case acc: 0.07877605
case acc: 0.09941568
top acc: 0.1798 ::: bot acc: 0.0249
top acc: 0.1285 ::: bot acc: 0.0429
top acc: 0.1491 ::: bot acc: 0.0225
top acc: 0.1050 ::: bot acc: 0.0332
top acc: 0.1390 ::: bot acc: 0.0318
top acc: 0.1586 ::: bot acc: 0.0472
current epoch: 44
train loss is 0.060609
average val loss: 0.080361, accuracy: 0.0795
average test loss: 0.081626, accuracy: 0.0806
case acc: 0.085801974
case acc: 0.0796466
case acc: 0.07607221
case acc: 0.067966655
case acc: 0.08214219
case acc: 0.0918722
top acc: 0.1715 ::: bot acc: 0.0178
top acc: 0.1226 ::: bot acc: 0.0378
top acc: 0.1466 ::: bot acc: 0.0233
top acc: 0.1025 ::: bot acc: 0.0311
top acc: 0.1428 ::: bot acc: 0.0343
top acc: 0.1493 ::: bot acc: 0.0432
current epoch: 45
train loss is 0.059438
average val loss: 0.068923, accuracy: 0.0685
average test loss: 0.069373, accuracy: 0.0683
case acc: 0.070076264
case acc: 0.06102493
case acc: 0.06823852
case acc: 0.05658123
case acc: 0.07537322
case acc: 0.078283265
top acc: 0.1512 ::: bot acc: 0.0116
top acc: 0.1018 ::: bot acc: 0.0235
top acc: 0.1302 ::: bot acc: 0.0331
top acc: 0.0901 ::: bot acc: 0.0219
top acc: 0.1352 ::: bot acc: 0.0293
top acc: 0.1317 ::: bot acc: 0.0380
current epoch: 46
train loss is 0.054196
average val loss: 0.058685, accuracy: 0.0590
average test loss: 0.057826, accuracy: 0.0572
case acc: 0.060384803
case acc: 0.041902926
case acc: 0.06259366
case acc: 0.044957586
case acc: 0.06573482
case acc: 0.06764588
top acc: 0.1298 ::: bot acc: 0.0256
top acc: 0.0779 ::: bot acc: 0.0143
top acc: 0.1100 ::: bot acc: 0.0525
top acc: 0.0762 ::: bot acc: 0.0149
top acc: 0.1237 ::: bot acc: 0.0235
top acc: 0.1151 ::: bot acc: 0.0395
current epoch: 47
train loss is 0.049691
average val loss: 0.050383, accuracy: 0.0512
average test loss: 0.049090, accuracy: 0.0492
case acc: 0.056781877
case acc: 0.032808416
case acc: 0.060156226
case acc: 0.03383194
case acc: 0.052674145
case acc: 0.059137568
top acc: 0.1064 ::: bot acc: 0.0490
top acc: 0.0512 ::: bot acc: 0.0347
top acc: 0.0857 ::: bot acc: 0.0768
top acc: 0.0582 ::: bot acc: 0.0179
top acc: 0.1056 ::: bot acc: 0.0209
top acc: 0.0968 ::: bot acc: 0.0511
current epoch: 48
train loss is 0.049071
average val loss: 0.047363, accuracy: 0.0482
average test loss: 0.046680, accuracy: 0.0469
case acc: 0.058320597
case acc: 0.034648106
case acc: 0.06340964
case acc: 0.027442392
case acc: 0.04331039
case acc: 0.054051947
top acc: 0.0843 ::: bot acc: 0.0711
top acc: 0.0256 ::: bot acc: 0.0603
top acc: 0.0608 ::: bot acc: 0.1017
top acc: 0.0382 ::: bot acc: 0.0360
top acc: 0.0830 ::: bot acc: 0.0356
top acc: 0.0790 ::: bot acc: 0.0686
current epoch: 49
train loss is 0.051441
average val loss: 0.049482, accuracy: 0.0498
average test loss: 0.049398, accuracy: 0.0499
case acc: 0.06264564
case acc: 0.042315386
case acc: 0.06920526
case acc: 0.028114814
case acc: 0.044190593
case acc: 0.052882805
top acc: 0.0701 ::: bot acc: 0.0859
top acc: 0.0123 ::: bot acc: 0.0783
top acc: 0.0418 ::: bot acc: 0.1209
top acc: 0.0219 ::: bot acc: 0.0522
top acc: 0.0620 ::: bot acc: 0.0566
top acc: 0.0672 ::: bot acc: 0.0804
current epoch: 50
train loss is 0.054160
average val loss: 0.050767, accuracy: 0.0502
average test loss: 0.050829, accuracy: 0.0513
case acc: 0.062986165
case acc: 0.043951817
case acc: 0.07132693
case acc: 0.029818185
case acc: 0.046772853
case acc: 0.05287876
top acc: 0.0694 ::: bot acc: 0.0868
top acc: 0.0110 ::: bot acc: 0.0815
top acc: 0.0364 ::: bot acc: 0.1268
top acc: 0.0164 ::: bot acc: 0.0577
top acc: 0.0508 ::: bot acc: 0.0678
top acc: 0.0672 ::: bot acc: 0.0804

		{"drop_out": 0.4, "drop_out_mc": 0.1, "repeat_mc": 50, "hidden": 30, "embedding_size": 5, "batch": 512, "lag": 3}
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
LME_Co_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5328 5328 5328
1.7082474 -0.6288155 0.29835227 -0.25048217
Validation: 594 594 594
Testing: 774 774 774
pre-processing time: 0.0003108978271484375
the split date is 2010-07-01
net initializing with time: 0.0046787261962890625
preparing training and testing date with time: 4.76837158203125e-07
current epoch: 1
train loss is 0.123600
average val loss: 0.111782, accuracy: 0.1124
average test loss: 0.134719, accuracy: 0.1391
case acc: 0.19893259
case acc: 0.064742476
case acc: 0.1588102
case acc: 0.13870503
case acc: 0.16504522
case acc: 0.10837966
top acc: 0.1670 ::: bot acc: 0.2368
top acc: 0.1164 ::: bot acc: 0.0194
top acc: 0.0872 ::: bot acc: 0.2259
top acc: 0.0970 ::: bot acc: 0.1775
top acc: 0.1338 ::: bot acc: 0.1943
top acc: 0.0652 ::: bot acc: 0.1557
current epoch: 2
train loss is 0.118590
average val loss: 0.090459, accuracy: 0.0927
average test loss: 0.071379, accuracy: 0.0685
case acc: 0.030484535
case acc: 0.21389659
case acc: 0.052003056
case acc: 0.036281794
case acc: 0.022520725
case acc: 0.05593493
top acc: 0.0093 ::: bot acc: 0.0629
top acc: 0.2768 ::: bot acc: 0.1461
top acc: 0.0873 ::: bot acc: 0.0515
top acc: 0.0703 ::: bot acc: 0.0130
top acc: 0.0283 ::: bot acc: 0.0326
top acc: 0.0953 ::: bot acc: 0.0166
current epoch: 3
train loss is 0.127343
average val loss: 0.146216, accuracy: 0.1466
average test loss: 0.113340, accuracy: 0.1126
case acc: 0.054628193
case acc: 0.26972494
case acc: 0.087802105
case acc: 0.09206559
case acc: 0.061618444
case acc: 0.10963401
top acc: 0.0770 ::: bot acc: 0.0355
top acc: 0.3329 ::: bot acc: 0.2019
top acc: 0.1568 ::: bot acc: 0.0270
top acc: 0.1342 ::: bot acc: 0.0534
top acc: 0.0928 ::: bot acc: 0.0323
top acc: 0.1532 ::: bot acc: 0.0620
current epoch: 4
train loss is 0.123339
average val loss: 0.081195, accuracy: 0.0829
average test loss: 0.063902, accuracy: 0.0615
case acc: 0.02970107
case acc: 0.18743531
case acc: 0.05166351
case acc: 0.032418597
case acc: 0.022690566
case acc: 0.045115568
top acc: 0.0098 ::: bot acc: 0.0616
top acc: 0.2506 ::: bot acc: 0.1195
top acc: 0.0844 ::: bot acc: 0.0543
top acc: 0.0602 ::: bot acc: 0.0206
top acc: 0.0268 ::: bot acc: 0.0342
top acc: 0.0771 ::: bot acc: 0.0209
current epoch: 5
train loss is 0.102125
average val loss: 0.060741, accuracy: 0.0617
average test loss: 0.058970, accuracy: 0.0613
case acc: 0.061183438
case acc: 0.13700758
case acc: 0.058336146
case acc: 0.035818417
case acc: 0.040241003
case acc: 0.03495109
top acc: 0.0295 ::: bot acc: 0.0991
top acc: 0.2001 ::: bot acc: 0.0691
top acc: 0.0436 ::: bot acc: 0.0965
top acc: 0.0203 ::: bot acc: 0.0614
top acc: 0.0182 ::: bot acc: 0.0655
top acc: 0.0372 ::: bot acc: 0.0568
current epoch: 6
train loss is 0.088490
average val loss: 0.059885, accuracy: 0.0609
average test loss: 0.055254, accuracy: 0.0569
case acc: 0.0527609
case acc: 0.13178614
case acc: 0.05730957
case acc: 0.034136366
case acc: 0.030122332
case acc: 0.03510014
top acc: 0.0213 ::: bot acc: 0.0906
top acc: 0.1950 ::: bot acc: 0.0638
top acc: 0.0457 ::: bot acc: 0.0939
top acc: 0.0246 ::: bot acc: 0.0566
top acc: 0.0167 ::: bot acc: 0.0511
top acc: 0.0426 ::: bot acc: 0.0519
current epoch: 7
train loss is 0.084654
average val loss: 0.062951, accuracy: 0.0641
average test loss: 0.052513, accuracy: 0.0529
case acc: 0.03877654
case acc: 0.13348393
case acc: 0.05383747
case acc: 0.031812143
case acc: 0.022428608
case acc: 0.03679401
top acc: 0.0100 ::: bot acc: 0.0752
top acc: 0.1968 ::: bot acc: 0.0654
top acc: 0.0549 ::: bot acc: 0.0837
top acc: 0.0357 ::: bot acc: 0.0456
top acc: 0.0296 ::: bot acc: 0.0320
top acc: 0.0535 ::: bot acc: 0.0414
current epoch: 8
train loss is 0.084477
average val loss: 0.068235, accuracy: 0.0693
average test loss: 0.052346, accuracy: 0.0520
case acc: 0.028701412
case acc: 0.13549486
case acc: 0.051711068
case acc: 0.031357735
case acc: 0.02518719
case acc: 0.03936013
top acc: 0.0101 ::: bot acc: 0.0601
top acc: 0.1991 ::: bot acc: 0.0673
top acc: 0.0661 ::: bot acc: 0.0727
top acc: 0.0462 ::: bot acc: 0.0350
top acc: 0.0470 ::: bot acc: 0.0145
top acc: 0.0627 ::: bot acc: 0.0326
current epoch: 9
train loss is 0.084717
average val loss: 0.069988, accuracy: 0.0709
average test loss: 0.051958, accuracy: 0.0515
case acc: 0.026092667
case acc: 0.13091533
case acc: 0.05130927
case acc: 0.031428833
case acc: 0.029325677
case acc: 0.04004698
top acc: 0.0162 ::: bot acc: 0.0527
top acc: 0.1946 ::: bot acc: 0.0627
top acc: 0.0703 ::: bot acc: 0.0684
top acc: 0.0497 ::: bot acc: 0.0316
top acc: 0.0561 ::: bot acc: 0.0087
top acc: 0.0645 ::: bot acc: 0.0311
current epoch: 10
train loss is 0.083808
average val loss: 0.068647, accuracy: 0.0694
average test loss: 0.050485, accuracy: 0.0502
case acc: 0.025742335
case acc: 0.12212306
case acc: 0.051317424
case acc: 0.031422645
case acc: 0.03141539
case acc: 0.03931087
top acc: 0.0183 ::: bot acc: 0.0507
top acc: 0.1857 ::: bot acc: 0.0539
top acc: 0.0699 ::: bot acc: 0.0689
top acc: 0.0487 ::: bot acc: 0.0326
top acc: 0.0597 ::: bot acc: 0.0080
top acc: 0.0623 ::: bot acc: 0.0332
current epoch: 11
train loss is 0.080854
average val loss: 0.065539, accuracy: 0.0661
average test loss: 0.048322, accuracy: 0.0482
case acc: 0.025837105
case acc: 0.110884346
case acc: 0.05178125
case acc: 0.031337433
case acc: 0.031506415
case acc: 0.037887704
top acc: 0.0173 ::: bot acc: 0.0516
top acc: 0.1747 ::: bot acc: 0.0425
top acc: 0.0667 ::: bot acc: 0.0724
top acc: 0.0455 ::: bot acc: 0.0358
top acc: 0.0599 ::: bot acc: 0.0080
top acc: 0.0581 ::: bot acc: 0.0369
current epoch: 12
train loss is 0.077166
average val loss: 0.060657, accuracy: 0.0612
average test loss: 0.045726, accuracy: 0.0458
case acc: 0.026986482
case acc: 0.09776345
case acc: 0.052853987
case acc: 0.03147468
case acc: 0.029487658
case acc: 0.036464147
top acc: 0.0131 ::: bot acc: 0.0559
top acc: 0.1609 ::: bot acc: 0.0308
top acc: 0.0603 ::: bot acc: 0.0789
top acc: 0.0399 ::: bot acc: 0.0414
top acc: 0.0565 ::: bot acc: 0.0088
top acc: 0.0519 ::: bot acc: 0.0430
current epoch: 13
train loss is 0.074493
average val loss: 0.057997, accuracy: 0.0585
average test loss: 0.044112, accuracy: 0.0444
case acc: 0.027459728
case acc: 0.08872837
case acc: 0.053592127
case acc: 0.031644408
case acc: 0.0287978
case acc: 0.035920985
top acc: 0.0120 ::: bot acc: 0.0572
top acc: 0.1505 ::: bot acc: 0.0244
top acc: 0.0571 ::: bot acc: 0.0820
top acc: 0.0378 ::: bot acc: 0.0435
top acc: 0.0552 ::: bot acc: 0.0093
top acc: 0.0492 ::: bot acc: 0.0457
current epoch: 14
train loss is 0.071694
average val loss: 0.058245, accuracy: 0.0587
average test loss: 0.043407, accuracy: 0.0436
case acc: 0.026341029
case acc: 0.08415542
case acc: 0.053203117
case acc: 0.031496394
case acc: 0.030064814
case acc: 0.036280207
top acc: 0.0152 ::: bot acc: 0.0537
top acc: 0.1452 ::: bot acc: 0.0213
top acc: 0.0591 ::: bot acc: 0.0801
top acc: 0.0406 ::: bot acc: 0.0408
top acc: 0.0575 ::: bot acc: 0.0086
top acc: 0.0513 ::: bot acc: 0.0435
current epoch: 15
train loss is 0.070119
average val loss: 0.056315, accuracy: 0.0567
average test loss: 0.042146, accuracy: 0.0423
case acc: 0.026341138
case acc: 0.077476874
case acc: 0.053725835
case acc: 0.031463765
case acc: 0.028794102
case acc: 0.035825863
top acc: 0.0149 ::: bot acc: 0.0539
top acc: 0.1370 ::: bot acc: 0.0179
top acc: 0.0579 ::: bot acc: 0.0817
top acc: 0.0398 ::: bot acc: 0.0415
top acc: 0.0552 ::: bot acc: 0.0091
top acc: 0.0498 ::: bot acc: 0.0448
current epoch: 16
train loss is 0.068012
average val loss: 0.054984, accuracy: 0.0553
average test loss: 0.041194, accuracy: 0.0413
case acc: 0.026233118
case acc: 0.072789244
case acc: 0.05380624
case acc: 0.031459745
case acc: 0.027809283
case acc: 0.035701632
top acc: 0.0154 ::: bot acc: 0.0534
top acc: 0.1304 ::: bot acc: 0.0172
top acc: 0.0577 ::: bot acc: 0.0820
top acc: 0.0401 ::: bot acc: 0.0413
top acc: 0.0534 ::: bot acc: 0.0098
top acc: 0.0490 ::: bot acc: 0.0456
current epoch: 17
train loss is 0.066789
average val loss: 0.054734, accuracy: 0.0550
average test loss: 0.040639, accuracy: 0.0407
case acc: 0.025733916
case acc: 0.07013273
case acc: 0.053363316
case acc: 0.031412266
case acc: 0.027499845
case acc: 0.035807025
top acc: 0.0176 ::: bot acc: 0.0513
top acc: 0.1261 ::: bot acc: 0.0176
top acc: 0.0595 ::: bot acc: 0.0801
top acc: 0.0419 ::: bot acc: 0.0395
top acc: 0.0528 ::: bot acc: 0.0102
top acc: 0.0494 ::: bot acc: 0.0453
current epoch: 18
train loss is 0.065026
average val loss: 0.054295, accuracy: 0.0545
average test loss: 0.040144, accuracy: 0.0400
case acc: 0.025343113
case acc: 0.067951806
case acc: 0.052971937
case acc: 0.031382
case acc: 0.026718365
case acc: 0.035663456
top acc: 0.0193 ::: bot acc: 0.0496
top acc: 0.1224 ::: bot acc: 0.0185
top acc: 0.0614 ::: bot acc: 0.0783
top acc: 0.0430 ::: bot acc: 0.0384
top acc: 0.0511 ::: bot acc: 0.0112
top acc: 0.0487 ::: bot acc: 0.0459
current epoch: 19
train loss is 0.064041
average val loss: 0.055335, accuracy: 0.0555
average test loss: 0.040168, accuracy: 0.0397
case acc: 0.02482829
case acc: 0.06751006
case acc: 0.052290972
case acc: 0.031333636
case acc: 0.026719222
case acc: 0.035767578
top acc: 0.0231 ::: bot acc: 0.0457
top acc: 0.1216 ::: bot acc: 0.0188
top acc: 0.0659 ::: bot acc: 0.0739
top acc: 0.0460 ::: bot acc: 0.0353
top acc: 0.0512 ::: bot acc: 0.0110
top acc: 0.0496 ::: bot acc: 0.0449
current epoch: 20
train loss is 0.063676
average val loss: 0.055360, accuracy: 0.0556
average test loss: 0.040005, accuracy: 0.0394
case acc: 0.024805734
case acc: 0.06656698
case acc: 0.051965475
case acc: 0.031343587
case acc: 0.025903717
case acc: 0.03556698
top acc: 0.0251 ::: bot acc: 0.0437
top acc: 0.1199 ::: bot acc: 0.0194
top acc: 0.0689 ::: bot acc: 0.0709
top acc: 0.0473 ::: bot acc: 0.0341
top acc: 0.0492 ::: bot acc: 0.0125
top acc: 0.0485 ::: bot acc: 0.0461
current epoch: 21
train loss is 0.063515
average val loss: 0.059744, accuracy: 0.0600
average test loss: 0.041529, accuracy: 0.0404
case acc: 0.025746107
case acc: 0.06943826
case acc: 0.051770616
case acc: 0.031604636
case acc: 0.02773186
case acc: 0.036319315
top acc: 0.0329 ::: bot acc: 0.0358
top acc: 0.1251 ::: bot acc: 0.0178
top acc: 0.0781 ::: bot acc: 0.0619
top acc: 0.0543 ::: bot acc: 0.0270
top acc: 0.0533 ::: bot acc: 0.0098
top acc: 0.0531 ::: bot acc: 0.0413
current epoch: 22
train loss is 0.064385
average val loss: 0.065425, accuracy: 0.0656
average test loss: 0.044194, accuracy: 0.0427
case acc: 0.028634986
case acc: 0.07350358
case acc: 0.052250907
case acc: 0.03283213
case acc: 0.030887881
case acc: 0.03782603
top acc: 0.0415 ::: bot acc: 0.0282
top acc: 0.1316 ::: bot acc: 0.0171
top acc: 0.0882 ::: bot acc: 0.0518
top acc: 0.0622 ::: bot acc: 0.0191
top acc: 0.0589 ::: bot acc: 0.0082
top acc: 0.0586 ::: bot acc: 0.0359
current epoch: 23
train loss is 0.065530
average val loss: 0.074299, accuracy: 0.0744
average test loss: 0.049505, accuracy: 0.0478
case acc: 0.03568481
case acc: 0.08066613
case acc: 0.054513555
case acc: 0.036958884
case acc: 0.037972763
case acc: 0.040899698
top acc: 0.0530 ::: bot acc: 0.0263
top acc: 0.1412 ::: bot acc: 0.0192
top acc: 0.1011 ::: bot acc: 0.0391
top acc: 0.0728 ::: bot acc: 0.0112
top acc: 0.0687 ::: bot acc: 0.0098
top acc: 0.0670 ::: bot acc: 0.0285
current epoch: 24
train loss is 0.068141
average val loss: 0.079798, accuracy: 0.0799
average test loss: 0.053257, accuracy: 0.0517
case acc: 0.040858638
case acc: 0.08455536
case acc: 0.05682762
case acc: 0.0410103
case acc: 0.04433063
case acc: 0.042907227
top acc: 0.0599 ::: bot acc: 0.0282
top acc: 0.1459 ::: bot acc: 0.0215
top acc: 0.1089 ::: bot acc: 0.0318
top acc: 0.0789 ::: bot acc: 0.0110
top acc: 0.0755 ::: bot acc: 0.0152
top acc: 0.0717 ::: bot acc: 0.0251
current epoch: 25
train loss is 0.069185
average val loss: 0.077615, accuracy: 0.0776
average test loss: 0.051655, accuracy: 0.0502
case acc: 0.03917105
case acc: 0.080424614
case acc: 0.05608504
case acc: 0.039548498
case acc: 0.043921735
case acc: 0.04184933
top acc: 0.0577 ::: bot acc: 0.0274
top acc: 0.1409 ::: bot acc: 0.0191
top acc: 0.1068 ::: bot acc: 0.0333
top acc: 0.0769 ::: bot acc: 0.0108
top acc: 0.0751 ::: bot acc: 0.0147
top acc: 0.0692 ::: bot acc: 0.0272
current epoch: 26
train loss is 0.068154
average val loss: 0.072371, accuracy: 0.0723
average test loss: 0.048036, accuracy: 0.0466
case acc: 0.034775116
case acc: 0.07350765
case acc: 0.054294735
case acc: 0.03627806
case acc: 0.041098595
case acc: 0.039830066
top acc: 0.0517 ::: bot acc: 0.0261
top acc: 0.1315 ::: bot acc: 0.0171
top acc: 0.1004 ::: bot acc: 0.0396
top acc: 0.0714 ::: bot acc: 0.0119
top acc: 0.0722 ::: bot acc: 0.0122
top acc: 0.0642 ::: bot acc: 0.0312
current epoch: 27
train loss is 0.066376
average val loss: 0.064148, accuracy: 0.0641
average test loss: 0.043060, accuracy: 0.0420
case acc: 0.02863184
case acc: 0.06543031
case acc: 0.05235956
case acc: 0.03293517
case acc: 0.035427704
case acc: 0.037345927
top acc: 0.0416 ::: bot acc: 0.0279
top acc: 0.1178 ::: bot acc: 0.0204
top acc: 0.0894 ::: bot acc: 0.0506
top acc: 0.0628 ::: bot acc: 0.0187
top acc: 0.0653 ::: bot acc: 0.0087
top acc: 0.0567 ::: bot acc: 0.0383
current epoch: 28
train loss is 0.062664
average val loss: 0.054505, accuracy: 0.0546
average test loss: 0.038732, accuracy: 0.0384
case acc: 0.025002439
case acc: 0.058082994
case acc: 0.051807154
case acc: 0.0314393
case acc: 0.028578596
case acc: 0.035432164
top acc: 0.0284 ::: bot acc: 0.0404
top acc: 0.1013 ::: bot acc: 0.0315
top acc: 0.0754 ::: bot acc: 0.0648
top acc: 0.0513 ::: bot acc: 0.0301
top acc: 0.0547 ::: bot acc: 0.0093
top acc: 0.0467 ::: bot acc: 0.0483
current epoch: 29
train loss is 0.059257
average val loss: 0.047676, accuracy: 0.0480
average test loss: 0.037206, accuracy: 0.0373
case acc: 0.025792941
case acc: 0.05422247
case acc: 0.052947033
case acc: 0.031425156
case acc: 0.024270914
case acc: 0.034927413
top acc: 0.0170 ::: bot acc: 0.0517
top acc: 0.0875 ::: bot acc: 0.0452
top acc: 0.0637 ::: bot acc: 0.0766
top acc: 0.0418 ::: bot acc: 0.0397
top acc: 0.0443 ::: bot acc: 0.0168
top acc: 0.0381 ::: bot acc: 0.0568
current epoch: 30
train loss is 0.057563
average val loss: 0.042702, accuracy: 0.0432
average test loss: 0.037693, accuracy: 0.0379
case acc: 0.02996693
case acc: 0.051839896
case acc: 0.05541994
case acc: 0.032313365
case acc: 0.022162147
case acc: 0.035902392
top acc: 0.0085 ::: bot acc: 0.0624
top acc: 0.0753 ::: bot acc: 0.0573
top acc: 0.0532 ::: bot acc: 0.0870
top acc: 0.0327 ::: bot acc: 0.0487
top acc: 0.0327 ::: bot acc: 0.0284
top acc: 0.0293 ::: bot acc: 0.0656
current epoch: 31
train loss is 0.057149
average val loss: 0.039162, accuracy: 0.0398
average test loss: 0.040737, accuracy: 0.0414
case acc: 0.03858338
case acc: 0.050552428
case acc: 0.05976635
case acc: 0.035118155
case acc: 0.025622476
case acc: 0.03892919
top acc: 0.0102 ::: bot acc: 0.0745
top acc: 0.0629 ::: bot acc: 0.0698
top acc: 0.0437 ::: bot acc: 0.0984
top acc: 0.0221 ::: bot acc: 0.0594
top acc: 0.0195 ::: bot acc: 0.0427
top acc: 0.0185 ::: bot acc: 0.0766
current epoch: 32
train loss is 0.057445
average val loss: 0.038293, accuracy: 0.0390
average test loss: 0.043828, accuracy: 0.0448
case acc: 0.044793535
case acc: 0.050429273
case acc: 0.06266672
case acc: 0.03746974
case acc: 0.031304486
case acc: 0.041836355
top acc: 0.0146 ::: bot acc: 0.0816
top acc: 0.0558 ::: bot acc: 0.0769
top acc: 0.0397 ::: bot acc: 0.1047
top acc: 0.0163 ::: bot acc: 0.0658
top acc: 0.0164 ::: bot acc: 0.0528
top acc: 0.0134 ::: bot acc: 0.0836
current epoch: 33
train loss is 0.057860
average val loss: 0.038291, accuracy: 0.0391
average test loss: 0.044563, accuracy: 0.0455
case acc: 0.045679577
case acc: 0.050437436
case acc: 0.06290636
case acc: 0.03787002
case acc: 0.033626497
case acc: 0.04263247
top acc: 0.0153 ::: bot acc: 0.0825
top acc: 0.0549 ::: bot acc: 0.0778
top acc: 0.0395 ::: bot acc: 0.1052
top acc: 0.0155 ::: bot acc: 0.0668
top acc: 0.0161 ::: bot acc: 0.0564
top acc: 0.0125 ::: bot acc: 0.0852
current epoch: 34
train loss is 0.057579
average val loss: 0.038372, accuracy: 0.0391
average test loss: 0.044132, accuracy: 0.0451
case acc: 0.044343736
case acc: 0.05043528
case acc: 0.062144324
case acc: 0.037416976
case acc: 0.034015153
case acc: 0.04226042
top acc: 0.0142 ::: bot acc: 0.0811
top acc: 0.0563 ::: bot acc: 0.0764
top acc: 0.0404 ::: bot acc: 0.1036
top acc: 0.0164 ::: bot acc: 0.0657
top acc: 0.0161 ::: bot acc: 0.0570
top acc: 0.0129 ::: bot acc: 0.0845
current epoch: 35
train loss is 0.057432
average val loss: 0.038612, accuracy: 0.0392
average test loss: 0.042826, accuracy: 0.0437
case acc: 0.04121876
case acc: 0.05046252
case acc: 0.060575277
case acc: 0.036380574
case acc: 0.03260557
case acc: 0.041074645
top acc: 0.0118 ::: bot acc: 0.0776
top acc: 0.0596 ::: bot acc: 0.0732
top acc: 0.0424 ::: bot acc: 0.1002
top acc: 0.0188 ::: bot acc: 0.0629
top acc: 0.0162 ::: bot acc: 0.0549
top acc: 0.0143 ::: bot acc: 0.0820
current epoch: 36
train loss is 0.057101
average val loss: 0.039000, accuracy: 0.0394
average test loss: 0.041655, accuracy: 0.0424
case acc: 0.038338337
case acc: 0.05058145
case acc: 0.059137404
case acc: 0.03543695
case acc: 0.031038424
case acc: 0.04012737
top acc: 0.0100 ::: bot acc: 0.0742
top acc: 0.0629 ::: bot acc: 0.0699
top acc: 0.0448 ::: bot acc: 0.0969
top acc: 0.0211 ::: bot acc: 0.0604
top acc: 0.0165 ::: bot acc: 0.0524
top acc: 0.0161 ::: bot acc: 0.0797
current epoch: 37
train loss is 0.057012
average val loss: 0.039532, accuracy: 0.0399
average test loss: 0.040689, accuracy: 0.0413
case acc: 0.035729244
case acc: 0.05078519
case acc: 0.05784151
case acc: 0.03460034
case acc: 0.02955601
case acc: 0.03937026
top acc: 0.0086 ::: bot acc: 0.0709
top acc: 0.0661 ::: bot acc: 0.0666
top acc: 0.0474 ::: bot acc: 0.0936
top acc: 0.0233 ::: bot acc: 0.0580
top acc: 0.0168 ::: bot acc: 0.0500
top acc: 0.0177 ::: bot acc: 0.0778
current epoch: 38
train loss is 0.056749
average val loss: 0.040455, accuracy: 0.0407
average test loss: 0.039567, accuracy: 0.0399
case acc: 0.03267765
case acc: 0.051208958
case acc: 0.056148447
case acc: 0.033617515
case acc: 0.027729083
case acc: 0.038302496
top acc: 0.0078 ::: bot acc: 0.0668
top acc: 0.0705 ::: bot acc: 0.0623
top acc: 0.0511 ::: bot acc: 0.0892
top acc: 0.0268 ::: bot acc: 0.0546
top acc: 0.0175 ::: bot acc: 0.0469
top acc: 0.0205 ::: bot acc: 0.0748
current epoch: 39
train loss is 0.056630
average val loss: 0.042321, accuracy: 0.0425
average test loss: 0.038351, accuracy: 0.0384
case acc: 0.029214479
case acc: 0.052162256
case acc: 0.054265488
case acc: 0.03233591
case acc: 0.02528229
case acc: 0.0369462
top acc: 0.0092 ::: bot acc: 0.0608
top acc: 0.0770 ::: bot acc: 0.0558
top acc: 0.0575 ::: bot acc: 0.0827
top acc: 0.0321 ::: bot acc: 0.0492
top acc: 0.0200 ::: bot acc: 0.0420
top acc: 0.0251 ::: bot acc: 0.0700
current epoch: 40
train loss is 0.056709
average val loss: 0.044619, accuracy: 0.0448
average test loss: 0.037743, accuracy: 0.0374
case acc: 0.02691551
case acc: 0.05343203
case acc: 0.052914523
case acc: 0.031669285
case acc: 0.02345758
case acc: 0.03593643
top acc: 0.0136 ::: bot acc: 0.0552
top acc: 0.0836 ::: bot acc: 0.0492
top acc: 0.0640 ::: bot acc: 0.0763
top acc: 0.0373 ::: bot acc: 0.0440
top acc: 0.0235 ::: bot acc: 0.0375
top acc: 0.0295 ::: bot acc: 0.0655
current epoch: 41
train loss is 0.057057
average val loss: 0.047492, accuracy: 0.0476
average test loss: 0.037697, accuracy: 0.0369
case acc: 0.025388569
case acc: 0.05499331
case acc: 0.052077234
case acc: 0.031344738
case acc: 0.02238909
case acc: 0.035148095
top acc: 0.0193 ::: bot acc: 0.0495
top acc: 0.0907 ::: bot acc: 0.0421
top acc: 0.0711 ::: bot acc: 0.0693
top acc: 0.0426 ::: bot acc: 0.0387
top acc: 0.0279 ::: bot acc: 0.0330
top acc: 0.0338 ::: bot acc: 0.0613
current epoch: 42
train loss is 0.057610
average val loss: 0.052483, accuracy: 0.0526
average test loss: 0.038667, accuracy: 0.0372
case acc: 0.024903204
case acc: 0.0579084
case acc: 0.051907733
case acc: 0.031378545
case acc: 0.022266831
case acc: 0.034865472
top acc: 0.0277 ::: bot acc: 0.0411
top acc: 0.1007 ::: bot acc: 0.0321
top acc: 0.0812 ::: bot acc: 0.0592
top acc: 0.0505 ::: bot acc: 0.0308
top acc: 0.0348 ::: bot acc: 0.0262
top acc: 0.0401 ::: bot acc: 0.0548
current epoch: 43
train loss is 0.058704
average val loss: 0.059547, accuracy: 0.0597
average test loss: 0.041428, accuracy: 0.0391
case acc: 0.027027585
case acc: 0.06291936
case acc: 0.052969124
case acc: 0.032377012
case acc: 0.023938911
case acc: 0.035456236
top acc: 0.0378 ::: bot acc: 0.0311
top acc: 0.1130 ::: bot acc: 0.0226
top acc: 0.0935 ::: bot acc: 0.0469
top acc: 0.0599 ::: bot acc: 0.0214
top acc: 0.0433 ::: bot acc: 0.0177
top acc: 0.0475 ::: bot acc: 0.0473
current epoch: 44
train loss is 0.060501
average val loss: 0.072467, accuracy: 0.0726
average test loss: 0.048541, accuracy: 0.0458
case acc: 0.03599352
case acc: 0.07332932
case acc: 0.058143165
case acc: 0.038325004
case acc: 0.0304574
case acc: 0.038430322
top acc: 0.0537 ::: bot acc: 0.0261
top acc: 0.1313 ::: bot acc: 0.0170
top acc: 0.1120 ::: bot acc: 0.0295
top acc: 0.0750 ::: bot acc: 0.0105
top acc: 0.0580 ::: bot acc: 0.0083
top acc: 0.0603 ::: bot acc: 0.0346
current epoch: 45
train loss is 0.064422
average val loss: 0.088497, accuracy: 0.0884
average test loss: 0.060011, accuracy: 0.0581
case acc: 0.0499396
case acc: 0.08902796
case acc: 0.06913958
case acc: 0.051444083
case acc: 0.044657655
case acc: 0.044515476
top acc: 0.0714 ::: bot acc: 0.0325
top acc: 0.1511 ::: bot acc: 0.0245
top acc: 0.1323 ::: bot acc: 0.0217
top acc: 0.0919 ::: bot acc: 0.0162
top acc: 0.0758 ::: bot acc: 0.0153
top acc: 0.0748 ::: bot acc: 0.0239
current epoch: 46
train loss is 0.070060
average val loss: 0.102123, accuracy: 0.1019
average test loss: 0.071032, accuracy: 0.0698
case acc: 0.061188143
case acc: 0.10200128
case acc: 0.08026336
case acc: 0.063194536
case acc: 0.06138926
case acc: 0.050663505
top acc: 0.0854 ::: bot acc: 0.0381
top acc: 0.1657 ::: bot acc: 0.0342
top acc: 0.1482 ::: bot acc: 0.0234
top acc: 0.1050 ::: bot acc: 0.0252
top acc: 0.0928 ::: bot acc: 0.0316
top acc: 0.0865 ::: bot acc: 0.0191
current epoch: 47
train loss is 0.079030
average val loss: 0.098826, accuracy: 0.0987
average test loss: 0.068163, accuracy: 0.0671
case acc: 0.05804215
case acc: 0.09541875
case acc: 0.0768569
case acc: 0.059679348
case acc: 0.0639265
case acc: 0.04875217
top acc: 0.0815 ::: bot acc: 0.0365
top acc: 0.1584 ::: bot acc: 0.0290
top acc: 0.1437 ::: bot acc: 0.0222
top acc: 0.1012 ::: bot acc: 0.0224
top acc: 0.0953 ::: bot acc: 0.0342
top acc: 0.0831 ::: bot acc: 0.0200
current epoch: 48
train loss is 0.080657
average val loss: 0.072428, accuracy: 0.0723
average test loss: 0.048141, accuracy: 0.0465
case acc: 0.03436825
case acc: 0.067512825
case acc: 0.057302747
case acc: 0.03763334
case acc: 0.04367113
case acc: 0.038446113
top acc: 0.0512 ::: bot acc: 0.0258
top acc: 0.1217 ::: bot acc: 0.0188
top acc: 0.1101 ::: bot acc: 0.0306
top acc: 0.0740 ::: bot acc: 0.0109
top acc: 0.0748 ::: bot acc: 0.0145
top acc: 0.0602 ::: bot acc: 0.0351
current epoch: 49
train loss is 0.070022
average val loss: 0.052789, accuracy: 0.0531
average test loss: 0.038222, accuracy: 0.0380
case acc: 0.024866067
case acc: 0.05425709
case acc: 0.051756695
case acc: 0.03150032
case acc: 0.030249417
case acc: 0.035141867
top acc: 0.0241 ::: bot acc: 0.0449
top acc: 0.0875 ::: bot acc: 0.0451
top acc: 0.0782 ::: bot acc: 0.0622
top acc: 0.0516 ::: bot acc: 0.0300
top acc: 0.0576 ::: bot acc: 0.0087
top acc: 0.0438 ::: bot acc: 0.0512
current epoch: 50
train loss is 0.059808
average val loss: 0.039279, accuracy: 0.0402
average test loss: 0.041379, accuracy: 0.0415
case acc: 0.04220013
case acc: 0.050626032
case acc: 0.06061511
case acc: 0.034822974
case acc: 0.022219956
case acc: 0.038269
top acc: 0.0125 ::: bot acc: 0.0788
top acc: 0.0480 ::: bot acc: 0.0846
top acc: 0.0426 ::: bot acc: 0.1003
top acc: 0.0226 ::: bot acc: 0.0589
top acc: 0.0305 ::: bot acc: 0.0306
top acc: 0.0202 ::: bot acc: 0.0747
LME_Co_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5352 5352 5352
1.7082474 -0.6288155 0.29835227 -0.25048217
Validation: 600 600 600
Testing: 744 744 744
pre-processing time: 0.00041985511779785156
the split date is 2011-01-01
net initializing with time: 0.0034973621368408203
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.167073
average val loss: 0.081461, accuracy: 0.0814
average test loss: 0.090881, accuracy: 0.0858
case acc: 0.058340285
case acc: 0.04502758
case acc: 0.048782375
case acc: 0.2371919
case acc: 0.060662463
case acc: 0.06493585
top acc: 0.1016 ::: bot acc: 0.0288
top acc: 0.0239 ::: bot acc: 0.0792
top acc: 0.0694 ::: bot acc: 0.0599
top acc: 0.2772 ::: bot acc: 0.2006
top acc: 0.0479 ::: bot acc: 0.0976
top acc: 0.0649 ::: bot acc: 0.0927
current epoch: 2
train loss is 0.134872
average val loss: 0.108291, accuracy: 0.1074
average test loss: 0.111867, accuracy: 0.1040
case acc: 0.05855734
case acc: 0.10524241
case acc: 0.103357755
case acc: 0.14191705
case acc: 0.10713876
case acc: 0.10798011
top acc: 0.0179 ::: bot acc: 0.1148
top acc: 0.0558 ::: bot acc: 0.1528
top acc: 0.0520 ::: bot acc: 0.1528
top acc: 0.1831 ::: bot acc: 0.1044
top acc: 0.0381 ::: bot acc: 0.1717
top acc: 0.0447 ::: bot acc: 0.1676
current epoch: 3
train loss is 0.124344
average val loss: 0.093883, accuracy: 0.0929
average test loss: 0.099434, accuracy: 0.0928
case acc: 0.052088037
case acc: 0.0839922
case acc: 0.096142255
case acc: 0.14854808
case acc: 0.08694257
case acc: 0.08932937
top acc: 0.0279 ::: bot acc: 0.0997
top acc: 0.0361 ::: bot acc: 0.1304
top acc: 0.0483 ::: bot acc: 0.1440
top acc: 0.1906 ::: bot acc: 0.1105
top acc: 0.0325 ::: bot acc: 0.1440
top acc: 0.0429 ::: bot acc: 0.1407
current epoch: 4
train loss is 0.107619
average val loss: 0.085673, accuracy: 0.0843
average test loss: 0.092460, accuracy: 0.0868
case acc: 0.05056872
case acc: 0.071628205
case acc: 0.09660817
case acc: 0.14548148
case acc: 0.07667334
case acc: 0.0796682
top acc: 0.0325 ::: bot acc: 0.0942
top acc: 0.0265 ::: bot acc: 0.1165
top acc: 0.0487 ::: bot acc: 0.1448
top acc: 0.1880 ::: bot acc: 0.1070
top acc: 0.0353 ::: bot acc: 0.1271
top acc: 0.0464 ::: bot acc: 0.1247
current epoch: 5
train loss is 0.099442
average val loss: 0.082966, accuracy: 0.0815
average test loss: 0.089860, accuracy: 0.0843
case acc: 0.05166199
case acc: 0.067216046
case acc: 0.10408358
case acc: 0.13273892
case acc: 0.07333641
case acc: 0.07694736
top acc: 0.0283 ::: bot acc: 0.0987
top acc: 0.0237 ::: bot acc: 0.1112
top acc: 0.0526 ::: bot acc: 0.1541
top acc: 0.1755 ::: bot acc: 0.0940
top acc: 0.0372 ::: bot acc: 0.1210
top acc: 0.0489 ::: bot acc: 0.1194
current epoch: 6
train loss is 0.096976
average val loss: 0.082802, accuracy: 0.0813
average test loss: 0.089270, accuracy: 0.0836
case acc: 0.05479403
case acc: 0.066404946
case acc: 0.11412004
case acc: 0.11613307
case acc: 0.072934076
case acc: 0.07695795
top acc: 0.0214 ::: bot acc: 0.1068
top acc: 0.0233 ::: bot acc: 0.1101
top acc: 0.0578 ::: bot acc: 0.1666
top acc: 0.1593 ::: bot acc: 0.0771
top acc: 0.0375 ::: bot acc: 0.1201
top acc: 0.0491 ::: bot acc: 0.1194
current epoch: 7
train loss is 0.095314
average val loss: 0.088572, accuracy: 0.0871
average test loss: 0.093313, accuracy: 0.0869
case acc: 0.06387856
case acc: 0.07463116
case acc: 0.13117537
case acc: 0.09115465
case acc: 0.07821383
case acc: 0.0822794
top acc: 0.0148 ::: bot acc: 0.1235
top acc: 0.0293 ::: bot acc: 0.1193
top acc: 0.0673 ::: bot acc: 0.1876
top acc: 0.1343 ::: bot acc: 0.0525
top acc: 0.0345 ::: bot acc: 0.1294
top acc: 0.0452 ::: bot acc: 0.1294
current epoch: 8
train loss is 0.096911
average val loss: 0.094147, accuracy: 0.0927
average test loss: 0.097608, accuracy: 0.0908
case acc: 0.07461792
case acc: 0.08222995
case acc: 0.14696838
case acc: 0.06904682
case acc: 0.083588645
case acc: 0.08821941
top acc: 0.0177 ::: bot acc: 0.1380
top acc: 0.0356 ::: bot acc: 0.1275
top acc: 0.0780 ::: bot acc: 0.2060
top acc: 0.1111 ::: bot acc: 0.0328
top acc: 0.0320 ::: bot acc: 0.1386
top acc: 0.0433 ::: bot acc: 0.1392
current epoch: 9
train loss is 0.100448
average val loss: 0.093843, accuracy: 0.0924
average test loss: 0.097162, accuracy: 0.0902
case acc: 0.07801322
case acc: 0.079202674
case acc: 0.15290977
case acc: 0.05747794
case acc: 0.084294565
case acc: 0.089406855
top acc: 0.0199 ::: bot acc: 0.1420
top acc: 0.0332 ::: bot acc: 0.1241
top acc: 0.0823 ::: bot acc: 0.2128
top acc: 0.0982 ::: bot acc: 0.0239
top acc: 0.0318 ::: bot acc: 0.1397
top acc: 0.0433 ::: bot acc: 0.1410
current epoch: 10
train loss is 0.104110
average val loss: 0.083283, accuracy: 0.0821
average test loss: 0.088263, accuracy: 0.0816
case acc: 0.06765811
case acc: 0.05986486
case acc: 0.14206597
case acc: 0.06104369
case acc: 0.07682787
case acc: 0.082084805
top acc: 0.0149 ::: bot acc: 0.1290
top acc: 0.0199 ::: bot acc: 0.1018
top acc: 0.0743 ::: bot acc: 0.2006
top acc: 0.1022 ::: bot acc: 0.0266
top acc: 0.0353 ::: bot acc: 0.1268
top acc: 0.0457 ::: bot acc: 0.1288
current epoch: 11
train loss is 0.098101
average val loss: 0.070327, accuracy: 0.0704
average test loss: 0.078011, accuracy: 0.0724
case acc: 0.056148347
case acc: 0.041435488
case acc: 0.12454038
case acc: 0.07096423
case acc: 0.06759898
case acc: 0.073617175
top acc: 0.0200 ::: bot acc: 0.1092
top acc: 0.0242 ::: bot acc: 0.0722
top acc: 0.0636 ::: bot acc: 0.1797
top acc: 0.1131 ::: bot acc: 0.0344
top acc: 0.0417 ::: bot acc: 0.1098
top acc: 0.0524 ::: bot acc: 0.1128
current epoch: 12
train loss is 0.085922
average val loss: 0.063954, accuracy: 0.0645
average test loss: 0.072847, accuracy: 0.0681
case acc: 0.051476926
case acc: 0.035257116
case acc: 0.11374693
case acc: 0.07394545
case acc: 0.06381793
case acc: 0.0701519
top acc: 0.0286 ::: bot acc: 0.0980
top acc: 0.0413 ::: bot acc: 0.0527
top acc: 0.0578 ::: bot acc: 0.1665
top acc: 0.1164 ::: bot acc: 0.0368
top acc: 0.0469 ::: bot acc: 0.1015
top acc: 0.0570 ::: bot acc: 0.1054
current epoch: 13
train loss is 0.077481
average val loss: 0.059779, accuracy: 0.0602
average test loss: 0.069345, accuracy: 0.0655
case acc: 0.049511805
case acc: 0.034738477
case acc: 0.10560967
case acc: 0.07480645
case acc: 0.06110836
case acc: 0.067418434
top acc: 0.0368 ::: bot acc: 0.0897
top acc: 0.0530 ::: bot acc: 0.0409
top acc: 0.0536 ::: bot acc: 0.1564
top acc: 0.1174 ::: bot acc: 0.0375
top acc: 0.0528 ::: bot acc: 0.0945
top acc: 0.0622 ::: bot acc: 0.0987
current epoch: 14
train loss is 0.074658
average val loss: 0.056319, accuracy: 0.0563
average test loss: 0.066476, accuracy: 0.0635
case acc: 0.048713498
case acc: 0.03515974
case acc: 0.0991312
case acc: 0.07470324
case acc: 0.05839575
case acc: 0.06468838
top acc: 0.0433 ::: bot acc: 0.0832
top acc: 0.0584 ::: bot acc: 0.0356
top acc: 0.0503 ::: bot acc: 0.1484
top acc: 0.1174 ::: bot acc: 0.0373
top acc: 0.0599 ::: bot acc: 0.0868
top acc: 0.0696 ::: bot acc: 0.0908
current epoch: 15
train loss is 0.073602
average val loss: 0.056075, accuracy: 0.0560
average test loss: 0.066118, accuracy: 0.0625
case acc: 0.04927142
case acc: 0.034717616
case acc: 0.102787755
case acc: 0.06408494
case acc: 0.059133288
case acc: 0.06516631
top acc: 0.0377 ::: bot acc: 0.0887
top acc: 0.0479 ::: bot acc: 0.0461
top acc: 0.0522 ::: bot acc: 0.1530
top acc: 0.1059 ::: bot acc: 0.0287
top acc: 0.0578 ::: bot acc: 0.0890
top acc: 0.0683 ::: bot acc: 0.0922
current epoch: 16
train loss is 0.074059
average val loss: 0.059130, accuracy: 0.0584
average test loss: 0.068275, accuracy: 0.0636
case acc: 0.052608233
case acc: 0.03852317
case acc: 0.1127266
case acc: 0.047907785
case acc: 0.06202
case acc: 0.06769493
top acc: 0.0256 ::: bot acc: 0.1012
top acc: 0.0294 ::: bot acc: 0.0652
top acc: 0.0573 ::: bot acc: 0.1653
top acc: 0.0874 ::: bot acc: 0.0172
top acc: 0.0507 ::: bot acc: 0.0968
top acc: 0.0618 ::: bot acc: 0.0992
current epoch: 17
train loss is 0.074344
average val loss: 0.067924, accuracy: 0.0662
average test loss: 0.075325, accuracy: 0.0696
case acc: 0.062056713
case acc: 0.051702473
case acc: 0.12886117
case acc: 0.032417405
case acc: 0.06853072
case acc: 0.07395966
top acc: 0.0153 ::: bot acc: 0.1204
top acc: 0.0183 ::: bot acc: 0.0904
top acc: 0.0659 ::: bot acc: 0.1853
top acc: 0.0620 ::: bot acc: 0.0215
top acc: 0.0409 ::: bot acc: 0.1113
top acc: 0.0523 ::: bot acc: 0.1133
current epoch: 18
train loss is 0.079925
average val loss: 0.079484, accuracy: 0.0781
average test loss: 0.085418, accuracy: 0.0800
case acc: 0.07535547
case acc: 0.067562334
case acc: 0.14595279
case acc: 0.03188761
case acc: 0.076968886
case acc: 0.082184985
top acc: 0.0177 ::: bot acc: 0.1391
top acc: 0.0244 ::: bot acc: 0.1110
top acc: 0.0772 ::: bot acc: 0.2053
top acc: 0.0369 ::: bot acc: 0.0472
top acc: 0.0352 ::: bot acc: 0.1267
top acc: 0.0456 ::: bot acc: 0.1289
current epoch: 19
train loss is 0.089527
average val loss: 0.073060, accuracy: 0.0718
average test loss: 0.080105, accuracy: 0.0749
case acc: 0.06937584
case acc: 0.057679188
case acc: 0.13972126
case acc: 0.03185311
case acc: 0.07236349
case acc: 0.07813858
top acc: 0.0150 ::: bot acc: 0.1315
top acc: 0.0192 ::: bot acc: 0.0988
top acc: 0.0726 ::: bot acc: 0.1983
top acc: 0.0369 ::: bot acc: 0.0472
top acc: 0.0380 ::: bot acc: 0.1184
top acc: 0.0483 ::: bot acc: 0.1215
current epoch: 20
train loss is 0.088158
average val loss: 0.057355, accuracy: 0.0565
average test loss: 0.066954, accuracy: 0.0624
case acc: 0.0553075
case acc: 0.038810086
case acc: 0.11888287
case acc: 0.030594269
case acc: 0.0621582
case acc: 0.068621375
top acc: 0.0211 ::: bot acc: 0.1075
top acc: 0.0287 ::: bot acc: 0.0660
top acc: 0.0605 ::: bot acc: 0.1731
top acc: 0.0530 ::: bot acc: 0.0304
top acc: 0.0505 ::: bot acc: 0.0970
top acc: 0.0599 ::: bot acc: 0.1016
current epoch: 21
train loss is 0.075123
average val loss: 0.052368, accuracy: 0.0520
average test loss: 0.062660, accuracy: 0.0588
case acc: 0.051088903
case acc: 0.03474774
case acc: 0.10914454
case acc: 0.031230412
case acc: 0.059799146
case acc: 0.0666888
top acc: 0.0294 ::: bot acc: 0.0970
top acc: 0.0469 ::: bot acc: 0.0472
top acc: 0.0555 ::: bot acc: 0.1611
top acc: 0.0567 ::: bot acc: 0.0269
top acc: 0.0561 ::: bot acc: 0.0907
top acc: 0.0638 ::: bot acc: 0.0968
current epoch: 22
train loss is 0.066908
average val loss: 0.050158, accuracy: 0.0499
average test loss: 0.060698, accuracy: 0.0574
case acc: 0.049587842
case acc: 0.0349487
case acc: 0.103234224
case acc: 0.03118052
case acc: 0.058999043
case acc: 0.066217184
top acc: 0.0347 ::: bot acc: 0.0915
top acc: 0.0561 ::: bot acc: 0.0379
top acc: 0.0524 ::: bot acc: 0.1537
top acc: 0.0565 ::: bot acc: 0.0273
top acc: 0.0584 ::: bot acc: 0.0884
top acc: 0.0654 ::: bot acc: 0.0953
current epoch: 23
train loss is 0.063202
average val loss: 0.048227, accuracy: 0.0478
average test loss: 0.059094, accuracy: 0.0562
case acc: 0.048925005
case acc: 0.03524248
case acc: 0.098407604
case acc: 0.031082755
case acc: 0.058063895
case acc: 0.06529724
top acc: 0.0389 ::: bot acc: 0.0873
top acc: 0.0596 ::: bot acc: 0.0344
top acc: 0.0499 ::: bot acc: 0.1478
top acc: 0.0559 ::: bot acc: 0.0280
top acc: 0.0612 ::: bot acc: 0.0854
top acc: 0.0682 ::: bot acc: 0.0924
current epoch: 24
train loss is 0.061994
average val loss: 0.047681, accuracy: 0.0473
average test loss: 0.058863, accuracy: 0.0559
case acc: 0.048922505
case acc: 0.034803018
case acc: 0.097735986
case acc: 0.030479494
case acc: 0.058227018
case acc: 0.06532634
top acc: 0.0383 ::: bot acc: 0.0878
top acc: 0.0550 ::: bot acc: 0.0390
top acc: 0.0495 ::: bot acc: 0.1470
top acc: 0.0510 ::: bot acc: 0.0330
top acc: 0.0610 ::: bot acc: 0.0857
top acc: 0.0684 ::: bot acc: 0.0923
current epoch: 25
train loss is 0.061426
average val loss: 0.048922, accuracy: 0.0488
average test loss: 0.060195, accuracy: 0.0570
case acc: 0.049673814
case acc: 0.034860015
case acc: 0.10087289
case acc: 0.030577147
case acc: 0.059451368
case acc: 0.066323355
top acc: 0.0334 ::: bot acc: 0.0926
top acc: 0.0445 ::: bot acc: 0.0495
top acc: 0.0511 ::: bot acc: 0.1509
top acc: 0.0419 ::: bot acc: 0.0421
top acc: 0.0576 ::: bot acc: 0.0893
top acc: 0.0654 ::: bot acc: 0.0953
current epoch: 26
train loss is 0.061959
average val loss: 0.053162, accuracy: 0.0535
average test loss: 0.064001, accuracy: 0.0609
case acc: 0.05274396
case acc: 0.038019646
case acc: 0.10789542
case acc: 0.035687894
case acc: 0.062168285
case acc: 0.068934195
top acc: 0.0246 ::: bot acc: 0.1019
top acc: 0.0306 ::: bot acc: 0.0637
top acc: 0.0547 ::: bot acc: 0.1597
top acc: 0.0313 ::: bot acc: 0.0559
top acc: 0.0509 ::: bot acc: 0.0967
top acc: 0.0592 ::: bot acc: 0.1023
current epoch: 27
train loss is 0.063313
average val loss: 0.057999, accuracy: 0.0583
average test loss: 0.068197, accuracy: 0.0656
case acc: 0.05638008
case acc: 0.042758923
case acc: 0.11437545
case acc: 0.043487806
case acc: 0.06477016
case acc: 0.071692415
top acc: 0.0190 ::: bot acc: 0.1101
top acc: 0.0227 ::: bot acc: 0.0747
top acc: 0.0580 ::: bot acc: 0.1678
top acc: 0.0284 ::: bot acc: 0.0689
top acc: 0.0457 ::: bot acc: 0.1031
top acc: 0.0546 ::: bot acc: 0.1087
current epoch: 28
train loss is 0.066076
average val loss: 0.062324, accuracy: 0.0626
average test loss: 0.071797, accuracy: 0.0696
case acc: 0.059698705
case acc: 0.045757562
case acc: 0.11934317
case acc: 0.05158807
case acc: 0.06709535
case acc: 0.074383326
top acc: 0.0165 ::: bot acc: 0.1162
top acc: 0.0198 ::: bot acc: 0.0805
top acc: 0.0606 ::: bot acc: 0.1740
top acc: 0.0297 ::: bot acc: 0.0804
top acc: 0.0425 ::: bot acc: 0.1082
top acc: 0.0515 ::: bot acc: 0.1142
current epoch: 29
train loss is 0.068270
average val loss: 0.060307, accuracy: 0.0609
average test loss: 0.070198, accuracy: 0.0684
case acc: 0.058169074
case acc: 0.042638727
case acc: 0.11681112
case acc: 0.053519413
case acc: 0.06579774
case acc: 0.07339473
top acc: 0.0176 ::: bot acc: 0.1134
top acc: 0.0229 ::: bot acc: 0.0744
top acc: 0.0593 ::: bot acc: 0.1708
top acc: 0.0304 ::: bot acc: 0.0830
top acc: 0.0441 ::: bot acc: 0.1054
top acc: 0.0524 ::: bot acc: 0.1123
current epoch: 30
train loss is 0.067669
average val loss: 0.054220, accuracy: 0.0553
average test loss: 0.065214, accuracy: 0.0637
case acc: 0.05371726
case acc: 0.036887232
case acc: 0.10854373
case acc: 0.050253034
case acc: 0.062511496
case acc: 0.07020928
top acc: 0.0233 ::: bot acc: 0.1040
top acc: 0.0342 ::: bot acc: 0.0601
top acc: 0.0551 ::: bot acc: 0.1606
top acc: 0.0292 ::: bot acc: 0.0787
top acc: 0.0499 ::: bot acc: 0.0977
top acc: 0.0564 ::: bot acc: 0.1056
current epoch: 31
train loss is 0.063919
average val loss: 0.047677, accuracy: 0.0488
average test loss: 0.059747, accuracy: 0.0586
case acc: 0.049617257
case acc: 0.03473024
case acc: 0.0973793
case acc: 0.04446717
case acc: 0.058888476
case acc: 0.066694535
top acc: 0.0348 ::: bot acc: 0.0915
top acc: 0.0511 ::: bot acc: 0.0432
top acc: 0.0493 ::: bot acc: 0.1468
top acc: 0.0281 ::: bot acc: 0.0706
top acc: 0.0585 ::: bot acc: 0.0880
top acc: 0.0636 ::: bot acc: 0.0968
current epoch: 32
train loss is 0.059434
average val loss: 0.043464, accuracy: 0.0441
average test loss: 0.056010, accuracy: 0.0555
case acc: 0.048527967
case acc: 0.036082342
case acc: 0.08738793
case acc: 0.039819017
case acc: 0.0568321
case acc: 0.06428831
top acc: 0.0456 ::: bot acc: 0.0808
top acc: 0.0641 ::: bot acc: 0.0303
top acc: 0.0446 ::: bot acc: 0.1342
top acc: 0.0291 ::: bot acc: 0.0631
top acc: 0.0661 ::: bot acc: 0.0803
top acc: 0.0707 ::: bot acc: 0.0895
current epoch: 33
train loss is 0.056755
average val loss: 0.040493, accuracy: 0.0406
average test loss: 0.053171, accuracy: 0.0531
case acc: 0.048407987
case acc: 0.0391177
case acc: 0.07810976
case acc: 0.035167806
case acc: 0.055476174
case acc: 0.062022742
top acc: 0.0559 ::: bot acc: 0.0705
top acc: 0.0735 ::: bot acc: 0.0218
top acc: 0.0418 ::: bot acc: 0.1217
top acc: 0.0315 ::: bot acc: 0.0550
top acc: 0.0736 ::: bot acc: 0.0728
top acc: 0.0786 ::: bot acc: 0.0817
current epoch: 34
train loss is 0.055504
average val loss: 0.038752, accuracy: 0.0384
average test loss: 0.051286, accuracy: 0.0513
case acc: 0.0489211
case acc: 0.04190541
case acc: 0.07052527
case acc: 0.03172113
case acc: 0.05463454
case acc: 0.06025211
top acc: 0.0652 ::: bot acc: 0.0612
top acc: 0.0792 ::: bot acc: 0.0188
top acc: 0.0423 ::: bot acc: 0.1101
top acc: 0.0372 ::: bot acc: 0.0469
top acc: 0.0807 ::: bot acc: 0.0658
top acc: 0.0864 ::: bot acc: 0.0740
current epoch: 35
train loss is 0.054956
average val loss: 0.038017, accuracy: 0.0375
average test loss: 0.050435, accuracy: 0.0504
case acc: 0.049347945
case acc: 0.04180842
case acc: 0.06648831
case acc: 0.030687
case acc: 0.054438144
case acc: 0.059526574
top acc: 0.0700 ::: bot acc: 0.0563
top acc: 0.0790 ::: bot acc: 0.0188
top acc: 0.0446 ::: bot acc: 0.1030
top acc: 0.0416 ::: bot acc: 0.0426
top acc: 0.0839 ::: bot acc: 0.0626
top acc: 0.0908 ::: bot acc: 0.0696
current epoch: 36
train loss is 0.054574
average val loss: 0.037522, accuracy: 0.0370
average test loss: 0.049965, accuracy: 0.0497
case acc: 0.049508844
case acc: 0.040239017
case acc: 0.06426289
case acc: 0.030357836
case acc: 0.054387912
case acc: 0.059328012
top acc: 0.0719 ::: bot acc: 0.0543
top acc: 0.0759 ::: bot acc: 0.0203
top acc: 0.0463 ::: bot acc: 0.0988
top acc: 0.0435 ::: bot acc: 0.0408
top acc: 0.0849 ::: bot acc: 0.0616
top acc: 0.0931 ::: bot acc: 0.0673
current epoch: 37
train loss is 0.054199
average val loss: 0.037171, accuracy: 0.0367
average test loss: 0.049919, accuracy: 0.0493
case acc: 0.049337257
case acc: 0.037529
case acc: 0.064178064
case acc: 0.030757565
case acc: 0.054522425
case acc: 0.05938713
top acc: 0.0701 ::: bot acc: 0.0561
top acc: 0.0696 ::: bot acc: 0.0249
top acc: 0.0463 ::: bot acc: 0.0987
top acc: 0.0415 ::: bot acc: 0.0428
top acc: 0.0829 ::: bot acc: 0.0636
top acc: 0.0923 ::: bot acc: 0.0681
current epoch: 38
train loss is 0.053726
average val loss: 0.037404, accuracy: 0.0373
average test loss: 0.050672, accuracy: 0.0497
case acc: 0.04877823
case acc: 0.035469767
case acc: 0.06645401
case acc: 0.032593187
case acc: 0.05496319
case acc: 0.059953112
top acc: 0.0642 ::: bot acc: 0.0621
top acc: 0.0608 ::: bot acc: 0.0338
top acc: 0.0446 ::: bot acc: 0.1030
top acc: 0.0352 ::: bot acc: 0.0493
top acc: 0.0775 ::: bot acc: 0.0690
top acc: 0.0878 ::: bot acc: 0.0725
current epoch: 39
train loss is 0.053544
average val loss: 0.038568, accuracy: 0.0388
average test loss: 0.052194, accuracy: 0.0512
case acc: 0.048372526
case acc: 0.03481028
case acc: 0.069898635
case acc: 0.036858033
case acc: 0.05592365
case acc: 0.06113458
top acc: 0.0569 ::: bot acc: 0.0694
top acc: 0.0520 ::: bot acc: 0.0426
top acc: 0.0427 ::: bot acc: 0.1091
top acc: 0.0306 ::: bot acc: 0.0579
top acc: 0.0714 ::: bot acc: 0.0752
top acc: 0.0823 ::: bot acc: 0.0781
current epoch: 40
train loss is 0.053450
average val loss: 0.040842, accuracy: 0.0415
average test loss: 0.054433, accuracy: 0.0535
case acc: 0.04837026
case acc: 0.035155926
case acc: 0.07444505
case acc: 0.042909317
case acc: 0.05720795
case acc: 0.06276701
top acc: 0.0488 ::: bot acc: 0.0774
top acc: 0.0435 ::: bot acc: 0.0511
top acc: 0.0416 ::: bot acc: 0.1164
top acc: 0.0285 ::: bot acc: 0.0680
top acc: 0.0650 ::: bot acc: 0.0816
top acc: 0.0762 ::: bot acc: 0.0842
current epoch: 41
train loss is 0.053848
average val loss: 0.044136, accuracy: 0.0449
average test loss: 0.057257, accuracy: 0.0565
case acc: 0.048673436
case acc: 0.036668885
case acc: 0.07984649
case acc: 0.05035521
case acc: 0.05882383
case acc: 0.064614624
top acc: 0.0408 ::: bot acc: 0.0854
top acc: 0.0358 ::: bot acc: 0.0588
top acc: 0.0421 ::: bot acc: 0.1243
top acc: 0.0293 ::: bot acc: 0.0788
top acc: 0.0589 ::: bot acc: 0.0876
top acc: 0.0700 ::: bot acc: 0.0903
current epoch: 42
train loss is 0.054586
average val loss: 0.048558, accuracy: 0.0492
average test loss: 0.060818, accuracy: 0.0604
case acc: 0.04999746
case acc: 0.03908727
case acc: 0.08609194
case acc: 0.05930193
case acc: 0.061040033
case acc: 0.06669765
top acc: 0.0327 ::: bot acc: 0.0936
top acc: 0.0286 ::: bot acc: 0.0662
top acc: 0.0441 ::: bot acc: 0.1326
top acc: 0.0332 ::: bot acc: 0.0902
top acc: 0.0532 ::: bot acc: 0.0937
top acc: 0.0637 ::: bot acc: 0.0967
current epoch: 43
train loss is 0.055457
average val loss: 0.052330, accuracy: 0.0529
average test loss: 0.063790, accuracy: 0.0637
case acc: 0.051787328
case acc: 0.041085005
case acc: 0.09096166
case acc: 0.067146145
case acc: 0.06254127
case acc: 0.06840084
top acc: 0.0270 ::: bot acc: 0.0994
top acc: 0.0249 ::: bot acc: 0.0710
top acc: 0.0463 ::: bot acc: 0.1388
top acc: 0.0377 ::: bot acc: 0.0997
top acc: 0.0497 ::: bot acc: 0.0977
top acc: 0.0597 ::: bot acc: 0.1012
current epoch: 44
train loss is 0.056648
average val loss: 0.056495, accuracy: 0.0571
average test loss: 0.067060, accuracy: 0.0672
case acc: 0.054095622
case acc: 0.042954423
case acc: 0.095849365
case acc: 0.07546155
case acc: 0.064279065
case acc: 0.070377015
top acc: 0.0226 ::: bot acc: 0.1050
top acc: 0.0228 ::: bot acc: 0.0748
top acc: 0.0485 ::: bot acc: 0.1450
top acc: 0.0435 ::: bot acc: 0.1093
top acc: 0.0466 ::: bot acc: 0.1018
top acc: 0.0557 ::: bot acc: 0.1061
current epoch: 45
train loss is 0.057951
average val loss: 0.057711, accuracy: 0.0585
average test loss: 0.068015, accuracy: 0.0683
case acc: 0.05459423
case acc: 0.04233168
case acc: 0.09726999
case acc: 0.08009814
case acc: 0.06447917
case acc: 0.07103937
top acc: 0.0218 ::: bot acc: 0.1061
top acc: 0.0235 ::: bot acc: 0.0736
top acc: 0.0493 ::: bot acc: 0.1468
top acc: 0.0469 ::: bot acc: 0.1146
top acc: 0.0462 ::: bot acc: 0.1023
top acc: 0.0547 ::: bot acc: 0.1077
current epoch: 46
train loss is 0.058239
average val loss: 0.057234, accuracy: 0.0583
average test loss: 0.067636, accuracy: 0.0681
case acc: 0.054004338
case acc: 0.040473234
case acc: 0.09639813
case acc: 0.08235897
case acc: 0.06405554
case acc: 0.07104686
top acc: 0.0227 ::: bot acc: 0.1048
top acc: 0.0259 ::: bot acc: 0.0696
top acc: 0.0488 ::: bot acc: 0.1457
top acc: 0.0486 ::: bot acc: 0.1171
top acc: 0.0470 ::: bot acc: 0.1013
top acc: 0.0547 ::: bot acc: 0.1077
current epoch: 47
train loss is 0.058678
average val loss: 0.052954, accuracy: 0.0543
average test loss: 0.064230, accuracy: 0.0648
case acc: 0.051267892
case acc: 0.03676343
case acc: 0.090712875
case acc: 0.07917275
case acc: 0.06186987
case acc: 0.06929225
top acc: 0.0286 ::: bot acc: 0.0979
top acc: 0.0355 ::: bot acc: 0.0591
top acc: 0.0461 ::: bot acc: 0.1385
top acc: 0.0462 ::: bot acc: 0.1135
top acc: 0.0511 ::: bot acc: 0.0960
top acc: 0.0578 ::: bot acc: 0.1035
current epoch: 48
train loss is 0.057423
average val loss: 0.045994, accuracy: 0.0473
average test loss: 0.058582, accuracy: 0.0597
case acc: 0.048715502
case acc: 0.034815546
case acc: 0.08013496
case acc: 0.07009569
case acc: 0.058352035
case acc: 0.0659153
top acc: 0.0415 ::: bot acc: 0.0850
top acc: 0.0515 ::: bot acc: 0.0431
top acc: 0.0422 ::: bot acc: 0.1246
top acc: 0.0398 ::: bot acc: 0.1031
top acc: 0.0601 ::: bot acc: 0.0863
top acc: 0.0656 ::: bot acc: 0.0945
current epoch: 49
train loss is 0.056125
average val loss: 0.039358, accuracy: 0.0401
average test loss: 0.052675, accuracy: 0.0545
case acc: 0.04862112
case acc: 0.03840099
case acc: 0.06709973
case acc: 0.05549663
case acc: 0.05540713
case acc: 0.061805233
top acc: 0.0602 ::: bot acc: 0.0664
top acc: 0.0720 ::: bot acc: 0.0229
top acc: 0.0443 ::: bot acc: 0.1041
top acc: 0.0313 ::: bot acc: 0.0855
top acc: 0.0740 ::: bot acc: 0.0723
top acc: 0.0793 ::: bot acc: 0.0809
current epoch: 50
train loss is 0.055703
average val loss: 0.038539, accuracy: 0.0383
average test loss: 0.049802, accuracy: 0.0518
case acc: 0.052203253
case acc: 0.054597903
case acc: 0.053666364
case acc: 0.036955886
case acc: 0.05404774
case acc: 0.059114475
top acc: 0.0863 ::: bot acc: 0.0402
top acc: 0.0977 ::: bot acc: 0.0199
top acc: 0.0627 ::: bot acc: 0.0749
top acc: 0.0304 ::: bot acc: 0.0582
top acc: 0.0941 ::: bot acc: 0.0523
top acc: 0.1000 ::: bot acc: 0.0602
LME_Co_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5328 5328 5328
1.7082474 -0.6288155 0.29835227 -0.25048217
Validation: 594 594 594
Testing: 774 774 774
pre-processing time: 0.0005147457122802734
the split date is 2011-07-01
net initializing with time: 0.003974437713623047
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.279754
average val loss: 0.200689, accuracy: 0.2036
average test loss: 0.206793, accuracy: 0.2034
case acc: 0.31740564
case acc: 0.2545362
case acc: 0.30502746
case acc: 0.08182939
case acc: 0.05886789
case acc: 0.20290643
top acc: 0.2644 ::: bot acc: 0.3715
top acc: 0.2094 ::: bot acc: 0.2961
top acc: 0.2408 ::: bot acc: 0.3719
top acc: 0.0336 ::: bot acc: 0.1414
top acc: 0.1101 ::: bot acc: 0.0186
top acc: 0.1404 ::: bot acc: 0.2563
current epoch: 2
train loss is 0.245610
average val loss: 0.121923, accuracy: 0.1182
average test loss: 0.120465, accuracy: 0.1212
case acc: 0.14001483
case acc: 0.07894574
case acc: 0.12964341
case acc: 0.104388185
case acc: 0.22470994
case acc: 0.049364865
top acc: 0.0871 ::: bot acc: 0.1940
top acc: 0.0346 ::: bot acc: 0.1201
top acc: 0.0651 ::: bot acc: 0.1963
top acc: 0.1706 ::: bot acc: 0.0371
top acc: 0.2862 ::: bot acc: 0.1640
top acc: 0.0395 ::: bot acc: 0.0769
current epoch: 3
train loss is 0.140857
average val loss: 0.130567, accuracy: 0.1272
average test loss: 0.132959, accuracy: 0.1324
case acc: 0.188531
case acc: 0.12958382
case acc: 0.17829192
case acc: 0.061437305
case acc: 0.15704295
case acc: 0.079322465
top acc: 0.1356 ::: bot acc: 0.2425
top acc: 0.0849 ::: bot acc: 0.1709
top acc: 0.1135 ::: bot acc: 0.2451
top acc: 0.1109 ::: bot acc: 0.0275
top acc: 0.2187 ::: bot acc: 0.0963
top acc: 0.0238 ::: bot acc: 0.1300
current epoch: 4
train loss is 0.168583
average val loss: 0.112618, accuracy: 0.1090
average test loss: 0.111416, accuracy: 0.1118
case acc: 0.13681725
case acc: 0.08114375
case acc: 0.12730305
case acc: 0.08642028
case acc: 0.18852901
case acc: 0.050814092
top acc: 0.0838 ::: bot acc: 0.1907
top acc: 0.0367 ::: bot acc: 0.1223
top acc: 0.0626 ::: bot acc: 0.1941
top acc: 0.1507 ::: bot acc: 0.0226
top acc: 0.2503 ::: bot acc: 0.1277
top acc: 0.0360 ::: bot acc: 0.0813
current epoch: 5
train loss is 0.129650
average val loss: 0.116063, accuracy: 0.1130
average test loss: 0.117473, accuracy: 0.1171
case acc: 0.16567951
case acc: 0.11262499
case acc: 0.15561676
case acc: 0.06038509
case acc: 0.13986729
case acc: 0.06818588
top acc: 0.1127 ::: bot acc: 0.2195
top acc: 0.0680 ::: bot acc: 0.1539
top acc: 0.0907 ::: bot acc: 0.2226
top acc: 0.1086 ::: bot acc: 0.0284
top acc: 0.2017 ::: bot acc: 0.0790
top acc: 0.0204 ::: bot acc: 0.1154
current epoch: 6
train loss is 0.145691
average val loss: 0.103855, accuracy: 0.1003
average test loss: 0.103195, accuracy: 0.1033
case acc: 0.13536398
case acc: 0.0854562
case acc: 0.12543371
case acc: 0.06904057
case acc: 0.15062933
case acc: 0.053716872
top acc: 0.0823 ::: bot acc: 0.1892
top acc: 0.0409 ::: bot acc: 0.1267
top acc: 0.0607 ::: bot acc: 0.1923
top acc: 0.1263 ::: bot acc: 0.0188
top acc: 0.2126 ::: bot acc: 0.0897
top acc: 0.0300 ::: bot acc: 0.0889
current epoch: 7
train loss is 0.121895
average val loss: 0.102803, accuracy: 0.1000
average test loss: 0.103017, accuracy: 0.1027
case acc: 0.14395623
case acc: 0.09691523
case acc: 0.13320954
case acc: 0.05876414
case acc: 0.12309567
case acc: 0.060148887
top acc: 0.0909 ::: bot acc: 0.1979
top acc: 0.0523 ::: bot acc: 0.1382
top acc: 0.0682 ::: bot acc: 0.2002
top acc: 0.1049 ::: bot acc: 0.0306
top acc: 0.1851 ::: bot acc: 0.0622
top acc: 0.0225 ::: bot acc: 0.1024
current epoch: 8
train loss is 0.124813
average val loss: 0.097917, accuracy: 0.0954
average test loss: 0.097782, accuracy: 0.0973
case acc: 0.13745539
case acc: 0.09359118
case acc: 0.12683871
case acc: 0.05629826
case acc: 0.11085723
case acc: 0.058715843
top acc: 0.0844 ::: bot acc: 0.1914
top acc: 0.0490 ::: bot acc: 0.1349
top acc: 0.0620 ::: bot acc: 0.1938
top acc: 0.0990 ::: bot acc: 0.0349
top acc: 0.1729 ::: bot acc: 0.0499
top acc: 0.0235 ::: bot acc: 0.0999
current epoch: 9
train loss is 0.115658
average val loss: 0.092852, accuracy: 0.0905
average test loss: 0.092215, accuracy: 0.0916
case acc: 0.12975092
case acc: 0.08892759
case acc: 0.11891208
case acc: 0.054638766
case acc: 0.10020878
case acc: 0.05692094
top acc: 0.0767 ::: bot acc: 0.1837
top acc: 0.0446 ::: bot acc: 0.1301
top acc: 0.0542 ::: bot acc: 0.1857
top acc: 0.0945 ::: bot acc: 0.0390
top acc: 0.1622 ::: bot acc: 0.0394
top acc: 0.0252 ::: bot acc: 0.0964
current epoch: 10
train loss is 0.111446
average val loss: 0.089483, accuracy: 0.0878
average test loss: 0.088701, accuracy: 0.0877
case acc: 0.12659602
case acc: 0.088982545
case acc: 0.115789264
case acc: 0.051752422
case acc: 0.08558524
case acc: 0.05735081
top acc: 0.0736 ::: bot acc: 0.1805
top acc: 0.0447 ::: bot acc: 0.1299
top acc: 0.0513 ::: bot acc: 0.1824
top acc: 0.0858 ::: bot acc: 0.0477
top acc: 0.1472 ::: bot acc: 0.0255
top acc: 0.0247 ::: bot acc: 0.0973
current epoch: 11
train loss is 0.105808
average val loss: 0.085576, accuracy: 0.0843
average test loss: 0.084493, accuracy: 0.0834
case acc: 0.121054254
case acc: 0.08673477
case acc: 0.110665016
case acc: 0.05023228
case acc: 0.074789934
case acc: 0.056643084
top acc: 0.0682 ::: bot acc: 0.1749
top acc: 0.0426 ::: bot acc: 0.1276
top acc: 0.0468 ::: bot acc: 0.1769
top acc: 0.0795 ::: bot acc: 0.0539
top acc: 0.1350 ::: bot acc: 0.0178
top acc: 0.0254 ::: bot acc: 0.0960
current epoch: 12
train loss is 0.101067
average val loss: 0.081818, accuracy: 0.0809
average test loss: 0.080492, accuracy: 0.0795
case acc: 0.11530316
case acc: 0.08435529
case acc: 0.105523475
case acc: 0.04933986
case acc: 0.06641049
case acc: 0.05582467
top acc: 0.0624 ::: bot acc: 0.1692
top acc: 0.0403 ::: bot acc: 0.1251
top acc: 0.0425 ::: bot acc: 0.1713
top acc: 0.0738 ::: bot acc: 0.0596
top acc: 0.1231 ::: bot acc: 0.0164
top acc: 0.0264 ::: bot acc: 0.0943
current epoch: 13
train loss is 0.095017
average val loss: 0.079215, accuracy: 0.0789
average test loss: 0.077985, accuracy: 0.0771
case acc: 0.11198329
case acc: 0.08453795
case acc: 0.10302208
case acc: 0.048673455
case acc: 0.058105003
case acc: 0.056057256
top acc: 0.0592 ::: bot acc: 0.1658
top acc: 0.0405 ::: bot acc: 0.1252
top acc: 0.0403 ::: bot acc: 0.1686
top acc: 0.0659 ::: bot acc: 0.0673
top acc: 0.1091 ::: bot acc: 0.0197
top acc: 0.0261 ::: bot acc: 0.0948
current epoch: 14
train loss is 0.091679
average val loss: 0.076582, accuracy: 0.0769
average test loss: 0.075408, accuracy: 0.0746
case acc: 0.10757006
case acc: 0.08367985
case acc: 0.099851966
case acc: 0.048765752
case acc: 0.05206488
case acc: 0.055703178
top acc: 0.0549 ::: bot acc: 0.1614
top acc: 0.0397 ::: bot acc: 0.1243
top acc: 0.0377 ::: bot acc: 0.1650
top acc: 0.0595 ::: bot acc: 0.0738
top acc: 0.0964 ::: bot acc: 0.0270
top acc: 0.0265 ::: bot acc: 0.0941
current epoch: 15
train loss is 0.087945
average val loss: 0.072706, accuracy: 0.0729
average test loss: 0.071019, accuracy: 0.0705
case acc: 0.098997995
case acc: 0.07870624
case acc: 0.093447044
case acc: 0.048911802
case acc: 0.049327046
case acc: 0.053522002
top acc: 0.0466 ::: bot acc: 0.1527
top acc: 0.0350 ::: bot acc: 0.1191
top acc: 0.0339 ::: bot acc: 0.1573
top acc: 0.0577 ::: bot acc: 0.0757
top acc: 0.0882 ::: bot acc: 0.0349
top acc: 0.0297 ::: bot acc: 0.0892
current epoch: 16
train loss is 0.082613
average val loss: 0.071151, accuracy: 0.0719
average test loss: 0.069439, accuracy: 0.0690
case acc: 0.09515976
case acc: 0.0783092
case acc: 0.0912298
case acc: 0.0497122
case acc: 0.046374314
case acc: 0.053448245
top acc: 0.0430 ::: bot acc: 0.1487
top acc: 0.0347 ::: bot acc: 0.1187
top acc: 0.0326 ::: bot acc: 0.1545
top acc: 0.0515 ::: bot acc: 0.0819
top acc: 0.0756 ::: bot acc: 0.0474
top acc: 0.0298 ::: bot acc: 0.0891
current epoch: 17
train loss is 0.079006
average val loss: 0.068620, accuracy: 0.0695
average test loss: 0.066432, accuracy: 0.0662
case acc: 0.08819828
case acc: 0.07474976
case acc: 0.08648317
case acc: 0.050180823
case acc: 0.045474995
case acc: 0.052069813
top acc: 0.0365 ::: bot acc: 0.1415
top acc: 0.0314 ::: bot acc: 0.1150
top acc: 0.0301 ::: bot acc: 0.1486
top acc: 0.0490 ::: bot acc: 0.0844
top acc: 0.0667 ::: bot acc: 0.0563
top acc: 0.0324 ::: bot acc: 0.0857
current epoch: 18
train loss is 0.074498
average val loss: 0.067473, accuracy: 0.0686
average test loss: 0.065062, accuracy: 0.0650
case acc: 0.083636984
case acc: 0.073398106
case acc: 0.08390581
case acc: 0.051247828
case acc: 0.045942467
case acc: 0.05171451
top acc: 0.0323 ::: bot acc: 0.1368
top acc: 0.0303 ::: bot acc: 0.1135
top acc: 0.0288 ::: bot acc: 0.1454
top acc: 0.0446 ::: bot acc: 0.0889
top acc: 0.0559 ::: bot acc: 0.0670
top acc: 0.0330 ::: bot acc: 0.0848
current epoch: 19
train loss is 0.071736
average val loss: 0.066418, accuracy: 0.0676
average test loss: 0.063674, accuracy: 0.0637
case acc: 0.07884067
case acc: 0.071067296
case acc: 0.08126253
case acc: 0.052378096
case acc: 0.047661427
case acc: 0.051209573
top acc: 0.0282 ::: bot acc: 0.1317
top acc: 0.0284 ::: bot acc: 0.1109
top acc: 0.0277 ::: bot acc: 0.1420
top acc: 0.0411 ::: bot acc: 0.0924
top acc: 0.0461 ::: bot acc: 0.0769
top acc: 0.0341 ::: bot acc: 0.0836
current epoch: 20
train loss is 0.069238
average val loss: 0.063668, accuracy: 0.0646
average test loss: 0.060161, accuracy: 0.0603
case acc: 0.07076256
case acc: 0.065146394
case acc: 0.07561268
case acc: 0.052011684
case acc: 0.048843376
case acc: 0.049264543
top acc: 0.0227 ::: bot acc: 0.1223
top acc: 0.0246 ::: bot acc: 0.1039
top acc: 0.0257 ::: bot acc: 0.1344
top acc: 0.0422 ::: bot acc: 0.0913
top acc: 0.0410 ::: bot acc: 0.0819
top acc: 0.0390 ::: bot acc: 0.0782
current epoch: 21
train loss is 0.065164
average val loss: 0.062346, accuracy: 0.0632
average test loss: 0.058463, accuracy: 0.0585
case acc: 0.06556341
case acc: 0.061615027
case acc: 0.072190836
case acc: 0.052386425
case acc: 0.051001858
case acc: 0.048487604
top acc: 0.0197 ::: bot acc: 0.1160
top acc: 0.0225 ::: bot acc: 0.0997
top acc: 0.0246 ::: bot acc: 0.1298
top acc: 0.0410 ::: bot acc: 0.0925
top acc: 0.0339 ::: bot acc: 0.0891
top acc: 0.0414 ::: bot acc: 0.0759
current epoch: 22
train loss is 0.062239
average val loss: 0.060712, accuracy: 0.0614
average test loss: 0.056358, accuracy: 0.0565
case acc: 0.060033023
case acc: 0.05724359
case acc: 0.06855292
case acc: 0.052166842
case acc: 0.05325219
case acc: 0.047497988
top acc: 0.0178 ::: bot acc: 0.1086
top acc: 0.0204 ::: bot acc: 0.0941
top acc: 0.0244 ::: bot acc: 0.1244
top acc: 0.0415 ::: bot acc: 0.0918
top acc: 0.0303 ::: bot acc: 0.0943
top acc: 0.0448 ::: bot acc: 0.0724
current epoch: 23
train loss is 0.059795
average val loss: 0.059239, accuracy: 0.0599
average test loss: 0.054425, accuracy: 0.0545
case acc: 0.05511885
case acc: 0.053094044
case acc: 0.06531348
case acc: 0.051773988
case acc: 0.055402502
case acc: 0.04654635
top acc: 0.0174 ::: bot acc: 0.1014
top acc: 0.0192 ::: bot acc: 0.0886
top acc: 0.0249 ::: bot acc: 0.1193
top acc: 0.0426 ::: bot acc: 0.0907
top acc: 0.0282 ::: bot acc: 0.0985
top acc: 0.0482 ::: bot acc: 0.0690
current epoch: 24
train loss is 0.057598
average val loss: 0.057369, accuracy: 0.0580
average test loss: 0.051926, accuracy: 0.0520
case acc: 0.049948048
case acc: 0.048094563
case acc: 0.061544966
case acc: 0.050802745
case acc: 0.056205202
case acc: 0.04533395
top acc: 0.0188 ::: bot acc: 0.0930
top acc: 0.0187 ::: bot acc: 0.0813
top acc: 0.0265 ::: bot acc: 0.1128
top acc: 0.0460 ::: bot acc: 0.0873
top acc: 0.0275 ::: bot acc: 0.1001
top acc: 0.0531 ::: bot acc: 0.0640
current epoch: 25
train loss is 0.055556
average val loss: 0.056373, accuracy: 0.0571
average test loss: 0.050570, accuracy: 0.0506
case acc: 0.046893217
case acc: 0.044787265
case acc: 0.059484527
case acc: 0.05037333
case acc: 0.057487484
case acc: 0.044742644
top acc: 0.0213 ::: bot acc: 0.0871
top acc: 0.0189 ::: bot acc: 0.0762
top acc: 0.0283 ::: bot acc: 0.1088
top acc: 0.0480 ::: bot acc: 0.0853
top acc: 0.0264 ::: bot acc: 0.1025
top acc: 0.0557 ::: bot acc: 0.0613
current epoch: 26
train loss is 0.054836
average val loss: 0.055307, accuracy: 0.0561
average test loss: 0.049119, accuracy: 0.0491
case acc: 0.044192668
case acc: 0.04137979
case acc: 0.057372686
case acc: 0.049752798
case acc: 0.057848994
case acc: 0.044194747
top acc: 0.0253 ::: bot acc: 0.0810
top acc: 0.0200 ::: bot acc: 0.0706
top acc: 0.0306 ::: bot acc: 0.1045
top acc: 0.0510 ::: bot acc: 0.0823
top acc: 0.0261 ::: bot acc: 0.1031
top acc: 0.0586 ::: bot acc: 0.0583
current epoch: 27
train loss is 0.053977
average val loss: 0.053842, accuracy: 0.0545
average test loss: 0.047141, accuracy: 0.0470
case acc: 0.041471504
case acc: 0.037226673
case acc: 0.05462821
case acc: 0.048880037
case acc: 0.05645758
case acc: 0.043570064
top acc: 0.0322 ::: bot acc: 0.0734
top acc: 0.0229 ::: bot acc: 0.0630
top acc: 0.0349 ::: bot acc: 0.0982
top acc: 0.0564 ::: bot acc: 0.0768
top acc: 0.0270 ::: bot acc: 0.1004
top acc: 0.0635 ::: bot acc: 0.0534
current epoch: 28
train loss is 0.052666
average val loss: 0.053262, accuracy: 0.0539
average test loss: 0.046369, accuracy: 0.0462
case acc: 0.040352844
case acc: 0.035188027
case acc: 0.053582717
case acc: 0.048640743
case acc: 0.056095783
case acc: 0.04340298
top acc: 0.0358 ::: bot acc: 0.0696
top acc: 0.0254 ::: bot acc: 0.0586
top acc: 0.0374 ::: bot acc: 0.0953
top acc: 0.0593 ::: bot acc: 0.0739
top acc: 0.0272 ::: bot acc: 0.0997
top acc: 0.0651 ::: bot acc: 0.0518
current epoch: 29
train loss is 0.052358
average val loss: 0.052659, accuracy: 0.0532
average test loss: 0.045613, accuracy: 0.0454
case acc: 0.039670162
case acc: 0.033477318
case acc: 0.05273207
case acc: 0.048479147
case acc: 0.05508927
case acc: 0.04322351
top acc: 0.0392 ::: bot acc: 0.0662
top acc: 0.0291 ::: bot acc: 0.0542
top acc: 0.0403 ::: bot acc: 0.0924
top acc: 0.0628 ::: bot acc: 0.0703
top acc: 0.0281 ::: bot acc: 0.0977
top acc: 0.0669 ::: bot acc: 0.0499
current epoch: 30
train loss is 0.051716
average val loss: 0.052050, accuracy: 0.0523
average test loss: 0.044897, accuracy: 0.0448
case acc: 0.039179824
case acc: 0.03215692
case acc: 0.052005865
case acc: 0.04850616
case acc: 0.05354026
case acc: 0.043149903
top acc: 0.0424 ::: bot acc: 0.0629
top acc: 0.0335 ::: bot acc: 0.0495
top acc: 0.0435 ::: bot acc: 0.0892
top acc: 0.0671 ::: bot acc: 0.0659
top acc: 0.0297 ::: bot acc: 0.0945
top acc: 0.0690 ::: bot acc: 0.0477
current epoch: 31
train loss is 0.051345
average val loss: 0.051873, accuracy: 0.0520
average test loss: 0.044672, accuracy: 0.0446
case acc: 0.03910716
case acc: 0.031728994
case acc: 0.051875874
case acc: 0.048609484
case acc: 0.05294425
case acc: 0.04314184
top acc: 0.0429 ::: bot acc: 0.0623
top acc: 0.0354 ::: bot acc: 0.0476
top acc: 0.0440 ::: bot acc: 0.0886
top acc: 0.0692 ::: bot acc: 0.0638
top acc: 0.0304 ::: bot acc: 0.0933
top acc: 0.0688 ::: bot acc: 0.0479
current epoch: 32
train loss is 0.050888
average val loss: 0.051944, accuracy: 0.0519
average test loss: 0.044717, accuracy: 0.0446
case acc: 0.03923941
case acc: 0.03170143
case acc: 0.05216567
case acc: 0.048646875
case acc: 0.052968107
case acc: 0.043154784
top acc: 0.0416 ::: bot acc: 0.0635
top acc: 0.0355 ::: bot acc: 0.0475
top acc: 0.0426 ::: bot acc: 0.0900
top acc: 0.0697 ::: bot acc: 0.0633
top acc: 0.0303 ::: bot acc: 0.0933
top acc: 0.0670 ::: bot acc: 0.0497
current epoch: 33
train loss is 0.050536
average val loss: 0.052090, accuracy: 0.0520
average test loss: 0.044860, accuracy: 0.0448
case acc: 0.039441448
case acc: 0.03175148
case acc: 0.052614626
case acc: 0.048650354
case acc: 0.053173795
case acc: 0.043309007
top acc: 0.0400 ::: bot acc: 0.0652
top acc: 0.0351 ::: bot acc: 0.0479
top acc: 0.0408 ::: bot acc: 0.0918
top acc: 0.0697 ::: bot acc: 0.0633
top acc: 0.0301 ::: bot acc: 0.0937
top acc: 0.0649 ::: bot acc: 0.0518
current epoch: 34
train loss is 0.050342
average val loss: 0.052506, accuracy: 0.0523
average test loss: 0.045355, accuracy: 0.0454
case acc: 0.039912418
case acc: 0.03215478
case acc: 0.053598117
case acc: 0.048546746
case acc: 0.054243486
case acc: 0.04364782
top acc: 0.0371 ::: bot acc: 0.0681
top acc: 0.0332 ::: bot acc: 0.0498
top acc: 0.0375 ::: bot acc: 0.0950
top acc: 0.0680 ::: bot acc: 0.0650
top acc: 0.0290 ::: bot acc: 0.0959
top acc: 0.0614 ::: bot acc: 0.0552
current epoch: 35
train loss is 0.050149
average val loss: 0.052929, accuracy: 0.0527
average test loss: 0.045891, accuracy: 0.0459
case acc: 0.040449794
case acc: 0.03251672
case acc: 0.054661434
case acc: 0.04845306
case acc: 0.05539442
case acc: 0.044051766
top acc: 0.0349 ::: bot acc: 0.0703
top acc: 0.0317 ::: bot acc: 0.0513
top acc: 0.0349 ::: bot acc: 0.0979
top acc: 0.0663 ::: bot acc: 0.0667
top acc: 0.0279 ::: bot acc: 0.0982
top acc: 0.0585 ::: bot acc: 0.0581
current epoch: 36
train loss is 0.050131
average val loss: 0.053119, accuracy: 0.0529
average test loss: 0.046136, accuracy: 0.0462
case acc: 0.040604725
case acc: 0.03254241
case acc: 0.05529741
case acc: 0.048419096
case acc: 0.05607863
case acc: 0.04427415
top acc: 0.0344 ::: bot acc: 0.0707
top acc: 0.0317 ::: bot acc: 0.0513
top acc: 0.0338 ::: bot acc: 0.0994
top acc: 0.0656 ::: bot acc: 0.0674
top acc: 0.0273 ::: bot acc: 0.0995
top acc: 0.0571 ::: bot acc: 0.0596
current epoch: 37
train loss is 0.050080
average val loss: 0.053224, accuracy: 0.0531
average test loss: 0.046274, accuracy: 0.0464
case acc: 0.04053407
case acc: 0.032443922
case acc: 0.055652767
case acc: 0.04840113
case acc: 0.056744523
case acc: 0.044441663
top acc: 0.0348 ::: bot acc: 0.0704
top acc: 0.0322 ::: bot acc: 0.0509
top acc: 0.0332 ::: bot acc: 0.1003
top acc: 0.0650 ::: bot acc: 0.0680
top acc: 0.0268 ::: bot acc: 0.1008
top acc: 0.0562 ::: bot acc: 0.0605
current epoch: 38
train loss is 0.050076
average val loss: 0.052631, accuracy: 0.0526
average test loss: 0.045520, accuracy: 0.0457
case acc: 0.039749116
case acc: 0.03161576
case acc: 0.054441843
case acc: 0.048526045
case acc: 0.055642918
case acc: 0.04402239
top acc: 0.0388 ::: bot acc: 0.0665
top acc: 0.0363 ::: bot acc: 0.0469
top acc: 0.0352 ::: bot acc: 0.0975
top acc: 0.0679 ::: bot acc: 0.0651
top acc: 0.0277 ::: bot acc: 0.0987
top acc: 0.0588 ::: bot acc: 0.0579
current epoch: 39
train loss is 0.050003
average val loss: 0.052132, accuracy: 0.0521
average test loss: 0.044925, accuracy: 0.0451
case acc: 0.03923986
case acc: 0.031022485
case acc: 0.053414173
case acc: 0.048741557
case acc: 0.054492023
case acc: 0.043675873
top acc: 0.0423 ::: bot acc: 0.0630
top acc: 0.0401 ::: bot acc: 0.0430
top acc: 0.0378 ::: bot acc: 0.0946
top acc: 0.0708 ::: bot acc: 0.0622
top acc: 0.0287 ::: bot acc: 0.0965
top acc: 0.0613 ::: bot acc: 0.0554
current epoch: 40
train loss is 0.049992
average val loss: 0.051667, accuracy: 0.0517
average test loss: 0.044432, accuracy: 0.0446
case acc: 0.03886633
case acc: 0.030709809
case acc: 0.05247925
case acc: 0.049063418
case acc: 0.0530315
case acc: 0.04341203
top acc: 0.0457 ::: bot acc: 0.0596
top acc: 0.0441 ::: bot acc: 0.0391
top acc: 0.0409 ::: bot acc: 0.0915
top acc: 0.0743 ::: bot acc: 0.0587
top acc: 0.0301 ::: bot acc: 0.0936
top acc: 0.0638 ::: bot acc: 0.0529
current epoch: 41
train loss is 0.049983
average val loss: 0.051303, accuracy: 0.0513
average test loss: 0.044120, accuracy: 0.0443
case acc: 0.038729906
case acc: 0.030796051
case acc: 0.051810287
case acc: 0.049530014
case acc: 0.0516757
case acc: 0.04323689
top acc: 0.0484 ::: bot acc: 0.0569
top acc: 0.0477 ::: bot acc: 0.0356
top acc: 0.0438 ::: bot acc: 0.0886
top acc: 0.0778 ::: bot acc: 0.0551
top acc: 0.0320 ::: bot acc: 0.0906
top acc: 0.0660 ::: bot acc: 0.0508
current epoch: 42
train loss is 0.050022
average val loss: 0.050926, accuracy: 0.0508
average test loss: 0.043887, accuracy: 0.0441
case acc: 0.038757846
case acc: 0.031139355
case acc: 0.051042795
case acc: 0.050607238
case acc: 0.05005024
case acc: 0.043119065
top acc: 0.0516 ::: bot acc: 0.0537
top acc: 0.0520 ::: bot acc: 0.0312
top acc: 0.0478 ::: bot acc: 0.0847
top acc: 0.0826 ::: bot acc: 0.0503
top acc: 0.0360 ::: bot acc: 0.0861
top acc: 0.0689 ::: bot acc: 0.0478
current epoch: 43
train loss is 0.050126
average val loss: 0.050747, accuracy: 0.0504
average test loss: 0.043871, accuracy: 0.0442
case acc: 0.038835734
case acc: 0.03159461
case acc: 0.050658733
case acc: 0.051780686
case acc: 0.04897396
case acc: 0.043102153
top acc: 0.0530 ::: bot acc: 0.0522
top acc: 0.0549 ::: bot acc: 0.0283
top acc: 0.0505 ::: bot acc: 0.0819
top acc: 0.0866 ::: bot acc: 0.0462
top acc: 0.0398 ::: bot acc: 0.0824
top acc: 0.0705 ::: bot acc: 0.0462
current epoch: 44
train loss is 0.049980
average val loss: 0.050680, accuracy: 0.0501
average test loss: 0.043917, accuracy: 0.0442
case acc: 0.038827453
case acc: 0.031899255
case acc: 0.05048876
case acc: 0.05280446
case acc: 0.04822784
case acc: 0.043094415
top acc: 0.0529 ::: bot acc: 0.0523
top acc: 0.0564 ::: bot acc: 0.0268
top acc: 0.0520 ::: bot acc: 0.0804
top acc: 0.0898 ::: bot acc: 0.0431
top acc: 0.0427 ::: bot acc: 0.0795
top acc: 0.0710 ::: bot acc: 0.0457
current epoch: 45
train loss is 0.049700
average val loss: 0.050716, accuracy: 0.0498
average test loss: 0.043905, accuracy: 0.0442
case acc: 0.038690183
case acc: 0.031702895
case acc: 0.050583117
case acc: 0.053157117
case acc: 0.047948163
case acc: 0.0430958
top acc: 0.0504 ::: bot acc: 0.0548
top acc: 0.0555 ::: bot acc: 0.0277
top acc: 0.0511 ::: bot acc: 0.0814
top acc: 0.0908 ::: bot acc: 0.0420
top acc: 0.0438 ::: bot acc: 0.0783
top acc: 0.0693 ::: bot acc: 0.0473
current epoch: 46
train loss is 0.049345
average val loss: 0.050924, accuracy: 0.0497
average test loss: 0.043923, accuracy: 0.0442
case acc: 0.038874477
case acc: 0.03111783
case acc: 0.05115333
case acc: 0.05262238
case acc: 0.048206884
case acc: 0.04325462
top acc: 0.0453 ::: bot acc: 0.0599
top acc: 0.0519 ::: bot acc: 0.0313
top acc: 0.0471 ::: bot acc: 0.0853
top acc: 0.0892 ::: bot acc: 0.0436
top acc: 0.0427 ::: bot acc: 0.0794
top acc: 0.0654 ::: bot acc: 0.0513
current epoch: 47
train loss is 0.049194
average val loss: 0.051564, accuracy: 0.0501
average test loss: 0.044369, accuracy: 0.0446
case acc: 0.039919727
case acc: 0.030729696
case acc: 0.05281537
case acc: 0.051118705
case acc: 0.04925704
case acc: 0.04400469
top acc: 0.0377 ::: bot acc: 0.0676
top acc: 0.0453 ::: bot acc: 0.0379
top acc: 0.0397 ::: bot acc: 0.0928
top acc: 0.0845 ::: bot acc: 0.0483
top acc: 0.0387 ::: bot acc: 0.0835
top acc: 0.0589 ::: bot acc: 0.0578
current epoch: 48
train loss is 0.049363
average val loss: 0.052339, accuracy: 0.0508
average test loss: 0.045232, accuracy: 0.0455
case acc: 0.04146937
case acc: 0.03112333
case acc: 0.055426236
case acc: 0.05000492
case acc: 0.05029279
case acc: 0.04494932
top acc: 0.0316 ::: bot acc: 0.0737
top acc: 0.0397 ::: bot acc: 0.0436
top acc: 0.0335 ::: bot acc: 0.0998
top acc: 0.0802 ::: bot acc: 0.0527
top acc: 0.0353 ::: bot acc: 0.0869
top acc: 0.0539 ::: bot acc: 0.0628
current epoch: 49
train loss is 0.050064
average val loss: 0.053669, accuracy: 0.0523
average test loss: 0.046886, accuracy: 0.0473
case acc: 0.043937474
case acc: 0.032401856
case acc: 0.05968289
case acc: 0.049009938
case acc: 0.05231153
case acc: 0.046298184
top acc: 0.0255 ::: bot acc: 0.0804
top acc: 0.0328 ::: bot acc: 0.0505
top acc: 0.0284 ::: bot acc: 0.1088
top acc: 0.0739 ::: bot acc: 0.0589
top acc: 0.0310 ::: bot acc: 0.0921
top acc: 0.0482 ::: bot acc: 0.0686
current epoch: 50
train loss is 0.051000
average val loss: 0.055595, accuracy: 0.0545
average test loss: 0.049454, accuracy: 0.0499
case acc: 0.046984985
case acc: 0.034882452
case acc: 0.06539858
case acc: 0.048457943
case acc: 0.055668704
case acc: 0.047905706
top acc: 0.0208 ::: bot acc: 0.0874
top acc: 0.0255 ::: bot acc: 0.0582
top acc: 0.0252 ::: bot acc: 0.1189
top acc: 0.0663 ::: bot acc: 0.0667
top acc: 0.0277 ::: bot acc: 0.0988
top acc: 0.0420 ::: bot acc: 0.0746
LME_Co_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5358 5358 5358
1.7082474 -0.6288155 0.2588177 -0.21218425
Validation: 600 600 600
Testing: 750 750 750
pre-processing time: 0.00026702880859375
the split date is 2012-01-01
net initializing with time: 0.0030994415283203125
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.153744
average val loss: 0.179977, accuracy: 0.1788
average test loss: 0.140629, accuracy: 0.1452
case acc: 0.18066226
case acc: 0.03964771
case acc: 0.08769418
case acc: 0.08959136
case acc: 0.05271768
case acc: 0.4209649
top acc: 0.1288 ::: bot acc: 0.2357
top acc: 0.0301 ::: bot acc: 0.0761
top acc: 0.0344 ::: bot acc: 0.1543
top acc: 0.0365 ::: bot acc: 0.1472
top acc: 0.0963 ::: bot acc: 0.0205
top acc: 0.3610 ::: bot acc: 0.4816
current epoch: 2
train loss is 0.185607
average val loss: 0.271719, accuracy: 0.2717
average test loss: 0.217772, accuracy: 0.2176
case acc: 0.2701411
case acc: 0.12455615
case acc: 0.16531023
case acc: 0.1825238
case acc: 0.06799872
case acc: 0.4951836
top acc: 0.2181 ::: bot acc: 0.3258
top acc: 0.0743 ::: bot acc: 0.1822
top acc: 0.0706 ::: bot acc: 0.2526
top acc: 0.1142 ::: bot acc: 0.2478
top acc: 0.0249 ::: bot acc: 0.1140
top acc: 0.4345 ::: bot acc: 0.5553
current epoch: 3
train loss is 0.143672
average val loss: 0.129589, accuracy: 0.1271
average test loss: 0.104856, accuracy: 0.1125
case acc: 0.10675127
case acc: 0.04513443
case acc: 0.06833184
case acc: 0.05748449
case acc: 0.08023513
case acc: 0.31721997
top acc: 0.0547 ::: bot acc: 0.1625
top acc: 0.0704 ::: bot acc: 0.0383
top acc: 0.0817 ::: bot acc: 0.0998
top acc: 0.0375 ::: bot acc: 0.0991
top acc: 0.1317 ::: bot acc: 0.0320
top acc: 0.2564 ::: bot acc: 0.3771
current epoch: 4
train loss is 0.122244
average val loss: 0.138103, accuracy: 0.1367
average test loss: 0.104650, accuracy: 0.1106
case acc: 0.115791224
case acc: 0.039551456
case acc: 0.072358906
case acc: 0.069183476
case acc: 0.057440937
case acc: 0.30925235
top acc: 0.0639 ::: bot acc: 0.1714
top acc: 0.0445 ::: bot acc: 0.0644
top acc: 0.0639 ::: bot acc: 0.1173
top acc: 0.0321 ::: bot acc: 0.1192
top acc: 0.1037 ::: bot acc: 0.0200
top acc: 0.2485 ::: bot acc: 0.3692
current epoch: 5
train loss is 0.118682
average val loss: 0.144408, accuracy: 0.1435
average test loss: 0.104945, accuracy: 0.1093
case acc: 0.11917527
case acc: 0.04215994
case acc: 0.07631789
case acc: 0.07849819
case acc: 0.04409097
case acc: 0.29557484
top acc: 0.0673 ::: bot acc: 0.1747
top acc: 0.0246 ::: bot acc: 0.0846
top acc: 0.0526 ::: bot acc: 0.1290
top acc: 0.0327 ::: bot acc: 0.1327
top acc: 0.0818 ::: bot acc: 0.0242
top acc: 0.2348 ::: bot acc: 0.3556
current epoch: 6
train loss is 0.103909
average val loss: 0.108264, accuracy: 0.1065
average test loss: 0.082468, accuracy: 0.0881
case acc: 0.07220984
case acc: 0.04144845
case acc: 0.06635241
case acc: 0.055093847
case acc: 0.06337827
case acc: 0.22989511
top acc: 0.0244 ::: bot acc: 0.1257
top acc: 0.0564 ::: bot acc: 0.0528
top acc: 0.0924 ::: bot acc: 0.0886
top acc: 0.0398 ::: bot acc: 0.0944
top acc: 0.1116 ::: bot acc: 0.0219
top acc: 0.1691 ::: bot acc: 0.2900
current epoch: 7
train loss is 0.093712
average val loss: 0.092481, accuracy: 0.0900
average test loss: 0.073291, accuracy: 0.0784
case acc: 0.054681946
case acc: 0.044010114
case acc: 0.06617387
case acc: 0.04875519
case acc: 0.069104604
case acc: 0.18747255
top acc: 0.0198 ::: bot acc: 0.1019
top acc: 0.0660 ::: bot acc: 0.0433
top acc: 0.1096 ::: bot acc: 0.0713
top acc: 0.0539 ::: bot acc: 0.0778
top acc: 0.1189 ::: bot acc: 0.0246
top acc: 0.1267 ::: bot acc: 0.2475
current epoch: 8
train loss is 0.086529
average val loss: 0.079628, accuracy: 0.0762
average test loss: 0.066890, accuracy: 0.0711
case acc: 0.043338295
case acc: 0.046663437
case acc: 0.0683242
case acc: 0.0464682
case acc: 0.074173406
case acc: 0.1476298
top acc: 0.0251 ::: bot acc: 0.0824
top acc: 0.0742 ::: bot acc: 0.0351
top acc: 0.1247 ::: bot acc: 0.0559
top acc: 0.0698 ::: bot acc: 0.0618
top acc: 0.1246 ::: bot acc: 0.0279
top acc: 0.0870 ::: bot acc: 0.2076
current epoch: 9
train loss is 0.080632
average val loss: 0.067244, accuracy: 0.0624
average test loss: 0.064288, accuracy: 0.0676
case acc: 0.03952267
case acc: 0.051912837
case acc: 0.07429979
case acc: 0.050735433
case acc: 0.08348914
case acc: 0.1058949
top acc: 0.0460 ::: bot acc: 0.0613
top acc: 0.0874 ::: bot acc: 0.0241
top acc: 0.1437 ::: bot acc: 0.0377
top acc: 0.0909 ::: bot acc: 0.0408
top acc: 0.1352 ::: bot acc: 0.0346
top acc: 0.0477 ::: bot acc: 0.1646
current epoch: 10
train loss is 0.076642
average val loss: 0.060416, accuracy: 0.0553
average test loss: 0.062462, accuracy: 0.0652
case acc: 0.039652992
case acc: 0.053920843
case acc: 0.07868233
case acc: 0.05601093
case acc: 0.08499054
case acc: 0.07818004
top acc: 0.0545 ::: bot acc: 0.0527
top acc: 0.0917 ::: bot acc: 0.0216
top acc: 0.1527 ::: bot acc: 0.0326
top acc: 0.1033 ::: bot acc: 0.0311
top acc: 0.1368 ::: bot acc: 0.0358
top acc: 0.0284 ::: bot acc: 0.1328
current epoch: 11
train loss is 0.074199
average val loss: 0.056446, accuracy: 0.0521
average test loss: 0.059853, accuracy: 0.0621
case acc: 0.03959756
case acc: 0.05276557
case acc: 0.07948365
case acc: 0.05909374
case acc: 0.08069438
case acc: 0.0609241
top acc: 0.0538 ::: bot acc: 0.0533
top acc: 0.0891 ::: bot acc: 0.0231
top acc: 0.1541 ::: bot acc: 0.0320
top acc: 0.1091 ::: bot acc: 0.0286
top acc: 0.1320 ::: bot acc: 0.0326
top acc: 0.0229 ::: bot acc: 0.1098
current epoch: 12
train loss is 0.071487
average val loss: 0.055039, accuracy: 0.0518
average test loss: 0.056091, accuracy: 0.0580
case acc: 0.03953873
case acc: 0.049021073
case acc: 0.07676008
case acc: 0.05843903
case acc: 0.071249925
case acc: 0.05269648
top acc: 0.0462 ::: bot acc: 0.0611
top acc: 0.0804 ::: bot acc: 0.0295
top acc: 0.1488 ::: bot acc: 0.0345
top acc: 0.1081 ::: bot acc: 0.0286
top acc: 0.1213 ::: bot acc: 0.0261
top acc: 0.0283 ::: bot acc: 0.0949
current epoch: 13
train loss is 0.066523
average val loss: 0.055262, accuracy: 0.0532
average test loss: 0.052812, accuracy: 0.0542
case acc: 0.040246397
case acc: 0.04549949
case acc: 0.07352791
case acc: 0.056726646
case acc: 0.061778612
case acc: 0.047688592
top acc: 0.0380 ::: bot acc: 0.0694
top acc: 0.0701 ::: bot acc: 0.0395
top acc: 0.1419 ::: bot acc: 0.0388
top acc: 0.1047 ::: bot acc: 0.0301
top acc: 0.1093 ::: bot acc: 0.0216
top acc: 0.0377 ::: bot acc: 0.0829
current epoch: 14
train loss is 0.063009
average val loss: 0.056762, accuracy: 0.0558
average test loss: 0.050434, accuracy: 0.0511
case acc: 0.041369725
case acc: 0.04232562
case acc: 0.07083384
case acc: 0.054267373
case acc: 0.053139303
case acc: 0.04495809
top acc: 0.0310 ::: bot acc: 0.0765
top acc: 0.0591 ::: bot acc: 0.0506
top acc: 0.1343 ::: bot acc: 0.0461
top acc: 0.0994 ::: bot acc: 0.0333
top acc: 0.0972 ::: bot acc: 0.0199
top acc: 0.0473 ::: bot acc: 0.0732
current epoch: 15
train loss is 0.058729
average val loss: 0.060798, accuracy: 0.0607
average test loss: 0.048581, accuracy: 0.0486
case acc: 0.04422086
case acc: 0.040016707
case acc: 0.0680416
case acc: 0.050461486
case acc: 0.04475115
case acc: 0.044387955
top acc: 0.0238 ::: bot acc: 0.0845
top acc: 0.0453 ::: bot acc: 0.0644
top acc: 0.1238 ::: bot acc: 0.0565
top acc: 0.0900 ::: bot acc: 0.0409
top acc: 0.0830 ::: bot acc: 0.0236
top acc: 0.0515 ::: bot acc: 0.0691
current epoch: 16
train loss is 0.055801
average val loss: 0.064493, accuracy: 0.0649
average test loss: 0.048020, accuracy: 0.0474
case acc: 0.04608414
case acc: 0.04000788
case acc: 0.066672966
case acc: 0.04765123
case acc: 0.03978221
case acc: 0.044066075
top acc: 0.0224 ::: bot acc: 0.0880
top acc: 0.0341 ::: bot acc: 0.0756
top acc: 0.1156 ::: bot acc: 0.0646
top acc: 0.0815 ::: bot acc: 0.0491
top acc: 0.0719 ::: bot acc: 0.0311
top acc: 0.0548 ::: bot acc: 0.0658
current epoch: 17
train loss is 0.053027
average val loss: 0.066454, accuracy: 0.0671
average test loss: 0.048076, accuracy: 0.0468
case acc: 0.045187216
case acc: 0.04139387
case acc: 0.06608988
case acc: 0.046496283
case acc: 0.037868638
case acc: 0.04380855
top acc: 0.0230 ::: bot acc: 0.0864
top acc: 0.0273 ::: bot acc: 0.0825
top acc: 0.1112 ::: bot acc: 0.0691
top acc: 0.0757 ::: bot acc: 0.0549
top acc: 0.0658 ::: bot acc: 0.0373
top acc: 0.0578 ::: bot acc: 0.0628
current epoch: 18
train loss is 0.051499
average val loss: 0.066495, accuracy: 0.0672
average test loss: 0.048095, accuracy: 0.0464
case acc: 0.042635564
case acc: 0.042171318
case acc: 0.06596306
case acc: 0.046186816
case acc: 0.037521448
case acc: 0.043690685
top acc: 0.0268 ::: bot acc: 0.0807
top acc: 0.0248 ::: bot acc: 0.0851
top acc: 0.1102 ::: bot acc: 0.0700
top acc: 0.0726 ::: bot acc: 0.0579
top acc: 0.0642 ::: bot acc: 0.0388
top acc: 0.0602 ::: bot acc: 0.0605
current epoch: 19
train loss is 0.050748
average val loss: 0.064755, accuracy: 0.0653
average test loss: 0.048046, accuracy: 0.0461
case acc: 0.040527456
case acc: 0.04169317
case acc: 0.06621343
case acc: 0.046144743
case acc: 0.038111035
case acc: 0.043618403
top acc: 0.0351 ::: bot acc: 0.0723
top acc: 0.0263 ::: bot acc: 0.0836
top acc: 0.1124 ::: bot acc: 0.0679
top acc: 0.0724 ::: bot acc: 0.0581
top acc: 0.0667 ::: bot acc: 0.0363
top acc: 0.0621 ::: bot acc: 0.0584
current epoch: 20
train loss is 0.050413
average val loss: 0.060900, accuracy: 0.0612
average test loss: 0.048406, accuracy: 0.0463
case acc: 0.039552145
case acc: 0.040274985
case acc: 0.06705606
case acc: 0.046536583
case acc: 0.040561117
case acc: 0.043623485
top acc: 0.0466 ::: bot acc: 0.0609
top acc: 0.0326 ::: bot acc: 0.0774
top acc: 0.1186 ::: bot acc: 0.0616
top acc: 0.0763 ::: bot acc: 0.0540
top acc: 0.0739 ::: bot acc: 0.0295
top acc: 0.0660 ::: bot acc: 0.0545
current epoch: 21
train loss is 0.050370
average val loss: 0.056724, accuracy: 0.0568
average test loss: 0.049669, accuracy: 0.0476
case acc: 0.04019141
case acc: 0.039836388
case acc: 0.068762906
case acc: 0.04801989
case acc: 0.045023337
case acc: 0.043920405
top acc: 0.0582 ::: bot acc: 0.0494
top acc: 0.0415 ::: bot acc: 0.0687
top acc: 0.1268 ::: bot acc: 0.0534
top acc: 0.0829 ::: bot acc: 0.0473
top acc: 0.0835 ::: bot acc: 0.0234
top acc: 0.0698 ::: bot acc: 0.0507
current epoch: 22
train loss is 0.050354
average val loss: 0.052657, accuracy: 0.0524
average test loss: 0.052214, accuracy: 0.0505
case acc: 0.04273761
case acc: 0.041115984
case acc: 0.07169106
case acc: 0.051280223
case acc: 0.0517417
case acc: 0.044657294
top acc: 0.0691 ::: bot acc: 0.0384
top acc: 0.0525 ::: bot acc: 0.0576
top acc: 0.1365 ::: bot acc: 0.0436
top acc: 0.0921 ::: bot acc: 0.0385
top acc: 0.0951 ::: bot acc: 0.0202
top acc: 0.0742 ::: bot acc: 0.0462
current epoch: 23
train loss is 0.050402
average val loss: 0.049297, accuracy: 0.0485
average test loss: 0.056384, accuracy: 0.0551
case acc: 0.04609336
case acc: 0.044441275
case acc: 0.076541536
case acc: 0.056633502
case acc: 0.061283298
case acc: 0.045880623
top acc: 0.0795 ::: bot acc: 0.0282
top acc: 0.0659 ::: bot acc: 0.0443
top acc: 0.1480 ::: bot acc: 0.0350
top acc: 0.1043 ::: bot acc: 0.0299
top acc: 0.1087 ::: bot acc: 0.0214
top acc: 0.0798 ::: bot acc: 0.0407
current epoch: 24
train loss is 0.050149
average val loss: 0.047593, accuracy: 0.0465
average test loss: 0.061563, accuracy: 0.0607
case acc: 0.049113084
case acc: 0.048874717
case acc: 0.08232829
case acc: 0.06461819
case acc: 0.07181441
case acc: 0.04761659
top acc: 0.0870 ::: bot acc: 0.0220
top acc: 0.0792 ::: bot acc: 0.0313
top acc: 0.1589 ::: bot acc: 0.0302
top acc: 0.1173 ::: bot acc: 0.0277
top acc: 0.1220 ::: bot acc: 0.0264
top acc: 0.0851 ::: bot acc: 0.0355
current epoch: 25
train loss is 0.050182
average val loss: 0.047577, accuracy: 0.0462
average test loss: 0.069032, accuracy: 0.0686
case acc: 0.052937042
case acc: 0.056000665
case acc: 0.09021952
case acc: 0.076141804
case acc: 0.085261345
case acc: 0.05096289
top acc: 0.0944 ::: bot acc: 0.0184
top acc: 0.0947 ::: bot acc: 0.0212
top acc: 0.1718 ::: bot acc: 0.0279
top acc: 0.1336 ::: bot acc: 0.0293
top acc: 0.1373 ::: bot acc: 0.0359
top acc: 0.0930 ::: bot acc: 0.0294
current epoch: 26
train loss is 0.051263
average val loss: 0.049523, accuracy: 0.0478
average test loss: 0.077464, accuracy: 0.0773
case acc: 0.055947848
case acc: 0.06482909
case acc: 0.09881573
case acc: 0.08943147
case acc: 0.09907121
case acc: 0.055820003
top acc: 0.0993 ::: bot acc: 0.0175
top acc: 0.1095 ::: bot acc: 0.0177
top acc: 0.1839 ::: bot acc: 0.0292
top acc: 0.1507 ::: bot acc: 0.0349
top acc: 0.1518 ::: bot acc: 0.0484
top acc: 0.1023 ::: bot acc: 0.0251
current epoch: 27
train loss is 0.054017
average val loss: 0.050938, accuracy: 0.0491
average test loss: 0.081203, accuracy: 0.0812
case acc: 0.053399332
case acc: 0.06938332
case acc: 0.10209415
case acc: 0.09846847
case acc: 0.10572537
case acc: 0.058083985
top acc: 0.0952 ::: bot acc: 0.0182
top acc: 0.1165 ::: bot acc: 0.0173
top acc: 0.1884 ::: bot acc: 0.0299
top acc: 0.1614 ::: bot acc: 0.0406
top acc: 0.1584 ::: bot acc: 0.0551
top acc: 0.1064 ::: bot acc: 0.0237
current epoch: 28
train loss is 0.057660
average val loss: 0.049623, accuracy: 0.0483
average test loss: 0.075900, accuracy: 0.0764
case acc: 0.04554718
case acc: 0.06504143
case acc: 0.09606646
case acc: 0.097210824
case acc: 0.098824665
case acc: 0.05540964
top acc: 0.0779 ::: bot acc: 0.0296
top acc: 0.1099 ::: bot acc: 0.0177
top acc: 0.1801 ::: bot acc: 0.0287
top acc: 0.1599 ::: bot acc: 0.0397
top acc: 0.1515 ::: bot acc: 0.0482
top acc: 0.1015 ::: bot acc: 0.0254
current epoch: 29
train loss is 0.061036
average val loss: 0.049325, accuracy: 0.0494
average test loss: 0.060674, accuracy: 0.0618
case acc: 0.039603822
case acc: 0.05049647
case acc: 0.07908744
case acc: 0.08042273
case acc: 0.07400205
case acc: 0.046995852
top acc: 0.0440 ::: bot acc: 0.0634
top acc: 0.0830 ::: bot acc: 0.0284
top acc: 0.1530 ::: bot acc: 0.0323
top acc: 0.1393 ::: bot acc: 0.0309
top acc: 0.1245 ::: bot acc: 0.0279
top acc: 0.0833 ::: bot acc: 0.0371
current epoch: 30
train loss is 0.065923
average val loss: 0.068041, accuracy: 0.0688
average test loss: 0.049427, accuracy: 0.0508
case acc: 0.06143106
case acc: 0.04037293
case acc: 0.06535602
case acc: 0.052098386
case acc: 0.040765595
case acc: 0.044640925
top acc: 0.0205 ::: bot acc: 0.1118
top acc: 0.0327 ::: bot acc: 0.0776
top acc: 0.1051 ::: bot acc: 0.0747
top acc: 0.0944 ::: bot acc: 0.0368
top acc: 0.0743 ::: bot acc: 0.0293
top acc: 0.0488 ::: bot acc: 0.0715
current epoch: 31
train loss is 0.066949
average val loss: 0.092436, accuracy: 0.0932
average test loss: 0.057413, accuracy: 0.0583
case acc: 0.08146215
case acc: 0.06102659
case acc: 0.069431625
case acc: 0.04711457
case acc: 0.04022532
case acc: 0.05032872
top acc: 0.0314 ::: bot acc: 0.1362
top acc: 0.0190 ::: bot acc: 0.1162
top acc: 0.0716 ::: bot acc: 0.1082
top acc: 0.0580 ::: bot acc: 0.0725
top acc: 0.0362 ::: bot acc: 0.0671
top acc: 0.0318 ::: bot acc: 0.0895
current epoch: 32
train loss is 0.063172
average val loss: 0.095846, accuracy: 0.0970
average test loss: 0.059313, accuracy: 0.0594
case acc: 0.07363802
case acc: 0.06982217
case acc: 0.07070447
case acc: 0.05021058
case acc: 0.04434705
case acc: 0.047870915
top acc: 0.0254 ::: bot acc: 0.1275
top acc: 0.0247 ::: bot acc: 0.1265
top acc: 0.0665 ::: bot acc: 0.1133
top acc: 0.0463 ::: bot acc: 0.0841
top acc: 0.0287 ::: bot acc: 0.0768
top acc: 0.0372 ::: bot acc: 0.0832
current epoch: 33
train loss is 0.063639
average val loss: 0.085519, accuracy: 0.0870
average test loss: 0.054189, accuracy: 0.0532
case acc: 0.05457228
case acc: 0.062871076
case acc: 0.06795574
case acc: 0.049140036
case acc: 0.04057435
case acc: 0.04433265
top acc: 0.0198 ::: bot acc: 0.1018
top acc: 0.0200 ::: bot acc: 0.1184
top acc: 0.0780 ::: bot acc: 0.1018
top acc: 0.0498 ::: bot acc: 0.0807
top acc: 0.0352 ::: bot acc: 0.0681
top acc: 0.0513 ::: bot acc: 0.0691
current epoch: 34
train loss is 0.062955
average val loss: 0.060864, accuracy: 0.0619
average test loss: 0.049443, accuracy: 0.0466
case acc: 0.039468523
case acc: 0.04186759
case acc: 0.06628201
case acc: 0.046776734
case acc: 0.039307527
case acc: 0.046195805
top acc: 0.0532 ::: bot acc: 0.0540
top acc: 0.0260 ::: bot acc: 0.0842
top acc: 0.1137 ::: bot acc: 0.0661
top acc: 0.0776 ::: bot acc: 0.0528
top acc: 0.0704 ::: bot acc: 0.0328
top acc: 0.0809 ::: bot acc: 0.0395
current epoch: 35
train loss is 0.059438
average val loss: 0.048112, accuracy: 0.0485
average test loss: 0.064315, accuracy: 0.0629
case acc: 0.057564404
case acc: 0.045967847
case acc: 0.08127042
case acc: 0.06416162
case acc: 0.06752103
case acc: 0.06065752
top acc: 0.1019 ::: bot acc: 0.0171
top acc: 0.0709 ::: bot acc: 0.0394
top acc: 0.1570 ::: bot acc: 0.0308
top acc: 0.1166 ::: bot acc: 0.0277
top acc: 0.1168 ::: bot acc: 0.0238
top acc: 0.1107 ::: bot acc: 0.0227
current epoch: 36
train loss is 0.056351
average val loss: 0.049404, accuracy: 0.0486
average test loss: 0.077135, accuracy: 0.0765
case acc: 0.070602536
case acc: 0.057312302
case acc: 0.09499016
case acc: 0.08068826
case acc: 0.09203773
case acc: 0.06365904
top acc: 0.1195 ::: bot acc: 0.0207
top acc: 0.0972 ::: bot acc: 0.0202
top acc: 0.1785 ::: bot acc: 0.0284
top acc: 0.1396 ::: bot acc: 0.0309
top acc: 0.1445 ::: bot acc: 0.0418
top acc: 0.1155 ::: bot acc: 0.0222
current epoch: 37
train loss is 0.053923
average val loss: 0.048951, accuracy: 0.0476
average test loss: 0.075978, accuracy: 0.0758
case acc: 0.06266976
case acc: 0.05999433
case acc: 0.09443518
case acc: 0.08443102
case acc: 0.097238876
case acc: 0.05595961
top acc: 0.1096 ::: bot acc: 0.0169
top acc: 0.1019 ::: bot acc: 0.0189
top acc: 0.1778 ::: bot acc: 0.0283
top acc: 0.1443 ::: bot acc: 0.0327
top acc: 0.1499 ::: bot acc: 0.0467
top acc: 0.1026 ::: bot acc: 0.0251
current epoch: 38
train loss is 0.052959
average val loss: 0.047339, accuracy: 0.0461
average test loss: 0.066463, accuracy: 0.0666
case acc: 0.048188105
case acc: 0.053948738
case acc: 0.08478858
case acc: 0.07846105
case acc: 0.08634557
case acc: 0.048017353
top acc: 0.0850 ::: bot acc: 0.0232
top acc: 0.0908 ::: bot acc: 0.0232
top acc: 0.1632 ::: bot acc: 0.0288
top acc: 0.1367 ::: bot acc: 0.0301
top acc: 0.1385 ::: bot acc: 0.0367
top acc: 0.0861 ::: bot acc: 0.0346
current epoch: 39
train loss is 0.051282
average val loss: 0.049165, accuracy: 0.0487
average test loss: 0.056403, accuracy: 0.0570
case acc: 0.040019087
case acc: 0.046259917
case acc: 0.07423474
case acc: 0.06820022
case acc: 0.0689659
case acc: 0.044411425
top acc: 0.0577 ::: bot acc: 0.0496
top acc: 0.0717 ::: bot acc: 0.0387
top acc: 0.1431 ::: bot acc: 0.0376
top acc: 0.1225 ::: bot acc: 0.0279
top acc: 0.1186 ::: bot acc: 0.0246
top acc: 0.0728 ::: bot acc: 0.0477
current epoch: 40
train loss is 0.050245
average val loss: 0.056911, accuracy: 0.0571
average test loss: 0.049244, accuracy: 0.0498
case acc: 0.040813316
case acc: 0.040477034
case acc: 0.067240775
case acc: 0.055627003
case acc: 0.050761417
case acc: 0.043589327
top acc: 0.0330 ::: bot acc: 0.0743
top acc: 0.0473 ::: bot acc: 0.0631
top acc: 0.1202 ::: bot acc: 0.0595
top acc: 0.1023 ::: bot acc: 0.0311
top acc: 0.0934 ::: bot acc: 0.0206
top acc: 0.0604 ::: bot acc: 0.0601
current epoch: 41
train loss is 0.050598
average val loss: 0.067372, accuracy: 0.0680
average test loss: 0.047688, accuracy: 0.0476
case acc: 0.046829667
case acc: 0.04229555
case acc: 0.065183304
case acc: 0.047687087
case acc: 0.039434444
case acc: 0.044361535
top acc: 0.0219 ::: bot acc: 0.0894
top acc: 0.0249 ::: bot acc: 0.0854
top acc: 0.1009 ::: bot acc: 0.0788
top acc: 0.0817 ::: bot acc: 0.0486
top acc: 0.0708 ::: bot acc: 0.0325
top acc: 0.0515 ::: bot acc: 0.0689
current epoch: 42
train loss is 0.052135
average val loss: 0.070988, accuracy: 0.0718
average test loss: 0.048260, accuracy: 0.0474
case acc: 0.04534868
case acc: 0.04627342
case acc: 0.06542843
case acc: 0.04607014
case acc: 0.03715518
case acc: 0.04424658
top acc: 0.0228 ::: bot acc: 0.0867
top acc: 0.0179 ::: bot acc: 0.0947
top acc: 0.0950 ::: bot acc: 0.0848
top acc: 0.0714 ::: bot acc: 0.0589
top acc: 0.0614 ::: bot acc: 0.0418
top acc: 0.0526 ::: bot acc: 0.0679
current epoch: 43
train loss is 0.053347
average val loss: 0.063266, accuracy: 0.0639
average test loss: 0.047975, accuracy: 0.0462
case acc: 0.039692905
case acc: 0.042297117
case acc: 0.0655013
case acc: 0.04674474
case acc: 0.039548375
case acc: 0.043607306
top acc: 0.0421 ::: bot acc: 0.0652
top acc: 0.0250 ::: bot acc: 0.0854
top acc: 0.1069 ::: bot acc: 0.0729
top acc: 0.0775 ::: bot acc: 0.0528
top acc: 0.0711 ::: bot acc: 0.0321
top acc: 0.0658 ::: bot acc: 0.0547
current epoch: 44
train loss is 0.052950
average val loss: 0.051161, accuracy: 0.0515
average test loss: 0.053891, accuracy: 0.0522
case acc: 0.044803195
case acc: 0.040960316
case acc: 0.07072704
case acc: 0.05457369
case acc: 0.053773355
case acc: 0.048534926
top acc: 0.0760 ::: bot acc: 0.0313
top acc: 0.0514 ::: bot acc: 0.0590
top acc: 0.1339 ::: bot acc: 0.0458
top acc: 0.0999 ::: bot acc: 0.0327
top acc: 0.0982 ::: bot acc: 0.0199
top acc: 0.0876 ::: bot acc: 0.0333
current epoch: 45
train loss is 0.051668
average val loss: 0.047333, accuracy: 0.0472
average test loss: 0.065773, accuracy: 0.0650
case acc: 0.057627574
case acc: 0.048698705
case acc: 0.08257611
case acc: 0.06932739
case acc: 0.075743616
case acc: 0.055955272
top acc: 0.1020 ::: bot acc: 0.0171
top acc: 0.0788 ::: bot acc: 0.0319
top acc: 0.1592 ::: bot acc: 0.0301
top acc: 0.1241 ::: bot acc: 0.0281
top acc: 0.1264 ::: bot acc: 0.0291
top acc: 0.1027 ::: bot acc: 0.0251
current epoch: 46
train loss is 0.050751
average val loss: 0.047964, accuracy: 0.0472
average test loss: 0.072303, accuracy: 0.0720
case acc: 0.061345063
case acc: 0.055487312
case acc: 0.0893059
case acc: 0.07965502
case acc: 0.08972323
case acc: 0.056279417
top acc: 0.1077 ::: bot acc: 0.0168
top acc: 0.0939 ::: bot acc: 0.0216
top acc: 0.1703 ::: bot acc: 0.0279
top acc: 0.1382 ::: bot acc: 0.0306
top acc: 0.1421 ::: bot acc: 0.0397
top acc: 0.1032 ::: bot acc: 0.0249
current epoch: 47
train loss is 0.050311
average val loss: 0.047534, accuracy: 0.0465
average test loss: 0.069860, accuracy: 0.0698
case acc: 0.053662963
case acc: 0.05563738
case acc: 0.087026656
case acc: 0.08119906
case acc: 0.090005755
case acc: 0.051463224
top acc: 0.0956 ::: bot acc: 0.0181
top acc: 0.0941 ::: bot acc: 0.0215
top acc: 0.1668 ::: bot acc: 0.0281
top acc: 0.1402 ::: bot acc: 0.0312
top acc: 0.1424 ::: bot acc: 0.0399
top acc: 0.0941 ::: bot acc: 0.0288
current epoch: 48
train loss is 0.049490
average val loss: 0.047439, accuracy: 0.0467
average test loss: 0.061500, accuracy: 0.0620
case acc: 0.043848384
case acc: 0.049918987
case acc: 0.07870547
case acc: 0.07465175
case acc: 0.07835691
case acc: 0.046284925
top acc: 0.0730 ::: bot acc: 0.0342
top acc: 0.0817 ::: bot acc: 0.0295
top acc: 0.1522 ::: bot acc: 0.0325
top acc: 0.1315 ::: bot acc: 0.0291
top acc: 0.1295 ::: bot acc: 0.0309
top acc: 0.0813 ::: bot acc: 0.0392
current epoch: 49
train loss is 0.048856
average val loss: 0.051508, accuracy: 0.0515
average test loss: 0.052832, accuracy: 0.0535
case acc: 0.03936969
case acc: 0.043141473
case acc: 0.07013402
case acc: 0.0633162
case acc: 0.06100755
case acc: 0.04376899
top acc: 0.0485 ::: bot acc: 0.0588
top acc: 0.0613 ::: bot acc: 0.0491
top acc: 0.1320 ::: bot acc: 0.0477
top acc: 0.1153 ::: bot acc: 0.0278
top acc: 0.1083 ::: bot acc: 0.0214
top acc: 0.0689 ::: bot acc: 0.0516
current epoch: 50
train loss is 0.049007
average val loss: 0.060220, accuracy: 0.0606
average test loss: 0.048036, accuracy: 0.0483
case acc: 0.04173538
case acc: 0.039897326
case acc: 0.06599438
case acc: 0.05248617
case acc: 0.045823324
case acc: 0.043699354
top acc: 0.0291 ::: bot acc: 0.0781
top acc: 0.0386 ::: bot acc: 0.0719
top acc: 0.1115 ::: bot acc: 0.0682
top acc: 0.0952 ::: bot acc: 0.0360
top acc: 0.0849 ::: bot acc: 0.0229
top acc: 0.0582 ::: bot acc: 0.0622
LME_Co_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5346 5346 5346
1.7082474 -0.6288155 0.25454274 -0.21218425
Validation: 594 594 594
Testing: 768 768 768
pre-processing time: 0.00037217140197753906
the split date is 2012-07-01
net initializing with time: 0.003216266632080078
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.163640
average val loss: 0.077709, accuracy: 0.0756
average test loss: 0.078905, accuracy: 0.0814
case acc: 0.09448253
case acc: 0.12594609
case acc: 0.10552399
case acc: 0.04337319
case acc: 0.06424224
case acc: 0.055095546
top acc: 0.0623 ::: bot acc: 0.1359
top acc: 0.0827 ::: bot acc: 0.1670
top acc: 0.0261 ::: bot acc: 0.1818
top acc: 0.0142 ::: bot acc: 0.0782
top acc: 0.0232 ::: bot acc: 0.1076
top acc: 0.0837 ::: bot acc: 0.0645
current epoch: 2
train loss is 0.098255
average val loss: 0.149557, accuracy: 0.1493
average test loss: 0.150553, accuracy: 0.1509
case acc: 0.1554693
case acc: 0.21343735
case acc: 0.19305131
case acc: 0.124991156
case acc: 0.13985902
case acc: 0.07844444
top acc: 0.0760 ::: bot acc: 0.2204
top acc: 0.1703 ::: bot acc: 0.2548
top acc: 0.1094 ::: bot acc: 0.2720
top acc: 0.0901 ::: bot acc: 0.1629
top acc: 0.0752 ::: bot acc: 0.1948
top acc: 0.0296 ::: bot acc: 0.1433
current epoch: 3
train loss is 0.137821
average val loss: 0.062678, accuracy: 0.0606
average test loss: 0.063290, accuracy: 0.0667
case acc: 0.071361706
case acc: 0.09319042
case acc: 0.08565004
case acc: 0.027301963
case acc: 0.052565686
case acc: 0.07030107
top acc: 0.0643 ::: bot acc: 0.1012
top acc: 0.0502 ::: bot acc: 0.1347
top acc: 0.0200 ::: bot acc: 0.1563
top acc: 0.0229 ::: bot acc: 0.0498
top acc: 0.0367 ::: bot acc: 0.0838
top acc: 0.1203 ::: bot acc: 0.0374
current epoch: 4
train loss is 0.105193
average val loss: 0.058936, accuracy: 0.0583
average test loss: 0.058389, accuracy: 0.0606
case acc: 0.057235807
case acc: 0.037038937
case acc: 0.06162883
case acc: 0.04779028
case acc: 0.045749325
case acc: 0.11398378
top acc: 0.1164 ::: bot acc: 0.0376
top acc: 0.0181 ::: bot acc: 0.0669
top acc: 0.0711 ::: bot acc: 0.0920
top acc: 0.0794 ::: bot acc: 0.0165
top acc: 0.0925 ::: bot acc: 0.0272
top acc: 0.1759 ::: bot acc: 0.0565
current epoch: 5
train loss is 0.073186
average val loss: 0.055556, accuracy: 0.0529
average test loss: 0.055747, accuracy: 0.0585
case acc: 0.061583392
case acc: 0.06661693
case acc: 0.07501107
case acc: 0.026764289
case acc: 0.049834453
case acc: 0.07122198
top acc: 0.0707 ::: bot acc: 0.0838
top acc: 0.0238 ::: bot acc: 0.1083
top acc: 0.0299 ::: bot acc: 0.1357
top acc: 0.0279 ::: bot acc: 0.0450
top acc: 0.0425 ::: bot acc: 0.0769
top acc: 0.1216 ::: bot acc: 0.0372
current epoch: 6
train loss is 0.075196
average val loss: 0.061475, accuracy: 0.0587
average test loss: 0.062163, accuracy: 0.0645
case acc: 0.071518034
case acc: 0.07937765
case acc: 0.08285022
case acc: 0.035540547
case acc: 0.059554458
case acc: 0.058381446
top acc: 0.0645 ::: bot acc: 0.1017
top acc: 0.0362 ::: bot acc: 0.1214
top acc: 0.0211 ::: bot acc: 0.1517
top acc: 0.0104 ::: bot acc: 0.0689
top acc: 0.0267 ::: bot acc: 0.0991
top acc: 0.0955 ::: bot acc: 0.0514
current epoch: 7
train loss is 0.079622
average val loss: 0.053758, accuracy: 0.0503
average test loss: 0.053856, accuracy: 0.0556
case acc: 0.060566224
case acc: 0.055215873
case acc: 0.07267284
case acc: 0.028981002
case acc: 0.052708507
case acc: 0.063733675
top acc: 0.0731 ::: bot acc: 0.0812
top acc: 0.0140 ::: bot acc: 0.0964
top acc: 0.0344 ::: bot acc: 0.1300
top acc: 0.0177 ::: bot acc: 0.0555
top acc: 0.0353 ::: bot acc: 0.0847
top acc: 0.1076 ::: bot acc: 0.0430
current epoch: 8
train loss is 0.073215
average val loss: 0.050183, accuracy: 0.0462
average test loss: 0.049727, accuracy: 0.0509
case acc: 0.056689184
case acc: 0.039182283
case acc: 0.06509476
case acc: 0.02688467
case acc: 0.047624584
case acc: 0.06996669
top acc: 0.0929 ::: bot acc: 0.0614
top acc: 0.0149 ::: bot acc: 0.0721
top acc: 0.0540 ::: bot acc: 0.1089
top acc: 0.0309 ::: bot acc: 0.0425
top acc: 0.0484 ::: bot acc: 0.0705
top acc: 0.1192 ::: bot acc: 0.0381
current epoch: 9
train loss is 0.068730
average val loss: 0.049878, accuracy: 0.0460
average test loss: 0.049107, accuracy: 0.0496
case acc: 0.05648419
case acc: 0.03365153
case acc: 0.062107563
case acc: 0.027129449
case acc: 0.045557175
case acc: 0.07240833
top acc: 0.1051 ::: bot acc: 0.0495
top acc: 0.0300 ::: bot acc: 0.0556
top acc: 0.0677 ::: bot acc: 0.0952
top acc: 0.0373 ::: bot acc: 0.0363
top acc: 0.0562 ::: bot acc: 0.0626
top acc: 0.1233 ::: bot acc: 0.0373
current epoch: 10
train loss is 0.065357
average val loss: 0.052128, accuracy: 0.0489
average test loss: 0.051104, accuracy: 0.0504
case acc: 0.05811011
case acc: 0.03274475
case acc: 0.060251236
case acc: 0.029702593
case acc: 0.043338858
case acc: 0.07821354
top acc: 0.1209 ::: bot acc: 0.0338
top acc: 0.0505 ::: bot acc: 0.0352
top acc: 0.0855 ::: bot acc: 0.0774
top acc: 0.0486 ::: bot acc: 0.0250
top acc: 0.0696 ::: bot acc: 0.0491
top acc: 0.1314 ::: bot acc: 0.0381
current epoch: 11
train loss is 0.064003
average val loss: 0.058577, accuracy: 0.0558
average test loss: 0.057839, accuracy: 0.0557
case acc: 0.0641003
case acc: 0.039367236
case acc: 0.06207361
case acc: 0.0370188
case acc: 0.044289302
case acc: 0.08706345
top acc: 0.1396 ::: bot acc: 0.0167
top acc: 0.0742 ::: bot acc: 0.0146
top acc: 0.1068 ::: bot acc: 0.0559
top acc: 0.0646 ::: bot acc: 0.0148
top acc: 0.0883 ::: bot acc: 0.0302
top acc: 0.1433 ::: bot acc: 0.0409
current epoch: 12
train loss is 0.062930
average val loss: 0.065068, accuracy: 0.0627
average test loss: 0.064947, accuracy: 0.0627
case acc: 0.07061163
case acc: 0.052160166
case acc: 0.065373145
case acc: 0.044678442
case acc: 0.05132992
case acc: 0.0919336
top acc: 0.1518 ::: bot acc: 0.0115
top acc: 0.0913 ::: bot acc: 0.0185
top acc: 0.1225 ::: bot acc: 0.0403
top acc: 0.0759 ::: bot acc: 0.0149
top acc: 0.1034 ::: bot acc: 0.0211
top acc: 0.1493 ::: bot acc: 0.0432
current epoch: 13
train loss is 0.062029
average val loss: 0.071805, accuracy: 0.0699
average test loss: 0.072329, accuracy: 0.0704
case acc: 0.07725608
case acc: 0.06399606
case acc: 0.070648536
case acc: 0.053230375
case acc: 0.061808858
case acc: 0.09539932
top acc: 0.1613 ::: bot acc: 0.0126
top acc: 0.1055 ::: bot acc: 0.0257
top acc: 0.1358 ::: bot acc: 0.0291
top acc: 0.0865 ::: bot acc: 0.0194
top acc: 0.1187 ::: bot acc: 0.0219
top acc: 0.1536 ::: bot acc: 0.0450
current epoch: 14
train loss is 0.060960
average val loss: 0.075271, accuracy: 0.0737
average test loss: 0.076144, accuracy: 0.0744
case acc: 0.07935338
case acc: 0.06987163
case acc: 0.07391287
case acc: 0.05823189
case acc: 0.07073662
case acc: 0.09450434
top acc: 0.1639 ::: bot acc: 0.0138
top acc: 0.1123 ::: bot acc: 0.0298
top acc: 0.1427 ::: bot acc: 0.0250
top acc: 0.0923 ::: bot acc: 0.0230
top acc: 0.1296 ::: bot acc: 0.0268
top acc: 0.1527 ::: bot acc: 0.0444
current epoch: 15
train loss is 0.059928
average val loss: 0.073269, accuracy: 0.0722
average test loss: 0.074013, accuracy: 0.0724
case acc: 0.0746876
case acc: 0.06723091
case acc: 0.07305907
case acc: 0.0571037
case acc: 0.0740255
case acc: 0.08822103
top acc: 0.1580 ::: bot acc: 0.0118
top acc: 0.1093 ::: bot acc: 0.0279
top acc: 0.1410 ::: bot acc: 0.0260
top acc: 0.0909 ::: bot acc: 0.0222
top acc: 0.1335 ::: bot acc: 0.0288
top acc: 0.1448 ::: bot acc: 0.0413
current epoch: 16
train loss is 0.058054
average val loss: 0.070765, accuracy: 0.0703
average test loss: 0.071299, accuracy: 0.0699
case acc: 0.06994128
case acc: 0.062607974
case acc: 0.07138623
case acc: 0.055857714
case acc: 0.07669655
case acc: 0.08266919
top acc: 0.1508 ::: bot acc: 0.0118
top acc: 0.1038 ::: bot acc: 0.0248
top acc: 0.1374 ::: bot acc: 0.0282
top acc: 0.0894 ::: bot acc: 0.0213
top acc: 0.1366 ::: bot acc: 0.0306
top acc: 0.1376 ::: bot acc: 0.0392
current epoch: 17
train loss is 0.056148
average val loss: 0.068298, accuracy: 0.0683
average test loss: 0.068617, accuracy: 0.0674
case acc: 0.066145614
case acc: 0.057014354
case acc: 0.069339156
case acc: 0.05489775
case acc: 0.07875199
case acc: 0.07853055
top acc: 0.1438 ::: bot acc: 0.0146
top acc: 0.0972 ::: bot acc: 0.0213
top acc: 0.1329 ::: bot acc: 0.0312
top acc: 0.0883 ::: bot acc: 0.0206
top acc: 0.1389 ::: bot acc: 0.0320
top acc: 0.1319 ::: bot acc: 0.0381
current epoch: 18
train loss is 0.054922
average val loss: 0.061820, accuracy: 0.0625
average test loss: 0.061305, accuracy: 0.0605
case acc: 0.060412817
case acc: 0.045085296
case acc: 0.0646925
case acc: 0.04807972
case acc: 0.073551014
case acc: 0.070922166
top acc: 0.1299 ::: bot acc: 0.0250
top acc: 0.0824 ::: bot acc: 0.0152
top acc: 0.1203 ::: bot acc: 0.0423
top acc: 0.0803 ::: bot acc: 0.0164
top acc: 0.1329 ::: bot acc: 0.0285
top acc: 0.1208 ::: bot acc: 0.0377
current epoch: 19
train loss is 0.052701
average val loss: 0.055283, accuracy: 0.0565
average test loss: 0.054138, accuracy: 0.0539
case acc: 0.057154153
case acc: 0.03513883
case acc: 0.061627436
case acc: 0.039954387
case acc: 0.06508323
case acc: 0.06444518
top acc: 0.1142 ::: bot acc: 0.0406
top acc: 0.0646 ::: bot acc: 0.0211
top acc: 0.1043 ::: bot acc: 0.0583
top acc: 0.0695 ::: bot acc: 0.0135
top acc: 0.1228 ::: bot acc: 0.0234
top acc: 0.1086 ::: bot acc: 0.0430
current epoch: 20
train loss is 0.051717
average val loss: 0.050028, accuracy: 0.0515
average test loss: 0.048784, accuracy: 0.0492
case acc: 0.05647529
case acc: 0.032421377
case acc: 0.060194395
case acc: 0.03268668
case acc: 0.054505367
case acc: 0.058806382
top acc: 0.0970 ::: bot acc: 0.0577
top acc: 0.0445 ::: bot acc: 0.0412
top acc: 0.0854 ::: bot acc: 0.0772
top acc: 0.0561 ::: bot acc: 0.0189
top acc: 0.1083 ::: bot acc: 0.0208
top acc: 0.0954 ::: bot acc: 0.0528
current epoch: 21
train loss is 0.052607
average val loss: 0.047847, accuracy: 0.0494
average test loss: 0.047274, accuracy: 0.0476
case acc: 0.060192145
case acc: 0.036943696
case acc: 0.06366559
case acc: 0.0271327
case acc: 0.0436553
case acc: 0.053878788
top acc: 0.0751 ::: bot acc: 0.0797
top acc: 0.0190 ::: bot acc: 0.0669
top acc: 0.0601 ::: bot acc: 0.1026
top acc: 0.0361 ::: bot acc: 0.0377
top acc: 0.0855 ::: bot acc: 0.0329
top acc: 0.0775 ::: bot acc: 0.0704
current epoch: 22
train loss is 0.056652
average val loss: 0.052792, accuracy: 0.0541
average test loss: 0.053024, accuracy: 0.0540
case acc: 0.07112123
case acc: 0.051902104
case acc: 0.07235514
case acc: 0.030373698
case acc: 0.0446304
case acc: 0.05339558
top acc: 0.0649 ::: bot acc: 0.1010
top acc: 0.0119 ::: bot acc: 0.0926
top acc: 0.0345 ::: bot acc: 0.1291
top acc: 0.0148 ::: bot acc: 0.0591
top acc: 0.0596 ::: bot acc: 0.0588
top acc: 0.0601 ::: bot acc: 0.0878
current epoch: 23
train loss is 0.060949
average val loss: 0.064046, accuracy: 0.0643
average test loss: 0.064954, accuracy: 0.0660
case acc: 0.08299002
case acc: 0.073956616
case acc: 0.084719405
case acc: 0.04464247
case acc: 0.05323372
case acc: 0.056212462
top acc: 0.0621 ::: bot acc: 0.1201
top acc: 0.0307 ::: bot acc: 0.1163
top acc: 0.0196 ::: bot acc: 0.1548
top acc: 0.0147 ::: bot acc: 0.0804
top acc: 0.0334 ::: bot acc: 0.0862
top acc: 0.0445 ::: bot acc: 0.1034
current epoch: 24
train loss is 0.064968
average val loss: 0.070879, accuracy: 0.0704
average test loss: 0.072119, accuracy: 0.0729
case acc: 0.08675346
case acc: 0.08352747
case acc: 0.0933006
case acc: 0.054280087
case acc: 0.062339645
case acc: 0.05734108
top acc: 0.0618 ::: bot acc: 0.1259
top acc: 0.0402 ::: bot acc: 0.1259
top acc: 0.0192 ::: bot acc: 0.1678
top acc: 0.0214 ::: bot acc: 0.0914
top acc: 0.0245 ::: bot acc: 0.1041
top acc: 0.0404 ::: bot acc: 0.1075
current epoch: 25
train loss is 0.070041
average val loss: 0.067407, accuracy: 0.0663
average test loss: 0.068487, accuracy: 0.0693
case acc: 0.07972743
case acc: 0.07433931
case acc: 0.089985274
case acc: 0.052328818
case acc: 0.06472308
case acc: 0.054988127
top acc: 0.0624 ::: bot acc: 0.1152
top acc: 0.0311 ::: bot acc: 0.1168
top acc: 0.0187 ::: bot acc: 0.1631
top acc: 0.0199 ::: bot acc: 0.0892
top acc: 0.0235 ::: bot acc: 0.1081
top acc: 0.0495 ::: bot acc: 0.0984
current epoch: 26
train loss is 0.069034
average val loss: 0.052257, accuracy: 0.0492
average test loss: 0.052375, accuracy: 0.0528
case acc: 0.059972897
case acc: 0.04304235
case acc: 0.07300095
case acc: 0.03266412
case acc: 0.053539377
case acc: 0.05438092
top acc: 0.0751 ::: bot acc: 0.0795
top acc: 0.0116 ::: bot acc: 0.0798
top acc: 0.0330 ::: bot acc: 0.1309
top acc: 0.0117 ::: bot acc: 0.0640
top acc: 0.0326 ::: bot acc: 0.0871
top acc: 0.0799 ::: bot acc: 0.0680
current epoch: 27
train loss is 0.061407
average val loss: 0.048954, accuracy: 0.0451
average test loss: 0.048087, accuracy: 0.0481
case acc: 0.056711048
case acc: 0.032492835
case acc: 0.06262973
case acc: 0.027097275
case acc: 0.045876253
case acc: 0.06354352
top acc: 0.1088 ::: bot acc: 0.0459
top acc: 0.0429 ::: bot acc: 0.0428
top acc: 0.0646 ::: bot acc: 0.0980
top acc: 0.0339 ::: bot acc: 0.0398
top acc: 0.0538 ::: bot acc: 0.0646
top acc: 0.1066 ::: bot acc: 0.0442
current epoch: 28
train loss is 0.060209
average val loss: 0.060689, accuracy: 0.0575
average test loss: 0.060281, accuracy: 0.0582
case acc: 0.0692786
case acc: 0.049889922
case acc: 0.062117517
case acc: 0.03919326
case acc: 0.044541657
case acc: 0.0843847
top acc: 0.1496 ::: bot acc: 0.0117
top acc: 0.0883 ::: bot acc: 0.0173
top acc: 0.1071 ::: bot acc: 0.0554
top acc: 0.0682 ::: bot acc: 0.0138
top acc: 0.0892 ::: bot acc: 0.0292
top acc: 0.1398 ::: bot acc: 0.0398
current epoch: 29
train loss is 0.063304
average val loss: 0.085016, accuracy: 0.0832
average test loss: 0.086249, accuracy: 0.0851
case acc: 0.100215286
case acc: 0.087224945
case acc: 0.0769212
case acc: 0.06827086
case acc: 0.06962511
case acc: 0.108388916
top acc: 0.1861 ::: bot acc: 0.0313
top acc: 0.1304 ::: bot acc: 0.0449
top acc: 0.1482 ::: bot acc: 0.0227
top acc: 0.1028 ::: bot acc: 0.0315
top acc: 0.1282 ::: bot acc: 0.0260
top acc: 0.1694 ::: bot acc: 0.0523
current epoch: 30
train loss is 0.067768
average val loss: 0.088874, accuracy: 0.0877
average test loss: 0.090345, accuracy: 0.0894
case acc: 0.101628125
case acc: 0.09357692
case acc: 0.08182022
case acc: 0.073231265
case acc: 0.08009908
case acc: 0.10612201
top acc: 0.1878 ::: bot acc: 0.0327
top acc: 0.1369 ::: bot acc: 0.0511
top acc: 0.1563 ::: bot acc: 0.0210
top acc: 0.1081 ::: bot acc: 0.0358
top acc: 0.1404 ::: bot acc: 0.0329
top acc: 0.1668 ::: bot acc: 0.0508
current epoch: 31
train loss is 0.066792
average val loss: 0.077902, accuracy: 0.0767
average test loss: 0.078953, accuracy: 0.0777
case acc: 0.083576694
case acc: 0.07768358
case acc: 0.074546576
case acc: 0.06301174
case acc: 0.07672514
case acc: 0.090729
top acc: 0.1688 ::: bot acc: 0.0163
top acc: 0.1205 ::: bot acc: 0.0361
top acc: 0.1437 ::: bot acc: 0.0244
top acc: 0.0972 ::: bot acc: 0.0270
top acc: 0.1366 ::: bot acc: 0.0305
top acc: 0.1478 ::: bot acc: 0.0427
current epoch: 32
train loss is 0.061643
average val loss: 0.066725, accuracy: 0.0662
average test loss: 0.066968, accuracy: 0.0657
case acc: 0.06836245
case acc: 0.058834106
case acc: 0.06677301
case acc: 0.052383896
case acc: 0.07079332
case acc: 0.077342026
top acc: 0.1482 ::: bot acc: 0.0125
top acc: 0.0994 ::: bot acc: 0.0222
top acc: 0.1265 ::: bot acc: 0.0362
top acc: 0.0855 ::: bot acc: 0.0188
top acc: 0.1297 ::: bot acc: 0.0266
top acc: 0.1304 ::: bot acc: 0.0378
current epoch: 33
train loss is 0.054951
average val loss: 0.058040, accuracy: 0.0582
average test loss: 0.057089, accuracy: 0.0564
case acc: 0.06009012
case acc: 0.041724253
case acc: 0.06227099
case acc: 0.04310353
case acc: 0.06344432
case acc: 0.06795409
top acc: 0.1286 ::: bot acc: 0.0266
top acc: 0.0777 ::: bot acc: 0.0144
top acc: 0.1084 ::: bot acc: 0.0540
top acc: 0.0740 ::: bot acc: 0.0141
top acc: 0.1208 ::: bot acc: 0.0224
top acc: 0.1156 ::: bot acc: 0.0394
current epoch: 34
train loss is 0.050218
average val loss: 0.051252, accuracy: 0.0520
average test loss: 0.049913, accuracy: 0.0500
case acc: 0.05690816
case acc: 0.033322055
case acc: 0.060125202
case acc: 0.034758102
case acc: 0.054016165
case acc: 0.060866214
top acc: 0.1093 ::: bot acc: 0.0460
top acc: 0.0555 ::: bot acc: 0.0304
top acc: 0.0887 ::: bot acc: 0.0737
top acc: 0.0602 ::: bot acc: 0.0167
top acc: 0.1076 ::: bot acc: 0.0208
top acc: 0.1009 ::: bot acc: 0.0479
current epoch: 35
train loss is 0.048879
average val loss: 0.047460, accuracy: 0.0484
average test loss: 0.046618, accuracy: 0.0468
case acc: 0.057543546
case acc: 0.03349009
case acc: 0.062366966
case acc: 0.027965426
case acc: 0.044182207
case acc: 0.055145007
top acc: 0.0883 ::: bot acc: 0.0670
top acc: 0.0315 ::: bot acc: 0.0544
top acc: 0.0658 ::: bot acc: 0.0967
top acc: 0.0419 ::: bot acc: 0.0321
top acc: 0.0877 ::: bot acc: 0.0307
top acc: 0.0839 ::: bot acc: 0.0638
current epoch: 36
train loss is 0.050493
average val loss: 0.048806, accuracy: 0.0493
average test loss: 0.048625, accuracy: 0.0491
case acc: 0.06196126
case acc: 0.040479593
case acc: 0.067975014
case acc: 0.027685877
case acc: 0.04348263
case acc: 0.05291886
top acc: 0.0714 ::: bot acc: 0.0843
top acc: 0.0139 ::: bot acc: 0.0749
top acc: 0.0451 ::: bot acc: 0.1175
top acc: 0.0241 ::: bot acc: 0.0499
top acc: 0.0663 ::: bot acc: 0.0522
top acc: 0.0695 ::: bot acc: 0.0780
current epoch: 37
train loss is 0.053301
average val loss: 0.051955, accuracy: 0.0518
average test loss: 0.052116, accuracy: 0.0526
case acc: 0.06571055
case acc: 0.046432123
case acc: 0.07260455
case acc: 0.031164601
case acc: 0.046936773
case acc: 0.053011686
top acc: 0.0668 ::: bot acc: 0.0921
top acc: 0.0104 ::: bot acc: 0.0854
top acc: 0.0336 ::: bot acc: 0.1301
top acc: 0.0135 ::: bot acc: 0.0611
top acc: 0.0502 ::: bot acc: 0.0684
top acc: 0.0632 ::: bot acc: 0.0842
current epoch: 38
train loss is 0.055133
average val loss: 0.051786, accuracy: 0.0506
average test loss: 0.051937, accuracy: 0.0523
case acc: 0.06335835
case acc: 0.044487394
case acc: 0.07262837
case acc: 0.0317058
case acc: 0.048938535
case acc: 0.05280119
top acc: 0.0689 ::: bot acc: 0.0876
top acc: 0.0107 ::: bot acc: 0.0824
top acc: 0.0336 ::: bot acc: 0.1302
top acc: 0.0127 ::: bot acc: 0.0623
top acc: 0.0438 ::: bot acc: 0.0748
top acc: 0.0674 ::: bot acc: 0.0800
current epoch: 39
train loss is 0.054747
average val loss: 0.048271, accuracy: 0.0460
average test loss: 0.047939, accuracy: 0.0483
case acc: 0.05780738
case acc: 0.0355338
case acc: 0.06693649
case acc: 0.02774648
case acc: 0.04659488
case acc: 0.055010285
top acc: 0.0864 ::: bot acc: 0.0688
top acc: 0.0224 ::: bot acc: 0.0634
top acc: 0.0479 ::: bot acc: 0.1147
top acc: 0.0233 ::: bot acc: 0.0506
top acc: 0.0514 ::: bot acc: 0.0672
top acc: 0.0836 ::: bot acc: 0.0639
current epoch: 40
train loss is 0.052345
average val loss: 0.048529, accuracy: 0.0459
average test loss: 0.047524, accuracy: 0.0473
case acc: 0.056957006
case acc: 0.032490037
case acc: 0.06170258
case acc: 0.027591879
case acc: 0.04360324
case acc: 0.061530665
top acc: 0.1095 ::: bot acc: 0.0457
top acc: 0.0472 ::: bot acc: 0.0386
top acc: 0.0694 ::: bot acc: 0.0932
top acc: 0.0397 ::: bot acc: 0.0342
top acc: 0.0654 ::: bot acc: 0.0532
top acc: 0.1026 ::: bot acc: 0.0465
current epoch: 41
train loss is 0.052209
average val loss: 0.056469, accuracy: 0.0539
average test loss: 0.055507, accuracy: 0.0538
case acc: 0.06350714
case acc: 0.04271248
case acc: 0.06093269
case acc: 0.03670522
case acc: 0.044675115
case acc: 0.07449
top acc: 0.1385 ::: bot acc: 0.0174
top acc: 0.0791 ::: bot acc: 0.0142
top acc: 0.0989 ::: bot acc: 0.0636
top acc: 0.0637 ::: bot acc: 0.0152
top acc: 0.0894 ::: bot acc: 0.0292
top acc: 0.1264 ::: bot acc: 0.0371
current epoch: 42
train loss is 0.055082
average val loss: 0.072283, accuracy: 0.0703
average test loss: 0.072915, accuracy: 0.0715
case acc: 0.08159512
case acc: 0.06919696
case acc: 0.068355195
case acc: 0.056481123
case acc: 0.061555564
case acc: 0.09195149
top acc: 0.1667 ::: bot acc: 0.0148
top acc: 0.1113 ::: bot acc: 0.0290
top acc: 0.1304 ::: bot acc: 0.0330
top acc: 0.0899 ::: bot acc: 0.0220
top acc: 0.1184 ::: bot acc: 0.0215
top acc: 0.1494 ::: bot acc: 0.0434
current epoch: 43
train loss is 0.058639
average val loss: 0.083767, accuracy: 0.0826
average test loss: 0.085116, accuracy: 0.0842
case acc: 0.093722805
case acc: 0.08534351
case acc: 0.07747042
case acc: 0.0703573
case acc: 0.07877605
case acc: 0.09941568
top acc: 0.1798 ::: bot acc: 0.0249
top acc: 0.1285 ::: bot acc: 0.0429
top acc: 0.1491 ::: bot acc: 0.0225
top acc: 0.1050 ::: bot acc: 0.0332
top acc: 0.1390 ::: bot acc: 0.0318
top acc: 0.1586 ::: bot acc: 0.0472
current epoch: 44
train loss is 0.060609
average val loss: 0.080361, accuracy: 0.0795
average test loss: 0.081626, accuracy: 0.0806
case acc: 0.085801974
case acc: 0.0796466
case acc: 0.07607221
case acc: 0.067966655
case acc: 0.08214219
case acc: 0.0918722
top acc: 0.1715 ::: bot acc: 0.0178
top acc: 0.1226 ::: bot acc: 0.0378
top acc: 0.1466 ::: bot acc: 0.0233
top acc: 0.1025 ::: bot acc: 0.0311
top acc: 0.1428 ::: bot acc: 0.0343
top acc: 0.1493 ::: bot acc: 0.0432
current epoch: 45
train loss is 0.059438
average val loss: 0.068923, accuracy: 0.0685
average test loss: 0.069373, accuracy: 0.0683
case acc: 0.070076264
case acc: 0.06102493
case acc: 0.06823852
case acc: 0.05658123
case acc: 0.07537322
case acc: 0.078283265
top acc: 0.1512 ::: bot acc: 0.0116
top acc: 0.1018 ::: bot acc: 0.0235
top acc: 0.1302 ::: bot acc: 0.0331
top acc: 0.0901 ::: bot acc: 0.0219
top acc: 0.1352 ::: bot acc: 0.0293
top acc: 0.1317 ::: bot acc: 0.0380
current epoch: 46
train loss is 0.054196
average val loss: 0.058685, accuracy: 0.0590
average test loss: 0.057826, accuracy: 0.0572
case acc: 0.060384803
case acc: 0.041902926
case acc: 0.06259366
case acc: 0.044957586
case acc: 0.06573482
case acc: 0.06764588
top acc: 0.1298 ::: bot acc: 0.0256
top acc: 0.0779 ::: bot acc: 0.0143
top acc: 0.1100 ::: bot acc: 0.0525
top acc: 0.0762 ::: bot acc: 0.0149
top acc: 0.1237 ::: bot acc: 0.0235
top acc: 0.1151 ::: bot acc: 0.0395
current epoch: 47
train loss is 0.049691
average val loss: 0.050383, accuracy: 0.0512
average test loss: 0.049090, accuracy: 0.0492
case acc: 0.056781877
case acc: 0.032808416
case acc: 0.060156226
case acc: 0.03383194
case acc: 0.052674145
case acc: 0.059137568
top acc: 0.1064 ::: bot acc: 0.0490
top acc: 0.0512 ::: bot acc: 0.0347
top acc: 0.0857 ::: bot acc: 0.0768
top acc: 0.0582 ::: bot acc: 0.0179
top acc: 0.1056 ::: bot acc: 0.0209
top acc: 0.0968 ::: bot acc: 0.0511
current epoch: 48
train loss is 0.049071
average val loss: 0.047363, accuracy: 0.0482
average test loss: 0.046680, accuracy: 0.0469
case acc: 0.058320597
case acc: 0.034648106
case acc: 0.06340964
case acc: 0.027442392
case acc: 0.04331039
case acc: 0.054051947
top acc: 0.0843 ::: bot acc: 0.0711
top acc: 0.0256 ::: bot acc: 0.0603
top acc: 0.0608 ::: bot acc: 0.1017
top acc: 0.0382 ::: bot acc: 0.0360
top acc: 0.0830 ::: bot acc: 0.0356
top acc: 0.0790 ::: bot acc: 0.0686
current epoch: 49
train loss is 0.051441
average val loss: 0.049482, accuracy: 0.0498
average test loss: 0.049398, accuracy: 0.0499
case acc: 0.06264564
case acc: 0.042315386
case acc: 0.06920526
case acc: 0.028114814
case acc: 0.044190593
case acc: 0.052882805
top acc: 0.0701 ::: bot acc: 0.0859
top acc: 0.0123 ::: bot acc: 0.0783
top acc: 0.0418 ::: bot acc: 0.1209
top acc: 0.0219 ::: bot acc: 0.0522
top acc: 0.0620 ::: bot acc: 0.0566
top acc: 0.0672 ::: bot acc: 0.0804
current epoch: 50
train loss is 0.054160
average val loss: 0.050767, accuracy: 0.0502
average test loss: 0.050829, accuracy: 0.0513
case acc: 0.062986165
case acc: 0.043951817
case acc: 0.07132693
case acc: 0.029818185
case acc: 0.046772853
case acc: 0.05287876
top acc: 0.0694 ::: bot acc: 0.0868
top acc: 0.0110 ::: bot acc: 0.0815
top acc: 0.0364 ::: bot acc: 0.1268
top acc: 0.0164 ::: bot acc: 0.0577
top acc: 0.0508 ::: bot acc: 0.0678
top acc: 0.0672 ::: bot acc: 0.0804

		{"drop_out": 0.4, "drop_out_mc": 0.15, "repeat_mc": 50, "hidden": 30, "embedding_size": 5, "batch": 512, "lag": 3}
{'generate_norm_params': 'v1', 'generate_tech_params': 'v3', 'generate_strat_params': None, 'generate_SD_params': 'v1', 'deal_with_abnormal_value': 'v2', 'labelling': 'v3', 'process_missing_value': 'v1', 'strategy_signal': None, 'normalize_without_1d_return': 'v1', 'technical_indication': 'v4', 'supply_and_demand': None, 'remove_unused_columns': 'v6', 'price_normalization': 'v3', 'scaling': None, 'construct': 'v4'}
LME_Co_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-01-01', '2010-07-01', '2014-07-01', '2015-01-01', '2015-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5328 5328 5328
1.7082474 -0.6288155 0.29835227 -0.25048217
Validation: 594 594 594
Testing: 774 774 774
pre-processing time: 0.00030493736267089844
the split date is 2010-07-01
net initializing with time: 0.004483938217163086
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.123600
average val loss: 0.111782, accuracy: 0.1124
average test loss: 0.134719, accuracy: 0.1391
case acc: 0.19893259
case acc: 0.064742476
case acc: 0.1588102
case acc: 0.13870503
case acc: 0.16504522
case acc: 0.10837966
top acc: 0.1670 ::: bot acc: 0.2368
top acc: 0.1164 ::: bot acc: 0.0194
top acc: 0.0872 ::: bot acc: 0.2259
top acc: 0.0970 ::: bot acc: 0.1775
top acc: 0.1338 ::: bot acc: 0.1943
top acc: 0.0652 ::: bot acc: 0.1557
current epoch: 2
train loss is 0.118590
average val loss: 0.090459, accuracy: 0.0927
average test loss: 0.071379, accuracy: 0.0685
case acc: 0.030484535
case acc: 0.21389659
case acc: 0.052003056
case acc: 0.036281794
case acc: 0.022520725
case acc: 0.05593493
top acc: 0.0093 ::: bot acc: 0.0629
top acc: 0.2768 ::: bot acc: 0.1461
top acc: 0.0873 ::: bot acc: 0.0515
top acc: 0.0703 ::: bot acc: 0.0130
top acc: 0.0283 ::: bot acc: 0.0326
top acc: 0.0953 ::: bot acc: 0.0166
current epoch: 3
train loss is 0.127343
average val loss: 0.146216, accuracy: 0.1466
average test loss: 0.113340, accuracy: 0.1126
case acc: 0.054628193
case acc: 0.26972494
case acc: 0.087802105
case acc: 0.09206559
case acc: 0.061618444
case acc: 0.10963401
top acc: 0.0770 ::: bot acc: 0.0355
top acc: 0.3329 ::: bot acc: 0.2019
top acc: 0.1568 ::: bot acc: 0.0270
top acc: 0.1342 ::: bot acc: 0.0534
top acc: 0.0928 ::: bot acc: 0.0323
top acc: 0.1532 ::: bot acc: 0.0620
current epoch: 4
train loss is 0.123339
average val loss: 0.081195, accuracy: 0.0829
average test loss: 0.063902, accuracy: 0.0615
case acc: 0.02970107
case acc: 0.18743531
case acc: 0.05166351
case acc: 0.032418597
case acc: 0.022690566
case acc: 0.045115568
top acc: 0.0098 ::: bot acc: 0.0616
top acc: 0.2506 ::: bot acc: 0.1195
top acc: 0.0844 ::: bot acc: 0.0543
top acc: 0.0602 ::: bot acc: 0.0206
top acc: 0.0268 ::: bot acc: 0.0342
top acc: 0.0771 ::: bot acc: 0.0209
current epoch: 5
train loss is 0.102125
average val loss: 0.060741, accuracy: 0.0617
average test loss: 0.058970, accuracy: 0.0613
case acc: 0.061183438
case acc: 0.13700758
case acc: 0.058336146
case acc: 0.035818417
case acc: 0.040241003
case acc: 0.03495109
top acc: 0.0295 ::: bot acc: 0.0991
top acc: 0.2001 ::: bot acc: 0.0691
top acc: 0.0436 ::: bot acc: 0.0965
top acc: 0.0203 ::: bot acc: 0.0614
top acc: 0.0182 ::: bot acc: 0.0655
top acc: 0.0372 ::: bot acc: 0.0568
current epoch: 6
train loss is 0.088490
average val loss: 0.059885, accuracy: 0.0609
average test loss: 0.055254, accuracy: 0.0569
case acc: 0.0527609
case acc: 0.13178614
case acc: 0.05730957
case acc: 0.034136366
case acc: 0.030122332
case acc: 0.03510014
top acc: 0.0213 ::: bot acc: 0.0906
top acc: 0.1950 ::: bot acc: 0.0638
top acc: 0.0457 ::: bot acc: 0.0939
top acc: 0.0246 ::: bot acc: 0.0566
top acc: 0.0167 ::: bot acc: 0.0511
top acc: 0.0426 ::: bot acc: 0.0519
current epoch: 7
train loss is 0.084654
average val loss: 0.062951, accuracy: 0.0641
average test loss: 0.052513, accuracy: 0.0529
case acc: 0.03877654
case acc: 0.13348393
case acc: 0.05383747
case acc: 0.031812143
case acc: 0.022428608
case acc: 0.03679401
top acc: 0.0100 ::: bot acc: 0.0752
top acc: 0.1968 ::: bot acc: 0.0654
top acc: 0.0549 ::: bot acc: 0.0837
top acc: 0.0357 ::: bot acc: 0.0456
top acc: 0.0296 ::: bot acc: 0.0320
top acc: 0.0535 ::: bot acc: 0.0414
current epoch: 8
train loss is 0.084477
average val loss: 0.068235, accuracy: 0.0693
average test loss: 0.052346, accuracy: 0.0520
case acc: 0.028701412
case acc: 0.13549486
case acc: 0.051711068
case acc: 0.031357735
case acc: 0.02518719
case acc: 0.03936013
top acc: 0.0101 ::: bot acc: 0.0601
top acc: 0.1991 ::: bot acc: 0.0673
top acc: 0.0661 ::: bot acc: 0.0727
top acc: 0.0462 ::: bot acc: 0.0350
top acc: 0.0470 ::: bot acc: 0.0145
top acc: 0.0627 ::: bot acc: 0.0326
current epoch: 9
train loss is 0.084717
average val loss: 0.069988, accuracy: 0.0709
average test loss: 0.051958, accuracy: 0.0515
case acc: 0.026092667
case acc: 0.13091533
case acc: 0.05130927
case acc: 0.031428833
case acc: 0.029325677
case acc: 0.04004698
top acc: 0.0162 ::: bot acc: 0.0527
top acc: 0.1946 ::: bot acc: 0.0627
top acc: 0.0703 ::: bot acc: 0.0684
top acc: 0.0497 ::: bot acc: 0.0316
top acc: 0.0561 ::: bot acc: 0.0087
top acc: 0.0645 ::: bot acc: 0.0311
current epoch: 10
train loss is 0.083808
average val loss: 0.068647, accuracy: 0.0694
average test loss: 0.050485, accuracy: 0.0502
case acc: 0.025742335
case acc: 0.12212306
case acc: 0.051317424
case acc: 0.031422645
case acc: 0.03141539
case acc: 0.03931087
top acc: 0.0183 ::: bot acc: 0.0507
top acc: 0.1857 ::: bot acc: 0.0539
top acc: 0.0699 ::: bot acc: 0.0689
top acc: 0.0487 ::: bot acc: 0.0326
top acc: 0.0597 ::: bot acc: 0.0080
top acc: 0.0623 ::: bot acc: 0.0332
current epoch: 11
train loss is 0.080854
average val loss: 0.065539, accuracy: 0.0661
average test loss: 0.048322, accuracy: 0.0482
case acc: 0.025837105
case acc: 0.110884346
case acc: 0.05178125
case acc: 0.031337433
case acc: 0.031506415
case acc: 0.037887704
top acc: 0.0173 ::: bot acc: 0.0516
top acc: 0.1747 ::: bot acc: 0.0425
top acc: 0.0667 ::: bot acc: 0.0724
top acc: 0.0455 ::: bot acc: 0.0358
top acc: 0.0599 ::: bot acc: 0.0080
top acc: 0.0581 ::: bot acc: 0.0369
current epoch: 12
train loss is 0.077166
average val loss: 0.060657, accuracy: 0.0612
average test loss: 0.045726, accuracy: 0.0458
case acc: 0.026986482
case acc: 0.09776345
case acc: 0.052853987
case acc: 0.03147468
case acc: 0.029487658
case acc: 0.036464147
top acc: 0.0131 ::: bot acc: 0.0559
top acc: 0.1609 ::: bot acc: 0.0308
top acc: 0.0603 ::: bot acc: 0.0789
top acc: 0.0399 ::: bot acc: 0.0414
top acc: 0.0565 ::: bot acc: 0.0088
top acc: 0.0519 ::: bot acc: 0.0430
current epoch: 13
train loss is 0.074493
average val loss: 0.057997, accuracy: 0.0585
average test loss: 0.044112, accuracy: 0.0444
case acc: 0.027459728
case acc: 0.08872837
case acc: 0.053592127
case acc: 0.031644408
case acc: 0.0287978
case acc: 0.035920985
top acc: 0.0120 ::: bot acc: 0.0572
top acc: 0.1505 ::: bot acc: 0.0244
top acc: 0.0571 ::: bot acc: 0.0820
top acc: 0.0378 ::: bot acc: 0.0435
top acc: 0.0552 ::: bot acc: 0.0093
top acc: 0.0492 ::: bot acc: 0.0457
current epoch: 14
train loss is 0.071694
average val loss: 0.058245, accuracy: 0.0587
average test loss: 0.043407, accuracy: 0.0436
case acc: 0.026341029
case acc: 0.08415542
case acc: 0.053203117
case acc: 0.031496394
case acc: 0.030064814
case acc: 0.036280207
top acc: 0.0152 ::: bot acc: 0.0537
top acc: 0.1452 ::: bot acc: 0.0213
top acc: 0.0591 ::: bot acc: 0.0801
top acc: 0.0406 ::: bot acc: 0.0408
top acc: 0.0575 ::: bot acc: 0.0086
top acc: 0.0513 ::: bot acc: 0.0435
current epoch: 15
train loss is 0.070119
average val loss: 0.056315, accuracy: 0.0567
average test loss: 0.042146, accuracy: 0.0423
case acc: 0.026341138
case acc: 0.077476874
case acc: 0.053725835
case acc: 0.031463765
case acc: 0.028794102
case acc: 0.035825863
top acc: 0.0149 ::: bot acc: 0.0539
top acc: 0.1370 ::: bot acc: 0.0179
top acc: 0.0579 ::: bot acc: 0.0817
top acc: 0.0398 ::: bot acc: 0.0415
top acc: 0.0552 ::: bot acc: 0.0091
top acc: 0.0498 ::: bot acc: 0.0448
current epoch: 16
train loss is 0.068012
average val loss: 0.054984, accuracy: 0.0553
average test loss: 0.041194, accuracy: 0.0413
case acc: 0.026233118
case acc: 0.072789244
case acc: 0.05380624
case acc: 0.031459745
case acc: 0.027809283
case acc: 0.035701632
top acc: 0.0154 ::: bot acc: 0.0534
top acc: 0.1304 ::: bot acc: 0.0172
top acc: 0.0577 ::: bot acc: 0.0820
top acc: 0.0401 ::: bot acc: 0.0413
top acc: 0.0534 ::: bot acc: 0.0098
top acc: 0.0490 ::: bot acc: 0.0456
current epoch: 17
train loss is 0.066789
average val loss: 0.054734, accuracy: 0.0550
average test loss: 0.040639, accuracy: 0.0407
case acc: 0.025733916
case acc: 0.07013273
case acc: 0.053363316
case acc: 0.031412266
case acc: 0.027499845
case acc: 0.035807025
top acc: 0.0176 ::: bot acc: 0.0513
top acc: 0.1261 ::: bot acc: 0.0176
top acc: 0.0595 ::: bot acc: 0.0801
top acc: 0.0419 ::: bot acc: 0.0395
top acc: 0.0528 ::: bot acc: 0.0102
top acc: 0.0494 ::: bot acc: 0.0453
current epoch: 18
train loss is 0.065026
average val loss: 0.054295, accuracy: 0.0545
average test loss: 0.040144, accuracy: 0.0400
case acc: 0.025343113
case acc: 0.067951806
case acc: 0.052971937
case acc: 0.031382
case acc: 0.026718365
case acc: 0.035663456
top acc: 0.0193 ::: bot acc: 0.0496
top acc: 0.1224 ::: bot acc: 0.0185
top acc: 0.0614 ::: bot acc: 0.0783
top acc: 0.0430 ::: bot acc: 0.0384
top acc: 0.0511 ::: bot acc: 0.0112
top acc: 0.0487 ::: bot acc: 0.0459
current epoch: 19
train loss is 0.064041
average val loss: 0.055335, accuracy: 0.0555
average test loss: 0.040168, accuracy: 0.0397
case acc: 0.02482829
case acc: 0.06751006
case acc: 0.052290972
case acc: 0.031333636
case acc: 0.026719222
case acc: 0.035767578
top acc: 0.0231 ::: bot acc: 0.0457
top acc: 0.1216 ::: bot acc: 0.0188
top acc: 0.0659 ::: bot acc: 0.0739
top acc: 0.0460 ::: bot acc: 0.0353
top acc: 0.0512 ::: bot acc: 0.0110
top acc: 0.0496 ::: bot acc: 0.0449
current epoch: 20
train loss is 0.063676
average val loss: 0.055360, accuracy: 0.0556
average test loss: 0.040005, accuracy: 0.0394
case acc: 0.024805734
case acc: 0.06656698
case acc: 0.051965475
case acc: 0.031343587
case acc: 0.025903717
case acc: 0.03556698
top acc: 0.0251 ::: bot acc: 0.0437
top acc: 0.1199 ::: bot acc: 0.0194
top acc: 0.0689 ::: bot acc: 0.0709
top acc: 0.0473 ::: bot acc: 0.0341
top acc: 0.0492 ::: bot acc: 0.0125
top acc: 0.0485 ::: bot acc: 0.0461
current epoch: 21
train loss is 0.063515
average val loss: 0.059744, accuracy: 0.0600
average test loss: 0.041529, accuracy: 0.0404
case acc: 0.025746107
case acc: 0.06943826
case acc: 0.051770616
case acc: 0.031604636
case acc: 0.02773186
case acc: 0.036319315
top acc: 0.0329 ::: bot acc: 0.0358
top acc: 0.1251 ::: bot acc: 0.0178
top acc: 0.0781 ::: bot acc: 0.0619
top acc: 0.0543 ::: bot acc: 0.0270
top acc: 0.0533 ::: bot acc: 0.0098
top acc: 0.0531 ::: bot acc: 0.0413
current epoch: 22
train loss is 0.064385
average val loss: 0.065425, accuracy: 0.0656
average test loss: 0.044194, accuracy: 0.0427
case acc: 0.028634986
case acc: 0.07350358
case acc: 0.052250907
case acc: 0.03283213
case acc: 0.030887881
case acc: 0.03782603
top acc: 0.0415 ::: bot acc: 0.0282
top acc: 0.1316 ::: bot acc: 0.0171
top acc: 0.0882 ::: bot acc: 0.0518
top acc: 0.0622 ::: bot acc: 0.0191
top acc: 0.0589 ::: bot acc: 0.0082
top acc: 0.0586 ::: bot acc: 0.0359
current epoch: 23
train loss is 0.065530
average val loss: 0.074299, accuracy: 0.0744
average test loss: 0.049505, accuracy: 0.0478
case acc: 0.03568481
case acc: 0.08066613
case acc: 0.054513555
case acc: 0.036958884
case acc: 0.037972763
case acc: 0.040899698
top acc: 0.0530 ::: bot acc: 0.0263
top acc: 0.1412 ::: bot acc: 0.0192
top acc: 0.1011 ::: bot acc: 0.0391
top acc: 0.0728 ::: bot acc: 0.0112
top acc: 0.0687 ::: bot acc: 0.0098
top acc: 0.0670 ::: bot acc: 0.0285
current epoch: 24
train loss is 0.068141
average val loss: 0.079798, accuracy: 0.0799
average test loss: 0.053257, accuracy: 0.0517
case acc: 0.040858638
case acc: 0.08455536
case acc: 0.05682762
case acc: 0.0410103
case acc: 0.04433063
case acc: 0.042907227
top acc: 0.0599 ::: bot acc: 0.0282
top acc: 0.1459 ::: bot acc: 0.0215
top acc: 0.1089 ::: bot acc: 0.0318
top acc: 0.0789 ::: bot acc: 0.0110
top acc: 0.0755 ::: bot acc: 0.0152
top acc: 0.0717 ::: bot acc: 0.0251
current epoch: 25
train loss is 0.069185
average val loss: 0.077615, accuracy: 0.0776
average test loss: 0.051655, accuracy: 0.0502
case acc: 0.03917105
case acc: 0.080424614
case acc: 0.05608504
case acc: 0.039548498
case acc: 0.043921735
case acc: 0.04184933
top acc: 0.0577 ::: bot acc: 0.0274
top acc: 0.1409 ::: bot acc: 0.0191
top acc: 0.1068 ::: bot acc: 0.0333
top acc: 0.0769 ::: bot acc: 0.0108
top acc: 0.0751 ::: bot acc: 0.0147
top acc: 0.0692 ::: bot acc: 0.0272
current epoch: 26
train loss is 0.068154
average val loss: 0.072371, accuracy: 0.0723
average test loss: 0.048036, accuracy: 0.0466
case acc: 0.034775116
case acc: 0.07350765
case acc: 0.054294735
case acc: 0.03627806
case acc: 0.041098595
case acc: 0.039830066
top acc: 0.0517 ::: bot acc: 0.0261
top acc: 0.1315 ::: bot acc: 0.0171
top acc: 0.1004 ::: bot acc: 0.0396
top acc: 0.0714 ::: bot acc: 0.0119
top acc: 0.0722 ::: bot acc: 0.0122
top acc: 0.0642 ::: bot acc: 0.0312
current epoch: 27
train loss is 0.066376
average val loss: 0.064148, accuracy: 0.0641
average test loss: 0.043060, accuracy: 0.0420
case acc: 0.02863184
case acc: 0.06543031
case acc: 0.05235956
case acc: 0.03293517
case acc: 0.035427704
case acc: 0.037345927
top acc: 0.0416 ::: bot acc: 0.0279
top acc: 0.1178 ::: bot acc: 0.0204
top acc: 0.0894 ::: bot acc: 0.0506
top acc: 0.0628 ::: bot acc: 0.0187
top acc: 0.0653 ::: bot acc: 0.0087
top acc: 0.0567 ::: bot acc: 0.0383
current epoch: 28
train loss is 0.062664
average val loss: 0.054505, accuracy: 0.0546
average test loss: 0.038732, accuracy: 0.0384
case acc: 0.025002439
case acc: 0.058082994
case acc: 0.051807154
case acc: 0.0314393
case acc: 0.028578596
case acc: 0.035432164
top acc: 0.0284 ::: bot acc: 0.0404
top acc: 0.1013 ::: bot acc: 0.0315
top acc: 0.0754 ::: bot acc: 0.0648
top acc: 0.0513 ::: bot acc: 0.0301
top acc: 0.0547 ::: bot acc: 0.0093
top acc: 0.0467 ::: bot acc: 0.0483
current epoch: 29
train loss is 0.059257
average val loss: 0.047676, accuracy: 0.0480
average test loss: 0.037206, accuracy: 0.0373
case acc: 0.025792941
case acc: 0.05422247
case acc: 0.052947033
case acc: 0.031425156
case acc: 0.024270914
case acc: 0.034927413
top acc: 0.0170 ::: bot acc: 0.0517
top acc: 0.0875 ::: bot acc: 0.0452
top acc: 0.0637 ::: bot acc: 0.0766
top acc: 0.0418 ::: bot acc: 0.0397
top acc: 0.0443 ::: bot acc: 0.0168
top acc: 0.0381 ::: bot acc: 0.0568
current epoch: 30
train loss is 0.057563
average val loss: 0.042702, accuracy: 0.0432
average test loss: 0.037693, accuracy: 0.0379
case acc: 0.02996693
case acc: 0.051839896
case acc: 0.05541994
case acc: 0.032313365
case acc: 0.022162147
case acc: 0.035902392
top acc: 0.0085 ::: bot acc: 0.0624
top acc: 0.0753 ::: bot acc: 0.0573
top acc: 0.0532 ::: bot acc: 0.0870
top acc: 0.0327 ::: bot acc: 0.0487
top acc: 0.0327 ::: bot acc: 0.0284
top acc: 0.0293 ::: bot acc: 0.0656
current epoch: 31
train loss is 0.057149
average val loss: 0.039162, accuracy: 0.0398
average test loss: 0.040737, accuracy: 0.0414
case acc: 0.03858338
case acc: 0.050552428
case acc: 0.05976635
case acc: 0.035118155
case acc: 0.025622476
case acc: 0.03892919
top acc: 0.0102 ::: bot acc: 0.0745
top acc: 0.0629 ::: bot acc: 0.0698
top acc: 0.0437 ::: bot acc: 0.0984
top acc: 0.0221 ::: bot acc: 0.0594
top acc: 0.0195 ::: bot acc: 0.0427
top acc: 0.0185 ::: bot acc: 0.0766
current epoch: 32
train loss is 0.057445
average val loss: 0.038293, accuracy: 0.0390
average test loss: 0.043828, accuracy: 0.0448
case acc: 0.044793535
case acc: 0.050429273
case acc: 0.06266672
case acc: 0.03746974
case acc: 0.031304486
case acc: 0.041836355
top acc: 0.0146 ::: bot acc: 0.0816
top acc: 0.0558 ::: bot acc: 0.0769
top acc: 0.0397 ::: bot acc: 0.1047
top acc: 0.0163 ::: bot acc: 0.0658
top acc: 0.0164 ::: bot acc: 0.0528
top acc: 0.0134 ::: bot acc: 0.0836
current epoch: 33
train loss is 0.057860
average val loss: 0.038291, accuracy: 0.0391
average test loss: 0.044563, accuracy: 0.0455
case acc: 0.045679577
case acc: 0.050437436
case acc: 0.06290636
case acc: 0.03787002
case acc: 0.033626497
case acc: 0.04263247
top acc: 0.0153 ::: bot acc: 0.0825
top acc: 0.0549 ::: bot acc: 0.0778
top acc: 0.0395 ::: bot acc: 0.1052
top acc: 0.0155 ::: bot acc: 0.0668
top acc: 0.0161 ::: bot acc: 0.0564
top acc: 0.0125 ::: bot acc: 0.0852
current epoch: 34
train loss is 0.057579
average val loss: 0.038372, accuracy: 0.0391
average test loss: 0.044132, accuracy: 0.0451
case acc: 0.044343736
case acc: 0.05043528
case acc: 0.062144324
case acc: 0.037416976
case acc: 0.034015153
case acc: 0.04226042
top acc: 0.0142 ::: bot acc: 0.0811
top acc: 0.0563 ::: bot acc: 0.0764
top acc: 0.0404 ::: bot acc: 0.1036
top acc: 0.0164 ::: bot acc: 0.0657
top acc: 0.0161 ::: bot acc: 0.0570
top acc: 0.0129 ::: bot acc: 0.0845
current epoch: 35
train loss is 0.057432
average val loss: 0.038612, accuracy: 0.0392
average test loss: 0.042826, accuracy: 0.0437
case acc: 0.04121876
case acc: 0.05046252
case acc: 0.060575277
case acc: 0.036380574
case acc: 0.03260557
case acc: 0.041074645
top acc: 0.0118 ::: bot acc: 0.0776
top acc: 0.0596 ::: bot acc: 0.0732
top acc: 0.0424 ::: bot acc: 0.1002
top acc: 0.0188 ::: bot acc: 0.0629
top acc: 0.0162 ::: bot acc: 0.0549
top acc: 0.0143 ::: bot acc: 0.0820
current epoch: 36
train loss is 0.057101
average val loss: 0.039000, accuracy: 0.0394
average test loss: 0.041655, accuracy: 0.0424
case acc: 0.038338337
case acc: 0.05058145
case acc: 0.059137404
case acc: 0.03543695
case acc: 0.031038424
case acc: 0.04012737
top acc: 0.0100 ::: bot acc: 0.0742
top acc: 0.0629 ::: bot acc: 0.0699
top acc: 0.0448 ::: bot acc: 0.0969
top acc: 0.0211 ::: bot acc: 0.0604
top acc: 0.0165 ::: bot acc: 0.0524
top acc: 0.0161 ::: bot acc: 0.0797
current epoch: 37
train loss is 0.057012
average val loss: 0.039532, accuracy: 0.0399
average test loss: 0.040689, accuracy: 0.0413
case acc: 0.035729244
case acc: 0.05078519
case acc: 0.05784151
case acc: 0.03460034
case acc: 0.02955601
case acc: 0.03937026
top acc: 0.0086 ::: bot acc: 0.0709
top acc: 0.0661 ::: bot acc: 0.0666
top acc: 0.0474 ::: bot acc: 0.0936
top acc: 0.0233 ::: bot acc: 0.0580
top acc: 0.0168 ::: bot acc: 0.0500
top acc: 0.0177 ::: bot acc: 0.0778
current epoch: 38
train loss is 0.056749
average val loss: 0.040455, accuracy: 0.0407
average test loss: 0.039567, accuracy: 0.0399
case acc: 0.03267765
case acc: 0.051208958
case acc: 0.056148447
case acc: 0.033617515
case acc: 0.027729083
case acc: 0.038302496
top acc: 0.0078 ::: bot acc: 0.0668
top acc: 0.0705 ::: bot acc: 0.0623
top acc: 0.0511 ::: bot acc: 0.0892
top acc: 0.0268 ::: bot acc: 0.0546
top acc: 0.0175 ::: bot acc: 0.0469
top acc: 0.0205 ::: bot acc: 0.0748
current epoch: 39
train loss is 0.056630
average val loss: 0.042321, accuracy: 0.0425
average test loss: 0.038351, accuracy: 0.0384
case acc: 0.029214479
case acc: 0.052162256
case acc: 0.054265488
case acc: 0.03233591
case acc: 0.02528229
case acc: 0.0369462
top acc: 0.0092 ::: bot acc: 0.0608
top acc: 0.0770 ::: bot acc: 0.0558
top acc: 0.0575 ::: bot acc: 0.0827
top acc: 0.0321 ::: bot acc: 0.0492
top acc: 0.0200 ::: bot acc: 0.0420
top acc: 0.0251 ::: bot acc: 0.0700
current epoch: 40
train loss is 0.056709
average val loss: 0.044619, accuracy: 0.0448
average test loss: 0.037743, accuracy: 0.0374
case acc: 0.02691551
case acc: 0.05343203
case acc: 0.052914523
case acc: 0.031669285
case acc: 0.02345758
case acc: 0.03593643
top acc: 0.0136 ::: bot acc: 0.0552
top acc: 0.0836 ::: bot acc: 0.0492
top acc: 0.0640 ::: bot acc: 0.0763
top acc: 0.0373 ::: bot acc: 0.0440
top acc: 0.0235 ::: bot acc: 0.0375
top acc: 0.0295 ::: bot acc: 0.0655
current epoch: 41
train loss is 0.057057
average val loss: 0.047492, accuracy: 0.0476
average test loss: 0.037697, accuracy: 0.0369
case acc: 0.025388569
case acc: 0.05499331
case acc: 0.052077234
case acc: 0.031344738
case acc: 0.02238909
case acc: 0.035148095
top acc: 0.0193 ::: bot acc: 0.0495
top acc: 0.0907 ::: bot acc: 0.0421
top acc: 0.0711 ::: bot acc: 0.0693
top acc: 0.0426 ::: bot acc: 0.0387
top acc: 0.0279 ::: bot acc: 0.0330
top acc: 0.0338 ::: bot acc: 0.0613
current epoch: 42
train loss is 0.057610
average val loss: 0.052483, accuracy: 0.0526
average test loss: 0.038667, accuracy: 0.0372
case acc: 0.024903204
case acc: 0.0579084
case acc: 0.051907733
case acc: 0.031378545
case acc: 0.022266831
case acc: 0.034865472
top acc: 0.0277 ::: bot acc: 0.0411
top acc: 0.1007 ::: bot acc: 0.0321
top acc: 0.0812 ::: bot acc: 0.0592
top acc: 0.0505 ::: bot acc: 0.0308
top acc: 0.0348 ::: bot acc: 0.0262
top acc: 0.0401 ::: bot acc: 0.0548
current epoch: 43
train loss is 0.058704
average val loss: 0.059547, accuracy: 0.0597
average test loss: 0.041428, accuracy: 0.0391
case acc: 0.027027585
case acc: 0.06291936
case acc: 0.052969124
case acc: 0.032377012
case acc: 0.023938911
case acc: 0.035456236
top acc: 0.0378 ::: bot acc: 0.0311
top acc: 0.1130 ::: bot acc: 0.0226
top acc: 0.0935 ::: bot acc: 0.0469
top acc: 0.0599 ::: bot acc: 0.0214
top acc: 0.0433 ::: bot acc: 0.0177
top acc: 0.0475 ::: bot acc: 0.0473
current epoch: 44
train loss is 0.060501
average val loss: 0.072467, accuracy: 0.0726
average test loss: 0.048541, accuracy: 0.0458
case acc: 0.03599352
case acc: 0.07332932
case acc: 0.058143165
case acc: 0.038325004
case acc: 0.0304574
case acc: 0.038430322
top acc: 0.0537 ::: bot acc: 0.0261
top acc: 0.1313 ::: bot acc: 0.0170
top acc: 0.1120 ::: bot acc: 0.0295
top acc: 0.0750 ::: bot acc: 0.0105
top acc: 0.0580 ::: bot acc: 0.0083
top acc: 0.0603 ::: bot acc: 0.0346
current epoch: 45
train loss is 0.064422
average val loss: 0.088497, accuracy: 0.0884
average test loss: 0.060011, accuracy: 0.0581
case acc: 0.0499396
case acc: 0.08902796
case acc: 0.06913958
case acc: 0.051444083
case acc: 0.044657655
case acc: 0.044515476
top acc: 0.0714 ::: bot acc: 0.0325
top acc: 0.1511 ::: bot acc: 0.0245
top acc: 0.1323 ::: bot acc: 0.0217
top acc: 0.0919 ::: bot acc: 0.0162
top acc: 0.0758 ::: bot acc: 0.0153
top acc: 0.0748 ::: bot acc: 0.0239
current epoch: 46
train loss is 0.070060
average val loss: 0.102123, accuracy: 0.1019
average test loss: 0.071032, accuracy: 0.0698
case acc: 0.061188143
case acc: 0.10200128
case acc: 0.08026336
case acc: 0.063194536
case acc: 0.06138926
case acc: 0.050663505
top acc: 0.0854 ::: bot acc: 0.0381
top acc: 0.1657 ::: bot acc: 0.0342
top acc: 0.1482 ::: bot acc: 0.0234
top acc: 0.1050 ::: bot acc: 0.0252
top acc: 0.0928 ::: bot acc: 0.0316
top acc: 0.0865 ::: bot acc: 0.0191
current epoch: 47
train loss is 0.079030
average val loss: 0.098826, accuracy: 0.0987
average test loss: 0.068163, accuracy: 0.0671
case acc: 0.05804215
case acc: 0.09541875
case acc: 0.0768569
case acc: 0.059679348
case acc: 0.0639265
case acc: 0.04875217
top acc: 0.0815 ::: bot acc: 0.0365
top acc: 0.1584 ::: bot acc: 0.0290
top acc: 0.1437 ::: bot acc: 0.0222
top acc: 0.1012 ::: bot acc: 0.0224
top acc: 0.0953 ::: bot acc: 0.0342
top acc: 0.0831 ::: bot acc: 0.0200
current epoch: 48
train loss is 0.080657
average val loss: 0.072428, accuracy: 0.0723
average test loss: 0.048141, accuracy: 0.0465
case acc: 0.03436825
case acc: 0.067512825
case acc: 0.057302747
case acc: 0.03763334
case acc: 0.04367113
case acc: 0.038446113
top acc: 0.0512 ::: bot acc: 0.0258
top acc: 0.1217 ::: bot acc: 0.0188
top acc: 0.1101 ::: bot acc: 0.0306
top acc: 0.0740 ::: bot acc: 0.0109
top acc: 0.0748 ::: bot acc: 0.0145
top acc: 0.0602 ::: bot acc: 0.0351
current epoch: 49
train loss is 0.070022
average val loss: 0.052789, accuracy: 0.0531
average test loss: 0.038222, accuracy: 0.0380
case acc: 0.024866067
case acc: 0.05425709
case acc: 0.051756695
case acc: 0.03150032
case acc: 0.030249417
case acc: 0.035141867
top acc: 0.0241 ::: bot acc: 0.0449
top acc: 0.0875 ::: bot acc: 0.0451
top acc: 0.0782 ::: bot acc: 0.0622
top acc: 0.0516 ::: bot acc: 0.0300
top acc: 0.0576 ::: bot acc: 0.0087
top acc: 0.0438 ::: bot acc: 0.0512
current epoch: 50
train loss is 0.059808
average val loss: 0.039279, accuracy: 0.0402
average test loss: 0.041379, accuracy: 0.0415
case acc: 0.04220013
case acc: 0.050626032
case acc: 0.06061511
case acc: 0.034822974
case acc: 0.022219956
case acc: 0.038269
top acc: 0.0125 ::: bot acc: 0.0788
top acc: 0.0480 ::: bot acc: 0.0846
top acc: 0.0426 ::: bot acc: 0.1003
top acc: 0.0226 ::: bot acc: 0.0589
top acc: 0.0305 ::: bot acc: 0.0306
top acc: 0.0202 ::: bot acc: 0.0747
LME_Co_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2010-07-01', '2011-01-01', '2015-01-01', '2015-07-01', '2016-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5352 5352 5352
1.7082474 -0.6288155 0.29835227 -0.25048217
Validation: 600 600 600
Testing: 744 744 744
pre-processing time: 0.0005071163177490234
the split date is 2011-01-01
net initializing with time: 0.0037429332733154297
preparing training and testing date with time: 4.76837158203125e-07
current epoch: 1
train loss is 0.167073
average val loss: 0.081461, accuracy: 0.0814
average test loss: 0.090881, accuracy: 0.0858
case acc: 0.058340285
case acc: 0.04502758
case acc: 0.048782375
case acc: 0.2371919
case acc: 0.060662463
case acc: 0.06493585
top acc: 0.1016 ::: bot acc: 0.0288
top acc: 0.0239 ::: bot acc: 0.0792
top acc: 0.0694 ::: bot acc: 0.0599
top acc: 0.2772 ::: bot acc: 0.2006
top acc: 0.0479 ::: bot acc: 0.0976
top acc: 0.0649 ::: bot acc: 0.0927
current epoch: 2
train loss is 0.134872
average val loss: 0.108291, accuracy: 0.1074
average test loss: 0.111867, accuracy: 0.1040
case acc: 0.05855734
case acc: 0.10524241
case acc: 0.103357755
case acc: 0.14191705
case acc: 0.10713876
case acc: 0.10798011
top acc: 0.0179 ::: bot acc: 0.1148
top acc: 0.0558 ::: bot acc: 0.1528
top acc: 0.0520 ::: bot acc: 0.1528
top acc: 0.1831 ::: bot acc: 0.1044
top acc: 0.0381 ::: bot acc: 0.1717
top acc: 0.0447 ::: bot acc: 0.1676
current epoch: 3
train loss is 0.124344
average val loss: 0.093883, accuracy: 0.0929
average test loss: 0.099434, accuracy: 0.0928
case acc: 0.052088037
case acc: 0.0839922
case acc: 0.096142255
case acc: 0.14854808
case acc: 0.08694257
case acc: 0.08932937
top acc: 0.0279 ::: bot acc: 0.0997
top acc: 0.0361 ::: bot acc: 0.1304
top acc: 0.0483 ::: bot acc: 0.1440
top acc: 0.1906 ::: bot acc: 0.1105
top acc: 0.0325 ::: bot acc: 0.1440
top acc: 0.0429 ::: bot acc: 0.1407
current epoch: 4
train loss is 0.107619
average val loss: 0.085673, accuracy: 0.0843
average test loss: 0.092460, accuracy: 0.0868
case acc: 0.05056872
case acc: 0.071628205
case acc: 0.09660817
case acc: 0.14548148
case acc: 0.07667334
case acc: 0.0796682
top acc: 0.0325 ::: bot acc: 0.0942
top acc: 0.0265 ::: bot acc: 0.1165
top acc: 0.0487 ::: bot acc: 0.1448
top acc: 0.1880 ::: bot acc: 0.1070
top acc: 0.0353 ::: bot acc: 0.1271
top acc: 0.0464 ::: bot acc: 0.1247
current epoch: 5
train loss is 0.099442
average val loss: 0.082966, accuracy: 0.0815
average test loss: 0.089860, accuracy: 0.0843
case acc: 0.05166199
case acc: 0.067216046
case acc: 0.10408358
case acc: 0.13273892
case acc: 0.07333641
case acc: 0.07694736
top acc: 0.0283 ::: bot acc: 0.0987
top acc: 0.0237 ::: bot acc: 0.1112
top acc: 0.0526 ::: bot acc: 0.1541
top acc: 0.1755 ::: bot acc: 0.0940
top acc: 0.0372 ::: bot acc: 0.1210
top acc: 0.0489 ::: bot acc: 0.1194
current epoch: 6
train loss is 0.096976
average val loss: 0.082802, accuracy: 0.0813
average test loss: 0.089270, accuracy: 0.0836
case acc: 0.05479403
case acc: 0.066404946
case acc: 0.11412004
case acc: 0.11613307
case acc: 0.072934076
case acc: 0.07695795
top acc: 0.0214 ::: bot acc: 0.1068
top acc: 0.0233 ::: bot acc: 0.1101
top acc: 0.0578 ::: bot acc: 0.1666
top acc: 0.1593 ::: bot acc: 0.0771
top acc: 0.0375 ::: bot acc: 0.1201
top acc: 0.0491 ::: bot acc: 0.1194
current epoch: 7
train loss is 0.095314
average val loss: 0.088572, accuracy: 0.0871
average test loss: 0.093313, accuracy: 0.0869
case acc: 0.06387856
case acc: 0.07463116
case acc: 0.13117537
case acc: 0.09115465
case acc: 0.07821383
case acc: 0.0822794
top acc: 0.0148 ::: bot acc: 0.1235
top acc: 0.0293 ::: bot acc: 0.1193
top acc: 0.0673 ::: bot acc: 0.1876
top acc: 0.1343 ::: bot acc: 0.0525
top acc: 0.0345 ::: bot acc: 0.1294
top acc: 0.0452 ::: bot acc: 0.1294
current epoch: 8
train loss is 0.096911
average val loss: 0.094147, accuracy: 0.0927
average test loss: 0.097608, accuracy: 0.0908
case acc: 0.07461792
case acc: 0.08222995
case acc: 0.14696838
case acc: 0.06904682
case acc: 0.083588645
case acc: 0.08821941
top acc: 0.0177 ::: bot acc: 0.1380
top acc: 0.0356 ::: bot acc: 0.1275
top acc: 0.0780 ::: bot acc: 0.2060
top acc: 0.1111 ::: bot acc: 0.0328
top acc: 0.0320 ::: bot acc: 0.1386
top acc: 0.0433 ::: bot acc: 0.1392
current epoch: 9
train loss is 0.100448
average val loss: 0.093843, accuracy: 0.0924
average test loss: 0.097162, accuracy: 0.0902
case acc: 0.07801322
case acc: 0.079202674
case acc: 0.15290977
case acc: 0.05747794
case acc: 0.084294565
case acc: 0.089406855
top acc: 0.0199 ::: bot acc: 0.1420
top acc: 0.0332 ::: bot acc: 0.1241
top acc: 0.0823 ::: bot acc: 0.2128
top acc: 0.0982 ::: bot acc: 0.0239
top acc: 0.0318 ::: bot acc: 0.1397
top acc: 0.0433 ::: bot acc: 0.1410
current epoch: 10
train loss is 0.104110
average val loss: 0.083283, accuracy: 0.0821
average test loss: 0.088263, accuracy: 0.0816
case acc: 0.06765811
case acc: 0.05986486
case acc: 0.14206597
case acc: 0.06104369
case acc: 0.07682787
case acc: 0.082084805
top acc: 0.0149 ::: bot acc: 0.1290
top acc: 0.0199 ::: bot acc: 0.1018
top acc: 0.0743 ::: bot acc: 0.2006
top acc: 0.1022 ::: bot acc: 0.0266
top acc: 0.0353 ::: bot acc: 0.1268
top acc: 0.0457 ::: bot acc: 0.1288
current epoch: 11
train loss is 0.098101
average val loss: 0.070327, accuracy: 0.0704
average test loss: 0.078011, accuracy: 0.0724
case acc: 0.056148347
case acc: 0.041435488
case acc: 0.12454038
case acc: 0.07096423
case acc: 0.06759898
case acc: 0.073617175
top acc: 0.0200 ::: bot acc: 0.1092
top acc: 0.0242 ::: bot acc: 0.0722
top acc: 0.0636 ::: bot acc: 0.1797
top acc: 0.1131 ::: bot acc: 0.0344
top acc: 0.0417 ::: bot acc: 0.1098
top acc: 0.0524 ::: bot acc: 0.1128
current epoch: 12
train loss is 0.085922
average val loss: 0.063954, accuracy: 0.0645
average test loss: 0.072847, accuracy: 0.0681
case acc: 0.051476926
case acc: 0.035257116
case acc: 0.11374693
case acc: 0.07394545
case acc: 0.06381793
case acc: 0.0701519
top acc: 0.0286 ::: bot acc: 0.0980
top acc: 0.0413 ::: bot acc: 0.0527
top acc: 0.0578 ::: bot acc: 0.1665
top acc: 0.1164 ::: bot acc: 0.0368
top acc: 0.0469 ::: bot acc: 0.1015
top acc: 0.0570 ::: bot acc: 0.1054
current epoch: 13
train loss is 0.077481
average val loss: 0.059779, accuracy: 0.0602
average test loss: 0.069345, accuracy: 0.0655
case acc: 0.049511805
case acc: 0.034738477
case acc: 0.10560967
case acc: 0.07480645
case acc: 0.06110836
case acc: 0.067418434
top acc: 0.0368 ::: bot acc: 0.0897
top acc: 0.0530 ::: bot acc: 0.0409
top acc: 0.0536 ::: bot acc: 0.1564
top acc: 0.1174 ::: bot acc: 0.0375
top acc: 0.0528 ::: bot acc: 0.0945
top acc: 0.0622 ::: bot acc: 0.0987
current epoch: 14
train loss is 0.074658
average val loss: 0.056319, accuracy: 0.0563
average test loss: 0.066476, accuracy: 0.0635
case acc: 0.048713498
case acc: 0.03515974
case acc: 0.0991312
case acc: 0.07470324
case acc: 0.05839575
case acc: 0.06468838
top acc: 0.0433 ::: bot acc: 0.0832
top acc: 0.0584 ::: bot acc: 0.0356
top acc: 0.0503 ::: bot acc: 0.1484
top acc: 0.1174 ::: bot acc: 0.0373
top acc: 0.0599 ::: bot acc: 0.0868
top acc: 0.0696 ::: bot acc: 0.0908
current epoch: 15
train loss is 0.073602
average val loss: 0.056075, accuracy: 0.0560
average test loss: 0.066118, accuracy: 0.0625
case acc: 0.04927142
case acc: 0.034717616
case acc: 0.102787755
case acc: 0.06408494
case acc: 0.059133288
case acc: 0.06516631
top acc: 0.0377 ::: bot acc: 0.0887
top acc: 0.0479 ::: bot acc: 0.0461
top acc: 0.0522 ::: bot acc: 0.1530
top acc: 0.1059 ::: bot acc: 0.0287
top acc: 0.0578 ::: bot acc: 0.0890
top acc: 0.0683 ::: bot acc: 0.0922
current epoch: 16
train loss is 0.074059
average val loss: 0.059130, accuracy: 0.0584
average test loss: 0.068275, accuracy: 0.0636
case acc: 0.052608233
case acc: 0.03852317
case acc: 0.1127266
case acc: 0.047907785
case acc: 0.06202
case acc: 0.06769493
top acc: 0.0256 ::: bot acc: 0.1012
top acc: 0.0294 ::: bot acc: 0.0652
top acc: 0.0573 ::: bot acc: 0.1653
top acc: 0.0874 ::: bot acc: 0.0172
top acc: 0.0507 ::: bot acc: 0.0968
top acc: 0.0618 ::: bot acc: 0.0992
current epoch: 17
train loss is 0.074344
average val loss: 0.067924, accuracy: 0.0662
average test loss: 0.075325, accuracy: 0.0696
case acc: 0.062056713
case acc: 0.051702473
case acc: 0.12886117
case acc: 0.032417405
case acc: 0.06853072
case acc: 0.07395966
top acc: 0.0153 ::: bot acc: 0.1204
top acc: 0.0183 ::: bot acc: 0.0904
top acc: 0.0659 ::: bot acc: 0.1853
top acc: 0.0620 ::: bot acc: 0.0215
top acc: 0.0409 ::: bot acc: 0.1113
top acc: 0.0523 ::: bot acc: 0.1133
current epoch: 18
train loss is 0.079925
average val loss: 0.079484, accuracy: 0.0781
average test loss: 0.085418, accuracy: 0.0800
case acc: 0.07535547
case acc: 0.067562334
case acc: 0.14595279
case acc: 0.03188761
case acc: 0.076968886
case acc: 0.082184985
top acc: 0.0177 ::: bot acc: 0.1391
top acc: 0.0244 ::: bot acc: 0.1110
top acc: 0.0772 ::: bot acc: 0.2053
top acc: 0.0369 ::: bot acc: 0.0472
top acc: 0.0352 ::: bot acc: 0.1267
top acc: 0.0456 ::: bot acc: 0.1289
current epoch: 19
train loss is 0.089527
average val loss: 0.073060, accuracy: 0.0718
average test loss: 0.080105, accuracy: 0.0749
case acc: 0.06937584
case acc: 0.057679188
case acc: 0.13972126
case acc: 0.03185311
case acc: 0.07236349
case acc: 0.07813858
top acc: 0.0150 ::: bot acc: 0.1315
top acc: 0.0192 ::: bot acc: 0.0988
top acc: 0.0726 ::: bot acc: 0.1983
top acc: 0.0369 ::: bot acc: 0.0472
top acc: 0.0380 ::: bot acc: 0.1184
top acc: 0.0483 ::: bot acc: 0.1215
current epoch: 20
train loss is 0.088158
average val loss: 0.057355, accuracy: 0.0565
average test loss: 0.066954, accuracy: 0.0624
case acc: 0.0553075
case acc: 0.038810086
case acc: 0.11888287
case acc: 0.030594269
case acc: 0.0621582
case acc: 0.068621375
top acc: 0.0211 ::: bot acc: 0.1075
top acc: 0.0287 ::: bot acc: 0.0660
top acc: 0.0605 ::: bot acc: 0.1731
top acc: 0.0530 ::: bot acc: 0.0304
top acc: 0.0505 ::: bot acc: 0.0970
top acc: 0.0599 ::: bot acc: 0.1016
current epoch: 21
train loss is 0.075123
average val loss: 0.052368, accuracy: 0.0520
average test loss: 0.062660, accuracy: 0.0588
case acc: 0.051088903
case acc: 0.03474774
case acc: 0.10914454
case acc: 0.031230412
case acc: 0.059799146
case acc: 0.0666888
top acc: 0.0294 ::: bot acc: 0.0970
top acc: 0.0469 ::: bot acc: 0.0472
top acc: 0.0555 ::: bot acc: 0.1611
top acc: 0.0567 ::: bot acc: 0.0269
top acc: 0.0561 ::: bot acc: 0.0907
top acc: 0.0638 ::: bot acc: 0.0968
current epoch: 22
train loss is 0.066908
average val loss: 0.050158, accuracy: 0.0499
average test loss: 0.060698, accuracy: 0.0574
case acc: 0.049587842
case acc: 0.0349487
case acc: 0.103234224
case acc: 0.03118052
case acc: 0.058999043
case acc: 0.066217184
top acc: 0.0347 ::: bot acc: 0.0915
top acc: 0.0561 ::: bot acc: 0.0379
top acc: 0.0524 ::: bot acc: 0.1537
top acc: 0.0565 ::: bot acc: 0.0273
top acc: 0.0584 ::: bot acc: 0.0884
top acc: 0.0654 ::: bot acc: 0.0953
current epoch: 23
train loss is 0.063202
average val loss: 0.048227, accuracy: 0.0478
average test loss: 0.059094, accuracy: 0.0562
case acc: 0.048925005
case acc: 0.03524248
case acc: 0.098407604
case acc: 0.031082755
case acc: 0.058063895
case acc: 0.06529724
top acc: 0.0389 ::: bot acc: 0.0873
top acc: 0.0596 ::: bot acc: 0.0344
top acc: 0.0499 ::: bot acc: 0.1478
top acc: 0.0559 ::: bot acc: 0.0280
top acc: 0.0612 ::: bot acc: 0.0854
top acc: 0.0682 ::: bot acc: 0.0924
current epoch: 24
train loss is 0.061994
average val loss: 0.047681, accuracy: 0.0473
average test loss: 0.058863, accuracy: 0.0559
case acc: 0.048922505
case acc: 0.034803018
case acc: 0.097735986
case acc: 0.030479494
case acc: 0.058227018
case acc: 0.06532634
top acc: 0.0383 ::: bot acc: 0.0878
top acc: 0.0550 ::: bot acc: 0.0390
top acc: 0.0495 ::: bot acc: 0.1470
top acc: 0.0510 ::: bot acc: 0.0330
top acc: 0.0610 ::: bot acc: 0.0857
top acc: 0.0684 ::: bot acc: 0.0923
current epoch: 25
train loss is 0.061426
average val loss: 0.048922, accuracy: 0.0488
average test loss: 0.060195, accuracy: 0.0570
case acc: 0.049673814
case acc: 0.034860015
case acc: 0.10087289
case acc: 0.030577147
case acc: 0.059451368
case acc: 0.066323355
top acc: 0.0334 ::: bot acc: 0.0926
top acc: 0.0445 ::: bot acc: 0.0495
top acc: 0.0511 ::: bot acc: 0.1509
top acc: 0.0419 ::: bot acc: 0.0421
top acc: 0.0576 ::: bot acc: 0.0893
top acc: 0.0654 ::: bot acc: 0.0953
current epoch: 26
train loss is 0.061959
average val loss: 0.053162, accuracy: 0.0535
average test loss: 0.064001, accuracy: 0.0609
case acc: 0.05274396
case acc: 0.038019646
case acc: 0.10789542
case acc: 0.035687894
case acc: 0.062168285
case acc: 0.068934195
top acc: 0.0246 ::: bot acc: 0.1019
top acc: 0.0306 ::: bot acc: 0.0637
top acc: 0.0547 ::: bot acc: 0.1597
top acc: 0.0313 ::: bot acc: 0.0559
top acc: 0.0509 ::: bot acc: 0.0967
top acc: 0.0592 ::: bot acc: 0.1023
current epoch: 27
train loss is 0.063313
average val loss: 0.057999, accuracy: 0.0583
average test loss: 0.068197, accuracy: 0.0656
case acc: 0.05638008
case acc: 0.042758923
case acc: 0.11437545
case acc: 0.043487806
case acc: 0.06477016
case acc: 0.071692415
top acc: 0.0190 ::: bot acc: 0.1101
top acc: 0.0227 ::: bot acc: 0.0747
top acc: 0.0580 ::: bot acc: 0.1678
top acc: 0.0284 ::: bot acc: 0.0689
top acc: 0.0457 ::: bot acc: 0.1031
top acc: 0.0546 ::: bot acc: 0.1087
current epoch: 28
train loss is 0.066076
average val loss: 0.062324, accuracy: 0.0626
average test loss: 0.071797, accuracy: 0.0696
case acc: 0.059698705
case acc: 0.045757562
case acc: 0.11934317
case acc: 0.05158807
case acc: 0.06709535
case acc: 0.074383326
top acc: 0.0165 ::: bot acc: 0.1162
top acc: 0.0198 ::: bot acc: 0.0805
top acc: 0.0606 ::: bot acc: 0.1740
top acc: 0.0297 ::: bot acc: 0.0804
top acc: 0.0425 ::: bot acc: 0.1082
top acc: 0.0515 ::: bot acc: 0.1142
current epoch: 29
train loss is 0.068270
average val loss: 0.060307, accuracy: 0.0609
average test loss: 0.070198, accuracy: 0.0684
case acc: 0.058169074
case acc: 0.042638727
case acc: 0.11681112
case acc: 0.053519413
case acc: 0.06579774
case acc: 0.07339473
top acc: 0.0176 ::: bot acc: 0.1134
top acc: 0.0229 ::: bot acc: 0.0744
top acc: 0.0593 ::: bot acc: 0.1708
top acc: 0.0304 ::: bot acc: 0.0830
top acc: 0.0441 ::: bot acc: 0.1054
top acc: 0.0524 ::: bot acc: 0.1123
current epoch: 30
train loss is 0.067669
average val loss: 0.054220, accuracy: 0.0553
average test loss: 0.065214, accuracy: 0.0637
case acc: 0.05371726
case acc: 0.036887232
case acc: 0.10854373
case acc: 0.050253034
case acc: 0.062511496
case acc: 0.07020928
top acc: 0.0233 ::: bot acc: 0.1040
top acc: 0.0342 ::: bot acc: 0.0601
top acc: 0.0551 ::: bot acc: 0.1606
top acc: 0.0292 ::: bot acc: 0.0787
top acc: 0.0499 ::: bot acc: 0.0977
top acc: 0.0564 ::: bot acc: 0.1056
current epoch: 31
train loss is 0.063919
average val loss: 0.047677, accuracy: 0.0488
average test loss: 0.059747, accuracy: 0.0586
case acc: 0.049617257
case acc: 0.03473024
case acc: 0.0973793
case acc: 0.04446717
case acc: 0.058888476
case acc: 0.066694535
top acc: 0.0348 ::: bot acc: 0.0915
top acc: 0.0511 ::: bot acc: 0.0432
top acc: 0.0493 ::: bot acc: 0.1468
top acc: 0.0281 ::: bot acc: 0.0706
top acc: 0.0585 ::: bot acc: 0.0880
top acc: 0.0636 ::: bot acc: 0.0968
current epoch: 32
train loss is 0.059434
average val loss: 0.043464, accuracy: 0.0441
average test loss: 0.056010, accuracy: 0.0555
case acc: 0.048527967
case acc: 0.036082342
case acc: 0.08738793
case acc: 0.039819017
case acc: 0.0568321
case acc: 0.06428831
top acc: 0.0456 ::: bot acc: 0.0808
top acc: 0.0641 ::: bot acc: 0.0303
top acc: 0.0446 ::: bot acc: 0.1342
top acc: 0.0291 ::: bot acc: 0.0631
top acc: 0.0661 ::: bot acc: 0.0803
top acc: 0.0707 ::: bot acc: 0.0895
current epoch: 33
train loss is 0.056755
average val loss: 0.040493, accuracy: 0.0406
average test loss: 0.053171, accuracy: 0.0531
case acc: 0.048407987
case acc: 0.0391177
case acc: 0.07810976
case acc: 0.035167806
case acc: 0.055476174
case acc: 0.062022742
top acc: 0.0559 ::: bot acc: 0.0705
top acc: 0.0735 ::: bot acc: 0.0218
top acc: 0.0418 ::: bot acc: 0.1217
top acc: 0.0315 ::: bot acc: 0.0550
top acc: 0.0736 ::: bot acc: 0.0728
top acc: 0.0786 ::: bot acc: 0.0817
current epoch: 34
train loss is 0.055504
average val loss: 0.038752, accuracy: 0.0384
average test loss: 0.051286, accuracy: 0.0513
case acc: 0.0489211
case acc: 0.04190541
case acc: 0.07052527
case acc: 0.03172113
case acc: 0.05463454
case acc: 0.06025211
top acc: 0.0652 ::: bot acc: 0.0612
top acc: 0.0792 ::: bot acc: 0.0188
top acc: 0.0423 ::: bot acc: 0.1101
top acc: 0.0372 ::: bot acc: 0.0469
top acc: 0.0807 ::: bot acc: 0.0658
top acc: 0.0864 ::: bot acc: 0.0740
current epoch: 35
train loss is 0.054956
average val loss: 0.038017, accuracy: 0.0375
average test loss: 0.050435, accuracy: 0.0504
case acc: 0.049347945
case acc: 0.04180842
case acc: 0.06648831
case acc: 0.030687
case acc: 0.054438144
case acc: 0.059526574
top acc: 0.0700 ::: bot acc: 0.0563
top acc: 0.0790 ::: bot acc: 0.0188
top acc: 0.0446 ::: bot acc: 0.1030
top acc: 0.0416 ::: bot acc: 0.0426
top acc: 0.0839 ::: bot acc: 0.0626
top acc: 0.0908 ::: bot acc: 0.0696
current epoch: 36
train loss is 0.054574
average val loss: 0.037522, accuracy: 0.0370
average test loss: 0.049965, accuracy: 0.0497
case acc: 0.049508844
case acc: 0.040239017
case acc: 0.06426289
case acc: 0.030357836
case acc: 0.054387912
case acc: 0.059328012
top acc: 0.0719 ::: bot acc: 0.0543
top acc: 0.0759 ::: bot acc: 0.0203
top acc: 0.0463 ::: bot acc: 0.0988
top acc: 0.0435 ::: bot acc: 0.0408
top acc: 0.0849 ::: bot acc: 0.0616
top acc: 0.0931 ::: bot acc: 0.0673
current epoch: 37
train loss is 0.054199
average val loss: 0.037171, accuracy: 0.0367
average test loss: 0.049919, accuracy: 0.0493
case acc: 0.049337257
case acc: 0.037529
case acc: 0.064178064
case acc: 0.030757565
case acc: 0.054522425
case acc: 0.05938713
top acc: 0.0701 ::: bot acc: 0.0561
top acc: 0.0696 ::: bot acc: 0.0249
top acc: 0.0463 ::: bot acc: 0.0987
top acc: 0.0415 ::: bot acc: 0.0428
top acc: 0.0829 ::: bot acc: 0.0636
top acc: 0.0923 ::: bot acc: 0.0681
current epoch: 38
train loss is 0.053726
average val loss: 0.037404, accuracy: 0.0373
average test loss: 0.050672, accuracy: 0.0497
case acc: 0.04877823
case acc: 0.035469767
case acc: 0.06645401
case acc: 0.032593187
case acc: 0.05496319
case acc: 0.059953112
top acc: 0.0642 ::: bot acc: 0.0621
top acc: 0.0608 ::: bot acc: 0.0338
top acc: 0.0446 ::: bot acc: 0.1030
top acc: 0.0352 ::: bot acc: 0.0493
top acc: 0.0775 ::: bot acc: 0.0690
top acc: 0.0878 ::: bot acc: 0.0725
current epoch: 39
train loss is 0.053544
average val loss: 0.038568, accuracy: 0.0388
average test loss: 0.052194, accuracy: 0.0512
case acc: 0.048372526
case acc: 0.03481028
case acc: 0.069898635
case acc: 0.036858033
case acc: 0.05592365
case acc: 0.06113458
top acc: 0.0569 ::: bot acc: 0.0694
top acc: 0.0520 ::: bot acc: 0.0426
top acc: 0.0427 ::: bot acc: 0.1091
top acc: 0.0306 ::: bot acc: 0.0579
top acc: 0.0714 ::: bot acc: 0.0752
top acc: 0.0823 ::: bot acc: 0.0781
current epoch: 40
train loss is 0.053450
average val loss: 0.040842, accuracy: 0.0415
average test loss: 0.054433, accuracy: 0.0535
case acc: 0.04837026
case acc: 0.035155926
case acc: 0.07444505
case acc: 0.042909317
case acc: 0.05720795
case acc: 0.06276701
top acc: 0.0488 ::: bot acc: 0.0774
top acc: 0.0435 ::: bot acc: 0.0511
top acc: 0.0416 ::: bot acc: 0.1164
top acc: 0.0285 ::: bot acc: 0.0680
top acc: 0.0650 ::: bot acc: 0.0816
top acc: 0.0762 ::: bot acc: 0.0842
current epoch: 41
train loss is 0.053848
average val loss: 0.044136, accuracy: 0.0449
average test loss: 0.057257, accuracy: 0.0565
case acc: 0.048673436
case acc: 0.036668885
case acc: 0.07984649
case acc: 0.05035521
case acc: 0.05882383
case acc: 0.064614624
top acc: 0.0408 ::: bot acc: 0.0854
top acc: 0.0358 ::: bot acc: 0.0588
top acc: 0.0421 ::: bot acc: 0.1243
top acc: 0.0293 ::: bot acc: 0.0788
top acc: 0.0589 ::: bot acc: 0.0876
top acc: 0.0700 ::: bot acc: 0.0903
current epoch: 42
train loss is 0.054586
average val loss: 0.048558, accuracy: 0.0492
average test loss: 0.060818, accuracy: 0.0604
case acc: 0.04999746
case acc: 0.03908727
case acc: 0.08609194
case acc: 0.05930193
case acc: 0.061040033
case acc: 0.06669765
top acc: 0.0327 ::: bot acc: 0.0936
top acc: 0.0286 ::: bot acc: 0.0662
top acc: 0.0441 ::: bot acc: 0.1326
top acc: 0.0332 ::: bot acc: 0.0902
top acc: 0.0532 ::: bot acc: 0.0937
top acc: 0.0637 ::: bot acc: 0.0967
current epoch: 43
train loss is 0.055457
average val loss: 0.052330, accuracy: 0.0529
average test loss: 0.063790, accuracy: 0.0637
case acc: 0.051787328
case acc: 0.041085005
case acc: 0.09096166
case acc: 0.067146145
case acc: 0.06254127
case acc: 0.06840084
top acc: 0.0270 ::: bot acc: 0.0994
top acc: 0.0249 ::: bot acc: 0.0710
top acc: 0.0463 ::: bot acc: 0.1388
top acc: 0.0377 ::: bot acc: 0.0997
top acc: 0.0497 ::: bot acc: 0.0977
top acc: 0.0597 ::: bot acc: 0.1012
current epoch: 44
train loss is 0.056648
average val loss: 0.056495, accuracy: 0.0571
average test loss: 0.067060, accuracy: 0.0672
case acc: 0.054095622
case acc: 0.042954423
case acc: 0.095849365
case acc: 0.07546155
case acc: 0.064279065
case acc: 0.070377015
top acc: 0.0226 ::: bot acc: 0.1050
top acc: 0.0228 ::: bot acc: 0.0748
top acc: 0.0485 ::: bot acc: 0.1450
top acc: 0.0435 ::: bot acc: 0.1093
top acc: 0.0466 ::: bot acc: 0.1018
top acc: 0.0557 ::: bot acc: 0.1061
current epoch: 45
train loss is 0.057951
average val loss: 0.057711, accuracy: 0.0585
average test loss: 0.068015, accuracy: 0.0683
case acc: 0.05459423
case acc: 0.04233168
case acc: 0.09726999
case acc: 0.08009814
case acc: 0.06447917
case acc: 0.07103937
top acc: 0.0218 ::: bot acc: 0.1061
top acc: 0.0235 ::: bot acc: 0.0736
top acc: 0.0493 ::: bot acc: 0.1468
top acc: 0.0469 ::: bot acc: 0.1146
top acc: 0.0462 ::: bot acc: 0.1023
top acc: 0.0547 ::: bot acc: 0.1077
current epoch: 46
train loss is 0.058239
average val loss: 0.057234, accuracy: 0.0583
average test loss: 0.067636, accuracy: 0.0681
case acc: 0.054004338
case acc: 0.040473234
case acc: 0.09639813
case acc: 0.08235897
case acc: 0.06405554
case acc: 0.07104686
top acc: 0.0227 ::: bot acc: 0.1048
top acc: 0.0259 ::: bot acc: 0.0696
top acc: 0.0488 ::: bot acc: 0.1457
top acc: 0.0486 ::: bot acc: 0.1171
top acc: 0.0470 ::: bot acc: 0.1013
top acc: 0.0547 ::: bot acc: 0.1077
current epoch: 47
train loss is 0.058678
average val loss: 0.052954, accuracy: 0.0543
average test loss: 0.064230, accuracy: 0.0648
case acc: 0.051267892
case acc: 0.03676343
case acc: 0.090712875
case acc: 0.07917275
case acc: 0.06186987
case acc: 0.06929225
top acc: 0.0286 ::: bot acc: 0.0979
top acc: 0.0355 ::: bot acc: 0.0591
top acc: 0.0461 ::: bot acc: 0.1385
top acc: 0.0462 ::: bot acc: 0.1135
top acc: 0.0511 ::: bot acc: 0.0960
top acc: 0.0578 ::: bot acc: 0.1035
current epoch: 48
train loss is 0.057423
average val loss: 0.045994, accuracy: 0.0473
average test loss: 0.058582, accuracy: 0.0597
case acc: 0.048715502
case acc: 0.034815546
case acc: 0.08013496
case acc: 0.07009569
case acc: 0.058352035
case acc: 0.0659153
top acc: 0.0415 ::: bot acc: 0.0850
top acc: 0.0515 ::: bot acc: 0.0431
top acc: 0.0422 ::: bot acc: 0.1246
top acc: 0.0398 ::: bot acc: 0.1031
top acc: 0.0601 ::: bot acc: 0.0863
top acc: 0.0656 ::: bot acc: 0.0945
current epoch: 49
train loss is 0.056125
average val loss: 0.039358, accuracy: 0.0401
average test loss: 0.052675, accuracy: 0.0545
case acc: 0.04862112
case acc: 0.03840099
case acc: 0.06709973
case acc: 0.05549663
case acc: 0.05540713
case acc: 0.061805233
top acc: 0.0602 ::: bot acc: 0.0664
top acc: 0.0720 ::: bot acc: 0.0229
top acc: 0.0443 ::: bot acc: 0.1041
top acc: 0.0313 ::: bot acc: 0.0855
top acc: 0.0740 ::: bot acc: 0.0723
top acc: 0.0793 ::: bot acc: 0.0809
current epoch: 50
train loss is 0.055703
average val loss: 0.038539, accuracy: 0.0383
average test loss: 0.049802, accuracy: 0.0518
case acc: 0.052203253
case acc: 0.054597903
case acc: 0.053666364
case acc: 0.036955886
case acc: 0.05404774
case acc: 0.059114475
top acc: 0.0863 ::: bot acc: 0.0402
top acc: 0.0977 ::: bot acc: 0.0199
top acc: 0.0627 ::: bot acc: 0.0749
top acc: 0.0304 ::: bot acc: 0.0582
top acc: 0.0941 ::: bot acc: 0.0523
top acc: 0.1000 ::: bot acc: 0.0602
LME_Co_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-01-01', '2011-07-01', '2015-07-01', '2016-01-01', '2016-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5328 5328 5328
1.7082474 -0.6288155 0.29835227 -0.25048217
Validation: 594 594 594
Testing: 774 774 774
pre-processing time: 0.0004818439483642578
the split date is 2011-07-01
net initializing with time: 0.17945289611816406
preparing training and testing date with time: 4.76837158203125e-07
current epoch: 1
train loss is 0.279754
average val loss: 0.200689, accuracy: 0.2036
average test loss: 0.206793, accuracy: 0.2034
case acc: 0.31740564
case acc: 0.2545362
case acc: 0.30502746
case acc: 0.08182939
case acc: 0.05886789
case acc: 0.20290643
top acc: 0.2644 ::: bot acc: 0.3715
top acc: 0.2094 ::: bot acc: 0.2961
top acc: 0.2408 ::: bot acc: 0.3719
top acc: 0.0336 ::: bot acc: 0.1414
top acc: 0.1101 ::: bot acc: 0.0186
top acc: 0.1404 ::: bot acc: 0.2563
current epoch: 2
train loss is 0.245610
average val loss: 0.121923, accuracy: 0.1182
average test loss: 0.120465, accuracy: 0.1212
case acc: 0.14001483
case acc: 0.07894574
case acc: 0.12964341
case acc: 0.104388185
case acc: 0.22470994
case acc: 0.049364865
top acc: 0.0871 ::: bot acc: 0.1940
top acc: 0.0346 ::: bot acc: 0.1201
top acc: 0.0651 ::: bot acc: 0.1963
top acc: 0.1706 ::: bot acc: 0.0371
top acc: 0.2862 ::: bot acc: 0.1640
top acc: 0.0395 ::: bot acc: 0.0769
current epoch: 3
train loss is 0.140857
average val loss: 0.130567, accuracy: 0.1272
average test loss: 0.132959, accuracy: 0.1324
case acc: 0.188531
case acc: 0.12958382
case acc: 0.17829192
case acc: 0.061437305
case acc: 0.15704295
case acc: 0.079322465
top acc: 0.1356 ::: bot acc: 0.2425
top acc: 0.0849 ::: bot acc: 0.1709
top acc: 0.1135 ::: bot acc: 0.2451
top acc: 0.1109 ::: bot acc: 0.0275
top acc: 0.2187 ::: bot acc: 0.0963
top acc: 0.0238 ::: bot acc: 0.1300
current epoch: 4
train loss is 0.168583
average val loss: 0.112618, accuracy: 0.1090
average test loss: 0.111416, accuracy: 0.1118
case acc: 0.13681725
case acc: 0.08114375
case acc: 0.12730305
case acc: 0.08642028
case acc: 0.18852901
case acc: 0.050814092
top acc: 0.0838 ::: bot acc: 0.1907
top acc: 0.0367 ::: bot acc: 0.1223
top acc: 0.0626 ::: bot acc: 0.1941
top acc: 0.1507 ::: bot acc: 0.0226
top acc: 0.2503 ::: bot acc: 0.1277
top acc: 0.0360 ::: bot acc: 0.0813
current epoch: 5
train loss is 0.129650
average val loss: 0.116063, accuracy: 0.1130
average test loss: 0.117473, accuracy: 0.1171
case acc: 0.16567951
case acc: 0.11262499
case acc: 0.15561676
case acc: 0.06038509
case acc: 0.13986729
case acc: 0.06818588
top acc: 0.1127 ::: bot acc: 0.2195
top acc: 0.0680 ::: bot acc: 0.1539
top acc: 0.0907 ::: bot acc: 0.2226
top acc: 0.1086 ::: bot acc: 0.0284
top acc: 0.2017 ::: bot acc: 0.0790
top acc: 0.0204 ::: bot acc: 0.1154
current epoch: 6
train loss is 0.145691
average val loss: 0.103855, accuracy: 0.1003
average test loss: 0.103195, accuracy: 0.1033
case acc: 0.13536398
case acc: 0.0854562
case acc: 0.12543371
case acc: 0.06904057
case acc: 0.15062933
case acc: 0.053716872
top acc: 0.0823 ::: bot acc: 0.1892
top acc: 0.0409 ::: bot acc: 0.1267
top acc: 0.0607 ::: bot acc: 0.1923
top acc: 0.1263 ::: bot acc: 0.0188
top acc: 0.2126 ::: bot acc: 0.0897
top acc: 0.0300 ::: bot acc: 0.0889
current epoch: 7
train loss is 0.121895
average val loss: 0.102803, accuracy: 0.1000
average test loss: 0.103017, accuracy: 0.1027
case acc: 0.14395623
case acc: 0.09691523
case acc: 0.13320954
case acc: 0.05876414
case acc: 0.12309567
case acc: 0.060148887
top acc: 0.0909 ::: bot acc: 0.1979
top acc: 0.0523 ::: bot acc: 0.1382
top acc: 0.0682 ::: bot acc: 0.2002
top acc: 0.1049 ::: bot acc: 0.0306
top acc: 0.1851 ::: bot acc: 0.0622
top acc: 0.0225 ::: bot acc: 0.1024
current epoch: 8
train loss is 0.124813
average val loss: 0.097917, accuracy: 0.0954
average test loss: 0.097782, accuracy: 0.0973
case acc: 0.13745539
case acc: 0.09359118
case acc: 0.12683871
case acc: 0.05629826
case acc: 0.11085723
case acc: 0.058715843
top acc: 0.0844 ::: bot acc: 0.1914
top acc: 0.0490 ::: bot acc: 0.1349
top acc: 0.0620 ::: bot acc: 0.1938
top acc: 0.0990 ::: bot acc: 0.0349
top acc: 0.1729 ::: bot acc: 0.0499
top acc: 0.0235 ::: bot acc: 0.0999
current epoch: 9
train loss is 0.115658
average val loss: 0.092852, accuracy: 0.0905
average test loss: 0.092215, accuracy: 0.0916
case acc: 0.12975092
case acc: 0.08892759
case acc: 0.11891208
case acc: 0.054638766
case acc: 0.10020878
case acc: 0.05692094
top acc: 0.0767 ::: bot acc: 0.1837
top acc: 0.0446 ::: bot acc: 0.1301
top acc: 0.0542 ::: bot acc: 0.1857
top acc: 0.0945 ::: bot acc: 0.0390
top acc: 0.1622 ::: bot acc: 0.0394
top acc: 0.0252 ::: bot acc: 0.0964
current epoch: 10
train loss is 0.111446
average val loss: 0.089483, accuracy: 0.0878
average test loss: 0.088701, accuracy: 0.0877
case acc: 0.12659602
case acc: 0.088982545
case acc: 0.115789264
case acc: 0.051752422
case acc: 0.08558524
case acc: 0.05735081
top acc: 0.0736 ::: bot acc: 0.1805
top acc: 0.0447 ::: bot acc: 0.1299
top acc: 0.0513 ::: bot acc: 0.1824
top acc: 0.0858 ::: bot acc: 0.0477
top acc: 0.1472 ::: bot acc: 0.0255
top acc: 0.0247 ::: bot acc: 0.0973
current epoch: 11
train loss is 0.105808
average val loss: 0.085576, accuracy: 0.0843
average test loss: 0.084493, accuracy: 0.0834
case acc: 0.121054254
case acc: 0.08673477
case acc: 0.110665016
case acc: 0.05023228
case acc: 0.074789934
case acc: 0.056643084
top acc: 0.0682 ::: bot acc: 0.1749
top acc: 0.0426 ::: bot acc: 0.1276
top acc: 0.0468 ::: bot acc: 0.1769
top acc: 0.0795 ::: bot acc: 0.0539
top acc: 0.1350 ::: bot acc: 0.0178
top acc: 0.0254 ::: bot acc: 0.0960
current epoch: 12
train loss is 0.101067
average val loss: 0.081818, accuracy: 0.0809
average test loss: 0.080492, accuracy: 0.0795
case acc: 0.11530316
case acc: 0.08435529
case acc: 0.105523475
case acc: 0.04933986
case acc: 0.06641049
case acc: 0.05582467
top acc: 0.0624 ::: bot acc: 0.1692
top acc: 0.0403 ::: bot acc: 0.1251
top acc: 0.0425 ::: bot acc: 0.1713
top acc: 0.0738 ::: bot acc: 0.0596
top acc: 0.1231 ::: bot acc: 0.0164
top acc: 0.0264 ::: bot acc: 0.0943
current epoch: 13
train loss is 0.095017
average val loss: 0.079215, accuracy: 0.0789
average test loss: 0.077985, accuracy: 0.0771
case acc: 0.11198329
case acc: 0.08453795
case acc: 0.10302208
case acc: 0.048673455
case acc: 0.058105003
case acc: 0.056057256
top acc: 0.0592 ::: bot acc: 0.1658
top acc: 0.0405 ::: bot acc: 0.1252
top acc: 0.0403 ::: bot acc: 0.1686
top acc: 0.0659 ::: bot acc: 0.0673
top acc: 0.1091 ::: bot acc: 0.0197
top acc: 0.0261 ::: bot acc: 0.0948
current epoch: 14
train loss is 0.091679
average val loss: 0.076582, accuracy: 0.0769
average test loss: 0.075408, accuracy: 0.0746
case acc: 0.10757006
case acc: 0.08367985
case acc: 0.099851966
case acc: 0.048765752
case acc: 0.05206488
case acc: 0.055703178
top acc: 0.0549 ::: bot acc: 0.1614
top acc: 0.0397 ::: bot acc: 0.1243
top acc: 0.0377 ::: bot acc: 0.1650
top acc: 0.0595 ::: bot acc: 0.0738
top acc: 0.0964 ::: bot acc: 0.0270
top acc: 0.0265 ::: bot acc: 0.0941
current epoch: 15
train loss is 0.087945
average val loss: 0.072706, accuracy: 0.0729
average test loss: 0.071019, accuracy: 0.0705
case acc: 0.098997995
case acc: 0.07870624
case acc: 0.093447044
case acc: 0.048911802
case acc: 0.049327046
case acc: 0.053522002
top acc: 0.0466 ::: bot acc: 0.1527
top acc: 0.0350 ::: bot acc: 0.1191
top acc: 0.0339 ::: bot acc: 0.1573
top acc: 0.0577 ::: bot acc: 0.0757
top acc: 0.0882 ::: bot acc: 0.0349
top acc: 0.0297 ::: bot acc: 0.0892
current epoch: 16
train loss is 0.082613
average val loss: 0.071151, accuracy: 0.0719
average test loss: 0.069439, accuracy: 0.0690
case acc: 0.09515976
case acc: 0.0783092
case acc: 0.0912298
case acc: 0.0497122
case acc: 0.046374314
case acc: 0.053448245
top acc: 0.0430 ::: bot acc: 0.1487
top acc: 0.0347 ::: bot acc: 0.1187
top acc: 0.0326 ::: bot acc: 0.1545
top acc: 0.0515 ::: bot acc: 0.0819
top acc: 0.0756 ::: bot acc: 0.0474
top acc: 0.0298 ::: bot acc: 0.0891
current epoch: 17
train loss is 0.079006
average val loss: 0.068620, accuracy: 0.0695
average test loss: 0.066432, accuracy: 0.0662
case acc: 0.08819828
case acc: 0.07474976
case acc: 0.08648317
case acc: 0.050180823
case acc: 0.045474995
case acc: 0.052069813
top acc: 0.0365 ::: bot acc: 0.1415
top acc: 0.0314 ::: bot acc: 0.1150
top acc: 0.0301 ::: bot acc: 0.1486
top acc: 0.0490 ::: bot acc: 0.0844
top acc: 0.0667 ::: bot acc: 0.0563
top acc: 0.0324 ::: bot acc: 0.0857
current epoch: 18
train loss is 0.074498
average val loss: 0.067473, accuracy: 0.0686
average test loss: 0.065062, accuracy: 0.0650
case acc: 0.083636984
case acc: 0.073398106
case acc: 0.08390581
case acc: 0.051247828
case acc: 0.045942467
case acc: 0.05171451
top acc: 0.0323 ::: bot acc: 0.1368
top acc: 0.0303 ::: bot acc: 0.1135
top acc: 0.0288 ::: bot acc: 0.1454
top acc: 0.0446 ::: bot acc: 0.0889
top acc: 0.0559 ::: bot acc: 0.0670
top acc: 0.0330 ::: bot acc: 0.0848
current epoch: 19
train loss is 0.071736
average val loss: 0.066418, accuracy: 0.0676
average test loss: 0.063674, accuracy: 0.0637
case acc: 0.07884067
case acc: 0.071067296
case acc: 0.08126253
case acc: 0.052378096
case acc: 0.047661427
case acc: 0.051209573
top acc: 0.0282 ::: bot acc: 0.1317
top acc: 0.0284 ::: bot acc: 0.1109
top acc: 0.0277 ::: bot acc: 0.1420
top acc: 0.0411 ::: bot acc: 0.0924
top acc: 0.0461 ::: bot acc: 0.0769
top acc: 0.0341 ::: bot acc: 0.0836
current epoch: 20
train loss is 0.069238
average val loss: 0.063668, accuracy: 0.0646
average test loss: 0.060161, accuracy: 0.0603
case acc: 0.07076256
case acc: 0.065146394
case acc: 0.07561268
case acc: 0.052011684
case acc: 0.048843376
case acc: 0.049264543
top acc: 0.0227 ::: bot acc: 0.1223
top acc: 0.0246 ::: bot acc: 0.1039
top acc: 0.0257 ::: bot acc: 0.1344
top acc: 0.0422 ::: bot acc: 0.0913
top acc: 0.0410 ::: bot acc: 0.0819
top acc: 0.0390 ::: bot acc: 0.0782
current epoch: 21
train loss is 0.065164
average val loss: 0.062346, accuracy: 0.0632
average test loss: 0.058463, accuracy: 0.0585
case acc: 0.06556341
case acc: 0.061615027
case acc: 0.072190836
case acc: 0.052386425
case acc: 0.051001858
case acc: 0.048487604
top acc: 0.0197 ::: bot acc: 0.1160
top acc: 0.0225 ::: bot acc: 0.0997
top acc: 0.0246 ::: bot acc: 0.1298
top acc: 0.0410 ::: bot acc: 0.0925
top acc: 0.0339 ::: bot acc: 0.0891
top acc: 0.0414 ::: bot acc: 0.0759
current epoch: 22
train loss is 0.062239
average val loss: 0.060712, accuracy: 0.0614
average test loss: 0.056358, accuracy: 0.0565
case acc: 0.060033023
case acc: 0.05724359
case acc: 0.06855292
case acc: 0.052166842
case acc: 0.05325219
case acc: 0.047497988
top acc: 0.0178 ::: bot acc: 0.1086
top acc: 0.0204 ::: bot acc: 0.0941
top acc: 0.0244 ::: bot acc: 0.1244
top acc: 0.0415 ::: bot acc: 0.0918
top acc: 0.0303 ::: bot acc: 0.0943
top acc: 0.0448 ::: bot acc: 0.0724
current epoch: 23
train loss is 0.059795
average val loss: 0.059239, accuracy: 0.0599
average test loss: 0.054425, accuracy: 0.0545
case acc: 0.05511885
case acc: 0.053094044
case acc: 0.06531348
case acc: 0.051773988
case acc: 0.055402502
case acc: 0.04654635
top acc: 0.0174 ::: bot acc: 0.1014
top acc: 0.0192 ::: bot acc: 0.0886
top acc: 0.0249 ::: bot acc: 0.1193
top acc: 0.0426 ::: bot acc: 0.0907
top acc: 0.0282 ::: bot acc: 0.0985
top acc: 0.0482 ::: bot acc: 0.0690
current epoch: 24
train loss is 0.057598
average val loss: 0.057369, accuracy: 0.0580
average test loss: 0.051926, accuracy: 0.0520
case acc: 0.049948048
case acc: 0.048094563
case acc: 0.061544966
case acc: 0.050802745
case acc: 0.056205202
case acc: 0.04533395
top acc: 0.0188 ::: bot acc: 0.0930
top acc: 0.0187 ::: bot acc: 0.0813
top acc: 0.0265 ::: bot acc: 0.1128
top acc: 0.0460 ::: bot acc: 0.0873
top acc: 0.0275 ::: bot acc: 0.1001
top acc: 0.0531 ::: bot acc: 0.0640
current epoch: 25
train loss is 0.055556
average val loss: 0.056373, accuracy: 0.0571
average test loss: 0.050570, accuracy: 0.0506
case acc: 0.046893217
case acc: 0.044787265
case acc: 0.059484527
case acc: 0.05037333
case acc: 0.057487484
case acc: 0.044742644
top acc: 0.0213 ::: bot acc: 0.0871
top acc: 0.0189 ::: bot acc: 0.0762
top acc: 0.0283 ::: bot acc: 0.1088
top acc: 0.0480 ::: bot acc: 0.0853
top acc: 0.0264 ::: bot acc: 0.1025
top acc: 0.0557 ::: bot acc: 0.0613
current epoch: 26
train loss is 0.054836
average val loss: 0.055307, accuracy: 0.0561
average test loss: 0.049119, accuracy: 0.0491
case acc: 0.044192668
case acc: 0.04137979
case acc: 0.057372686
case acc: 0.049752798
case acc: 0.057848994
case acc: 0.044194747
top acc: 0.0253 ::: bot acc: 0.0810
top acc: 0.0200 ::: bot acc: 0.0706
top acc: 0.0306 ::: bot acc: 0.1045
top acc: 0.0510 ::: bot acc: 0.0823
top acc: 0.0261 ::: bot acc: 0.1031
top acc: 0.0586 ::: bot acc: 0.0583
current epoch: 27
train loss is 0.053977
average val loss: 0.053842, accuracy: 0.0545
average test loss: 0.047141, accuracy: 0.0470
case acc: 0.041471504
case acc: 0.037226673
case acc: 0.05462821
case acc: 0.048880037
case acc: 0.05645758
case acc: 0.043570064
top acc: 0.0322 ::: bot acc: 0.0734
top acc: 0.0229 ::: bot acc: 0.0630
top acc: 0.0349 ::: bot acc: 0.0982
top acc: 0.0564 ::: bot acc: 0.0768
top acc: 0.0270 ::: bot acc: 0.1004
top acc: 0.0635 ::: bot acc: 0.0534
current epoch: 28
train loss is 0.052666
average val loss: 0.053262, accuracy: 0.0539
average test loss: 0.046369, accuracy: 0.0462
case acc: 0.040352844
case acc: 0.035188027
case acc: 0.053582717
case acc: 0.048640743
case acc: 0.056095783
case acc: 0.04340298
top acc: 0.0358 ::: bot acc: 0.0696
top acc: 0.0254 ::: bot acc: 0.0586
top acc: 0.0374 ::: bot acc: 0.0953
top acc: 0.0593 ::: bot acc: 0.0739
top acc: 0.0272 ::: bot acc: 0.0997
top acc: 0.0651 ::: bot acc: 0.0518
current epoch: 29
train loss is 0.052358
average val loss: 0.052659, accuracy: 0.0532
average test loss: 0.045613, accuracy: 0.0454
case acc: 0.039670162
case acc: 0.033477318
case acc: 0.05273207
case acc: 0.048479147
case acc: 0.05508927
case acc: 0.04322351
top acc: 0.0392 ::: bot acc: 0.0662
top acc: 0.0291 ::: bot acc: 0.0542
top acc: 0.0403 ::: bot acc: 0.0924
top acc: 0.0628 ::: bot acc: 0.0703
top acc: 0.0281 ::: bot acc: 0.0977
top acc: 0.0669 ::: bot acc: 0.0499
current epoch: 30
train loss is 0.051716
average val loss: 0.052050, accuracy: 0.0523
average test loss: 0.044897, accuracy: 0.0448
case acc: 0.039179824
case acc: 0.03215692
case acc: 0.052005865
case acc: 0.04850616
case acc: 0.05354026
case acc: 0.043149903
top acc: 0.0424 ::: bot acc: 0.0629
top acc: 0.0335 ::: bot acc: 0.0495
top acc: 0.0435 ::: bot acc: 0.0892
top acc: 0.0671 ::: bot acc: 0.0659
top acc: 0.0297 ::: bot acc: 0.0945
top acc: 0.0690 ::: bot acc: 0.0477
current epoch: 31
train loss is 0.051345
average val loss: 0.051873, accuracy: 0.0520
average test loss: 0.044672, accuracy: 0.0446
case acc: 0.03910716
case acc: 0.031728994
case acc: 0.051875874
case acc: 0.048609484
case acc: 0.05294425
case acc: 0.04314184
top acc: 0.0429 ::: bot acc: 0.0623
top acc: 0.0354 ::: bot acc: 0.0476
top acc: 0.0440 ::: bot acc: 0.0886
top acc: 0.0692 ::: bot acc: 0.0638
top acc: 0.0304 ::: bot acc: 0.0933
top acc: 0.0688 ::: bot acc: 0.0479
current epoch: 32
train loss is 0.050888
average val loss: 0.051944, accuracy: 0.0519
average test loss: 0.044717, accuracy: 0.0446
case acc: 0.03923941
case acc: 0.03170143
case acc: 0.05216567
case acc: 0.048646875
case acc: 0.052968107
case acc: 0.043154784
top acc: 0.0416 ::: bot acc: 0.0635
top acc: 0.0355 ::: bot acc: 0.0475
top acc: 0.0426 ::: bot acc: 0.0900
top acc: 0.0697 ::: bot acc: 0.0633
top acc: 0.0303 ::: bot acc: 0.0933
top acc: 0.0670 ::: bot acc: 0.0497
current epoch: 33
train loss is 0.050536
average val loss: 0.052090, accuracy: 0.0520
average test loss: 0.044860, accuracy: 0.0448
case acc: 0.039441448
case acc: 0.03175148
case acc: 0.052614626
case acc: 0.048650354
case acc: 0.053173795
case acc: 0.043309007
top acc: 0.0400 ::: bot acc: 0.0652
top acc: 0.0351 ::: bot acc: 0.0479
top acc: 0.0408 ::: bot acc: 0.0918
top acc: 0.0697 ::: bot acc: 0.0633
top acc: 0.0301 ::: bot acc: 0.0937
top acc: 0.0649 ::: bot acc: 0.0518
current epoch: 34
train loss is 0.050342
average val loss: 0.052506, accuracy: 0.0523
average test loss: 0.045355, accuracy: 0.0454
case acc: 0.039912418
case acc: 0.03215478
case acc: 0.053598117
case acc: 0.048546746
case acc: 0.054243486
case acc: 0.04364782
top acc: 0.0371 ::: bot acc: 0.0681
top acc: 0.0332 ::: bot acc: 0.0498
top acc: 0.0375 ::: bot acc: 0.0950
top acc: 0.0680 ::: bot acc: 0.0650
top acc: 0.0290 ::: bot acc: 0.0959
top acc: 0.0614 ::: bot acc: 0.0552
current epoch: 35
train loss is 0.050149
average val loss: 0.052929, accuracy: 0.0527
average test loss: 0.045891, accuracy: 0.0459
case acc: 0.040449794
case acc: 0.03251672
case acc: 0.054661434
case acc: 0.04845306
case acc: 0.05539442
case acc: 0.044051766
top acc: 0.0349 ::: bot acc: 0.0703
top acc: 0.0317 ::: bot acc: 0.0513
top acc: 0.0349 ::: bot acc: 0.0979
top acc: 0.0663 ::: bot acc: 0.0667
top acc: 0.0279 ::: bot acc: 0.0982
top acc: 0.0585 ::: bot acc: 0.0581
current epoch: 36
train loss is 0.050131
average val loss: 0.053119, accuracy: 0.0529
average test loss: 0.046136, accuracy: 0.0462
case acc: 0.040604725
case acc: 0.03254241
case acc: 0.05529741
case acc: 0.048419096
case acc: 0.05607863
case acc: 0.04427415
top acc: 0.0344 ::: bot acc: 0.0707
top acc: 0.0317 ::: bot acc: 0.0513
top acc: 0.0338 ::: bot acc: 0.0994
top acc: 0.0656 ::: bot acc: 0.0674
top acc: 0.0273 ::: bot acc: 0.0995
top acc: 0.0571 ::: bot acc: 0.0596
current epoch: 37
train loss is 0.050080
average val loss: 0.053224, accuracy: 0.0531
average test loss: 0.046274, accuracy: 0.0464
case acc: 0.04053407
case acc: 0.032443922
case acc: 0.055652767
case acc: 0.04840113
case acc: 0.056744523
case acc: 0.044441663
top acc: 0.0348 ::: bot acc: 0.0704
top acc: 0.0322 ::: bot acc: 0.0509
top acc: 0.0332 ::: bot acc: 0.1003
top acc: 0.0650 ::: bot acc: 0.0680
top acc: 0.0268 ::: bot acc: 0.1008
top acc: 0.0562 ::: bot acc: 0.0605
current epoch: 38
train loss is 0.050076
average val loss: 0.052631, accuracy: 0.0526
average test loss: 0.045520, accuracy: 0.0457
case acc: 0.039749116
case acc: 0.03161576
case acc: 0.054441843
case acc: 0.048526045
case acc: 0.055642918
case acc: 0.04402239
top acc: 0.0388 ::: bot acc: 0.0665
top acc: 0.0363 ::: bot acc: 0.0469
top acc: 0.0352 ::: bot acc: 0.0975
top acc: 0.0679 ::: bot acc: 0.0651
top acc: 0.0277 ::: bot acc: 0.0987
top acc: 0.0588 ::: bot acc: 0.0579
current epoch: 39
train loss is 0.050003
average val loss: 0.052132, accuracy: 0.0521
average test loss: 0.044925, accuracy: 0.0451
case acc: 0.03923986
case acc: 0.031022485
case acc: 0.053414173
case acc: 0.048741557
case acc: 0.054492023
case acc: 0.043675873
top acc: 0.0423 ::: bot acc: 0.0630
top acc: 0.0401 ::: bot acc: 0.0430
top acc: 0.0378 ::: bot acc: 0.0946
top acc: 0.0708 ::: bot acc: 0.0622
top acc: 0.0287 ::: bot acc: 0.0965
top acc: 0.0613 ::: bot acc: 0.0554
current epoch: 40
train loss is 0.049992
average val loss: 0.051667, accuracy: 0.0517
average test loss: 0.044432, accuracy: 0.0446
case acc: 0.03886633
case acc: 0.030709809
case acc: 0.05247925
case acc: 0.049063418
case acc: 0.0530315
case acc: 0.04341203
top acc: 0.0457 ::: bot acc: 0.0596
top acc: 0.0441 ::: bot acc: 0.0391
top acc: 0.0409 ::: bot acc: 0.0915
top acc: 0.0743 ::: bot acc: 0.0587
top acc: 0.0301 ::: bot acc: 0.0936
top acc: 0.0638 ::: bot acc: 0.0529
current epoch: 41
train loss is 0.049983
average val loss: 0.051303, accuracy: 0.0513
average test loss: 0.044120, accuracy: 0.0443
case acc: 0.038729906
case acc: 0.030796051
case acc: 0.051810287
case acc: 0.049530014
case acc: 0.0516757
case acc: 0.04323689
top acc: 0.0484 ::: bot acc: 0.0569
top acc: 0.0477 ::: bot acc: 0.0356
top acc: 0.0438 ::: bot acc: 0.0886
top acc: 0.0778 ::: bot acc: 0.0551
top acc: 0.0320 ::: bot acc: 0.0906
top acc: 0.0660 ::: bot acc: 0.0508
current epoch: 42
train loss is 0.050022
average val loss: 0.050926, accuracy: 0.0508
average test loss: 0.043887, accuracy: 0.0441
case acc: 0.038757846
case acc: 0.031139355
case acc: 0.051042795
case acc: 0.050607238
case acc: 0.05005024
case acc: 0.043119065
top acc: 0.0516 ::: bot acc: 0.0537
top acc: 0.0520 ::: bot acc: 0.0312
top acc: 0.0478 ::: bot acc: 0.0847
top acc: 0.0826 ::: bot acc: 0.0503
top acc: 0.0360 ::: bot acc: 0.0861
top acc: 0.0689 ::: bot acc: 0.0478
current epoch: 43
train loss is 0.050126
average val loss: 0.050747, accuracy: 0.0504
average test loss: 0.043871, accuracy: 0.0442
case acc: 0.038835734
case acc: 0.03159461
case acc: 0.050658733
case acc: 0.051780686
case acc: 0.04897396
case acc: 0.043102153
top acc: 0.0530 ::: bot acc: 0.0522
top acc: 0.0549 ::: bot acc: 0.0283
top acc: 0.0505 ::: bot acc: 0.0819
top acc: 0.0866 ::: bot acc: 0.0462
top acc: 0.0398 ::: bot acc: 0.0824
top acc: 0.0705 ::: bot acc: 0.0462
current epoch: 44
train loss is 0.049980
average val loss: 0.050680, accuracy: 0.0501
average test loss: 0.043917, accuracy: 0.0442
case acc: 0.038827453
case acc: 0.031899255
case acc: 0.05048876
case acc: 0.05280446
case acc: 0.04822784
case acc: 0.043094415
top acc: 0.0529 ::: bot acc: 0.0523
top acc: 0.0564 ::: bot acc: 0.0268
top acc: 0.0520 ::: bot acc: 0.0804
top acc: 0.0898 ::: bot acc: 0.0431
top acc: 0.0427 ::: bot acc: 0.0795
top acc: 0.0710 ::: bot acc: 0.0457
current epoch: 45
train loss is 0.049700
average val loss: 0.050716, accuracy: 0.0498
average test loss: 0.043905, accuracy: 0.0442
case acc: 0.038690183
case acc: 0.031702895
case acc: 0.050583117
case acc: 0.053157117
case acc: 0.047948163
case acc: 0.0430958
top acc: 0.0504 ::: bot acc: 0.0548
top acc: 0.0555 ::: bot acc: 0.0277
top acc: 0.0511 ::: bot acc: 0.0814
top acc: 0.0908 ::: bot acc: 0.0420
top acc: 0.0438 ::: bot acc: 0.0783
top acc: 0.0693 ::: bot acc: 0.0473
current epoch: 46
train loss is 0.049345
average val loss: 0.050924, accuracy: 0.0497
average test loss: 0.043923, accuracy: 0.0442
case acc: 0.038874477
case acc: 0.03111783
case acc: 0.05115333
case acc: 0.05262238
case acc: 0.048206884
case acc: 0.04325462
top acc: 0.0453 ::: bot acc: 0.0599
top acc: 0.0519 ::: bot acc: 0.0313
top acc: 0.0471 ::: bot acc: 0.0853
top acc: 0.0892 ::: bot acc: 0.0436
top acc: 0.0427 ::: bot acc: 0.0794
top acc: 0.0654 ::: bot acc: 0.0513
current epoch: 47
train loss is 0.049194
average val loss: 0.051564, accuracy: 0.0501
average test loss: 0.044369, accuracy: 0.0446
case acc: 0.039919727
case acc: 0.030729696
case acc: 0.05281537
case acc: 0.051118705
case acc: 0.04925704
case acc: 0.04400469
top acc: 0.0377 ::: bot acc: 0.0676
top acc: 0.0453 ::: bot acc: 0.0379
top acc: 0.0397 ::: bot acc: 0.0928
top acc: 0.0845 ::: bot acc: 0.0483
top acc: 0.0387 ::: bot acc: 0.0835
top acc: 0.0589 ::: bot acc: 0.0578
current epoch: 48
train loss is 0.049363
average val loss: 0.052339, accuracy: 0.0508
average test loss: 0.045232, accuracy: 0.0455
case acc: 0.04146937
case acc: 0.03112333
case acc: 0.055426236
case acc: 0.05000492
case acc: 0.05029279
case acc: 0.04494932
top acc: 0.0316 ::: bot acc: 0.0737
top acc: 0.0397 ::: bot acc: 0.0436
top acc: 0.0335 ::: bot acc: 0.0998
top acc: 0.0802 ::: bot acc: 0.0527
top acc: 0.0353 ::: bot acc: 0.0869
top acc: 0.0539 ::: bot acc: 0.0628
current epoch: 49
train loss is 0.050064
average val loss: 0.053669, accuracy: 0.0523
average test loss: 0.046886, accuracy: 0.0473
case acc: 0.043937474
case acc: 0.032401856
case acc: 0.05968289
case acc: 0.049009938
case acc: 0.05231153
case acc: 0.046298184
top acc: 0.0255 ::: bot acc: 0.0804
top acc: 0.0328 ::: bot acc: 0.0505
top acc: 0.0284 ::: bot acc: 0.1088
top acc: 0.0739 ::: bot acc: 0.0589
top acc: 0.0310 ::: bot acc: 0.0921
top acc: 0.0482 ::: bot acc: 0.0686
current epoch: 50
train loss is 0.051000
average val loss: 0.055595, accuracy: 0.0545
average test loss: 0.049454, accuracy: 0.0499
case acc: 0.046984985
case acc: 0.034882452
case acc: 0.06539858
case acc: 0.048457943
case acc: 0.055668704
case acc: 0.047905706
top acc: 0.0208 ::: bot acc: 0.0874
top acc: 0.0255 ::: bot acc: 0.0582
top acc: 0.0252 ::: bot acc: 0.1189
top acc: 0.0663 ::: bot acc: 0.0667
top acc: 0.0277 ::: bot acc: 0.0988
top acc: 0.0420 ::: bot acc: 0.0746
LME_Co_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2011-07-01', '2012-01-01', '2016-01-01', '2016-07-01', '2017-01-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5358 5358 5358
1.7082474 -0.6288155 0.2588177 -0.21218425
Validation: 600 600 600
Testing: 750 750 750
pre-processing time: 0.000461578369140625
the split date is 2012-01-01
net initializing with time: 0.0040435791015625
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.153744
average val loss: 0.179977, accuracy: 0.1788
average test loss: 0.140629, accuracy: 0.1452
case acc: 0.18066226
case acc: 0.03964771
case acc: 0.08769418
case acc: 0.08959136
case acc: 0.05271768
case acc: 0.4209649
top acc: 0.1288 ::: bot acc: 0.2357
top acc: 0.0301 ::: bot acc: 0.0761
top acc: 0.0344 ::: bot acc: 0.1543
top acc: 0.0365 ::: bot acc: 0.1472
top acc: 0.0963 ::: bot acc: 0.0205
top acc: 0.3610 ::: bot acc: 0.4816
current epoch: 2
train loss is 0.185607
average val loss: 0.271719, accuracy: 0.2717
average test loss: 0.217772, accuracy: 0.2176
case acc: 0.2701411
case acc: 0.12455615
case acc: 0.16531023
case acc: 0.1825238
case acc: 0.06799872
case acc: 0.4951836
top acc: 0.2181 ::: bot acc: 0.3258
top acc: 0.0743 ::: bot acc: 0.1822
top acc: 0.0706 ::: bot acc: 0.2526
top acc: 0.1142 ::: bot acc: 0.2478
top acc: 0.0249 ::: bot acc: 0.1140
top acc: 0.4345 ::: bot acc: 0.5553
current epoch: 3
train loss is 0.143672
average val loss: 0.129589, accuracy: 0.1271
average test loss: 0.104856, accuracy: 0.1125
case acc: 0.10675127
case acc: 0.04513443
case acc: 0.06833184
case acc: 0.05748449
case acc: 0.08023513
case acc: 0.31721997
top acc: 0.0547 ::: bot acc: 0.1625
top acc: 0.0704 ::: bot acc: 0.0383
top acc: 0.0817 ::: bot acc: 0.0998
top acc: 0.0375 ::: bot acc: 0.0991
top acc: 0.1317 ::: bot acc: 0.0320
top acc: 0.2564 ::: bot acc: 0.3771
current epoch: 4
train loss is 0.122244
average val loss: 0.138103, accuracy: 0.1367
average test loss: 0.104650, accuracy: 0.1106
case acc: 0.115791224
case acc: 0.039551456
case acc: 0.072358906
case acc: 0.069183476
case acc: 0.057440937
case acc: 0.30925235
top acc: 0.0639 ::: bot acc: 0.1714
top acc: 0.0445 ::: bot acc: 0.0644
top acc: 0.0639 ::: bot acc: 0.1173
top acc: 0.0321 ::: bot acc: 0.1192
top acc: 0.1037 ::: bot acc: 0.0200
top acc: 0.2485 ::: bot acc: 0.3692
current epoch: 5
train loss is 0.118682
average val loss: 0.144408, accuracy: 0.1435
average test loss: 0.104945, accuracy: 0.1093
case acc: 0.11917527
case acc: 0.04215994
case acc: 0.07631789
case acc: 0.07849819
case acc: 0.04409097
case acc: 0.29557484
top acc: 0.0673 ::: bot acc: 0.1747
top acc: 0.0246 ::: bot acc: 0.0846
top acc: 0.0526 ::: bot acc: 0.1290
top acc: 0.0327 ::: bot acc: 0.1327
top acc: 0.0818 ::: bot acc: 0.0242
top acc: 0.2348 ::: bot acc: 0.3556
current epoch: 6
train loss is 0.103909
average val loss: 0.108264, accuracy: 0.1065
average test loss: 0.082468, accuracy: 0.0881
case acc: 0.07220984
case acc: 0.04144845
case acc: 0.06635241
case acc: 0.055093847
case acc: 0.06337827
case acc: 0.22989511
top acc: 0.0244 ::: bot acc: 0.1257
top acc: 0.0564 ::: bot acc: 0.0528
top acc: 0.0924 ::: bot acc: 0.0886
top acc: 0.0398 ::: bot acc: 0.0944
top acc: 0.1116 ::: bot acc: 0.0219
top acc: 0.1691 ::: bot acc: 0.2900
current epoch: 7
train loss is 0.093712
average val loss: 0.092481, accuracy: 0.0900
average test loss: 0.073291, accuracy: 0.0784
case acc: 0.054681946
case acc: 0.044010114
case acc: 0.06617387
case acc: 0.04875519
case acc: 0.069104604
case acc: 0.18747255
top acc: 0.0198 ::: bot acc: 0.1019
top acc: 0.0660 ::: bot acc: 0.0433
top acc: 0.1096 ::: bot acc: 0.0713
top acc: 0.0539 ::: bot acc: 0.0778
top acc: 0.1189 ::: bot acc: 0.0246
top acc: 0.1267 ::: bot acc: 0.2475
current epoch: 8
train loss is 0.086529
average val loss: 0.079628, accuracy: 0.0762
average test loss: 0.066890, accuracy: 0.0711
case acc: 0.043338295
case acc: 0.046663437
case acc: 0.0683242
case acc: 0.0464682
case acc: 0.074173406
case acc: 0.1476298
top acc: 0.0251 ::: bot acc: 0.0824
top acc: 0.0742 ::: bot acc: 0.0351
top acc: 0.1247 ::: bot acc: 0.0559
top acc: 0.0698 ::: bot acc: 0.0618
top acc: 0.1246 ::: bot acc: 0.0279
top acc: 0.0870 ::: bot acc: 0.2076
current epoch: 9
train loss is 0.080632
average val loss: 0.067244, accuracy: 0.0624
average test loss: 0.064288, accuracy: 0.0676
case acc: 0.03952267
case acc: 0.051912837
case acc: 0.07429979
case acc: 0.050735433
case acc: 0.08348914
case acc: 0.1058949
top acc: 0.0460 ::: bot acc: 0.0613
top acc: 0.0874 ::: bot acc: 0.0241
top acc: 0.1437 ::: bot acc: 0.0377
top acc: 0.0909 ::: bot acc: 0.0408
top acc: 0.1352 ::: bot acc: 0.0346
top acc: 0.0477 ::: bot acc: 0.1646
current epoch: 10
train loss is 0.076642
average val loss: 0.060416, accuracy: 0.0553
average test loss: 0.062462, accuracy: 0.0652
case acc: 0.039652992
case acc: 0.053920843
case acc: 0.07868233
case acc: 0.05601093
case acc: 0.08499054
case acc: 0.07818004
top acc: 0.0545 ::: bot acc: 0.0527
top acc: 0.0917 ::: bot acc: 0.0216
top acc: 0.1527 ::: bot acc: 0.0326
top acc: 0.1033 ::: bot acc: 0.0311
top acc: 0.1368 ::: bot acc: 0.0358
top acc: 0.0284 ::: bot acc: 0.1328
current epoch: 11
train loss is 0.074199
average val loss: 0.056446, accuracy: 0.0521
average test loss: 0.059853, accuracy: 0.0621
case acc: 0.03959756
case acc: 0.05276557
case acc: 0.07948365
case acc: 0.05909374
case acc: 0.08069438
case acc: 0.0609241
top acc: 0.0538 ::: bot acc: 0.0533
top acc: 0.0891 ::: bot acc: 0.0231
top acc: 0.1541 ::: bot acc: 0.0320
top acc: 0.1091 ::: bot acc: 0.0286
top acc: 0.1320 ::: bot acc: 0.0326
top acc: 0.0229 ::: bot acc: 0.1098
current epoch: 12
train loss is 0.071487
average val loss: 0.055039, accuracy: 0.0518
average test loss: 0.056091, accuracy: 0.0580
case acc: 0.03953873
case acc: 0.049021073
case acc: 0.07676008
case acc: 0.05843903
case acc: 0.071249925
case acc: 0.05269648
top acc: 0.0462 ::: bot acc: 0.0611
top acc: 0.0804 ::: bot acc: 0.0295
top acc: 0.1488 ::: bot acc: 0.0345
top acc: 0.1081 ::: bot acc: 0.0286
top acc: 0.1213 ::: bot acc: 0.0261
top acc: 0.0283 ::: bot acc: 0.0949
current epoch: 13
train loss is 0.066523
average val loss: 0.055262, accuracy: 0.0532
average test loss: 0.052812, accuracy: 0.0542
case acc: 0.040246397
case acc: 0.04549949
case acc: 0.07352791
case acc: 0.056726646
case acc: 0.061778612
case acc: 0.047688592
top acc: 0.0380 ::: bot acc: 0.0694
top acc: 0.0701 ::: bot acc: 0.0395
top acc: 0.1419 ::: bot acc: 0.0388
top acc: 0.1047 ::: bot acc: 0.0301
top acc: 0.1093 ::: bot acc: 0.0216
top acc: 0.0377 ::: bot acc: 0.0829
current epoch: 14
train loss is 0.063009
average val loss: 0.056762, accuracy: 0.0558
average test loss: 0.050434, accuracy: 0.0511
case acc: 0.041369725
case acc: 0.04232562
case acc: 0.07083384
case acc: 0.054267373
case acc: 0.053139303
case acc: 0.04495809
top acc: 0.0310 ::: bot acc: 0.0765
top acc: 0.0591 ::: bot acc: 0.0506
top acc: 0.1343 ::: bot acc: 0.0461
top acc: 0.0994 ::: bot acc: 0.0333
top acc: 0.0972 ::: bot acc: 0.0199
top acc: 0.0473 ::: bot acc: 0.0732
current epoch: 15
train loss is 0.058729
average val loss: 0.060798, accuracy: 0.0607
average test loss: 0.048581, accuracy: 0.0486
case acc: 0.04422086
case acc: 0.040016707
case acc: 0.0680416
case acc: 0.050461486
case acc: 0.04475115
case acc: 0.044387955
top acc: 0.0238 ::: bot acc: 0.0845
top acc: 0.0453 ::: bot acc: 0.0644
top acc: 0.1238 ::: bot acc: 0.0565
top acc: 0.0900 ::: bot acc: 0.0409
top acc: 0.0830 ::: bot acc: 0.0236
top acc: 0.0515 ::: bot acc: 0.0691
current epoch: 16
train loss is 0.055801
average val loss: 0.064493, accuracy: 0.0649
average test loss: 0.048020, accuracy: 0.0474
case acc: 0.04608414
case acc: 0.04000788
case acc: 0.066672966
case acc: 0.04765123
case acc: 0.03978221
case acc: 0.044066075
top acc: 0.0224 ::: bot acc: 0.0880
top acc: 0.0341 ::: bot acc: 0.0756
top acc: 0.1156 ::: bot acc: 0.0646
top acc: 0.0815 ::: bot acc: 0.0491
top acc: 0.0719 ::: bot acc: 0.0311
top acc: 0.0548 ::: bot acc: 0.0658
current epoch: 17
train loss is 0.053027
average val loss: 0.066454, accuracy: 0.0671
average test loss: 0.048076, accuracy: 0.0468
case acc: 0.045187216
case acc: 0.04139387
case acc: 0.06608988
case acc: 0.046496283
case acc: 0.037868638
case acc: 0.04380855
top acc: 0.0230 ::: bot acc: 0.0864
top acc: 0.0273 ::: bot acc: 0.0825
top acc: 0.1112 ::: bot acc: 0.0691
top acc: 0.0757 ::: bot acc: 0.0549
top acc: 0.0658 ::: bot acc: 0.0373
top acc: 0.0578 ::: bot acc: 0.0628
current epoch: 18
train loss is 0.051499
average val loss: 0.066495, accuracy: 0.0672
average test loss: 0.048095, accuracy: 0.0464
case acc: 0.042635564
case acc: 0.042171318
case acc: 0.06596306
case acc: 0.046186816
case acc: 0.037521448
case acc: 0.043690685
top acc: 0.0268 ::: bot acc: 0.0807
top acc: 0.0248 ::: bot acc: 0.0851
top acc: 0.1102 ::: bot acc: 0.0700
top acc: 0.0726 ::: bot acc: 0.0579
top acc: 0.0642 ::: bot acc: 0.0388
top acc: 0.0602 ::: bot acc: 0.0605
current epoch: 19
train loss is 0.050748
average val loss: 0.064755, accuracy: 0.0653
average test loss: 0.048046, accuracy: 0.0461
case acc: 0.040527456
case acc: 0.04169317
case acc: 0.06621343
case acc: 0.046144743
case acc: 0.038111035
case acc: 0.043618403
top acc: 0.0351 ::: bot acc: 0.0723
top acc: 0.0263 ::: bot acc: 0.0836
top acc: 0.1124 ::: bot acc: 0.0679
top acc: 0.0724 ::: bot acc: 0.0581
top acc: 0.0667 ::: bot acc: 0.0363
top acc: 0.0621 ::: bot acc: 0.0584
current epoch: 20
train loss is 0.050413
average val loss: 0.060900, accuracy: 0.0612
average test loss: 0.048406, accuracy: 0.0463
case acc: 0.039552145
case acc: 0.040274985
case acc: 0.06705606
case acc: 0.046536583
case acc: 0.040561117
case acc: 0.043623485
top acc: 0.0466 ::: bot acc: 0.0609
top acc: 0.0326 ::: bot acc: 0.0774
top acc: 0.1186 ::: bot acc: 0.0616
top acc: 0.0763 ::: bot acc: 0.0540
top acc: 0.0739 ::: bot acc: 0.0295
top acc: 0.0660 ::: bot acc: 0.0545
current epoch: 21
train loss is 0.050370
average val loss: 0.056724, accuracy: 0.0568
average test loss: 0.049669, accuracy: 0.0476
case acc: 0.04019141
case acc: 0.039836388
case acc: 0.068762906
case acc: 0.04801989
case acc: 0.045023337
case acc: 0.043920405
top acc: 0.0582 ::: bot acc: 0.0494
top acc: 0.0415 ::: bot acc: 0.0687
top acc: 0.1268 ::: bot acc: 0.0534
top acc: 0.0829 ::: bot acc: 0.0473
top acc: 0.0835 ::: bot acc: 0.0234
top acc: 0.0698 ::: bot acc: 0.0507
current epoch: 22
train loss is 0.050354
average val loss: 0.052657, accuracy: 0.0524
average test loss: 0.052214, accuracy: 0.0505
case acc: 0.04273761
case acc: 0.041115984
case acc: 0.07169106
case acc: 0.051280223
case acc: 0.0517417
case acc: 0.044657294
top acc: 0.0691 ::: bot acc: 0.0384
top acc: 0.0525 ::: bot acc: 0.0576
top acc: 0.1365 ::: bot acc: 0.0436
top acc: 0.0921 ::: bot acc: 0.0385
top acc: 0.0951 ::: bot acc: 0.0202
top acc: 0.0742 ::: bot acc: 0.0462
current epoch: 23
train loss is 0.050402
average val loss: 0.049297, accuracy: 0.0485
average test loss: 0.056384, accuracy: 0.0551
case acc: 0.04609336
case acc: 0.044441275
case acc: 0.076541536
case acc: 0.056633502
case acc: 0.061283298
case acc: 0.045880623
top acc: 0.0795 ::: bot acc: 0.0282
top acc: 0.0659 ::: bot acc: 0.0443
top acc: 0.1480 ::: bot acc: 0.0350
top acc: 0.1043 ::: bot acc: 0.0299
top acc: 0.1087 ::: bot acc: 0.0214
top acc: 0.0798 ::: bot acc: 0.0407
current epoch: 24
train loss is 0.050149
average val loss: 0.047593, accuracy: 0.0465
average test loss: 0.061563, accuracy: 0.0607
case acc: 0.049113084
case acc: 0.048874717
case acc: 0.08232829
case acc: 0.06461819
case acc: 0.07181441
case acc: 0.04761659
top acc: 0.0870 ::: bot acc: 0.0220
top acc: 0.0792 ::: bot acc: 0.0313
top acc: 0.1589 ::: bot acc: 0.0302
top acc: 0.1173 ::: bot acc: 0.0277
top acc: 0.1220 ::: bot acc: 0.0264
top acc: 0.0851 ::: bot acc: 0.0355
current epoch: 25
train loss is 0.050182
average val loss: 0.047577, accuracy: 0.0462
average test loss: 0.069032, accuracy: 0.0686
case acc: 0.052937042
case acc: 0.056000665
case acc: 0.09021952
case acc: 0.076141804
case acc: 0.085261345
case acc: 0.05096289
top acc: 0.0944 ::: bot acc: 0.0184
top acc: 0.0947 ::: bot acc: 0.0212
top acc: 0.1718 ::: bot acc: 0.0279
top acc: 0.1336 ::: bot acc: 0.0293
top acc: 0.1373 ::: bot acc: 0.0359
top acc: 0.0930 ::: bot acc: 0.0294
current epoch: 26
train loss is 0.051263
average val loss: 0.049523, accuracy: 0.0478
average test loss: 0.077464, accuracy: 0.0773
case acc: 0.055947848
case acc: 0.06482909
case acc: 0.09881573
case acc: 0.08943147
case acc: 0.09907121
case acc: 0.055820003
top acc: 0.0993 ::: bot acc: 0.0175
top acc: 0.1095 ::: bot acc: 0.0177
top acc: 0.1839 ::: bot acc: 0.0292
top acc: 0.1507 ::: bot acc: 0.0349
top acc: 0.1518 ::: bot acc: 0.0484
top acc: 0.1023 ::: bot acc: 0.0251
current epoch: 27
train loss is 0.054017
average val loss: 0.050938, accuracy: 0.0491
average test loss: 0.081203, accuracy: 0.0812
case acc: 0.053399332
case acc: 0.06938332
case acc: 0.10209415
case acc: 0.09846847
case acc: 0.10572537
case acc: 0.058083985
top acc: 0.0952 ::: bot acc: 0.0182
top acc: 0.1165 ::: bot acc: 0.0173
top acc: 0.1884 ::: bot acc: 0.0299
top acc: 0.1614 ::: bot acc: 0.0406
top acc: 0.1584 ::: bot acc: 0.0551
top acc: 0.1064 ::: bot acc: 0.0237
current epoch: 28
train loss is 0.057660
average val loss: 0.049623, accuracy: 0.0483
average test loss: 0.075900, accuracy: 0.0764
case acc: 0.04554718
case acc: 0.06504143
case acc: 0.09606646
case acc: 0.097210824
case acc: 0.098824665
case acc: 0.05540964
top acc: 0.0779 ::: bot acc: 0.0296
top acc: 0.1099 ::: bot acc: 0.0177
top acc: 0.1801 ::: bot acc: 0.0287
top acc: 0.1599 ::: bot acc: 0.0397
top acc: 0.1515 ::: bot acc: 0.0482
top acc: 0.1015 ::: bot acc: 0.0254
current epoch: 29
train loss is 0.061036
average val loss: 0.049325, accuracy: 0.0494
average test loss: 0.060674, accuracy: 0.0618
case acc: 0.039603822
case acc: 0.05049647
case acc: 0.07908744
case acc: 0.08042273
case acc: 0.07400205
case acc: 0.046995852
top acc: 0.0440 ::: bot acc: 0.0634
top acc: 0.0830 ::: bot acc: 0.0284
top acc: 0.1530 ::: bot acc: 0.0323
top acc: 0.1393 ::: bot acc: 0.0309
top acc: 0.1245 ::: bot acc: 0.0279
top acc: 0.0833 ::: bot acc: 0.0371
current epoch: 30
train loss is 0.065923
average val loss: 0.068041, accuracy: 0.0688
average test loss: 0.049427, accuracy: 0.0508
case acc: 0.06143106
case acc: 0.04037293
case acc: 0.06535602
case acc: 0.052098386
case acc: 0.040765595
case acc: 0.044640925
top acc: 0.0205 ::: bot acc: 0.1118
top acc: 0.0327 ::: bot acc: 0.0776
top acc: 0.1051 ::: bot acc: 0.0747
top acc: 0.0944 ::: bot acc: 0.0368
top acc: 0.0743 ::: bot acc: 0.0293
top acc: 0.0488 ::: bot acc: 0.0715
current epoch: 31
train loss is 0.066949
average val loss: 0.092436, accuracy: 0.0932
average test loss: 0.057413, accuracy: 0.0583
case acc: 0.08146215
case acc: 0.06102659
case acc: 0.069431625
case acc: 0.04711457
case acc: 0.04022532
case acc: 0.05032872
top acc: 0.0314 ::: bot acc: 0.1362
top acc: 0.0190 ::: bot acc: 0.1162
top acc: 0.0716 ::: bot acc: 0.1082
top acc: 0.0580 ::: bot acc: 0.0725
top acc: 0.0362 ::: bot acc: 0.0671
top acc: 0.0318 ::: bot acc: 0.0895
current epoch: 32
train loss is 0.063172
average val loss: 0.095846, accuracy: 0.0970
average test loss: 0.059313, accuracy: 0.0594
case acc: 0.07363802
case acc: 0.06982217
case acc: 0.07070447
case acc: 0.05021058
case acc: 0.04434705
case acc: 0.047870915
top acc: 0.0254 ::: bot acc: 0.1275
top acc: 0.0247 ::: bot acc: 0.1265
top acc: 0.0665 ::: bot acc: 0.1133
top acc: 0.0463 ::: bot acc: 0.0841
top acc: 0.0287 ::: bot acc: 0.0768
top acc: 0.0372 ::: bot acc: 0.0832
current epoch: 33
train loss is 0.063639
average val loss: 0.085519, accuracy: 0.0870
average test loss: 0.054189, accuracy: 0.0532
case acc: 0.05457228
case acc: 0.062871076
case acc: 0.06795574
case acc: 0.049140036
case acc: 0.04057435
case acc: 0.04433265
top acc: 0.0198 ::: bot acc: 0.1018
top acc: 0.0200 ::: bot acc: 0.1184
top acc: 0.0780 ::: bot acc: 0.1018
top acc: 0.0498 ::: bot acc: 0.0807
top acc: 0.0352 ::: bot acc: 0.0681
top acc: 0.0513 ::: bot acc: 0.0691
current epoch: 34
train loss is 0.062955
average val loss: 0.060864, accuracy: 0.0619
average test loss: 0.049443, accuracy: 0.0466
case acc: 0.039468523
case acc: 0.04186759
case acc: 0.06628201
case acc: 0.046776734
case acc: 0.039307527
case acc: 0.046195805
top acc: 0.0532 ::: bot acc: 0.0540
top acc: 0.0260 ::: bot acc: 0.0842
top acc: 0.1137 ::: bot acc: 0.0661
top acc: 0.0776 ::: bot acc: 0.0528
top acc: 0.0704 ::: bot acc: 0.0328
top acc: 0.0809 ::: bot acc: 0.0395
current epoch: 35
train loss is 0.059438
average val loss: 0.048112, accuracy: 0.0485
average test loss: 0.064315, accuracy: 0.0629
case acc: 0.057564404
case acc: 0.045967847
case acc: 0.08127042
case acc: 0.06416162
case acc: 0.06752103
case acc: 0.06065752
top acc: 0.1019 ::: bot acc: 0.0171
top acc: 0.0709 ::: bot acc: 0.0394
top acc: 0.1570 ::: bot acc: 0.0308
top acc: 0.1166 ::: bot acc: 0.0277
top acc: 0.1168 ::: bot acc: 0.0238
top acc: 0.1107 ::: bot acc: 0.0227
current epoch: 36
train loss is 0.056351
average val loss: 0.049404, accuracy: 0.0486
average test loss: 0.077135, accuracy: 0.0765
case acc: 0.070602536
case acc: 0.057312302
case acc: 0.09499016
case acc: 0.08068826
case acc: 0.09203773
case acc: 0.06365904
top acc: 0.1195 ::: bot acc: 0.0207
top acc: 0.0972 ::: bot acc: 0.0202
top acc: 0.1785 ::: bot acc: 0.0284
top acc: 0.1396 ::: bot acc: 0.0309
top acc: 0.1445 ::: bot acc: 0.0418
top acc: 0.1155 ::: bot acc: 0.0222
current epoch: 37
train loss is 0.053923
average val loss: 0.048951, accuracy: 0.0476
average test loss: 0.075978, accuracy: 0.0758
case acc: 0.06266976
case acc: 0.05999433
case acc: 0.09443518
case acc: 0.08443102
case acc: 0.097238876
case acc: 0.05595961
top acc: 0.1096 ::: bot acc: 0.0169
top acc: 0.1019 ::: bot acc: 0.0189
top acc: 0.1778 ::: bot acc: 0.0283
top acc: 0.1443 ::: bot acc: 0.0327
top acc: 0.1499 ::: bot acc: 0.0467
top acc: 0.1026 ::: bot acc: 0.0251
current epoch: 38
train loss is 0.052959
average val loss: 0.047339, accuracy: 0.0461
average test loss: 0.066463, accuracy: 0.0666
case acc: 0.048188105
case acc: 0.053948738
case acc: 0.08478858
case acc: 0.07846105
case acc: 0.08634557
case acc: 0.048017353
top acc: 0.0850 ::: bot acc: 0.0232
top acc: 0.0908 ::: bot acc: 0.0232
top acc: 0.1632 ::: bot acc: 0.0288
top acc: 0.1367 ::: bot acc: 0.0301
top acc: 0.1385 ::: bot acc: 0.0367
top acc: 0.0861 ::: bot acc: 0.0346
current epoch: 39
train loss is 0.051282
average val loss: 0.049165, accuracy: 0.0487
average test loss: 0.056403, accuracy: 0.0570
case acc: 0.040019087
case acc: 0.046259917
case acc: 0.07423474
case acc: 0.06820022
case acc: 0.0689659
case acc: 0.044411425
top acc: 0.0577 ::: bot acc: 0.0496
top acc: 0.0717 ::: bot acc: 0.0387
top acc: 0.1431 ::: bot acc: 0.0376
top acc: 0.1225 ::: bot acc: 0.0279
top acc: 0.1186 ::: bot acc: 0.0246
top acc: 0.0728 ::: bot acc: 0.0477
current epoch: 40
train loss is 0.050245
average val loss: 0.056911, accuracy: 0.0571
average test loss: 0.049244, accuracy: 0.0498
case acc: 0.040813316
case acc: 0.040477034
case acc: 0.067240775
case acc: 0.055627003
case acc: 0.050761417
case acc: 0.043589327
top acc: 0.0330 ::: bot acc: 0.0743
top acc: 0.0473 ::: bot acc: 0.0631
top acc: 0.1202 ::: bot acc: 0.0595
top acc: 0.1023 ::: bot acc: 0.0311
top acc: 0.0934 ::: bot acc: 0.0206
top acc: 0.0604 ::: bot acc: 0.0601
current epoch: 41
train loss is 0.050598
average val loss: 0.067372, accuracy: 0.0680
average test loss: 0.047688, accuracy: 0.0476
case acc: 0.046829667
case acc: 0.04229555
case acc: 0.065183304
case acc: 0.047687087
case acc: 0.039434444
case acc: 0.044361535
top acc: 0.0219 ::: bot acc: 0.0894
top acc: 0.0249 ::: bot acc: 0.0854
top acc: 0.1009 ::: bot acc: 0.0788
top acc: 0.0817 ::: bot acc: 0.0486
top acc: 0.0708 ::: bot acc: 0.0325
top acc: 0.0515 ::: bot acc: 0.0689
current epoch: 42
train loss is 0.052135
average val loss: 0.070988, accuracy: 0.0718
average test loss: 0.048260, accuracy: 0.0474
case acc: 0.04534868
case acc: 0.04627342
case acc: 0.06542843
case acc: 0.04607014
case acc: 0.03715518
case acc: 0.04424658
top acc: 0.0228 ::: bot acc: 0.0867
top acc: 0.0179 ::: bot acc: 0.0947
top acc: 0.0950 ::: bot acc: 0.0848
top acc: 0.0714 ::: bot acc: 0.0589
top acc: 0.0614 ::: bot acc: 0.0418
top acc: 0.0526 ::: bot acc: 0.0679
current epoch: 43
train loss is 0.053347
average val loss: 0.063266, accuracy: 0.0639
average test loss: 0.047975, accuracy: 0.0462
case acc: 0.039692905
case acc: 0.042297117
case acc: 0.0655013
case acc: 0.04674474
case acc: 0.039548375
case acc: 0.043607306
top acc: 0.0421 ::: bot acc: 0.0652
top acc: 0.0250 ::: bot acc: 0.0854
top acc: 0.1069 ::: bot acc: 0.0729
top acc: 0.0775 ::: bot acc: 0.0528
top acc: 0.0711 ::: bot acc: 0.0321
top acc: 0.0658 ::: bot acc: 0.0547
current epoch: 44
train loss is 0.052950
average val loss: 0.051161, accuracy: 0.0515
average test loss: 0.053891, accuracy: 0.0522
case acc: 0.044803195
case acc: 0.040960316
case acc: 0.07072704
case acc: 0.05457369
case acc: 0.053773355
case acc: 0.048534926
top acc: 0.0760 ::: bot acc: 0.0313
top acc: 0.0514 ::: bot acc: 0.0590
top acc: 0.1339 ::: bot acc: 0.0458
top acc: 0.0999 ::: bot acc: 0.0327
top acc: 0.0982 ::: bot acc: 0.0199
top acc: 0.0876 ::: bot acc: 0.0333
current epoch: 45
train loss is 0.051668
average val loss: 0.047333, accuracy: 0.0472
average test loss: 0.065773, accuracy: 0.0650
case acc: 0.057627574
case acc: 0.048698705
case acc: 0.08257611
case acc: 0.06932739
case acc: 0.075743616
case acc: 0.055955272
top acc: 0.1020 ::: bot acc: 0.0171
top acc: 0.0788 ::: bot acc: 0.0319
top acc: 0.1592 ::: bot acc: 0.0301
top acc: 0.1241 ::: bot acc: 0.0281
top acc: 0.1264 ::: bot acc: 0.0291
top acc: 0.1027 ::: bot acc: 0.0251
current epoch: 46
train loss is 0.050751
average val loss: 0.047964, accuracy: 0.0472
average test loss: 0.072303, accuracy: 0.0720
case acc: 0.061345063
case acc: 0.055487312
case acc: 0.0893059
case acc: 0.07965502
case acc: 0.08972323
case acc: 0.056279417
top acc: 0.1077 ::: bot acc: 0.0168
top acc: 0.0939 ::: bot acc: 0.0216
top acc: 0.1703 ::: bot acc: 0.0279
top acc: 0.1382 ::: bot acc: 0.0306
top acc: 0.1421 ::: bot acc: 0.0397
top acc: 0.1032 ::: bot acc: 0.0249
current epoch: 47
train loss is 0.050311
average val loss: 0.047534, accuracy: 0.0465
average test loss: 0.069860, accuracy: 0.0698
case acc: 0.053662963
case acc: 0.05563738
case acc: 0.087026656
case acc: 0.08119906
case acc: 0.090005755
case acc: 0.051463224
top acc: 0.0956 ::: bot acc: 0.0181
top acc: 0.0941 ::: bot acc: 0.0215
top acc: 0.1668 ::: bot acc: 0.0281
top acc: 0.1402 ::: bot acc: 0.0312
top acc: 0.1424 ::: bot acc: 0.0399
top acc: 0.0941 ::: bot acc: 0.0288
current epoch: 48
train loss is 0.049490
average val loss: 0.047439, accuracy: 0.0467
average test loss: 0.061500, accuracy: 0.0620
case acc: 0.043848384
case acc: 0.049918987
case acc: 0.07870547
case acc: 0.07465175
case acc: 0.07835691
case acc: 0.046284925
top acc: 0.0730 ::: bot acc: 0.0342
top acc: 0.0817 ::: bot acc: 0.0295
top acc: 0.1522 ::: bot acc: 0.0325
top acc: 0.1315 ::: bot acc: 0.0291
top acc: 0.1295 ::: bot acc: 0.0309
top acc: 0.0813 ::: bot acc: 0.0392
current epoch: 49
train loss is 0.048856
average val loss: 0.051508, accuracy: 0.0515
average test loss: 0.052832, accuracy: 0.0535
case acc: 0.03936969
case acc: 0.043141473
case acc: 0.07013402
case acc: 0.0633162
case acc: 0.06100755
case acc: 0.04376899
top acc: 0.0485 ::: bot acc: 0.0588
top acc: 0.0613 ::: bot acc: 0.0491
top acc: 0.1320 ::: bot acc: 0.0477
top acc: 0.1153 ::: bot acc: 0.0278
top acc: 0.1083 ::: bot acc: 0.0214
top acc: 0.0689 ::: bot acc: 0.0516
current epoch: 50
train loss is 0.049007
average val loss: 0.060220, accuracy: 0.0606
average test loss: 0.048036, accuracy: 0.0483
case acc: 0.04173538
case acc: 0.039897326
case acc: 0.06599438
case acc: 0.05248617
case acc: 0.045823324
case acc: 0.043699354
top acc: 0.0291 ::: bot acc: 0.0781
top acc: 0.0386 ::: bot acc: 0.0719
top acc: 0.1115 ::: bot acc: 0.0682
top acc: 0.0952 ::: bot acc: 0.0360
top acc: 0.0849 ::: bot acc: 0.0229
top acc: 0.0582 ::: bot acc: 0.0622
LME_Co_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_EMA125', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_Close_bollinger125']
features ['LME_Co_Close', 'LME_Co_Close_EMA12', 'LME_Co_Close_EMA125', 'LME_Co_Close_EMA26', 'LME_Co_Close_EMA40', 'LME_Co_Close_EMA65', 'LME_Co_Close_bollinger12', 'LME_Co_Close_bollinger125', 'LME_Co_Close_bollinger26', 'LME_Co_Close_bollinger40', 'LME_Co_Close_bollinger65', 'LME_Co_High', 'LME_Co_Low']
LME_Al_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_EMA125', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_Close_bollinger125']
features ['LME_Al_Close', 'LME_Al_Close_EMA12', 'LME_Al_Close_EMA125', 'LME_Al_Close_EMA26', 'LME_Al_Close_EMA40', 'LME_Al_Close_EMA65', 'LME_Al_Close_bollinger12', 'LME_Al_Close_bollinger125', 'LME_Al_Close_bollinger26', 'LME_Al_Close_bollinger40', 'LME_Al_Close_bollinger65', 'LME_Al_High', 'LME_Al_Low']
LME_Ni_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_Close_bollinger125']
features ['LME_Ni_Close', 'LME_Ni_Close_EMA12', 'LME_Ni_Close_EMA125', 'LME_Ni_Close_EMA26', 'LME_Ni_Close_EMA40', 'LME_Ni_Close_EMA65', 'LME_Ni_Close_bollinger12', 'LME_Ni_Close_bollinger125', 'LME_Ni_Close_bollinger26', 'LME_Ni_Close_bollinger40', 'LME_Ni_Close_bollinger65', 'LME_Ni_High', 'LME_Ni_Low']
LME_Ti_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_Close_bollinger125']
features ['LME_Ti_Close', 'LME_Ti_Close_EMA12', 'LME_Ti_Close_EMA125', 'LME_Ti_Close_EMA26', 'LME_Ti_Close_EMA40', 'LME_Ti_Close_EMA65', 'LME_Ti_Close_bollinger12', 'LME_Ti_Close_bollinger125', 'LME_Ti_Close_bollinger26', 'LME_Ti_Close_bollinger40', 'LME_Ti_Close_bollinger65', 'LME_Ti_High', 'LME_Ti_Low']
LME_Zi_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_Close_bollinger125']
features ['LME_Zi_Close', 'LME_Zi_Close_EMA12', 'LME_Zi_Close_EMA125', 'LME_Zi_Close_EMA26', 'LME_Zi_Close_EMA40', 'LME_Zi_Close_EMA65', 'LME_Zi_Close_bollinger12', 'LME_Zi_Close_bollinger125', 'LME_Zi_Close_bollinger26', 'LME_Zi_Close_bollinger40', 'LME_Zi_Close_bollinger65', 'LME_Zi_High', 'LME_Zi_Low']
LME_Le_Spot
Before Load Data
['2012-01-01', '2012-07-01', '2016-07-01', '2017-01-01', '2017-07-01']
Normalizing Spread:LME_Al_Close=>LME_Al_Spot
Normalizing Spread:LME_Co_Close=>LME_Co_Spot
Normalizing Spread:LME_Ti_Close=>LME_Ti_Spot
Normalizing Spread:LME_Le_Close=>LME_Le_Spot
Normalizing Spread:LME_Ni_Close=>LME_Ni_Spot
Normalizing Spread:LME_Zi_Close=>LME_Zi_Spot
====================================technical indicator========================================
origin features ['LME_Co_Spot', 'LME_Al_Spot', 'LME_Le_Spot', 'LME_Ni_Spot', 'LME_Zi_Spot', 'LME_Ti_Spot', 'LME_Al_Open', 'LME_Al_High', 'LME_Al_Low', 'LME_Al_Close', 'LME_Al_Volume', 'LME_Co_Open', 'LME_Co_High', 'LME_Co_Low', 'LME_Co_Close', 'LME_Co_Volume', 'LME_Ti_Open', 'LME_Ti_High', 'LME_Ti_Low', 'LME_Ti_Close', 'LME_Ti_Volume', 'LME_Le_Open', 'LME_Le_High', 'LME_Le_Low', 'LME_Le_Close', 'LME_Le_Volume', 'LME_Ni_Open', 'LME_Ni_High', 'LME_Ni_Low', 'LME_Ni_Close', 'LME_Ni_Volume', 'LME_Zi_Open', 'LME_Zi_High', 'LME_Zi_Low', 'LME_Zi_Close', 'LME_Zi_Volume', 'LME_Al_n3MSpread', 'LME_Co_n3MSpread', 'LME_Ti_n3MSpread', 'LME_Le_n3MSpread', 'LME_Ni_n3MSpread', 'LME_Zi_n3MSpread', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_EMA125', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_Close_bollinger125']
features ['LME_Le_Close', 'LME_Le_Close_EMA12', 'LME_Le_Close_EMA125', 'LME_Le_Close_EMA26', 'LME_Le_Close_EMA40', 'LME_Le_Close_EMA65', 'LME_Le_Close_bollinger12', 'LME_Le_Close_bollinger125', 'LME_Le_Close_bollinger26', 'LME_Le_Close_bollinger40', 'LME_Le_Close_bollinger65', 'LME_Le_High', 'LME_Le_Low']
Dataset statistic: #examples
Train: 5346 5346 5346
1.7082474 -0.6288155 0.25454274 -0.21218425
Validation: 594 594 594
Testing: 768 768 768
pre-processing time: 0.00046634674072265625
the split date is 2012-07-01
net initializing with time: 0.0037202835083007812
preparing training and testing date with time: 2.384185791015625e-07
current epoch: 1
train loss is 0.163640
average val loss: 0.077709, accuracy: 0.0756
average test loss: 0.078905, accuracy: 0.0814
case acc: 0.09448253
case acc: 0.12594609
case acc: 0.10552399
case acc: 0.04337319
case acc: 0.06424224
case acc: 0.055095546
top acc: 0.0623 ::: bot acc: 0.1359
top acc: 0.0827 ::: bot acc: 0.1670
top acc: 0.0261 ::: bot acc: 0.1818
top acc: 0.0142 ::: bot acc: 0.0782
top acc: 0.0232 ::: bot acc: 0.1076
top acc: 0.0837 ::: bot acc: 0.0645
current epoch: 2
train loss is 0.098255
average val loss: 0.149557, accuracy: 0.1493
average test loss: 0.150553, accuracy: 0.1509
case acc: 0.1554693
case acc: 0.21343735
case acc: 0.19305131
case acc: 0.124991156
case acc: 0.13985902
case acc: 0.07844444
top acc: 0.0760 ::: bot acc: 0.2204
top acc: 0.1703 ::: bot acc: 0.2548
top acc: 0.1094 ::: bot acc: 0.2720
top acc: 0.0901 ::: bot acc: 0.1629
top acc: 0.0752 ::: bot acc: 0.1948
top acc: 0.0296 ::: bot acc: 0.1433
current epoch: 3
train loss is 0.137821
average val loss: 0.062678, accuracy: 0.0606
average test loss: 0.063290, accuracy: 0.0667
case acc: 0.071361706
case acc: 0.09319042
case acc: 0.08565004
case acc: 0.027301963
case acc: 0.052565686
case acc: 0.07030107
top acc: 0.0643 ::: bot acc: 0.1012
top acc: 0.0502 ::: bot acc: 0.1347
top acc: 0.0200 ::: bot acc: 0.1563
top acc: 0.0229 ::: bot acc: 0.0498
top acc: 0.0367 ::: bot acc: 0.0838
top acc: 0.1203 ::: bot acc: 0.0374
current epoch: 4
train loss is 0.105193
average val loss: 0.058936, accuracy: 0.0583
average test loss: 0.058389, accuracy: 0.0606
case acc: 0.057235807
case acc: 0.037038937
case acc: 0.06162883
case acc: 0.04779028
case acc: 0.045749325
case acc: 0.11398378
top acc: 0.1164 ::: bot acc: 0.0376
top acc: 0.0181 ::: bot acc: 0.0669
top acc: 0.0711 ::: bot acc: 0.0920
top acc: 0.0794 ::: bot acc: 0.0165
top acc: 0.0925 ::: bot acc: 0.0272
top acc: 0.1759 ::: bot acc: 0.0565
current epoch: 5
train loss is 0.073186
average val loss: 0.055556, accuracy: 0.0529
average test loss: 0.055747, accuracy: 0.0585
case acc: 0.061583392
case acc: 0.06661693
case acc: 0.07501107
case acc: 0.026764289
case acc: 0.049834453
case acc: 0.07122198
top acc: 0.0707 ::: bot acc: 0.0838
top acc: 0.0238 ::: bot acc: 0.1083
top acc: 0.0299 ::: bot acc: 0.1357
top acc: 0.0279 ::: bot acc: 0.0450
top acc: 0.0425 ::: bot acc: 0.0769
top acc: 0.1216 ::: bot acc: 0.0372
current epoch: 6
train loss is 0.075196
average val loss: 0.061475, accuracy: 0.0587
average test loss: 0.062163, accuracy: 0.0645
case acc: 0.071518034
case acc: 0.07937765
case acc: 0.08285022
case acc: 0.035540547
case acc: 0.059554458
case acc: 0.058381446
top acc: 0.0645 ::: bot acc: 0.1017
top acc: 0.0362 ::: bot acc: 0.1214
top acc: 0.0211 ::: bot acc: 0.1517
top acc: 0.0104 ::: bot acc: 0.0689
top acc: 0.0267 ::: bot acc: 0.0991
top acc: 0.0955 ::: bot acc: 0.0514
current epoch: 7
train loss is 0.079622
average val loss: 0.053758, accuracy: 0.0503
average test loss: 0.053856, accuracy: 0.0556
case acc: 0.060566224
case acc: 0.055215873
case acc: 0.07267284
case acc: 0.028981002
case acc: 0.052708507
case acc: 0.063733675
top acc: 0.0731 ::: bot acc: 0.0812
top acc: 0.0140 ::: bot acc: 0.0964
top acc: 0.0344 ::: bot acc: 0.1300
top acc: 0.0177 ::: bot acc: 0.0555
top acc: 0.0353 ::: bot acc: 0.0847
top acc: 0.1076 ::: bot acc: 0.0430
current epoch: 8
train loss is 0.073215
average val loss: 0.050183, accuracy: 0.0462
average test loss: 0.049727, accuracy: 0.0509
case acc: 0.056689184
case acc: 0.039182283
case acc: 0.06509476
case acc: 0.02688467
case acc: 0.047624584
case acc: 0.06996669
top acc: 0.0929 ::: bot acc: 0.0614
top acc: 0.0149 ::: bot acc: 0.0721
top acc: 0.0540 ::: bot acc: 0.1089
top acc: 0.0309 ::: bot acc: 0.0425
top acc: 0.0484 ::: bot acc: 0.0705
top acc: 0.1192 ::: bot acc: 0.0381
current epoch: 9
train loss is 0.068730
average val loss: 0.049878, accuracy: 0.0460
average test loss: 0.049107, accuracy: 0.0496
case acc: 0.05648419
case acc: 0.03365153
case acc: 0.062107563
case acc: 0.027129449
case acc: 0.045557175
case acc: 0.07240833
top acc: 0.1051 ::: bot acc: 0.0495
top acc: 0.0300 ::: bot acc: 0.0556
top acc: 0.0677 ::: bot acc: 0.0952
top acc: 0.0373 ::: bot acc: 0.0363
top acc: 0.0562 ::: bot acc: 0.0626
top acc: 0.1233 ::: bot acc: 0.0373
current epoch: 10
train loss is 0.065357
average val loss: 0.052128, accuracy: 0.0489
average test loss: 0.051104, accuracy: 0.0504
case acc: 0.05811011
case acc: 0.03274475
case acc: 0.060251236
case acc: 0.029702593
case acc: 0.043338858
case acc: 0.07821354
top acc: 0.1209 ::: bot acc: 0.0338
top acc: 0.0505 ::: bot acc: 0.0352
top acc: 0.0855 ::: bot acc: 0.0774
top acc: 0.0486 ::: bot acc: 0.0250
top acc: 0.0696 ::: bot acc: 0.0491
top acc: 0.1314 ::: bot acc: 0.0381
current epoch: 11
train loss is 0.064003
average val loss: 0.058577, accuracy: 0.0558
average test loss: 0.057839, accuracy: 0.0557
case acc: 0.0641003
case acc: 0.039367236
case acc: 0.06207361
case acc: 0.0370188
case acc: 0.044289302
case acc: 0.08706345
top acc: 0.1396 ::: bot acc: 0.0167
top acc: 0.0742 ::: bot acc: 0.0146
top acc: 0.1068 ::: bot acc: 0.0559
top acc: 0.0646 ::: bot acc: 0.0148
top acc: 0.0883 ::: bot acc: 0.0302
top acc: 0.1433 ::: bot acc: 0.0409
current epoch: 12
train loss is 0.062930
average val loss: 0.065068, accuracy: 0.0627
average test loss: 0.064947, accuracy: 0.0627
case acc: 0.07061163
case acc: 0.052160166
case acc: 0.065373145
case acc: 0.044678442
case acc: 0.05132992
case acc: 0.0919336
top acc: 0.1518 ::: bot acc: 0.0115
top acc: 0.0913 ::: bot acc: 0.0185
top acc: 0.1225 ::: bot acc: 0.0403
top acc: 0.0759 ::: bot acc: 0.0149
top acc: 0.1034 ::: bot acc: 0.0211
top acc: 0.1493 ::: bot acc: 0.0432
current epoch: 13
train loss is 0.062029
average val loss: 0.071805, accuracy: 0.0699
average test loss: 0.072329, accuracy: 0.0704
case acc: 0.07725608
case acc: 0.06399606
case acc: 0.070648536
case acc: 0.053230375
case acc: 0.061808858
case acc: 0.09539932
top acc: 0.1613 ::: bot acc: 0.0126
top acc: 0.1055 ::: bot acc: 0.0257
top acc: 0.1358 ::: bot acc: 0.0291
top acc: 0.0865 ::: bot acc: 0.0194
top acc: 0.1187 ::: bot acc: 0.0219
top acc: 0.1536 ::: bot acc: 0.0450
current epoch: 14
train loss is 0.060960
average val loss: 0.075271, accuracy: 0.0737
average test loss: 0.076144, accuracy: 0.0744
case acc: 0.07935338
case acc: 0.06987163
case acc: 0.07391287
case acc: 0.05823189
case acc: 0.07073662
case acc: 0.09450434
top acc: 0.1639 ::: bot acc: 0.0138
top acc: 0.1123 ::: bot acc: 0.0298
top acc: 0.1427 ::: bot acc: 0.0250
top acc: 0.0923 ::: bot acc: 0.0230
top acc: 0.1296 ::: bot acc: 0.0268
top acc: 0.1527 ::: bot acc: 0.0444
current epoch: 15
train loss is 0.059928
average val loss: 0.073269, accuracy: 0.0722
average test loss: 0.074013, accuracy: 0.0724
case acc: 0.0746876
case acc: 0.06723091
case acc: 0.07305907
case acc: 0.0571037
case acc: 0.0740255
case acc: 0.08822103
top acc: 0.1580 ::: bot acc: 0.0118
top acc: 0.1093 ::: bot acc: 0.0279
top acc: 0.1410 ::: bot acc: 0.0260
top acc: 0.0909 ::: bot acc: 0.0222
top acc: 0.1335 ::: bot acc: 0.0288
top acc: 0.1448 ::: bot acc: 0.0413
current epoch: 16
train loss is 0.058054
average val loss: 0.070765, accuracy: 0.0703
average test loss: 0.071299, accuracy: 0.0699
case acc: 0.06994128
case acc: 0.062607974
case acc: 0.07138623
case acc: 0.055857714
case acc: 0.07669655
case acc: 0.08266919
top acc: 0.1508 ::: bot acc: 0.0118
top acc: 0.1038 ::: bot acc: 0.0248
top acc: 0.1374 ::: bot acc: 0.0282
top acc: 0.0894 ::: bot acc: 0.0213
top acc: 0.1366 ::: bot acc: 0.0306
top acc: 0.1376 ::: bot acc: 0.0392
current epoch: 17
train loss is 0.056148
average val loss: 0.068298, accuracy: 0.0683
average test loss: 0.068617, accuracy: 0.0674
case acc: 0.066145614
case acc: 0.057014354
case acc: 0.069339156
case acc: 0.05489775
case acc: 0.07875199
case acc: 0.07853055
top acc: 0.1438 ::: bot acc: 0.0146
top acc: 0.0972 ::: bot acc: 0.0213
top acc: 0.1329 ::: bot acc: 0.0312
top acc: 0.0883 ::: bot acc: 0.0206
top acc: 0.1389 ::: bot acc: 0.0320
top acc: 0.1319 ::: bot acc: 0.0381
current epoch: 18
train loss is 0.054922
average val loss: 0.061820, accuracy: 0.0625
average test loss: 0.061305, accuracy: 0.0605
case acc: 0.060412817
case acc: 0.045085296
case acc: 0.0646925
case acc: 0.04807972
case acc: 0.073551014
case acc: 0.070922166
top acc: 0.1299 ::: bot acc: 0.0250
top acc: 0.0824 ::: bot acc: 0.0152
top acc: 0.1203 ::: bot acc: 0.0423
top acc: 0.0803 ::: bot acc: 0.0164
top acc: 0.1329 ::: bot acc: 0.0285
top acc: 0.1208 ::: bot acc: 0.0377
current epoch: 19
train loss is 0.052701
average val loss: 0.055283, accuracy: 0.0565
average test loss: 0.054138, accuracy: 0.0539
case acc: 0.057154153
case acc: 0.03513883
case acc: 0.061627436
case acc: 0.039954387
case acc: 0.06508323
case acc: 0.06444518
top acc: 0.1142 ::: bot acc: 0.0406
top acc: 0.0646 ::: bot acc: 0.0211
top acc: 0.1043 ::: bot acc: 0.0583
top acc: 0.0695 ::: bot acc: 0.0135
top acc: 0.1228 ::: bot acc: 0.0234
top acc: 0.1086 ::: bot acc: 0.0430
current epoch: 20
train loss is 0.051717
average val loss: 0.050028, accuracy: 0.0515
average test loss: 0.048784, accuracy: 0.0492
case acc: 0.05647529
case acc: 0.032421377
case acc: 0.060194395
case acc: 0.03268668
case acc: 0.054505367
case acc: 0.058806382
top acc: 0.0970 ::: bot acc: 0.0577
top acc: 0.0445 ::: bot acc: 0.0412
top acc: 0.0854 ::: bot acc: 0.0772
top acc: 0.0561 ::: bot acc: 0.0189
top acc: 0.1083 ::: bot acc: 0.0208
top acc: 0.0954 ::: bot acc: 0.0528
current epoch: 21
train loss is 0.052607
average val loss: 0.047847, accuracy: 0.0494
average test loss: 0.047274, accuracy: 0.0476
case acc: 0.060192145
case acc: 0.036943696
case acc: 0.06366559
case acc: 0.0271327
case acc: 0.0436553
case acc: 0.053878788
top acc: 0.0751 ::: bot acc: 0.0797
top acc: 0.0190 ::: bot acc: 0.0669
top acc: 0.0601 ::: bot acc: 0.1026
top acc: 0.0361 ::: bot acc: 0.0377
top acc: 0.0855 ::: bot acc: 0.0329
top acc: 0.0775 ::: bot acc: 0.0704
current epoch: 22
train loss is 0.056652
average val loss: 0.052792, accuracy: 0.0541
average test loss: 0.053024, accuracy: 0.0540
case acc: 0.07112123
case acc: 0.051902104
case acc: 0.07235514
case acc: 0.030373698
case acc: 0.0446304
case acc: 0.05339558
top acc: 0.0649 ::: bot acc: 0.1010
top acc: 0.0119 ::: bot acc: 0.0926
top acc: 0.0345 ::: bot acc: 0.1291
top acc: 0.0148 ::: bot acc: 0.0591
top acc: 0.0596 ::: bot acc: 0.0588
top acc: 0.0601 ::: bot acc: 0.0878
current epoch: 23
train loss is 0.060949
average val loss: 0.064046, accuracy: 0.0643
average test loss: 0.064954, accuracy: 0.0660
case acc: 0.08299002
case acc: 0.073956616
case acc: 0.084719405
case acc: 0.04464247
case acc: 0.05323372
case acc: 0.056212462
top acc: 0.0621 ::: bot acc: 0.1201
top acc: 0.0307 ::: bot acc: 0.1163
top acc: 0.0196 ::: bot acc: 0.1548
top acc: 0.0147 ::: bot acc: 0.0804
top acc: 0.0334 ::: bot acc: 0.0862
top acc: 0.0445 ::: bot acc: 0.1034
current epoch: 24
train loss is 0.064968
average val loss: 0.070879, accuracy: 0.0704
average test loss: 0.072119, accuracy: 0.0729
case acc: 0.08675346
case acc: 0.08352747
case acc: 0.0933006
case acc: 0.054280087
case acc: 0.062339645
case acc: 0.05734108
top acc: 0.0618 ::: bot acc: 0.1259
top acc: 0.0402 ::: bot acc: 0.1259
top acc: 0.0192 ::: bot acc: 0.1678
top acc: 0.0214 ::: bot acc: 0.0914
top acc: 0.0245 ::: bot acc: 0.1041
top acc: 0.0404 ::: bot acc: 0.1075
current epoch: 25
train loss is 0.070041
average val loss: 0.067407, accuracy: 0.0663
average test loss: 0.068487, accuracy: 0.0693
case acc: 0.07972743
case acc: 0.07433931
case acc: 0.089985274
case acc: 0.052328818
case acc: 0.06472308
case acc: 0.054988127
top acc: 0.0624 ::: bot acc: 0.1152
top acc: 0.0311 ::: bot acc: 0.1168
top acc: 0.0187 ::: bot acc: 0.1631
top acc: 0.0199 ::: bot acc: 0.0892
top acc: 0.0235 ::: bot acc: 0.1081
top acc: 0.0495 ::: bot acc: 0.0984
current epoch: 26
train loss is 0.069034
average val loss: 0.052257, accuracy: 0.0492
average test loss: 0.052375, accuracy: 0.0528
case acc: 0.059972897
case acc: 0.04304235
case acc: 0.07300095
case acc: 0.03266412
case acc: 0.053539377
case acc: 0.05438092
top acc: 0.0751 ::: bot acc: 0.0795
top acc: 0.0116 ::: bot acc: 0.0798
top acc: 0.0330 ::: bot acc: 0.1309
top acc: 0.0117 ::: bot acc: 0.0640
top acc: 0.0326 ::: bot acc: 0.0871
top acc: 0.0799 ::: bot acc: 0.0680
current epoch: 27
train loss is 0.061407
average val loss: 0.048954, accuracy: 0.0451
average test loss: 0.048087, accuracy: 0.0481
case acc: 0.056711048
case acc: 0.032492835
case acc: 0.06262973
case acc: 0.027097275
case acc: 0.045876253
case acc: 0.06354352
top acc: 0.1088 ::: bot acc: 0.0459
top acc: 0.0429 ::: bot acc: 0.0428
top acc: 0.0646 ::: bot acc: 0.0980
top acc: 0.0339 ::: bot acc: 0.0398
top acc: 0.0538 ::: bot acc: 0.0646
top acc: 0.1066 ::: bot acc: 0.0442
current epoch: 28
train loss is 0.060209
average val loss: 0.060689, accuracy: 0.0575
average test loss: 0.060281, accuracy: 0.0582
case acc: 0.0692786
case acc: 0.049889922
case acc: 0.062117517
case acc: 0.03919326
case acc: 0.044541657
case acc: 0.0843847
top acc: 0.1496 ::: bot acc: 0.0117
top acc: 0.0883 ::: bot acc: 0.0173
top acc: 0.1071 ::: bot acc: 0.0554
top acc: 0.0682 ::: bot acc: 0.0138
top acc: 0.0892 ::: bot acc: 0.0292
top acc: 0.1398 ::: bot acc: 0.0398
current epoch: 29
train loss is 0.063304
average val loss: 0.085016, accuracy: 0.0832
average test loss: 0.086249, accuracy: 0.0851
case acc: 0.100215286
case acc: 0.087224945
case acc: 0.0769212
case acc: 0.06827086
case acc: 0.06962511
case acc: 0.108388916
top acc: 0.1861 ::: bot acc: 0.0313
top acc: 0.1304 ::: bot acc: 0.0449
top acc: 0.1482 ::: bot acc: 0.0227
top acc: 0.1028 ::: bot acc: 0.0315
top acc: 0.1282 ::: bot acc: 0.0260
top acc: 0.1694 ::: bot acc: 0.0523
current epoch: 30
train loss is 0.067768
average val loss: 0.088874, accuracy: 0.0877
average test loss: 0.090345, accuracy: 0.0894
case acc: 0.101628125
case acc: 0.09357692
case acc: 0.08182022
case acc: 0.073231265
case acc: 0.08009908
case acc: 0.10612201
top acc: 0.1878 ::: bot acc: 0.0327
top acc: 0.1369 ::: bot acc: 0.0511
top acc: 0.1563 ::: bot acc: 0.0210
top acc: 0.1081 ::: bot acc: 0.0358
top acc: 0.1404 ::: bot acc: 0.0329
top acc: 0.1668 ::: bot acc: 0.0508
current epoch: 31
train loss is 0.066792
average val loss: 0.077902, accuracy: 0.0767
average test loss: 0.078953, accuracy: 0.0777
case acc: 0.083576694
case acc: 0.07768358
case acc: 0.074546576
case acc: 0.06301174
case acc: 0.07672514
case acc: 0.090729
top acc: 0.1688 ::: bot acc: 0.0163
top acc: 0.1205 ::: bot acc: 0.0361
top acc: 0.1437 ::: bot acc: 0.0244
top acc: 0.0972 ::: bot acc: 0.0270
top acc: 0.1366 ::: bot acc: 0.0305
top acc: 0.1478 ::: bot acc: 0.0427
current epoch: 32
train loss is 0.061643
average val loss: 0.066725, accuracy: 0.0662
average test loss: 0.066968, accuracy: 0.0657
case acc: 0.06836245
case acc: 0.058834106
case acc: 0.06677301
case acc: 0.052383896
case acc: 0.07079332
case acc: 0.077342026
top acc: 0.1482 ::: bot acc: 0.0125
top acc: 0.0994 ::: bot acc: 0.0222
top acc: 0.1265 ::: bot acc: 0.0362
top acc: 0.0855 ::: bot acc: 0.0188
top acc: 0.1297 ::: bot acc: 0.0266
top acc: 0.1304 ::: bot acc: 0.0378
current epoch: 33
train loss is 0.054951
average val loss: 0.058040, accuracy: 0.0582
average test loss: 0.057089, accuracy: 0.0564
case acc: 0.06009012
case acc: 0.041724253
case acc: 0.06227099
case acc: 0.04310353
case acc: 0.06344432
case acc: 0.06795409
top acc: 0.1286 ::: bot acc: 0.0266
top acc: 0.0777 ::: bot acc: 0.0144
top acc: 0.1084 ::: bot acc: 0.0540
top acc: 0.0740 ::: bot acc: 0.0141
top acc: 0.1208 ::: bot acc: 0.0224
top acc: 0.1156 ::: bot acc: 0.0394
current epoch: 34
train loss is 0.050218
average val loss: 0.051252, accuracy: 0.0520
average test loss: 0.049913, accuracy: 0.0500
case acc: 0.05690816
case acc: 0.033322055
case acc: 0.060125202
case acc: 0.034758102
case acc: 0.054016165
case acc: 0.060866214
top acc: 0.1093 ::: bot acc: 0.0460
top acc: 0.0555 ::: bot acc: 0.0304
top acc: 0.0887 ::: bot acc: 0.0737
top acc: 0.0602 ::: bot acc: 0.0167
top acc: 0.1076 ::: bot acc: 0.0208
top acc: 0.1009 ::: bot acc: 0.0479
current epoch: 35
train loss is 0.048879
average val loss: 0.047460, accuracy: 0.0484
average test loss: 0.046618, accuracy: 0.0468
case acc: 0.057543546
case acc: 0.03349009
case acc: 0.062366966
case acc: 0.027965426
case acc: 0.044182207
case acc: 0.055145007
top acc: 0.0883 ::: bot acc: 0.0670
top acc: 0.0315 ::: bot acc: 0.0544
top acc: 0.0658 ::: bot acc: 0.0967
top acc: 0.0419 ::: bot acc: 0.0321
top acc: 0.0877 ::: bot acc: 0.0307
top acc: 0.0839 ::: bot acc: 0.0638
current epoch: 36
train loss is 0.050493
average val loss: 0.048806, accuracy: 0.0493
average test loss: 0.048625, accuracy: 0.0491
case acc: 0.06196126
case acc: 0.040479593
case acc: 0.067975014
case acc: 0.027685877
case acc: 0.04348263
case acc: 0.05291886
top acc: 0.0714 ::: bot acc: 0.0843
top acc: 0.0139 ::: bot acc: 0.0749
top acc: 0.0451 ::: bot acc: 0.1175
top acc: 0.0241 ::: bot acc: 0.0499
top acc: 0.0663 ::: bot acc: 0.0522
top acc: 0.0695 ::: bot acc: 0.0780
current epoch: 37
train loss is 0.053301
average val loss: 0.051955, accuracy: 0.0518
average test loss: 0.052116, accuracy: 0.0526
case acc: 0.06571055
case acc: 0.046432123
case acc: 0.07260455
case acc: 0.031164601
case acc: 0.046936773
case acc: 0.053011686
top acc: 0.0668 ::: bot acc: 0.0921
top acc: 0.0104 ::: bot acc: 0.0854
top acc: 0.0336 ::: bot acc: 0.1301
top acc: 0.0135 ::: bot acc: 0.0611
top acc: 0.0502 ::: bot acc: 0.0684
top acc: 0.0632 ::: bot acc: 0.0842
current epoch: 38
train loss is 0.055133
average val loss: 0.051786, accuracy: 0.0506
average test loss: 0.051937, accuracy: 0.0523
case acc: 0.06335835
case acc: 0.044487394
case acc: 0.07262837
case acc: 0.0317058
case acc: 0.048938535
case acc: 0.05280119
top acc: 0.0689 ::: bot acc: 0.0876
top acc: 0.0107 ::: bot acc: 0.0824
top acc: 0.0336 ::: bot acc: 0.1302
top acc: 0.0127 ::: bot acc: 0.0623
top acc: 0.0438 ::: bot acc: 0.0748
top acc: 0.0674 ::: bot acc: 0.0800
current epoch: 39
train loss is 0.054747
average val loss: 0.048271, accuracy: 0.0460
average test loss: 0.047939, accuracy: 0.0483
case acc: 0.05780738
case acc: 0.0355338
case acc: 0.06693649
case acc: 0.02774648
case acc: 0.04659488
case acc: 0.055010285
top acc: 0.0864 ::: bot acc: 0.0688
top acc: 0.0224 ::: bot acc: 0.0634
top acc: 0.0479 ::: bot acc: 0.1147
top acc: 0.0233 ::: bot acc: 0.0506
top acc: 0.0514 ::: bot acc: 0.0672
top acc: 0.0836 ::: bot acc: 0.0639
current epoch: 40
train loss is 0.052345
average val loss: 0.048529, accuracy: 0.0459
average test loss: 0.047524, accuracy: 0.0473
case acc: 0.056957006
case acc: 0.032490037
case acc: 0.06170258
case acc: 0.027591879
case acc: 0.04360324
case acc: 0.061530665
top acc: 0.1095 ::: bot acc: 0.0457
top acc: 0.0472 ::: bot acc: 0.0386
top acc: 0.0694 ::: bot acc: 0.0932
top acc: 0.0397 ::: bot acc: 0.0342
top acc: 0.0654 ::: bot acc: 0.0532
top acc: 0.1026 ::: bot acc: 0.0465
current epoch: 41
train loss is 0.052209
average val loss: 0.056469, accuracy: 0.0539
average test loss: 0.055507, accuracy: 0.0538
case acc: 0.06350714
case acc: 0.04271248
case acc: 0.06093269
case acc: 0.03670522
case acc: 0.044675115
case acc: 0.07449
top acc: 0.1385 ::: bot acc: 0.0174
top acc: 0.0791 ::: bot acc: 0.0142
top acc: 0.0989 ::: bot acc: 0.0636
top acc: 0.0637 ::: bot acc: 0.0152
top acc: 0.0894 ::: bot acc: 0.0292
top acc: 0.1264 ::: bot acc: 0.0371
current epoch: 42
train loss is 0.055082
average val loss: 0.072283, accuracy: 0.0703
average test loss: 0.072915, accuracy: 0.0715
case acc: 0.08159512
case acc: 0.06919696
case acc: 0.068355195
case acc: 0.056481123
case acc: 0.061555564
case acc: 0.09195149
top acc: 0.1667 ::: bot acc: 0.0148
top acc: 0.1113 ::: bot acc: 0.0290
top acc: 0.1304 ::: bot acc: 0.0330
top acc: 0.0899 ::: bot acc: 0.0220
top acc: 0.1184 ::: bot acc: 0.0215
top acc: 0.1494 ::: bot acc: 0.0434
current epoch: 43
train loss is 0.058639
average val loss: 0.083767, accuracy: 0.0826
average test loss: 0.085116, accuracy: 0.0842
case acc: 0.093722805
case acc: 0.08534351
case acc: 0.07747042
case acc: 0.0703573
case acc: 0.07877605
case acc: 0.09941568
top acc: 0.1798 ::: bot acc: 0.0249
top acc: 0.1285 ::: bot acc: 0.0429
top acc: 0.1491 ::: bot acc: 0.0225
top acc: 0.1050 ::: bot acc: 0.0332
top acc: 0.1390 ::: bot acc: 0.0318
top acc: 0.1586 ::: bot acc: 0.0472
current epoch: 44
train loss is 0.060609
average val loss: 0.080361, accuracy: 0.0795
average test loss: 0.081626, accuracy: 0.0806
case acc: 0.085801974
case acc: 0.0796466
case acc: 0.07607221
case acc: 0.067966655
case acc: 0.08214219
case acc: 0.0918722
top acc: 0.1715 ::: bot acc: 0.0178
top acc: 0.1226 ::: bot acc: 0.0378
top acc: 0.1466 ::: bot acc: 0.0233
top acc: 0.1025 ::: bot acc: 0.0311
top acc: 0.1428 ::: bot acc: 0.0343
top acc: 0.1493 ::: bot acc: 0.0432
current epoch: 45
train loss is 0.059438
average val loss: 0.068923, accuracy: 0.0685
average test loss: 0.069373, accuracy: 0.0683
case acc: 0.070076264
case acc: 0.06102493
case acc: 0.06823852
case acc: 0.05658123
case acc: 0.07537322
case acc: 0.078283265
top acc: 0.1512 ::: bot acc: 0.0116
top acc: 0.1018 ::: bot acc: 0.0235
top acc: 0.1302 ::: bot acc: 0.0331
top acc: 0.0901 ::: bot acc: 0.0219
top acc: 0.1352 ::: bot acc: 0.0293
top acc: 0.1317 ::: bot acc: 0.0380
current epoch: 46
train loss is 0.054196
average val loss: 0.058685, accuracy: 0.0590
average test loss: 0.057826, accuracy: 0.0572
case acc: 0.060384803
case acc: 0.041902926
case acc: 0.06259366
case acc: 0.044957586
case acc: 0.06573482
case acc: 0.06764588
top acc: 0.1298 ::: bot acc: 0.0256
top acc: 0.0779 ::: bot acc: 0.0143
top acc: 0.1100 ::: bot acc: 0.0525
top acc: 0.0762 ::: bot acc: 0.0149
top acc: 0.1237 ::: bot acc: 0.0235
top acc: 0.1151 ::: bot acc: 0.0395
current epoch: 47
train loss is 0.049691
average val loss: 0.050383, accuracy: 0.0512
average test loss: 0.049090, accuracy: 0.0492
case acc: 0.056781877
case acc: 0.032808416
case acc: 0.060156226
case acc: 0.03383194
case acc: 0.052674145
case acc: 0.059137568
top acc: 0.1064 ::: bot acc: 0.0490
top acc: 0.0512 ::: bot acc: 0.0347
top acc: 0.0857 ::: bot acc: 0.0768
top acc: 0.0582 ::: bot acc: 0.0179
top acc: 0.1056 ::: bot acc: 0.0209
top acc: 0.0968 ::: bot acc: 0.0511
current epoch: 48
train loss is 0.049071
average val loss: 0.047363, accuracy: 0.0482
average test loss: 0.046680, accuracy: 0.0469
case acc: 0.058320597
case acc: 0.034648106
case acc: 0.06340964
case acc: 0.027442392
case acc: 0.04331039
case acc: 0.054051947
top acc: 0.0843 ::: bot acc: 0.0711
top acc: 0.0256 ::: bot acc: 0.0603
top acc: 0.0608 ::: bot acc: 0.1017
top acc: 0.0382 ::: bot acc: 0.0360
top acc: 0.0830 ::: bot acc: 0.0356
top acc: 0.0790 ::: bot acc: 0.0686
current epoch: 49
train loss is 0.051441
average val loss: 0.049482, accuracy: 0.0498
average test loss: 0.049398, accuracy: 0.0499
case acc: 0.06264564
case acc: 0.042315386
case acc: 0.06920526
case acc: 0.028114814
case acc: 0.044190593
case acc: 0.052882805
top acc: 0.0701 ::: bot acc: 0.0859
top acc: 0.0123 ::: bot acc: 0.0783
top acc: 0.0418 ::: bot acc: 0.1209
top acc: 0.0219 ::: bot acc: 0.0522
top acc: 0.0620 ::: bot acc: 0.0566
top acc: 0.0672 ::: bot acc: 0.0804
current epoch: 50
train loss is 0.054160
average val loss: 0.050767, accuracy: 0.0502
average test loss: 0.050829, accuracy: 0.0513
case acc: 0.062986165
case acc: 0.043951817
case acc: 0.07132693
case acc: 0.029818185
case acc: 0.046772853
case acc: 0.05287876
top acc: 0.0694 ::: bot acc: 0.0868
top acc: 0.0110 ::: bot acc: 0.0815
top acc: 0.0364 ::: bot acc: 0.1268
top acc: 0.0164 ::: bot acc: 0.0577
top acc: 0.0508 ::: bot acc: 0.0678
top acc: 0.0672 ::: bot acc: 0.0804
