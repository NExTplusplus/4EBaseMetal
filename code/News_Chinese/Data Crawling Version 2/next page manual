import argparse
import time
import datetime
import sys
import pickle
from random import randint
import pandas as pd
import sqlalchemy as sq
import os

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.common.exceptions import *
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.proxy import Proxy, ProxyType
from bs4 import BeautifulSoup


def crawler_type_keyword(wait_time,analyst_company, news_type, webdriver_link,website_link,data_base_conn
                         ,keyword_report = 'All',xpath = None,tag_element = None,keyword_next_page =None,keyword_filter = []):
    # Load the Chrome Driver
    driver = webdriver.Chrome(webdriver_link)
    print('init driver')
    
    # Open the website firstpage
    current_website_link = website_link
    driver.get(current_website_link)
    current_page = 1
    print('Load Page')
    time.sleep(wait_time)
    print('Start Crawling')
    
    
    driver.execute_script("window.open('');")
    driver.switch_to.window(driver.window_handles[0])
    
    try:
        # Record we have proceed how many page
        record_page = 0
        crawled_data = 0
        while True:
            if keyword_report!= 'All':
            # Check if we have particular report type we want to extract:
                if type(keyword_report)!= str:
                    print('Error : keyword_report can only be one keyword and the type should be string')
                    driver.close()
                    return 
                else:
                    total_link = driver.find_elements_by_partial_link_text(keyword_report)
            else:
                if xpath == None or tag_element == None:
                    print('Error : class_type cannot be none, have to key in class name if u choose for All link option, if not pls enter keyword_report')
                    driver.close()
                    return 
                else:
                    temp = driver.find_element_by_xpath(xpath)
                    total_link = temp.find_elements_by_tag_name(tag_element)
            
            # Now select potentail links for report type we are interested in the current page
            
            if keyword_filter == 'All':
                filter_link = list(set(total_link))
            else:
                selected_link = []
                for keyword in keyword_filter:
                
                    current_link = driver.find_elements_by_partial_link_text(keyword)
                    selected_link = list(set().union(selected_link,current_link))
                filter_link = list(set(selected_link) & set(total_link))
           
            for link in total_link:
                
                # Crawl url,html content, title and update to database
                
                print(link)
                new_input = {}
                new_input['title'] = [link.text]
                new_input['company'] = [analyst_company]
                new_input['type'] = [news_type]
                new_input['url'] = [link.get_attribute("href")]
                
                if link in filter_link:
                    
                    temp = link.get_attribute("href")
                    driver.switch_to.window(driver.window_handles[1])
                    driver.get(temp)              
                    driver.implicitly_wait(2)
                    
                    new_input['html'] = [str(BeautifulSoup(driver.page_source, "html.parser"))]
                    driver.switch_to.window(driver.window_handles[0])
                
                
                df_input = pd.DataFrame(new_input)
                df_input.to_sql(name='analyst_table', con=data_base_conn, if_exists='append',index=False)
                crawled_data+=1
                
            record_page += 1
            print('At page : ',record_page)
            print('Link crawled : ',crawled_data)
            
            #Check condition whehter to go to next page
            
            if keyword_next_page:
                current_page +=1
                temp_next = driver.find_element_by_xpath(keyword_next_page)
                next_page_link = temp_next.find_elements_by_partial_link_text(str(current_page))
                # Check whether there is next page to go
                if next_page_link:
                    page_link_check = next_page_link[0].click()
                else:
                    break
            else:
                break
    except Exception as e: 
        print('Program Died at '+str(record_page))
        print('reason is : ')
        print(e)
        return 
    driver.close()
    print("Completed")
    print('Link crawled : ',crawled_data)      
    
    
############################################################################################################################
Example

engine = sq.create_engine("mysql+pymysql://root:next++4e@localhost/Alternative_DB?host=localhost?port=3306")
conn = engine.connect()
analyst_company = '兴证'
news_type = 'Daily'
webdriver_link = '/home/liangchen/4e/Alternative Data/chromedriver'
website_link = 'http://www.xzfutures.com/deeconomic_cid_106.html'
keyword_filter = ['中心早评']
#'铜','铅','锌','镍','铝','锡']
data_base_conn = conn
keyword_next_page = '//ul[@class="homelist"]'
xpath = '//tbody[@class="arti_cc"]'
wait_time = 2
#class_type = 'kf_jy

crawler_type_keyword(wait_time,analyst_company, news_type, webdriver_link,website_link,data_base_conn,
                     keyword_report = '中心早评',xpath = xpath,tag_element='a',
                     keyword_next_page =keyword_next_page,keyword_filter = keyword_filter)
